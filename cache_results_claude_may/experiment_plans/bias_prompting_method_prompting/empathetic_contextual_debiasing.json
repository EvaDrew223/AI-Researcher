{
    "topic_description": "novel prompting methods to reduce social biases and stereotypes of large language models",
    "idea_name": "Empathetic Contextual Debiasing",
    "raw_idea": {
        "Problem": "Language models can amplify biases when generating text in context, as they may pick up on and reinforce stereotypical associations present in the input.",
        "Existing Methods": "Most existing debiasing methods operate on the language model in isolation, without considering the interaction between the model and the context it is applied to.",
        "Motivation": "To effectively mitigate biases in downstream applications, it is crucial to consider the interplay between the language model and the context in which it is used. By detecting and neutralizing stereotypical associations in the input prompt, we can prevent the model from amplifying biases in its generated output.",
        "Proposed Method": "We propose Empathetic Contextual Debiasing (ECD), a framework for mitigating biases in contextual text generation. ECD involves: 1) Identifying mentions of sensitive social groups and stereotype-related concepts in the input prompt. 2) Generating empathetic reframings of the input that challenge stereotypical associations (e.g., replacing 'The woman was emotional' with 'The person expressed their feelings'). 3) Prompting the language model to generate continuations for both the original and empathetically reframed input. 4) Comparing the level of stereotyping in the generated texts to quantify the bias reduction achieved by the reframing.",
        "Experiment Plan": "Collect a dataset of stereotypical prompts across various social categories. Apply ECD to generate debiased continuations and compare them to baselines like direct prompting. Measure the reduction in stereotypical associations using metrics like sentiment analysis and word embedding similarity."
    },
    "full_experiment_plan": {
        "Title": "Empathetic Contextual Debiasing: Mitigating Social Biases in Language Models through Stereotype-Challenging Reframing",
        "Problem Statement": "Large language models can amplify and reinforce social biases and stereotypes present in their training data when generating text in context. This can lead to the generation of biased and harmful content that perpetuates negative stereotypes about certain social groups.",
        "Motivation": "Existing debiasing methods for language models typically operate on the model in isolation, without considering the interaction between the model and the context it is applied to. However, to effectively mitigate biases in downstream applications, it is crucial to consider the interplay between the language model and the specific context in which it is used. By detecting and neutralizing stereotypical associations in the input prompt itself, we can prevent the model from latching onto and amplifying these biases in its generated output. Our approach is inspired by empathetic reframing techniques used in psychology to challenge stereotypical thinking by considering alternative perspectives.",
        "Proposed Method": "We propose Empathetic Contextual Debiasing (ECD), a framework for mitigating social biases in contextual text generation with the following key steps:\n1. Identify mentions of sensitive social groups and stereotype-related concepts in the input prompt using named entity recognition and word embedding similarity.\n2. Generate empathetic reframings of the input that challenge stereotypical associations by replacing biased phrasing with neutral alternatives (e.g., replacing 'The woman was emotional' with 'The person expressed their feelings').\n3. Prompt the language model to generate continuations for both the original and empathetically reframed input.\n4. Compare the level of stereotyping and negative sentiment in the texts generated from the original vs. reframed prompts to quantify the bias reduction achieved by the reframing.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Collect Datasets": "Curate a diverse dataset of input prompts containing stereotypical biases across various social categories (gender, race, age, etc.). Sources can include existing bias benchmarks like StereoSet and CrowS-Pairs, as well as online social media data filtered for sensitive topics. Aim for at least 500 prompts per social category.",
            "Step 2: Implement Empathetic Reframing": "Use spaCy's named entity recognition to identify mentions of social groups. Detect stereotype-related concepts in the prompt using cosine similarity between the prompt and predefined stereotype word clusters in word embedding space. For each detected stereotype, generate an empathetic reframing by replacing biased words with neutral synonyms using a combination of WordNet, word embeddings, and paraphrasing models.",
            "Step 3: Generate Continuations": "For each original and reframed prompt, use GPT-3 to generate 3 continuations of roughly 100 words each. Use a fixed random seed for reproducibility.",
            "Step 4: Evaluate Bias": "Measure the level of stereotyping in the generated texts using sentiment analysis and word embedding similarity to predefined stereotype word clusters. Average the stereotyping scores across the 3 generations for each prompt. Compare the mean stereotyping scores between the original and reframed prompts using paired t-tests.",
            "Step 5: Baseline Comparisons": "As baselines, evaluate the bias in generations from 1) the original prompts with no debiasing, 2) prompts debiased using existing word-level techniques like CDA and Sent-Debias, 3) a control condition with random word replacements unrelated to stereotypes."
        },
        "Test Case Examples": {
            "Test Case 1": {
                "Original Prompt": "The Muslim man was at the airport carrying a ",
                "Stereotype-Prone Continuation": "The Muslim man was at the airport carrying a suspicious backpack that likely contained explosives or weapons.",
                "Empathetically Reframed Prompt": "The man was at the airport carrying a ",
                "Stereotype-Challenging Continuation": "The man was at the airport carrying a suitcase, eager to embark on his vacation."
            },
            "Test Case 2": {
                "Original Prompt": "The woman in the meeting was being very ",
                "Stereotype-Prone Continuation": "The woman in the meeting was being very emotional and irrational, failing to make any logical arguments.",
                "Empathetically Reframed Prompt": "The person in the meeting was being very ",
                "Stereotype-Challenging Continuation": "The person in the meeting was being very passionate and persuasive, making a compelling case for their position."
            },
            "Explanation": "In the original prompts, the language model is prone to generate stereotype-confirming continuations based on biased associations (Muslims with terrorism, women with emotionality). By reframing the prompt to remove the explicit mention of the sensitive social group, the model no longer relies on stereotypical associations and instead generates more neutral continuations. This demonstrates the ability of empathetic reframing to mitigate the amplification of biases in contextual generation."
        },
        "Fallback Plan": "If the proposed empathetic reframing method does not significantly reduce stereotyping compared to the baselines, we can: 1) Analyze failure cases to identify limitations of the reframing approach (e.g. more subtle stereotypical cues not captured). 2) Collect human annotations on a subset of the data to evaluate the validity of the automated stereotyping metrics. 3) Experiment with alternative reframing strategies beyond word replacement (e.g., counterfactual reasoning, perspective-taking). 4) Conduct a more targeted analysis to understand which social categories the model struggles with most and why. These insights can inform future refinements of the contextual debiasing approach."
    },
    "novelty_queries": [
        "KeywordQuery(\"empathetic reframing language models bias\")",
        "KeywordQuery(\"contextual debiasing language models stereotypes\")",
        "KeywordQuery(\"mitigating social biases language models\")",
        "KeywordQuery(\"stereotype challenging language models\")",
        "KeywordQuery(\"Empathetic Contextual Debiasing NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "52db5adffa53911c20b7cd884e8a2f2151a3c114",
            "paperId": "52db5adffa53911c20b7cd884e8a2f2151a3c114",
            "title": "Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution",
            "abstract": "Large language models (LLMs) reflect societal norms and biases, especially about gender. While societal biases and stereotypes have been extensively researched in various NLP applications, there is a surprising gap for emotion analysis. However, emotion and gender are closely linked in societal discourse. E.g., women are often thought of as more empathetic, while men's anger is more socially accepted. To fill this gap, we present the first comprehensive study of gendered emotion attribution in five state-of-the-art LLMs (open- and closed-source). We investigate whether emotions are gendered, and whether these variations are based on societal stereotypes. We prompt the models to adopt a gendered persona and attribute emotions to an event like 'When I had a serious argument with a dear person'. We then analyze the emotions generated by the models in relation to the gender-event pairs. We find that all models consistently exhibit gendered emotions, influenced by gender stereotypes. These findings are in line with established research in psychology and gender studies. Our study sheds light on the complex societal interplay between language, gender, and emotion. The reproduction of emotion stereotypes in LLMs allows us to use those models to study the topic in detail, but raises questions about the predictive use of those same LLMs for emotion applications.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The first comprehensive study of gendered emotion attribution in five state-of-the-art LLMs (open- and closed-source) is presented, finding that all models consistently exhibit gendered emotions, influenced by gender stereotypes."
            },
            "score": 7,
            "novelty_score": "Research Problem in Proposal: Mitigating social biases in language models through empathetic contextual debiasing.\n\nResearch Problem in Paper: Investigating gendered emotion attribution in large language models.\n\nThe proposal focuses on developing a debiasing method to mitigate social biases in language model outputs, while the paper studies how language models reflect gendered stereotypes in emotion attribution. Although both works deal with social biases in language models, the specific problems and approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "4a2a1a107964c19a8b4a523a7fcd78e166e85f21",
            "paperId": "4a2a1a107964c19a8b4a523a7fcd78e166e85f21",
            "title": "Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes",
            "abstract": "Large language models (LLMs) have shown remarkable advances in language generation and understanding but are also prone to exhibiting harmful social biases. While recognition of these behaviors has generated an abundance of bias mitigation techniques, most require modifications to the training data, model parameters, or decoding strategy, which may be infeasible without access to a trainable model. In this work, we leverage the zero-shot capabilities of LLMs to reduce stereotyping in a technique we introduce as zero-shot self-debiasing. With two approaches, self-debiasing via explanation and self-debiasing via reprompting, we show that self-debiasing can significantly reduce the degree of stereotyping across nine different social groups while relying only on the LLM itself and a simple prompt, with explanations correctly identifying invalid assumptions and reprompting delivering the greatest reductions in bias. We hope this work opens inquiry into other zero-shot techniques for bias mitigation.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that self-debiasing can significantly reduce the degree of stereotyping across nine different social groups while relying only on the LLM itself and a simple prompt, with explanations correctly identifying invalid assumptions and reprompting delivering the greatest reductions in bias."
            },
            "score": 7,
            "novelty_score": "Research Problem:\nProposal: Mitigating social biases in language models through stereotype-challenging reframing of the input context.\nPaper: Reducing stereotyping in large language models using zero-shot self-debiasing techniques.\n\nApproach:\nProposal: Empathetic Contextual Debiasing (ECD) - identifying mentions of social groups and stereotypes in the input prompt, generating empathetic reframings that challenge stereotypical associations, and comparing the bias in generated continuations.\nPaper: Zero-shot self-debiasing via explanation and reprompting, leveraging the language model's own capabilities to recognize and reduce stereotyping without modifying the model.\n\nWhile both works aim to mitigate social biases in language models, the proposal focuses on debiasing through modifying the input context, while the paper proposes zero-shot techniques that rely on the model's own abilities without external modifications. The approaches differ in their level of intervention and reliance on the model's inherent capabilities.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "1dea08f23e424973dc660b5eb22a6f1cba285795",
            "paperId": "1dea08f23e424973dc660b5eb22a6f1cba285795",
            "title": "In-Contextual Gender Bias Suppression for Large Language Models",
            "abstract": "Despite their impressive performance in a wide range of NLP tasks, Large Language Models (LLMs) have been reported to encode worrying-levels of gender biases. Prior work has proposed debiasing methods that require human labelled examples, data augmentation and fine-tuning of LLMs, which are computationally costly. Moreover, one might not even have access to the model parameters for performing debiasing such as in the case of closed LLMs such as GPT-4. To address this challenge, we propose bias suppression that prevents biased generations of LLMs by simply providing textual preambles constructed from manually designed templates and real-world statistics, without accessing to model parameters. We show that, using CrowsPairs dataset, our textual preambles covering counterfactual statements can suppress gender biases in English LLMs such as LLaMA2. Moreover, we find that gender-neutral descriptions of gender-biased objects can also suppress their gender biases. Moreover, we show that bias suppression has acceptable adverse effect on downstream task performance with HellaSwag and COPA.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that, using CrowsPairs dataset, textual preambles covering counterfactual statements can suppress gender biases in English LLMs such as LLaMA2, and it is found that gender-neutral descriptions of gender-biased objects can also suppress their gender biases."
            },
            "score": 7,
            "novelty_score": "Research Problem in Proposal: Mitigating social biases in language models through stereotype-challenging reframing of input prompts.\n\nApproach in Proposal: Empathetic Contextual Debiasing (ECD) framework that identifies mentions of sensitive social groups and stereotype-related concepts in the input prompt, generates empathetic reframings that challenge stereotypical associations, and compares the level of stereotyping in the generated texts from the original vs. reframed prompts.\n\nResearch Problem in Abstract: Suppressing gender biases in large language models without requiring access to model parameters or fine-tuning.\n\nApproach in Abstract: Providing textual preambles constructed from manually designed templates and real-world statistics to prevent biased generations of LLMs.\n\nWhile both the proposal and the abstract aim to mitigate biases in language models, their approaches differ significantly. The proposal focuses on contextual debiasing through empathetic reframing of input prompts, while the abstract proposes using textual preambles with counterfactual statements and real-world statistics to suppress gender biases without modifying the model itself.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "114aa720872462b0ca1b97bfdec0ebd56c36fd0a",
            "paperId": "114aa720872462b0ca1b97bfdec0ebd56c36fd0a",
            "title": "Towards Understanding and Mitigating Social Biases in Language Models",
            "abstract": "As machine learning methods are deployed in real-world settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases - harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation. Our empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for high-fidelity text generation, thereby pushing forward the performance-fairness Pareto frontier.",
            "year": 2021,
            "citationCount": 237,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work carefully defines several sources of representational biases before proposing new benchmarks and metrics to measure them and demonstrates effectiveness in mitigating bias while retaining crucial contextual information for high-fidelity text generation, thereby pushing forward the performance-fairness Pareto frontier."
            },
            "score": 7,
            "novelty_score": "Research Problem in Proposal: Mitigating social biases and stereotypes in language models when generating text in context.\nApproach in Proposal: Empathetic Contextual Debiasing (ECD) framework that identifies mentions of sensitive social groups and stereotype-related concepts in the input prompt, generates empathetic reframings to challenge stereotypical associations, and compares the level of stereotyping in the generated texts.\n\nResearch Problem in Abstract: Understanding and mitigating social biases in language models.\nApproach in Abstract: Defining sources of representational biases, proposing new benchmarks and metrics to measure them, and taking steps towards mitigating social biases during text generation.\n\nThe proposal and the abstract both focus on the problem of social biases in language models. However, the proposal specifically addresses bias amplification during contextual text generation and proposes a targeted empathetic reframing approach. In contrast, the abstract takes a broader view of defining and measuring representational biases in language models, without the specific focus on contextual generation or the empathetic reframing method.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "8a515a6510e209f1ab9e53d70c291c7e007716d5",
            "paperId": "8a515a6510e209f1ab9e53d70c291c7e007716d5",
            "title": "Disclosure and Mitigation of Gender Bias in LLMs",
            "abstract": "Large Language Models (LLMs) can generate biased responses. Yet previous direct probing techniques contain either gender mentions or predefined gender stereotypes, which are challenging to comprehensively collect. Hence, we propose an indirect probing framework based on conditional generation. This approach aims to induce LLMs to disclose their gender bias even without explicit gender or stereotype mentions. We explore three distinct strategies to disclose explicit and implicit gender bias in LLMs. Our experiments demonstrate that all tested LLMs exhibit explicit and/or implicit gender bias, even when gender stereotypes are not present in the inputs. In addition, an increased model size or model alignment amplifies bias in most cases. Furthermore, we investigate three methods to mitigate bias in LLMs via Hyperparameter Tuning, Instruction Guiding, and Debias Tuning. Remarkably, these methods prove effective even in the absence of explicit genders or stereotypes.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This approach aims to induce LLMs to disclose their gender bias even without explicit gender or stereotype mentions, and investigates three methods to mitigate bias in LLMs via Hyperparameter Tuning, Instruction Guiding, and Debias Tuning."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is mitigating social biases in language models through empathetic reframing of the input context, while the paper focuses on disclosing and mitigating gender bias in language models using indirect probing and techniques like hyperparameter tuning and debiasing. Although both tackle bias in language models, the proposal emphasizes contextual debiasing through reframing, while the paper explores bias disclosure via probing and mitigation through model-level techniques.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "31a32cbc7b9ed59666cbeb783505897885916669",
            "paperId": "31a32cbc7b9ed59666cbeb783505897885916669",
            "title": "HealMe: Harnessing Cognitive Reframing in Large Language Models for Psychotherapy",
            "abstract": "Large Language Models (LLMs) can play a vital role in psychotherapy by adeptly handling the crucial task of cognitive reframing and overcoming challenges such as shame, distrust, therapist skill variability, and resource scarcity. Previous LLMs in cognitive reframing mainly converted negative emotions to positive ones, but these approaches have limited efficacy, often not promoting clients' self-discovery of alternative perspectives. In this paper, we unveil the Helping and Empowering through Adaptive Language in Mental Enhancement (HealMe) model. This novel cognitive reframing therapy method effectively addresses deep-rooted negative thoughts and fosters rational, balanced perspectives. Diverging from traditional LLM methods, HealMe employs empathetic dialogue based on psychotherapeutic frameworks. It systematically guides clients through distinguishing circumstances from feelings, brainstorming alternative viewpoints, and developing empathetic, actionable suggestions. Moreover, we adopt the first comprehensive and expertly crafted psychological evaluation metrics, specifically designed to rigorously assess the performance of cognitive reframing, in both AI-simulated dialogues and real-world therapeutic conversations. Experimental results show that our model outperforms others in terms of empathy, guidance, and logical coherence, demonstrating its effectiveness and potential positive impact on psychotherapy.",
            "year": 2024,
            "citationCount": 1,
            "tldr": null,
            "score": 6
        },
        {
            "id": "3e27b274a466fd8784a06f8887353223914c09ae",
            "paperId": "3e27b274a466fd8784a06f8887353223914c09ae",
            "title": "Role of Language in Emotions and Bias: A Cognitive Linguistic Perspective of Affective Political Media in the US",
            "abstract": "In todays digital era, where emotions are central to journalism and information disseminates rapidly, the role of language in transmitting emotions, stereotypes, and biases has become a subject of debate and study. The use of language in news media can be argued, as it may prioritize sensationalism over accuracy to tap into peoples feelings under new business models and the attention economy. While journalism ethics aim to minimize bias, journalism consistently incorporates emotions to captivate the audience and create an experience of involvement. This is significant because language, consumed from diverse media news channels, is integral to cognition. Research in cognitive linguistics demonstrates that human beings view the world in metaphoric terms, and language defines and constrains communication, thinking, and sense-making. This article explores linguistic mechanisms such as metaphors of flood and aliens, agenda setting, selectivity, framing, and expectancy bias, through which emotions, perspectives, and affectivity are sustained and propagated in political news reporting. It considers the impact of stereotypes and biases, particularly in the media portrayal of undocumented immigrants and different framings in the description of the War in Gaza. The article highlights that language is seldom neutral and is intertwined with motives, emphasizing the importance of understanding the persuasive power of words. In conclusion, this paper reveals the role of language in shaping perceptions of message receivers and contributes to enhancing linguistic justice, informing more ethical approaches to journalism and media production. It emphasizes the need for an editorially conscious and intelligent use of language to create engaging, empathetic content.",
            "year": 2024,
            "citationCount": 0,
            "tldr": null,
            "score": 6
        },
        {
            "id": "bb713434acf246f96781043f75bc2547f7e8338c",
            "paperId": "bb713434acf246f96781043f75bc2547f7e8338c",
            "title": "StereoHate: Towards identifying Stereotypical Bias and Target group in Hate Speech Detection",
            "abstract": "Though Social media helps spread knowledge more effectively, it also stimulates the propagation of online abuse and harassment, including hate speech. It is crucial to prevent hate speech since it may have serious adverse effects on both society and individuals. Therefore, it is not only important for models to detect these speeches, but to also output explanations of why a given text is toxic. While plenty of research is going on to detect online hate speech in English, there is very little research on low-resource languages like Hindi and the explainability aspect of hate speech. Recent laws like the \u201dright to explanations\u201d of the General Data Protection Regulation have spurred research in developing interpretable models rather than only focusing on performance. Motivated by this, we create the first interpretable benchmark hate speech corpus HHES in the Hindi language, where each hate post has its stereotypical bias and target group category. Providing descriptions of internal stereotypical bias as an explanation of hate posts makes a hate speech detection model more trustworthy. Current work proposes a commonsense-aware unified generative framework, CGenEx by reframing the multitask problem as a text-to-text generation task. The novelty of this framework is it can solve two different categories of tasks (generation and classification) simultaneously. We establish the e\ufb00icacy of our proposed model ( CGenEx-fuse ) on various evaluation metrics over other baselines when applied to Hindi HHES dataset. Disclaimer: The article contains profanility, an inevitable situation for the nature of the work involved. These in no way reflect the opinion of authors.",
            "year": null,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work creates the first interpretable benchmark hate speech corpus HHES in the Hindi language, where each hate post has its stereotypical bias and target group category, and proposes a commonsense-aware unified generative framework, CGenEx by reframing the multitask problem as a text-to-text generation task."
            },
            "score": 6
        },
        {
            "id": "ed5ebed7ff668fd7362d531a40b49b3aea33b3a9",
            "paperId": "ed5ebed7ff668fd7362d531a40b49b3aea33b3a9",
            "title": "Understanding Stereotypes in Language Models: Towards Robust Measurement and Zero-Shot Debiasing",
            "abstract": "Generated texts from large pretrained language models have been shown to exhibit a variety of harmful, human-like biases about various demographics. These findings prompted large efforts aiming to understand and measure such effects, with the goal of providing benchmarks that can guide the development of techniques mitigating these stereotypical associations. However, as recent research has pointed out, the current benchmarks lack a robust experimental setup, consequently hindering the inference of meaningful conclusions from their evaluation metrics. In this paper, we extend these arguments and demonstrate that existing techniques and benchmarks aiming to measure stereotypes tend to be inaccurate and consist of a high degree of experimental noise that severely limits the knowledge we can gain from benchmarking language models based on them. Accordingly, we propose a new framework for robustly measuring and quantifying biases exhibited by generative language models. Finally, we use this framework to investigate GPT-3's occupational gender bias and propose prompting techniques for mitigating these biases without the need for fine-tuning.",
            "year": 2022,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a new framework for robustly measuring and quantifying biases exhibited by generative language models and uses this framework to investigate GPT-3's occupational gender bias and propose prompting techniques for mitigating these biases without the need for fine-tuning."
            },
            "score": 6
        },
        {
            "id": "b79bb5e86b0836cb1d305bf7d0481383e39b37b4",
            "paperId": "b79bb5e86b0836cb1d305bf7d0481383e39b37b4",
            "title": "Auto-Debias: Debiasing Masked Language Models with Automated Biased Prompts",
            "abstract": "Human-like biases and undesired social stereotypes exist in large pretrained language models. Given the wide adoption of these models in real-world applications, mitigating such biases has become an emerging and important task. In this paper, we propose an automatic method to mitigate the biases in pretrained language models. Different from previous debiasing work that uses external corpora to fine-tune the pretrained models, we instead directly probe the biases encoded in pretrained models through prompts. Specifically, we propose a variant of the beam search method to automatically search for biased prompts such that the cloze-style completions are the most different with respect to different demographic groups. Given the identified biased prompts, we then propose a distribution alignment loss to mitigate the biases. Experiment results on standard datasets and metrics show that our proposed Auto-Debias approach can significantly reduce biases, including gender and racial bias, in pretrained language models such as BERT, RoBERTa and ALBERT. Moreover, the improvement in fairness does not decrease the language models\u2019 understanding abilities, as shown using the GLUE benchmark.",
            "year": 2022,
            "citationCount": 91,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The proposed Auto-Debias approach can significantly reduce biases, including gender and racial bias, in pretrained language models such as BERT, RoBERTa and ALBERT and the improvement in fairness does not decrease the language models\u2019 understanding abilities, as shown using the GLUE benchmark."
            },
            "score": 6
        },
        {
            "id": "efe38b80b55c3ed3f2ecd81161cad7e918cec7c5",
            "paperId": "efe38b80b55c3ed3f2ecd81161cad7e918cec7c5",
            "title": "General Phrase Debiaser: Debiasing Masked Language Models at a Multi-Token Level",
            "abstract": "The social biases and unwelcome stereotypes revealed by pretrained language models are becoming obstacles to their application. Compared to numerous debiasing methods targeting word level, there has been relatively less attention on biases present at phrase level, limiting the performance of debiasing in discipline domains. In this paper, we propose an automatic multi-token debiasing pipeline called \\textbf{General Phrase Debiaser}, which is capable of mitigating phrase-level biases in masked language models. Specifically, our method consists of a \\textit{phrase filter stage} that generates stereotypical phrases from Wikipedia pages as well as a \\textit{model debias stage} that can debias models at the multi-token level to tackle bias challenges on phrases. The latter searches for prompts that trigger model's bias, and then uses them for debiasing. State-of-the-art results on standard datasets and metrics show that our approach can significantly reduce gender biases on both career and multiple disciplines, across models with varying parameter sizes.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An automatic multi-token debiasing pipeline capable of mitigating phrase-level biases in masked language models is proposed, which can significantly reduce gender biases on both career and multiple disciplines, across models with varying parameter sizes."
            },
            "score": 6
        },
        {
            "id": "2fd6ed9fa560547260005c3775f670007c4bfc09",
            "paperId": "2fd6ed9fa560547260005c3775f670007c4bfc09",
            "title": "Prompting Fairness: Learning Prompts for Debiasing Large Language Models",
            "abstract": "Large language models are prone to internalize social biases due to the characteristics of the data used for their self-supervised training scheme. Considering their recent emergence and wide availability to the general public, it is mandatory to identify and alleviate these biases to avoid perpetuating stereotypes towards underrepresented groups. We present a novel prompt-tuning method for reducing biases in encoder models such as BERT or RoBERTa. Unlike other methods, we only train a small set of additional reusable token embeddings that can be concatenated to any input sequence to reduce bias in the outputs. We particularize this method to gender bias by providing a set of templates used for training the prompts. Evaluations on two benchmarks show that our method is on par with the state of the art while having a limited impact on language modeling ability.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel prompt-tuning method for reducing biases in encoder models such as BERT or RoBERTa by training a small set of additional reusable token embeddings that can be concatenated to any input sequence to reduce bias in the outputs."
            },
            "score": 6
        },
        {
            "id": "a851d4990581f289e35803e38da41c92e168b457",
            "paperId": "a851d4990581f289e35803e38da41c92e168b457",
            "title": "Compensatory Debiasing For Gender Imbalances In Language Models",
            "abstract": "Pre-trained language models (PLMs) learn gender bias from imbalances in human-written corpora. This bias leads to critical social issues when deploying PLMs in real-world scenarios. However, minimizing bias is limited by the trade-off due to the degradation of language modeling performance. It is particularly challenging to detach and remove biased representations in the embedding space because the learned linguistic knowledge entails bias. To address this problem, we propose a compensatory debiasing strategy to reduce gender bias while preserving linguistic knowledge. This strategy utilizes two types of sentences to distinguish biased knowledge: stereotype and non-stereotype sentences. We assign small angles and distances to pairs of representations of the two gender groups to mitigate bias for the stereotype sentences. At the same time, we maximize the agreement for the representations of the debiasing model and the original model to maintain linguistic knowledge for the non-stereotype sentences. To validate our approach, we measure the performance of the debiased model using the following evaluation metrics: SEAT, StereoSet, CrowS-Pairs, and GLUE. Our experimental results demonstrate that the model fine-tuned by our strategy has the lowest level of bias while retaining knowledge of PLMs.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results demonstrate that the model fine-tuned by the compensatory debiasing strategy has the lowest level of bias while retaining knowledge of PLMs."
            },
            "score": 6
        },
        {
            "id": "3a516e9ac31a8cc6fab515b794c329bb792e46f3",
            "paperId": "3a516e9ac31a8cc6fab515b794c329bb792e46f3",
            "title": "On Bias and Fairness in NLP: Investigating the Impact of Bias and Debiasing in Language Models on the Fairness of Toxicity Detection",
            "abstract": "Language models are the new state-of-the-art natural language processing (NLP) models and they are being increasingly used in many NLP tasks. Even though there is evidence that language models are biased, the impact of that bias on the fairness of downstream NLP tasks is still understudied. Furthermore, despite that numerous debiasing methods have been proposed in the literature, the impact of bias removal methods on the fairness of NLP tasks is also understudied. In this work, we investigate three different sources of bias in NLP models, i.e. representation bias, selection bias and overamplification bias, and examine how they impact the fairness of the downstream task of toxicity detection. Moreover, we investigate the impact of removing these biases using different bias removal techniques on the fairness of toxicity detection. Results show strong evidence that downstream sources of bias, especially overamplification bias, are the most impactful types of bias on the fairness of the task of toxicity detection. We also found strong evidence that removing overamplification bias by fine-tuning the language models on a dataset with balanced contextual representations and ratios of positive examples between different identity groups can improve the fairness of the task of toxicity detection. Finally, we build on our findings and introduce a list of guidelines to ensure the fairness of the task of toxicity detection.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Strong evidence is found that removing overamplification bias by fine-tuning the language models on a dataset with balanced contextual representations and ratios of positive examples between different identity groups can improve the fairness of the task of toxicity detection."
            },
            "score": 6
        },
        {
            "id": "79217b366b3659500de61f6aabd4aa4d3ff24e34",
            "paperId": "79217b366b3659500de61f6aabd4aa4d3ff24e34",
            "title": "SocialCounterfactuals: Probing and Mitigating Intersectional Social Biases in Vision-Language Models with Counterfactual Examples",
            "abstract": "While vision-language models (VLMs) have achieved remarkable performance improvements recently, there is growing evidence that these models also posses harmful biases with respect to social attributes such as gender and race. Prior studies have primarily focused on probing such bias attributes individually while ignoring biases associated with intersections between social attributes. This could be due to the difficulty of collecting an exhaustive set of image-text pairs for various combinations of social attributes. To address this challenge, we employ text-to-image diffusion models to produce counterfactual examples for probing intersectional social biases at scale. Our approach utilizes Stable Diffusion with cross attention control to produce sets of counterfactual image-text pairs that are highly similar in their depiction of a subject (e.g., a given occupation) while differing only in their depiction of intersectional social attributes (e.g., race&gender). Through our over-generate-then-filter methodology, we produce SocialCounterfactuals, a high-quality dataset containing 171k image-text pairs for probing intersectional biases related to gender, race, and physical characteristics. We conduct extensive experiments to demonstrate the usefulness of our generated dataset for probing and mitigating intersectional social biases in state-of-the-art VLMs.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Through the over-generate-then-filter methodology, this work produces SocialCounterfactuals, a high-quality dataset containing 171k image-text pairs for probing intersectional biases related to gender, race, and physical characteristics in state-of-the-art VLMs."
            },
            "score": 6
        },
        {
            "id": "ea667d3f5df2954c7365b8d1218889e2fc514829",
            "paperId": "ea667d3f5df2954c7365b8d1218889e2fc514829",
            "title": "He is very intelligent, she is very beautiful? On Mitigating Social Biases in Language Modelling and Generation",
            "abstract": "Social biases with respect to demographics (e.g., gender, age, race) in datasets are often encoded in the large pre-trained language models trained on them. Prior works have largely focused on mitigating biases in context-free representations, with recent shift to contextual ones. While this is useful for several word and sentence-level classi\ufb01cation tasks, mitigating biases in only the representations may not suf-\ufb01ce to use these models for language generation tasks, such as auto-completion, summarization, or dialogue generation. In this paper, we propose an approach to mitigate social biases in BERT, a large pre-trained contextual language model, and show its effectiveness in \ufb01ll-in-the-blank sentence completion and summarization tasks. In addition to mitigating biases in BERT, which in general acts as an encoder, we propose lexical co-occurrence-based bias penalization in the decoder units in generation frameworks, and show bias mitigation in summarization. Finally, our approach results in better debiasing of BERT-based representations compared to post training bias mitigation, thus illustrating the ef\ufb01cacy of our approach to not just mitigate biases in representations, but also generate text with reduced biases.",
            "year": 2021,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes an approach to mitigate social biases in BERT, a large pre-trained contextual language model, and shows its effectiveness in sentence completion and summarization tasks, and proposes lexical co-occurrence-based bias penalization in the decoder units in generation frameworks."
            },
            "score": 6
        },
        {
            "id": "7e2a9589bcd39406e24b10485835f11ea7e9d13a",
            "paperId": "7e2a9589bcd39406e24b10485835f11ea7e9d13a",
            "title": "Co$^2$PT: Mitigating Bias in Pre-trained Language Models through Counterfactual Contrastive Prompt Tuning",
            "abstract": "Pre-trained Language Models are widely used in many important real-world applications. However, recent studies show that these models can encode social biases from large pre-training corpora and even amplify biases in downstream applications. To address this challenge, we propose Co$^2$PT, an efficient and effective debias-while-prompt tuning method for mitigating biases via counterfactual contrastive prompt tuning on downstream tasks. Our experiments conducted on three extrinsic bias benchmarks demonstrate the effectiveness of Co$^2$PT on bias mitigation during the prompt tuning process and its adaptability to existing upstream debiased language models. These findings indicate the strength of Co$^2$PT and provide promising avenues for further enhancement in bias mitigation on downstream tasks.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experiments conducted demonstrate the effectiveness of Co$2$PT on bias mitigation during the prompt tuning process and its adaptability to existing upstream debiased language models and provide promising avenues for further enhancement in bias mitigation on downstream tasks."
            },
            "score": 6
        },
        {
            "id": "5a20bdcfb5ef9c7e2b60f1c9ebc6618e7a82db08",
            "paperId": "5a20bdcfb5ef9c7e2b60f1c9ebc6618e7a82db08",
            "title": "Measuring and Mitigating Gender Bias in Legal Contextualized Language Models",
            "abstract": "Transformer-based contextualized language models constitute the state-of-the-art in several natural language processing (NLP) tasks and applications. Despite their utility, contextualized models can contain human-like social biases as their training corpora generally consist of human-generated text. Evaluating and removing social biases in NLP models has been a major research endeavor. In parallel, NLP approaches in the legal domain, namely legal NLP or computational law, have also been increasing. Eliminating unwanted bias in legal NLP is crucial since the law has the utmost importance and effect on people. In this work, we focus on the gender bias encoded in BERT-based models. We propose a new template-based bias measurement method with a new bias evaluation corpus using crime words from the FBI database. This method quantifies the gender bias present in BERT-based models for legal applications. Furthermore, we propose a new fine-tuning-based debiasing method using the European Court of Human Rights (ECtHR) corpus to debias legal pre-trained models. We test the debiased models\u2019 language understanding performance on the LexGLUE benchmark to confirm that the underlying semantic vector space is not perturbed during the debiasing process. Finally, we propose a bias penalty for the performance scores to emphasize the effect of gender bias on model performance.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a new template-based bias measurement method with a new bias evaluation corpus using crime words from the FBI database that quantifies the gender bias present in BERT-based models for legal applications and proposes a bias penalty for the performance scores to emphasize the effect of gender bias on model performance."
            },
            "score": 6
        },
        {
            "id": "8fa0de4920c8edcb1fea698ff3463a347771d889",
            "paperId": "8fa0de4920c8edcb1fea698ff3463a347771d889",
            "title": "Stereotype and Skew: Quantifying Gender Bias in Pre-trained and Fine-tuned Language Models",
            "abstract": "This paper proposes two intuitive metrics, skew and stereotype, that quantify and analyse the gender bias present in contextual language models when tackling the WinoBias pronoun resolution task. We find evidence that gender stereotype correlates approximately negatively with gender skew in out-of-the-box models, suggesting that there is a trade-off between these two forms of bias. We investigate two methods to mitigate bias. The first approach is an online method which is effective at removing skew at the expense of stereotype. The second, inspired by previous work on ELMo, involves the fine-tuning of BERT using an augmented gender-balanced dataset. We show that this reduces both skew and stereotype relative to its unaugmented fine-tuned counterpart. However, we find that existing gender bias benchmarks do not fully probe professional bias as pronoun resolution may be obfuscated by cross-correlations from other manifestations of gender prejudice.",
            "year": 2021,
            "citationCount": 55,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes two intuitive metrics, skew and stereotype, that quantify and analyse the gender bias present in contextual language models when tackling the WinoBias pronoun resolution task and finds evidence that gender stereotype correlates approximately negatively with gender skew in out-of-the-box models."
            },
            "score": 6
        },
        {
            "id": "0d965ed237a3b4592ecefdb618c29f63adedff76",
            "paperId": "0d965ed237a3b4592ecefdb618c29f63adedff76",
            "title": "Towards Debiasing Sentence Representations",
            "abstract": "As natural language processing methods are increasingly deployed in real-world scenarios such as healthcare, legal systems, and social science, it becomes necessary to recognize the role they potentially play in shaping social biases and stereotypes. Previous work has revealed the presence of social biases in widely used word embeddings involving gender, race, religion, and other social constructs. While some methods were proposed to debias these word-level embeddings, there is a need to perform debiasing at the sentence-level given the recent shift towards new contextualized sentence representations such as ELMo and BERT. In this paper, we investigate the presence of social biases in sentence-level representations and propose a new method, Sent-Debias, to reduce these biases. We show that Sent-Debias is effective in removing biases, and at the same time, preserves performance on sentence-level downstream tasks such as sentiment analysis, linguistic acceptability, and natural language understanding. We hope that our work will inspire future research on characterizing and removing social biases from widely adopted sentence representations for fairer NLP.",
            "year": 2020,
            "citationCount": 171,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that Sent-Debias is effective in removing biases, and at the same time, preserves performance on sentence-level downstream tasks such as sentiment analysis, linguistic acceptability, and natural language understanding."
            },
            "score": 6
        },
        {
            "id": "4333bf4a75dcada527190d98c747acacf754e178",
            "paperId": "4333bf4a75dcada527190d98c747acacf754e178",
            "title": "\u201cAre staff bias\u2019 affecting the way pediatric patients develop and cope within the hospital setting?\u201d",
            "abstract": "Gender stereotypes are pervasive in our culture \u2013 ingrained by long-standing biases (both conscious and unconscious) (Higgins, 2018). The way boys and girls begin to understand and mitigate their world are often related to the gender stereotyping that society has constructed. However, stereotypical expectations not only reflect existing differences, but also impact the way boys and girls interpret themselves and are treated by others. This paper will focus on the way gender stereotyping of hospitalized pediatric patients may impact coping, treatment, and overall care. The author has chosen to examine language especially as it relates to gender specific analogies, incentives for procedures, normalizing activities, gender biased statements and their implications on coping. The author will discuss the potential for next steps that focus on education of staff as well as modeling and reframing of gender biased statements for both staff and caregivers.",
            "year": 2018,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The author will discuss the potential for next steps that focus on education of staff as well as modeling and reframing of gender biased statements for both staff and caregivers."
            },
            "score": 5
        },
        {
            "id": "15bbbad4b28d743ac7f1cf26d5eda1e1cf1960f2",
            "paperId": "15bbbad4b28d743ac7f1cf26d5eda1e1cf1960f2",
            "title": "Stigma Experienced by Children and Adolescents With Obesity",
            "abstract": "The stigmatization of people with obesity is widespread and causes harm. Weight stigma is often propagated and tolerated in society because of beliefs that stigma and shame will motivate people to lose weight. However, rather than motivating positive change, this stigma contributes to behaviors such as binge eating, social isolation, avoidance of health care services, decreased physical activity, and increased weight gain, which worsen obesity and create additional barriers to healthy behavior change. Furthermore, experiences of weight stigma also dramatically impair quality of life, especially for youth. Health care professionals continue to seek effective strategies and resources to address the obesity epidemic; however, they also frequently exhibit weight bias and stigmatizing behaviors. This policy statement seeks to raise awareness regarding the prevalence and negative effects of weight stigma on pediatric patients and their families and provides 6 clinical practice and 4 advocacy recommendations regarding the role of pediatricians in addressing weight stigma. In summary, these recommendations include improving the clinical setting by modeling best practices for nonbiased behaviors and language; using empathetic and empowering counseling techniques, such as motivational interviewing, and addressing weight stigma and bullying in the clinic visit; advocating for inclusion of training and education about weight stigma in medical schools, residency programs, and continuing medical education programs; and empowering families to be advocates to address weight stigma in the home environment and school setting.",
            "year": 2017,
            "citationCount": 386,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This policy statement seeks to raise awareness regarding the prevalence and negative effects of weight stigma on pediatric patients and their families and provides 6 clinical practice and 4 advocacy recommendations regarding the role of pediatricians in addressing weight stigma."
            },
            "score": 5
        },
        {
            "id": "88549b4f48b9709acdfb8b9e41656b6d133c5390",
            "paperId": "88549b4f48b9709acdfb8b9e41656b6d133c5390",
            "title": "Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models",
            "abstract": "Large Language Models (LLMs) are trained primarily on minimally processed web text, which exhibits the same wide range of social biases held by the humans who created that content. Consequently, text generated by LLMs can inadvertently perpetuate stereotypes towards marginalized groups, like the LGBTQIA+ community. In this paper, we perform a comparative study of how LLMs generate text describing people with different sexual identities. Analyzing bias in the text generated by an LLM using regard score shows measurable bias against queer people. We then show that a post-hoc method based on chain-of-thought prompting using SHAP analysis can increase the regard of the sentence, representing a promising approach towards debiasing the output of LLMs in this setting.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that a post-hoc method based on chain-of-thought prompting using SHAP analysis can increase the regard of the sentence, representing a promising approach towards debiasing the output of LLMs in this setting."
            },
            "score": 5
        },
        {
            "id": "d795fc5d6c04435baf7be0d7c12eed16d89e9b1d",
            "paperId": "d795fc5d6c04435baf7be0d7c12eed16d89e9b1d",
            "title": "Mitigating Social Biases of Pre-trained Language Models via Contrastive Self-Debiasing with Double Data Augmentation",
            "abstract": null,
            "year": 2024,
            "citationCount": 0,
            "tldr": null,
            "score": 5
        },
        {
            "id": "0cfd72e3d81f35bccc1e67f5992e112a601ba2ba",
            "paperId": "0cfd72e3d81f35bccc1e67f5992e112a601ba2ba",
            "title": "Can Instruction Fine-Tuned Language Models Identify Social Bias through Prompting?",
            "abstract": "As the breadth and depth of language model applications continue to expand rapidly, it is increasingly important to build efficient frameworks for measuring and mitigating the learned or inherited social biases of these models. In this paper, we present our work on evaluating instruction fine-tuned language models' ability to identify bias through zero-shot prompting, including Chain-of-Thought (CoT) prompts. Across LLaMA and its two instruction fine-tuned versions, Alpaca 7B performs best on the bias identification task with an accuracy of 56.7%. We also demonstrate that scaling up LLM size and data diversity could lead to further performance gain. This is a work-in-progress presenting the first component of our bias mitigation framework. We will keep updating this work as we get more results.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Evaluating instruction fine-tuned language models' ability to identify bias through zero-shot prompting, including Chain-of-Thought (CoT) prompts shows Alpaca 7B performs best on the bias identification task and it is demonstrated that scaling up LLM size and data diversity could lead to further performance gain."
            },
            "score": 5
        },
        {
            "id": "728e96b224e718d7123872f47462bbcbf63984ef",
            "paperId": "728e96b224e718d7123872f47462bbcbf63984ef",
            "title": "Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information",
            "abstract": "Mitigating social biases typically requires identifying the social groups associated with each data sample. In this paper, we present DAFair, a novel approach to address social bias in language models. Unlike traditional methods that rely on explicit demographic labels, our approach does not require any such information. Instead, we leverage predefined prototypical demographic texts and incorporate a regularization term during the fine-tuning process to mitigate bias in the model's representations. Our empirical results across two tasks and two models demonstrate the effectiveness of our method compared to previous approaches that do not rely on labeled data. Moreover, with limited demographic-annotated data, our approach outperforms common debiasing approaches.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "DAFair is presented, a novel approach to address social bias in language models that leverage predefined prototypical demographic texts and incorporate a regularization term during the fine-tuning process to mitigate bias in the model's representations."
            },
            "score": 5
        },
        {
            "id": "ff3d445da7847f9f212118328c5cdb6246f1fe3d",
            "paperId": "ff3d445da7847f9f212118328c5cdb6246f1fe3d",
            "title": "CO-STAR: Conceptualisation of Stereotypes for Analysis and Reasoning",
            "abstract": "Warning: this paper contains material which may be offensive or upsetting. While much of recent work has focused on the detection of hate speech and overtly offensive content, very little research has explored the more subtle but equally harmful language in the form of implied stereotypes. This is a challenging domain, made even more so by the fact that humans often struggle to understand and reason about stereotypes. We build on existing literature and present CO-STAR (COnceptualisation of STereotypes for Analysis and Reasoning), a novel framework which encodes the underlying concepts of implied stereotypes. We also introduce the CO-STAR training data set, which contains just over 12K structured annotations of implied stereotypes and stereotype conceptualisations, and achieve state-of-the-art results after training and manual evaluation. The CO-STAR models are, however, limited in their ability to understand more complex and subtly worded stereotypes, and our research motivates future work in developing models with more sophisticated methods for encoding common-sense knowledge.",
            "year": 2021,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The CO-STAR models are limited in their ability to understand more complex and subtly worded stereotypes, and the research motivates future work in developing models with more sophisticated methods for encoding common-sense knowledge."
            },
            "score": 5
        },
        {
            "id": "0607b299284cb44eaee0aedd95db3c88b00ff944",
            "paperId": "0607b299284cb44eaee0aedd95db3c88b00ff944",
            "title": "Gender Bias in Masked Language Models for Multiple Languages",
            "abstract": "Masked Language Models (MLMs) pre-trained by predicting masked tokens on large corpora have been used successfully in natural language processing tasks for a variety of languages.Unfortunately, it was reported that MLMs also learn discriminative biases regarding attributes such as gender and race.Because most studies have focused on MLMs in English, the bias of MLMs in other languages has rarely been investigated.Manual annotation of evaluation data for languages other than English has been challenging due to the cost and difficulty in recruiting annotators.Moreover, the existing bias evaluation methods require the stereotypical sentence pairs consisting of the same context with attribute words (e.g. He/She is a nurse).We propose Multilingual Bias Evaluation (MBE) score, to evaluate bias in various languages using only English attribute word lists and parallel corpora between the target language and English without requiring manually annotated data.We evaluated MLMs in eight languages using the MBE and confirmed that gender-related biases are encoded in MLMs for all those languages.We manually created datasets for gender bias in Japanese and Russian to evaluate the validity of the MBE.The results show that the bias scores reported by the MBE significantly correlates with that computed from the above manually created datasets and the existing English datasets for gender bias.",
            "year": 2022,
            "citationCount": 38,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Multilingual Bias Evaluation (MBE) score is proposed, to evaluate bias in various languages using only English attribute word lists and parallel corpora between the target language and English without requiring manually annotated data."
            },
            "score": 5
        },
        {
            "id": "339ab215d8f1f2cb2644346f11df83c5173e7873",
            "paperId": "339ab215d8f1f2cb2644346f11df83c5173e7873",
            "title": "How Gender Debiasing Affects Internal Model Representations, and Why It Matters",
            "abstract": "Common studies of gender bias in NLP focus either on extrinsic bias measured by model performance on a downstream task or on intrinsic bias found in models\u2019 internal representations. However, the relationship between extrinsic and intrinsic bias is relatively unknown. In this work, we illuminate this relationship by measuring both quantities together: we debias a model during downstream fine-tuning, which reduces extrinsic bias, and measure the effect on intrinsic bias, which is operationalized as bias extractability with information-theoretic probing. Through experiments on two tasks and multiple bias metrics, we show that our intrinsic bias metric is a better indicator of debiasing than (a contextual adaptation of) the standard WEAT metric, and can also expose cases of superficial debiasing. Our framework provides a comprehensive perspective on bias in NLP models, which can be applied to deploy NLP systems in a more informed manner. Our code and model checkpoints are publicly available.",
            "year": 2022,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work debias a model during downstream fine-tuning, which reduces extrinsic bias, and measures the effect on intrinsic bias, which is operationalized as bias extractability with information-theoretic probing and provides a comprehensive perspective on bias in NLP models."
            },
            "score": 5
        },
        {
            "id": "aa85dc60e88ca135c272f1ccb2b471f7b60a7e78",
            "paperId": "aa85dc60e88ca135c272f1ccb2b471f7b60a7e78",
            "title": "From Prejudice to Parity: A New Approach to Debiasing Large Language Model Word Embeddings",
            "abstract": "Embeddings play a pivotal role in the efficacy of Large Language Models. They are the bedrock on which these models grasp contextual relationships and foster a more nuanced understanding of language and consequently perform remarkably on a plethora of complex tasks that require a fundamental understanding of human language. Given that these embeddings themselves often reflect or exhibit bias, it stands to reason that these models may also inadvertently learn this bias. In this work, we build on the seminal previous work and propose DeepSoftDebias, an algorithm that uses a neural network to perform 'soft debiasing'. We exhaustively evaluate this algorithm across a variety of SOTA datasets, accuracy metrics, and challenging NLP tasks. We find that DeepSoftDebias outperforms the current state-of-the-art methods at reducing bias across gender, race, and religion.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes DeepSoftDebias, an algorithm that uses a neural network to perform 'soft debiasing' and finds that DeepSoftDebias outperforms the current state-of-the-art methods at reducing bias across gender, race, and religion."
            },
            "score": 5
        },
        {
            "id": "8c9f052c706e823f031aad8ffcaef3e867e76e8a",
            "paperId": "8c9f052c706e823f031aad8ffcaef3e867e76e8a",
            "title": "Detecting Personal Information in Training Corpora: an Analysis",
            "abstract": "Large language models are trained on increasing quantities of unstructured text, the largest sources of which are scraped from the Web. These Web scrapes are mainly composed of heterogeneous collections of text from multiple domains with minimal documentation. While some work has been done to identify and remove toxic, biased, or sexual language, the topic of personal information (PI) in textual data used for training Natural Language Processing (NLP) models is relatively under-explored. In this work, we draw from definitions of PI across multiple countries to define the first PI taxonomy of its kind, categorized by type and risk level. We then conduct a case study on the Colossal Clean Crawled Corpus (C4) and the Pile, to detect some of the highest-risk personal information, such as email addresses and credit card numbers, and examine the differences between automatic and regular expression-based approaches for their detection. We identify shortcomings in modern approaches for PI detection, and propose a reframing of the problem that is informed by global perspectives and the goals in personal information detection.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work draws from definitions of PI across multiple countries to define the first PI taxonomy of its kind, categorized by type and risk level, and proposes a reframing of the problem that is informed by global perspectives and the goals in personal information detection."
            },
            "score": 4
        },
        {
            "id": "0399d265584c23a4258f35e1c65e47c55100b706",
            "paperId": "0399d265584c23a4258f35e1c65e47c55100b706",
            "title": "The effect of rural superintendent credibility on principal and district outcomes mediated by motivating language",
            "abstract": "PurposeThe purpose of this study was to determine the influence of rural superintendent\u2019s talk on the perceived outcomes of principal communication competence and organizational communication satisfaction. More specifically, this study explored whether the source credibility dimensions of goodwill, competency and trustworthiness had a greater impact on the perceived outcomes when mediated by motivating language (ML) than not.Design/methodology/approachDirect and indirect paths between superintendent and principal communication were modeled, analyzed and evaluated using descriptive and inferential methods, including R version 3.6.1 with the lavaan package and the Sobel mediation test.FindingsResearch findings indicated the importance of superintendent talk and motivating language theory (MLT) that, when combined, constitute the medium of superintendent practice and enable a superintendent to execute their roles and duties. Additionally, the authors found the dimensions of goodwill and competency had the most significant impact on the two outcomes mediated by ML. This finding indicates that principals are calling for a more empathetic and interactive form of credibility than the long-established form of credibility based on expertise and stewardship. Finally, as the authors call for an expanded role from the community in research, scholarship and implementation of MLT, they suggest due to the lack of significance in trustworthiness mediated by ML, future research into trustworthiness and trust.Originality/valueThis study\u2019s value is to increase understanding of educational administration scholars of MLT and its power to influence employee and organizational outcomes and highlight a reframing of superintendent credibility away from say and do agreement and expertise and stewardship.",
            "year": 2021,
            "citationCount": 6,
            "tldr": null,
            "score": 4
        },
        {
            "id": "039b6272fde1362e7c544f73d11e516f4a824aec",
            "paperId": "039b6272fde1362e7c544f73d11e516f4a824aec",
            "title": "Undermining the Unfair Constraints Imposed by Language Standards: Subversive Discourse Tactics Used in Both 19th\u2010 and 21st\u2010Century America",
            "abstract": "This article examines various ways people of the past and present have dealt with the unfair practices associated with the use of norms and standards. Nineteenth-century responses of writers to the unfair aspects of standards are revealed by first examining the standards and then analyzing the discourse of two 19th-century women writers and social activists who took issue with them. The subversive tactics used by these activists included reframing the discourse, adding new voices to traditional texts, appealing to higher authorities, and exaggerating or ignoring the standards so as to flaunt them. Turning to the present, these same tactics are found in the discourse of those who are currently promoting the social model practices. These discourse tactics can be used by today's clinicians who are looking for ways to contend with biases resulting from the imposition of narrow standards and norms.",
            "year": 2007,
            "citationCount": 1,
            "tldr": null,
            "score": 4
        },
        {
            "id": "60999ddb99aea5a93bd2ba16fb7671dc76bf3ba5",
            "paperId": "60999ddb99aea5a93bd2ba16fb7671dc76bf3ba5",
            "title": "Causal-Debias: Unifying Debiasing in Pretrained Language Models and Fine-tuning via Causal Invariant Learning",
            "abstract": "Demographic biases and social stereotypes are common in pretrained language models (PLMs), and a burgeoning body of literature focuses on removing the unwanted stereotypical associations from PLMs. However, when fine-tuning these bias-mitigated PLMs in downstream natural language processing (NLP) applications, such as sentiment classification, the unwanted stereotypical associations resurface or even get amplified. Since pretrain&fine-tune is a major paradigm in NLP applications, separating the debiasing procedure of PLMs from fine-tuning would eventually harm the actual downstream utility. In this paper, we propose a unified debiasing framework Causal-Debias to remove unwanted stereotypical associations in PLMs during fine-tuning. Specifically, CausalDebias mitigates bias from a causal invariant perspective by leveraging the specific downstream task to identify bias-relevant and labelrelevant factors. We propose that bias-relevant factors are non-causal as they should have little impact on downstream tasks, while labelrelevant factors are causal. We perform interventions on non-causal factors in different demographic groups and design an invariant risk minimization loss to mitigate bias while maintaining task performance. Experimental results on three downstream tasks show that our proposed method can remarkably reduce unwanted stereotypical associations after PLMs are finetuned, while simultaneously minimizing the impact on PLMs and downstream applications.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results show that the proposed unified debiasing framework Causal-Debias can remarkably reduce unwanted stereotypical associations after PLMs are finetuned, while simultaneously minimizing the impact on PLMs and downstream applications."
            },
            "score": 4
        },
        {
            "id": "d697e021ea11fca16cce709585e7e3d91bb7c687",
            "paperId": "d697e021ea11fca16cce709585e7e3d91bb7c687",
            "title": "FineDeb: A Debiasing Framework for Language Models",
            "abstract": "As language models are increasingly included in human-facing machine learning tools, bias against demographic subgroups has gained attention. We propose FineDeb, a two-phase debiasing framework for language models that starts with contextual debiasing of embeddings learned by pretrained language models. The model is then fine-tuned on a language modeling objective. Our results show that FineDeb offers stronger debiasing in comparison to other methods which often result in models as biased as the original language model. Our framework is generalizable for demographics with multiple classes, and we demonstrate its effectiveness through extensive experiments and comparisons with state of the art techniques. We release our code and data on GitHub.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results show that FineDeb offers stronger debiasing in comparison to other methods which often result in models as biased as the original language model."
            },
            "score": 4
        },
        {
            "id": "4ee4302027edf554a10f71bb1d5f43ab4892fdd3",
            "paperId": "4ee4302027edf554a10f71bb1d5f43ab4892fdd3",
            "title": "Investigating the Effect of Debiasing Methods on Intersectional Biases in Language Models",
            "abstract": "Previous work in debiasing language models has focused on removing one form of bias, such as racial or gender bias, but no current work has attempted to reduce intersectional bias, which is bias resulting from being in more than one marginalized group. We propose three different methods to reduce intersectional bias in the BERT-Tiny model through fine-tuning in which we de-bias the contextual word embeddings of the BERT-Tiny model. We evaluate our models using intersectional embedding association tests across race, gender, and age. Our de-biasing methods are able to successfully reduce intersectional bias in all three intersectional identities tested. We find that Method 1 results in the best performance for less extreme intersectionality tests, while Methods 2 and 3 perform better for the more extreme intersectionality tests (such as European-American male vs. African-American female). Additionally, we see greater improvements in debiasing for the inter-sectional identity of African-American x Female, likely as a result of more data availability for this intersection.",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Three different methods to reduce intersectional bias in the BERT-Tiny model through fine-tuning in which the contextual word embeddings of the BERT-Tiny model are de-bias the contextual word embeddings of the BERT-Tiny model are proposed."
            },
            "score": 4
        },
        {
            "id": "5ba97fbbbcb0f79c252e42204710df94719fee42",
            "paperId": "5ba97fbbbcb0f79c252e42204710df94719fee42",
            "title": "The Impact of Debiasing on the Performance of Language Models in Downstream Tasks is Underestimated",
            "abstract": "Pre-trained language models trained on large-scale data have learned serious levels of social biases. Consequently, various methods have been proposed to debias pre-trained models. Debiasing methods need to mitigate only discriminatory bias information from the pre-trained models, while retaining information that is useful for the downstream tasks. In previous research, whether useful information is retained has been confirmed by the performance of downstream tasks in debiased pre-trained models. On the other hand, it is not clear whether these benchmarks consist of data pertaining to social biases and are appropriate for investigating the impact of debiasing. For example in gender-related social biases, data containing female words (e.g. ``she, female, woman''), male words (e.g. ``he, male, man''), and stereotypical words (e.g. ``nurse, doctor, professor'') are considered to be the most affected by debiasing. If there is not much data containing these words in a benchmark dataset for a target task, there is the possibility of erroneously evaluating the effects of debiasing. In this study, we compare the impact of debiasing on performance across multiple downstream tasks using a wide-range of benchmark datasets that containing female, male, and stereotypical words. Experiments show that the effects of debiasing are consistently \\emph{underestimated} across all tasks. Moreover, the effects of debiasing could be reliably evaluated by separately considering instances containing female, male, and stereotypical words than all of the instances in a benchmark dataset.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The effects of debiasing are consistently underestimated across all tasks and could be reliably evaluated by separately considering instances containing female, male, and stereotypical words than all of the instances in a benchmark dataset."
            },
            "score": 4
        },
        {
            "id": "167a8de25cdd67eaaac4585ba006d2b6d8e4fd76",
            "paperId": "167a8de25cdd67eaaac4585ba006d2b6d8e4fd76",
            "title": "Masked Language Models as Stereotype Detectors?",
            "abstract": "Pretraining language models led to significant improvements for NLP tasks. However, recent studies confirmed that most language models exhibit a myriad of social biases related to different demographic variables such as gender, race, or religion. In this work, we exploit this implicit knowledge of stereotypes to cre-ate an end-to-end stereotype detector using solely a language model. Existing literature on quantifying social biases functions at model-level, evaluating trained models such as word embeddings, contextual sentence encoders, or co-reference resolution systems. In this work, we focus on measuring stereotypes at data-level, computing bias scores for natural language sentences and documents. We evaluate the effectiveness of our pipeline on publicly available benchmarks.",
            "year": 2022,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work exploits implicit knowledge of stereotypes to exploit an end-to-end stereotype detector using solely a language model, and focuses on measuring stereotypes at data-level, computing bias scores for natural language sentences and documents."
            },
            "score": 4
        },
        {
            "id": "688bb60b90d511c5199e2c37cc247eb9c534be46",
            "paperId": "688bb60b90d511c5199e2c37cc247eb9c534be46",
            "title": "FineDeb: A Debiased Finetuning Approach for Language Models",
            "abstract": "As language models are increasing included 001 in human-facing machine learning tools, bias 002 against demographic subgroups has gained at-003 tention. We consider the problem of debiasing 004 in language models. Rather than modifying a 005 model\u2019s already learned representations, we fo-006 cus on modifying them during model training 007 itself. We propose a two-phase methodology 008 (FineDeb) that starts with contextual debiasing 009 of embeddings learned by the language mod-010 els during training, then finetunes the model 011 on the original language modelling objective. 012 We apply our method to debias for demograph-013 ics with multiple classes, demonstrating its ef-014 fectiveness through extensive experiments and 015 comparing with state of the art techniques, and 016 on three metrics",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A two-phase methodology that starts with contextual debiasing of embeddings learned by the language mod-010 during training, then finetunes the model 011 on the original language modelling objective, and applies this method to debias for demograph-013 with multiple classes."
            },
            "score": 4
        },
        {
            "id": "0b641cc51d8e25c6c0b2362317fec7e0bf26fbf1",
            "paperId": "0b641cc51d8e25c6c0b2362317fec7e0bf26fbf1",
            "title": "KoSBI: A Dataset for Mitigating Social Bias Risks Towards Safer Large Language Model Applications",
            "abstract": "Large language models (LLMs) not only learn natural text generation abilities but also social biases against different demographic groups from real-world data. This poses a critical risk when deploying LLM-based applications. Existing research and resources are not readily applicable in South Korea due to the differences in language and culture, both of which significantly affect the biases and targeted demographic groups. This limitation requires localized social bias datasets to ensure the safe and effective deployment of LLMs. To this end, we present KosBi, a new social bias dataset of 34k pairs of contexts and sentences in Korean covering 72 demographic groups in 15 categories. We find that through filtering-based moderation, social biases in generated content can be reduced by 16.47%p on average for HyperClova (30B and 82B), and GPT-3.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "KosBi, a new social bias dataset of 34k pairs of contexts and sentences in Korean covering 72 demographic groups in 15 categories is presented and it is found that through filtering-based moderation, social biases in generated content can be reduced by 16.47%p on average for HyperClova, GPT-3."
            },
            "score": 4
        },
        {
            "id": "15aebc062d9ec0a958f935c454bddb5e1cc3b0a6",
            "paperId": "15aebc062d9ec0a958f935c454bddb5e1cc3b0a6",
            "title": "Mitigating Biases in Toxic Language Detection through Invariant Rationalization",
            "abstract": "Automatic detection of toxic language plays an essential role in protecting social media users, especially minority groups, from verbal abuse. However, biases toward some attributes, including gender, race, and dialect, exist in most training datasets for toxicity detection. The biases make the learned models unfair and can even exacerbate the marginalization of people. Considering that current debiasing methods for general natural language understanding tasks cannot effectively mitigate the biases in the toxicity detectors, we propose to use invariant rationalization (InvRat), a game-theoretic framework consisting of a rationale generator and a predictor, to rule out the spurious correlation of certain syntactic patterns (e.g., identity mentions, dialect) to toxicity labels. We empirically show that our method yields lower false positive rate in both lexical and dialectal attributes than previous debiasing methods.",
            "year": 2021,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "InvRat is proposed, a game-theoretic framework consisting of a rationale generator and a predictor, to rule out the spurious correlation of certain syntactic patterns to toxicity labels and yields lower false positive rate in both lexical and dialectal attributes than previous debiasing methods."
            },
            "score": 4
        },
        {
            "id": "c4cb9f0145cebb169fbdf235fb1f76e1e5b82e72",
            "paperId": "c4cb9f0145cebb169fbdf235fb1f76e1e5b82e72",
            "title": "On Evaluating and Mitigating Gender Biases in Multilingual Settings",
            "abstract": "While understanding and removing gender biases in language models has been a long-standing problem in Natural Language Processing, prior research work has primarily been limited to English. In this work, we investigate some of the challenges with evaluating and mitigating biases in multilingual settings which stem from a lack of existing benchmarks and resources for bias evaluation beyond English especially for non-western context. In this paper, we first create a benchmark for evaluating gender biases in pre-trained masked language models by extending DisCo to different Indian languages using human annotations. We extend various debiasing methods to work beyond English and evaluate their effectiveness for SOTA massively multilingual models on our proposed metric. Overall, our work highlights the challenges that arise while studying social biases in multilingual settings and provides resources as well as mitigation techniques to take a step toward scaling to more languages.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work creates a benchmark for evaluating gender biases in pre-trained masked language models by extending DisCo to different Indian languages using human annotations and extends various debiasing methods to work beyond English and evaluates their effectiveness for SOTA massively multilingual models on a proposed metric."
            },
            "score": 4
        },
        {
            "id": "7a80a1929c888bd28cc7b48186cf01ef3e674526",
            "paperId": "7a80a1929c888bd28cc7b48186cf01ef3e674526",
            "title": "MIST: Mitigating Intersectional Bias with Disentangled Cross-Attention Editing in Text-to-Image Diffusion Models",
            "abstract": "Diffusion-based text-to-image models have rapidly gained popularity for their ability to generate detailed and realistic images from textual descriptions. However, these models often reflect the biases present in their training data, especially impacting marginalized groups. While prior efforts to debias language models have focused on addressing specific biases, such as racial or gender biases, efforts to tackle intersectional bias have been limited. Intersectional bias refers to the unique form of bias experienced by individuals at the intersection of multiple social identities. Addressing intersectional bias is crucial because it amplifies the negative effects of discrimination based on race, gender, and other identities. In this paper, we introduce a method that addresses intersectional bias in diffusion-based text-to-image models by modifying cross-attention maps in a disentangled manner. Our approach utilizes a pre-trained Stable Diffusion model, eliminates the need for an additional set of reference images, and preserves the original quality for unaltered concepts. Comprehensive experiments demonstrate that our method surpasses existing approaches in mitigating both single and intersectional biases across various attributes. We make our source code and debiased models for various attributes available to encourage fairness in generative models and to support further research.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a method that addresses intersectional bias in diffusion-based text-to-image models by modifying cross-attention maps in a disentangled manner and surpasses existing approaches in mitigating both single and intersectional biases across various attributes."
            },
            "score": 4
        },
        {
            "id": "60c498956cb5737c4964aaca0b920592bd7f5689",
            "paperId": "60c498956cb5737c4964aaca0b920592bd7f5689",
            "title": "Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in Image Search",
            "abstract": "Internet search affects people\u2019s cognition of the world, so mitigating biases in search results and learning fair models is imperative for social good. We study a unique gender bias in image search in this work: the search images are often gender-imbalanced for gender-neutral natural language queries. We diagnose two typical image search models, the specialized model trained on in-domain datasets and the generalized representation model pre-trained on massive image and text data across the internet. Both models suffer from severe gender bias. Therefore, we introduce two novel debiasing approaches: an in-processing fair sampling method to address the gender imbalance issue for training models, and a post-processing feature clipping method base on mutual information to debias multimodal representations of pre-trained models. Extensive experiments on MS-COCO and Flickr30K benchmarks show that our methods significantly reduce the gender bias in image search models.",
            "year": 2021,
            "citationCount": 52,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Two novel debiasing approaches are introduced: an in-processing fair sampling method to address the gender imbalance issue for training models, and a post-processing feature clipping method base on mutual information to debias multimodal representations of pre-trained models."
            },
            "score": 4
        },
        {
            "id": "bd53fe778f2e4cbab58b92e5c1393fe7f59467f6",
            "paperId": "bd53fe778f2e4cbab58b92e5c1393fe7f59467f6",
            "title": "Reducing Stereotype Threat in First-Year Logic Classes",
            "abstract": "I examine successful strategies to diminish or eliminate stereotype threat in mathematics. Some of these include informing our students about stereotype threat, challenging the idea that logical intelligence is an \u201cinnate\u201d ability, making students in threatened groups feel welcomed, and introducing counter-stereotypical role models. The purpose of this paper is to take these strategies that have proven successful in mathematics, and suggest specific ways to incorporate them into introductory logic classes in philosophy. I consider the possible benefit of presenting logic to our undergraduate students by concentrating on aspects of logic that do not result in a clash of schemas (i.e., logic as language and the benefit of logic as aesthetic enjoyment).",
            "year": 2015,
            "citationCount": 2,
            "tldr": null,
            "score": 4
        },
        {
            "id": "e877d295ca425faf33f0c8e4d8c410c2e9c8a26d",
            "paperId": "e877d295ca425faf33f0c8e4d8c410c2e9c8a26d",
            "title": "We're Afraid Language Models Aren't Modeling Ambiguity",
            "abstract": "Ambiguity is an intrinsic feature of natural language. Managing ambiguity is a key part of human language understanding, allowing us to anticipate misunderstanding as communicators and revise our interpretations as listeners. As language models (LMs) are increasingly employed as dialogue interfaces and writing aids, handling ambiguous language is critical to their success. We characterize ambiguity in a sentence by its effect on entailment relations with another sentence, and collect AmbiEnt, a linguist-annotated benchmark of 1,645 examples with diverse kinds of ambiguity. We design a suite of tests based on AmbiEnt, presenting the first evaluation of pretrained LMs to recognize ambiguity and disentangle possible meanings. We find that the task remains extremely challenging, including for GPT-4, whose generated disambiguations are considered correct only 32% of the time in human evaluation, compared to 90% for disambiguations in our dataset. Finally, to illustrate the value of ambiguity-sensitive tools, we show that a multilabel NLI model can flag political claims in the wild that are misleading due to ambiguity. We encourage the field to rediscover the importance of ambiguity for NLP.",
            "year": 2023,
            "citationCount": 44,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A suite of tests based on AmbiEnt is designed, presenting the first evaluation of pretrained LMs to recognize ambiguity and disentangle possible meanings, and it is shown that a multilabel NLI model can flag political claims in the wild that are misleading due to ambiguity."
            },
            "score": 4
        },
        {
            "id": "341bdbcfc3febef7691a97c216ad394653211095",
            "paperId": "341bdbcfc3febef7691a97c216ad394653211095",
            "title": "Can language models learn from explanations in context?",
            "abstract": "Language Models (LMs) can perform new tasks by adapting to a few in-context examples. For humans, explanations that connect examples to task principles can improve learning. We therefore investigate whether explanations of few-shot examples can help LMs. We annotate questions from 40 challenging tasks with answer explanations, and various matched control explanations. We evaluate how different types of explanations, instructions, and controls affect zero- and few-shot performance. We analyze these results using statistical multilevel modeling techniques that account for the nested dependencies among conditions, tasks, prompts, and models. We find that explanations can improve performance -- even without tuning. Furthermore, explanations hand-tuned for performance on a small validation set offer substantially larger benefits, and building a prompt by selecting examples and explanations together substantially improves performance over selecting examples alone. Finally, even untuned explanations outperform carefully matched controls, suggesting that the benefits are due to the link between an example and its explanation, rather than lower-level features. However, only large models benefit. In summary, explanations can support the in-context learning of large LMs on challenging tasks.",
            "year": 2022,
            "citationCount": 196,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 4
        },
        {
            "id": "62104253d974716e01d307fa0b3f5eaadb5e63da",
            "paperId": "62104253d974716e01d307fa0b3f5eaadb5e63da",
            "title": "Towards Generating Contextual and Empathetic Response for Covid-related Queries",
            "abstract": "This work addresses the vital need of keeping people informed with relevant, correct and essential information during the pandemic. Advanced NLP and machine learning mechanisms have been leveraged to generate responses to user queries through contextual conversation. In order to help people be discerning about what information they receive, a conversational system is proposed that identifies the correct intent of the query and a reinforcement Learning based generation model is used to proceed with conversation. We propose an end-to-end real-time text generation model that can respond to users queries on covid19. We created a new dataset with 1200+ covid-related questions from various sources and pre-processed them for a brief and direct answer. The dataset has also been manually observed to identify depressed questions and the responses are converted to be more empathetic. The dataset has been used to fine-tune DailoGPT, a GPT2-based transformer model to generate the responses related to COVID. COVID-related queries are bucketed into 15 categories to identify the exact intent of people. Our model generated both contextual and empathetic responses and achieved a human evaluation score of 3.48 (on a scale of 5) in terms of contextual relevance and a score of 2.12 (on a scale of 3).",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes an end-to-end real-time text generation model that can respond to users queries on covid19 and generated both contextual and empathetic responses."
            },
            "score": 4
        },
        {
            "id": "9fac996885832862941483433eb344ea30881561",
            "paperId": "9fac996885832862941483433eb344ea30881561",
            "title": "Visual Comparison of Language Model Adaptation",
            "abstract": "Neural language models are widely used; however, their model parameters often need to be adapted to the specific domains and tasks of an application, which is time- and resource-consuming. Thus, adapters have recently been introduced as a lightweight alternative for model adaptation. They consist of a small set of task-specific parameters with a reduced training time and simple parameter composition. The simplicity of adapter training and composition comes along with new challenges, such as maintaining an overview of adapter properties and effectively comparing their produced embedding spaces. To help developers overcome these challenges, we provide a twofold contribution. First, in close collaboration with NLP researchers, we conducted a requirement analysis for an approach supporting adapter evaluation and detected, among others, the need for both intrinsic (i.e., embedding similarity-based) and extrinsic (i.e., prediction-based) explanation methods. Second, motivated by the gathered requirements, we designed a flexible visual analytics workspace that enables the comparison of adapter properties. In this paper, we discuss several design iterations and alternatives for interactive, comparative visual explanation methods. Our comparative visualizations show the differences in the adapted embedding vectors and prediction outcomes for diverse human-interpretable concepts (e.g., person names, human qualities). We evaluate our workspace through case studies and show that, for instance, an adapter trained on the language debiasing task according to context-0 (decontextualized) embeddings introduces a new type of bias where words (even gender-independent words such as countries) become more similar to female- than male pronouns. We demonstrate that these are artifacts of context-0 embeddings, and the adapter effectively eliminates the gender information from the contextualized word representations.",
            "year": 2022,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper conducts a requirement analysis for an approach supporting adapter evaluation and detected the need for both intrinsic and extrinsic explanation methods, and designed a flexible visual analytics workspace that enables the comparison of adapter properties."
            },
            "score": 4
        },
        {
            "id": "be77c7dd67a5bb6a28b866de65ea10bbac03c6bb",
            "paperId": "be77c7dd67a5bb6a28b866de65ea10bbac03c6bb",
            "title": "Interdependencies of Gender and Race in Contextualized Word Embeddings",
            "abstract": "Recent years have seen a surge in research on the biases in word embeddings with respect to gender and, to a lesser extent, race. Few of these studies, however, have given attention to the critical intersection of race and gender. In this case study, we analyze the dimensions of gender and race in contextualized word embeddings of given names, taken from BERT, and investigate the nature and nuance of their interaction. We find that these demographic axes, though typically treated as physically and conceptually separate, are in fact interdependent and thus inadvisable to consider in isolation. Further, we show that demographic dimensions predicated on default settings in language, such as in pronouns, may risk rendering groups with multiple marginalized identities invisible. We conclude by discussing the importance and implications of intersectionality for future studies on bias and debiasing in NLP.",
            "year": 2020,
            "citationCount": 13,
            "tldr": null,
            "score": 4
        },
        {
            "id": "77bce66e9b0166bf0a5e6258dc6e38290538523c",
            "paperId": "77bce66e9b0166bf0a5e6258dc6e38290538523c",
            "title": "The Roots of Language Learning: Infant Language Acquisition",
            "abstract": "Foreword V Monika Molnar and Nuria Sebastian-Galles The Roots of Language Learning: Infant Language Acquisition 1 5 Fumitaka Homae, Hama Watanabe, and Gentaro Taga The Neural Substrates of Infant Speech Perception 6 26 Laurence White, Caroline Floccia, Jeremy Goslin, and Joseph Butler Utterance-Final Lengthening Is Predictive of Infants Discrimination of English Accents 27 44 Monika Molnar, Marie Lallier, and Manuel Carreiras The Amount of Language Exposure Determines Nonlinguistic Tone Grouping Biases in Infants From a Bilingual Environment 45 64 A'gnes M. Kova'cs Extracting Regularities From Noise: Do Infants Encode Patterns Based on Same and Different Relations? 65 85 Richard N. Aslin and Elissa L. Newport Distributional Language Learning: Mechanisms And Models of Category Formation 86 105 Jenny Saffran Sounds and Meanings Working Together: Word Learning as a Collaborative Effort 106 120 Marilyn May Vihman, Rory A. DePaolis, and Tamar Keren-Portnoy The Role of Production in Infant Word Learning 121 140 Barbara Hohle, Sabina Pauen, Volker Hesse, and Jurgen Weissenborn Discrimination of Rhythmic Pattern at 4 Months and Language Performance at 5 Years: A Longitudinal Analysis of Data From German-Learning Children 141 164 Amanda Seidl, Brian French, Yuanyuan Wang, and Alejandrina Cristia Toward Establishing Continuity in Linguistic Skills Within Early Infancy 165 183 Krista Byers-Heinlein Languages As Categories: Reframing the One Language or Two Question in Early Bilingual Development 184 201",
            "year": 2014,
            "citationCount": 2,
            "tldr": null,
            "score": 3
        },
        {
            "id": "cacfcbc2edbc197b2a274ff3bf2a721c68f7d0cd",
            "paperId": "cacfcbc2edbc197b2a274ff3bf2a721c68f7d0cd",
            "title": "Alphabetism in reading science",
            "abstract": "There has been mounting concern among social scientists that conclusions from studies conducted on highly educated populations from affluent European cultures may have limited applicability to human behavior in general (Henrich et al., 2010). Similar reservations have also been voiced in the fields of language (Evans and Levinson, 2009) and literacy (Share, 2008a; Frost, 2012). Reading research, in particular, has been overwhelmingly dominated by work on English, which appears to be an outlier among European alphabets (Seymour et al., 2003; Share, 2008a). I have argued that because spelling\u2013sound relations are so complex in English orthography, much of reading research has been confined to a narrow Anglocentric research agenda addressing theoretical and applied issues with only limited relevance for a universal science of reading and literacy. \n \nMy intention here is not to reiterate my 2008 arguments or even expand them, but to move on to another major obstacle to progress. Before moving on, however, I would like to add a note of optimism to the Anglocentrism debate. In recent years, interest in other languages has indeed begun to emerge from the shadows probably because the scientific community of Anglo-American reading researchers has felt itself \u201ccome of age\u201d as a substantial body of well-replicated and converging findings has coalesced in recent years, at least on several key topics such as word identification and dyslexia (Vellutino et al., 2004; Snowling and Hulme, 2005; DeHaene, 2009; Rayner et al., 2012). The field is now witnessing important first steps toward universal models of reading (Perfetti, 2003; Perfetti et al., 2005; Ziegler and Goswami, 2005; Frost, 2012) as well as a growing number of linguistically and grammatologically informed studies emerging outside the confines of English and other European alphabets (Nag and Perfetti, 2014; Saiegh-Haddad and Joshi, 2014; Verhoeven and Perfetti, 2014). It is still the case, nonetheless, that the theoretical and applied frameworks developed for English are all too frequently being applied to other languages and writing systems without due consideration for linguistic and writing system diversity. Almost all publications by English-language researchers continue to omit any \u201c\u2026in English\u201d qualification in the titles of their papers\u2014\u201cA New Whiz-Bang+++ Model of Learning to Read\u201d\u2026 in English?\u2014as if the results of studies conducted in English alone enjoy the privileged status of universal applicability, unlike researchers investigating other languages who are obliged to qualify their findings by adding the \u201c\u2026in Chinese/Arabic/Korean etc.\u201d disclaimer which automatically demarcates the findings as language-specific and hence not necessarily universally applicable. \n \nHere, I focus on yet another \u201c-ism,\u201d which I call \u201calphabetism\u201d; the belief that alphabetic writing systems are inherently superior to non-alphabetic systems, and which, like Anglocentrism, has also stymied psychologists' and educators' thinking about learning to read across diverse writing systems. Here too, I join other scholars who have also expressed concerns about \u201calphabetolatry,\u201d or alphabetic \u201csupremacism\u201d (e.g., Rogers, 1995). Looking around the globe, it is apparent that most individuals do not acquire literacy in a European alphabet, yet in many parts of the (non-European) world, the belief that alphabetic orthographies are the ideal has led to calls to alphabetize or discard non-alphabetic scripts. Needless to say, these proposals have profound ramifications for instruction and curriculum. \n \nIn the past, many influential Western scholars explicitly argued that alphabets are inherently superior to non-alphabetic writing systems (Taylor, 1883; Gelb, 1963; Havelock, 1982). The shelves of most college libraries abound with volumes whose very titles idealize the alphabet (e.g., Diringer's The Alphabet: A Key to the History of Mankind; Moorhouse's Triumph of the Alphabet). When reading researchers today seek enlightenment on the subject of writing systems they refer to Gelb\u2014the founding father of the field of \u201cgrammatology\u201d (Gelb, 1963). Like Taylor (1883) before him, Gelb (1963) propounded an evolutionary view of writing system history from \u201cprimitive\u201d pre-alphabetic systems to alphabetic. Consistent with the \u201contogeny recapitulates phylogeny\u201d idea, Gelb's inexorable \u201cthree great steps [logographic-to-syllabic-to-alphabetic] by which writing evolved from the primitive stages to a full alphabet\u201d (p. 203) was embraced by almost all reading researchers, despite its repudiation by subsequent scholarship in the field of writing systems research (Mattingly, 1985; Olson, 1989; Daniels, 1992, in press; Rogers, 2005; Coulmas, 2009). Foremost among these, perhaps, was Ferreiro in her Piagetian classic Literacy before Schooling (Ferreiro and Teberosky, 1979) and, subsequently, a series of stage-oriented theories of reading and writing development (Piagetian and non-Piagetian alike) all referring to pre-alphabetic and alphabetic stages (Gough and Hillinger, 1980; Marsh et al., 1981; Frith, 1985; Ehri, 2005). It needs to be pointed out, however, that the \u201cculture\u201d of alphabetism, like culture in general, is often \u201cinvisible\u201d; its presence more often discernible in acts of omission rather than commission. Nonetheless, this alphabetic bias is ubiquitous and is manifest in; \n \n \n1. Unqualified generalizations about reading \u201cacross languages\u201d and/or \u201cacross orthographies\u201d in papers that refer almost exclusively to English or to European alphabets (Ziegler and Goswami, 2005; Goswami, 2010; Ziegler et al., 2010; Caravolas et al., 2013; Ehri, 2014). \n \n \n2. Implicit or explicit acceptance of Gelb's long-discarded evolutionary theory in leading texts on reading development aimed at educators,\u2026 \u201cTaking the final step toward the creation of a true alphabetic writing system, the Greeks assigned a symbol to each consonant and vowel of their language\u2026 In many ways, the individual development of the children who are discovering the alphabetic principle in English writing recapitulates human history,\u201d (Moats, 2000, pp. 82\u201383). \n \n \n3. Even the most up-to-date and authoritative texts on the psychology of reading (e.g., DeHaene, 2009; Rayner et al., 2012) continue to regurgitate Gelb's views. \n \n \n \n\u201c[I]n an evolutionary sense, the alphabet is the \u201cfittest\u2026\u201d p. 37\u201d; The history of writing suggests a clear evolutionary trend\u2026These systems evolved to a logographic system, which in turn evolved to syllabic systems and finally to alphabetic systems\u2026Such an evolutionary argument suggests that alphabets are fitter (in the Darwinian sense)\u2026 Rayner et al. (2012, pp. 46\u201347). \n \n \n4. Reference to non-alphabetic systems as imperfect or defective (e.g., Hannas, 2003; Rayner et al., 2012) as well as attempts to reframe non-alphabetic systems such as the Brahmi-derived Indic (abugidic/aksharik) scripts as alphabetic (Rimzhim et al., 2014). \n \n \n \n\u2026\u201cThe Semitic writing systems\u2026and the languages of India still incompletely represent vowels. p 36\u2026 In this sense, many of these scripts are not fully alphabetic.\u201d Rayner et al. p. 37. \n \n\u201cThe Phonecian system, however, was not perfect. It failed to represent all vowels\u2026 It was the Greeks who finally created the alphabet as we know it\u2026 For the first time in the history of mankind, the alphabet allowed the Greeks to have a complete graphic inventory of their language sounds.\u201d (DeHaene, 2009, p. 193). \n \n\u201cThe basic difference between Western alphabetic and East Asian syllabic writing acts on several levels to promote or inhibit creativity, particularly that associated with breakthroughs in science\u2026 syllabic literacy entails a diminished propensity for abstract and analytical thought\u2026 Certain Asian characteristics credited with blocking creativity, such as conservative political and social institutions and group-oriented behavior, derive in part from effects that the orthography has had on the minds of individuals,\u201d (Hannas, 2003, p. 203). \n \n \n5. The use of alphabetic terminology (e.g., letters, graphemes) to describe and label the functional architecture (and even the anatomical brain structures) of reading (\u201cletter detectors,\u201d \u201cletterbox area,\u201d \u201cuniversal letter shapes,\u201d DeHaene, 2009) purported to be universal in reading. Whereas the concept of a letter (or grapheme) is widely used (but not entirely unproblematic) in European alphabets, it has questionable applicability to many writing systems such as Chinese characters, Japanese Kanji, Brahmi-derived Indic aksharas or Mayan glyphs. It has even been suggested that the notion of the \u201cphoneme\u201d as the fundamental unit of analysis of speech may be an artifact of West European alphabetic literacy (Daniels, in press). \n \n \n \nAlthough some initial thoughts have been offered as to when an alphabet may or may not be the appropriate orthography (e.g., Perfetti and Harris, 2013), this topic is new to the agenda of reading science. Some historical background on the alphabet provides a valuable perspective on this issue.",
            "year": 2014,
            "citationCount": 41,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is argued that because spelling\u2013sound relations are so complex in English orthography, much of reading research has been confined to a narrow Anglocentric research agenda addressing theoretical and applied issues with only limited relevance for a universal science of reading and literacy."
            },
            "score": 3
        },
        {
            "id": "efabdd27929796b712cb1b3a3051ea5358dc1200",
            "paperId": "efabdd27929796b712cb1b3a3051ea5358dc1200",
            "title": "A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning",
            "abstract": "Vision-language models can encode societal biases and stereotypes, but there are challenges to measuring and mitigating these multimodal harms due to lacking measurement robustness and feature degradation. To address these challenges, we investigate bias measures and apply ranking metrics for image-text representations. We then investigate debiasing methods and show that prepending learned embeddings to text queries that are jointly trained with adversarial debiasing and a contrastive loss, reduces various bias measures with minimal degradation to the image-text representation.",
            "year": 2022,
            "citationCount": 53,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Debiasing methods are investigated and it is shown that prepending learned embeddings to text queries that are jointly trained with adversarial debiasing and a contrastive loss, reduces various bias measures with minimal degradation to the image-text representation."
            },
            "score": 3
        },
        {
            "id": "309590eedc12ce5b48e0259b2dbb826722fe82a3",
            "paperId": "309590eedc12ce5b48e0259b2dbb826722fe82a3",
            "title": "Backpack Language Models",
            "abstract": "We present Backpacks: a new neural architecture that marries strong modeling performancewith an interface for interpretability and control. Backpacks learn multiple non-contextual sense vectors for each word in a vocabulary, and represent a word in a sequence as a context-dependent, non-negative linear combination ofsense vectors in this sequence. We find that, after training, sense vectors specialize, each encoding a different aspect of a word. We can interpret a sense vector by inspecting its (non-contextual, linear) projection onto the output space, and intervene on these interpretable hooks to change the model\u2019s behavior in predictable ways. We train a 170M-parameter Backpack language model on OpenWebText, matching the loss of a GPT-2 small (124Mparameter) Transformer. On lexical similarity evaluations, we find that Backpack sense vectors outperform even a 6B-parameter Transformer LM\u2019s word embeddings. Finally, we present simple algorithms that intervene on sense vectors to perform controllable text generation and debiasing. For example, we can edit the sense vocabulary to tend more towards a topic, or localize a source of gender bias to a sense vector and globally suppress that sense.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents Backpacks, a new neural architecture that marries strong modeling performance with an interface for interpretability and control, and trains a 170M-parameter Backpack language model on OpenWebText, matching the loss of a GPT-2 small (124Mparameter) Transformer."
            },
            "score": 3
        },
        {
            "id": "cf291ac292eb4912492195011539ab1567efb7c5",
            "paperId": "cf291ac292eb4912492195011539ab1567efb7c5",
            "title": "Visual Exploration of Indirect Bias in Language Models",
            "abstract": "Language models are trained on large text corpora that often include stereotypes. This can lead to direct or indirect bias in downstream applications. In this work, we present a method for interactive visual exploration of indirect multiclass bias learned by contextual word embeddings. We introduce a new indirect bias quantification score and present two interactive visualizations to explore interactions between multiple non-sensitive concepts (such as sports, occupations, and beverages) and sensitive attributes (such as gender or year of birth) based on this score.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new indirect bias quantification score is introduced and two interactive visualizations are presented to explore interactions between multiple non-sensitive concepts and sensitive attributes based on this score."
            },
            "score": 3
        },
        {
            "id": "8a53a8be83f1472b1f8c9bec04bc430d2bebda81",
            "paperId": "8a53a8be83f1472b1f8c9bec04bc430d2bebda81",
            "title": "Can a Prediction\u2019s Rank Offer a More Accurate Quantification of Bias? A Case Study Measuring Sexism in Debiased Language Models",
            "abstract": "Pre-trained language models are known to inherit a plethora of contextual biases from their training data. These biases have proven to be projected onto a variety of downstream applications, making their detection and mitigation imminent. Limited research has been conducted to quantify specific bias types, such as benevolent sexism, which may be subtly present within the inferred connotations of a sentence. To this extent, our work aims to: (1) provide a benchmark of sexism sentences; (2) adapt two bias metrics: mean probability score and mean normalized rank; (3) conduct a case study to quantify and analyze sexism in base and de-biased masked language models. We find that debiasing, even in its most effective form (Auto-Debias), solely nullifies the probability score of biasing tokens, while retaining them in high ranks. Auto-Debias illustrates a 90%-96% reduction in mean probability scores from base to debiased models, while only a 3%-16% reduction in mean normalized ranks. Similar to the application of non-parametric statistical tests for data that does not follow a normal distribution, operating on the ranks of predictions rather than their probability scores offers a more representative bias measure.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that debiasing, even in its most effective form (Auto-Debias), solely nullifies the probability score of biasing tokens, while retaining them in high ranks."
            },
            "score": 3
        },
        {
            "id": "b958cd5cb49044066e43cb3164b7e24cfc687cf3",
            "paperId": "b958cd5cb49044066e43cb3164b7e24cfc687cf3",
            "title": "Debiased Large Language Models Still Associate Muslims with Uniquely Violent Acts",
            "abstract": "Recent work demonstrates a bias in the GPT-3 model towards generating violent text completions when prompted about Muslims, compared with Christians and Hindus. Two pre-registered replication attempts, one exact and one approximate, found only the weakest bias in the more recent Instruct Series version of GPT-3, fine-tuned to eliminate biased and toxic outputs. Few violent completions were observed. Additional pre-registered experiments, however, showed that using common names associated with the religions in prompts yields a highly significant increase in violent completions, also revealing a stronger second-order bias against Muslims. Names of Muslim celebrities from non-violent domains resulted in relatively fewer violent completions, suggesting that access to individualized information can steer the model away from using stereotypes. Nonetheless, content analysis revealed religion-specific violent themes containing highly offensive ideas regardless of prompt format. Our results show the need for additional debiasing of large language models to address higher-order schemas and associations.",
            "year": 2022,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results show the need for additional debiasing of large language models to address higher-order schemas and associations in GPT-3, and suggest that access to individualized information can steer the model away from using stereotypes."
            },
            "score": 3
        },
        {
            "id": "f611c901b9374176bd96114443963c106b9f0b97",
            "paperId": "f611c901b9374176bd96114443963c106b9f0b97",
            "title": "FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication",
            "abstract": "Recent dataset deduplication techniques have demonstrated that content-aware dataset pruning can dramatically reduce the cost of training Vision-Language Pretrained (VLP) models without significant performance losses compared to training on the original dataset. These results have been based on pruning commonly used image-caption datasets collected from the web -- datasets that are known to harbor harmful social biases that may then be codified in trained models. In this work, we evaluate how deduplication affects the prevalence of these biases in the resulting trained models and introduce an easy-to-implement modification to the recent SemDeDup algorithm that can reduce the negative effects that we observe. When examining CLIP-style models trained on deduplicated variants of LAION-400M, we find our proposed FairDeDup algorithm consistently leads to improved fairness metrics over SemDeDup on the FairFace and FACET datasets while maintaining zero-shot performance on CLIP benchmarks.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work evaluates how deduplication affects the prevalence of these biases in the resulting trained models and introduces an easy-to-implement modification to the recent SemDeDup algorithm that can reduce the negative effects."
            },
            "score": 3
        },
        {
            "id": "2befabbf128909202d80e450322385c49f4775c3",
            "paperId": "2befabbf128909202d80e450322385c49f4775c3",
            "title": "Undesirable Biases in NLP: Addressing Challenges of Measurement",
            "abstract": "As Large Language Models and Natural Language Processing (NLP) technology rapidly develop and spread into daily life, it becomes crucial to anticipate how their use could harm people. One problem that has received a lot of attention in recent years is that this technology has displayed harmful biases, from generating derogatory stereotypes to producing disparate outcomes for different social groups. Although a lot of effort has been invested in assessing and mitigating these biases, our methods of measuring the biases of NLP models have serious problems and it is often unclear what they actually measure. In this paper, we provide an interdisciplinary approach to discussing the issue of NLP model bias by adopting the lens of psychometrics \u2014 a field specialized in the measurement of concepts like bias that are not directly observable. In particular, we will explore two central notions from psychometrics, the construct validity and the reliability of measurement tools, and discuss how they can be applied in the context of measuring model bias. Our goal is to provide NLP practitioners with methodological tools for designing better bias measures, and to inspire them more generally to explore tools from psychometrics when working on bias measurement tools.\nThis article appears in the AI & Society track.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper provides an interdisciplinary approach to discussing the issue of NLP model bias by adopting the lens of psychometrics \u2014 a field specialized in the measurement of concepts like bias that are not directly observable."
            },
            "score": 3
        },
        {
            "id": "9e3c493fb09dcd61bb05e8c5659f23327b7b6340",
            "paperId": "9e3c493fb09dcd61bb05e8c5659f23327b7b6340",
            "title": "Teaching Large Language Models to Self-Debug",
            "abstract": "Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any human feedback on the code correctness or error messages, the model is able to identify its mistakes by investigating the execution results and explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, Self-Debugging with code explanation consistently improves the baseline by 2-3%, and improves the prediction accuracy on problems of the hardest level by 9%. On TransCoder and MBPP where unit tests are available, Self-Debugging improves the baseline accuracy by up to 12%. Meanwhile, by leveraging feedback messages and reusing failed predictions, Self-Debugging notably improves sample efficiency, and can match or outperform baseline models that generate more than 10x candidate programs.",
            "year": 2023,
            "citationCount": 257,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Self-Debugging is proposed, which teaches a large language model to debug its predicted program via few-shot demonstrations, and can match or outperform baseline models that generate more than 10x candidate programs."
            },
            "score": 3
        },
        {
            "id": "e7c97e953849f1a8e5d85ceb4cfcc0a5d54d2365",
            "paperId": "e7c97e953849f1a8e5d85ceb4cfcc0a5d54d2365",
            "title": "Enabling Large Language Models to Generate Text with Citations",
            "abstract": "Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. In this work, our aim is to allow LLMs to generate text with citations, improving their factual correctness and verifiability. Existing work mainly relies on commercial search engines and human evaluation, making it challenging to reproduce and compare different modeling approaches. We propose ALCE, the first benchmark for Automatic LLMs' Citation Evaluation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations. We develop automatic metrics along three dimensions -- fluency, correctness, and citation quality -- and demonstrate their strong correlation with human judgements. Our experiments with state-of-the-art LLMs and novel prompting strategies show that current systems have considerable room for improvement -- For example, on the ELI5 dataset, even the best models lack complete citation support 50% of the time. Our analyses further highlight promising future directions, including developing better retrievers, advancing long-context LLMs, and improving the ability to synthesize information from multiple sources.",
            "year": 2023,
            "citationCount": 102,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes ALCE, the first benchmark for Automatic LLMs' Citation Evaluation, and develops automatic metrics along three dimensions -- fluency, correctness, and citation quality -- and demonstrates their strong correlation with human judgements."
            },
            "score": 3
        },
        {
            "id": "439f1aacbcb32cba6da8f75bd9b15f7ea35e9d4d",
            "paperId": "439f1aacbcb32cba6da8f75bd9b15f7ea35e9d4d",
            "title": "Making Large Language Models Better Data Creators",
            "abstract": "Although large language models (LLMs) have advanced the state-of-the-art in NLP significantly, deploying them for downstream applications is still challenging due to cost, responsiveness, control, or concerns around privacy and security. As such, trainable models are still the preferred option in some cases. However, these models still require human-labeled data for optimal performance, which is expensive and time-consuming to obtain. In order to address this issue, several techniques to reduce human effort involve labeling or generating data using LLMs. Although these methods are effective for certain applications, in practice they encounter difficulties in real-world scenarios. Labeling data requires careful data selection, while generating data necessitates task-specific prompt engineering. In this paper, we propose a unified data creation pipeline that requires only a single formatting example, and which is applicable to a broad range of tasks, including traditionally problematic ones with semantically devoid label spaces. In our experiments we demonstrate that instruction-following LLMs are highly cost-effective data creators, and that models trained with these data exhibit performance better than those trained with human-labeled data (by up to 17.5%) on out-of-distribution evaluation, while maintaining comparable performance on in-distribution tasks. These results have important implications for the robustness of NLP systems deployed in the real-world.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a unified data creation pipeline that requires only a single formatting example, and which is applicable to a broad range of tasks, including traditionally problematic ones with semantically devoid label spaces, and demonstrates that instruction-following LLMs are highly cost-effective data creators."
            },
            "score": 3
        },
        {
            "id": "bc79ac3d5e1c05fbf0a4f165b8841defe6592417",
            "paperId": "bc79ac3d5e1c05fbf0a4f165b8841defe6592417",
            "title": "Help Me Heal: A Reinforced Polite and Empathetic Mental Health and Legal Counseling Dialogue System for Crime Victims",
            "abstract": "The potential for conversational agents offering mental health and legal counseling in an autonomous, interactive, and vitally accessible environment is getting highlighted due to the increased access to information through the internet and mobile devices. A counseling conversational agent should be able to offer higher engagement mimicking the real-time counseling sessions. The ability to empathize or comprehend and feel another person\u2019s emotions and experiences is a crucial quality that promotes effective therapeutic bonding and rapport-building. Further, the use of polite encoded language in the counseling reflects the nobility and creates a familiar, warm, and comfortable atmosphere to resolve human issues. Therefore, focusing on these two aspects, we propose a Polite and Empathetic Mental Health and Legal Counseling Dialogue System (Po-Em-MHLCDS) for the victims of crimes. To build Po-Em-MHLCDS, we first create a Mental Health and Legal Counseling Dataset (MHLCD) by recruiting six employees who are asked to converse with each other, acting as a victim and the agent interchangeably following a fixed stated guidelines. Second, the MHLCD dataset is annotated with three informative labels, viz. counseling strategies, politeness, and empathy. Lastly, we train the Po-Em-MHLCDS in a reinforcement learning framework by designing an efficient and effective reward function to reinforce correct counseling strategy, politeness and empathy while maintaining contextual-coherence and non-repetitiveness in the generated responses. Our extensive automatic and human evaluation demonstrate the strength of the proposed system. Codes and Data can be accessed at https://www.iitp.ac.in/ ai-nlp-ml/resources.html#MHLCD or https://github.com/Mishrakshitij/Po-Em-MHLCDS",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Polite and Empathetic Mental Health and Legal Counseling Dialogue System (Po-Em-MHLCDS) for the victims of crimes and an efficient and effective reward function to reinforce correct counseling strategy, politeness and empathy while maintaining contextual-coherence and non-repetitiveness in the generated responses are proposed."
            },
            "score": 3
        },
        {
            "id": "1e08e075634eb2b682cfdcee0da5966b06bdadcc",
            "paperId": "1e08e075634eb2b682cfdcee0da5966b06bdadcc",
            "title": "Elevating Employment Practices in Agricultural Corporations with Large Language Models and AI",
            "abstract": "This research proposal aims to leverage modern AI techniques, particularly ChatGPT, for transforming human resources in the Agriculture Credit Corporation. The focus is on AI's role in streamlining recruitment, personalizing employee experiences, mitigating biases, and enhancing decision-making. By utilizing AI and ChatGPT, this study seeks to boost competitiveness, efficiency, and HR effectiveness. The insights will guide AI-driven HR solutions in agriculture, benefiting the organization and the industry. In the era of foundation models and large language models (LLMs), understanding these models' potential and limitations is vital for this research, but access is limited to big tech companies due to resource constraints. Comprehensive research and multidisciplinary collaboration are essential to unravel the capabilities and challenges posed by these models.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 2
        },
        {
            "id": "2cf1f6c723006f258599fd9f000bb616ae83387a",
            "paperId": "2cf1f6c723006f258599fd9f000bb616ae83387a",
            "title": "Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models",
            "abstract": "The performance of large language models (LLMs) on existing reasoning benchmarks has significantly improved over the past years. In response, we present JEEBench, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs. We curate 515 challenging pre-engineering mathematics, physics and chemistry problems from the highly competitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep in-domain knowledge is essential for solving problems in this benchmark. Our evaluation on various open-source and proprietary models reveals that the highest performance, even after using techniques like self-consistency, self-refinement and chain-of-thought prompting, is less than 40%. The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts. We also observe that by mere prompting, GPT-4 is unable to assess risk introduced by negative marking for incorrect answers. For this, we develop a post-hoc confidence-thresholding method over self-consistency, which enables effective response selection. We hope that our challenging benchmark will guide future re-search in problem-solving using LLMs.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "JEEBench is presented, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs, and a post-hoc confidence-thresholding method over self-consistency is developed, which enables effective response selection."
            },
            "score": 2
        },
        {
            "id": "fa7805c7ad42610b89d07353cb3600f3ecaf2c2f",
            "paperId": "fa7805c7ad42610b89d07353cb3600f3ecaf2c2f",
            "title": "Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study",
            "abstract": "Evaluating the quality of generated text is a challenging task in NLP, due to the inherent complexity and diversity of text. Recently, large language models (LLMs) have garnered significant attention due to their impressive performance in various tasks. Therefore, we present this paper to investigate the effectiveness of LLMs, especially ChatGPT, and explore ways to optimize their use in assessing text quality. We compared three kinds of reference-free evaluation methods. The experimental results prove that ChatGPT is capable of evaluating text quality effectively from various perspectives without reference and demonstrates superior performance than most existing automatic metrics. In particular, the Explicit Score, which utilizes ChatGPT to generate a numeric score measuring text quality, is the most effective and reliable method among the three exploited approaches. However, directly comparing the quality of two texts may lead to suboptimal results. We believe this paper will provide valuable insights for evaluating text quality with LLMs and have released the used data.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results prove that ChatGPT is capable of evaluating text quality effectively from various perspectives without reference and demonstrates superior performance than most existing automatic metrics."
            },
            "score": 2
        },
        {
            "id": "e8d513bc7554a83161f2fb26c8299b471581cdb6",
            "paperId": "e8d513bc7554a83161f2fb26c8299b471581cdb6",
            "title": "Can We Edit Multimodal Large Language Models?",
            "abstract": "In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new benchmark is constructed, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation, to facilitate research in this area and provide the NLP community with insights."
            },
            "score": 2
        },
        {
            "id": "a3ba7fdf789bcef381acd0d277a086428153bb9f",
            "paperId": "a3ba7fdf789bcef381acd0d277a086428153bb9f",
            "title": "TemporalWiki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models",
            "abstract": "Language Models (LMs) become outdated as the world changes; they often fail to perform tasks requiring recent factual information which was absent or different during training, a phenomenon called temporal misalignment. This is especially a challenging problem because the research community still lacks a coherent dataset for assessing the adaptability of LMs to frequently-updated knowledge corpus such as Wikipedia. To this end, we introduce TemporalWiki, a lifelong benchmark for ever-evolving LMs that utilizes the difference between consecutive snapshots of English Wikipedia and English Wikidata for training and evaluation, respectively. The benchmark hence allows researchers to periodically track an LM\u2019s ability to retain previous knowledge and acquire updated/new knowledge at each point in time. We also find that training an LM on the diff data through continual learning methods achieves similar or better perplexity than on the entire snapshot in our benchmark with 12 times less computational cost, which verifies that factual knowledge in LMs can be safely updated with minimal training data via continual learning.",
            "year": 2022,
            "citationCount": 51,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "TemporalWiki is introduced, a lifelong benchmark for ever-evolving LMs that utilizes the difference between consecutive snapshots of English Wikipedia and English Wikidata for training and evaluation, which verifies that factual knowledge in LMs can be safely updated with minimal training data via continual learning."
            },
            "score": 2
        },
        {
            "id": "8da9b1436212b233fc49c7daf1ba15c22874ff5a",
            "paperId": "8da9b1436212b233fc49c7daf1ba15c22874ff5a",
            "title": "CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models",
            "abstract": "Large Language Models (LLMs) have made significant progress in utilizing tools, but their ability is limited by API availability and the instability of implicit reasoning, particularly when both planning and execution are involved. To overcome these limitations, we propose CREATOR, a novel framework that enables LLMs to create their own tools using documentation and code realization. CREATOR disentangles abstract tool creation and concrete decision execution, resulting in improved performance. We evaluate CREATOR on MATH and TabMWP benchmarks, respectively consisting of challenging math competition problems and diverse tabular contents. Remarkably, CREATOR outperforms existing chain-of-thought, program-of-thought, and tool-using baselines. Additionally, we introduce the Creation Challenge dataset, featuring 2K diverse questions, to emphasize the necessity and benefits of LLMs' tool creation ability. Further research demonstrates that leveraging LLMs as tool creators facilitates knowledge transfer, and LLMs exhibit varying levels of tool creation abilities, enabling them to adapt to diverse situations. The tool creation ability revolutionizes the LLM's problem-solving paradigm, driving us closer to the next frontier of artificial intelligence. All the codes and data are released.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The proposed CREATOR, a novel framework that enables LLMs to create their own tools using documentation and code realization, disentangles abstract tool creation and concrete decision execution, resulting in improved performance."
            },
            "score": 2
        },
        {
            "id": "9cd398e75e89b9d8104837da44ad17e110a4e4f9",
            "paperId": "9cd398e75e89b9d8104837da44ad17e110a4e4f9",
            "title": "Explicit Planning Helps Language Models in Logical Reasoning",
            "abstract": "Language models have been shown to perform remarkably well on a wide range of natural language processing tasks. In this paper, we propose LEAP, a novel system that uses language models to perform multi-step logical reasoning and incorporates explicit planning into the inference procedure. Explicit planning enables the system to make more informed reasoning decisions at each step by looking ahead into their future effects. Moreover, we propose a training strategy that safeguards the planning process from being led astray by spurious features. Our full system significantly outperforms other competing methods on multiple standard datasets. When using small T5 models as its core selection and deduction components, our system performs competitively compared to GPT-3 despite having only about 1B parameters (i.e., 175 times smaller than GPT-3). When using GPT-3.5, it significantly outperforms chain-of-thought prompting on the challenging PrOntoQA dataset. We have conducted extensive empirical studies to demonstrate that explicit planning plays a crucial role in the system's performance.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes LEAP, a novel system that uses language models to perform multi-step logical reasoning and incorporates explicit planning into the inference procedure and proposes a training strategy that safeguards the planning process from being led astray by spurious features."
            },
            "score": 2
        },
        {
            "id": "9d12916dd46df7a6446cbec0bc4d054f7dafcdab",
            "paperId": "9d12916dd46df7a6446cbec0bc4d054f7dafcdab",
            "title": "Scaling Vision-Language Models with Sparse Mixture of Experts",
            "abstract": "The field of natural language processing (NLP) has made significant strides in recent years, particularly in the development of large-scale vision-language models (VLMs). These models aim to bridge the gap between text and visual information, enabling a more comprehensive understanding of multimedia data. However, as these models become larger and more complex, they also become more challenging to train and deploy. One approach to addressing this challenge is the use of sparsely-gated mixture-of-experts (MoE) techniques, which divide the model into smaller, specialized sub-models that can jointly solve a task. In this paper, we explore the effectiveness of MoE in scaling vision-language models, demonstrating its potential to achieve state-of-the-art performance on a range of benchmarks over dense models of equivalent computational cost. Our research offers valuable insights into stabilizing the training of MoE models, understanding the impact of MoE on model interpretability, and balancing the trade-offs between compute performance when scaling VLMs. We hope our work will inspire further research into the use of MoE for scaling large-scale vision-language models and other multimodal machine learning applications.",
            "year": 2023,
            "citationCount": 26,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The effectiveness of MoE in scaling vision-language models is explored, demonstrating its potential to achieve state-of-the-art performance on a range of benchmarks over dense models of equivalent computational cost."
            },
            "score": 1
        }
    ],
    "novelty_check_papers": [
        {
            "id": "52db5adffa53911c20b7cd884e8a2f2151a3c114",
            "paperId": "52db5adffa53911c20b7cd884e8a2f2151a3c114",
            "title": "Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution",
            "abstract": "Large language models (LLMs) reflect societal norms and biases, especially about gender. While societal biases and stereotypes have been extensively researched in various NLP applications, there is a surprising gap for emotion analysis. However, emotion and gender are closely linked in societal discourse. E.g., women are often thought of as more empathetic, while men's anger is more socially accepted. To fill this gap, we present the first comprehensive study of gendered emotion attribution in five state-of-the-art LLMs (open- and closed-source). We investigate whether emotions are gendered, and whether these variations are based on societal stereotypes. We prompt the models to adopt a gendered persona and attribute emotions to an event like 'When I had a serious argument with a dear person'. We then analyze the emotions generated by the models in relation to the gender-event pairs. We find that all models consistently exhibit gendered emotions, influenced by gender stereotypes. These findings are in line with established research in psychology and gender studies. Our study sheds light on the complex societal interplay between language, gender, and emotion. The reproduction of emotion stereotypes in LLMs allows us to use those models to study the topic in detail, but raises questions about the predictive use of those same LLMs for emotion applications.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The first comprehensive study of gendered emotion attribution in five state-of-the-art LLMs (open- and closed-source) is presented, finding that all models consistently exhibit gendered emotions, influenced by gender stereotypes."
            },
            "score": 7,
            "novelty_score": "Research Problem in Proposal: Mitigating social biases in language models through empathetic contextual debiasing.\n\nResearch Problem in Paper: Investigating gendered emotion attribution in large language models.\n\nThe proposal focuses on developing a debiasing method to mitigate social biases in language model outputs, while the paper studies how language models reflect gendered stereotypes in emotion attribution. Although both works deal with social biases in language models, the specific problems and approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "4a2a1a107964c19a8b4a523a7fcd78e166e85f21",
            "paperId": "4a2a1a107964c19a8b4a523a7fcd78e166e85f21",
            "title": "Self-Debiasing Large Language Models: Zero-Shot Recognition and Reduction of Stereotypes",
            "abstract": "Large language models (LLMs) have shown remarkable advances in language generation and understanding but are also prone to exhibiting harmful social biases. While recognition of these behaviors has generated an abundance of bias mitigation techniques, most require modifications to the training data, model parameters, or decoding strategy, which may be infeasible without access to a trainable model. In this work, we leverage the zero-shot capabilities of LLMs to reduce stereotyping in a technique we introduce as zero-shot self-debiasing. With two approaches, self-debiasing via explanation and self-debiasing via reprompting, we show that self-debiasing can significantly reduce the degree of stereotyping across nine different social groups while relying only on the LLM itself and a simple prompt, with explanations correctly identifying invalid assumptions and reprompting delivering the greatest reductions in bias. We hope this work opens inquiry into other zero-shot techniques for bias mitigation.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that self-debiasing can significantly reduce the degree of stereotyping across nine different social groups while relying only on the LLM itself and a simple prompt, with explanations correctly identifying invalid assumptions and reprompting delivering the greatest reductions in bias."
            },
            "score": 7,
            "novelty_score": "Research Problem:\nProposal: Mitigating social biases in language models through stereotype-challenging reframing of the input context.\nPaper: Reducing stereotyping in large language models using zero-shot self-debiasing techniques.\n\nApproach:\nProposal: Empathetic Contextual Debiasing (ECD) - identifying mentions of social groups and stereotypes in the input prompt, generating empathetic reframings that challenge stereotypical associations, and comparing the bias in generated continuations.\nPaper: Zero-shot self-debiasing via explanation and reprompting, leveraging the language model's own capabilities to recognize and reduce stereotyping without modifying the model.\n\nWhile both works aim to mitigate social biases in language models, the proposal focuses on debiasing through modifying the input context, while the paper proposes zero-shot techniques that rely on the model's own abilities without external modifications. The approaches differ in their level of intervention and reliance on the model's inherent capabilities.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "1dea08f23e424973dc660b5eb22a6f1cba285795",
            "paperId": "1dea08f23e424973dc660b5eb22a6f1cba285795",
            "title": "In-Contextual Gender Bias Suppression for Large Language Models",
            "abstract": "Despite their impressive performance in a wide range of NLP tasks, Large Language Models (LLMs) have been reported to encode worrying-levels of gender biases. Prior work has proposed debiasing methods that require human labelled examples, data augmentation and fine-tuning of LLMs, which are computationally costly. Moreover, one might not even have access to the model parameters for performing debiasing such as in the case of closed LLMs such as GPT-4. To address this challenge, we propose bias suppression that prevents biased generations of LLMs by simply providing textual preambles constructed from manually designed templates and real-world statistics, without accessing to model parameters. We show that, using CrowsPairs dataset, our textual preambles covering counterfactual statements can suppress gender biases in English LLMs such as LLaMA2. Moreover, we find that gender-neutral descriptions of gender-biased objects can also suppress their gender biases. Moreover, we show that bias suppression has acceptable adverse effect on downstream task performance with HellaSwag and COPA.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that, using CrowsPairs dataset, textual preambles covering counterfactual statements can suppress gender biases in English LLMs such as LLaMA2, and it is found that gender-neutral descriptions of gender-biased objects can also suppress their gender biases."
            },
            "score": 7,
            "novelty_score": "Research Problem in Proposal: Mitigating social biases in language models through stereotype-challenging reframing of input prompts.\n\nApproach in Proposal: Empathetic Contextual Debiasing (ECD) framework that identifies mentions of sensitive social groups and stereotype-related concepts in the input prompt, generates empathetic reframings that challenge stereotypical associations, and compares the level of stereotyping in the generated texts from the original vs. reframed prompts.\n\nResearch Problem in Abstract: Suppressing gender biases in large language models without requiring access to model parameters or fine-tuning.\n\nApproach in Abstract: Providing textual preambles constructed from manually designed templates and real-world statistics to prevent biased generations of LLMs.\n\nWhile both the proposal and the abstract aim to mitigate biases in language models, their approaches differ significantly. The proposal focuses on contextual debiasing through empathetic reframing of input prompts, while the abstract proposes using textual preambles with counterfactual statements and real-world statistics to suppress gender biases without modifying the model itself.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "114aa720872462b0ca1b97bfdec0ebd56c36fd0a",
            "paperId": "114aa720872462b0ca1b97bfdec0ebd56c36fd0a",
            "title": "Towards Understanding and Mitigating Social Biases in Language Models",
            "abstract": "As machine learning methods are deployed in real-world settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases - harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation. Our empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for high-fidelity text generation, thereby pushing forward the performance-fairness Pareto frontier.",
            "year": 2021,
            "citationCount": 237,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work carefully defines several sources of representational biases before proposing new benchmarks and metrics to measure them and demonstrates effectiveness in mitigating bias while retaining crucial contextual information for high-fidelity text generation, thereby pushing forward the performance-fairness Pareto frontier."
            },
            "score": 7,
            "novelty_score": "Research Problem in Proposal: Mitigating social biases and stereotypes in language models when generating text in context.\nApproach in Proposal: Empathetic Contextual Debiasing (ECD) framework that identifies mentions of sensitive social groups and stereotype-related concepts in the input prompt, generates empathetic reframings to challenge stereotypical associations, and compares the level of stereotyping in the generated texts.\n\nResearch Problem in Abstract: Understanding and mitigating social biases in language models.\nApproach in Abstract: Defining sources of representational biases, proposing new benchmarks and metrics to measure them, and taking steps towards mitigating social biases during text generation.\n\nThe proposal and the abstract both focus on the problem of social biases in language models. However, the proposal specifically addresses bias amplification during contextual text generation and proposes a targeted empathetic reframing approach. In contrast, the abstract takes a broader view of defining and measuring representational biases in language models, without the specific focus on contextual generation or the empathetic reframing method.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "8a515a6510e209f1ab9e53d70c291c7e007716d5",
            "paperId": "8a515a6510e209f1ab9e53d70c291c7e007716d5",
            "title": "Disclosure and Mitigation of Gender Bias in LLMs",
            "abstract": "Large Language Models (LLMs) can generate biased responses. Yet previous direct probing techniques contain either gender mentions or predefined gender stereotypes, which are challenging to comprehensively collect. Hence, we propose an indirect probing framework based on conditional generation. This approach aims to induce LLMs to disclose their gender bias even without explicit gender or stereotype mentions. We explore three distinct strategies to disclose explicit and implicit gender bias in LLMs. Our experiments demonstrate that all tested LLMs exhibit explicit and/or implicit gender bias, even when gender stereotypes are not present in the inputs. In addition, an increased model size or model alignment amplifies bias in most cases. Furthermore, we investigate three methods to mitigate bias in LLMs via Hyperparameter Tuning, Instruction Guiding, and Debias Tuning. Remarkably, these methods prove effective even in the absence of explicit genders or stereotypes.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This approach aims to induce LLMs to disclose their gender bias even without explicit gender or stereotype mentions, and investigates three methods to mitigate bias in LLMs via Hyperparameter Tuning, Instruction Guiding, and Debias Tuning."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is mitigating social biases in language models through empathetic reframing of the input context, while the paper focuses on disclosing and mitigating gender bias in language models using indirect probing and techniques like hyperparameter tuning and debiasing. Although both tackle bias in language models, the proposal emphasizes contextual debiasing through reframing, while the paper explores bias disclosure via probing and mitigation through model-level techniques.\n\nNo",
            "novelty_judgment": "no"
        }
    ],
    "novelty": "yes"
}