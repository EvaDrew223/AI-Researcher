{
    "topic_description": "novel prompting methods for large language models to improve mathematical problem solving",
    "idea_name": "Proof-Guided Solution Prompting",
    "raw_idea": {
        "Problem": "Language models can generate plausible-looking solutions to mathematical problems, but often make subtle logical mistakes and arrive at incorrect conclusions. Existing methods focus on the final answer but neglect the underlying proof and reasoning.",
        "Existing Methods": "Benchmarks like IsarStep test mathematical proof understanding and generation. Baselines include direct prompting to generate proofs.",
        "Motivation": "In mathematical problem-solving, a rigorous proof is just as important as the final answer. A correct proof provides strong evidence that the answer is correct and trustworthy. We hypothesize that prompting a language model to generate a complete mathematical proof in a recognized format, in addition to the final answer, will lead to more reliable and accurate problem-solving.",
        "Proposed Method": "We propose Proof-Guided Solution Prompting (PGSP), where we prompt the model to generate a formal mathematical proof for each problem-solving step, in addition to the final numerical answer. The proofs are prompted to follow a standard format like natural deduction or sequent calculus. Key intermediate results in the proof are referenced in the calculation steps. The final answer must be derived as the conclusion of the proof.",
        "Experiment Plan": "We will evaluate PGSP on the IsarStep benchmark, as well as proof-oriented versions of MATH and GSM8K where the model must output a complete proof in addition to the final answer. We will compare to direct prompting and chain-of-thought baselines. The key metrics are proof validity and completeness, as well as overall problem-solving accuracy. We will perform qualitative analysis of the generated proofs on representative problems."
    },
    "full_experiment_plan": {
        "Title": "Proof-Guided Solution Prompting: Improving Mathematical Problem Solving in Large Language Models",
        "Problem Statement": "Large language models can generate plausible-looking solutions to mathematical problems, but often make subtle logical mistakes and arrive at incorrect conclusions. Existing methods focus on the final answer but neglect the underlying proof and reasoning, which is crucial for ensuring the correctness and reliability of the solution.",
        "Motivation": "Mathematical problem-solving requires not only arriving at the correct final answer but also providing a rigorous and logically sound proof. A complete and correct proof provides strong evidence that the answer is correct and trustworthy. Existing benchmarks like IsarStep test mathematical proof understanding and generation, but baselines like direct prompting only focus on generating the final answer without the underlying reasoning. We hypothesize that prompting a language model to generate a formal mathematical proof in a recognized format, in addition to the final answer, will lead to more reliable and accurate problem-solving by encouraging the model to reason through the problem step-by-step and catch any logical inconsistencies or errors.",
        "Proposed Method": "We propose Proof-Guided Solution Prompting (PGSP), where we prompt the model to generate a formal mathematical proof for each problem-solving step, in addition to the final numerical answer. The proofs are prompted to follow a standard format like natural deduction or sequent calculus. Key intermediate results in the proof are referenced in the calculation steps. The final answer must be derived as the conclusion of the proof. By requiring the model to generate a complete proof, we aim to improve the logical rigor and accuracy of the problem-solving process.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "Evaluate PGSP on the IsarStep benchmark for mathematical proof understanding and generation. Also create proof-oriented versions of MATH and GSM8K where the model must output a complete proof in addition to the final answer.",
            "Step 2: Construct Prompts": "Compare PGSP to baselines including direct prompting (only prompt for the final answer) and chain-of-thought prompting (prompt the model to show its work and intermediate calculation steps). For PGSP, prompt the model to generate a full formal proof in addition to the calculation steps and final answer. The proof should be in a standard format like natural deduction or sequent calculus. Intermediate results calculated in the proof should be referenced in the solution steps.",
            "Step 3: Select Models": "Evaluate PGSP on both GPT-3.5 (text-davinci-003) and GPT-4 via the OpenAI API.",
            "Step 4: Get Results": "For each task and model, generate solutions using the baseline prompts and PGSP. For PGSP, parse the generated proofs to check their validity and completeness.",
            "Step 5: Analyze Results": "Compare PGSP to the baselines on key metrics including proof validity and completeness, as well as overall problem-solving accuracy. Perform qualitative analysis of the generated proofs on representative problems to understand the strengths and failure modes of PGSP."
        },
        "Test Case Examples": {
            "Baseline Prompt Input (Direct Prompting)": "Prove that for any integer n, if n^2 is even then n is even.",
            "Baseline Prompt Expected Output (Direct Prompting)": "If n is odd, then n = 2k+1 for some integer k. Then n^2 = (2k+1)^2 = 4k^2 + 4k + 1 = 2(2k^2+2k) + 1, which is odd. Therefore, if n^2 is even, n must be even.",
            "Baseline Prompt Input (Chain-of-Thought Prompting)": "Prove that for any integer n, if n^2 is even then n is even. Show your reasoning step-by-step.",
            "Baseline Prompt Expected Output (Chain-of-Thought Prompting)": "Proof by contraposition. Assume n is odd. Then n = 2k+1 for some integer k.\nSquaring both sides: n^2 = (2k+1)^2\nExpanding: n^2 = 4k^2 + 4k + 1\n            = 2(2k^2+2k) + 1\nSince 2k^2+2k is an integer, 2(2k^2+2k) is even, and adding 1 makes n^2 odd.\nTherefore, if n is odd, n^2 is odd.\nBy contraposition, if n^2 is even, n must be even.",
            "Proposed Prompt Input (PGSP)": "Prove that for any integer n, if n^2 is even then n is even. Provide a complete formal proof in natural deduction format, and show how the proof steps connect to the final answer.",
            "Proposed Prompt Expected Output (PGSP)": "Theorem: For any integer n, if n^2 is even then n is even.\nProof: By contraposition.\n1. Assume n is odd. [Assumption]\n2. Then n = 2k+1 for some integer k. [Definition of odd]\n3. n^2 = (2k+1)^2 [Square both sides]\n4.    = 4k^2 + 4k + 1 [Expand]\n5.    = 2(2k^2 + 2k) + 1 [Factor out 2]\n6. 2k^2 + 2k is an integer, so 2(2k^2 + 2k) is even. [Definition of even]\n7. Adding 1 to an even number results in an odd number. [Property of even/odd numbers]\n8. Therefore, n^2 is odd. [Conclusion from 5, 6, 7]\n9. We have shown: if n is odd, then n^2 is odd. [Conclusion from 1-8]\n10. By contraposition, if n^2 is even, then n is even. [Contraposition of 9]\nThe key steps are:\n- Assume n is odd, so n = 2k+1. (2)\n- Square both sides: n^2 = (2k+1)^2 and expand. (3-5)\n- Factor out 2 to show n^2 = 2(...) + 1, an odd number. (5-8) \n- By contraposition, if n^2 is even, n must be even. (9-10)\nTherefore, we have proven that for any integer n, if n^2 is even then n is even.",
            "Explanation": "The baselines of direct prompting and chain-of-thought prompting show the key reasoning steps, but do not provide a complete formal proof. In contrast, PGSP generates a full proof in natural deduction format, with each step justified. The proof clearly concludes the desired statement by contraposition. Key steps of the proof are referenced to show how they lead to the final answer. By generating a complete proof, PGSP encourages greater logical rigor and consistency in the problem-solving process."
        },
        "Fallback Plan": "If PGSP does not outperform the baselines, we can analyze the generated proofs to understand why. Potential issues could include: proofs are logically invalid or incomplete, proofs do not align with the calculations and final answer, or the model struggles to generate formal proofs in the required format. Based on the failure analysis, we can either refine the prompting approach (e.g., provide more detailed instructions or examples of valid proofs), or focus the project on understanding the limitations of proof generation in language models and identifying opportunities for improvement. The analysis of failure modes in the generated proofs could yield valuable insights to inform future work on integrating formal reasoning into language models."
    }
}