{
    "topic_description": "novel prompting methods for large language models to improve mathematical problem solving",
    "idea_name": "Iterative Solution Refinement Prompting",
    "raw_idea": {
        "Problem": "Large language models can generate complete solution steps for math problems in one shot, but the generated solutions often contain logical flaws or calculation errors that lead to incorrect final answers.",
        "Existing Methods": "Current methods focus on one-shot solution generation via chain-of-thought prompting or self-consistency decoding to marginalize over multiple solution paths.",
        "Motivation": "When humans solve complex problems, they often refine their solution through multiple iterations, by critically analyzing the solution steps to identify and correct mistakes. Language models can be prompted to similarly critique and iteratively refine their own generated solutions.",
        "Proposed Method": "We propose Iterative Solution Refinement Prompting (ISRP), which prompts the model to: 1) Generate an initial solution via chain-of-thought prompting. 2) Critically analyze each step of the solution to identify potential logical flaws or calculation errors. 3) Revise the erroneous parts of the solution while keeping the correct parts intact. 4) Repeat steps 2-3 for multiple iterations until no further errors are spotted or changes are made. The refined solution from the last iteration is returned as the final output.",
        "Experiment Plan": "Evaluate ISRP on GSM8K and SVAMP benchmarks. Compare to one-shot CoT prompting and self-consistency decoding. Vary the number of refinement iterations and analyze the refinement process."
    },
    "full_experiment_plan": {
        "Title": "Iterative Solution Refinement Prompting for Improved Mathematical Problem Solving",
        "Problem Statement": "Large language models can generate complete solution steps for math problems in one shot, but the generated solutions often contain logical flaws or calculation errors that lead to incorrect final answers.",
        "Motivation": "Current methods for mathematical problem solving with language models focus on one-shot solution generation via chain-of-thought prompting or self-consistency decoding to marginalize over multiple solution paths. However, these approaches do not address the fundamental issue of flaws within each generated solution. Humans often refine their solutions through multiple iterations, critically analyzing the solution steps to identify and correct mistakes. We propose prompting language models to similarly critique and iteratively refine their own generated solutions, leveraging their strong language understanding and generation capabilities.",
        "Proposed Method": "We propose Iterative Solution Refinement Prompting (ISRP), which prompts the model to: \n1) Generate an initial solution via chain-of-thought prompting. \n2) Critically analyze each step of the solution to identify potential logical flaws or calculation errors. \n3) Revise the erroneous parts of the solution while keeping the correct parts intact. \n4) Repeat steps 2-3 for multiple iterations until no further errors are spotted or changes are made. \nThe refined solution from the last iteration is returned as the final output.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Datasets": "Evaluate ISRP on the GSM8K and SVAMP datasets for mathematical problem solving. GSM8K contains 8.5K high quality grade school math word problems, and SVAMP contains 1K problems that test multi-step reasoning.",
            "Step 2: Evaluation Metrics": "Use answer accuracy as the main metric, checking if the final generated answer exactly matches the ground truth answer. Also report the percentage of solutions that contain logical flaws or calculation errors, to quantify the error correction capabilities of ISRP.",
            "Step 3: Baselines": "Compare ISRP to two main baselines:\n1) One-shot CoT prompting: Prompt the model to generate the full solution in one shot, using the prompt \"Q: [Question] A: Let's solve this step-by-step:\"\n2) Self-consistency decoding: Sample multiple solution paths via one-shot CoT prompting, and return the most consistent final answer across samples. Use 10 samples.",
            "Step 4: ISRP Implementation": "Implement the key components of ISRP:\n1) Initial solution generation: Use the same one-shot CoT prompt as the baseline.\n2) Solution step analysis: Prompt the model with \"Analyze each step of the solution to identify any logical flaws or calculation errors:\", with the initial solution appended.\n3) Solution revision: Prompt the model with \"Revise the solution to fix the identified errors, while keeping the correct parts unchanged:\", with the initial solution and identified errors appended.\n4) Iterative refinement: Repeat steps 2-3 for a fixed number of iterations N. We experiment with N = 1, 2, 3.",
            "Step 5: Models": "Evaluate ISRP and the baselines using GPT-4 via the OpenAI API.",
            "Step 6: Result Analysis": "Report the answer accuracy and error percentages for ISRP and the baselines on both GSM8K and SVAMP. Analyze how the metrics change with increasing refinement iterations N. Provide qualitative examples showing ISRP's ability to identify and correct different types of errors."
        },
        "Test Case Examples": {
            "Test Case 1": {
                "Question": "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?",
                "One-shot CoT": "Natalia sold clips to 48 of her friends in April.\nIn May, she sold half as many clips as in April.\nSo in May, Natalia sold 48/2 = 24 clips.\nIn total, Natalia sold 48 + 24 = 72 clips in April and May.",
                "ISRP Step 1: Initial Solution": "Natalia sold clips to 48 of her friends in April.\nIn May, she sold half as many clips as in April.\nSo in May, Natalia sold 48/2 = 24 clips.\nIn total, Natalia sold 48 + 24 = 72 clips in April and May.",
                "ISRP Step 2: Error Analysis": "The solution looks correct. There are no logical flaws or calculation errors.",
                "ISRP Step 3: Revised Solution": "Natalia sold clips to 48 of her friends in April.\nIn May, she sold half as many clips as in April.\nSo in May, Natalia sold 48/2 = 24 clips.\nIn total, Natalia sold 48 + 24 = 72 clips in April and May.",
                "Explanation": "The one-shot CoT solution is already correct, so ISRP does not make any changes. This shows that ISRP can identify correct solutions and keep them unchanged."
            },
            "Test Case 2": {
                "Question": "Michael has 58 golf balls. If he loses 23 balls, but then finds 14 of them, how many golf balls will he have?",
                "One-shot CoT": "Michael starts with 58 golf balls.\nHe loses 23 balls, so now he has 58 - 23 = 35 balls.\nThen he finds 14 of the balls he lost.\nSo he now has 35 + 14 = 39 golf balls.",
                "ISRP Step 1: Initial Solution": "Michael starts with 58 golf balls.\nHe loses 23 balls, so now he has 58 - 23 = 35 balls.\nThen he finds 14 of the balls he lost.\nSo he now has 35 + 14 = 39 golf balls.",
                "ISRP Step 2: Error Analysis": "The calculation 35 + 14 = 39 is incorrect. The correct calculation is 35 + 14 = 49.",
                "ISRP Step 3: Revised Solution": "Michael starts with 58 golf balls.\nHe loses 23 balls, so now he has 58 - 23 = 35 balls.\nThen he finds 14 of the balls he lost.\nSo he now has 35 + 14 = 49 golf balls.",
                "Explanation": "The one-shot CoT solution contains a calculation error in the final step. ISRP identifies this error and fixes it in the revised solution, demonstrating its ability to correct mistakes."
            }
        },
        "Fallback Plan": "If ISRP does not outperform the baselines, we can conduct additional analysis to understand why:\n1) Check if the error analysis step correctly identifies the majority of logical and calculation errors. If not, the revision step will not be able to fix those errors.\n2) Check if the revision step successfully fixes the identified errors. If the model struggles to make targeted revisions, the overall solution quality will not improve.\n3) Analyze how the error correction ability changes with the number of refinement iterations. It may be that too many iterations lead to deterioration.\nBased on these findings, we can propose improvements to the prompting strategies for error analysis and revision. If the results are still unsatisfactory, we can pivot to an analysis paper that examines the limitations of iterative refinement and provides insights into why the straightforward approach did not work, which can inform future work on this direction."
    }
}