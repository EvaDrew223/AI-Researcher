{
    "topic_description": "novel prompting methods for large language models to improve mathematical problem solving",
    "idea_name": "Intuition-First Prompting",
    "raw_idea": {
        "Problem": "Large language models can generate step-by-step solutions to math problems, but often lack the ability to provide intuitive explanations of the high-level approach before diving into the details.",
        "Existing Methods": "Existing methods like Chain-of-Thought prompting focus on generating detailed solution steps, but do not explicitly prompt for intuitive explanations of the overall approach.",
        "Motivation": "When humans approach complex math problems, they often start by developing a high-level intuition of how to solve the problem before working out the details. Prompting LLMs to first provide an intuitive explanation of their approach could lead to more strategic and reliable problem-solving.",
        "Proposed Method": "We introduce Intuition-First Prompting (IFP), a two-stage prompting method: 1) Given a math problem, prompt the LLM to generate an intuitive, high-level explanation of how to approach solving the problem, without going into step-by-step details. 2) Prompt the LLM to generate a detailed step-by-step solution, guided by the intuitive approach explained in the first stage.",
        "Experiment Plan": "Evaluate IFP on challenging math problem benchmarks like MATH and MathQA, and compare with baselines like Chain-of-Thought prompting. Conduct human evaluations to assess the quality and usefulness of the generated intuitive explanations in guiding the problem-solving process."
    },
    "full_experiment_plan": {
        "Title": "Intuition-First Prompting: Improving Mathematical Problem Solving in Large Language Models",
        "Problem Statement": "Large language models can generate step-by-step solutions to math problems, but often lack the ability to provide intuitive explanations of the high-level approach before diving into the details.",
        "Motivation": "Existing methods like Chain-of-Thought prompting focus on generating detailed solution steps, but do not explicitly prompt for intuitive explanations of the overall approach. When humans approach complex math problems, they often start by developing a high-level intuition of how to solve the problem before working out the details. Prompting LLMs to first provide an intuitive explanation of their approach could lead to more strategic and reliable problem-solving.",
        "Proposed Method": "We introduce Intuition-First Prompting (IFP), a two-stage prompting method: 1) Given a math problem, prompt the LLM to generate an intuitive, high-level explanation of how to approach solving the problem, without going into step-by-step details. 2) Prompt the LLM to generate a detailed step-by-step solution, guided by the intuitive approach explained in the first stage.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "Evaluate IFP on challenging math problem benchmarks like MATH and MathQA. These datasets contain a diverse set of mathematical problems that require strategic problem-solving and detailed solution steps.",
            "Step 2: Construct Prompts": "We include several baseline prompting methods: (1) direct prompting: prompt the model with the math problem only; (2) zero-shot CoT: append \"Let's solve this step by step:\" to the math problem; (3) few-shot CoT: include a few math problems with step-by-step solutions in the prompt.\nThen we implement our Intuition-First Prompting method. In the first stage, we prompt the model with: \"[Math Problem] Before diving into the step-by-step solution, let's first think about how to approach this problem at a high level. Explain the intuition and overall strategy for solving this problem, without going into the detailed steps:\". In the second stage, we prompt the model with: \"[Math Problem] [Intuitive Explanation from Stage 1] Now, let's solve this problem step by step, following the high-level approach explained above:\"",
            "Step 3: Select Models": "We test the following models from the OpenAI API: GPT-3.5 (text-davinci-003), GPT-4, and InstructGPT. We also test the open-source models LLaMA-7B and PaLM-8B.",
            "Step 4: Get Results": "For each math problem in the evaluation datasets, generate solutions using the baseline prompts and the Intuition-First Prompting method. Compute the percentage of problems solved correctly for each prompting method and model combination.",
            "Step 5: Analyze Results": "Compare the performance of Intuition-First Prompting with the baseline methods for each model. Conduct statistical significance tests to determine if the improvements from IFP are significant. Qualitatively analyze the generated intuitive explanations to assess their quality and usefulness in guiding the problem-solving process."
        },
        "Test Case Examples": {
            "Baseline Prompt Input (Direct Prompting)": "Mia has 42 apples. She gives some apples to her friend Sam. Mia now has 27 apples. How many apples did Mia give to Sam?",
            "Baseline Prompt Expected Output (Direct Prompting)": "Mia gave 15 apples to Sam.",
            "Baseline Prompt Input (Chain-of-Thought Prompting)": "Mia has 42 apples. She gives some apples to her friend Sam. Mia now has 27 apples. How many apples did Mia give to Sam? Let's solve this step by step:",
            "Baseline Prompt Expected Output (Chain-of-Thought Prompting)": "Okay, let's solve this step by step:\n1. Mia started with 42 apples.\n2. After giving some apples to Sam, Mia now has 27 apples.\n3. To find how many apples Mia gave to Sam, we need to subtract the number of apples Mia has now from the number of apples she started with.\n4. Number of apples Mia gave to Sam = Initial number of apples - Current number of apples\n5. Number of apples Mia gave to Sam = 42 - 27 = 15\nTherefore, Mia gave 15 apples to Sam.",
            "Proposed Prompt Input (Intuition-First Prompting; Stage 1: Intuitive Explanation)": "Mia has 42 apples. She gives some apples to her friend Sam. Mia now has 27 apples. How many apples did Mia give to Sam? Before diving into the step-by-step solution, let's first think about how to approach this problem at a high level. Explain the intuition and overall strategy for solving this problem, without going into the detailed steps:",
            "Proposed Prompt Expected Output (Intuition-First Prompting; Stage 1: Intuitive Explanation)": "To find out how many apples Mia gave to Sam, we need to think about the change in the number of apples Mia has. We know Mia's initial number of apples and her current number of apples after giving some to Sam. The key insight is that the difference between these two numbers will tell us the number of apples Mia gave away. So, the high-level approach is to subtract Mia's current number of apples from her initial number of apples.",
            "Proposed Prompt Input (Intuition-First Prompting; Stage 2: Step-by-Step Solution)": "Mia has 42 apples. She gives some apples to her friend Sam. Mia now has 27 apples. How many apples did Mia give to Sam? To find out how many apples Mia gave to Sam, we need to think about the change in the number of apples Mia has. We know Mia's initial number of apples and her current number of apples after giving some to Sam. The key insight is that the difference between these two numbers will tell us the number of apples Mia gave away. So, the high-level approach is to subtract Mia's current number of apples from her initial number of apples. Now, let's solve this problem step by step, following the high-level approach explained above:",
            "Proposed Prompt Expected Output (Intuition-First Prompting; Stage 2: Step-by-Step Solution)": "Step 1: Identify the initial number of apples Mia had.\nMia started with 42 apples.\n\nStep 2: Identify the number of apples Mia has after giving some to Sam.\nAfter giving some apples to Sam, Mia now has 27 apples.\n\nStep 3: Subtract the current number of apples from the initial number of apples.\nNumber of apples Mia gave to Sam = Initial apples - Current apples\nNumber of apples Mia gave to Sam = 42 - 27 = 15\n\nTherefore, Mia gave 15 apples to Sam.",
            "Explanation": "The Intuition-First Prompting method encourages the model to first think about the high-level approach to solving the problem before diving into the detailed steps. This leads to a more strategic and organized problem-solving process, as evidenced by the clear explanation of the intuition behind the solution. In contrast, the baseline methods either provide a direct answer without any explanation (direct prompting) or dive straight into the step-by-step solution without first providing a high-level overview (Chain-of-Thought prompting)."
        },
        "Fallback Plan": "If the Intuition-First Prompting method does not lead to significant improvements over the baseline methods, we can conduct further analysis to understand why. Some potential areas to investigate:\n1. Analyze the quality of the generated intuitive explanations. Are they relevant to the problem and do they provide useful insights for guiding the problem-solving process? If not, we may need to refine the prompts for generating intuitive explanations.\n2. Examine the relationship between the intuitive explanations and the step-by-step solutions. Do the step-by-step solutions follow the high-level approach outlined in the intuitive explanations? If not, we may need to modify the prompts to encourage stronger alignment between the two stages.\n3. Investigate the types of math problems where Intuition-First Prompting performs well or poorly. Are there certain problem characteristics that make it more challenging for the model to generate useful intuitive explanations? This analysis could inform the development of more targeted prompting strategies.\n4. Consider alternative prompting methods that incorporate elements of Intuition-First Prompting, such as iteratively generating and refining the intuitive explanations based on feedback from the step-by-step solutions.\nIf the Intuition-First Prompting method does not yield the desired results after these investigations, we can pivot the project to focus on analyzing the limitations of current prompting methods for mathematical problem solving and proposing potential avenues for future research."
    }
}