{
    "topic_description": "novel prompting methods for large language models to improve mathematical problem solving",
    "idea_name": "Symbolic Scaffolding Prompting",
    "raw_idea": {
        "Problem": "Large language models can generate the final answers to math problems, but often fail to generate complete and rigorous solution steps, especially for problems that require complex symbolic manipulation.",
        "Existing Methods": "Benchmarks like MathQA and MATH contain problems that require symbolic reasoning. Existing methods generate solution steps in natural language (e.g., chain-of-thought prompting) or programming languages (e.g., program-of-thoughts prompting), but lack explicit symbolic scaffolding.",
        "Motivation": "Symbolic mathematics involves manipulating mathematical expressions according to well-defined rules. Generating the complete symbolic solution steps requires know-how of both the high-level problem solving strategies and low-level rewrite rules. We hypothesize that explicitly prompting LLMs to generate symbolic steps with scaffolding from problem-solving strategies can lead to more complete and rigorous solution steps.",
        "Proposed Method": "We propose Symbolic Scaffolding Prompting (SSP) which first prompts LLMs to generate a high-level description of the solution approach (e.g., 'solve the equation for x by first isolating the variable on the LHS'). This is followed by prompting the LLM to fill in the actual symbolic steps guided by the high-level description. The generated steps are then executed by a symbolic math engine to get the final answer.",
        "Experiment Plan": "Evaluate SSP on math problem benchmarks that focus on symbolic manipulation, like MathQA and subsets of MATH. Compare with chain-of-thought and program-of-thoughts prompting in terms of the correctness and completeness of the generated solution steps. Perform qualitative analysis on how explicit scaffolding impacts the generated steps."
    },
    "full_experiment_plan": {
        "Title": "Symbolic Scaffolding Prompting: Improving Mathematical Problem Solving in Large Language Models",
        "Problem Statement": "Large language models can generate the final answers to math problems, but often fail to generate complete and rigorous solution steps, especially for problems that require complex symbolic manipulation.",
        "Motivation": "Existing methods like chain-of-thought prompting generate solution steps in natural language, while program-of-thoughts prompting generates steps in programming languages. However, they lack explicit symbolic scaffolding to guide the step-by-step mathematical reasoning. Symbolic mathematics involves manipulating mathematical expressions according to well-defined rules. Generating the complete symbolic solution steps requires know-how of both the high-level problem solving strategies and low-level rewrite rules. We hypothesize that explicitly prompting LLMs to generate symbolic steps with scaffolding from problem-solving strategies can lead to more complete and rigorous solution steps.",
        "Proposed Method": "We propose Symbolic Scaffolding Prompting (SSP) which first prompts LLMs to generate a high-level description of the solution approach (e.g., 'solve the equation for x by first isolating the variable on the LHS'). This is followed by prompting the LLM to fill in the actual symbolic steps guided by the high-level description. The generated steps are then executed by a symbolic math engine to get the final answer.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "Evaluate SSP on math problem benchmarks that focus on symbolic manipulation, like MathQA and subsets of MATH that require symbolic reasoning. Use answer accuracy as the main metric.",
            "Step 2: Construct Prompts": "We compare with two baselines: (1) direct prompting: only give the math question; (2) chain-of-thought (CoT) prompting: append 'Let's solve this step-by-step:' to the question. For SSP, in the first stage, prompt the model to generate a high-level solution sketch, e.g., 'To find the roots of the quadratic equation 2x^2 + 3x - 2 = 0: 1) Use the quadratic formula with a=2, b=3, c=-2. 2) Simplify the resulting expressions.' In the second stage, prompt the model to generate the actual symbolic steps by filling in the sketch, e.g., 'Using quadratic formula, x = [-b \u00b1 \u221a(b^2 - 4ac)] / 2a. Substituting a=2, b=3, c=-2, we get x = [-3 \u00b1 \u221a(3^2 - 4*2*(-2))] / (2*2) = [-3 \u00b1 \u221a(9 + 16)] / 4 = [-3 \u00b1 \u221a25] / 4 = [-3 \u00b1 5] / 4. So x = (-3 + 5) / 4 = 1/2 or x = (-3 - 5) / 4 = -2.'",
            "Step 3: Select Models": "We test both GPT-3.5 (text-davinci-003) and GPT-4 via the OpenAI API.",
            "Step 4: Get Results": "For each test example, first get the high-level solution sketch from the model, then get the symbolic steps by prompting the model to fill in the sketch. Finally, execute the generated steps with a symbolic math engine (e.g., SymPy) to get the final answer, and compare it with the ground-truth answer to compute the accuracy.",
            "Step 5: Analyze Results": "Compare the answer accuracy of SSP with the baselines. Also perform qualitative analysis on the generated high-level sketches and low-level steps to understand the impact of the explicit symbolic scaffolding."
        },
        "Test Case Examples": {
            "Baseline Prompt Input (Direct Prompting)": "Solve the equation e^(3x) = 5 for x.",
            "Baseline Prompt Expected Output (Direct Prompting)": "To solve for x, take the natural log of both sides of the equation: ln(e^(3x)) = ln(5). This simplifies to 3x = ln(5). Dividing both sides by 3 gives x = ln(5)/3.",
            "Baseline Prompt Input (Chain-of-Thought Prompting)": "Solve the equation e^(3x) = 5 for x. Let's solve this step-by-step:",
            "Baseline Prompt Expected Output (Chain-of-Thought Prompting)": "Given the equation e^(3x) = 5, let's solve for x step-by-step:\n1) Take the natural log of both sides: ln(e^(3x)) = ln(5)\n2) The left side simplifies: 3x * ln(e) = ln(5)\n3) Since ln(e) = 1, this simplifies to: 3x = ln(5)\n4) Divide both sides by 3: x = ln(5) / 3\nTherefore, the solution is x = ln(5) / 3.",
            "Proposed Prompt Input (SSP Stage 1: High-Level Sketch)": "Solve the equation e^(3x) = 5 for x. First describe at a high-level how you would symbolically solve for x step-by-step:",
            "Proposed Prompt Expected Output (SSP Stage 1: High-Level Sketch)": "To solve the equation e^(3x) = 5 for x:\n1) Take the natural logarithm of both sides of the equation.\n2) Use the logarithm property ln(e^x) = x to simplify the left side.\n3) Divide both sides by the coefficient of x to isolate x.",
            "Proposed Prompt Input (SSP Stage 2: Symbolic Steps)": "Now, follow the above step-by-step sketch to solve the equation e^(3x) = 5 for x. Show your work at each step:",
            "Proposed Prompt Expected Output (SSP Stage 2: Symbolic Steps)": "1) Take the natural logarithm of both sides of the equation:\nln(e^(3x)) = ln(5)\n2) Use the logarithm property ln(e^x) = x to simplify the left side:\n3x = ln(5)\n3) Divide both sides by the coefficient of x to isolate x:\n3x / 3 = ln(5) / 3\nx = ln(5) / 3\nTherefore, the solution is x = ln(5) / 3.",
            "Explanation": "SSP first generates a high-level solution sketch, providing a symbolic scaffolding for the step-by-step reasoning. It then fills in the actual symbolic steps following the sketch. The explicit scaffolding leads to more complete and rigorous steps compared to directly generating the steps or describing them in natural language."
        },
        "Fallback Plan": "If SSP does not outperform the baselines, we can: 1) Analyze the relevance and correctness of the generated high-level sketches. If they are of low quality, experiment with alternative sketch prompts or few-shot examples. 2) Analyze the faithfulness of the generated symbolic steps to the sketches. If the steps deviate from the sketches, experiment with more explicit prompts to make the model adhere to the sketch. 3) If the symbolic steps are still error-prone, experiment with prompting the model to self-check and self-correct the steps (e.g., by executing them). 4) If SSP still underperforms, do an error analysis to understand where and why it fails compared to the baselines, which can inform the design of improved methods or shed light on the limitations of prompting-based approaches for this task."
    }
}