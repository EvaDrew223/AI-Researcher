{
    "topic_description": "novel prompting methods for large language models to improve mathematical problem solving",
    "idea_name": "Multilingual Solution Prompting",
    "raw_idea": {
        "Problem": "Most existing work on mathematical problem-solving with language models has focused on English, limiting the accessibility and generalizability of the techniques.",
        "Existing Methods": "Some multilingual language models exist but have not been extensively evaluated on mathematical reasoning tasks across languages.",
        "Motivation": "Mathematical problem-solving skills should be language-agnostic. We hypothesize that prompting models to generate solutions in multiple languages could improve the robustness and transferability of the reasoning skills.",
        "Proposed Method": "We propose Multilingual Solution Prompting, where the model is instructed to: 1) Generate the solution to a mathematical problem in a source language. 2) Translate the problem and solution into one or more target languages. 3) Cross-verify the consistency of the solutions across languages and revise if inconsistencies are found. The prompts include examples in multiple languages and encourage the model to use the multilingual reasoning process for error detection and correction.",
        "Experiment Plan": "Evaluate Multilingual Solution Prompting on mathematical problems in multiple languages, such as the Multilingual ArithmeticWord Problem Solving dataset. Compare to monolingual baselines and assess the cross-lingual transferability of the reasoning skills. Analyze the effectiveness of using multilingual consistency for error correction."
    },
    "full_experiment_plan": {
        "Title": "Multilingual Solution Prompting: Improving Mathematical Reasoning in Language Models via Cross-Lingual Consistency",
        "Problem Statement": "Most existing work on mathematical problem-solving with language models has focused on English, limiting the accessibility and generalizability of the techniques. We aim to explore whether prompting models to generate solutions in multiple languages could improve the robustness and transferability of the reasoning skills.",
        "Motivation": "Mathematical problem-solving skills should be language-agnostic. While some multilingual language models exist, they have not been extensively evaluated on mathematical reasoning tasks across languages. We hypothesize that by prompting models to solve problems in multiple languages and cross-verify the consistency of the solutions, we can encourage the models to acquire more robust and transferable reasoning skills that are less dependent on the specific language used.",
        "Proposed Method": "We propose Multilingual Solution Prompting, where the model is instructed to: 1) Generate the solution to a mathematical problem in a source language. 2) Translate the problem and solution into one or more target languages. 3) Cross-verify the consistency of the solutions across languages and revise if inconsistencies are found. The prompts include examples in multiple languages and encourage the model to use the multilingual reasoning process for error detection and correction.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "Use the Multilingual ArithmeticWord Problem Solving (MAWPS) dataset which contains arithmetic word problems in English, French and Chinese. Split the dataset into train, validation and test sets for each language.",
            "Step 2: Construct Prompts": "Design three types of prompts:\n1) Monolingual Baseline Prompt: Prompt the model to directly generate the solution given a math problem in the source language.\n2) Multilingual Prompt: Prompt the model to 1) generate the solution in the source language, 2) translate the problem and solution into the target language(s), 3) cross-verify consistency and flag any inconsistencies.\n3) Multilingual Correction Prompt: If inconsistencies are flagged, prompt the model to revise the solution based on cross-lingual comparison until consistency is reached.\nInclude 3-5 few-shot exemplars in each prompt showcasing the desired behavior.",
            "Step 3: Select Models": "Evaluate on multilingual models like GPT-4 and BLOOM. For GPT-4, use the Chat API. For BLOOM, use the HuggingFace transformers library.",
            "Step 4: Get Results": "For each language in MAWPS:\n1) Evaluate the monolingual baseline by prompting the model to solve the test problems directly.\n2) Evaluate the multilingual prompting method:\n- Prompt the model with test problems using the Multilingual Prompt\n- Check flagged cases and apply the Multilingual Correction Prompt\n- Compute the final accuracy after corrections\n3) Compute the cross-lingual consistency rate: % of cases where solutions are equivalent across all languages before and after correction.",
            "Step 5: Analyze Results": "1) Compare the accuracy of monolingual baseline vs. multilingual prompting for each language to see if multilingual reasoning improves performance.\n2) Analyze how cross-lingual consistency rate changes before vs. after the correction step to assess the effectiveness of using cross-lingual consistency for error correction.\n3) Qualitatively analyze cases where multilingual prompting outperforms the monolingual baseline to gain insights."
        },
        "Test Case Examples": {
            "Test Case 1": {
                "Problem": "A school has 120 students. If each school bus can hold 30 students, how many buses are needed?",
                "Monolingual Baseline Prompt": "Q: A school has 120 students. If each school bus can hold 30 students, how many buses are needed?\nA:",
                "Monolingual Baseline Output": "4 buses are needed.",
                "Multilingual Prompt": "Solve the following math problem in English, translate it into French and Chinese, and check for consistency:\nQ: A school has 120 students. If each school bus can hold 30 students, how many buses are needed?\nA: ",
                "Multilingual Prompt Output": "English Solution: The school needs 120 \u00f7 30 = 4 buses.\nFrench Translation:\nQ: Une \u00e9cole a 120 \u00e9l\u00e8ves. Si chaque bus scolaire peut contenir 30 \u00e9l\u00e8ves, combien de bus sont n\u00e9cessaires ?\nR: L'\u00e9cole a besoin de 120 \u00f7 30 = 4 bus.\nChinese Translation:\nQ: \u4e00\u6240\u5b66\u6821\u6709120\u540d\u5b66\u751f\u3002\u5982\u679c\u6bcf\u8f86\u6821\u8f66\u53ef\u4ee5\u5bb9\u7eb330\u540d\u5b66\u751f\uff0c\u90a3\u4e48\u9700\u8981\u591a\u5c11\u8f86\u6821\u8f66\uff1f\nA: \u5b66\u6821\u9700\u8981 120 \u00f7 30 = 4 \u8f86\u6821\u8f66\u3002\nConsistency: The solutions are consistent across all three languages.",
                "Multilingual Correction Prompt": "N/A",
                "Multilingual Correction Output": "N/A",
                "Explanation": "The monolingual baseline and multilingual prompting both produce the correct solution. The multilingual prompt confirms the solutions are consistent across languages."
            },
            "Test Case 2": {
                "Problem": "A bakery sold 57 loaves of bread on Monday and 39 loaves on Tuesday. What was the average number of loaves sold per day?",
                "Monolingual Baseline Prompt": "Q: A bakery sold 57 loaves of bread on Monday and 39 loaves on Tuesday. What was the average number of loaves sold per day?\nA:",
                "Monolingual Baseline Output": "The average number of loaves sold per day is (57 + 39) / 2 = 48.",
                "Multilingual Prompt": "Solve the following math problem in English, translate it into French and Chinese, and check for consistency:\nQ: A bakery sold 57 loaves of bread on Monday and 39 loaves on Tuesday. What was the average number of loaves sold per day?\nA:",
                "Multilingual Prompt Output": "English Solution: The average number of loaves sold per day is (57 + 39) / 2 = 48 loaves.\nFrench Translation:\nQ: Une boulangerie a vendu 57 pains le lundi et 39 pains le mardi. Quel \u00e9tait le nombre moyen de pains vendus par jour ?\nR: Le nombre moyen de pains vendus par jour est (57 + 39) / 2 = 48 pains.\nChinese Translation:\nQ: \u4e00\u5bb6\u9762\u5305\u5e97\u5468\u4e00\u5356\u51fa57\u6761\u9762\u5305\uff0c\u5468\u4e8c\u5356\u51fa39\u6761\u9762\u5305\u3002\u5e73\u5747\u6bcf\u5929\u5356\u51fa\u591a\u5c11\u6761\u9762\u5305\uff1f\nA: \u5e73\u5747\u6bcf\u5929\u5356\u51fa\u7684\u9762\u5305\u6570\u91cf\u662f (57 + 39) / 2 = 98 \u6761\u9762\u5305\u3002\nConsistency: The English and French solutions are consistent, but the Chinese solution is incorrect.",
                "Multilingual Correction Prompt": "The Chinese solution is inconsistent with the English and French solutions. Revise the Chinese solution to match the correct answer of 48 loaves per day on average.",
                "Multilingual Correction Output": "Corrected Chinese Solution:\nA: \u5e73\u5747\u6bcf\u5929\u5356\u51fa\u7684\u9762\u5305\u6570\u91cf\u662f (57 + 39) / 2 = 48 \u6761\u9762\u5305\u3002\nConsistency: The solutions are now consistent across all three languages.",
                "Explanation": "The monolingual baseline produces the correct solution. The multilingual prompt detects an inconsistency in the Chinese solution, which is corrected after prompting for revision based on the English and French solutions."
            }
        },
        "Fallback Plan": "If the proposed multilingual prompting method does not outperform the monolingual baselines, we can:\n1) Analyze the types of errors made by the model in each language to see if there are language-specific challenges.\n2) Experiment with different prompt formats and exemplars to optimize the multilingual prompting strategy.\n3) Evaluate the quality of the translations generated by the model to ensure errors are not introduced in the translation step.\n4) Conduct few-shot learning experiments to see if providing more in-context examples improves the multilingual prompting performance.\n5) Collect human evaluations on the cross-lingual consistency and quality of the generated solutions to gain further insights.\nIf the results are still unsatisfactory, we can pivot the project to focus on analyzing the limitations and challenges of multilingual mathematical problem solving with language models, which can inform future work in this direction."
    }
}