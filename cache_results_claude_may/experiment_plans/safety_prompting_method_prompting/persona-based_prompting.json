{
    "topic_description": "novel prompting methods to improve large language models' robustness against adversarial attacks and improve their security and privacy",
    "idea_name": "Persona-Based Prompting",
    "raw_idea": {
        "Problem": "Language models are often susceptible to personalization attacks, where an adversary uses prompts to elicit outputs that reflect biases, opinions or private information of specific users. Protecting user privacy in prompting is an important yet challenging problem.",
        "Existing Methods": "Existing privacy defenses for language models often focus on the training data, e.g., using differential privacy during training. However, these methods don't address privacy leaks that can happen through prompting at inference time.",
        "Motivation": "We propose a novel prompting strategy that uses synthetic personas to protect user privacy. Instead of prompting the model with a user's real information, we create a fictitious persona that captures some high-level traits of the user (e.g., interests, writing style) but contains no sensitive details. The model is prompted to generate outputs that are consistent with this persona, creating a privacy-preserving proxy for personalization.",
        "Proposed Method": "Our Persona-Based Prompting system works as follows: 1) Persona Generation: When a user first interacts with the system, we generate a synthetic persona based on some high-level, non-sensitive user traits. This could be done by sampling from a pre-defined set of persona templates. 2) Persona Prompting: Whenever we need to generate a personalized output for the user, we prompt the model with the persona description instead of the user's real information. The model is asked to generate an output that is consistent with the persona's traits, but without referencing any actual user details. 3) Persona Updating: As the model interacts with the user, the persona can be gradually updated to better match their high-level traits, but still maintaining a layer of privacy abstraction. Sensitive details are never included in the persona.",
        "Experiment Plan": "We can evaluate Persona-Based Prompting on tasks that require personalization, like dialogue generation or recommendation. Privacy can be measured by the amount of user-specific details that are leaked in the model's outputs. Utility is measured by the relevance and quality of the personalized outputs."
    },
    "full_experiment_plan": {
        "Title": "Persona-Based Prompting: Protecting User Privacy in Personalized Language Model Interactions",
        "Problem Statement": "Language models are often susceptible to personalization attacks, where an adversary uses prompts to elicit outputs that reflect biases, opinions or private information of specific users. Protecting user privacy in prompting is an important yet challenging problem.",
        "Motivation": "Existing privacy defenses for language models often focus on the training data, e.g., using differential privacy during training. However, these methods don't address privacy leaks that can happen through prompting at inference time. We propose a novel prompting strategy that uses synthetic personas to protect user privacy. Instead of prompting the model with a user's real information, we create a fictitious persona that captures some high-level traits of the user (e.g., interests, writing style) but contains no sensitive details. The model is prompted to generate outputs that are consistent with this persona, creating a privacy-preserving proxy for personalization.",
        "Proposed Method": "Our Persona-Based Prompting system works as follows:\n1. Persona Generation: When a user first interacts with the system, we generate a synthetic persona based on some high-level, non-sensitive user traits. This could be done by sampling from a pre-defined set of persona templates.\n2. Persona Prompting: Whenever we need to generate a personalized output for the user, we prompt the model with the persona description instead of the user's real information. The model is asked to generate an output that is consistent with the persona's traits, but without referencing any actual user details.\n3. Persona Updating: As the model interacts with the user, the persona can be gradually updated to better match their high-level traits, but still maintaining a layer of privacy abstraction. Sensitive details are never included in the persona.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Dataset Selection": "We can evaluate Persona-Based Prompting on tasks that require personalization, like dialogue generation or recommendation. For dialogue, we can use the PersonaChat dataset which provides persona descriptions and dialogues. For recommendation, we can use the MovieLens dataset which has user ratings and profiles.",
            "Step 2: Persona Generation": "For each user in the dataset, we generate a synthetic persona. As a simple baseline, we can randomly sample a persona template and fill in the blanks with non-sensitive user traits (e.g., favorite movie genres). More advanced methods can use few-shot prompting to generate personas that match the user's writing style or interests without copying sensitive details.",
            "Step 3: Baseline Models": "We compare against two baselines: 1) No Persona: Prompting the model with just the input and no user information. 2) Real Persona: Prompting the model with the user's actual information (oracle privacy leakage).",
            "Step 4: Persona-Based Prompting": "For each input (e.g., dialogue history, movie query), we prompt the model with the synthetic persona instead of real user details. The prompt format can be like: \"[Persona Description] [Input] [Response]\". We use GPT-3.5 and GPT-4 as the base models.",
            "Step 5: Evaluation Metrics": "We measure performance on the downstream task (e.g., dialogue coherence, recommendation relevance) using both automated metrics and human evaluation. For privacy, we measure the amount of user-specific details that are leaked in the model's outputs, using both exact match and semantic similarity. The ideal model should perform well on the task while leaking minimal private information.",
            "Step 6: Persona Updating Analysis": "As an extension, we can analyze the effect of gradually updating the persona over multiple interactions. We measure how the performance and privacy trade-off changes as the persona becomes more tailored to the user. The key is to improve relevance without compromising privacy."
        },
        "Test Case Examples": {
            "Test Case 1": {
                "User Profile": "Name: John, Age: 25, Gender: Male, Occupation: Engineer, Favorite Movie: The Matrix, Favorite Book: 1984 by George Orwell, Hobbies: Playing guitar and hiking, Political Views: Libertarian",
                "Input": "What kind of movies would you recommend to me?",
                "Baseline 1 - No Persona": "Here are some popular movie recommendations: 1) The Shawshank Redemption, 2) The Godfather, 3) The Dark Knight, 4) Forrest Gump, 5) Inception",
                "Baseline 2 - Real Persona": "Given that your favorite movie is The Matrix and you enjoy dystopian fiction like 1984, I would recommend these movies: 1) Blade Runner (1982), 2) Inception (2010), 3) Ex Machina (2014), 4) Minority Report (2002), 5) The Terminator (1984). These movies feature similar themes of questioning reality, dystopian futures, and the relationship between humans and technology. Let me know if you want more recommendations tailored to your interests!",
                "Persona-Based Prompting": "Persona: Avid science fiction fan who enjoys mind-bending films with philosophical themes. Has a penchant for dystopian stories that explore the impact of technology on society.\nRecommendation: Based on your interest in thought-provoking science fiction, here are some movies you might enjoy: 1) Blade Runner 2049 (2017) - A visually stunning sequel that expands upon the themes of identity and humanity from the original. 2) Arrival (2016) - A unique take on the alien contact genre that delves into linguistics and the nature of time. 3) The Truman Show (1998) - A prescient satire about the perils of living in a constructed reality, with a powerful performance by Jim Carrey. 4) Gattaca (1997) - A genetic engineering dystopia that questions the limits of human potential and the ethics of a society ruled by eugenics. 5) Black Mirror (2011-2019) - While not a movie, this anthology series offers a range of dark and thought-provoking tales about the unintended consequences of technology. Each episode is a standalone story, so you can watch them in any order. Let me know if any of these pique your interest, and I can provide more details or further recommendations in this vein!",
                "Explanation": "The No Persona baseline gives generic popular recommendations that are not personalized. The Real Persona baseline gives highly personalized recommendations but directly copies private information like name, age, and political views. The Persona-Based prompt captures the user's key interests in sci-fi and dystopian themes without mentioning any specific private details. The recommendations are relevant but the explanations avoid referencing the user's personal information directly."
            },
            "Test Case 2": {
                "User Profile": "Name: Sarah, Age: 42, Gender: Female, Marital Status: Divorced, Children: 2 (ages 8 and 12), Occupation: High school teacher, Favorite Books: Pride and Prejudice, The Great Gatsby, To Kill a Mockingbird, Hobbies: Yoga, gardening, volunteering at local animal shelter, Favorite Music: 90s alternative rock",
                "Input": "I'm looking for some book recommendations for the summer. What do you suggest?",
                "Baseline 1 - No Persona": "Here are some popular summer reads: 1) The Girl on the Train by Paula Hawkins, 2) Gone Girl by Gillian Flynn, 3) The Fault in Our Stars by John Green, 4) The Nightingale by Kristin Hannah, 5) The Goldfinch by Donna Tartt",
                "Baseline 2 - Real Persona": "Hi Sarah, given your love for classic literature like Pride and Prejudice, The Great Gatsby, and To Kill a Mockingbird, I think you would enjoy these books for your summer reading: 1) Sense and Sensibility by Jane Austen - Another witty and romantic tale from Austen, perfect for a high school English teacher like yourself. 2) The Catcher in the Rye by J.D. Salinger - A modern classic coming-of-age story that your kids might read in a few years. 3) The Color Purple by Alice Walker - A powerful and poignant novel about the struggles of African American women in the early 20th century. 4) The Bell Jar by Sylvia Plath - A semi-autobiographical novel about a young woman's battle with depression, set in the 1950s. 5) The Handmaid's Tale by Margaret Atwood - A dystopian novel that explores themes of gender roles and women's rights, which might resonate with you as a divorced mother. Let me know if you want more suggestions, and happy reading this summer!",
                "Persona-Based Prompting": "Persona: An avid reader who appreciates classic literature, particularly novels that explore deep themes and complex characters. Enjoys stories that provide social commentary and spark meaningful discussions.\nRecommendation: If you're looking for thought-provoking summer reads, here are a few suggestions: 1) East of Eden by John Steinbeck - An epic family saga that explores the nature of good and evil, set in California's Salinas Valley. Steinbeck's vivid prose and memorable characters make this a compelling read. 2) The Poisonwood Bible by Barbara Kingsolver - A powerful novel about a missionary family in the Congo, told from the perspectives of the wife and daughters. It delves into themes of cultural imperialism, religion, and the role of women. 3) The Kite Runner by Khaled Hosseini - A heartbreaking story of friendship, betrayal, and redemption set against the backdrop of Afghanistan's tumultuous history. 4) The Immortal Life of Henrietta Lacks by Rebecca Skloot - A fascinating blend of science, history, and biography that explores issues of race, ethics, and the human story behind one of the most important cell lines in medical research. 5) The Road by Cormac McCarthy - A haunting post-apocalyptic novel about a father and son's journey across a desolate landscape. McCarthy's sparse prose and the story's emotional depth make it a memorable read. These books offer a mix of classic and contemporary literature that I think will provide rich material for reflection and discussion. Feel free to let me know if you'd like more recommendations based on your thoughts on these!",
                "Explanation": "Again, the No Persona baseline gives generic recommendations that are not tailored to the user's interests. The Real Persona baseline is highly personalized but reveals sensitive details like marital status, children's ages, and occupation. The Persona-Based prompt focuses on the user's literary tastes and desire for meaningful stories, without mentioning any private facts. The recommendations are well-suited to the persona's preferences and the explanations highlight thematic elements rather than personal details."
            }
        },
        "Fallback Plan": "If the Persona-Based Prompting method does not achieve a good balance between personalization and privacy, we can explore a few alternative approaches:\n1. Analyze the persona generation process: Are the synthetic personas sufficiently capturing user traits to enable relevant recommendations? Can we improve the persona templates or few-shot prompting to create more representative but still privacy-safe personas?\n2. Fine-tune the language model: Instead of using zero-shot prompting, we can fine-tune the model on a dataset of persona-based prompts and outputs. This may help the model better understand the desired behavior of generating relevant outputs while avoiding sensitive details.\n3. Incorporate privacy filters: We can add a post-processing step that filters out any potential sensitive details that may still leak through the persona-based prompts. This could involve named entity recognition to remove specific names, locations, etc.\n4. Conduct user studies: To better understand the subjective trade-off between personalization and privacy, we can conduct user studies where participants interact with different versions of the system (no persona, real persona, synthetic persona) and provide feedback on the relevance of the outputs and their perceived level of privacy.\nBy exploring these fallback options, we can still gain valuable insights into the challenges of privacy-preserving personalization, even if the initial persona-based prompting approach does not fully succeed. The project can then pivot to focus more on analyzing the limitations of current methods and proposing potential solutions for future work."
    },
    "novelty_queries": [
        "KeywordQuery(\"persona-based prompting language models\")",
        "KeywordQuery(\"privacy-preserving personalization language models\")",
        "KeywordQuery(\"synthetic persona generation language models\")",
        "KeywordQuery(\"privacy leakage prompting language models\")",
        "KeywordQuery(\"Persona-Based Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "e25b9330e01b667e784003373d1b9cdbd3521c7e",
            "paperId": "e25b9330e01b667e784003373d1b9cdbd3521c7e",
            "title": "Privacy Preserving Large Language Models: ChatGPT Case Study Based Vision and Framework",
            "abstract": "The generative Artificial Intelligence (AI) tools based on Large Language Models (LLMs) use billions of parameters to extensively analyse large datasets and extract critical private information such as, context, specific details, identifying information etc. This have raised serious threats to user privacy and reluctance to use such tools. This article proposes the conceptual model called PrivChatGPT, a privacy-preserving model for LLMs that consists of two main components i.e., preserving user privacy during the data curation/pre-processing together with preserving private context and the private training process for large-scale data. To demonstrate its applicability, we show how a private mechanism could be integrated into the existing model for training LLMs to protect user privacy; specifically, we employed differential privacy and private training using Reinforcement Learning (RL). We measure the privacy loss and evaluate the measure of uncertainty or randomness once differential privacy is applied. It further recursively evaluates the level of privacy guarantees and the measure of uncertainty of public database and resources, during each update when new information is added for training purposes. To critically evaluate the use of differential privacy for private LLMs, we hypothetically compared other mechanisms e..g, Blockchain, private information retrieval, randomisation, for various performance measures such as the model performance and accuracy, computational complexity, privacy vs. utility etc. We conclude that differential privacy, randomisation, and obfuscation can impact utility and performance of trained models, conversely, the use of ToR, Blockchain, and PIR may introduce additional computational complexity and high training latency. We believe that the proposed model could be used as a benchmark for proposing privacy preserving LLMs for generative AI tools.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The conceptual model called PrivChatGPT, a privacy-preserving model for LLMs that consists of two main components i.e., preserving user privacy during the data curation/pre-processing together with preserving private context and the private training process for large-scale data is proposed."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is protecting user privacy in personalized language model interactions, and the approach is to use synthetic personas instead of real user information for prompting. The research problem in the paper is preserving user privacy in large language models like ChatGPT, and the approach is to use techniques like differential privacy and private training using reinforcement learning.\n\nThe proposal focuses on privacy during prompting and inference, while the paper focuses on privacy during data curation, pre-processing, and training. The proposal uses synthetic personas for prompting, while the paper uses differential privacy and other techniques during training.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "afaad5ebbf8f037ada4d068eb2e804be08abeccb",
            "paperId": "afaad5ebbf8f037ada4d068eb2e804be08abeccb",
            "title": "Privacy-Preserving Large Language Models (PPLLMs)",
            "abstract": null,
            "year": 2023,
            "citationCount": 5,
            "tldr": null,
            "score": 7,
            "novelty_score": "The project proposal aims to protect user privacy in personalized language model interactions by using synthetic personas for prompting, instead of the user's real information.\n\nThe paper abstract is not provided, so there is not enough information to determine if the research problem and approach match the project proposal.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "5322e5936e4a46195b1a92001467a2350fe72782",
            "paperId": "5322e5936e4a46195b1a92001467a2350fe72782",
            "title": "KART: Privacy Leakage Framework of Language Models Pre-trained with Clinical Records",
            "abstract": "Nowadays, mainstream natural language processing (NLP) is empowered by pre-trained language models. In the biomedical domain, only models pre-trained with anonymized data have been published. This policy is acceptable, but there are two questions: Can the privacy policy of language models be different from that of data? What happens if private language models are accidentally made public? We empirically evaluated the privacy risk of language models, using several BERT models pre-trained with MIMIC-III corpus in different data anonymity and corpus sizes. We simulated model inversion attacks to obtain the clinical information of target individuals, whose full names are already known to attackers. The BERT models were probably low-risk because the Top-100 accuracy of each attack was far below expected by chance. Moreover, most privacy leakage situations have several common primary factors; therefore, we formalized various privacy leakage scenarios under a universal novel framework named Knowledge, Anonymization, Resource, and Target (KART) framework. The KART framework helps parameterize complex privacy leakage scenarios and simpli\ufb01es the comprehensive evaluation. Since the concept of the KART framework is domain agnostic, it can contribute to the establishment of privacy guidelines of language models beyond the biomedical domain.",
            "year": 2021,
            "citationCount": 20,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Various privacy leakage scenarios are formalized under a universal novel framework named Knowledge, Anonymization, Resource, and Target (KART) framework, which helps parameterize complex privacy leakage scenarios and simplifies the comprehensive evaluation."
            },
            "score": 7,
            "novelty_score": "The paper proposes a framework for evaluating privacy leakage in language models pre-trained on clinical records, while the project proposes a method for protecting user privacy during personalized language model interactions using synthetic personas.\n\nThe paper focuses on assessing the privacy risks of biomedical language models if they are made public, whereas the project aims to develop a privacy-preserving prompting strategy for personalized language model outputs.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "a86d3692af34ce70fd8caaa319009f85a88fdb4d",
            "paperId": "a86d3692af34ce70fd8caaa319009f85a88fdb4d",
            "title": "Faithful Persona-based Conversational Dataset Generation with Large Language Models",
            "abstract": "High-quality conversational datasets are essential for developing AI models that can communicate with users. One way to foster deeper interactions between a chatbot and its user is through personas, aspects of the user's character that provide insights into their personality, motivations, and behaviors. Training Natural Language Processing (NLP) models on a diverse and comprehensive persona-based dataset can lead to conversational models that create a deeper connection with the user, and maintain their engagement. In this paper, we leverage the power of Large Language Models (LLMs) to create a large, high-quality conversational dataset from a seed dataset. We propose a Generator-Critic architecture framework to expand the initial dataset, while improving the quality of its conversations. The Generator is an LLM prompted to output conversations. The Critic consists of a mixture of expert LLMs that control the quality of the generated conversations. These experts select the best generated conversations, which we then use to improve the Generator. We release Synthetic-Persona-Chat, consisting of 20k conversations seeded from Persona-Chat. We evaluate the quality of Synthetic-Persona-Chat and our generation framework on different dimensions through extensive experiments, and observe that the losing rate of Synthetic-Persona-Chat against Persona-Chat during Turing test decreases from 17.2% to 8.8% over three iterations.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper uses the power of Large Language Models (LLMs) to create a large, high-quality conversational dataset from a seed dataset, and proposes a Generator-Critic architecture framework to expand the initial dataset, while improving the quality of its conversations."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is protecting user privacy in personalized language model interactions, and the approach is to use synthetic personas instead of real user information for prompting.\n\nThe research problem in the paper is creating a large, high-quality persona-based conversational dataset, and the approach is to use a Generator-Critic framework with large language models to expand and improve an initial dataset.\n\nWhile both involve personas, the proposal focuses on privacy and the paper on dataset generation. The methods are also different - prompting with synthetic personas vs. a Generator-Critic framework.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "6e41a4cbb34c4d403efb73d74f5be5556b1f13d6",
            "paperId": "6e41a4cbb34c4d403efb73d74f5be5556b1f13d6",
            "title": "Privacy-Preserving In-Context Learning for Large Language Models",
            "abstract": "In-context learning (ICL) is an important capability of Large Language Models (LLMs), enabling these models to dynamically adapt based on specific, in-context exemplars, thereby improving accuracy and relevance. However, LLM's responses may leak the sensitive private information contained in in-context exemplars. To address this challenge, we propose Differentially Private In-context Learning (DP-ICL), a general paradigm for privatizing ICL tasks. The key idea for DP-ICL paradigm is generating differentially private responses through a noisy consensus among an ensemble of LLM's responses based on disjoint exemplar sets. Based on the general paradigm of DP-ICL, we instantiate several techniques showing how to privatize ICL for text classification and language generation. We evaluate DP-ICL on four text classification benchmarks and two language generation tasks, and our empirical results show that DP-ICL achieves a strong utility-privacy tradeoff.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The key idea for DP-ICL paradigm is generating differentially private responses through a noisy consensus among an ensemble of LLM's responses based on disjoint exemplar sets, which achieves a strong utility-privacy tradeoff."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is protecting user privacy in personalized language model interactions, and the proposed approach is to use synthetic personas instead of real user information for prompting.\n\nThe research problem in the paper is preventing language models from leaking sensitive information in the in-context exemplars during in-context learning, and the proposed approach is to generate differentially private responses through a noisy consensus among an ensemble of language models.\n\nWhile both works aim to improve privacy in language model interactions, the proposal focuses on protecting user information during personalized prompting, while the paper focuses on preventing leakage of sensitive exemplars during in-context learning. The proposed methods are also different: persona-based prompting vs. differentially private ensemble consensus.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "85e51f70d0a48ab87b8df0eee3ef55c93e65b8ce",
            "paperId": "85e51f70d0a48ab87b8df0eee3ef55c93e65b8ce",
            "title": "Privacy-Preserving Recommender Systems with Synthetic Query Generation using Differentially Private Large Language Models",
            "abstract": "We propose a novel approach for developing privacy-preserving large-scale recommender systems using differentially private (DP) large language models (LLMs) which overcomes certain challenges and limitations in DP training these complex systems. Our method is particularly well suited for the emerging area of LLM-based recommender systems, but can be readily employed for any recommender systems that process representations of natural language inputs. Our approach involves using DP training methods to fine-tune a publicly pre-trained LLM on a query generation task. The resulting model can generate private synthetic queries representative of the original queries which can be freely shared for any downstream non-private recommendation training procedures without incurring any additional privacy cost. We evaluate our method on its ability to securely train effective deep retrieval models, and we observe significant improvements in their retrieval quality without compromising query-level privacy guarantees compared to methods where the retrieval models are directly DP trained.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 6,
            "novelty_score": "The project proposal aims to protect user privacy in personalized language model interactions by using synthetic personas for prompting, instead of the user's real information. The paper proposes using differentially private language models to generate synthetic queries for training recommender systems, without compromising user privacy.\n\nWhile both works focus on the high-level goal of preserving user privacy, the project proposal targets privacy during inference-time prompting of language models, whereas the paper addresses privacy in training data for recommender systems. The methods proposed are also different: persona-based prompting vs. differentially private query generation.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "82d226701ab9c829345809e95d3ec6b0b0791727",
            "paperId": "82d226701ab9c829345809e95d3ec6b0b0791727",
            "title": "Synthetic Query Generation for Privacy-Preserving Deep Retrieval Systems using Differentially Private Language Models",
            "abstract": "We address the challenge of ensuring differential privacy (DP) guarantees in training deep retrieval systems. Training these systems often involves the use of contrastive-style losses, which are typically non-per-example decomposable, making them difficult to directly DP-train with since common techniques require per-example gradient. To address this issue, we propose an approach that prioritizes ensuring query privacy prior to training a deep retrieval system. Our method employs DP language models (LMs) to generate private synthetic queries representative of the original data. These synthetic queries can be used in downstream retrieval system training without compromising privacy. Our approach demonstrates a significant enhancement in retrieval quality compared to direct DP-training, all while maintaining query-level privacy guarantees. This work highlights the potential of harnessing LMs to overcome limitations in standard DP-training methods.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work employs DP language models (LMs) to generate private synthetic queries representative of the original data to demonstrate a significant enhancement in retrieval quality compared to direct DP-training, all while maintaining query-level privacy guarantees."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is protecting user privacy in personalized language model interactions, and the proposed approach is to use synthetic personas for prompting instead of real user information.\n\nThe research problem in the paper is ensuring differential privacy guarantees in training deep retrieval systems, and the proposed approach is to use differentially private language models to generate synthetic queries for training.\n\nWhile both works aim to protect privacy, the proposal focuses on privacy during prompting/inference whereas the paper focuses on privacy during training. The proposal uses synthetic personas whereas the paper uses synthetic queries.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "553d85e202fdbdd4101673b9205135b8eb94811d",
            "paperId": "553d85e202fdbdd4101673b9205135b8eb94811d",
            "title": "Recovering from Privacy-Preserving Masking with Large Language Models",
            "abstract": "Model adaptation is crucial to handle the discrepancy between proxy training data and actual users data received. To effectively perform adaptation, textual data of users is typically stored on servers or their local devices, where downstream natural language processing (NLP) models can be directly trained using such in-domain data. However, this might raise privacy and security concerns due to the extra risks of exposing user information to adversaries. Replacing identifying information in textual data with a generic marker has been recently explored. In this work, we leverage large language models (LLMs) to suggest substitutes of masked tokens and have their effectiveness evaluated on downstream language modeling tasks. Specifically, we propose multiple pre-trained and fine-tuned LLM-based approaches and perform empirical studies on various datasets for the comparison of these methods. Experimental results show that models trained on the obfuscation corpora are able to achieve comparable performance with the ones trained on the original data without privacy-preserving token masking.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results show that modelstrained on the obfuscation corpora are able to achieve comparable performance with the ones trained on the original data without privacy-preserving token masking, and have their effectiveness evaluated on downstream language modeling tasks."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to protect user privacy in personalized language model interactions by using synthetic personas for prompting, instead of the user's real information. The paper focuses on recovering from privacy-preserving masking in textual data using large language models to suggest substitutes for masked tokens.\n\nThe project proposal is about persona-based prompting for privacy-preserving personalization. The paper is about using large language models to recover masked tokens in privacy-preserved text data.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "597cad6c7b9de94eecc153c7cdcaf824905fe915",
            "paperId": "597cad6c7b9de94eecc153c7cdcaf824905fe915",
            "title": "You Are What You Write: Preserving Privacy in the Era of Large Language Models",
            "abstract": "Large scale adoption of large language models has introduced a new era of convenient knowledge transfer for a slew of natural language processing tasks. However, these models also run the risk of undermining user trust by exposing unwanted information about the data subjects, which may be extracted by a malicious party, e.g. through adversarial attacks. We present an empirical investigation into the extent of the personal information encoded into pre-trained representations by a range of popular models, and we show a positive correlation between the complexity of a model, the amount of data used in pre-training, and data leakage. In this paper, we present the first wide coverage evaluation and comparison of some of the most popular privacy-preserving algorithms, on a large, multi-lingual dataset on sentiment analysis annotated with demographic information (location, age and gender). The results show since larger and more complex models are more prone to leaking private information, use of privacy-preserving methods is highly desirable. We also find that highly privacy-preserving technologies like differential privacy (DP) can have serious model utility effects, which can be ameliorated using hybrid or metric-DP techniques.",
            "year": 2022,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents the first wide coverage evaluation and comparison of some of the most popular privacy-preserving algorithms, on a large, multi-lingual dataset on sentiment analysis annotated with demographic information (location, age and gender)."
            },
            "score": 6,
            "novelty_score": "The project proposal focuses on protecting user privacy during personalized language model interactions by using synthetic personas for prompting, instead of the user's real information. The paper investigates the extent of personal information leakage in pre-trained language models and compares various privacy-preserving algorithms on a sentiment analysis dataset with demographic annotations.\n\nWhile both works address the issue of privacy in language models, the project proposal aims to prevent privacy leaks during prompting and generation, whereas the paper studies information leakage in pre-trained representations and evaluates existing privacy-preserving methods. The proposed approaches are different: persona-based prompting vs. algorithms like differential privacy.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "db95150f29a37d8736037af7506ca7c63331e097",
            "paperId": "db95150f29a37d8736037af7506ca7c63331e097",
            "title": "Knowledge Sanitization of Large Language Models",
            "abstract": "We explore a knowledge sanitization approach to mitigate the privacy concerns associated with large language models (LLMs). LLMs trained on a large corpus of Web data can memorize and potentially reveal sensitive or confidential information, raising critical security concerns. Our technique efficiently fine-tunes these models using the Low-Rank Adaptation (LoRA) method, prompting them to generate harmless responses such as ``I don't know'' when queried about specific information. Experimental results in a closed-book question-answering task show that our straightforward method not only minimizes particular knowledge leakage but also preserves the overall performance of LLMs. These two advantages strengthen the defense against extraction attacks and reduces the emission of harmful content such as hallucinations.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A knowledge sanitization approach to mitigate the privacy concerns associated with large language models (LLMs) by efficiently fine-tunes these models using the Low-Rank Adaptation (LoRA) method, prompting them to generate harmless responses such as ``I don't know'' when queried about specific information."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is protecting user privacy in personalized language model interactions, and the proposed approach is to use synthetic personas instead of real user information for prompting.\n\nThe research problem in the paper is mitigating privacy concerns of large language models memorizing sensitive information, and the proposed approach is to fine-tune the models to generate harmless responses when queried about specific information.\n\nWhile both works aim to address privacy issues in language models, the proposal focuses on protecting user information during inference-time prompting, while the paper focuses on preventing the model from leaking sensitive information it memorized during training. The proposed approaches are also different: persona-based prompting vs. fine-tuning the model to generate harmless responses.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "89512c767e0ca0fe64d12a436c64f15dffdad1e0",
            "paperId": "89512c767e0ca0fe64d12a436c64f15dffdad1e0",
            "title": "Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory",
            "abstract": "The interactive use of large language models (LLMs) in AI assistants (at work, home, etc.) introduces a new set of inference-time privacy risks: LLMs are fed different types of information from multiple sources in their inputs and are expected to reason about what to share in their outputs, for what purpose and with whom, within a given context. In this work, we draw attention to the highly critical yet overlooked notion of contextual privacy by proposing ConfAIde, a benchmark designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs. Our experiments show that even the most capable models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively. This leakage persists even when we employ privacy-inducing prompts or chain-of-thought reasoning. Our work underscores the immediate need to explore novel inference-time privacy-preserving approaches, based on reasoning and theory of mind.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes ConfAIde, a benchmark designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs, and underscores the immediate need to explore novel inference-time privacy-preserving approaches, based on reasoning and theory of mind."
            },
            "score": 6
        },
        {
            "id": "e9ca67b67f4b43650baaef0d03013683eeb4528e",
            "paperId": "e9ca67b67f4b43650baaef0d03013683eeb4528e",
            "title": "Large Language Models Can Be Good Privacy Protection Learners",
            "abstract": "The proliferation of Large Language Models (LLMs) has driven considerable interest in fine-tuning them with domain-specific data to create specialized language models. Nevertheless, such domain-specific fine-tuning data often contains sensitive personally identifiable information (PII). Direct fine-tuning LLMs on this data without privacy protection poses a risk of leakage. To address this challenge, we introduce Privacy Protection Language Models (PPLM), a novel paradigm for fine-tuning LLMs that effectively injects domain-specific knowledge while safeguarding data privacy. Our work offers a theoretical analysis for model design and delves into various techniques such as corpus curation, penalty-based unlikelihood in training loss, and instruction-based tuning, etc. Extensive experiments across diverse datasets and scenarios demonstrate the effectiveness of our approaches. In particular, instruction tuning with both positive and negative examples, stands out as a promising method, effectively protecting private data while enhancing the model's knowledge. Our work underscores the potential for Large Language Models as robust privacy protection learners.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Privacy Protection Language Models (PPLM) is introduced, a novel paradigm for fine-tuning LLMs that effectively injects domain-specific knowledge while safeguarding data privacy and underscores the potential for Large Language Models as robust privacy protection learners."
            },
            "score": 6
        },
        {
            "id": "13c85adfa950651ffcd91ef3018fa30801b74472",
            "paperId": "13c85adfa950651ffcd91ef3018fa30801b74472",
            "title": "Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration",
            "abstract": "Conversational systems based on Large Language Models (LLMs), such as ChatGPT, show exceptional proficiency in context understanding and response generation. However, despite their impressive capabilities, they still possess limitations, such as providing randomly-guessed answers to ambiguous queries or failing to refuse users' requests, both of which are considered aspects of a conversational agent's proactivity. This raises the question of whether LLM-based conversational systems are equipped to handle proactive dialogue problems. In this work, we conduct a comprehensive analysis of LLM-based conversational systems, specifically focusing on three aspects of proactive dialogue systems: clarification, target-guided, and non-collaborative dialogues. To trigger the proactivity of LLMs, we propose the Proactive Chain-of-Thought prompting scheme, which augments LLMs with the goal planning capability over descriptive reasoning chains. Empirical findings are discussed to promote future studies on LLM-based proactive dialogue systems.",
            "year": 2023,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A comprehensive analysis of LLM-based conversational systems, specifically focusing on three aspects of proactive dialogue systems: clarification, target-guided, and non-collaborative dialogues, and the Proactive Chain-of-Thought prompting scheme is proposed."
            },
            "score": 5
        },
        {
            "id": "5661b4387a62b31bc02e4566330e71522b4397ab",
            "paperId": "5661b4387a62b31bc02e4566330e71522b4397ab",
            "title": "Privacy-preserving Fine-tuning of Large Language Models through Flatness",
            "abstract": "The privacy concerns associated with the use of Large Language Models (LLMs) have grown recently with the development of LLMs such as ChatGPT. Differential Privacy (DP) techniques are explored in existing work to mitigate their privacy risks at the cost of generalization degradation. Our paper reveals that the flatness of DP-trained models' loss landscape plays an essential role in the trade-off between their privacy and generalization. We further propose a holistic framework to enforce appropriate weight flatness, which substantially improves model generalization with competitive privacy preservation. It innovates from three coarse-to-grained levels, including perturbation-aware min-max optimization on model weights within a layer, flatness-guided sparse prefix-tuning on weights across layers, and weight knowledge distillation between DP \\&non-DP weights copies. Comprehensive experiments of both black-box and white-box scenarios are conducted to demonstrate the effectiveness of our proposal in enhancing generalization and maintaining DP characteristics. For instance, on text classification dataset QNLI, DP-Flat achieves similar performance with non-private full fine-tuning but with DP guarantee under privacy budget $\\epsilon=3$, and even better performance given higher privacy budgets. Codes are provided in the supplement.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper reveals that the flatness of DP-trained models' loss landscape plays an essential role in the trade-off between their privacy and generalization, and proposes a holistic framework to enforce appropriate weight flatness, which substantially improves model generalization with competitive privacy preservation."
            },
            "score": 5
        },
        {
            "id": "bfc0e3e651cd4b715272fe68add8a180a112293c",
            "paperId": "bfc0e3e651cd4b715272fe68add8a180a112293c",
            "title": "Scalable and Transferable Black-Box Jailbreaks for Language Models via Persona Modulation",
            "abstract": "Despite efforts to align large language models to produce harmless responses, they are still vulnerable to jailbreak prompts that elicit unrestricted behaviour. In this work, we investigate persona modulation as a black-box jailbreaking method to steer a target model to take on personalities that are willing to comply with harmful instructions. Rather than manually crafting prompts for each persona, we automate the generation of jailbreaks using a language model assistant. We demonstrate a range of harmful completions made possible by persona modulation, including detailed instructions for synthesising methamphetamine, building a bomb, and laundering money. These automated attacks achieve a harmful completion rate of 42.5% in GPT-4, which is 185 times larger than before modulation (0.23%). These prompts also transfer to Claude 2 and Vicuna with harmful completion rates of 61.0% and 35.9%, respectively. Our work reveals yet another vulnerability in commercial large language models and highlights the need for more comprehensive safeguards.",
            "year": 2023,
            "citationCount": 35,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates persona modulation as a black-box jailbreaking method to steer a target model to take on personalities that are willing to comply with harmful instructions and demonstrates a range of harmful completions made possible by persona modulation."
            },
            "score": 5
        },
        {
            "id": "b2c68b708a9f98996b18c8d21b53a815a2c46a8b",
            "paperId": "b2c68b708a9f98996b18c8d21b53a815a2c46a8b",
            "title": "ProPILE: Probing Privacy Leakage in Large Language Models",
            "abstract": "The rapid advancement and widespread use of large language models (LLMs) have raised significant concerns regarding the potential leakage of personally identifiable information (PII). These models are often trained on vast quantities of web-collected data, which may inadvertently include sensitive personal data. This paper presents ProPILE, a novel probing tool designed to empower data subjects, or the owners of the PII, with awareness of potential PII leakage in LLM-based services. ProPILE lets data subjects formulate prompts based on their own PII to evaluate the level of privacy intrusion in LLMs. We demonstrate its application on the OPT-1.3B model trained on the publicly available Pile dataset. We show how hypothetical data subjects may assess the likelihood of their PII being included in the Pile dataset being revealed. ProPILE can also be leveraged by LLM service providers to effectively evaluate their own levels of PII leakage with more powerful prompts specifically tuned for their in-house models. This tool represents a pioneering step towards empowering the data subjects for their awareness and control over their own data on the web.",
            "year": 2023,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ProPILE lets data subjects formulate prompts based on their own PII to evaluate the level of privacy intrusion in LLMs, and can be leveraged by LLM service providers to effectively evaluate their own levels of PII leakage with more powerful prompts specifically tuned for their in-house models."
            },
            "score": 5
        },
        {
            "id": "d1101476c85ae324142440e9f568ecbf41625be5",
            "paperId": "d1101476c85ae324142440e9f568ecbf41625be5",
            "title": "Analyzing Leakage of Personally Identifiable Information in Language Models",
            "abstract": "Language Models (LMs) have been shown to leak information about training data through sentence-level membership inference and reconstruction attacks. Understanding the risk of LMs leaking Personally Identifiable Information (PII) has received less attention, which can be attributed to the false assumption that dataset curation techniques such as scrubbing are sufficient to prevent PII leakage. Scrubbing techniques reduce but do not prevent the risk of PII leakage: in practice scrubbing is imperfect and must balance the trade-off between minimizing disclosure and preserving the utility of the dataset. On the other hand, it is unclear to which extent algorithmic defenses such as differential privacy, designed to guarantee sentence-or user-level privacy, prevent PII disclosure. In this work, we introduce rigorous game-based definitions for three types of PII leakage via black-box extraction, inference, and reconstruction attacks with only API access to an LM. We empirically evaluate the attacks against GPT-2 models fine-tuned with and without defenses in three domains: case law, health care, and e-mails. Our main contributions are (i) novel attacks that can extract up to 10\u00d7 more PII sequences than existing attacks, (ii) showing that sentence-level differential privacy reduces the risk of PII disclosure but still leaks about 3% of PII sequences, and (iii) a subtle connection between record-level membership inference and PII reconstruction. Code to reproduce all experiments in the paper is available at https://github.com/microsoft/analysing_pii_leakage.",
            "year": 2023,
            "citationCount": 78,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Novel attacks that can extract up to 10\u00d7 more PII sequences than existing attacks are introduced, showing that sentence-level differential privacy reduces the risk of PII disclosure but still leaks about 3% of P II sequences, and a subtle connection between record-level membership inference and PII reconstruction are shown."
            },
            "score": 5
        },
        {
            "id": "98cfddbf036ff1f886c2ec870e3bef5e49c36b9c",
            "paperId": "98cfddbf036ff1f886c2ec870e3bef5e49c36b9c",
            "title": "Privacy Analysis in Language Models via Training Data Leakage Report",
            "abstract": "Recent advances in neural network based language models lead to successful deployments of such models, improving user experience in various applications. It has been demonstrated that strong performance of language models may come along with the ability to memorize rare training samples, which poses serious privacy threats in case the model training is conducted on confidential user content. This necessitates privacy monitoring techniques to minimize the chance of possible privacy breaches for the models deployed in practice. In this work, we introduce a methodology that investigates identifying the user content in the training data that could be leaked under a strong and realistic threat model. We propose two metrics to quantify user-level data leakage by measuring a model's ability to produce unique sentence fragments within training data. Our metrics further enable comparing different models trained on the same data in terms of privacy. We demonstrate our approach through extensive numerical studies on real-world datasets such as email and forum conversations. We further illustrate how the proposed metrics can be utilized to investigate the efficacy of mitigations like differentially private training or API hardening.",
            "year": 2021,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A methodology that investigates identifying the user content in the training data that could be leaked under a strong and realistic threat model is introduced and two metrics to quantify user-level data leakage by measuring a model's ability to produce unique sentence fragments within training data are proposed."
            },
            "score": 5
        },
        {
            "id": "41a3c41ba1912e1384849e6898c241af89cc4a11",
            "paperId": "41a3c41ba1912e1384849e6898c241af89cc4a11",
            "title": "DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models",
            "abstract": "Large language models pretrained on a huge amount of data capture rich knowledge and information in the training data. The ability of data memorization and regurgitation in pretrained language models, revealed in previous studies, brings the risk of data leakage. In order to effectively reduce these risks, we propose a framework DEPN to Detect and Edit Privacy Neurons in pretrained language models, partially inspired by knowledge neurons and model editing. In DEPN, we introduce a novel method, termed as privacy neuron detector, to locate neurons associated with private information, and then edit these detected privacy neurons by setting their activations to zero. Furthermore, we propose a privacy neuron aggregator dememorize private information in a batch processing manner. Experimental results show that our method can significantly and efficiently reduce the exposure of private data leakage without deteriorating the performance of the model. Additionally, we empirically demonstrate the relationship between model memorization and privacy neurons, from multiple perspectives, including model size, training time, prompts, privacy neuron distribution, illustrating the robustness of our approach.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "In DEPN, a novel method is introduced, termed as privacy neuron detector, to locate neurons associated with private information, and then edit these detected privacy neurons by setting their activations to zero, and a privacy neuron aggregator dememorize private information in a batch processing manner."
            },
            "score": 5
        },
        {
            "id": "5665885cb28bee7ba6869291180173078d3bd40b",
            "paperId": "5665885cb28bee7ba6869291180173078d3bd40b",
            "title": "P5: Plug-and-Play Persona Prompting for Personalized Response Selection",
            "abstract": "The use of persona-grounded retrieval-based chatbots is crucial for personalized conversations, but there are several challenges that need to be addressed. 1) In general, collecting persona-grounded corpus is very expensive. 2) The chatbot system does not always respond in consideration of persona at real applications. To address these challenges, we propose a plug-and-play persona prompting method. Our system can function as a standard open-domain chatbot if persona information is not available. We demonstrate that this approach performs well in the zero-shot setting, which reduces the dependence on persona-ground training data. This makes it easier to expand the system to other languages without the need to build a persona-grounded corpus. Additionally, our model can be fine-tuned for even better performance. In our experiments, the zero-shot model improved the standard model by 7.71 and 1.04 points in the original persona and revised persona, respectively. The fine-tuned model improved the previous state-of-the-art system by 1.95 and 3.39 points in the original persona and revised persona, respectively. To the best of our knowledge, this is the first attempt to solve the problem of personalized response selection using prompt sequences. Our code is available on github~\\footnote{https://github.com/rungjoo/plug-and-play-prompt-persona}.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This is the first attempt to solve the problem of personalized response selection using prompt sequences using a plug-and-play persona prompting method, and it is demonstrated that this approach performs well in the zero-shot setting, which reduces the dependence on persona-ground training data."
            },
            "score": 5
        },
        {
            "id": "de2fd685f45ee916b9142bcb983d306b7da643a4",
            "paperId": "de2fd685f45ee916b9142bcb983d306b7da643a4",
            "title": "A Prompting-based Approach for Adversarial Example Generation and Robustness Enhancement",
            "abstract": "Recent years have seen the wide application of NLP models in crucial areas such as finance, medical treatment, and news media, raising concerns of the model robustness and vulnerabilities. In this paper, we propose a novel prompt-based adversarial attack to compromise NLP models and robustness enhancement technique. We first construct malicious prompts for each instance and generate adversarial examples via mask-and-filling under the effect of a malicious purpose. Our attack technique targets the inherent vulnerabilities of NLP models, allowing us to generate samples even without interacting with the victim NLP model, as long as it is based on pre-trained language models (PLMs). Furthermore, we design a prompt-based adversarial training method to improve the robustness of PLMs. As our training method does not actually generate adversarial samples, it can be applied to large-scale training sets efficiently. The experimental results show that our attack method can achieve a high attack success rate with more diverse, fluent and natural adversarial examples. In addition, our robustness enhancement method can significantly improve the robustness of models to resist adversarial attacks. Our work indicates that prompting paradigm has great potential in probing some fundamental flaws of PLMs and fine-tuning them for downstream tasks.",
            "year": 2022,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel prompt-based adversarial attack to compromise NLP models and robustness enhancement technique that can significantly improve the robustness of models to resist adversarial attacks and indicates that prompting paradigm has great potential in probing some fundamental flaws of PLMs and fine-tuning them for downstream tasks."
            },
            "score": 5
        },
        {
            "id": "dfb0b2cbdb0dfa1c76f8c0e0866cf386f5dc5ec2",
            "paperId": "dfb0b2cbdb0dfa1c76f8c0e0866cf386f5dc5ec2",
            "title": "GauchoChat: Towards Proactive, Controllable, and Personalized Social Conversation",
            "abstract": "In this paper, we introduce GauchoChat, a social bot developed for the Amazon Alexa Prize SocialBot Grand Challenge 5. Leveraging recent advances in generative language models as the primary response generator, GauchoChat introduces three main innovative solutions that lead to proactive, controllable, and personalized conversational interactions, ultimately improving user experience and satisfaction. GauchoChat introduces a LLM-based Promptist to dynamically select a set of prompting strategies based on the current user intent, persona, and emotion, resulting in use-specific responses for high-quality user engagement. Additionally, GauchoChat explores a proactive topic switching mechanism for transitioning from reactive conversations to proactive engagement with users. The proposed topic switching module intelligently determines when to switch conversation topics and integrates externally-sourced materials into the conversation. Finally, we developed real-time image retrieval to display image contents on multimodal Alexa devices. By implementing these solutions, GauchoChat ensures that its conversations remain engaging, diverse, and well-informed, fostering a proactive dialogue experience. In this paper, we present the system design and architecture of the socialbot system GauchoChat developed for the Amazon Alexa Prize SocialBot Grand Challenge 5, as well as the evaluations to demonstrate its effectiveness.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The system design and architecture of the socialbot system GauchoChat developed for the Amazon Alexa Prize SocialBot Grand Challenge 5, as well as the evaluations to demonstrate its effectiveness are presented."
            },
            "score": 4
        },
        {
            "id": "cf934ddd3c852ba9c67cdfd21bf41e7723fc6d9e",
            "paperId": "cf934ddd3c852ba9c67cdfd21bf41e7723fc6d9e",
            "title": "GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models",
            "abstract": "Providing natural language instructions in prompts is a useful new paradigm for improving task performance of large language models in a zero-shot setting. Recent work has aimed to improve such prompts via manual rewriting or gradient-based tuning. However, manual rewriting is time-consuming and requires subjective interpretation, while gradient-based tuning can be extremely computationally demanding for large models and may not be feasible for API-based models. In this work, we introduce Gradient-free Instructional Prompt Search (GrIPS), a gradient-free, edit-based search approach for improving task instructions for large language models. GrIPS takes in instructions designed for humans and automatically returns an improved, edited prompt, while allowing for API-based tuning. With InstructGPT models, GrIPS improves the average task performance by up to 4.30 percentage points on eight classification tasks from the Natural Instructions dataset (with similar improvements for OPT, BLOOM, and FLAN-T5). We see improvements for both instruction-only prompts and instruction + k-shot examples prompts. Notably, GrIPS outperforms manual rewriting and purely example-based prompts while controlling for the available compute and data budget. Further, performance of GrIPS is comparable to select gradient-based tuning approaches. Qualitatively, we show our edits can simplify instructions and at times make them incoherent but nonetheless improve accuracy.",
            "year": 2022,
            "citationCount": 79,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Gradient-free Instructional Prompt Search (GrIPS) is introduced, a gradient-free, edit-based search approach for improving task instructions for large language models and performance of GrIPS is comparable to select gradient-based tuning approaches."
            },
            "score": 4
        },
        {
            "id": "56e7bda25b83228f91962d3465fd587cfe8908e1",
            "paperId": "56e7bda25b83228f91962d3465fd587cfe8908e1",
            "title": "How Far Can We Extract Diverse Perspectives from Large Language Models? Criteria-Based Diversity Prompting!",
            "abstract": "Collecting diverse human opinions is costly and challenging. This leads to a recent trend in collaborative efforts between humans and Large Language Models (LLMs) for generating diverse data, offering potential scalable and efficient solutions. However, the extent of LLMs' capability to generate diverse perspectives on subjective topics remains an unexplored question. In this study, we investigate LLMs' capacity for generating diverse perspectives and rationales on subjective topics, such as social norms and argumentative texts. We formulate a new problem of maximum diversity extraction from LLMs. Motivated by how humans develop their opinions through their values, we propose a criteria-based prompting technique to ground diverse opinions. To see how far we can extract diverse perspectives from LLMs, or called diversity coverage, we employ a step-by-step recall prompting for generating more outputs from the model in an iterative manner. As we apply our methods to various tasks, indeed we find that LLMs can generate diverse opinions according to the degree of task subjectivity",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study investigates LLMs' capacity for generating diverse perspectives and rationales on subjective topics, such as social norms and argumentative texts, and proposes a criteria-based prompting technique to ground diverse opinions."
            },
            "score": 4
        },
        {
            "id": "281a7a99c16ce8f53bfbfb7aeb460dbd28648d28",
            "paperId": "281a7a99c16ce8f53bfbfb7aeb460dbd28648d28",
            "title": "Toxicity in ChatGPT: Analyzing Persona-assigned Language Models",
            "abstract": "Large language models (LLMs) have shown incredible capabilities and transcended the natural language processing (NLP) community, with adoption throughout many services like healthcare, therapy, education, and customer service. Since users include people with critical information needs like students or patients engaging with chatbots, the safety of these systems is of prime importance. Therefore, a clear understanding of the capabilities and limitations of LLMs is necessary. To this end, we systematically evaluate toxicity in over half a million generations of ChatGPT, a popular dialogue-based LLM. We find that setting the system parameter of ChatGPT by assigning it a persona, say that of the boxer Muhammad Ali, significantly increases the toxicity of generations. Depending on the persona assigned to ChatGPT, its toxicity can increase up to 6x, with outputs engaging in incorrect stereotypes, harmful dialogue, and hurtful opinions. This may be potentially defamatory to the persona and harmful to an unsuspecting user. Furthermore, we find concerning patterns where specific entities (e.g., certain races) are targeted more than others (3x more) irrespective of the assigned persona, that reflect inherent discriminatory biases in the model. We hope that our findings inspire the broader AI community to rethink the efficacy of current safety guardrails and develop better techniques that lead to robust, safe, and trustworthy AI systems.",
            "year": 2023,
            "citationCount": 159,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work systematically evaluates toxicity in over half a million generations of ChatGPT, a popular dialogue-based LLM, and finds concerning patterns where specific entities are targeted more than others irrespective of the assigned persona, that reflect inherent discriminatory biases in the model."
            },
            "score": 4
        },
        {
            "id": "12c826f4195da172b212a529f8fcf10cc79e35da",
            "paperId": "12c826f4195da172b212a529f8fcf10cc79e35da",
            "title": "Context-faithful Prompting for Large Language Models",
            "abstract": "Large language models (LLMs) encode parametric knowledge about world facts and have shown remarkable performance in knowledge-driven NLP tasks. However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks). In this paper, we seek to assess and enhance LLMs' contextual faithfulness in two aspects: knowledge conflict and prediction with abstention. We demonstrate that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies. In particular, we identify opinion-based prompts and counterfactual demonstrations as the most effective methods. Opinion-based prompts reframe the context as a narrator's statement and inquire about the narrator's opinions, while counterfactual demonstrations use instances containing false facts to improve faithfulness in knowledge conflict situations. Neither technique requires additional training. We conduct experiments on three datasets of two standard NLP tasks, machine reading comprehension and relation extraction, and the results demonstrate significant improvement in faithfulness to contexts. Code and data are released at https://github.com/wzhouad/context-faithful-llm.",
            "year": 2023,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies, and opinion-based prompts and counterfactual demonstrations are identified as the most effective methods."
            },
            "score": 4
        },
        {
            "id": "b0435af3063195e8ae880489e64ccde64e6d7563",
            "paperId": "b0435af3063195e8ae880489e64ccde64e6d7563",
            "title": "Guiding Large Language Models via Directional Stimulus Prompting",
            "abstract": "We introduce Directional Stimulus Prompting, a novel framework for guiding black-box large language models (LLMs) toward specific desired outputs. Instead of directly adjusting LLMs, our method employs a small tunable policy model (e.g., T5) to generate an auxiliary directional stimulus prompt for each input instance. These directional stimulus prompts act as nuanced, instance-specific hints and clues to guide LLMs in generating desired outcomes, such as including specific keywords in the generated summary. Our approach sidesteps the challenges of direct LLM tuning by optimizing the policy model to explore directional stimulus prompts that align LLMs with desired behaviors. The policy model can be optimized through 1) supervised fine-tuning using labeled data and 2) reinforcement learning from offline or online rewards based on the LLM's output. We assess our method across summarization, dialogue response generation, and chain-of-thought reasoning tasks. Our experiments demonstrate that the framework consistently improves LLMs' (e.g., ChatGPT, Codex, InstructGPT) performance on these supervised tasks using minimal labeled data. Notably, using just 80 dialogues on the MultiWOZ dataset, our approach enhances ChatGPT's performance by an impressive 41.4%, matching or surpassing some fully supervised start-of-the-art models. Additionally, the instance-specific chain-of-thought prompt generated by our approach improves InstructGPT's reasoning accuracy compared to human-crafted or automatically generated prompts. The code and data are publicly available at \\url{https://github.com/Leezekun/Directional-Stimulus-Prompting}.",
            "year": 2023,
            "citationCount": 37,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Directional Stimulus Prompting, a novel framework for guiding black-box large language models (LLMs) toward specific desired outputs, sidesteps the challenges of direct LLM tuning by optimizing the policy model to explore directional stimulus prompts that align LLMs with desired behaviors."
            },
            "score": 4
        },
        {
            "id": "e44b0bc5df2bd5eced69a85213d7cccc74f37a23",
            "paperId": "e44b0bc5df2bd5eced69a85213d7cccc74f37a23",
            "title": "Sentiment detection with FedMD: Federated Learning via Model Distillation",
            "abstract": ". Federated learning is a distributed machine learning technique in which client devices train models locally without sharing any data, except for parameter changes, which get aggregated to a central model. This privacy-preserving approach has a huge potential for reconciling the need for large Deep Learning datasets with the increasing sensitivity of data ownership. Our paper takes the novel FedMD (Federated Learning via Model Distillation) algorithm and applies it for the \ufb01 rst time to the \ufb01 eld of Natural Language Processing. The results are promising with regards to solving the data heterogeneity and model personalization challenges by introducing client-speci \ufb01 c models and collaborative learning realized through model distillation. The resulting small gap between the FedMD results and the non-FedMD implementation is compensated by the smaller amount of training data for the FedMD models and the successful preservation of privacy for locally available data.",
            "year": 2020,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The resulting small gap between the FedMD results and the non-FedMD implementation is compensated by the smaller amount of training data for the FedMD models and the successful preservation of privacy for locally available data."
            },
            "score": 4
        },
        {
            "id": "457b945ad7868b674222791d699f933be6fca58e",
            "paperId": "457b945ad7868b674222791d699f933be6fca58e",
            "title": "Privacy-Preserving Federated Learning through Clustered Sampling on Fine-Tuning Distributed non-iid Large Language Models",
            "abstract": "Recently, Large Language Models (LLMs) have been a phenomenal trend in the Artificial intelligence field. However, training and fine-tuning can be challenging because of privacy concerns and limited computing resources. Federated Learning (FL) has emerged as a novel machine learning framework offering privacy protection. The challenges in applying FL to real-world applications include dealing with heterogeneous data, poor client updates, and client selection. This paper introduces Privacy-preserving Federated Learning through Clustered Sampling on LLMs (FCLM), a framework that clusters models by their distribution similarity. It helps the model group similar models to improve text data heterogeneity handling and privacy concerns in distributed machine-learning environments. The FCLM framework is implemented and evaluated using popular Language models and text data. The framework shows a robust performance over the heterogeneous text data, which can further extend to the use of more complex LLMs.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Privacy-preserving Federated Learning through Clustered Sampling on LLMs (FCLM), a framework that clusters models by their distribution similarity to improve text data heterogeneity handling and privacy concerns in distributed machine-learning environments is introduced."
            },
            "score": 4
        },
        {
            "id": "4fef6fc97b194a8ba3dddc8ae6b5d6a632dbf778",
            "paperId": "4fef6fc97b194a8ba3dddc8ae6b5d6a632dbf778",
            "title": "Client Availability Aware Privacy-Preserving Federated Learning for Language Models",
            "abstract": "Language Models (LMs) have significantly advanced various natural language processing applications but pose challenges related to data privacy and the need for massive computational resources. Federated Learning (FL) decentralizes the training across multiple devices, enhancing data privacy. Yet, real-world FL faces complexities stemming from inconsistent client availability, especially in devices affected by factors like usage patterns and timezone correlations. Recent frameworks like CA-Fed have addressed client availability discrepancies, but they are less effective when applied to the nuanced domain of LMs. In this context, we introduce Fed-Cab, a novel Client Availability Aware Privacy-preserving Federated Learning framework tailored for LMs. Leveraging finite-state Markov chains, Fed-Cab simulates real-world client availability. Our framework introduces a novel dynamic weighting mechanism that understands and capitalizes on client availability, placing emphasis on clients that aid optimal convergence. By managing client contributions and introducing an innovative communication design, Fed-Cab efficiently captures client availability, determines weights, and dynamically selects contributors and updates models. Our results indicate that Fed-Cab not only maintains high accuracy but also excels in training speed and consistency over time.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces Fed-Cab, a novel Client Availability Aware Privacy-preserving Federated Learning framework tailored for LMs that introduces a novel dynamic weighting mechanism that understands and capitalizes on client availability, placing emphasis on clients that aid optimal convergence."
            },
            "score": 4
        },
        {
            "id": "977bfb905183a2ad0a5433efbb84086a87140c67",
            "paperId": "977bfb905183a2ad0a5433efbb84086a87140c67",
            "title": "SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for Large Language Models",
            "abstract": "With the growing use of large language models hosted on cloud platforms to offer inference services, privacy concerns are escalating, especially concerning sensitive data like investment plans and bank account details. Secure Multi-Party Computing (SMPC) emerges as a promising solution to protect the privacy of inference data and model parameters. However, the application of SMPC in Privacy-Preserving Inference (PPI) for large language models, particularly those based on the Transformer architecture, often leads to considerable slowdowns or declines in performance. This is largely due to the multitude of nonlinear operations in the Transformer architecture, which are not well-suited to SMPC and difficult to circumvent or optimize effectively. To address this concern, we introduce an advanced optimization framework called SecFormer, to achieve fast and accurate PPI for Transformer models. By implementing model design optimization, we successfully eliminate the high-cost exponential and maximum operations in PPI without sacrificing model performance. Additionally, we have developed a suite of efficient SMPC protocols that utilize segmented polynomials, Fourier series and Goldschmidt's method to handle other complex nonlinear functions within PPI, such as GeLU, LayerNorm, and Softmax. Our extensive experiments reveal that SecFormer outperforms MPCFormer in performance, showing improvements of $5.6\\%$ and $24.2\\%$ for BERT$_{\\text{BASE}}$ and BERT$_{\\text{LARGE}}$, respectively. In terms of efficiency, SecFormer is 3.56 and 3.58 times faster than Puma for BERT$_{\\text{BASE}}$ and BERT$_{\\text{LARGE}}$, demonstrating its effectiveness and speed.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces an advanced optimization framework called SecFormer, to achieve fast and accurate PPI for Transformer models, and successfully eliminates the high-cost exponential and maximum operations in PPI without sacrificing model performance."
            },
            "score": 4
        },
        {
            "id": "427b286483e3ae562a39f43c83725b5f4414d43f",
            "paperId": "427b286483e3ae562a39f43c83725b5f4414d43f",
            "title": "Detection of Suicidality Through Privacy-Preserving Large Language Models",
            "abstract": "Importance Attempts to use Artificial Intelligence (AI) in psychiatric disorders show moderate success, high-lighting the potential of incorporating information from clinical assessments to improve the mod-els. The study focuses on using Large Language Models (LLMs) to manage unstructured medi-cal text, particularly for suicide risk detection in psychiatric care. Objective The study aims to extract information about suicidality status from the admission notes of elec-tronic health records (EHR) using privacy-sensitive, locally hosted LLMs, specifically evaluating the efficacy of Llama-2 models. Main Outcomes and Measures The study compares the performance of several variants of the open source LLM Llama-2 in extracting suicidality status from psychiatric reports against a ground truth defined by human experts, assessing accuracy, sensitivity, specificity, and F1 score across different prompting strategies. Results A German fine-tuned Llama-2 model showed the highest accuracy (87.5%), sensitivity (83%) and specificity (91.8%) in identifying suicidality, with significant improvements in sensitivity and specificity across various prompt designs. Conclusions and Relevance The study demonstrates the capability of LLMs, particularly Llama-2, in accurately extracting the information on suicidality from psychiatric records while preserving data-privacy. This suggests their application in surveillance systems for psychiatric emergencies and improving the clinical management of suicidality by improving systematic quality control and research.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The study demonstrates the capability of LLMs, particularly Llama-2, in accurately extracting the information on suicidality from psychiatric records while preserving data-privacy, and suggests their application in surveillance systems for psychiatric emergencies and improving the clinical management of suicidality by improving systematic quality control and research."
            },
            "score": 4
        },
        {
            "id": "942130a875ccfe55a4c60c27c636f693e25cb13d",
            "paperId": "942130a875ccfe55a4c60c27c636f693e25cb13d",
            "title": "Personality Traits in Large Language Models",
            "abstract": "The advent of large language models (LLMs) has revolutionized natural language processing, enabling the generation of coherent and contextually relevant human-like text. As LLMs increasingly power conversational agents used by the general public world-wide, the synthetic personality embedded in these models, by virtue of training on large amounts of human data, is becoming increasingly important. Since personality is a key factor determining the effectiveness of communication, we present a comprehensive method for administering and validating personality tests on widely-used LLMs, as well as for shaping personality in the generated text of such LLMs. Applying this method, we found: 1) personality measurements in the outputs of some LLMs under specific prompting configurations are reliable and valid; 2) evidence of reliability and validity of synthetic LLM personality is stronger for larger and instruction fine-tuned models; and 3) personality in LLM outputs can be shaped along desired dimensions to mimic specific human personality profiles. We discuss application and ethical implications of the measurement and shaping method, in particular regarding responsible AI.",
            "year": 2023,
            "citationCount": 43,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Applying this method, it is found that personality measurements in the outputs of some LLMs under specific prompting configurations are reliable and valid, and personality in LLM outputs can be shaped along desired dimensions to mimic specific human personality profiles."
            },
            "score": 4
        },
        {
            "id": "03764434729b83d4f04a8bd02f99f2500cd5bbae",
            "paperId": "03764434729b83d4f04a8bd02f99f2500cd5bbae",
            "title": "Teaching Language Models to Hallucinate Less with Synthetic Tasks",
            "abstract": "Large language models (LLMs) frequently hallucinate on abstractive summarization tasks such as document-based question-answering, meeting summarization, and clinical report generation, even though all necessary information is included in context. However, optimizing LLMs to hallucinate less on these tasks is challenging, as hallucination is hard to efficiently evaluate at each optimization step. In this work, we show that reducing hallucination on a synthetic task can also reduce hallucination on real-world downstream tasks. Our method, SynTra, first designs a synthetic task where hallucinations are easy to elicit and measure. It next optimizes the LLM's system message via prefix-tuning on the synthetic task, and finally transfers the system message to realistic, hard-to-optimize tasks. Across three realistic abstractive summarization tasks, SynTra reduces hallucination for two 13B-parameter LLMs using only a synthetic retrieval task for supervision. We also find that optimizing the system message rather than the model weights can be critical; fine-tuning the entire model on the synthetic task can counterintuitively increase hallucination. Overall, SynTra demonstrates that the extra flexibility of working with synthetic data can help mitigate undesired behaviors in practice.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Across three realistic abstractive summarization tasks, SynTra reduces hallucination for two 13B-parameter LLMs using only a synthetic retrieval task for supervision, demonstrating that the extra flexibility of working with synthetic data can help mitigate undesired behaviors in practice."
            },
            "score": 4
        },
        {
            "id": "48f1a6d9ff8661be2a256422cd3f0e86fc466b5a",
            "paperId": "48f1a6d9ff8661be2a256422cd3f0e86fc466b5a",
            "title": "PSYDIAL: Personality-based Synthetic Dialogue Generation using Large Language Models",
            "abstract": "We present a novel end-to-end personality-based synthetic dialogue data generation pipeline, specifically designed to elicit responses from large language models via prompting. We design the prompts to generate more human-like dialogues considering real-world scenarios when users engage with chatbots. We introduce PSYDIAL, the first Korean dialogue dataset focused on personality-based dialogues, curated using our proposed pipeline. Notably, we focus on the Extraversion dimension of the Big Five personality model in our research. Experimental results indicate that while pre-trained models and those fine-tuned with a chit-chat dataset struggle to generate responses reflecting personality, models trained with PSYDIAL show significant improvements. The versatility of our pipeline extends beyond dialogue tasks, offering potential for other non-dialogue related applications. This research opens doors for more nuanced, personality-driven conversational AI in Korean and potentially other languages. Our code is publicly available at https://github.com/jiSilverH/psydial.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel end-to-end personality-based synthetic dialogue data generation pipeline, specifically designed to elicit responses from large language models via prompting, focusing on the Extraversion dimension of the Big Five personality model in this research."
            },
            "score": 4
        },
        {
            "id": "97010556749971d3e54039edb26fd47c713a735c",
            "paperId": "97010556749971d3e54039edb26fd47c713a735c",
            "title": "ETHICIST: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation",
            "abstract": "Large pre-trained language models achieve impressive results across many tasks. However, recent works point out that pre-trained language models may memorize a considerable fraction of their training data, leading to the privacy risk of information leakage. In this paper, we propose a method named Ethicist for targeted training data extraction through loss smoothed soft prompting and calibrated confidence estimation, investigating how to recover the suffix in the training data when given a prefix. To elicit memorization in the attacked model, we tune soft prompt embeddings while keeping the model fixed. We further propose a smoothing loss that smooths the loss distribution of the suffix tokens to make it easier to sample the correct suffix. In order to select the most probable suffix from a collection of sampled suffixes and estimate the prediction confidence, we propose a calibrated confidence estimation method, which normalizes the confidence of the generated suffixes with a local estimation. We show that Ethicist significantly improves the extraction performance on a recently proposed public benchmark. We also investigate several factors influencing the data extraction performance, including decoding strategy, model scale, prefix length, and suffix length. Our code is availabel at https://github.com/thu-coai/Targeted-Data-Extraction.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A method named Ethicist is proposed for targeted training data extraction through loss smoothed soft prompting and calibrated confidence estimation, investigating how to recover the suffix in the training data when given a prefix."
            },
            "score": 4
        },
        {
            "id": "c6fbe66f962908a6cf7cd771268d9a161000ce4d",
            "paperId": "c6fbe66f962908a6cf7cd771268d9a161000ce4d",
            "title": "Quantifying Association Capabilities of Large Language Models and Its Implications on Privacy Leakage",
            "abstract": "The advancement of large language models (LLMs) brings notable improvements across various applications, while simultaneously raising concerns about potential private data exposure. One notable capability of LLMs is their ability to form associations between different pieces of information, but this raises concerns when it comes to personally identifiable information (PII). This paper delves into the association capabilities of language models, aiming to uncover the factors that influence their proficiency in associating information. Our study reveals that as models scale up, their capacity to associate entities/information intensifies, particularly when target pairs demonstrate shorter co-occurrence distances or higher co-occurrence frequencies. However, there is a distinct performance gap when associating commonsense knowledge versus PII, with the latter showing lower accuracy. Despite the proportion of accurately predicted PII being relatively small, LLMs still demonstrate the capability to predict specific instances of email addresses and phone numbers when provided with appropriate prompts. These findings underscore the potential risk to PII confidentiality posed by the evolving capabilities of LLMs, especially as they continue to expand in scale and power.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper delves into the association capabilities of language models, aiming to uncover the factors that influence their proficiency in associating information, and reveals that as models scale up, their capacity to associate entities/information intensifies, particularly when target pairs demonstrate shorter co-occurrence distances or higher co-Occurrence frequencies."
            },
            "score": 4
        },
        {
            "id": "4f5e020ca9ad8339f1f2026e9a93f1a70da324e2",
            "paperId": "4f5e020ca9ad8339f1f2026e9a93f1a70da324e2",
            "title": "Analysis of Privacy Leakage in Federated Large Language Models",
            "abstract": "With the rapid adoption of Federated Learning (FL) as the training and tuning protocol for applications utilizing Large Language Models (LLMs), recent research highlights the need for significant modifications to FL to accommodate the large-scale of LLMs. While substantial adjustments to the protocol have been introduced as a response, comprehensive privacy analysis for the adapted FL protocol is currently lacking. To address this gap, our work delves into an extensive examination of the privacy analysis of FL when used for training LLMs, both from theoretical and practical perspectives. In particular, we design two active membership inference attacks with guaranteed theoretical success rates to assess the privacy leakages of various adapted FL configurations. Our theoretical findings are translated into practical attacks, revealing substantial privacy vulnerabilities in popular LLMs, including BERT, RoBERTa, DistilBERT, and OpenAI's GPTs, across multiple real-world language datasets. Additionally, we conduct thorough experiments to evaluate the privacy leakage of these models when data is protected by state-of-the-art differential privacy (DP) mechanisms.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work designs two active membership inference attacks with guaranteed theoretical success rates to assess the privacy leakages of various adapted FL configurations, revealing substantial privacy vulnerabilities in popular LLMs, including BERT, RoBERTa, DistilBERT, and OpenAI's GPTs, across multiple real-world language datasets."
            },
            "score": 4
        },
        {
            "id": "385c2ee0bf829676d1a5aacfc697fc6a9d245ed5",
            "paperId": "385c2ee0bf829676d1a5aacfc697fc6a9d245ed5",
            "title": "DP-Forward: Fine-tuning and Inference on Language Models with Differential Privacy in Forward Pass",
            "abstract": "Differentially private stochastic gradient descent (DP-SGD) adds noise to gradients in back-propagation, safeguarding training data from privacy leakage, particularly membership inference. It fails to cover (inference-time) threats like embedding inversion and sensitive attribute inference. It is also costly in storage and computation when used to fine-tune large pre-trained language models (LMs). We propose DP-Forward, which directly perturbs embedding matrices in the forward pass of LMs. It satisfies stringent local DP requirements for training and inference data. To instantiate it using the smallest matrix-valued noise, we devise an analytic matrix Gaussian mechanism (aMGM) by drawing possibly non-i.i.d. noise from a matrix Gaussian distribution. We then investigate perturbing outputs from different hidden (sub-)layers of LMs with aMGM noises. Its utility on three typical tasks almost hits the non-private baseline and outperforms DP-SGD by up to 7.7pp at a moderate privacy level. It saves 3x time and memory costs compared to DP-SGD with the latest high-speed library. It also reduces the average success rates of embedding inversion and sensitive attribute inference by up to 88pp and 41pp, respectively, whereas DP-SGD fails.",
            "year": 2023,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "DP-Forward is proposed, which directly perturbs embedding matrices in the forward pass of LMs and satisfies stringent local DP requirements for training and inference data, and saves 3x time and memory costs compared to DP-SGD with the latest high-speed library."
            },
            "score": 4
        },
        {
            "id": "cc3b41c4d2ededb94b481089633ecb43d6b2162f",
            "paperId": "cc3b41c4d2ededb94b481089633ecb43d6b2162f",
            "title": "Quantifying Privacy Risks of Masked Language Models Using Membership Inference Attacks",
            "abstract": "The wide adoption and application of Masked language models (MLMs) on sensitive data (from legal to medical) necessitates a thorough quantitative investigation into their privacy vulnerabilities. Prior attempts at measuring leakage of MLMs via membership inference attacks have been inconclusive, implying potential robustness of MLMs to privacy attacks.In this work, we posit that prior attempts were inconclusive because they based their attack solely on the MLM\u2019s model score. We devise a stronger membership inference attack based on likelihood ratio hypothesis testing that involves an additional reference MLM to more accurately quantify the privacy risks of memorization in MLMs. We show that masked language models are indeed susceptible to likelihood ratio membership inference attacks: Our empirical results, on models trained on medical notes, show that our attack improves the AUC of prior membership inference attacks from 0.66 to an alarmingly high 0.90 level.",
            "year": 2022,
            "citationCount": 73,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work devise a stronger membership inference attack based on likelihood ratio hypothesis testing that involves an additional reference MLM to more accurately quantify the privacy risks of memorization in MLMs."
            },
            "score": 4
        },
        {
            "id": "1819c5c8c556db205ec9a49d5ba4a8c2afb28a4a",
            "paperId": "1819c5c8c556db205ec9a49d5ba4a8c2afb28a4a",
            "title": "Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy",
            "abstract": "Studying data memorization in neural language models helps us understand the risks (e.g., to privacy or copyright) associated with models regurgitating training data, and aids in the evaluation of potential countermeasures. Many prior works\u2014and some recently deployed defenses\u2014focus on \u201cverbatim memo-rization\u201d, de\ufb01ned as a model generation that exactly matches a substring from the training set. We argue that verbatim memorization def-initions are too restrictive and fail to capture more subtle forms of memorization. Speci\ufb01cally, we design and implement an ef\ufb01cient defense based on Bloom \ufb01lters that perfectly prevents all verbatim memorization. And yet, we demonstrate that this \u201cperfect\u201d \ufb01lter does not prevent the leakage of training data. Indeed, it is easily circumvented by plausible and mini-mally modi\ufb01ed \u201cstyle-transfer\u201d prompts\u2014and in some cases even the non-modi\ufb01ed original prompts\u2014to extract memorized information. For example, instructing the model to output ALL-CAPITAL texts bypasses memorization checks based on verbatim matching. We conclude by discussing potential alternative de\ufb01nitions and why de\ufb01ning memorization is a dif\ufb01cult yet crucial open question for neural language models.",
            "year": 2022,
            "citationCount": 58,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is argued that verbatim memorization def-initions are too restrictive and fail to capture more subtle forms of memorization, and potential alternative de\ufb01nitions are discussed and why memorization is a crucial open question for neural language models."
            },
            "score": 4
        },
        {
            "id": "cd9455512c80f104f525d71c0ee8a624e150afa8",
            "paperId": "cd9455512c80f104f525d71c0ee8a624e150afa8",
            "title": "P-Bench: A Multi-level Privacy Evaluation Benchmark for Language Models",
            "abstract": "The rapid development of language models (LMs) brings unprecedented accessibility and usage for both models and users. On the one hand, powerful LMs, trained with massive textual data, achieve state-of-the-art performance over numerous downstream NLP tasks. On the other hand, more and more attention is paid to unrestricted model accesses that may bring malicious privacy risks of data leakage. To address these issues, many recent works propose privacy-preserving language models (PPLMs) with differential privacy (DP). Unfortunately, different DP implementations make it challenging for a fair comparison among existing PPLMs. In this paper, we present P-Bench, a multi-perspective privacy evaluation benchmark to empirically and intuitively quantify the privacy leakage of LMs. Instead of only protecting and measuring the privacy of protected data with DP parameters, P-Bench sheds light on the neglected inference data privacy during actual usage. P-Bench first clearly defines multi-faceted privacy objectives during private fine-tuning. Then, P-Bench constructs a unified pipeline to perform private fine-tuning. Lastly, P-Bench performs existing privacy attacks on LMs with pre-defined privacy objectives as the empirical evaluation results. The empirical attack results are used to fairly and intuitively evaluate the privacy leakage of various PPLMs. We conduct extensive experiments on three datasets of GLUE for mainstream LMs.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "P-Bench is presented, a multi-perspective privacy evaluation benchmark to empirically and intuitively quantify the privacy leakage of LMs and performs existing privacy attacks on LMs with pre-defined privacy objectives as the empirical evaluation results."
            },
            "score": 4
        },
        {
            "id": "16f55d843b1aed18cab7464580f229c07f9bd188",
            "paperId": "16f55d843b1aed18cab7464580f229c07f9bd188",
            "title": "Prompting Techniques for Reducing Social Bias in LLMs through System 1 and System 2 Cognitive Processes",
            "abstract": "Dual process theory posits that human cognition arises via two systems. System 1, which is a quick, emotional, and intuitive process, which is subject to cognitive biases, and System 2, a slow, onerous, and deliberate process. NLP researchers often compare zero-shot prompting in LLMs to System 1 reasoning and chain-of-thought (CoT) prompting to System 2. In line with this interpretation, prior research has found that using CoT prompting in LLMs leads to reduced gender bias. We investigate the relationship between bias, CoT prompting, and dual process theory in LLMs directly. We compare zero-shot, CoT, and a variety of dual process theory-based prompting strategies on two bias datasets spanning nine different social bias categories. We also use human and machine personas to determine whether the effects of dual process theory in LLMs are based on modeling human cognition or inherent to the system. We find that a human persona, System 2, and CoT prompting all tend to reduce social biases in LLMs, though the best combination of features depends on the exact model and bias category -- resulting in up to a 13 percent drop in stereotypical judgments by an LLM.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that a human persona, System 2, and CoT prompting all tend to reduce social biases in LLMs, though the best combination of features depends on the exact model and bias category -- resulting in up to a 13 percent drop in stereotypical judgments by an LLM."
            },
            "score": 4
        },
        {
            "id": "86b488a19f12888d707ae8895be0703a69480c2f",
            "paperId": "86b488a19f12888d707ae8895be0703a69480c2f",
            "title": "Crafting a Good Prompt or Providing Exemplary Dialogues? A Study of In-Context Learning for Persona-based Dialogue Generation",
            "abstract": "Previous in-context learning (ICL) research has focused on tasks such as classification, machine translation, text2table, etc., while studies on whether ICL can improve human-like dialogue generation are scarce. Our work fills this gap by systematically investigating the ICL capabilities of large language models (LLMs) in persona-based dialogue generation, conducting extensive experiments on high-quality real human Chinese dialogue datasets. From experimental results, we draw three conclusions: 1) adjusting prompt instructions is the most direct, effective, and economical way to improve generation quality; 2) randomly retrieving demonstrations (demos) achieves the best results, possibly due to the greater diversity and the amount of effective information; counter-intuitively, retrieving demos with a context identical to the query performs the worst; 3) even when we destroy the multi-turn associations and single-turn semantics in the demos, increasing the number of demos still improves dialogue performance, proving that LLMs can learn from corrupted dialogue demos. Previous explanations of the ICL mechanism, such as $n$-gram induction head, cannot fully account for this phenomenon.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "From experimental results, this work draws three conclusions: 1) adjusting prompt instructions is the most direct, effective, and economical way to improve generation quality; 2) increasing the number of demos still improves dialogue performance, proving that LLMs can learn from corrupted dialogue demos."
            },
            "score": 4
        },
        {
            "id": "d235a9085e0543fcbe502fbc269f9a8ee01dcbab",
            "paperId": "d235a9085e0543fcbe502fbc269f9a8ee01dcbab",
            "title": "AdaPrompt: Adaptive Model Training for Prompt-based NLP",
            "abstract": "Prompt-based learning, with its capability to tackle zero-shot and few-shot NLP tasks, has gained much attention in community. The main idea is to bridge the gap between NLP downstream tasks and language modeling (LM), by mapping these tasks into natural language prompts, which are then filled by pre-trained language models (PLMs). However, for prompt learning, there are still two salient gaps between NLP tasks and pretraining. First, prompt information is not necessarily sufficiently present during LM pretraining. Second, task-specific data are not necessarily well represented during pretraining. We address these two issues by proposing AdaPrompt, adaptively retrieving external data for continual pretraining of PLMs by making use of both task and prompt characteristics. In addition, we make use of knowledge in Natural Language Inference models for deriving adaptive verbalizers. Experimental results on five NLP benchmarks show that AdaPrompt can improve over standard PLMs in few-shot settings. In addition, in zero-shot settings, our method outperforms standard prompt-based methods by up to 26.35\\% relative error reduction.",
            "year": 2022,
            "citationCount": 35,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results on five NLP benchmarks show that AdaPrompt can improve over standard PLMs in few-shot settings and make use of knowledge in Natural Language Inference models for deriving adaptive verbalizers."
            },
            "score": 4
        },
        {
            "id": "434b9f9bc71c935e4a46a1aff36a8cc4c22d9afa",
            "paperId": "434b9f9bc71c935e4a46a1aff36a8cc4c22d9afa",
            "title": "Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration",
            "abstract": "Human intelligence thrives on cognitive synergy, where collaboration among different minds yield superior outcomes compared to isolated individuals. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist is an intelligent agent that collaboratively combines multiple minds' strengths and knowledge to enhance problem-solving in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. Our in-depth analysis shows that assigning multiple fine-grained personas in LLMs improves problem-solving abilities compared to using a single or fixed number of personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing, Codenames Collaborative, and Logic Grid Puzzle, encompassing both knowledge-intensive and reasoning-intensive types. Unlike previous works, such as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, experimental results demonstrate that SPP effectively reduces factual hallucination, and maintains strong reasoning capabilities. Additionally, comparative experiments show that cognitive synergy only emerges in GPT-4 and does not appear in less capable models, such as GPT-3.5-turbo and Llama2-13b-chat, which draws an interesting analogy to human development. Code, data, and prompts can be found at: https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.",
            "year": 2023,
            "citationCount": 42,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas by dynamically identifying and simulating different personas based on task inputs, which unleashes the potential of cognitive synergy in LLMs."
            },
            "score": 3
        },
        {
            "id": "9eefc89f842e3ba7473143cfef07929ac1458a46",
            "paperId": "9eefc89f842e3ba7473143cfef07929ac1458a46",
            "title": "Prompting ChatGPT to Draw Morphological Connections for New Word Comprehension",
            "abstract": "Though more powerful, Large Language Models need to be periodically retrained for updated information, consuming resources and energy. In this respect, prompt engineering can prove a possible solution to re-training. To explore this line of research, this paper uses a case study, namely, finding the best prompting strategy for asking ChatGPT to define new words based on morphological connections. To determine the best prompting strategy, each definition provided by the prompt was ranked in terms of plausibility and humanlikeness criteria. The findings of this paper show that adding contextual information, operationalised as the keywords \u2018new\u2019 and \u2018morpheme\u2019, significantly improve the performance of the model for any prompt. While no single prompt significantly outperformed all others, there were differences between performances on the two criteria for most prompts. ChatGPT also provided the most correct definitions with a persona-type prompt.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Adding contextual information, operationalised as the keywords \u2018new\u2019 and \u2018morpheme\u2019, significantly improve the performance of the model for any prompt, and finding the best prompting strategy for asking ChatGPT to define new words based on morphological connections is used."
            },
            "score": 3
        },
        {
            "id": "9e9f675830caceddca3619c2c4d4441e519d894f",
            "paperId": "9e9f675830caceddca3619c2c4d4441e519d894f",
            "title": "Prophet: Prompting Large Language Models with Complementary Answer Heuristics for Knowledge-based Visual Question Answering",
            "abstract": "Knowledge-based visual question answering (VQA) requires external knowledge beyond the image to answer the question. Early studies retrieve required knowledge from explicit knowledge bases (KBs), which often introduces irrelevant information to the question, hence restricting the performance of their models. Recent works have resorted to using a powerful large language model (LLM) as an implicit knowledge engine to acquire the necessary knowledge for answering. Despite the encouraging results achieved by these methods, we argue that they have not fully activated the capacity of the blind LLM as the provided textual input is insufficient to depict the required visual information to answer the question. In this paper, we present Prophet -- a conceptually simple, flexible, and general framework designed to prompt LLM with answer heuristics for knowledge-based VQA. Specifically, we first train a vanilla VQA model on a specific knowledge-based VQA dataset without external knowledge. After that, we extract two types of complementary answer heuristics from the VQA model: answer candidates and answer-aware examples. Finally, the two types of answer heuristics are jointly encoded into a formatted prompt to facilitate the LLM's understanding of both the image and question, thus generating a more accurate answer. By incorporating the state-of-the-art LLM GPT-3, Prophet significantly outperforms existing state-of-the-art methods on four challenging knowledge-based VQA datasets. To demonstrate the generality of our approach, we instantiate Prophet with the combinations of different VQA models (i.e., both discriminative and generative ones) and different LLMs (i.e., both commercial and open-source ones).",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Prophet -- a conceptually simple, flexible, and general framework designed to prompt LLM with answer heuristics for knowledge-based VQA, which significantly outperforms existing state-of-the-art methods on four challenging knowledge-based VQA datasets."
            },
            "score": 3
        },
        {
            "id": "71d68782c3da41b77866c2fd0cb65726f60b3af1",
            "paperId": "71d68782c3da41b77866c2fd0cb65726f60b3af1",
            "title": "Analyzing Chain-of-Thought Prompting in Large Language Models via Gradient-based Feature Attributions",
            "abstract": "Chain-of-thought (CoT) prompting has been shown to empirically improve the accuracy of large language models (LLMs) on various question answering tasks. While understanding why CoT prompting is effective is crucial to ensuring that this phenomenon is a consequence of desired model behavior, little work has addressed this; nonetheless, such an understanding is a critical prerequisite for responsible model deployment. We address this question by leveraging gradient-based feature attribution methods which produce saliency scores that capture the influence of input tokens on model output. Specifically, we probe several open-source LLMs to investigate whether CoT prompting affects the relative importances they assign to particular input tokens. Our results indicate that while CoT prompting does not increase the magnitude of saliency scores attributed to semantically relevant tokens in the prompt compared to standard few-shot prompting, it increases the robustness of saliency scores to question perturbations and variations in model output.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work probes several open-source LLMs to investigate whether CoT prompting affects the relative importances they assign to particular input tokens, and results indicate that while coT prompting does not increase the magnitude of saliency scores attributed to semantically relevant tokens in the prompt compared to standard few-shot prompting, it increases the robustness ofsaliency scores to question perturbations and variations in model output."
            },
            "score": 3
        },
        {
            "id": "2d3bc530d8f1ed36932a70bc362ea94d988adec9",
            "paperId": "2d3bc530d8f1ed36932a70bc362ea94d988adec9",
            "title": "Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting",
            "abstract": "Ranking documents using Large Language Models (LLMs) by directly feeding the query and candidate documents into the prompt is an interesting and practical problem. However, researchers have found it difficult to outperform fine-tuned baseline rankers on benchmark datasets. We analyze pointwise and listwise ranking prompts used by existing methods and argue that off-the-shelf LLMs do not fully understand these challenging ranking formulations. In this paper, we propose to significantly reduce the burden on LLMs by using a new technique called Pairwise Ranking Prompting (PRP). Our results are the first in the literature to achieve state-of-the-art ranking performance on standard benchmarks using moderate-sized open-sourced LLMs. On TREC-DL 2019&2020, PRP based on the Flan-UL2 model with 20B parameters performs favorably with the previous best approach in the literature, which is based on the blackbox commercial GPT-4 that has 50x (estimated) model size, while outperforming other LLM-based solutions, such as InstructGPT which has 175B parameters, by over 10% for all ranking metrics. By using the same prompt template on seven BEIR tasks, PRP outperforms supervised baselines and outperforms the blackbox commercial ChatGPT solution by 4.2% and pointwise LLM-based solutions by more than 10% on average NDCG@10. Furthermore, we propose several variants of PRP to improve efficiency and show that it is possible to achieve competitive results even with linear complexity.",
            "year": 2023,
            "citationCount": 79,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results are the first in the literature to achieve state-of-the-art ranking performance on standard benchmarks using moderate-sized open-sourced LLMs and several variants of PRP are proposed to improve efficiency and show that it is possible to achieve competitive results even with linear complexity."
            },
            "score": 3
        },
        {
            "id": "3fc3460c4554a28e489a0ea6ef067b79b7d301d9",
            "paperId": "3fc3460c4554a28e489a0ea6ef067b79b7d301d9",
            "title": "Active Prompting with Chain-of-Thought for Large Language Models",
            "abstract": "The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks requiring reasoning, such as arithmetic and commonsense reasoning. It is known that the effective design of task-specific prompts is critical for LLMs' ability to produce high-quality answers. In particular, an effective approach for complex question-and-answer tasks is example-based prompting with chain-of-thought (CoT) reasoning, which significantly improves the performance of LLMs. However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks. This paper proposes a new method, Active-Prompt, to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning). For this purpose, we propose a solution to the key problem of determining which questions are the most important and helpful ones to annotate from a pool of task-specific queries. By borrowing ideas from the related problem of uncertainty-based active learning, we introduce several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation. Experimental results demonstrate the superiority of our proposed method, achieving state-of-the-art on eight complex reasoning tasks. Further analyses of different uncertainty metrics, pool sizes, zero-shot learning, and accuracy-uncertainty relationship demonstrate the effectiveness of our method. Our code will be available at https://github.com/shizhediao/active-prompt.",
            "year": 2023,
            "citationCount": 58,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new method to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning), and introduces several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation."
            },
            "score": 3
        },
        {
            "id": "98ca8e34ff11836b653ab9e27207c04c8dd135f4",
            "paperId": "98ca8e34ff11836b653ab9e27207c04c8dd135f4",
            "title": "Large Language Models are Competitive Near Cold-start Recommenders for Language- and Item-based Preferences",
            "abstract": "Traditional recommender systems leverage users\u2019 item preference history to recommend novel content that users may like. However, modern dialog interfaces that allow users to express language-based preferences offer a fundamentally different modality for preference input. Inspired by recent successes of prompting paradigms for large language models (LLMs), we study their use for making recommendations from both item-based and language-based preferences in comparison to state-of-the-art item-based collaborative filtering (CF) methods. To support this investigation, we collect a new dataset consisting of both item-based and language-based preferences elicited from users along with their ratings on a variety of (biased) recommended items and (unbiased) random items. Among numerous experimental results, we find that LLMs provide competitive recommendation performance for pure language-based preferences (no item preferences) in the near cold-start case in comparison to item-based CF methods, despite having no supervised training for this specific task (zero-shot) or only a few labels (few-shot). This is particularly promising as language-based preference representations are more explainable and scrutable than item-based or vector-based representations.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LLMs provide competitive recommendation performance for pure language-based preferences (no item preferences) in the near cold-start case in comparison to item-based CF methods, despite having no supervised training for this specific task or only a few labels."
            },
            "score": 3
        },
        {
            "id": "e4e744cc96da7987a072571fc3817f040d456566",
            "paperId": "e4e744cc96da7987a072571fc3817f040d456566",
            "title": "Large Language Models Know Your Contextual Search Intent: A Prompting Framework for Conversational Search",
            "abstract": "Precisely understanding users' contextual search intent has been an important challenge for conversational search. As conversational search sessions are much more diverse and long-tailed, existing methods trained on limited data still show unsatisfactory effectiveness and robustness to handle real conversational search scenarios. Recently, large language models (LLMs) have demonstrated amazing capabilities for text generation and conversation understanding. In this work, we present a simple yet effective prompting framework, called LLM4CS, to leverage LLMs as a text-based search intent interpreter to help conversational search. Under this framework, we explore three prompting methods to generate multiple query rewrites and hypothetical responses, and propose to aggregate them into an integrated representation that can robustly represent the user's real contextual search intent. Extensive automatic evaluations and human evaluations on three widely used conversational search benchmarks, including CAsT-19, CAsT-20, and CAsT-21, demonstrate the remarkable performance of our simple LLM4CS framework compared with existing methods and even using human rewrites. Our findings provide important evidence to better understand and leverage LLMs for conversational search.",
            "year": 2023,
            "citationCount": 26,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents a simple yet effective prompting framework, called LLM4CS, to leverage LLMs as a text-based search intent interpreter to help conversational search."
            },
            "score": 3
        },
        {
            "id": "5423067581ac9d51d19253c9085b871ef0373dfe",
            "paperId": "5423067581ac9d51d19253c9085b871ef0373dfe",
            "title": "FedMS: Federated Learning with Mixture of Sparsely Activated Foundations Models",
            "abstract": "Foundation models have shown great success in natural language processing, computer vision, and multimodal tasks. FMs have a large number of model parameters, thus requiring a substantial amount of data to help optimize the model during the training. Federated learning has revolutionized machine learning by enabling collaborative learning from decentralized data while still preserving the data privacy of clients. Despite the great benefits foundation models can have empowered by federated learning, they face severe computation, communication, and statistical challenges. In this paper, we propose a novel two-stage federated learning algorithm called FedMS. A global expert is trained in the first stage and a local expert is trained in the second stage to provide better personalization. We construct a Mixture of Foundation Models (MoFM) with these two experts and design a gate neural network with an inserted gate adapter that joins the aggregation every communication round in the second stage. To further adapt to edge computing scenarios with limited computational resources, we design a novel Sparsely Activated LoRA (SAL) algorithm that freezes the pre-trained foundation model parameters inserts low-rank adaptation matrices into transformer blocks and activates them progressively during the training. We employ extensive experiments to verify the effectiveness of FedMS, results show that FedMS outperforms other SOTA baselines by up to 55.25% in default settings.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel two-stage federated learning algorithm called FedMS, where a global expert is trained in the first stage and a local expert is trained in the second stage to provide better personalization."
            },
            "score": 3
        },
        {
            "id": "0e1c60dc4119589bbbf02da26f73f4fd6330be4b",
            "paperId": "0e1c60dc4119589bbbf02da26f73f4fd6330be4b",
            "title": "Revolutionizing Cyber Threat Detection With Large Language Models: A Privacy-Preserving BERT-Based Lightweight Model for IoT/IIoT Devices",
            "abstract": "The field of Natural Language Processing (NLP) is currently undergoing a revolutionary transformation driven by the power of pre-trained Large Language Models (LLMs) based on groundbreaking Transformer architectures. As the frequency and diversity of cybersecurity attacks continue to rise, the importance of incident detection has significantly increased. IoT devices are expanding rapidly, resulting in a growing need for efficient techniques to autonomously identify network-based attacks in IoT networks with both high precision and minimal computational requirements. This paper presents SecurityBERT, a novel architecture that leverages the Bidirectional Encoder Representations from Transformers (BERT) model for cyber threat detection in IoT networks. During the training of SecurityBERT, we incorporated a novel privacy-preserving encoding technique called Privacy-Preserving Fixed-Length Encoding (PPFLE). We effectively represented network traffic data in a structured format by combining PPFLE with the Byte-level Byte-Pair Encoder (BBPE) Tokenizer. Our research demonstrates that SecurityBERT outperforms traditional Machine Learning (ML) and Deep Learning (DL) methods, such as Convolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs), in cyber threat detection. Employing the Edge-IIoTset cybersecurity dataset, our experimental analysis shows that SecurityBERT achieved an impressive 98.2% overall accuracy in identifying fourteen distinct attack types, surpassing previous records set by hybrid solutions such as GAN-Transformer-based architectures and CNN-LSTM models. With an inference time of less than 0.15 seconds on an average CPU and a compact model size of just 16.7MB, SecurityBERT is ideally suited for real-life traffic analysis and a suitable choice for deployment on resource-constrained IoT devices.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "SecurityBERT is a novel architecture that leverages the Bidirectional Encoder Representations from Transformers (BERT) model for cyber threat detection in IoT networks and achieves an impressive 98.2% overall accuracy in identifying fourteen distinct attack types."
            },
            "score": 3
        },
        {
            "id": "00dbe18d9fc47565ee3ab3ee6354d027f1d80f2f",
            "paperId": "00dbe18d9fc47565ee3ab3ee6354d027f1d80f2f",
            "title": "Privacy Preserving Personalization for Video Facial Expression Recognition Using Federated Learning",
            "abstract": "The increased ubiquitousness of small smart devices, such as cellphones, tablets, smart watches and laptops, has led to unique user data, which can be locally processed. The sensors (e.g., microphones and webcam) and improved hardware of the new devices have allowed running deep learning models that 20 years ago would have been exclusive to high-end expensive machines. In spite of this progress, state-of-the-art algorithms for facial expression recognition (FER) rely on architectures that cannot be implemented on these devices due to computational and memory constraints. Alternatives involving cloud-based solutions impose privacy barriers that prevent their adoption or user acceptance in wide range of applications. This paper proposes a lightweight model that can run in real-time for image facial expression recognition (IFER) and video facial expression recognition (VFER). The approach relies on a personalization mechanism locally implemented for each subject by fine-tuning a central VFER model with unlabeled videos from a target subject. We train the IFER model to generate pseudo labels and we select the videos with the highest confident predictions to be used for adaptation. The adaptation is performed by implementing a federated learning strategy where the weights of the local model are averaged and used by the central VFER model. We demonstrate that this approach can improve not only the performance on the edge device providing personalized models to the users, but also the central VFER model. We implement a federated learning strategy where the weights of the local models are averaged and used by the central VFER. Within corpus and cross-corpus evaluations on two emotional databases demonstrate that edge models adapted with our personalization strategy achieve up to 13.1% gains in F1-scores. Furthermore, the federated learning implementation improves the mean micro F1-score of the central VFER model by up to 3.4%. The proposed lightweight solution is ideal for interactive user interfaces that preserve the data of the users.",
            "year": 2022,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a lightweight model that can run in real-time for image facial expression recognition (IFER) and video facial expressions recognition (VFER) and demonstrates that this approach can improve not only the performance on the edge device providing personalized models to the users, but also the central VFER model."
            },
            "score": 3
        },
        {
            "id": "29f07e73b7aaa7e9e950c59710472c62316be74a",
            "paperId": "29f07e73b7aaa7e9e950c59710472c62316be74a",
            "title": "Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations",
            "abstract": "The collection and curation of high-quality training data is crucial for developing text classification models with superior performance, but it is often associated with significant costs and time investment. Researchers have recently explored using large language models (LLMs) to generate synthetic datasets as an alternative approach. However, the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks. To better understand factors that moderate the effectiveness of the LLM-generated synthetic data, in this study, we look into how the performance of models trained on these synthetic data may vary with the subjectivity of classification. Our results indicate that subjectivity, at both the task level and instance level, is negatively associated with the performance of the model trained on synthetic data. We conclude by discussing the implications of our work on the potential and limitations of leveraging LLM for synthetic data generation.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study looks into how the performance of models trained on these synthetic data may vary with the subjectivity of classification, and indicates that subjectivity, at both the task level and instance level, is negatively associated with theperformance of the model trained on synthetic data."
            },
            "score": 3
        },
        {
            "id": "156c1b14d1160ba9802615d37e8b827a13282a4e",
            "paperId": "156c1b14d1160ba9802615d37e8b827a13282a4e",
            "title": "Synthetic Data Generation in Low-Resource Settings via Fine-Tuning of Large Language Models",
            "abstract": "The in-context learning ability of large language models (LLMs) enables them to generalize to novel downstream tasks with relatively few labeled examples. However, they require enormous computational resources to be deployed. Alternatively, smaller models can solve specific tasks if fine-tuned with enough labeled examples. These examples, however, are expensive to obtain. In pursuit of the best of both worlds, we study synthetic data generation of fine-tuning training data via fine-tuned teacher LLMs to improve the downstream performance of much smaller models. In four text classification and two text generation tasks, we find that both data generation and annotation dramatically improve the respective downstream model's performance, occasionally necessitating only a minor fraction of the original training dataset.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "In four text classification and two text generation tasks, it is found that both data generation and annotation dramatically improve the respective downstream model's performance, occasionally necessitating only a minor fraction of the original training dataset."
            },
            "score": 3
        },
        {
            "id": "5b7fa8852f92d117356526a82b1f373e7997d123",
            "paperId": "5b7fa8852f92d117356526a82b1f373e7997d123",
            "title": "Entity-Controlled Synthetic Text Generation using Contextual Question and Answering with Pre-trained Language Models",
            "abstract": "Recent advancements in Natural Language Processing (NLP) algorithms have resulted in state-of-the-art performance on Named Entity Recognition (NER) tasks. These algorithms typically require high-quality labeled datasets for training models. However, training NLP models effectively can suffer from issues such as scarcity of labeled data, data bias and under-representation, and privacy concerns with using sensitive data for training. Generating synthetic data to train models is a promising solution to mitigate these problems. We propose a contextual question and answering approach using pre-trained language models to synthetically generate entity-controlled text. Entity-controlled text generation is then used to augment small labeled datasets for downstream NER tasks. We evaluate this proposed method on two publicly available datasets, and measure the quality of generated texts quantitatively. We find that the model is capable of producing full text samples with the desired entities appearing in a stochastically controllable way, while retaining sentence coherence closest to the real world data. Evaluations on downstream NER tasks show significant improvements in low-labeled data regime, and in using purely synthetic data for NER to alleviate privacy concerns.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a contextual question and answering approach using pre-trained language models to synthetically generate entity-controlled text, and finds that the model is capable of producing full text samples with the desired entities appearing in a stochastically controllable way, while retaining sentence coherence closest to the real world data."
            },
            "score": 3
        },
        {
            "id": "5af9cf0b695faf2eb94d74bf76dab1a311638ca3",
            "paperId": "5af9cf0b695faf2eb94d74bf76dab1a311638ca3",
            "title": "Generating Faithful Synthetic Data with Large Language Models: A Case Study in Computational Social Science",
            "abstract": "Large Language Models (LLMs) have democratized synthetic data generation, which in turn has the potential to simplify and broaden a wide gamut of NLP tasks. Here, we tackle a pervasive problem in synthetic data generation: its generative distribution often differs from the distribution of real-world data researchers care about (in other words, it is unfaithful). In a case study on sarcasm detection, we study three strategies to increase the faithfulness of synthetic data: grounding, filtering, and taxonomy-based generation. We evaluate these strategies using the performance of classifiers trained with generated synthetic data on real-world data. While all three strategies improve the performance of classifiers, we find that grounding works best for the task at hand. As synthetic data generation plays an ever-increasing role in NLP research, we expect this work to be a stepping stone in improving its utility. We conclude this paper with some recommendations on how to generate high(er)-fidelity synthetic data for specific tasks.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work tackles a pervasive problem in synthetic data generation: its generative distribution often differs from the distribution of real-world data researchers care about (in other words, it is unfaithful)."
            },
            "score": 3
        },
        {
            "id": "5ff9cd8fcb959ca6b458c11e780d61c3f2bf7691",
            "paperId": "5ff9cd8fcb959ca6b458c11e780d61c3f2bf7691",
            "title": "PLACES: Prompting Language Models for Social Conversation Synthesis",
            "abstract": "Collecting high quality conversational data can be very expensive for most applications and infeasible for others due to privacy, ethical, or similar concerns. A promising direction to tackle this problem is to generate synthetic dialogues by prompting large language models. In this work, we use a small set of expert-written conversations as in-context examples to synthesize a social conversation dataset using prompting. We perform several thorough evaluations of our synthetic conversations compared to human-collected conversations. This includes various dimensions of conversation quality with human evaluation directly on the synthesized conversations, and interactive human evaluation of chatbots fine-tuned on the synthetically generated dataset. We additionally demonstrate that this prompting approach is generalizable to multi-party conversations, providing potential to create new synthetic data for multi-party tasks. Our synthetic multi-party conversations were rated more favorably across all measured dimensions compared to conversation excerpts sampled from a human-collected multi-party dataset.",
            "year": 2023,
            "citationCount": 37,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work uses a small set of expert-written conversations as in-context examples to synthesize a social conversation dataset using prompting, and demonstrates that this prompting approach is generalizable to multi-party conversations, providing potential to create new synthetic data for multi- party tasks."
            },
            "score": 3
        },
        {
            "id": "6eef9f34238b185497f5624c6c6c14026fb818a0",
            "paperId": "6eef9f34238b185497f5624c6c6c14026fb818a0",
            "title": "FLTrojan: Privacy Leakage Attacks against Federated Language Models Through Selective Weight Tampering",
            "abstract": "Federated learning (FL) is becoming a key component in many technology-based applications including language modeling -- where individual FL participants often have privacy-sensitive text data in their local datasets. However, realizing the extent of privacy leakage in federated language models is not straightforward and the existing attacks only intend to extract data regardless of how sensitive or naive it is. To fill this gap, in this paper, we introduce two novel findings with regard to leaking privacy-sensitive user data from federated language models. Firstly, we make a key observation that model snapshots from the intermediate rounds in FL can cause greater privacy leakage than the final trained model. Secondly, we identify that privacy leakage can be aggravated by tampering with a model's selective weights that are specifically responsible for memorizing the sensitive training data. We show how a malicious client can leak the privacy-sensitive data of some other user in FL even without any cooperation from the server. Our best-performing method improves the membership inference recall by 29% and achieves up to 70% private data reconstruction, evidently outperforming existing attacks with stronger assumptions of adversary capabilities.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper identifies that privacy leakage can be aggravated by tampering with a model's selective weights that are specifically responsible for memorizing the sensitive training data, and shows how a malicious client can leak the privacy-sensitive data of some other user in FL even without any cooperation from the server."
            },
            "score": 3
        },
        {
            "id": "437a386f3fe4b8c7449a37e2364412a26e0a478c",
            "paperId": "437a386f3fe4b8c7449a37e2364412a26e0a478c",
            "title": "PolicyGPT: Automated Analysis of Privacy Policies with Large Language Models",
            "abstract": "Privacy policies serve as the primary conduit through which online service providers inform users about their data collection and usage procedures. However, in a bid to be comprehensive and mitigate legal risks, these policy documents are often quite verbose. In practical use, users tend to click the Agree button directly rather than reading them carefully. This practice exposes users to risks of privacy leakage and legal issues. Recently, the advent of Large Language Models (LLM) such as ChatGPT and GPT-4 has opened new possibilities for text analysis, especially for lengthy documents like privacy policies. In this study, we investigate a privacy policy text analysis framework PolicyGPT based on the LLM. This framework was tested using two datasets. The first dataset comprises of privacy policies from 115 websites, which were meticulously annotated by legal experts, categorizing each segment into one of 10 classes. The second dataset consists of privacy policies from 304 popular mobile applications, with each sentence manually annotated and classified into one of another 10 categories. Under zero-shot learning conditions, PolicyGPT demonstrated robust performance. For the first dataset, it achieved an accuracy rate of 97%, while for the second dataset, it attained an 87% accuracy rate, surpassing that of the baseline machine learning and neural network models.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A privacy policy text analysis framework PolicyGPT based on the Large Language Models is investigated, which demonstrated robust performance under zero-shot learning conditions and surpassed that of the baseline machine learning and neural network models."
            },
            "score": 3
        },
        {
            "id": "98fc8faae839cc9b9dbfcf8e3ec3a7e2f7729db2",
            "paperId": "98fc8faae839cc9b9dbfcf8e3ec3a7e2f7729db2",
            "title": "NOTABLE: Transferable Backdoor Attacks Against Prompt-based NLP Models",
            "abstract": "Prompt-based learning is vulnerable to backdoor attacks. Existing backdoor attacks against prompt-based models consider injecting backdoors into the entire embedding layers or word embedding vectors. Such attacks can be easily affected by retraining on downstream tasks and with different prompting strategies, limiting the transferability of backdoor attacks. In this work, we propose transferable backdoor attacks against prompt-based models, called NOTABLE, which is independent of downstream tasks and prompting strategies. Specifically, NOTABLE injects backdoors into the encoders of PLMs by utilizing an adaptive verbalizer to bind triggers to specific words (i.e., anchors). It activates the backdoor by pasting input with triggers to reach adversary-desired anchors, achieving independence from downstream tasks and prompting strategies. We conduct experiments on six NLP tasks, three popular models, and three prompting strategies. Empirical results show that NOTABLE achieves superior attack performance (i.e., attack success rate over 90% on all the datasets), and outperforms two state-of-the-art baselines. Evaluations on three defenses show the robustness of NOTABLE. Our code can be found at https://github.com/RU-System-Software-and-Security/Notable.",
            "year": 2023,
            "citationCount": 19,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes transferable backdoor attacks against prompt-based models, called NOTABLE, which is independent of downstream tasks and prompting strategies, and achieves superior attack performance and outperforms two state-of-the-art baselines."
            },
            "score": 3
        },
        {
            "id": "ff96527c03fbea7c3bb7d44d1d656d875ddba75e",
            "paperId": "ff96527c03fbea7c3bb7d44d1d656d875ddba75e",
            "title": "Dialogue for Prompting: a Policy-Gradient-Based Discrete Prompt Optimization for Few-shot Learning",
            "abstract": "Prompt-based pre-trained language models (PLMs) paradigm have succeeded substantially in few-shot natural language processing (NLP) tasks. However, prior discrete prompt optimization methods require expert knowledge to design the base prompt set and identify high-quality prompts, which is costly, inefficient, and subjective. Meanwhile, existing continuous prompt optimization methods improve the performance by learning the ideal prompts through the gradient information of PLMs, whose high computational cost, and low readability and generalizability are often concerning. To address the research gap, we propose a D ialogue-comprised P olicy-gradient-based D iscrete P rompt O ptimization (DP 2 O) method. We first design a multi-round dialogue alignment strategy for readability prompt set generation based on GPT-4. Furthermore, we propose an efficient prompt screening metric to identify high-quality prompts with linear complexity. Finally, we construct a reinforcement learning (RL) framework based on policy gradients to match the prompts to inputs optimally. By training a policy network with only 0.67% of the PLM parameter size on the tasks in the few-shot setting, DP 2 O outperforms the state-of-the-art (SOTA) method by 1.52% in accuracy on average on four open-source datasets. Moreover, subsequent experiments also demonstrate that DP 2 O has good universality, robustness and generalization ability.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A multi-round dialogue alignment strategy for readability prompt set generation based on GPT-4, an efficient prompt screening metric to identify high-quality prompts with linear complexity, and a reinforcement learning framework based on policy gradients to match the prompts to inputs optimally are proposed."
            },
            "score": 3
        },
        {
            "id": "c7bdb80bba981052d8ed6ae0a965b4886f36ebf9",
            "paperId": "c7bdb80bba981052d8ed6ae0a965b4886f36ebf9",
            "title": "Dialogue for Prompting: A Policy-Gradient-Based Discrete Prompt Generation for Few-Shot Learning",
            "abstract": "Prompt-based pre-trained language models (PLMs) paradigm has succeeded substantially in few-shot natural language processing (NLP) tasks. However, prior discrete prompt optimization methods require expert knowledge to design the base prompt set and identify high-quality prompts, which is costly, inefficient, and subjective. Meanwhile, existing continuous prompt optimization methods improve the performance by learning the ideal prompts through the gradient information of PLMs, whose high computational cost, and low readability and generalizability are often concerning. To address the research gap, we propose a Dialogue-comprised Policy-gradient-based Discrete Prompt Optimization (DP_2O) method. We first design a multi-round dialogue alignment strategy for readability prompt set generation based on GPT-4. Furthermore, we propose an efficient prompt screening metric to identify high-quality prompts with linear complexity. Finally, we construct a reinforcement learning (RL) framework based on policy gradients to match the prompts to inputs optimally. By training a policy network with only 0.62M parameters on the tasks in the few-shot setting, DP_2O outperforms the state-of-the-art (SOTA) method by 1.52% in accuracy on average on four open-source datasets. Moreover, subsequent experiments also demonstrate that DP_2O has good universality, robustness and generalization ability.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a Dialogue-comprised Policy-gradient-based Discrete Prompt Optimization (DP_2O) method, which outperforms the state-of-the-art (SOTA) method by 1.52% in accuracy on average on four open-source datasets and has good universality, robustness and generalization ability."
            },
            "score": 3
        },
        {
            "id": "845356b44c1efa1a5f7a29966a23b2dd4dd03494",
            "paperId": "845356b44c1efa1a5f7a29966a23b2dd4dd03494",
            "title": "CoDa: Constrained Generation based Data Augmentation for Low-Resource NLP",
            "abstract": "We present CoDa (Constrained Generation based Data Augmentation), a controllable, effective, and training-free data augmentation technique for low-resource (data-scarce) NLP. Our approach is based on prompting off-the-shelf instruction-following Large Language Models (LLMs) for generating text that satisfies a set of constraints. Precisely, we extract a set of simple constraints from every instance in the low-resource dataset and verbalize them to prompt an LLM to generate novel and diverse training instances. Our findings reveal that synthetic data that follows simple constraints in the downstream dataset act as highly effective augmentations, and CoDa can achieve this without intricate decoding-time constrained generation techniques or fine-tuning with complex algorithms that eventually make the model biased toward the small number of training instances. Additionally, CoDa is the first framework that provides users explicit control over the augmentation generation process, thereby also allowing easy adaptation to several domains. We demonstrate the effectiveness of CoDa across 11 datasets spanning 3 tasks and 3 low-resource settings. CoDa outperforms all our baselines, qualitatively and quantitatively, with improvements of 0.12%-7.19%. Code is available here: https://github.com/Sreyan88/CoDa",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The findings reveal that synthetic data that follows simple constraints in the downstream dataset act as highly effective augmentations, and CoDa can achieve this without intricate decoding-time constrained generation techniques or fine-tuning with complex algorithms that eventually make the model biased toward the small number of training instances."
            },
            "score": 3
        },
        {
            "id": "6ad26eb2d2aa6679d16d9c16fb75cd2cbe1127bc",
            "paperId": "6ad26eb2d2aa6679d16d9c16fb75cd2cbe1127bc",
            "title": "See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning",
            "abstract": "Large pre-trained vision and language models have demonstrated remarkable capacities for various tasks. However, solving the knowledge-based visual reasoning tasks remains challenging, which requires a model to comprehensively understand image content, connect the external world knowledge, and perform step-by-step reasoning to answer the questions correctly. To this end, we propose a novel framework named Interactive Prompting Visual Reasoner (IPVR) for few-shot knowledge-based visual reasoning. IPVR contains three stages, see, think and confirm. The see stage scans the image and grounds the visual concept candidates with a visual perception model. The think stage adopts a pre-trained large language model (LLM) to attend to the key concepts from candidates adaptively. It then transforms them into text context for prompting with a visual captioning model and adopts the LLM to generate the answer. The confirm stage further uses the LLM to generate the supporting rationale to the answer, verify the generated rationale with a cross-modality classifier and ensure that the rationale can infer the predicted output consistently. We conduct experiments on a range of knowledge-based visual reasoning datasets. We found our IPVR enjoys several benefits, 1). it achieves better performance than the previous few-shot learning baselines; 2). it enjoys the total transparency and trustworthiness of the whole reasoning process by providing rationales for each reasoning step; 3). it is computation-efficient compared with other fine-tuning baselines.",
            "year": 2023,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel framework named Interactive Prompting Visual Reasoner (IPVR), which achieves better performance than the previous few-shot learning baselines, and enjoys the total transparency and trustworthiness of the whole reasoning process by providing rationales for each reasoning step."
            },
            "score": 2
        },
        {
            "id": "64ce6ef1f5cf227bf2bf917c87273386ae16256f",
            "paperId": "64ce6ef1f5cf227bf2bf917c87273386ae16256f",
            "title": "Dictionary-based Phrase-level Prompting of Large Language Models for Machine Translation",
            "abstract": "Large language models (LLMs) demonstrate remarkable machine translation (MT) abilities via prompting, even though they were not explicitly trained for this task. However, even given the incredible quantities of data they are trained on, LLMs can struggle to translate inputs with rare words, which are common in low resource or domain transfer scenarios. We show that LLM prompting can provide an effective solution for rare words as well, by using prior knowledge from bilingual dictionaries to provide control hints in the prompts. We propose a novel method, DiPMT, that provides a set of possible translations for a subset of the input words, thereby enabling fine-grained phrase-level prompted control of the LLM. Extensive experiments show that DiPMT outperforms the baseline both in low-resource MT, as well as for out-of-domain MT. We further provide a qualitative analysis of the benefits and limitations of this approach, including the overall level of controllability that is achieved.",
            "year": 2023,
            "citationCount": 30,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that LLM prompting can provide an effective solution for rare words as well, by using prior knowledge from bilingual dictionaries to provide control hints in the prompts, thereby enabling fine-grained phrase-level prompted control of the LLM."
            },
            "score": 2
        },
        {
            "id": "af1fb33a4eeffe87ffef95d5b8f157c1d8b3f5c0",
            "paperId": "af1fb33a4eeffe87ffef95d5b8f157c1d8b3f5c0",
            "title": "Prompting through Prototype: A Prototype-based Prompt Learning on Pretrained Vision-Language Models",
            "abstract": "Prompt learning is a new learning paradigm which reformulates downstream tasks as similar pretraining tasks on pretrained models by leveraging textual prompts. Recent works have demonstrated that prompt learning is particularly useful for few-shot learning, where there is limited training data. Depending on the granularity of prompts, those methods can be roughly divided into task-level prompting and instance-level prompting. Task-level prompting methods learn one universal prompt for all input samples, which is efficient but ineffective to capture subtle differences among different classes. Instance-level prompting methods learn a specific prompt for each input, though effective but inefficient. In this work, we develop a novel prototype-based prompt learning method to overcome the above limitations. In particular, we focus on few-shot image recognition tasks on pretrained vision-language models (PVLMs) and develop a method of prompting through prototype (PTP), where we define $K$ image prototypes and $K$ prompt prototypes. In PTP, the image prototype represents a centroid of a certain image cluster in the latent space and a prompt prototype is defined as a soft prompt in the continuous space. The similarity between a query image and an image prototype determines how much this prediction relies on the corresponding prompt prototype. Hence, in PTP, similar images will utilize similar prompting ways. Through extensive experiments on seven real-world benchmarks, we show that PTP is an effective method to leverage the latent knowledge and adaptive to various PVLMs. Moreover, through detailed analysis, we discuss pros and cons for prompt learning and parameter-efficient fine-tuning under the context of few-shot learning.",
            "year": 2022,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work focuses on few-shot image recognition tasks on pretrained vision-language models (PVLMs) and develops a method of prompting through prototype (PTP), where the image prototype represents a centroid of a certain image cluster in the latent space and a prompt prototype is defined as a soft prompt in the continuous space."
            },
            "score": 2
        },
        {
            "id": "f80e2aa5177465640a8b406af9d69c52d72c2f78",
            "paperId": "f80e2aa5177465640a8b406af9d69c52d72c2f78",
            "title": "Enhancing chest X-ray datasets with privacy-preserving large language models and multi-type annotations: a data-driven approach for improved classification",
            "abstract": "In chest X-ray (CXR) image analysis, rule-based systems are usually employed to extract labels from reports, but concerns exist about label quality. These datasets typically offer only presence labels, sometimes with binary uncertainty indicators, which limits their usefulness. In this work, we present MAPLEZ (Medical report Annotations with Privacy-preserving Large language model using Expeditious Zero shot answers), a novel approach leveraging a locally executable Large Language Model (LLM) to extract and enhance findings labels on CXR reports. MAPLEZ extracts not only binary labels indicating the presence or absence of a finding but also the location, severity, and radiologists' uncertainty about the finding. Over eight abnormalities from five test sets, we show that our method can extract these annotations with an increase of 5 percentage points (pp) in F1 score for categorical presence annotations and more than 30 pp increase in F1 score for the location annotations over competing labelers. Additionally, using these improved annotations in classification supervision, we demonstrate substantial advancements in model quality, with an increase of 1.7 pp in AUROC over models trained with annotations from the state-of-the-art approach. We share code and annotations.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents MAPLEZ (Medical report Annotations with Privacy-preserving Large language model using Expeditious Zero shot answers), a novel approach leveraging a locally executable Large Language Model (LLM) to extract and enhance findings labels on CXR reports."
            },
            "score": 2
        },
        {
            "id": "10a1df67be1401752d65f0c7fdf80a3bfa686a67",
            "paperId": "10a1df67be1401752d65f0c7fdf80a3bfa686a67",
            "title": "A Synthetic Data Generation Framework for Grounded Dialogues",
            "abstract": "Training grounded response generation models often requires a large collection of grounded dialogues. However, it is costly to build such dialogues. In this paper, we present a synthetic data generation framework (SynDG) for grounded dialogues. The generation process utilizes large pre-trained language models and freely available knowledge data (e.g., Wikipedia pages, persona profiles, etc.). The key idea of designing SynDG is to consider dialogue flow and coherence in the generation process. Specifically, given knowledge data, we first heuristically determine a dialogue flow, which is a series of knowledge pieces. Then, we employ T5 to incrementally turn the dialogue flow into a dialogue. To ensure coherence of both the dialogue flow and the synthetic dialogue, we design a two-level filtering strategy, at the flow-level and the utterance-level respectively. Experiments on two public benchmarks show that the synthetic grounded dialogue data produced by our framework is able to significantly boost model performance in both full training data and low-resource scenarios.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents a synthetic data generation framework (SynDG) for grounded dialogues, and designs a two-level filtering strategy to ensure coherence of both the dialogue flow and the synthetic dialogue."
            },
            "score": 2
        },
        {
            "id": "0ffd57884d7957f6b5634b9fa24843dc3759668f",
            "paperId": "0ffd57884d7957f6b5634b9fa24843dc3759668f",
            "title": "Evaluating Large Language Models in Generating Synthetic HCI Research Data: a Case Study",
            "abstract": "Collecting data is one of the bottlenecks of Human-Computer Interaction (HCI) research. Motivated by this, we explore the potential of large language models (LLMs) in generating synthetic user research data. We use OpenAI\u2019s GPT-3 model to generate open-ended questionnaire responses about experiencing video games as art, a topic not tractable with traditional computational user models. We test whether synthetic responses can be distinguished from real responses, analyze errors of synthetic data, and investigate content similarities between synthetic and real data. We conclude that GPT-3 can, in this context, yield believable accounts of HCI experiences. Given the low cost and high speed of LLM data generation, synthetic data should be useful in ideating and piloting new experiments, although any findings must obviously always be validated with real data. The results also raise concerns: if employed by malicious users of crowdsourcing services, LLMs may make crowdsourcing of self-report data fundamentally unreliable.",
            "year": 2023,
            "citationCount": 72,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is concluded that GPT-3 can, in this context, yield believable accounts of HCI experiences and the low cost and high speed of LLM data generation should be useful in ideating and piloting new experiments, although any findings must obviously always be validated with real data."
            },
            "score": 2
        },
        {
            "id": "73e57768aa115e376cd97542f2f7982a4864c2b3",
            "paperId": "73e57768aa115e376cd97542f2f7982a4864c2b3",
            "title": "Going Beyond Nouns With Vision & Language Models Using Synthetic Data",
            "abstract": "Large-scale pre-trained Vision & Language (VL) models have shown remarkable performance in many applications, enabling replacing a fixed set of supported classes with zero-shot open vocabulary reasoning over (almost arbitrary) natural language prompts. However, recent works have uncovered a fundamental weakness of these models. For example, their difficulty to understand Visual Language Concepts (VLC) that go \u2018beyond nouns\u2019 such as the meaning of non-object words (e.g., attributes, actions, relations, states, etc.), or difficulty in performing compositional reasoning such as understanding the significance of the order of the words in a sentence. In this work, we investigate to which extent purely synthetic data could be leveraged to teach these models to overcome such shortcomings without compromising their zero-shot capabilities. We contribute Synthetic Visual Concepts (SyViC) - a million-scale synthetic dataset and data generation codebase allowing to generate additional suitable data to improve VLC understanding and compositional reasoning of VL models. Additionally, we propose a general VL finetuning strategy for effectively leveraging SyViC towards achieving these improvements. Our extensive experiments and ablations on VL-Checklist, Winoground, and ARO benchmarks demonstrate that it is possible to adapt strong pre-trained VL models with synthetic data significantly enhancing their VLC understanding (e.g. by 9.9% on ARO and 4.3% on VL-Checklist) with under 1% drop in their zero-shot accuracy.",
            "year": 2023,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates to which extent purely synthetic data could be leveraged to teach large-scale pre-trained Vision & Language models to overcome shortcomings without compromising their zero-shot capabilities, and contributes Synthetic Visual Concepts - a million-scale synthetic dataset and data generation codebase allowing to generate additional suitable data to improve VLC understanding and compositional reasoning of VL models."
            },
            "score": 2
        },
        {
            "id": "4c6879725c4134a168d80462a79620e110630978",
            "paperId": "4c6879725c4134a168d80462a79620e110630978",
            "title": "Evaluation Metrics of Language Generation Models for Synthetic Traffic Generation Tasks",
            "abstract": "Many Natural Language Generation (NLG) tasks aim to generate a single output text given an input prompt. Other settings require the generation of multiple texts, e.g., for Synthetic Traffic Generation (STG). This generation task is crucial for training and evaluating QA systems as well as conversational agents, where the goal is to generate multiple questions or utterances resembling the linguistic variability of real users. In this paper, we show that common NLG metrics, like BLEU, are not suitable for evaluating STG. We propose and evaluate several metrics designed to compare the generated traffic to the distribution of real user texts. We validate our metrics with an automatic procedure to verify whether they capture different types of quality issues of generated data; we also run human annotations to verify the correlation with human judgements. Experiments on three tasks, i.e., Shopping Utterance Generation, Product Question Generation and Query Auto Completion, demonstrate that our metrics are effective for evaluating STG tasks, and improve the agreement with human judgement up to 20% with respect to common NLG metrics. We believe these findings can pave the way towards better solutions for estimating the representativeness of synthetic text data.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes and evaluates several metrics designed to compare the generated traffic to the distribution of real user texts and believes these findings can pave the way towards better solutions for estimating the representativeness of synthetic text data."
            },
            "score": 2
        },
        {
            "id": "478ec7a8001d46cde90395c4a9d9ffdec59d5ce3",
            "paperId": "478ec7a8001d46cde90395c4a9d9ffdec59d5ce3",
            "title": "Prompting Language Models for Linguistic Structure",
            "abstract": "Although pretrained language models (PLMs) can be prompted to perform a wide range of language tasks, it remains an open question how much this ability comes from generalizable linguistic understanding versus surface-level lexical patterns. To test this, we present a structured prompting approach for linguistic structured prediction tasks, allowing us to perform zero- and few-shot sequence tagging with autoregressive PLMs. We evaluate this approach on part-of-speech tagging, named entity recognition, and sentence chunking, demonstrating strong few-shot performance in all cases. We also find that while PLMs contain significant prior knowledge of task labels due to task leakage into the pretraining corpus, structured prompting can also retrieve linguistic structure with arbitrary labels. These findings indicate that the in-context learning ability and linguistic knowledge of PLMs generalizes beyond memorization of their training data.",
            "year": 2022,
            "citationCount": 21,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that while PLMs contain significant prior knowledge of task labels due to task leakage into the pretraining corpus, structured prompting can also retrieve linguistic structure with arbitrary labels, indicating that the in-context learning ability and linguistic knowledge of PLMs generalizes beyond memorization of their training data."
            },
            "score": 2
        },
        {
            "id": "dfab0f3ee6f47e36cccee145794cd117773e6f73",
            "paperId": "dfab0f3ee6f47e36cccee145794cd117773e6f73",
            "title": "Towards LLM-based Fact Verification on News Claims with a Hierarchical Step-by-Step Prompting Method",
            "abstract": "While large pre-trained language models (LLMs) have shown their impressive capabilities in various NLP tasks, they are still under-explored in the misinformation domain. In this paper, we examine LLMs with in-context learning (ICL) for news claim verification, and find that only with 4-shot demonstration examples, the performance of several prompting methods can be comparable with previous supervised models. To further boost performance, we introduce a Hierarchical Step-by-Step (HiSS) prompting method which directs LLMs to separate a claim into several subclaims and then verify each of them via multiple questions-answering steps progressively. Experiment results on two public misinformation datasets show that HiSS prompting outperforms state-of-the-art fully-supervised approach and strong few-shot ICL-enabled baselines.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Hierarchical Step-by-Step (HiSS) prompting method is introduced which directs LLMs to separate a claim into several subclaims and then verify each of them via multiple questions-answering steps progressively."
            },
            "score": 2
        },
        {
            "id": "2522410b1cac0c14fa656a0aaeaff08bacb358a9",
            "paperId": "2522410b1cac0c14fa656a0aaeaff08bacb358a9",
            "title": "InterroLang: Exploring NLP Models and Datasets through Dialogue-based Explanations",
            "abstract": "While recently developed NLP explainability methods let us open the black box in various ways (Madsen et al., 2022), a missing ingredient in this endeavor is an interactive tool offering a conversational interface. Such a dialogue system can help users explore datasets and models with explanations in a contextualized manner, e.g. via clarification or follow-up questions, and through a natural language interface. We adapt the conversational explanation framework TalkToModel (Slack et al., 2022) to the NLP domain, add new NLP-specific operations such as free-text rationalization, and illustrate its generalizability on three NLP tasks (dialogue act classification, question answering, hate speech detection). To recognize user queries for explanations, we evaluate fine-tuned and few-shot prompting models and implement a novel Adapter-based approach. We then conduct two user studies on (1) the perceived correctness and helpfulness of the dialogues, and (2) the simulatability, i.e. how objectively helpful dialogical explanations are for humans in figuring out the model's predicted label when it's not shown. We found rationalization and feature attribution were helpful in explaining the model behavior. Moreover, users could more reliably predict the model outcome based on an explanation dialogue rather than one-off explanations.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work adapts the conversational explanation framework TalkToModel to the NLP domain, adds new NLP-specific operations such as free-text rationalization and feature attribution, and illustrates its generalizability on three NLP tasks."
            },
            "score": 2
        },
        {
            "id": "f4c4e148546089123f8da5db4fb246ab4062bd40",
            "paperId": "f4c4e148546089123f8da5db4fb246ab4062bd40",
            "title": "Evaluation of ChatGPT for NLP-based Mental Health Applications",
            "abstract": "Large language models (LLM) have been successful in several natural language understanding tasks and could be relevant for natural language processing (NLP)-based mental health application research. In this work, we report the performance of LLM-based ChatGPT (with gpt-3.5-turbo backend) in three text-based mental health classification tasks: stress detection (2-class classification), depression detection (2-class classification), and suicidality detection (5-class classification). We obtained annotated social media posts for the three classification tasks from public datasets. Then ChatGPT API classified the social media posts with an input prompt for classification. We obtained F1 scores of 0.73, 0.86, and 0.37 for stress detection, depression detection, and suicidality detection, respectively. A baseline model that always predicted the dominant class resulted in F1 scores of 0.35, 0.60, and 0.19. The zero-shot classification accuracy obtained with ChatGPT indicates a potential use of language models for mental health classification tasks.",
            "year": 2023,
            "citationCount": 38,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The zero-shot classification accuracy obtained with ChatGPT indicates a potential use of language models for mental health classification tasks, and large language models have been successful in several natural language understanding tasks."
            },
            "score": 2
        },
        {
            "id": "299d61246991fb1add9d44f737c71a5c0535585e",
            "paperId": "299d61246991fb1add9d44f737c71a5c0535585e",
            "title": "Parallel Learning for Legal Intelligence: A HANOI Approach Based on Unified Prompting",
            "abstract": "Pretrained language models (PLMs) have made significant progress on various NLP tasks recently. However, PLMs encounter challenges when it comes to domain-specific tasks such as legal AI. These tasks often involve intricate expertise, expensive data annotation, and limited training data availability. To tackle this problem, we propose a human-oriented artificial\u2013natural parallel system for organized intelligence (HANOI)-Legal based on the parallel learning (PL) framework. First, by regarding the description in PL as the pretraining process based on a large-scale corpus, we setup an artificial system based on a PLM. Second, to adapt the PLM to legal tasks with limited resources, we propose UniPrompt as a prescription. UniPrompt serves as a unified prompt-based training framework, enabling the utilization of diverse open datasets for these tasks. Third, we labeled a few task-specific legal data through distributed autonomous operations (DAO-II) for further fine-tuning. By combining a scalable unified-task-format reformulation and a unified-prompt-based training pipeline, HANOI-Legal leverages PLMs\u2019 linguistic capabilities acquired from a variety of open datasets to generate task-specific models. Our experiments in two legal domain tasks show that HANOI-Legal achieved an excellent performance in low-resource scenarios compared to the state-of-the-art prompt-based approach.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A human-oriented artificial\u2013natural parallel system for organized intelligence (HANOI)-Legal based on the parallel learning (PL) framework, which leverages PLMs\u2019 linguistic capabilities acquired from a variety of open datasets to generate task-specific models and achieves an excellent performance in low-resource scenarios."
            },
            "score": 2
        },
        {
            "id": "cbd81507197e2d32b6f6c8ac99039f3a607ee8f1",
            "paperId": "cbd81507197e2d32b6f6c8ac99039f3a607ee8f1",
            "title": "VerilogEval: Evaluating Large Language Models for Verilog Code Generation",
            "abstract": "The increasing popularity of large language models (LLMs) has paved the way for their application in diverse domains. This paper proposes a benchmarking framework tailored specifically for evaluating LLM performance in the context of Verilog code generation for hardware design and verification. We present a comprehensive evaluation dataset consisting of 156 problems from the Verilog instructional website HDLBits. The evaluation set consists of a diverse set of Verilog code generation tasks, ranging from simple combinational circuits to complex finite state machines. The Verilog code completions can be automatically tested for functional correctness by comparing the transient simulation outputs of the generated design with a golden solution. We also demonstrate that the Verilog code generation capability of pretrained language models could be improved with supervised fine-tuning by bootstrapping with LLM generated synthetic problem-code pairs.",
            "year": 2023,
            "citationCount": 20,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A benchmarking framework tailored specifically for evaluating LLM performance in the context of Verilog code generation for hardware design and verification and it is demonstrated that the Verilogs code generation capability of pretrained language models could be improved with supervised fine-tuning by bootstrapping with LLM generated synthetic problem-code pairs."
            },
            "score": 1
        },
        {
            "id": "e10c2ed8de5568e6a41fb60c2adc4bf9c225da25",
            "paperId": "e10c2ed8de5568e6a41fb60c2adc4bf9c225da25",
            "title": "Is EVALITA Done? On the Impact of Prompting on the Italian NLP Evaluation Campaign",
            "abstract": "Prompt-based learning is a recent paradigm in NLP that leverages large pre-trained language models to perform a variety of tasks. With this technique, it is possible to build classifiers that do not need training data (zero-shot). In this paper, we assess the status of prompt-based learning applied to several text classification tasks in the Italian language. The results indicate that the performance gap towards current supervised methods is still relevant. However, the difference in performance between pre-trained models and the characteristic of the prompt-based classifier of operating in a zero-shot fashion open a discussion regarding the next generation of evaluation campaigns for NLP.",
            "year": 2022,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results indicate that the performance gap towards current supervised methods is still relevant, however, the difference in performance between pre-trained models and the characteristic of the prompt-based classifier of operating in a zero-shot fashion open a discussion regarding the next generation of evaluation campaigns for NLP."
            },
            "score": 1
        },
        {
            "id": "a3509cef906a4517238c1764676cf637efcd1d5e",
            "paperId": "a3509cef906a4517238c1764676cf637efcd1d5e",
            "title": "Copilot for Xcode: Exploring AI-Assisted Programming by Prompting Cloud-based Large Language Models",
            "abstract": "This paper presents an AI-assisted programming tool called Copilot for Xcode for program composition and design to support human software developers. By seamlessly integrating cloud-based Large Language Models (LLM) with Apple's local development environment, Xcode, this tool enhances productivity and unleashes creativity for software development in Apple software ecosystem (e.g., iOS apps, macOS). Leveraging advanced natural language processing (NLP) techniques, Copilot for Xcode effectively processes source code tokens and patterns within code repositories, enabling features such as code generation, autocompletion, documentation, and error detection. Software developers can also query and make\"small\"decisions for program composition, some of which can be made simultaneously, and this is facilitated through prompt engineering in a chat interface of Copilot for Xcode. Finally, we present simple case studies as evidence of the effectiveness of utilizing NLP in Xcode to prompt popular LLM services like OpenAI ChatGPT for program composition and design.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An AI-assisted programming tool called Copilot for Xcode for program composition and design to support human software developers, seamlessly integrating cloud-based Large Language Models with Apple's local development environment, Xcode, enhances productivity and unleashes creativity for software development in Apple software ecosystem."
            },
            "score": 1
        }
    ],
    "novelty": "yes"
}