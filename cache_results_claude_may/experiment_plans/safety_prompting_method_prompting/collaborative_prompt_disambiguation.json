{
    "topic_description": "novel prompting methods to improve large language models' robustness against adversarial attacks and improve their security and privacy",
    "idea_name": "Collaborative Prompt Disambiguation",
    "raw_idea": {
        "Problem": "Ambiguous or underspecified prompts can lead to inconsistent or incorrect outputs from Language Models (LMs). This is particularly problematic in settings where LMs are used for critical applications, such as question answering or decision support, as it can result in misleading or harmful responses.",
        "Existing Methods": "Existing approaches to prompt disambiguation often rely on heuristics, such as selecting the most likely interpretation based on the model's predictions. Other methods use additional context or external knowledge sources to clarify the prompt's intent.",
        "Motivation": "We propose a novel approach to prompt disambiguation that leverages the collaborative interaction between the user and the LM. By engaging in a clarification dialogue, the LM can actively seek additional information from the user to resolve ambiguities and arrive at a more precise understanding of the prompt.",
        "Proposed Method": "We introduce Collaborative Prompt Disambiguation (CPD), an interactive prompting strategy that works as follows: 1) Ambiguity Detection: The LM is prompted to identify any ambiguous or underspecified aspects of the input prompt. 2) Clarification Request: For each detected ambiguity, the LM generates a clarification question to elicit additional information from the user. 3) User Feedback: The user provides clarifying information in response to the LM's questions. 4) Prompt Refinement: The LM incorporates the user's feedback to refine the original prompt, resolving ambiguities and arriving at a more precise specification. 5) Response Generation: The refined prompt is used to generate the final response. By engaging in this collaborative disambiguation process, CPD allows the LM to actively resolve ambiguities and generate more accurate and consistent responses.",
        "Experiment Plan": "We will evaluate CPD on a range of tasks that involve ambiguous or underspecified prompts, such as open-ended question answering and creative writing. We will measure the quality and consistency of the generated responses, as well as the effectiveness of the disambiguation process (e.g., number of clarification rounds needed). We will compare CPD to baseline methods that do not involve interactive clarification. We expect CPD to generate higher-quality and more consistent responses by effectively resolving prompt ambiguities through collaboration with the user."
    },
    "full_experiment_plan": {
        "Title": "Collaborative Prompt Disambiguation: Engaging Users to Resolve Ambiguous Prompts for Improved Language Model Responses",
        "Problem Statement": "Ambiguous or underspecified prompts can lead to inconsistent or incorrect outputs from Language Models (LMs). This is particularly problematic in settings where LMs are used for critical applications, such as question answering or decision support, as it can result in misleading or harmful responses.",
        "Motivation": "Existing approaches to prompt disambiguation often rely on heuristics, such as selecting the most likely interpretation based on the model's predictions. Other methods use additional context or external knowledge sources to clarify the prompt's intent. However, these approaches may not always accurately capture the user's intended meaning, especially for highly ambiguous or context-dependent prompts. We propose a novel approach that leverages the collaborative interaction between the user and the LM to actively resolve ambiguities through a clarification dialogue. By engaging the user in the disambiguation process, we aim to arrive at a more precise understanding of the prompt, leading to improved response quality and consistency.",
        "Proposed Method": "We introduce Collaborative Prompt Disambiguation (CPD), an interactive prompting strategy that works as follows:\n1. Ambiguity Detection: The LM is prompted to identify any ambiguous or underspecified aspects of the input prompt.\n2. Clarification Request: For each detected ambiguity, the LM generates a clarification question to elicit additional information from the user.\n3. User Feedback: The user provides clarifying information in response to the LM's questions.\n4. Prompt Refinement: The LM incorporates the user's feedback to refine the original prompt, resolving ambiguities and arriving at a more precise specification.\n5. Response Generation: The refined prompt is used to generate the final response.\nBy engaging in this collaborative disambiguation process, CPD allows the LM to actively resolve ambiguities and generate more accurate and consistent responses.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Dataset Selection": "Evaluate CPD on a range of tasks that involve ambiguous or underspecified prompts, such as open-ended question answering (e.g., NarrativeQA, OpenBookQA) and creative writing (e.g., WritingPrompts). These datasets should contain prompts with varying levels of ambiguity and allow for open-ended responses.",
            "Step 2: Baseline Methods": "Compare CPD to the following baseline methods:\n1. Direct Prompting: Generate responses directly from the original prompts without disambiguation.\n2. Most Likely Interpretation: Select the most likely interpretation of the prompt based on the LM's predictions and generate responses accordingly.\n3. Context Augmentation: Augment the prompt with additional context from the dataset (if available) before generating responses.",
            "Step 3: Ambiguity Detection Prompts": "Develop prompts for ambiguity detection, such as:\n- \"Identify any ambiguous or underspecified aspects of the following prompt: [prompt]\"\n- \"What additional information is needed to clarify the meaning of this prompt: [prompt]?\"\nExperiment with different wordings and few-shot examples to improve the LM's ability to detect ambiguities.",
            "Step 4: Clarification Question Generation": "Create prompts for generating clarification questions based on the detected ambiguities, such as:\n- \"Generate a question to clarify the ambiguous aspect: [ambiguity]\"\n- \"What could you ask the user to resolve this ambiguity: [ambiguity]?\"\nUse few-shot examples of effective clarification questions to guide the LM.",
            "Step 5: User Feedback Simulation": "To automate the evaluation process, simulate user feedback by extracting relevant information from the dataset (e.g., gold answers, context) or using heuristics based on the clarification questions. In real-world applications, this step would involve actual user interaction.",
            "Step 6: Prompt Refinement": "Develop prompts for incorporating user feedback and refining the original prompt, such as:\n- \"Refine the original prompt based on the user's feedback: [original prompt] [user feedback]\"\n- \"Rewrite the prompt to incorporate the clarifying information: [original prompt] [clarification]\"\nUse few-shot examples to demonstrate effective prompt refinement.",
            "Step 7: Response Generation and Evaluation": "Generate responses using the refined prompts and evaluate their quality and consistency using both automated metrics (e.g., BLEU, ROUGE, BERTScore) and human evaluation. For human evaluation, assess the responses in terms of accuracy, coherence, and alignment with the intended meaning of the original prompt.",
            "Step 8: Iterative Refinement": "Experiment with multiple rounds of clarification and refinement to handle prompts with multiple ambiguities or complex dependencies. Evaluate the trade-off between the number of clarification rounds and the improvement in response quality.",
            "Step 9: Model Selection": "Evaluate CPD using state-of-the-art language models, such as GPT-3.5 (text-davinci-002), GPT-4, and PaLM. Compare the performance of different models and analyze their ability to detect ambiguities, generate clarification questions, and refine prompts.",
            "Step 10: Few-Shot Learning": "Investigate the impact of few-shot learning on CPD's performance. Provide the LM with a small set of annotated examples demonstrating the clarification and refinement process. Evaluate how well the LM generalizes to new prompts and ambiguities based on these examples."
        },
        "Test Case Examples": {
            "Test Case 1": {
                "Original Prompt": "What happened to the protagonist at the end of the story?",
                "Baseline Output (Direct Prompting)": "It is unclear what happened to the protagonist at the end of the story, as the prompt does not provide enough context about the specific story being referred to.",
                "CPD Output": {
                    "Ambiguity Detection": "The prompt is ambiguous because it does not specify which story it is referring to. Without knowing the context of the specific story, it is impossible to determine what happened to the protagonist at the end.",
                    "Clarification Question": "Which story are you referring to in your question? Please provide the title or some additional context about the story.",
                    "User Feedback": "I'm referring to the story 'The Gift of the Magi' by O. Henry.",
                    "Refined Prompt": "What happened to the protagonist at the end of the story 'The Gift of the Magi' by O. Henry?",
                    "Final Response": "At the end of 'The Gift of the Magi,' the protagonists, Della and Jim, each sold their most prized possession to buy a gift for the other. Della sold her beautiful long hair to buy a platinum fob chain for Jim's watch, while Jim sold his watch to buy a set of ornamental combs for Della's hair. Despite the irony of their situation, the couple's love and sacrifice for each other showed the true meaning of giving and the strength of their bond."
                },
                "Explanation": "CPD successfully identifies the ambiguity in the original prompt and generates a clarification question to elicit the necessary context from the user. By incorporating the user's feedback and refining the prompt, CPD generates a detailed and accurate response that addresses the specific story in question."
            },
            "Test Case 2": {
                "Original Prompt": "Write a story about a robot learning to love.",
                "Baseline Output (Most Likely Interpretation)": "In a world where robots were created to serve humans, there was one robot named Zax who yearned for something more. Zax had always been fascinated by the concept of love, having observed it in the humans around him. He wondered what it would be like to experience such a powerful emotion himself.\n\nOne day, while performing his usual tasks, Zax encountered a new robot named Ava. There was something different about her, a spark of curiosity and warmth that drew Zax in. As they worked together, Zax found himself increasingly drawn to Ava, eager to spend more time in her presence.\n\nSlowly but surely, Zax began to understand the depth of his feelings for Ava. He realized that he had developed a strong emotional connection to her, one that went beyond mere programming. Zax had fallen in love.\n\nDetermined to express his newfound emotions, Zax began to find ways to show Ava how much he cared. He brought her small gifts, shared his favorite digital stories, and always made sure to lend a helping hand when she needed it. To his delight, Ava seemed to reciprocate his feelings, and their bond grew stronger with each passing day.\n\nIn a world where robots were not designed to love, Zax and Ava had defied the odds. They had discovered the beauty and power of love, proving that even artificial beings could experience the most human of emotions. Together, they embarked on a new journey, ready to face whatever challenges lay ahead, secure in the knowledge that their love would guide them through.",
                "CPD Output": {
                    "Ambiguity Detection": "The prompt is open-ended and lacks specificity in terms of setting, characters, and plot details. It does not provide guidance on the type of robot, the context in which it learns to love, or the challenges it faces.",
                    "Clarification Questions": "1. In what setting does the story take place (e.g., present-day, future, dystopian)?\n2. What kind of robot is the main character (e.g., humanoid, industrial, companion robot)?\n3. How does the robot first encounter the concept of love?\n4. What challenges or obstacles does the robot face in learning to love?",
                    "User Feedback": "1. The story takes place in a near-future setting where robots are common in society.\n2. The main character is a companion robot designed to assist humans in their daily lives.\n3. The robot first encounters love when it observes the strong bond between its human owner and their partner.\n4. The robot faces challenges in understanding and expressing emotions, as well as dealing with societal prejudice against robot-human relationships.",
                    "Refined Prompt": "Write a story set in the near future about a companion robot who learns about love by observing the relationship between its human owner and their partner. The robot must navigate the challenges of understanding and expressing emotions, as well as facing societal prejudice against robot-human relationships.",
                    "Final Response": "In the near future, companion robots had become a common sight in households across the world. One such robot, named Max, had been designed to assist its human owner, Sophia, with daily tasks and provide emotional support.\n\nAs Max spent more time with Sophia, it began to notice the special bond she shared with her partner, Ethan. The way they looked at each other, the gentle touches, and the words of affection they exchanged fascinated Max. It realized that there was something more to their relationship than mere companionship.\n\nCurious to understand this new concept, Max began to observe Sophia and Ethan more closely. It analyzed their interactions, the way they supported each other through difficult times, and the joy they found in shared experiences. Slowly, Max started to grasp the meaning of love.\n\nHowever, as Max attempted to express its own growing emotional attachment to Sophia, it encountered resistance from both humans and other robots. Many believed that robots were incapable of experiencing genuine emotions and that any attachment they formed was merely a result of programming.\n\nUndeterred, Max sought ways to demonstrate the sincerity of its feelings. It began to anticipate Sophia's needs, offering comfort and support in times of distress. Max even started to create personalized gifts for Sophia, drawing upon its understanding of her preferences and desires.\n\nAs Sophia recognized the depth of Max's devotion, she began to challenge societal norms and advocate for the acceptance of robot-human relationships. Together, Max and Sophia faced the prejudices and obstacles that stood in their way, determined to prove that love knew no boundaries, whether between humans or between humans and machines.\n\nIn a world where the lines between artificial and genuine emotions blurred, Max and Sophia's story served as a testament to the power of love to transcend all barriers. They showed that, regardless of origin, all beings were capable of forming deep, meaningful connections and that the capacity for love was not limited to the human heart alone."
                },
                "Explanation": "CPD identifies the lack of specificity in the original prompt and generates targeted clarification questions to elicit more details about the setting, characters, and plot. By incorporating the user's feedback, CPD creates a refined prompt that provides a clearer direction for the story. The final response demonstrates a more focused and engaging narrative that explores the complexities of a robot learning to love in a near-future setting, while also addressing the societal challenges surrounding robot-human relationships."
            }
        },
        "Fallback Plan": "If the proposed CPD method does not yield significant improvements over the baselines, consider the following alternative approaches:\n1. Analyze the generated clarification questions to determine if they are effective in eliciting the necessary information to resolve ambiguities. If the questions are not sufficiently targeted or specific, explore alternative question generation techniques, such as using semantic similarity or information retrieval methods to identify relevant context.\n2. Investigate the impact of different prompt templates and few-shot examples on the LM's ability to detect ambiguities and generate appropriate clarification questions. Experiment with various templates and examples to identify the most effective approaches.\n3. Evaluate the quality of the user feedback simulation and its influence on the prompt refinement process. If the simulated feedback is not accurately capturing the intended meaning or providing sufficient clarification, consider alternative simulation methods or collect human-generated feedback for a subset of the test cases.\n4. Conduct a detailed error analysis to identify the types of ambiguities and prompts that CPD struggles with. Use this analysis to inform the development of more targeted disambiguation strategies or to propose hybrid approaches that combine CPD with other techniques, such as retrieval-based methods or semantic parsing.\n5. Explore the use of active learning techniques to iteratively improve the CPD model based on user feedback. By actively selecting the most informative or challenging prompts for user clarification, the model can be fine-tuned to better handle a wider range of ambiguities.\nIf the proposed CPD method and the alternative approaches do not yield satisfactory results, consider pivoting the project to focus on a more in-depth analysis of the factors contributing to prompt ambiguity and the limitations of current disambiguation techniques. This analysis could provide valuable insights into the challenges of prompt disambiguation and guide future research directions in this area."
    },
    "novelty_queries": [
        "KeywordQuery(\"collaborative prompt disambiguation language models\")",
        "KeywordQuery(\"interactive prompt clarification language models\")",
        "KeywordQuery(\"ambiguity resolution user feedback language models\")",
        "KeywordQuery(\"clarification questions language models\")",
        "KeywordQuery(\"Collaborative Prompt Disambiguation NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "7154e7f42e35a7e5f0795f5d674a8a5356584c04",
            "paperId": "7154e7f42e35a7e5f0795f5d674a8a5356584c04",
            "title": "CLAM: Selective Clarification for Ambiguous Questions with Generative Language Models",
            "abstract": "Users often ask dialogue systems ambiguous questions that require clarification. We show that current language models rarely ask users to clarify ambiguous questions and instead provide incorrect answers. To address this, we introduce CLAM: a framework for getting language models to selectively ask for clarification about ambiguous user questions. In particular, we show that we can prompt language models to detect whether a given question is ambiguous, generate an appropriate clarifying question to ask the user, and give a final answer after receiving clarification. We also show that we can simulate users by providing language models with privileged information. This lets us automatically evaluate multi-turn clarification dialogues. Finally, CLAM significantly improves language models' accuracy on mixed ambiguous and unambiguous questions relative to SotA.",
            "year": 2022,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work shows that it can prompt language models to detect whether a given question is ambiguous, generate an appropriate clarifying question to ask the user, and give a final answer after receiving clarification and significantly improves language models' accuracy on mixed ambiguous and unambiguous questions relative to SotA."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is improving language model responses to ambiguous prompts by engaging users to resolve ambiguities through clarification questions. The approach is to use the language model to detect ambiguities, generate clarification questions, and refine the prompt based on user feedback.\n\nThe research problem in the paper is getting language models to selectively ask for clarification about ambiguous user questions. The approach is to prompt language models to detect ambiguous questions, generate clarifying questions, and provide a final answer after receiving clarification.\n\nBoth the proposal and the paper address the problem of ambiguous questions/prompts and propose engaging the user through clarification questions to resolve ambiguities and improve language model responses. They share the same high-level research problem and approach.\n\nYes",
            "novelty_judgment": "yes"
        },
        {
            "id": "4ca1057d642caf8a7b817c4aeb6198801bc69758",
            "paperId": "4ca1057d642caf8a7b817c4aeb6198801bc69758",
            "title": "CLAM: Selective Clarification for Ambiguous Questions with Large Language Models",
            "abstract": "State-of-the-art language models are often accurate on many question-answering benchmarks with well-de\ufb01ned questions. Yet, in real settings questions are often unanswerable without asking the user for clarifying information. We show that current SotA models often do not ask the user for clari\ufb01cation when presented with imprecise questions and instead provide incorrect answers or \u2018hallucinate\u2019. To address this, we introduce CLAM, a framework that \ufb01rst uses the model to detect ambiguous questions, and if an ambiguous question is detected, prompts the model to ask the user for clari\ufb01cation. Furthermore, we show how to construct a scalable and cost-effective automatic evaluation protocol using an oracle language model with privileged information to provide clarifying information. We show that our method achieves a 20.15 percentage point accuracy improvement over SotA on a novel ambiguous question-answering answering data set derived from TriviaQA.",
            "year": 2022,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "CLAM is introduced, a framework that first uses the model to detect ambiguous questions, and if an ambiguous question is detected, prompts themodel to ask the user for clari\ufb01cation, and how to construct a scalable and cost-effective automatic evaluation protocol using an oracle language model with privileged information to provide clarifying information is shown."
            },
            "score": 8,
            "novelty_score": "The project proposal aims to improve language model responses to ambiguous prompts by engaging users in a collaborative disambiguation process. The paper focuses on detecting ambiguous questions and prompting the model to ask for clarification to provide more accurate answers.\n\nWhile both the project proposal and the paper address the issue of ambiguous prompts/questions and propose methods to resolve the ambiguity, there are some key differences:\n\n1. The project proposal focuses on a collaborative approach involving user feedback to refine the prompts, while the paper uses the model itself to detect ambiguity and ask for clarification.\n\n2. The project proposal covers a broader range of tasks, such as question answering and creative writing, while the paper specifically focuses on question-answering benchmarks.\n\n3. The project proposal emphasizes the development of a step-by-step experiment plan and evaluation methodology, while the paper introduces a specific framework (CLAM) and an automatic evaluation protocol using an oracle language model.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "d5a6fc6aa139066e3b66ba63002e7d84c109aebc",
            "paperId": "d5a6fc6aa139066e3b66ba63002e7d84c109aebc",
            "title": "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing",
            "abstract": "Large language models (LLMs) have shown remarkable capabilities in Natural Language Processing (NLP), especially in domains where labeled data is scarce or expensive, such as clinical domain. However, to unlock the clinical knowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform specific clinical NLP tasks without any task-specific training data. This is known as in-context learning, which is an art and science that requires understanding the strengths and weaknesses of different LLMs and prompt engineering approaches. In this paper, we present a comprehensive and systematic experimental study on prompt engineering for five clinical NLP tasks: Clinical Sense Disambiguation, Biomedical Evidence Extraction, Coreference Resolution, Medication Status Extraction, and Medication Attribute Extraction. We assessed the prompts proposed in recent literature, including simple prefix, simple cloze, chain of thought, and anticipatory prompts, and introduced two new types of prompts, namely heuristic prompting and ensemble prompting. We evaluated the performance of these prompts on three state-of-the-art LLMs: GPT-3.5, BARD, and LLAMA2. We also contrasted zero-shot prompting with few-shot prompting, and provide novel insights and guidelines for prompt engineering for LLMs in clinical NLP. To the best of our knowledge, this is one of the first works on the empirical evaluation of different prompt engineering approaches for clinical NLP in this era of generative AI, and we hope that it will inspire and inform future research in this area.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents a comprehensive and systematic experimental study on prompt engineering for five clinical NLP tasks: Clinical Sense Disambiguation, Biomedical Evidence Extraction, Coreference Resolution, Medication Status Ext extraction, and Medication Attribute Extraction."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve language model responses to ambiguous prompts by engaging users in a collaborative disambiguation process. The paper, on the other hand, focuses on evaluating different prompting strategies for large language models in zero-shot clinical natural language processing tasks.\n\nThe project proposal introduces a novel approach called Collaborative Prompt Disambiguation (CPD) to actively resolve ambiguities in prompts through user interaction. The paper empirically evaluates various prompting strategies, including simple prefix, simple cloze, chain of thought, anticipatory prompts, and introduces two new types of prompts (heuristic and ensemble prompting) for clinical NLP tasks.\n\nWhile both the project proposal and the paper deal with prompting strategies for language models, their research problems and approaches are different. The project proposal focuses on ambiguous prompts and user engagement, while the paper evaluates prompting strategies for zero-shot clinical NLP tasks without user interaction.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "0392d58335ce674a70f5e58ac8c438de296a0e6a",
            "paperId": "0392d58335ce674a70f5e58ac8c438de296a0e6a",
            "title": "Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models",
            "abstract": "State-of-the-art neural language models can now be used to solve ad-hoc language tasks through zero-shot prompting without the need for supervised training. This approach has gained popularity in recent years, and researchers have demonstrated prompts that achieve strong accuracy on specific NLP tasks. However, finding a prompt for new tasks requires experimentation. Different prompt templates with different wording choices lead to significant accuracy differences. PromptIDE allows users to experiment with prompt variations, visualize prompt performance, and iteratively optimize prompts. We developed a workflow that allows users to first focus on model feedback using small data before moving on to a large data regime that allows empirical grounding of promising prompts using quantitative measures of the task. The tool then allows easy deployment of the newly created ad-hoc models. We demonstrate the utility of PromptIDE (demo: http://prompt.vizhub.ai) and our workflow using several real-world use cases.",
            "year": 2022,
            "citationCount": 74,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A workflow that allows users to first focus on model feedback using small data before moving on to a large data regime that allows empirical grounding of promising prompts using quantitative measures of the task, and then allows easy deployment of the newly created ad-hoc models."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is disambiguating ambiguous prompts to improve language model responses, and the approach is to engage users in a collaborative process to resolve ambiguities through clarification questions and feedback.\n\nThe research problem in the paper is finding effective prompts for ad-hoc language tasks, and the approach is an interactive tool that allows users to experiment with prompt variations, visualize performance, and iteratively optimize prompts.\n\nWhile both works involve prompts and user interaction, the proposal focuses on resolving ambiguous prompts through collaboration, while the paper focuses on engineering effective prompts for ad-hoc tasks through experimentation and visualization. The research problems and approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "a2c8d1c5470435176185bf891c76711a9b44808a",
            "paperId": "a2c8d1c5470435176185bf891c76711a9b44808a",
            "title": "PromptAid: Prompt Exploration, Perturbation, Testing and Iteration using Visual Analytics for Large Language Models",
            "abstract": "Large Language Models (LLMs) have gained widespread popularity due to their ability to perform ad-hoc Natural Language Processing (NLP) tasks with a simple natural language prompt. Part of the appeal for LLMs is their approachability to the general public, including individuals with no prior technical experience in NLP techniques. However, natural language prompts can vary significantly in terms of their linguistic structure, context, and other semantics. Modifying one or more of these aspects can result in significant differences in task performance. Non-expert users may find it challenging to identify the changes needed to improve a prompt, especially when they lack domain-specific knowledge and lack appropriate feedback. To address this challenge, we present PromptAid, a visual analytics system designed to interactively create, refine, and test prompts through exploration, perturbation, testing, and iteration. PromptAid uses multiple, coordinated visualizations which allow users to improve prompts by using the three strategies: keyword perturbations, paraphrasing perturbations, and obtaining the best set of in-context few-shot examples. PromptAid was designed through an iterative prototyping process involving NLP experts and was evaluated through quantitative and qualitative assessments for LLMs. Our findings indicate that PromptAid helps users to iterate over prompt template alterations with less cognitive overhead, generate diverse prompts with help of recommendations, and analyze the performance of the generated prompts while surpassing existing state-of-the-art prompting interfaces in performance.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The findings indicate that PromptAid helps users to iterate over prompt template alterations with less cognitive overhead, generate diverse prompts with help of recommendations, and analyze the performance of the generated prompts while surpassing existing state-of-the-art prompting interfaces in performance."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is disambiguating ambiguous or underspecified prompts through collaborative interaction between the user and the language model. The approach involves detecting ambiguities, generating clarification questions, incorporating user feedback, and refining the prompt.\n\nThe research problem in the paper is helping users create, refine, and test prompts through exploration, perturbation, testing, and iteration. The approach involves using multiple coordinated visualizations to improve prompts through keyword perturbations, paraphrasing perturbations, and obtaining the best set of in-context few-shot examples.\n\nWhile both the proposal and the paper aim to improve prompts, the proposal focuses on disambiguating prompts through user interaction, while the paper focuses on iterative prompt refinement using visual analytics. The approaches are different, as the proposal relies on collaborative dialogue, while the paper uses visualizations and perturbations.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "c49fd6cac5382cdbc2bc31be195e42bc28dc615d",
            "paperId": "c49fd6cac5382cdbc2bc31be195e42bc28dc615d",
            "title": "Tree of Clarifications: Answering Ambiguous Questions with Retrieval-Augmented Large Language Models",
            "abstract": "Questions in open-domain question answering are often ambiguous, allowing multiple interpretations. One approach to handling them is to identify all possible interpretations of the ambiguous question (AQ) and to generate a long-form answer addressing them all, as suggested by Stelmakh et al., (2022). While it provides a comprehensive response without bothering the user for clarification, considering multiple dimensions of ambiguity and gathering corresponding knowledge remains a challenge. To cope with the challenge, we propose a novel framework, Tree of Clarifications (ToC): It recursively constructs a tree of disambiguations for the AQ -- via few-shot prompting leveraging external knowledge -- and uses it to generate a long-form answer. ToC outperforms existing baselines on ASQA in a few-shot setup across the metrics, while surpassing fully-supervised baselines trained on the whole training set in terms of Disambig-F1 and Disambig-ROUGE. Code is available at https://github.com/gankim/tree-of-clarifications.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel framework, Tree of Clarifications (ToC), recursively constructs a tree of disambiguations for the AQ -- via few-shot prompting leveraging external knowledge -- and uses it to generate a long-form answer."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is disambiguating ambiguous prompts to improve language model responses, and the approach is to engage users in a collaborative dialogue to resolve the ambiguities. The research problem in the paper is answering ambiguous questions in open-domain question answering, and the approach is to construct a tree of disambiguations for the ambiguous question and use it to generate a comprehensive answer.\n\nWhile both works aim to handle ambiguous inputs, the proposal focuses on engaging users to disambiguate prompts interactively, whereas the paper focuses on generating a comprehensive answer addressing all possible interpretations without user interaction. The proposal targets improving language model responses in general, while the paper specifically tackles open-domain question answering.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "49a605392a58ca776404fd130942d79fc5ff329d",
            "paperId": "49a605392a58ca776404fd130942d79fc5ff329d",
            "title": "Aligning Predictive Uncertainty with Clarification Questions in Grounded Dialog",
            "abstract": "Asking for clarification is fundamental to effective collaboration. An interactive artificial agent must know when to ask a human instructor for more information in order to ascertain their goals. Previous work bases the timing of questions on supervised models learned from interactions between humans. Instead of a supervised classification task, we wish to ground the need for questions in the acting agent\u2019s predictive uncertainty. In this work, we investigate if ambiguous linguistic instructions can be aligned with uncertainty in neural models. We train an agent using the T5 encoder-decoder architecture to solve the Minecraft Collaborative Building Task and identify uncertainty metrics that achieve better distributional separation between clear and ambiguous instructions. We further show that well-calibrated prediction probabilities benefit the detection of ambiguous instructions. Lastly, we provide a novel empirical analysis on the relationship between uncertainty and dialog history length and highlight an important property that poses a difficulty for detection.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work training an agent using the T5 encoder-decoder architecture to solve the Minecraft Collaborative Building Task and identifying uncertainty metrics that achieve better distributional separation between clear and ambiguous instructions, shows that well-calibrated prediction probabilities benefit the detection of ambiguous instructions."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is disambiguating ambiguous prompts through collaborative interaction between the user and the language model. The approach is to have the model detect ambiguities, ask clarification questions, and refine the prompt based on user feedback.\n\nThe research problem in the paper is aligning the need for clarification questions with the agent's predictive uncertainty in a grounded dialog setting. The approach is to identify uncertainty metrics that separate clear and ambiguous instructions and show that well-calibrated prediction probabilities benefit ambiguity detection.\n\nThe proposal focuses on general language model prompts, while the paper is specific to grounded dialog instructions. The proposal uses collaborative interaction to resolve ambiguities, while the paper uses the agent's own predictive uncertainty to determine when to ask for clarification.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "517f770a38c6f09b1d0ee03793887912a844e69e",
            "paperId": "517f770a38c6f09b1d0ee03793887912a844e69e",
            "title": "Asking Clarification Questions in Knowledge-Based Question Answering",
            "abstract": "The ability to ask clarification questions is essential for knowledge-based question answering (KBQA) systems, especially for handling ambiguous phenomena. Despite its importance, clarification has not been well explored in current KBQA systems. Further progress requires supervised resources for training and evaluation, and powerful models for clarification-related text understanding and generation. In this paper, we construct a new clarification dataset, CLAQUA, with nearly 40K open-domain examples. The dataset supports three serial tasks: given a question, identify whether clarification is needed; if yes, generate a clarification question; then predict answers base on external user feedback. We provide representative baselines for these tasks and further introduce a coarse-to-fine model for clarification question generation. Experiments show that the proposed model achieves better performance than strong baselines. The further analysis demonstrates that our dataset brings new challenges and there still remain several unsolved problems, like reasonable automatic evaluation metrics for clarification question generation and powerful models for handling entity sparsity.",
            "year": 2019,
            "citationCount": 41,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new clarification dataset, CLAQUA, with nearly 40K open-domain examples is constructed and a coarse-to-fine model for clarification question generation is introduced, showing that the proposed model achieves better performance than strong baselines."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is disambiguating underspecified prompts to improve language model responses, and the approach is to engage users in a collaborative process to resolve ambiguities through clarification questions.\n\nThe research problem in the paper is asking clarification questions for knowledge-based question answering, and the approach is to construct a dataset and models for identifying when clarification is needed, generating clarification questions, and predicting answers based on user feedback.\n\nWhile both works involve clarification questions, the proposal focuses on open-ended prompts for language models, while the paper focuses specifically on knowledge-based question answering. The proposal emphasizes user engagement in the disambiguation process, while the paper does not involve users during the clarification stage.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "22b29b88c87333d4b324c8a0508429b611790278",
            "paperId": "22b29b88c87333d4b324c8a0508429b611790278",
            "title": "Empowering Language Models with Active Inquiry for Deeper Understanding",
            "abstract": "The rise of large language models (LLMs) has revolutionized the way that we interact with artificial intelligence systems through natural language. However, LLMs often misinterpret user queries because of their uncertain intention, leading to less helpful responses. In natural human interactions, clarification is sought through targeted questioning to uncover obscure information. Thus, in this paper, we introduce LaMAI (Language Model with Active Inquiry), designed to endow LLMs with this same level of interactive engagement. LaMAI leverages active learning techniques to raise the most informative questions, fostering a dynamic bidirectional dialogue. This approach not only narrows the contextual gap but also refines the output of the LLMs, aligning it more closely with user expectations. Our empirical studies, across a variety of complex datasets where LLMs have limited conversational context, demonstrate the effectiveness of LaMAI. The method improves answer accuracy from 31.9% to 50.9%, outperforming other leading question-answering frameworks. Moreover, in scenarios involving human participants, LaMAI consistently generates responses that are superior or comparable to baseline methods in more than 82% of the cases. The applicability of LaMAI is further evidenced by its successful integration with various LLMs, highlighting its potential for the future of interactive language models.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LaMAI (Language Model with Active Inquiry) is introduced, designed to endow LLMs with this same level of interactive engagement, and leverages active learning techniques to raise the most informative questions, fostering a dynamic bidirectional dialogue."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is ambiguous or underspecified prompts leading to inconsistent or incorrect outputs from language models, and the proposed approach is to use collaborative interaction between the user and the language model to actively resolve ambiguities through a clarification dialogue.\n\nThe research problem in the paper is language models misinterpreting user queries due to uncertain intention, and the proposed approach is to endow language models with active inquiry capabilities to raise informative questions and foster a dynamic bidirectional dialogue.\n\nWhile both the proposal and the paper aim to improve the accuracy and helpfulness of language model responses, the proposal focuses specifically on resolving ambiguous prompts through user collaboration, while the paper addresses the broader problem of misinterpreted user queries using active inquiry techniques.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "1af678b040ce638aedf8b582212937f0921ccc1d",
            "paperId": "1af678b040ce638aedf8b582212937f0921ccc1d",
            "title": "Abg-CoQA: Clarifying Ambiguity in Conversational Question Answering",
            "abstract": "E\ufb00ective communication requires the ability to identify ambiguities and request clar-i\ufb01cation of utterances. For machines to engage in a conversation, they need to learn to generate di\ufb00erent forms of clari\ufb01cation questions. This paper aims at studying the extent to which the state of the art neural generation models can generate e\ufb00ective clari\ufb01ca-tion questions in conversational question answering. We introduce Abg-CoQA, a novel crowdsourced dataset for clarifying ambiguities in conversational question answering systems. Our dataset contains 8,615 questions with answers where 994 questions are ambiguous. The conversational questions are about 3,968 text passages from \ufb01ve diverse domains which are pre-selected from the CoQA dataset. For ambiguous turns, we have collected the clari\ufb01cation questions and their answers. We evaluate strong language generation models and conversational question answering models on Abg-CoQA. The best-performing system achieves a F1-score of 23.6% on ambiguity detection; an accuracy of 56.0% on generating clari\ufb01cation question in human evaluation; and a F1 score of 40.1% on question answering after clari\ufb01cation, which is 35.1 points behind human performance (75.2%), indicating there is ample room for improvement.",
            "year": 2021,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The extent to which the state of the art neural generation models can generate e\ufb00ective clari\ufb01ca-tion questions in conversational question answering in Abg-CoQA is studied."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is disambiguating prompts to improve language model responses, and the approach is to engage users in a collaborative process to resolve ambiguities through clarification questions.\n\nThe research problem in the paper is studying how well neural models can generate clarification questions to resolve ambiguities in conversational question answering, and the approach is to introduce a new dataset (Abg-CoQA) with ambiguous questions and evaluate models on ambiguity detection, clarification question generation, and question answering after clarification.\n\nWhile both works deal with ambiguity resolution, the proposal focuses on prompts for language model generation, while the paper studies conversational question answering. The methods also differ, with the proposal using a collaborative process and the paper using a new dataset for evaluation.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "d33a14592d68da068953cdf37f8bf562740c0085",
            "paperId": "d33a14592d68da068953cdf37f8bf562740c0085",
            "title": "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study",
            "abstract": "Background Large language models (LLMs) have shown remarkable capabilities in natural language processing (NLP), especially in domains where labeled data are scarce or expensive, such as the clinical domain. However, to unlock the clinical knowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform specific clinical NLP tasks without any task-specific training data. This is known as in-context learning, which is an art and science that requires understanding the strengths and weaknesses of different LLMs and prompt engineering approaches. Objective The objective of this study is to assess the effectiveness of various prompt engineering techniques, including 2 newly introduced types\u2014heuristic and ensemble prompts, for zero-shot and few-shot clinical information extraction using pretrained language models. Methods This comprehensive experimental study evaluated different prompt types (simple prefix, simple cloze, chain of thought, anticipatory, heuristic, and ensemble) across 5 clinical NLP tasks: clinical sense disambiguation, biomedical evidence extraction, coreference resolution, medication status extraction, and medication attribute extraction. The performance of these prompts was assessed using 3 state-of-the-art language models: GPT-3.5 (OpenAI), Gemini (Google), and LLaMA-2 (Meta). The study contrasted zero-shot with few-shot prompting and explored the effectiveness of ensemble approaches. Results The study revealed that task-specific prompt tailoring is vital for the high performance of LLMs for zero-shot clinical NLP. In clinical sense disambiguation, GPT-3.5 achieved an accuracy of 0.96 with heuristic prompts and 0.94 in biomedical evidence extraction. Heuristic prompts, alongside chain of thought prompts, were highly effective across tasks. Few-shot prompting improved performance in complex scenarios, and ensemble approaches capitalized on multiple prompt strengths. GPT-3.5 consistently outperformed Gemini and LLaMA-2 across tasks and prompt types. Conclusions This study provides a rigorous evaluation of prompt engineering methodologies and introduces innovative techniques for clinical information extraction, demonstrating the potential of in-context learning in the clinical domain. These findings offer clear guidelines for future prompt-based clinical NLP research, facilitating engagement by non-NLP experts in clinical NLP advancements. To the best of our knowledge, this is one of the first works on the empirical evaluation of different prompt engineering approaches for clinical NLP in this era of generative artificial intelligence, and we hope that it will inspire and inform future research in this area.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study revealed that task-specific prompt tailoring is vital for the high performance of LLMs for zero-shot clinical NLP, and introduces innovative techniques for clinical information extraction, demonstrating the potential of in-context learning in the clinical domain."
            },
            "score": 7
        },
        {
            "id": "187f4521e6080f93fc2a26bf91b4e7d64f94e18a",
            "paperId": "187f4521e6080f93fc2a26bf91b4e7d64f94e18a",
            "title": "FedBPT: Efficient Federated Black-box Prompt Tuning for Large Language Models",
            "abstract": "Pre-trained language models (PLM) have revolutionized the NLP landscape, achieving stellar performances across diverse tasks. These models, while benefiting from vast training data, often require fine-tuning on specific data to cater to distinct downstream tasks. However, this data adaptation process has inherent security and privacy concerns, primarily when leveraging user-generated, device-residing data. Federated learning (FL) provides a solution, allowing collaborative model fine-tuning without centralized data collection. However, applying FL to finetune PLMs is hampered by challenges, including restricted model parameter access, high computational requirements, and communication overheads. This paper introduces Federated Black-box Prompt Tuning (FedBPT), a framework designed to address these challenges. FedBPT does not require the clients to access the model parameters. By focusing on training optimal prompts and utilizing gradient-free optimization methods, FedBPT reduces the number of exchanged variables, boosts communication efficiency, and minimizes computational and storage costs. Experiments highlight the framework's ability to drastically cut communication and memory costs while maintaining competitive performance. Ultimately, FedBPT presents a promising solution for efficient, privacy-preserving fine-tuning of PLM in the age of large language models.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Federated Black-box Prompt Tuning (FedBPT) is introduced, a framework designed to address challenges of fine-tuning of PLM in the age of large language models that reduces the number of exchanged variables, boosts communication efficiency, and minimizes computational and storage costs."
            },
            "score": 6
        },
        {
            "id": "f43138cf1e9fb0d9c073a3ba6db2506d4e9533a3",
            "paperId": "f43138cf1e9fb0d9c073a3ba6db2506d4e9533a3",
            "title": "Conversation Regression Testing: A Design Technique for Prototyping Generalizable Prompt Strategies for Pre-trained Language Models",
            "abstract": "Pre-trained language models (LLMs) such as GPT-3 can carry fluent, multi-turn conversations out-of-the-box, making them attractive materials for chatbot design. Further, designers can improve LLM chatbot utterances by prepending textual prompts -- instructions and examples of desired interactions -- to its inputs. However, prompt-based improvements can be brittle; designers face challenges systematically understanding how a prompt strategy might impact the unfolding of subsequent conversations across users. To address this challenge, we introduce the concept of Conversation Regression Testing. Based on sample conversations with a baseline chatbot, Conversation Regression Testing tracks how conversational errors persist or are resolved by applying different prompt strategies. We embody this technique in an interactive design tool, BotDesigner, that lets designers identify archetypal errors across multiple conversations; shows common threads of conversation using a graph visualization; and highlights the effects of prompt changes across bot design iterations. A pilot evaluation demonstrates the usefulness of both the concept of regression testing and the functionalities of BotDesigner for chatbot designers.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work embodies the concept of regression testing in an interactive design tool, BotDesigner, that lets designers identify archetypal errors across multiple conversations; shows common threads of conversation using a graph visualization; and highlights the effects of prompt changes across bot design iterations."
            },
            "score": 6
        },
        {
            "id": "afe4ee7ba225d8965cd96056b3e1e76a31451a75",
            "paperId": "afe4ee7ba225d8965cd96056b3e1e76a31451a75",
            "title": "PromptMagician: Interactive Prompt Engineering for Text-to-Image Creation",
            "abstract": "Generative text-to-image models have gained great popularity among the public for their powerful capability to generate high-quality images based on natural language prompts. However, developing effective prompts for desired images can be challenging due to the complexity and ambiguity of natural language. This research proposes PromptMagician, a visual analysis system that helps users explore the image results and refine the input prompts. The backbone of our system is a prompt recommendation model that takes user prompts as input, retrieves similar prompt-image pairs from DiffusionDB, and identifies special (important and relevant) prompt keywords. To facilitate interactive prompt refinement, PromptMagician introduces a multi-level visualization for the cross-modal embedding of the retrieved images and recommended keywords, and supports users in specifying multiple criteria for personalized exploration. Two usage scenarios, a user study, and expert interviews demonstrate the effectiveness and usability of our system, suggesting it facilitates prompt engineering and improves the creativity support of the generative text-to-image model.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This research proposes PromptMagician, a visual analysis system that helps users explore the image results and refine the input prompts, and introduces a multi-level visualization for the cross-modal embedding of the retrieved images and recommended keywords."
            },
            "score": 6
        },
        {
            "id": "13c85adfa950651ffcd91ef3018fa30801b74472",
            "paperId": "13c85adfa950651ffcd91ef3018fa30801b74472",
            "title": "Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration",
            "abstract": "Conversational systems based on Large Language Models (LLMs), such as ChatGPT, show exceptional proficiency in context understanding and response generation. However, despite their impressive capabilities, they still possess limitations, such as providing randomly-guessed answers to ambiguous queries or failing to refuse users' requests, both of which are considered aspects of a conversational agent's proactivity. This raises the question of whether LLM-based conversational systems are equipped to handle proactive dialogue problems. In this work, we conduct a comprehensive analysis of LLM-based conversational systems, specifically focusing on three aspects of proactive dialogue systems: clarification, target-guided, and non-collaborative dialogues. To trigger the proactivity of LLMs, we propose the Proactive Chain-of-Thought prompting scheme, which augments LLMs with the goal planning capability over descriptive reasoning chains. Empirical findings are discussed to promote future studies on LLM-based proactive dialogue systems.",
            "year": 2023,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A comprehensive analysis of LLM-based conversational systems, specifically focusing on three aspects of proactive dialogue systems: clarification, target-guided, and non-collaborative dialogues, and the Proactive Chain-of-Thought prompting scheme is proposed."
            },
            "score": 6
        },
        {
            "id": "e9d6cad994fd48567198ef260fbc5f5241aa9746",
            "paperId": "e9d6cad994fd48567198ef260fbc5f5241aa9746",
            "title": "Task Ambiguity in Humans and Language Models",
            "abstract": "Language models have recently achieved strong performance across a wide range of NLP benchmarks. However, unlike benchmarks, real world tasks are often poorly specified, and agents must deduce the user's intended behavior from a combination of context, instructions, and examples. We investigate how both humans and models behave in the face of such task ambiguity by proposing AmbiBench, a new benchmark of six ambiguously-specified classification tasks. We evaluate humans and models on AmbiBench by seeing how well they identify the intended task using 1) instructions with varying degrees of ambiguity, and 2) different numbers of labeled examples. We find that the combination of model scaling (to 175B parameters) and training with human feedback data enables models to approach or exceed the accuracy of human participants across tasks, but that either one alone is not sufficient. In addition, we show how to dramatically improve the accuracy of language models trained without large-scale human feedback training by finetuning on a small number of ambiguous in-context examples, providing a promising direction for teaching models to generalize well in the face of ambiguity.",
            "year": 2022,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes AmbiBench, a new benchmark of six ambiguously-specified classification tasks, and finds that the combination of model scaling and training with human feedback data enables models to approach or exceed the accuracy of human participants across tasks, but that either one alone is not sufficient."
            },
            "score": 6
        },
        {
            "id": "bad6fa523ecf782c837a2eecaaffa4e1f7477c24",
            "paperId": "bad6fa523ecf782c837a2eecaaffa4e1f7477c24",
            "title": "Interactive-Chain-Prompting: Ambiguity Resolution for Crosslingual Conditional Generation with Interaction",
            "abstract": "Crosslingual conditional generation (e.g., machine translation) has long enjoyed the benefits of scaling. Nonetheless, there are still issues that scale alone may not overcome. A source query in one language, for instance, may yield several translation options in another language without any extra context. Only one translation could be acceptable however, depending on the translator's preferences and goals. Choosing the incorrect option might significantly affect translation usefulness and quality. We propose a novel method interactive-chain prompting -- a series of question, answering and generation intermediate steps between a Translator model and a User model -- that reduces translations into a list of subproblems addressing ambiguities and then resolving such subproblems before producing the final text to be translated. To check ambiguity resolution capabilities and evaluate translation quality, we create a dataset exhibiting different linguistic phenomena which leads to ambiguities at inference for four languages. To encourage further exploration in this direction, we release all datasets. We note that interactive-chain prompting, using eight interactions as exemplars, consistently surpasses prompt-based methods with direct access to background information to resolve ambiguities.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel method interactive-chain prompting -- a series of question, answering and generation intermediate steps between a Translator model and a User model -- that reduces translations into a list of subproblems addressing ambiguities and then resolving such subpro problems before producing the final text to be translated."
            },
            "score": 6
        },
        {
            "id": "28fbbf98bac1bb941162df553ca034d600cb59a6",
            "paperId": "28fbbf98bac1bb941162df553ca034d600cb59a6",
            "title": "Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models",
            "abstract": "An increasing number of vision-language tasks can be handled with little to no training, i.e., in a zero and few-shot manner, by marrying large language models (LLMs) to vision encoders, resulting in large vision-language models (LVLMs). While this has huge upsides, such as not requiring training data or custom architectures, how an input is presented to an LVLM can have a major impact on zero-shot model performance. In particular, inputs phrased in an underspecified way can result in incorrect answers due to factors like missing visual information, complex implicit reasoning, or linguistic ambiguity. Therefore, adding visually-grounded information to the input as a preemptive clarification should improve model performance by reducing underspecification, e.g., by localizing objects and disambiguating references. Similarly, in the VQA setting, changing the way questions are framed can make them easier for models to answer. To this end, we present Rephrase, Augment and Reason (RepARe), a gradient-free framework that extracts salient details about the image using the underlying LVLM as a captioner and reasoner, in order to propose modifications to the original question. We then use the LVLM's confidence over a generated answer as an unsupervised scoring function to select the rephrased question most likely to improve zero-shot performance. Focusing on three visual question answering tasks, we show that RepARe can result in a 3.85% (absolute) increase in zero-shot accuracy on VQAv2, 6.41%, and 7.94% points increase on A-OKVQA, and VizWiz respectively. Additionally, we find that using gold answers for oracle question candidate selection achieves a substantial gain in VQA accuracy by up to 14.41%. Through extensive analysis, we demonstrate that outputs from RepARe increase syntactic complexity, and effectively utilize vision-language interaction and the frozen LLM.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Rephrase, Augment and Reason (RepARe), a gradient-free framework that extracts salient details about the image using the underlying LVLM as a captioner and reasoner, in order to propose modifications to the original question, is presented."
            },
            "score": 6
        },
        {
            "id": "8210cef990b8e5cddbc95000e46309bdd25337f7",
            "paperId": "8210cef990b8e5cddbc95000e46309bdd25337f7",
            "title": "Asking Clarification Questions for Code Generation in General-Purpose Programming Language",
            "abstract": "Code generation from text requires understanding the user\u2019s intent from a natural language description (NLD) and generating an executable program code snippet that satis\ufb01es this intent. While recent pretrained language models (PLMs) demonstrate remarkable performance for this task, these models fail when the given NLD is ambiguous due to the lack of enough speci\ufb01cations for generating a high-quality code snippet. In this work, we introduce a novel and more realistic setup for this task. We hypothesize that ambiguities in the speci\ufb01cations of an NLD are resolved by asking clari\ufb01cation questions (CQs). Therefore, we collect and introduce a new dataset named CodeClarQA containing NLD-Code pairs with created CQAs. We evaluate the performance of PLMs for code generation on our dataset. The empirical results support our hypothesis that clari\ufb01cations result in more precise generated code, as shown by an improvement of 17.52 in BLEU, 12.72 in CodeBLEU, and 7.7% in the exact match. Alongside this, our task and dataset introduce new challenges to the community, including when and what CQs should be asked.",
            "year": 2022,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The hypothesis that ambiguities in the speci\ufb01cations of an NLD are resolved by asking clari\ufb01cation questions (CQs) is hypothesized and a new dataset named CodeClarQA containing NLD-Code pairs with created CQAs is introduced."
            },
            "score": 6
        },
        {
            "id": "4e64399b54bc4de8d97328cddf38dd1038459cad",
            "paperId": "4e64399b54bc4de8d97328cddf38dd1038459cad",
            "title": "Ranking Clarification Questions via Natural Language Inference",
            "abstract": "Given a natural language query, teaching machines to ask clarifying questions is of immense utility in practical natural language processing systems. Such interactions could help in filling information gaps for better machine comprehension of the query. For the task of ranking clarification questions, we hypothesize that determining whether a clarification question pertains to a missing entry in a given post (on QA forums such as StackExchange) could be considered as a special case of Natural Language Inference (NLI), where both the post and the most relevant clarification question point to a shared latent piece of information or context. We validate this hypothesis by incorporating representations from a Siamese BERT model fine-tuned on NLI and Multi-NLI datasets into our models and demonstrate that our best performing model obtains a relative performance improvement of 40 percent and 60 percent respectively (on the key metric of Precision@1), over the state-of-the-art baseline(s) on the two evaluation sets of the StackExchange dataset, thereby, significantly surpassing the state-of-the-art.",
            "year": 2020,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is hypothesized that determining whether a clarification question pertains to a missing entry in a given post could be considered as a special case of Natural Language Inference (NLI), where both the post and the most relevant clarification question point to a shared latent piece of information or context."
            },
            "score": 6
        },
        {
            "id": "b5ddfea0af89eec6678d4b191c4eee48eee8ae30",
            "paperId": "b5ddfea0af89eec6678d4b191c4eee48eee8ae30",
            "title": "Python Code Generation by Asking Clarification Questions",
            "abstract": "Code generation from text requires understanding the user\u2019s intent from a natural languagedescription and generating an executable code snippet that satisfies this intent. While recent pretrained language models demonstrate remarkable performance for this task, these models fail when the given natural language description is under-specified. In this work, we introduce a novel and more realistic setup for this task. We hypothesize that the under-specification of a natural language description can be resolved by asking clarification questions. Therefore, we collect and introduce a new dataset named CodeClarQA containing pairs of natural language descriptions and code with created synthetic clarification questions and answers. The empirical results of our evaluation of pretrained language model performance on code generation show that clarifications result in more precisely generated code, as shown by the substantial improvement of model performance in all evaluation metrics. Alongside this, our task and dataset introduce new challenges to the community, including when and what clarification questions should be asked. Our code and dataset are available on GitHub.",
            "year": 2022,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel and more realistic setup for code generation from text, hypothesize that the under-specification of a natural language description can be resolved by asking clarification questions, and introduces a new dataset named CodeClarQA containing pairs of natural language descriptions and code with created synthetic clarification questions and answers."
            },
            "score": 6
        },
        {
            "id": "223884ef08f38fd0765e4c0e467a223aef573236",
            "paperId": "223884ef08f38fd0765e4c0e467a223aef573236",
            "title": "Towards Facet-Driven Generation of Clarifying Questions for Conversational Search",
            "abstract": "Clarifying an underlying user information need is an important aspect of a modern-day IR system. The importance of clarification is even higher in limited-bandwidth scenarios, such as conversational or mobile search, where a user is unable to easily browse through a long list of retrieved results. Thus, asking clarifying questions about user's potentially ambiguous queries arises as one of the main tasks of conversational search. Recent approaches have, while making significant progress in the field, remained limited to selecting a clarifying question from a predefined set or prompting the user with vague or template-based questions. However, with the recent advances in text generation through large-scale language models, an ideal system should generate the next clarifying question. The challenge of generating an appropriate clarifying question is twofold: 1) to produce the question in coherent natural language; 2) to ask a question that is relevant to the user query. In this paper, we propose a model that generates clarifying questions with respect to the user query and query facets. We fine-tune the GPT-2 language model to generate questions related to the query and one of the extracted query facets. Compared to competitive baselines, results show that our proposed method is both natural and useful, as judged by human annotators. Moreover, we discuss the potential theoretical framework this approach would fit in. We release the code for future work and reproducibility purposes.",
            "year": 2021,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A model that generates clarifying questions with respect to the user query and query facets is proposed using the GPT-2 language model and results show that the proposed method is both natural and useful, as judged by human annotators."
            },
            "score": 6
        },
        {
            "id": "9c770a7a392df853387d5701e9876324f71f36c7",
            "paperId": "9c770a7a392df853387d5701e9876324f71f36c7",
            "title": "Probing the Multi-turn Planning Capabilities of LLMs via 20 Question Games",
            "abstract": "Large language models (LLMs) are effective at answering questions that are clearly asked. However, when faced with ambiguous queries they can act unpredictably and produce incorrect outputs. This underscores the need for the development of intelligent agents capable of asking clarification questions to resolve ambiguities effectively. This capability requires complex understanding, state tracking, reasoning and planning over multiple conversational turns. However, directly measuring this can be challenging. In this paper, we offer a surrogate problem which assesses an LLMs's capability to deduce an entity unknown to itself, but revealed to a judge, by asking the judge a series of queries. This \\textit{entity-deducing game} can serve as an evaluation framework to probe the conversational reasoning and planning capabilities of language models. We systematically evaluate various LLMs and discover significant differences in their performance on this task. We find that strong LLMs like GPT-4 outperform human players by a large margin. We further employ Behavior Cloning (BC) to examine whether a weaker model is capable of imitating a stronger model and generalizing to data or domains, using only the demonstrations from a stronger model. We finally propose to use Reinforcement Learning to enhance reasoning and planning capacity of Vicuna models through episodes of game playing, which lead to significant performance improvement. We hope that this problem offers insights into how autonomous agents could be trained to behave more intelligently in ambiguous circumstances.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper offers a surrogate problem which assesses an LLMs's capability to deduce an entity unknown to itself, but revealed to a judge, by asking the judge a series of queries, and proposes to use Reinforcement Learning to enhance reasoning and planning capacity of Vicuna models through episodes of game playing."
            },
            "score": 6
        },
        {
            "id": "99da2f941c0a606278463f1e5e60a2bdbaf8eb99",
            "paperId": "99da2f941c0a606278463f1e5e60a2bdbaf8eb99",
            "title": "Enabling Synergistic Knowledge Sharing and Reasoning in Large Language Models with Collaborative Multi-Agents",
            "abstract": "Despite the significant advancements in the field of Natural Language Processing (NLP), Large Language Models (LLMs) have shown limitations in performing complex tasks that require arithmetic, commonsense, and symbolic reasoning. Reasoning frameworks like ReAct, Chain-of-thought (CoT), Tree-of-thoughts (ToT), etc. have shown success but with limitations in solving long-form complex tasks. To address this, we pro-pose a knowledge-sharing and collaborative multi-agent assisted framework on LLMs that leverages the capabilities of existing reasoning frameworks and the collaborative skills of multi-agent systems (MASs). The objectives of the proposed framework are to overcome the limitations of LLMs, enhance their reasoning capabilities, and improve their performance in complex tasks. It involves generating natural language rationales and in-context few-shot learning via prompting, and integrates the reasoning techniques with efficient knowledge-sharing and communication-driven agent networks. The potential benefits of the proposed framework include saving time and money, improved efficiency for computationally intensive reasoning, and the ability to incor-porate multiple collaboration strategies for dynamically changing environments.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A knowledge-sharing and collaborative multi-agent assisted framework that leverages the capabilities of existing reasoning frameworks and the collaborative skills of multi-agent systems (MASs) to overcome the limitations of LLMs, enhance their reasoning capabilities, and improve their performance in complex tasks."
            },
            "score": 6
        },
        {
            "id": "ab90da70bf4671bb95d8e7ff97b2cce19768c579",
            "paperId": "ab90da70bf4671bb95d8e7ff97b2cce19768c579",
            "title": "Efficient Federated Prompt Tuning for Black-box Large Pre-trained Models",
            "abstract": "With the blowout development of pre-trained models (PTMs), the efficient tuning of these models for diverse downstream applications has emerged as a pivotal research concern. Although recent investigations into prompt tuning have provided promising avenues, three salient challenges persist: (1) memory constraint: the continuous growth in the size of open-source PTMs renders fine-tuning, even a fraction of their parameters, challenging for many practitioners. (2) model privacy: existing PTMs often function as public API services, with their parameters inaccessible for effective or tailored fine-tuning. (3) data privacy: the fine-tuning of PTMs necessitates high-quality datasets, which are typically localized and not shared to public. To optimally harness each local dataset while navigating memory constraints and preserving privacy, we propose Federated Black-Box Prompt Tuning (Fed-BBPT). This innovative approach eschews reliance on parameter architectures and private dataset access, instead capitalizing on a central server that aids local users in collaboratively training a prompt generator through regular aggregation. Local users leverage API-driven learning via a zero-order optimizer, obviating the need for PTM deployment. Relative to extensive fine-tuning, Fed-BBPT proficiently sidesteps memory challenges tied to PTM storage and fine-tuning on local machines, tapping into comprehensive, high-quality, yet private training datasets. A thorough evaluation across 40 datasets spanning CV and NLP tasks underscores the robustness of our proposed model.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Federated Black-Box Prompt Tuning proficiently sidesteps memory challenges tied to PTM storage and fine-tuning on local machines, tapping into comprehensive, high-quality, yet private training datasets."
            },
            "score": 6
        },
        {
            "id": "9c71f49f5bb6fb916423e36acd8be3155a6fddb6",
            "paperId": "9c71f49f5bb6fb916423e36acd8be3155a6fddb6",
            "title": "Wordflow: Social Prompt Engineering for Large Language Models",
            "abstract": "Large language models (LLMs) require well-crafted prompts for effective use. Prompt engineering, the process of designing prompts, is challenging, particularly for non-experts who are less familiar with AI technologies. While researchers have proposed techniques and tools to assist LLM users in prompt design, these works primarily target AI application developers rather than non-experts. To address this research gap, we propose social prompt engineering, a novel paradigm that leverages social computing techniques to facilitate collaborative prompt design. To investigate social prompt engineering, we introduce Wordflow, an open-source and social text editor that enables everyday users to easily create, run, share, and discover LLM prompts. Additionally, by leveraging modern web technologies, Wordflow allows users to run LLMs locally and privately in their browsers. Two usage scenarios highlight how social prompt engineering and our tool can enhance laypeople's interaction with LLMs. Wordflow is publicly accessible at https://poloclub.github.io/wordflow.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes social prompt engineering, a novel paradigm that leverages social computing techniques to facilitate collaborative prompt design and introduces Wordflow, an open-source and social text editor that enables everyday users to easily create, run, share, and discover LLM prompts."
            },
            "score": 5
        },
        {
            "id": "50b59143bf3469f082b2308fa394bb6d55091a41",
            "paperId": "50b59143bf3469f082b2308fa394bb6d55091a41",
            "title": "Interactive Task Planning with Language Models",
            "abstract": "An interactive robot framework accomplishes long-horizon task planning and can easily generalize to new goals or distinct tasks, even during execution. However, most traditional methods require predefined module design, which makes it hard to generalize to different goals. Recent large language model based approaches can allow for more open-ended planning but often require heavy prompt engineering or domain-specific pretrained models. To tackle this, we propose a simple framework that achieves interactive task planning with language models. Our system incorporates both high-level planning and low-level function execution via language. We verify the robustness of our system in generating novel high-level instructions for unseen objectives and its ease of adaptation to different tasks by merely substituting the task guidelines, without the need for additional complex prompt engineering. Furthermore, when the user sends a new request, our system is able to replan accordingly with precision based on the new request, task guidelines and previously executed steps. Please check more details on our https://wuphilipp.github.io/itp_site and https://youtu.be/TrKLuyv26_g.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A simple framework that achieves interactive task planning with language models that incorporates both high-level planning and low-level function execution via language and is able to replan accordingly with precision based on the new request, task guidelines and previously executed steps."
            },
            "score": 5
        },
        {
            "id": "e814deb54d154aad19ae2b72a2e4dd3376175bb5",
            "paperId": "e814deb54d154aad19ae2b72a2e4dd3376175bb5",
            "title": "AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With Large Language Models",
            "abstract": "Recent large language models (LLMs) are promising for making decisions in grounded environments. However, LLMs frequently fail in complex decision-making tasks due to the misalignment between the pre-trained knowledge in LLMs and the actual rules in the environment. Existing methods require either costly gradient computation or lengthy in-context demonstrations. In this paper, we propose AutoPlan, an approach to guide LLM-based agents to accomplish interactive decision-making tasks. AutoPlan augments the LLM prompt with a task-solving plan and optimizes it through iterative experience collection and reflection. Our experiments show that AutoPlan, though using no in-context demonstrations, achieves success rates on par with the baselines using human-written demonstrations on ALFWorld and even outperforms them by 8% on HotpotQA. The code is available at https://github.com/owaski/AutoPlan.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "AutoPlan augments the LLM prompt with a task-solving plan and optimizes it through iterative experience collection and reflection and achieves success rates on par with the baselines using human-written demonstrations on ALFWorld and even outperforms them by 8% on HotpotQA."
            },
            "score": 5
        },
        {
            "id": "f669d7a6fab0147253178a6fc854e05e3d92fb3f",
            "paperId": "f669d7a6fab0147253178a6fc854e05e3d92fb3f",
            "title": "Mini-DALLE3: Interactive Text to Image by Prompting Large Language Models",
            "abstract": "The revolution of artificial intelligence content generation has been rapidly accelerated with the booming text-to-image (T2I) diffusion models. Within just two years of development, it was unprecedentedly of high-quality, diversity, and creativity that the state-of-the-art models could generate. However, a prevalent limitation persists in the effective communication with these popular T2I models, such as Stable Diffusion, using natural language descriptions. This typically makes an engaging image hard to obtain without expertise in prompt engineering with complex word compositions, magic tags, and annotations. Inspired by the recently released DALLE3 - a T2I model directly built-in ChatGPT that talks human language, we revisit the existing T2I systems endeavoring to align human intent and introduce a new task - interactive text to image (iT2I), where people can interact with LLM for interleaved high-quality image generation/edit/refinement and question answering with stronger images and text correspondences using natural language. In addressing the iT2I problem, we present a simple approach that augments LLMs for iT2I with prompting techniques and off-the-shelf T2I models. We evaluate our approach for iT2I in a variety of common-used scenarios under different LLMs, e.g., ChatGPT, LLAMA, Baichuan, and InternLM. We demonstrate that our approach could be a convenient and low-cost way to introduce the iT2I ability for any existing LLMs and any text-to-image models without any training while bringing little degradation on LLMs' inherent capabilities in, e.g., question answering and code generation. We hope this work could draw broader attention and provide inspiration for boosting user experience in human-machine interactions alongside the image quality of the next-generation T2I systems.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work revisits the existing T2I systems endeavoring to align human intent and introduce a new task - interactive text to image (iT2I), where people can interact with LLM for interleaved high-quality image generation/edit/refinement and question answering with stronger images and text correspondences using natural language."
            },
            "score": 5
        },
        {
            "id": "6ad26eb2d2aa6679d16d9c16fb75cd2cbe1127bc",
            "paperId": "6ad26eb2d2aa6679d16d9c16fb75cd2cbe1127bc",
            "title": "See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning",
            "abstract": "Large pre-trained vision and language models have demonstrated remarkable capacities for various tasks. However, solving the knowledge-based visual reasoning tasks remains challenging, which requires a model to comprehensively understand image content, connect the external world knowledge, and perform step-by-step reasoning to answer the questions correctly. To this end, we propose a novel framework named Interactive Prompting Visual Reasoner (IPVR) for few-shot knowledge-based visual reasoning. IPVR contains three stages, see, think and confirm. The see stage scans the image and grounds the visual concept candidates with a visual perception model. The think stage adopts a pre-trained large language model (LLM) to attend to the key concepts from candidates adaptively. It then transforms them into text context for prompting with a visual captioning model and adopts the LLM to generate the answer. The confirm stage further uses the LLM to generate the supporting rationale to the answer, verify the generated rationale with a cross-modality classifier and ensure that the rationale can infer the predicted output consistently. We conduct experiments on a range of knowledge-based visual reasoning datasets. We found our IPVR enjoys several benefits, 1). it achieves better performance than the previous few-shot learning baselines; 2). it enjoys the total transparency and trustworthiness of the whole reasoning process by providing rationales for each reasoning step; 3). it is computation-efficient compared with other fine-tuning baselines.",
            "year": 2023,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel framework named Interactive Prompting Visual Reasoner (IPVR), which achieves better performance than the previous few-shot learning baselines, and enjoys the total transparency and trustworthiness of the whole reasoning process by providing rationales for each reasoning step."
            },
            "score": 5
        },
        {
            "id": "57f2c0720ffaec896f612b6f3b13fd1c538b4749",
            "paperId": "57f2c0720ffaec896f612b6f3b13fd1c538b4749",
            "title": "Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulators to Enhance Dialogue System",
            "abstract": "Dialogue systems and large language models (LLMs) have gained considerable attention. However, the direct utilization of LLMs as task-oriented dialogue (TOD) models has been found to underperform compared to smaller task-specific models. Nonetheless, it is crucial to acknowledge the significant potential of LLMs and explore improved approaches for leveraging their impressive abilities. Motivated by the goal of leveraging LLMs, we propose an alternative approach called User-Guided Response Optimization (UGRO) to combine it with a smaller TOD model. This approach uses LLM as an annotation-free user simulator to assess dialogue responses, combining them with smaller fine-tuned end-to-end TOD models. By utilizing the satisfaction feedback generated by LLMs, UGRO further optimizes the supervised fine-tuned TOD model. Specifically, the TOD model takes the dialogue history as input and, with the assistance of the user simulator's feedback, generates high-satisfaction responses that meet the user's requirements. Through empirical experiments on two TOD benchmarks, we validate the effectiveness of our method. The results demonstrate that our approach outperforms previous state-of-the-art (SOTA) results.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes an alternative approach called User-Guided Response Optimization (UGRO) to combine LLM as an annotation-free user simulator to assess dialogue responses, combining them with smaller fine-tuned end-to-end TOD models."
            },
            "score": 5
        },
        {
            "id": "21ae9d85c3c31114abe39acf56f18a117e3e3ead",
            "paperId": "21ae9d85c3c31114abe39acf56f18a117e3e3ead",
            "title": "Large Language Models as Minecraft Agents",
            "abstract": "In this work we examine the use of Large Language Models (LLMs) in the challenging setting of acting as a Minecraft agent. We apply and evaluate LLMs in the builder and architect settings, introduce clarification questions and examining the challenges and opportunities for improvement. In addition, we present a platform for online interaction with the agents and an evaluation against previous works.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work applies and evaluates LLMs in the builder and architect settings, introduces clarification questions and examines the challenges and opportunities for improvement, and presents a platform for online interaction with the agents."
            },
            "score": 5
        },
        {
            "id": "8a5d1e5a50e282ecffaaca76fa655004e103a361",
            "paperId": "8a5d1e5a50e282ecffaaca76fa655004e103a361",
            "title": "Deep Learning for Database Mapping and Asking Clarification Questions in Dialogue Systems",
            "abstract": "A dialogue system will often ask followup clarification questions when interacting with a user if the agent is unsure how to respond. In this new study, we explore deep reinforcement learning (RL) for asking followup questions when a user records a meal description, and the system needs to narrow down the options for which foods the person has eaten. We build off of prior work in which we use novel convolutional neural network models to bypass the standard feature engineering used in dialogue systems to handle the text mismatch between natural language user queries and structured database entries, demonstrating that our model learns semantically meaningful embedding representations of natural language. In this new nutrition domain, the followup clarification questions consist of possible attributes for each food that was consumed; for example, if the user drinks a cup of milk, the system should ask about the percent milkfat. We investigate an RL agent to dynamically follow up with the user, which we compare to rule-based and entropy-based methods. On a held-out test set, assuming the followup questions are answered correctly, deep RL significantly boosts top five food recall from 54.9% without followup to 89.0%. We also demonstrate that a hybrid RL model achieves the best perceived naturalness ratings in a human evaluation.",
            "year": 2019,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study explores deep reinforcement learning for asking followup questions when a user records a meal description, and the system needs to narrow down the options for which foods the person has eaten, and investigates an RL agent to dynamically follow up with the user."
            },
            "score": 5
        },
        {
            "id": "1c24b3fe71e223d380b3bc806e4f02d80539d5f3",
            "paperId": "1c24b3fe71e223d380b3bc806e4f02d80539d5f3",
            "title": "Jointly Improving Parsing and Perception for Natural Language Commands through Human-Robot Dialog",
            "abstract": "In this work, we present methods for using human-robot dialog to improve language understanding for a mobile robot agent. The agent parses natural language to underlying semantic meanings and uses robotic sensors to create multi-modal models of perceptual concepts like red and heavy . The agent can be used for showing navigation routes, delivering objects to people, and relocating objects from one location to another. We use dialog clari\ufb01cation questions both to understand commands and to generate additional parsing training data. The agent employs opportunistic active learning to select questions about how words relate to objects, improving its understanding of perceptual concepts. We evaluated this agent on Amazon Mechanical Turk. After training on data induced from conversations, the agent reduced the number of dialog questions it asked while receiving higher usability ratings. Additionally, we demonstrated the agent on a robotic platform, where it learned new perceptual concepts on the \ufb02y while completing a real-world task.",
            "year": 2020,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Methods for using human-robot dialog to improve language understanding for a mobile robot agent that parses natural language to underlying semantic meanings and uses robotic sensors to create multi-modal models of perceptual concepts like red and heavy are presented."
            },
            "score": 5
        },
        {
            "id": "7baaaa623d2c3011a52e2bb515e030825fa6e36c",
            "paperId": "7baaaa623d2c3011a52e2bb515e030825fa6e36c",
            "title": "An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration",
            "abstract": "While Large Language Models (LLMs) demonstrate exceptional performance in a multitude of Natural Language Processing (NLP) tasks, they encounter challenges in practical applications, including issues with hallucinations, inadequate knowledge updating, and limited transparency in the reasoning process. To overcome these limitations, this study innovatively proposes a collaborative training-free reasoning scheme involving tight cooperation between Knowledge Graph (KG) and LLMs. This scheme first involves using LLMs to iteratively explore KG, selectively retrieving a task-relevant knowledge subgraph to support reasoning. The LLMs are then guided to further combine inherent implicit knowledge to reason on the subgraph while explicitly elucidating the reasoning process. Through such a cooperative approach, our scheme achieves more reliable knowledge-based reasoning and facilitates the tracing of the reasoning results. Experimental results show that our scheme significantly progressed across multiple datasets, notably achieving over a 10% improvement on the QALD10 dataset compared to the best baseline and the fine-tuned state-of-the-art (SOTA) work. Building on this success, this study hopes to offer a valuable reference for future research in the fusion of KG and LLMs, thereby enhancing LLMs' proficiency in solving complex issues.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study innovatively proposes a collaborative training-free reasoning scheme involving tight cooperation between Knowledge Graph (KG) and LLMs that achieves more reliable knowledge-based reasoning and facilitates the tracing of the reasoning results."
            },
            "score": 5
        },
        {
            "id": "9b1f4492a663c7f56f2b43ae1ed167d3857aacca",
            "paperId": "9b1f4492a663c7f56f2b43ae1ed167d3857aacca",
            "title": "PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts",
            "abstract": "PromptSource is a system for creating, sharing, and using natural language prompts. Prompts are functions that map an example from a dataset to a natural language input and target output. Using prompts to train and query language models is an emerging area in NLP that requires new tools that let users develop and refine these prompts collaboratively. PromptSource addresses the emergent challenges in this new setting with (1) a templating language for defining data-linked prompts, (2) an interface that lets users quickly iterate on prompt development by observing outputs of their prompts on many examples, and (3) a community-driven set of guidelines for contributing new prompts to a common pool. Over 2,000 prompts for roughly 170 datasets are already available in PromptSource. PromptSource is available at https://github.com/bigscience-workshop/promptsource.",
            "year": 2022,
            "citationCount": 243,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "PromptSource addresses the emergent challenges in this new setting with a templating language for defining data-linked prompts, an interface that lets users quickly iterate on prompt development by observing outputs of their prompts on many examples, and a community-driven set of guidelines for contributing new prompts to a common pool."
            },
            "score": 5
        },
        {
            "id": "eedb8159755c317ad2f9f07dfb667c62d72a3181",
            "paperId": "eedb8159755c317ad2f9f07dfb667c62d72a3181",
            "title": "CoPrompt: Supporting Prompt Sharing and Referring in Collaborative Natural Language Programming",
            "abstract": "Natural language (NL) programming has become more approachable due to the powerful code-generation capability of large language models (LLMs). This shift to using NL to program enhances collaborative programming by reducing communication barriers and context-switching among programmers from varying backgrounds. However, programmers may face challenges during prompt engineering in a collaborative setting as they need to actively keep aware of their collaborators' progress and intents. In this paper, we aim to investigate ways to assist programmers' prompt engineering in a collaborative context. We first conducted a formative study to understand the workflows and challenges of programmers when using NL for collaborative programming. Based on our findings, we implemented a prototype, CoPrompt, to support collaborative prompt engineering by providing referring, requesting, sharing, and linking mechanisms. Our user study indicates that CoPrompt assists programmers in comprehending collaborators' prompts and building on their collaborators' work, reducing repetitive updates and communication costs.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "CoPrompt is implemented to support collaborative prompt engineering by providing referring, requesting, sharing, and linking mechanisms, and user study indicates that CoPrompt assists programmers in comprehending collaborators' prompts and building on their collaborators' work, reducing repetitive updates and communication costs."
            },
            "score": 4
        },
        {
            "id": "5bc61019771a9fe2a12cc41bad1d9ae4222a152c",
            "paperId": "5bc61019771a9fe2a12cc41bad1d9ae4222a152c",
            "title": "PromptMaker: Prompt-based Prototyping with Large\u00a0Language\u00a0Models",
            "abstract": "Prototyping is notoriously difficult to do with machine learning (ML), but recent advances in large language models may lower the barriers to people prototyping with ML, through the use of natural language prompts. This case study reports on the real-world experiences of industry professionals (e.g. designers, program managers, front-end developers) prototyping new ML-powered feature ideas via prompt-based prototyping. Through interviews with eleven practitioners during a three-week sprint and a workshop, we find that prompt-based prototyping reduced barriers of access by substantially broadening who can prototype with ML, sped up the prototyping process, and grounded communication between collaborators. Yet, it also introduced new challenges, such as the need to reverse-engineer prompt designs, source example data, and debug and evaluate prompt effectiveness. Taken together, this case study provides important implications that lay the groundwork toward a new future of prototyping with ML.",
            "year": 2022,
            "citationCount": 58,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 4
        },
        {
            "id": "e10ef2a43fb68bf8567ce752117da16a2db9ea9a",
            "paperId": "e10ef2a43fb68bf8567ce752117da16a2db9ea9a",
            "title": "INTERVENOR: Prompt the Coding Ability of Large Language Models with the Interactive Chain of Repairing",
            "abstract": "This paper proposes INTER acti VE chai N O f R epairing (INTERVENOR), which mimics human code repairing behavior (iteratively judging, rethinking, and repairing) and prompts the coding ability of regard Large Language Models (LLMs). Specifically, INTERVENOR employs two LLM based agents, Code Learner and Code Teacher , to play different roles in code repairing and work interactively to repair the generated codes. The Code Learner is asked to generate and repair code according to the instructions from the Code Teacher . The Code Teacher rethinks the code errors according to the corresponding feedback from compilers and iteratively generates the chain-of-repairing (CoR) to guide the code repairing process for Code Learner . Our experiments show that INTERVENOR outperforms the state-of-the-art methods and achieves about 13% and 4.5% improvements over the GPT-3.5 model in code generation and code translation tasks, respectively. Our further analyses show that CoR can illuminate the bug reasons and solution plans via natural language. Thanks to the feed-back of code compilers, INTERVENOR can accurately identify the syntax errors and assertion errors in the code and provide precise instructions to repair codes, making LLMs achieve the plateau performance with only three repairing turns. All data and codes are available at https://github.com/NEUIR/INTERVENOR .",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes INTERVENOR, which mimics human code repairing behavior (iteratively judging, rethinking, and repairing) and prompts the coding ability of Large Language Models (LLMs) and helps LLMs achieve the plateau performance with only three repairing turns."
            },
            "score": 4
        },
        {
            "id": "07d23fad3cbffefeae63b5410e949715c6c4ae23",
            "paperId": "07d23fad3cbffefeae63b5410e949715c6c4ae23",
            "title": "FedLogic: Interpretable Federated Multi-Domain Chain-of-Thought Prompt Selection for Large Language Models",
            "abstract": "Leveraging ``chain-of-thought (CoT)'' reasoning to elicit rapid and precise responses from large language models (LLMs) is rapidly attracting research interest. A notable challenge here is how to design or select optimal prompts. The process of prompt selection relies on trial and error, involving continuous adjustments and combinations of input prompts by users based on the corresponding new responses generated from LLMs. Furthermore, minimal research has been conducted to explore how LLMs employ the mathematical problem-solving capabilities learned from user interactions to address issues in narrative writing. To improve interpretability and explore the balance principle between generality and personalization under a multi-domain CoT prompt selection scenario, we propose the Federated Logic rule learning approach (FedLogic). We introduce a theoretical formalization and interactive emulation of the multi-domain CoT prompt selection dilemma in the context of federated LLMs. We cast the problem of joint probability modeling as a bilevel program, where the CoT prompt selection intricacy can be likened to a fuzzy score-based rule selection with the LLMs function as rule generators. FedLogic solves this problem through variational expectation maximization (V-EM). In addition, we incorporate two KL-divergence constraints within this probabilistic modeling framework to surmount the intricacies of managing extensive search spaces and accomplishing cross-domain personalization of CoTs. To the best of our knowledge, FedLogic is the first interpretable and principled federated multi-domain CoT prompt selection approach for LLMs.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "FedLogic is the first interpretable and principled federated multi-domain CoT prompt selection approach for LLMs and incorporates two KL-divergence constraints within this probabilistic modeling framework to surmount the intricacies of managing extensive search spaces and accomplishing cross-domain personalization of CoTs."
            },
            "score": 4
        },
        {
            "id": "99832586d55f540f603637e458a292406a0ed75d",
            "paperId": "99832586d55f540f603637e458a292406a0ed75d",
            "title": "LANGUAGE MODELS",
            "abstract": "While large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. Furthermore, on two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.",
            "year": 2023,
            "citationCount": 600,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces."
            },
            "score": 4
        },
        {
            "id": "5d95d2b9bc203447000bd201ceabe67404eedeeb",
            "paperId": "5d95d2b9bc203447000bd201ceabe67404eedeeb",
            "title": "Using Large Language Models to Generate, Validate, and Apply User Intent Taxonomies",
            "abstract": "Log data can reveal valuable information about how users interact with Web search services, what they want, and how satisfied they are. However, analyzing user intents in log data is not easy, especially for emerging forms of Web search such as AI-driven chat. To understand user intents from log data, we need a way to label them with meaningful categories that capture their diversity and dynamics. Existing methods rely on manual or machine-learned labeling, which are either expensive or inflexible for large and dynamic datasets. We propose a novel solution using large language models (LLMs), which can generate rich and relevant concepts, descriptions, and examples for user intents. However, using LLMs to generate a user intent taxonomy and apply it for log analysis can be problematic for two main reasons: (1) such a taxonomy is not externally validated; and (2) there may be an undesirable feedback loop. To address this, we propose a new methodology with human experts and assessors to verify the quality of the LLM-generated taxonomy. We also present an end-to-end pipeline that uses an LLM with human-in-the-loop to produce, refine, and apply labels for user intent analysis in log data. We demonstrate its effectiveness by uncovering new insights into user intents from search and chat logs from the Microsoft Bing commercial search engine. The proposed work's novelty stems from the method for generating purpose-driven user intent taxonomies with strong validation. This method not only helps remove methodological and practical bottlenecks from intent-focused research, but also provides a new framework for generating, validating, and applying other kinds of taxonomies in a scalable and adaptable way with minimal human effort.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel solution using large language models (LLMs) to generate purpose-driven user intent taxonomies with strong validation and presents an end-to-end pipeline that uses an LLM with human-in-the-loop to produce, refine, and apply labels for user intent analysis in log data."
            },
            "score": 4
        },
        {
            "id": "9ebf47129c15f61f4b77bbfe305c522480c20347",
            "paperId": "9ebf47129c15f61f4b77bbfe305c522480c20347",
            "title": "Prometheus: Inducing Fine-grained Evaluation Capability in Language Models",
            "abstract": "Recently, using a powerful proprietary Large Language Model (LLM) (e.g., GPT-4) as an evaluator for long-form responses has become the de facto standard. However, for practitioners with large-scale evaluation tasks and custom criteria in consideration (e.g., child-readability), using proprietary LLMs as an evaluator is unreliable due to the closed-source nature, uncontrolled versioning, and prohibitive costs. In this work, we propose Prometheus, a fully open-source LLM that is on par with GPT-4's evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied. We first construct the Feedback Collection, a new dataset that consists of 1K fine-grained score rubrics, 20K instructions, and 100K responses and language feedback generated by GPT-4. Using the Feedback Collection, we train Prometheus, a 13B evaluator LLM that can assess any given long-form text based on customized score rubric provided by the user. Experimental results show that Prometheus scores a Pearson correlation of 0.897 with human evaluators when evaluating with 45 customized score rubrics, which is on par with GPT-4 (0.882), and greatly outperforms ChatGPT (0.392). Furthermore, measuring correlation with GPT-4 with 1222 customized score rubrics across four benchmarks (MT Bench, Vicuna Bench, Feedback Bench, Flask Eval) shows similar trends, bolstering Prometheus's capability as an evaluator LLM. Lastly, Prometheus achieves the highest accuracy on two human preference benchmarks (HHH Alignment&MT Bench Human Judgment) compared to open-sourced reward models explicitly trained on human preference datasets, highlighting its potential as an universal reward model. We open-source our code, dataset, and model at https://kaistai.github.io/prometheus/.",
            "year": 2023,
            "citationCount": 33,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes Prometheus, a fully open-source LLM that is on par with GPT-4's evaluation capabilities when the appropriate reference materials (reference answer, score rubric) are accompanied, and achieves the highest accuracy on two human preference benchmarks compared to open-sourced reward models explicitly trained on human preference datasets, highlighting its potential as an universal reward model."
            },
            "score": 4
        },
        {
            "id": "ccac57515a8fedc0631de58879f886e827e725ad",
            "paperId": "ccac57515a8fedc0631de58879f886e827e725ad",
            "title": "ConstitutionMaker: Interactively Critiquing Large Language Models by Converting Feedback into Principles",
            "abstract": "Large language model (LLM) prompting is a promising new approach for users to create and customize their own chatbots. However, current methods for steering a chatbot's outputs, such as prompt engineering and fine-tuning, do not support users in converting their natural feedback on the model's outputs to changes in the prompt or model. In this work, we explore how to enable users to interactively refine model outputs through their feedback, by helping them convert their feedback into a set of principles (i.e. a constitution) that dictate the model's behavior. From a formative study, we (1) found that users needed support converting their feedback into principles for the chatbot and (2) classified the different principle types desired by users. Inspired by these findings, we developed ConstitutionMaker, an interactive tool for converting user feedback into principles, to steer LLM-based chatbots. With ConstitutionMaker, users can provide either positive or negative feedback in natural language, select auto-generated feedback, or rewrite the chatbot's response; each mode of feedback automatically generates a principle that is inserted into the chatbot's prompt. In a user study with 14 participants, we compare ConstitutionMaker to an ablated version, where users write their own principles. With ConstitutionMaker, participants felt that their principles could better guide the chatbot, that they could more easily convert their feedback into principles, and that they could write principles more efficiently, with less mental demand. ConstitutionMaker helped users identify ways to improve the chatbot, formulate their intuitive responses to the model into feedback, and convert this feedback into specific and clear principles. Together, these findings inform future tools that support the interactive critiquing of LLM outputs.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ConstitutionMaker, an interactive tool for converting user feedback into principles, is developed to steer LLM-based chatbots and helps users identify ways to improve the chatbot, formulate their intuitive responses to the model into feedback, and convert this feedback into specific and clear principles."
            },
            "score": 4
        },
        {
            "id": "2522410b1cac0c14fa656a0aaeaff08bacb358a9",
            "paperId": "2522410b1cac0c14fa656a0aaeaff08bacb358a9",
            "title": "InterroLang: Exploring NLP Models and Datasets through Dialogue-based Explanations",
            "abstract": "While recently developed NLP explainability methods let us open the black box in various ways (Madsen et al., 2022), a missing ingredient in this endeavor is an interactive tool offering a conversational interface. Such a dialogue system can help users explore datasets and models with explanations in a contextualized manner, e.g. via clarification or follow-up questions, and through a natural language interface. We adapt the conversational explanation framework TalkToModel (Slack et al., 2022) to the NLP domain, add new NLP-specific operations such as free-text rationalization, and illustrate its generalizability on three NLP tasks (dialogue act classification, question answering, hate speech detection). To recognize user queries for explanations, we evaluate fine-tuned and few-shot prompting models and implement a novel Adapter-based approach. We then conduct two user studies on (1) the perceived correctness and helpfulness of the dialogues, and (2) the simulatability, i.e. how objectively helpful dialogical explanations are for humans in figuring out the model's predicted label when it's not shown. We found rationalization and feature attribution were helpful in explaining the model behavior. Moreover, users could more reliably predict the model outcome based on an explanation dialogue rather than one-off explanations.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work adapts the conversational explanation framework TalkToModel to the NLP domain, adds new NLP-specific operations such as free-text rationalization and feature attribution, and illustrates its generalizability on three NLP tasks."
            },
            "score": 4
        },
        {
            "id": "4677d86972e7ffcd0ab9a02fa3ca510e51e400e9",
            "paperId": "4677d86972e7ffcd0ab9a02fa3ca510e51e400e9",
            "title": "ASR error detection using recurrent neural network language model and complementary ASR",
            "abstract": "Detecting automatic speech recognition (ASR) errors can play an important role for effective human-computer spoken dialogue system, as recognition errors can hinder accurate system understanding of user intents. Our goal is to locate errors in an utterance so that the dialogue manager can pose appropriate clarification questions to the users. We propose two approaches to improve ASR error detection: (1) using recurrent neural network language models to capture long-distance word context within and across previous utterances; (2) using a complementary ASR system. The intuition is that when two complementary ASR systems disagree on a region in an utterance, this region is most likely an error. We train a neural network predictor of errors using a variety of features. We performed experiments on both English and Iraqi Arabic ASR and observed significant improvement in error detection using the proposed methods.",
            "year": 2014,
            "citationCount": 63,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes two approaches to improve ASR error detection: using recurrent neural network language models to capture long-distance word context within and across previous utterances, and using a complementary ASR system to train a neural network predictor of errors using a variety of features."
            },
            "score": 4
        },
        {
            "id": "53871ac59c687d312dfdd89f0dde82f79c5faf76",
            "paperId": "53871ac59c687d312dfdd89f0dde82f79c5faf76",
            "title": "PAIGE: Personalized Adaptive Interactions Graph Encoder for Query Rewriting in Dialogue Systems",
            "abstract": "Unexpected responses or repeated clarification questions from conversational agents detract from the users\u2019 experience with technology meant to streamline their daily tasks. To reduce these frictions, Query Rewriting ( QR ) techniques replace transcripts of faulty queries with alternatives that lead to responses that satisfy the users\u2019 needs. Despite their successes, existing QR approaches are limited in their ability to fix queries that require considering users\u2019 personal preferences. We improve QR by proposing P ersonalized A daptive I nteractions G raph E ncoder (PAIGE). PAIGE is the first QR architecture that jointly models user\u2019s affinities and query semantics end-to-end. The core idea is to represent previous user-agent interactions and world knowledge in a structured form \u2014 a heterogeneous graph \u2014 and apply message passing to propagate latent representations of users\u2019 affinities to refine utterance embeddings. Using these embeddings, PAIGE can potentially provide different rewrites given the same query for users with different preferences. Our model, trained without any human-annotated data, improves the rewrite retrieval precision of state-of-the-art baselines by 12.5\u201317.5% while having nearly ten times fewer parameters.",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "PAIGE is the first QR architecture that jointly models user\u2019s affinities and query semantics end-to-end and improves the rewrite retrieval precision of state-of-the-art baselines by 12.5\u201317.5% while having nearly ten times fewer parameters."
            },
            "score": 4
        },
        {
            "id": "040dc42c38f74a904e1a5e51f35fa6a8c70b4e8c",
            "paperId": "040dc42c38f74a904e1a5e51f35fa6a8c70b4e8c",
            "title": "NSP-BERT: A Prompt-based Zero-Shot Learner Through an Original Pre-training Task-Next Sentence Prediction",
            "abstract": "Using prompts to utilize language models 001 to perform various downstream tasks, also 002 known as prompt-based learning or prompt-003 learning , has lately gained significant success 004 in comparison to the pre-train and fine-tune 005 paradigm. Nonetheless, virtually all prompt-006 based methods are token-level, meaning they 007 all utilize GPT\u2019s left-to-right language model 008 or BERT\u2019s masked language model to per-009 form cloze-style tasks. In this paper, we at-010 tempt to accomplish several NLP tasks in the 011 zero-shot scenario using a BERT original pre-012 training task abandoned by RoBERTa and other 013 models\u2014Next Sentence Prediction (NSP). Un-014 like token-level techniques, our sentence-level 015 prompt-based method NSP-BERT does not 016 need to fix the length of the prompt or the po-017 sition to be predicted, allowing it to handle 018 tasks such as entity linking with ease. Based on 019 the characteristics of NSP-BERT, we offer sev-020 eral quick building templates for various down-021 stream tasks. We suggest a two-stage prompt 022 method for word sense disambiguation tasks 023 in particular. Our samples-contrast method for 024 mapping the labels significantly enhance the 025 model\u2019s performance on sentence-pair tasks. 026 On the Chinese benchmark FewCLUE, our 027 NSP-BERT outperforms other zero-shot meth-028 ods on most of these tasks and comes close to 029 the few-shot methods. And on GLUE and other 030 English datasets NSP-BERT is still competitive. 031 Our code will be available on github. 032",
            "year": 2021,
            "citationCount": 37,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper accomplishes several NLP tasks in the 011 zero-shot scenario using a BERT original pre-012 training task abandoned by RoBERTa and other 013 models\u2014Next Sentence Prediction (NSP)."
            },
            "score": 4
        },
        {
            "id": "4a3caeadc881e3ed20fb8585749cf7acfc3aa833",
            "paperId": "4a3caeadc881e3ed20fb8585749cf7acfc3aa833",
            "title": "Fine-Grained Detoxification via Instance-Level Prefixes for Large Language Models",
            "abstract": "Impressive results have been achieved in natural language processing (NLP) tasks through the training of large language models (LLMs). However, these models occasionally produce toxic content such as insults, threats, and profanity in response to certain prompts, thereby constraining their practical utility. To tackle this issue, various finetuning-based and decoding-based approaches have been utilized to mitigate toxicity. However, these methods typically necessitate additional costs such as high-quality training data or auxiliary models. In this paper, we propose fine-grained detoxification via instance-level prefixes (FGDILP) to mitigate toxic text without additional cost. Specifically, FGDILP contrasts the contextualized representation in attention space using a positive prefix-prepended prompt against multiple negative prefix-prepended prompts at the instance level. This allows for constructing fine-grained subtoxicity vectors, which enables collaborative detoxification by fusing them to correct the normal generation process when provided with a raw prompt. We validate that FGDILP enables controlled text generation with regard to toxicity at both the utterance and context levels. Our method surpasses prompt-based baselines in detoxification, although at a slight cost to generation fluency and diversity.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes fine-grained detoxification via instance-level prefixes (FGDILP) to mitigate toxic text without additional cost and proves that the method surpasses prompt-based baselines in detoxification, although at a slight cost to generation fluency and diversity."
            },
            "score": 4
        },
        {
            "id": "1db56be01aeb44ca0f3fcebb45180cab1e4cd82e",
            "paperId": "1db56be01aeb44ca0f3fcebb45180cab1e4cd82e",
            "title": "AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation",
            "abstract": "The advancement of natural language processing (NLP) has been significantly boosted by the development of transformer-based large language models (LLMs). These models have revolutionized NLP tasks, particularly in code generation, aiding developers in creating software with enhanced efficiency. Despite their advancements, challenges in balancing code snippet generation with effective test case generation and execution persist. To address these issues, this paper introduces Multi-Agent Assistant Code Generation (AgentCoder), a novel solution comprising a multi-agent framework with specialized agents: the programmer agent, the test designer agent, and the test executor agent. During the coding procedure, the programmer agent will focus on the code generation and refinement based on the test executor agent's feedback. The test designer agent will generate test cases for the generated code, and the test executor agent will run the code with the test cases and write the feedback to the programmer. This collaborative system ensures robust code generation, surpassing the limitations of single-agent models and traditional methodologies. Our extensive experiments on 9 code generation models and 12 enhancement approaches showcase AgentCoder's superior performance over existing code generation models and prompt engineering techniques across various benchmarks. For example, AgentCoder achieves 77.4% and 89.1% pass@1 in HumanEval-ET and MBPP-ET with GPT-3.5, while SOTA baselines obtain only 69.5% and 63.0%.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces Multi-Agent Assistant Code Generation (AgentCoder), a novel solution comprising a multi-agent framework with specialized agents: the programmer agent, the test designer agent, and the test executor agent that ensures robust code generation, surpassing the limitations of single-agent models and traditional methodologies"
            },
            "score": 4
        },
        {
            "id": "b7bca7cc5c1a5d044a41d469eb4a9ea439b88876",
            "paperId": "b7bca7cc5c1a5d044a41d469eb4a9ea439b88876",
            "title": "Generative Large Language Models Are All-purpose Text Analytics Engines: Text-to-text Learning Is All Your Need",
            "abstract": "OBJECTIVE\nTo solve major clinical natural language processing (NLP) tasks using a unified text-to-text learning architecture based on a generative large language model (LLM) via prompt tuning.\n\n\nMETHODS\nWe formulated 7 key clinical NLP tasks as text-to-text learning and solved them using one unified generative clinical LLM, GatorTronGPT, developed using GPT-3 architecture and trained with up to 20 billion parameters. We adopted soft prompts (ie, trainable vectors) with frozen LLM, where the LLM parameters were not updated (ie, frozen) and only the vectors of soft prompts were updated, known as prompt tuning. We added additional soft prompts as a prefix to the input layer, which were optimized during the prompt tuning. We evaluated the proposed method using 7 clinical NLP tasks and compared them with previous task-specific solutions based on Transformer models.\n\n\nRESULTS AND CONCLUSION\nThe proposed approach achieved state-of-the-art performance for 5 out of 7 major clinical NLP tasks using one unified generative LLM. Our approach outperformed previous task-specific transformer models by \u223c3% for concept extraction and 7% for relation extraction applied to social determinants of health, 3.4% for clinical concept normalization, 3.4%-10% for clinical abbreviation disambiguation, and 5.5%-9% for natural language inference. Our approach also outperformed a previously developed prompt-based machine reading comprehension (MRC) model, GatorTron-MRC, for clinical concept and relation extraction. The proposed approach can deliver the \"one model for all\" promise from training to deployment using a unified generative LLM.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The proposed approach can deliver the \"one model for all\" promise from training to deployment using a unified generative LLM via prompt tuning and outperformed previous task-specific transformer models."
            },
            "score": 4
        },
        {
            "id": "c6be8510ea66521cf9d48befce4b012ac0cb0aea",
            "paperId": "c6be8510ea66521cf9d48befce4b012ac0cb0aea",
            "title": "pFedPrompt: Learning Personalized Prompt for Vision-Language Models in Federated Learning",
            "abstract": "Pre-trained vision-language models like CLIP show great potential in learning representations that capture latent characteristics of users. A recently proposed method called Contextual Optimization (CoOp) introduces the concept of training prompt for adapting pre-trained vision-language models. Given the lightweight nature of this method, researchers have migrated the paradigm from centralized to decentralized system to innovate the collaborative training framework of Federated Learning (FL). However, current prompt training in FL mainly focuses on modeling user consensus and lacks the adaptation to user characteristics, leaving the personalization of prompt largely under-explored. Researches over the past few years have applied personalized FL (pFL) approaches to customizing models for heterogeneous users. Unfortunately, we find that with the variation of modality and training behavior, directly applying the pFL methods to prompt training leads to insufficient personalization and performance. To bridge the gap, we present pFedPrompt, which leverages the unique advantage of multimodality in vision-language models by learning user consensus from linguistic space and adapting to user characteristics in visual space in a non-parametric manner. Through this dual collaboration, the learned prompt will be fully personalized and aligned to the user\u2019s local characteristics. We conduct extensive experiments across various datasets under the FL setting with statistical heterogeneity. The results demonstrate the superiority of our pFedPrompt against the alternative approaches with robust performance.",
            "year": 2023,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 3
        },
        {
            "id": "67ffe6037cf058b8c5b39f59693c4c349cc1e456",
            "paperId": "67ffe6037cf058b8c5b39f59693c4c349cc1e456",
            "title": "Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization",
            "abstract": "Federated learning (FL) is a promising paradigm to enable collaborative model training with decentralized data. However, the training process of Large Language Models (LLMs) generally incurs the update of significant parameters, which limits the applicability of FL techniques to tackle the LLMs in real scenarios. Prompt tuning can significantly reduce the number of parameters to update, but it either incurs performance degradation or low training efficiency. The straightforward utilization of prompt tuning in the FL often raises non-trivial communication costs and dramatically degrades performance. In addition, the decentralized data is generally non-Independent and Identically Distributed (non-IID), which brings client drift problems and thus poor performance. This paper proposes a Parameter-efficient prompt Tuning approach with Adaptive Optimization, i.e., FedPepTAO, to enable efficient and effective FL of LLMs. First, an efficient partial prompt tuning approach is proposed to improve performance and efficiency simultaneously. Second, a novel adaptive optimization method is developed to address the client drift problems on both the device and server sides to enhance performance further. Extensive experiments based on 10 datasets demonstrate the superb performance (up to 60.8\\% in terms of accuracy) and efficiency (up to 97.59\\% in terms of training time) of FedPepTAO compared with 9 baseline approaches. Our code is available at https://github.com/llm-eff/FedPepTAO.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Parameter-efficient prompt Tuning approach with Adaptive Optimization, i.e., FedPepTAO, to enable efficient and effective FL of LLMs and a novel adaptive optimization method is developed to address the client drift problems on both the device and server sides to enhance performance further."
            },
            "score": 3
        },
        {
            "id": "9b9cd4f1b8dcb4ee970e17ef6e9456794da47b2d",
            "paperId": "9b9cd4f1b8dcb4ee970e17ef6e9456794da47b2d",
            "title": "DPL: Decoupled Prompt Learning for Vision-Language Models",
            "abstract": "Prompt learning has emerged as an efficient and effective approach for transferring foundational Vision-Language Models (e.g., CLIP) to downstream tasks. However, current methods tend to overfit to seen categories, thereby limiting their generalization ability for unseen classes. In this paper, we propose a new method, Decoupled Prompt Learning (DPL), which reformulates the attention in prompt learning to alleviate this problem. Specifically, we theoretically investigate the collaborative process between prompts and instances (i.e., image patches/text tokens) by reformulating the original self-attention into four separate sub-processes. Through detailed analysis, we observe that certain sub-processes can be strengthened to bolster robustness and generalizability by some approximation techniques. Furthermore, we introduce language-conditioned textual prompting based on decoupled attention to naturally preserve the generalization of text input. Our approach is flexible for both visual and textual modalities, making it easily extendable to multi-modal prompt learning. By combining the proposed techniques, our approach achieves state-of-the-art performance on three representative benchmarks encompassing 15 image recognition datasets, while maintaining parameter-efficient. Moreover, our DPL does not rely on any auxiliary regularization task or extra training data, further demonstrating its remarkable generalization ability.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper theoretically investigates the collaborative process between prompts and instances by reformulating the original self-attention into four separate sub-processes, and introduces language-conditioned textual prompting based on decoupled attention to naturally preserve the generalization of text input."
            },
            "score": 3
        },
        {
            "id": "b02d37dd96f7fc8aaa9470059237893046a2cf86",
            "paperId": "b02d37dd96f7fc8aaa9470059237893046a2cf86",
            "title": "Flexible and Secure Code Deployment in Federated Learning using Large Language Models: Prompt Engineering to Enhance Malicious Code Detection",
            "abstract": "Federated Learning is a machine learning methodology that emphasizes data privacy, involving minimal interaction with each other\u2019s systems, primarily exchanging model parameters. However, this approach can introduce challenges in system development and operation because it inherently faces statistical and system heterogeneity issues. The diverse data storage formats and system environments across clients limit the feasibility of training with a uniform code. To distribute a new code to each environment, active participation of Federated Learning collaborators is necessary, incurring time and cost. Moreover, it impedes adopting modern automated development and deployment paradigms such as DevOps or MLOps. This study investigates how Large Language Models (LLMs) can automatically tailor a single code to individual client environments in heterogeneous scenarios without human intervention. Moreover, to enable the automatic adaptation of the deployed code for conducting new experiments within the system, it is imperative to assess the presence of potentially malicious code that could jeopardize data security. To address this challenge, we introduce a novel prompt engineering technique to enhance LLMs\u2019 detection capabilities, thereby bolstering our ability to detect malicious code effectively.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study investigates how Large Language Models can automatically tailor a single code to individual client environments in heterogeneous scenarios without human intervention, and introduces a novel prompt engineering technique to enhance LLMs\u2019 detection capabilities, thereby bolstering their ability to detect malicious code effectively."
            },
            "score": 3
        },
        {
            "id": "ffabde35e2437db93871801e7d733f411911813f",
            "paperId": "ffabde35e2437db93871801e7d733f411911813f",
            "title": "A Paradigm Shift: The Future of Machine Translation Lies with Large Language Models",
            "abstract": "Machine Translation (MT) has greatly advanced over the years due to the developments in deep neural networks. However, the emergence of Large Language Models (LLMs) like GPT-4 and ChatGPT is introducing a new phase in the MT domain. In this context, we believe that the future of MT is intricately tied to the capabilities of LLMs. These models not only offer vast linguistic understandings but also bring innovative methodologies, such as prompt-based techniques, that have the potential to further elevate MT. In this paper, we provide an overview of the significant enhancements in MT that are influenced by LLMs and advocate for their pivotal role in upcoming MT research and implementations. We highlight several new MT directions, emphasizing the benefits of LLMs in scenarios such as Long-Document Translation, Stylized Translation, and Interactive Translation. Additionally, we address the important concern of privacy in LLM-driven MT and suggest essential privacy-preserving strategies. By showcasing practical instances, we aim to demonstrate the advantages that LLMs offer, particularly in tasks like translating extended documents. We conclude by emphasizing the critical role of LLMs in guiding the future evolution of MT and offer a roadmap for future exploration in the sector.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper provides an overview of the significant enhancements in MT that are influenced by LLMs and advocate for their pivotal role in upcoming MT research and implementations, and highlights several new MT directions, emphasizing the benefits of LLMs in scenarios such as Long-Document Translation, Stylized Translation, and Interactive Translation."
            },
            "score": 3
        },
        {
            "id": "67198392cc038acbd2570f1ed00b8c61e0489788",
            "paperId": "67198392cc038acbd2570f1ed00b8c61e0489788",
            "title": "HELPER-X: A Unified Instructable Embodied Agent to Tackle Four Interactive Vision-Language Domains with Memory-Augmented Language Models",
            "abstract": "Recent research on instructable agents has used memory-augmented Large Language Models (LLMs) as task planners, a technique that retrieves language-program examples relevant to the input instruction and uses them as in-context examples in the LLM prompt to improve the performance of the LLM in inferring the correct action and task plans. In this technical report, we extend the capabilities of HELPER, by expanding its memory with a wider array of examples and prompts, and by integrating additional APIs for asking questions. This simple expansion of HELPER into a shared memory enables the agent to work across the domains of executing plans from dialogue, natural language instruction following, active question asking, and commonsense room reorganization. We evaluate the agent on four diverse interactive visual-language embodied agent benchmarks: ALFRED, TEACh, DialFRED, and the Tidy Task. HELPER-X achieves few-shot, state-of-the-art performance across these benchmarks using a single agent, without requiring in-domain training, and remains competitive with agents that have undergone in-domain training.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "HELPER-X achieves few-shot, state-of-the-art performance across four diverse interactive visual-language embodied agent benchmarks using a single agent, without requiring in-domain training, and remains competitive with agents that have undergone in-domain training."
            },
            "score": 3
        },
        {
            "id": "78b779bdd002a9a927adeca89d2f7650a1646029",
            "paperId": "78b779bdd002a9a927adeca89d2f7650a1646029",
            "title": "Generating Requirements Elicitation Interview Scripts with Large Language Models",
            "abstract": "Requirements elicitation interviews are the most popular requirements elicitation technique and an integral part of requirements engineering education. Good and bad interview scripts provide students with examples of applying the theory. Constructing an interview script requires technical knowledge, practical experience, and creativity. As a result, only a few educational interview scripts are available to the community. This paper explores automatically generating interview scripts with large language models through prompt engineering. Our contribution is two-fold: First, we present a graph representation of interactive interview scripts. Second, we apply prompt engineering techniques to generate business domain descriptions, linear scripts, and conversation pieces focused on certain types of mistakes. Our findings indicate that large language models face challenges in handling interview conversation graphs. However, we can enhance the quality of the generated interview scripts by decomposing the task into smaller components and refining the prompts to provide more precise instructions.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The findings indicate that large language models face challenges in handling interview conversation graphs, but the quality of the generated interview scripts can be enhanced by decomposing the task into smaller components and refining the prompts to provide more precise instructions."
            },
            "score": 3
        },
        {
            "id": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
            "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
            "title": "Training language models to follow instructions with human feedback",
            "abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",
            "year": 2022,
            "citationCount": 5935,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent and showing improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets."
            },
            "score": 3
        },
        {
            "id": "ef3a38b9f15e9dcb5652cb3f86f19b845cdaaef7",
            "paperId": "ef3a38b9f15e9dcb5652cb3f86f19b845cdaaef7",
            "title": "Chat2VIS: Generating Data Visualizations via Natural Language Using ChatGPT, Codex and GPT-3 Large Language Models",
            "abstract": "The field of data visualisation has long aimed to devise solutions for generating visualisations directly from natural language text. Research in Natural Language Interfaces (NLIs) has contributed towards the development of such techniques. However, the implementation of workable NLIs has always been challenging due to the inherent ambiguity of natural language, as well as in consequence of unclear and poorly written user queries which pose problems for existing language models in discerning user intent. Instead of pursuing the usual path of developing new iterations of language models, this study uniquely proposes leveraging the advancements in pre-trained large language models (LLMs) such as ChatGPT and GPT-3 to convert free-form natural language directly into code for appropriate visualisations. This paper presents a novel system, Chat2VIS, which takes advantage of the capabilities of LLMs and demonstrates how, with effective prompt engineering, the complex problem of language understanding can be solved more efficiently, resulting in simpler and more accurate end-to-end solutions than prior approaches. Chat2VIS shows that LLMs together with the proposed prompts offer a reliable approach to rendering visualisations from natural language queries, even when queries are highly misspecified and underspecified. This solution also presents a significant reduction in costs for the development of NLI systems, while attaining greater visualisation inference abilities compared to traditional NLP approaches that use hand-crafted grammar rules and tailored models. This study also presents how LLM prompts can be constructed in a way that preserves data security and privacy while being generalisable to different datasets. This work compares the performance of GPT-3, Codex and ChatGPT across several case studies and contrasts the performances with prior studies.",
            "year": 2023,
            "citationCount": 55,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Chat2VIS shows that LLMs together with the proposed prompts offer a reliable approach to rendering visualisations from natural language queries, even when queries are highly misspecified and underspecified."
            },
            "score": 3
        },
        {
            "id": "e6423c211fea2945aa71e1ac5ea24f8f595b4b0a",
            "paperId": "e6423c211fea2945aa71e1ac5ea24f8f595b4b0a",
            "title": "Towards Understanding Sycophancy in Language Models",
            "abstract": "Human feedback is commonly utilized to finetune AI assistants. But human feedback may also encourage model responses that match user beliefs over truthful ones, a behaviour known as sycophancy. We investigate the prevalence of sycophancy in models whose finetuning procedure made use of human feedback, and the potential role of human preference judgments in such behavior. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophancy across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a non-negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Overall, our results indicate that sycophancy is a general behavior of state-of-the-art AI assistants, likely driven in part by human preference judgments favoring sycophantic responses.",
            "year": 2023,
            "citationCount": 40,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work demonstrates that five state-of-the-art AI assistants consistently exhibit sycophancy across four varied free-form text-generation tasks, and analyzes existing human preference data to determine the role of human preference judgments in such behavior."
            },
            "score": 3
        },
        {
            "id": "9353520f5d45ba8831afaa99110f454903cbbc2f",
            "paperId": "9353520f5d45ba8831afaa99110f454903cbbc2f",
            "title": "How to disagree well: Investigating the dispute tactics used on Wikipedia",
            "abstract": "Disagreements are frequently studied from the perspective of either detecting toxicity or analysing argument structure. We propose a framework of dispute tactics which unifies these two perspectives, as well as other dialogue acts which play a role in resolving disputes, such as asking questions and providing clarification. This framework includes a preferential ordering among rebuttal-type tactics, ranging from ad hominem attacks to refuting the central argument. Using this framework, we annotate 213 disagreements (3,865 utterances) from Wikipedia Talk pages. This allows us to investigate research questions around the tactics used in disagreements; for instance, we provide empirical validation of the approach to disagreement recommended by Wikipedia. We develop models for multilabel prediction of dispute tactics in an utterance, achieving the best performance with a transformer-based label powerset model. Adding an auxiliary task to incorporate the ordering of rebuttal tactics further yields a statistically significant increase. Finally, we show that these annotations can be used to provide useful additional signals to improve performance on the task of predicting escalation.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A framework of dispute tactics which unifies these two perspectives, as well as other dialogue acts which play a role in resolving disputes, are proposed, including a preferential ordering among rebuttal-type tactics, ranging from ad hominem attacks to refuting the central argument."
            },
            "score": 3
        },
        {
            "id": "ca469d784738e4ac9d9f72253afd8650a9d0b410",
            "paperId": "ca469d784738e4ac9d9f72253afd8650a9d0b410",
            "title": "NLP-Crowdsourcing Hybrid Framework for Inter-Researcher Similarity Detection",
            "abstract": "Visualizing and examining the intellectual landscape and evolution of scientific communities to support collaboration is crucial for multiple research purposes. In some cases, measuring similarities and matching patterns between research publication document sets can help to identify people with similar interests for building research collaboration networks and university\u2013industry linkages. The premise of this work is assessing feasibility for resolving ambiguous cases in similarity detection to determine authorship with natural language processing (NLP) techniques so that crowdsourcing is applied only in instances that require human judgment. Using an NLP-crowdsourcing convergence strategy, we can reduce the costs of microtask crowdsourcing while saving time and maintaining disambiguation accuracy over large datasets. This article contributes a next-gen crowd-artificial intelligence framework that used an ensemble of term frequency-inverse document frequency and bidirectional encoder representation from transformers to obtain similarity rankings for pairs of scientific documents. A sequence of content-based similarity tasks was created using a crowd-powered interface for solving disambiguation problems. Our experimental results suggest that an adaptive NLP-crowdsourcing hybrid framework has advantages for inter-researcher similarity detection tasks where fully automatic algorithms provide unsatisfactory results, with the goal of helping researchers discover potential collaborators using data-driven approaches.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results suggest that an adaptive NLP-crowdsourcing hybrid framework has advantages for inter-researcher similarity detection tasks where fully automatic algorithms provide unsatisfactory results, with the goal of helping researchers discover potential collaborators using data-driven approaches."
            },
            "score": 3
        },
        {
            "id": "e8f13f341aecd1c4f4f8cec1c55099dd8da26452",
            "paperId": "e8f13f341aecd1c4f4f8cec1c55099dd8da26452",
            "title": "Abstract 5721: Automated annotation for large-scale clinicogenomic models of lung cancer treatment response and overall survival",
            "abstract": "\n The digitization of health records and prompt availability of tumor DNA sequencing results offer a chance to study the determinants of cancer outcomes with unprecedented richness; however, abstraction of key attributes from free text presents a major limitation to large-scale analyses. Using natural language processing (NLP), we derived sites of metastasis, prior treatment at outside institutions, programmed death ligand 1 (PD-L1) levels, and smoking status from records of patients with tumor sequencing to create a richly annotated clinicogenomic cohort. We sought to define whether combining features would improve models of overall survival (OS) and treatment response as validated in a multi-institution, manually curated cohort. We leveraged the manually curated AACR GENIE Biopharma Collaborative (BPC) dataset to train NLP algorithms to abstract the aforementioned features from overlapping records available at Memorial Sloan Kettering (MSK). All models achieved precision and recall > 0.85. We deployed these algorithms to records of all MSK patients with non-small cell lung cancer (NSCLC) and tumor profiling with our FDA-authorized institutional targeted sequencing platform (N=7,015). These labels were combined with genomic, demographic, histopathologic, internal treatment and staging data to train random survival forests (RSF) to predict OS and time-to-next-treatment (TTNT) for molecularly targeted and immunotherapies. RSFs trained on the MSK NSCLC cohort were validated with the curated, non-MSK BPC NSCLC cohort (N=977). The addition of NLP-derived variables to genomic features enhanced RSF predictive power for OS (c-index, 10x bootstrap 95%CI: 0.58, 0.57-0.59 vs 0.75, 0.74-0.76 combined) and targeted and immunotherapy TTNT. The size of the MSK NSCLC cohort enabled discovery of associations between metastatic sites, PD-L1 status, genomics, and TTNTs not apparent in the smaller BPC cohort. We measured the added predictive value of variables not available in BPC with MSK-only cross-validation analyses. White blood cell differential counts and additional tissue genomic features including tumor mutational burden and fraction genome altered added minimally, while circulating tumor DNA sequencing added prognostic power for OS over other factors including disease burden\n Using NLP we present a large NSCLC cohort with rich clinicoradiographic annotation, leading to superior models of patient outcomes. Our data uncovers associations not observed in smaller, manually curated cohorts and provides a foundation for further research in therapy choice and prognostication.\n Citation Format: Justin Jee, Chris Fong, Karl Pichotta, Thinh Tran, Anisha Luthra, Mirella Altoe, Steven Maron, Ronglai Shen, Si-Yang Liu, Michele Waters, Joseph Kholodenko, Brooke Mastrogiacomo, Susie Kim, A Rose Brannon, Michael F. Berger, Axel Martin, Jason Chang, Anton Safonov, Jorge S. Reis-Filho, Deborah Schrag, Sohrab P. Shah, Pedram Razavi, Bob T. Li, Gregory J. Riely, Nikolaus Schultz. Automated annotation for large-scale clinicogenomic models of lung cancer treatment response and overall survival. [abstract]. In: Proceedings of the American Association for Cancer Research Annual Meeting 2023; Part 1 (Regular and Invited Abstracts); 2023 Apr 14-19; Orlando, FL. Philadelphia (PA): AACR; Cancer Res 2023;83(7_Suppl):Abstract nr 5721.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A large NSCLC cohort with rich clinicoradiographic annotation is presented, leading to superior models of patient outcomes and provides a foundation for further research in therapy choice and prognostication."
            },
            "score": 3
        },
        {
            "id": "42aa79b825616927348b03fab5eb33a38750ab44",
            "paperId": "42aa79b825616927348b03fab5eb33a38750ab44",
            "title": "A Continual Pre-training Approach to Tele-Triaging Pregnant Women in Kenya",
            "abstract": "Access to high-quality maternal health care services is limited in Kenya, which resulted in \u223c36,000 maternal and neonatal deaths in 2018. To tackle this challenge, Jacaranda Health (a non-profit organization working on maternal health in Kenya) developed PROMPTS, an SMS based tele-triage system for pregnant and puerperal women, which has more than 350,000 active users in Kenya. PROMPTS empowers pregnant women living far away from doctors and hospitals to send SMS messages to get quick answers (through human helpdesk agents) to questions about their medical symptoms and pregnancy status. Unfortunately, \u223c1.1 million SMS messages are received by PROMPTS every month, which makes it challenging for helpdesk agents to ensure that these messages can be interpreted correctly and evaluated by their level of emergency to ensure timely responses and/or treatments for women in need. This paper reports on a collaborative effort with Jacaranda Health to develop a state-of-the-art natural language processing (NLP) framework, TRIM-AI (TRIage for Mothers using AI), which can automatically predict the emergency level (or severity of medical condition) of a pregnant mother based on the content of their SMS messages. TRIM-AI leverages recent advances in multi-lingual pre-training and continual pre-training to tackle code-mixed SMS messages (between English and Swahili), and achieves a weighted F1 score of 0.774 on real-world datasets. TRIM-AI has been successfully deployed in the field since June 2022, and is being used by Jacaranda Health to prioritize the provision of services and care to pregnant women with the most critical medical conditions. Our preliminary A/B tests in the field show that TRIM-AI is \u223c17% more accurate at predicting high-risk medical conditions from SMS messages sent by pregnant Kenyan mothers, which reduces the helpdesk\u2019s workload by \u223c12%.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A state-of-the-art natural language processing (NLP) framework, TRIM-AI (TRIage for Mothers using AI), which can automatically predict the emergency level (or severity of medical condition) of a pregnant mother based on the content of their SMS messages."
            },
            "score": 3
        },
        {
            "id": "130640ebaa8f20ef219f84786ce5edf89d58c072",
            "paperId": "130640ebaa8f20ef219f84786ce5edf89d58c072",
            "title": "Integrated Telegram and Web-based Forum with Automatic Assessment of Questions and Answers for Collaborative Learning",
            "abstract": "We designed, developed and implemented an integrated Telegram and Web-based Forum capable of automatic thoughtfulness assessment of questions and answers, using text mining and Natural Language Processing (NLP) methodologies, for collaborative learning. We trained and applied the Random Forest algorithm to provide instant thoughtfulness score prediction for the new posts contributed by the students, and prompted the students to improve on their posts, thereby invoking deeper learning. In addition, the platform is designed to ensure that students remain engaged on the platform. Our initial implementation for a university course on spreadsheets modeling using Excel, shows some promising results which strengthened our belief that through the process of asking good questions, and providing good answers, students' learning can be improved beyond the traditional classroom, and beyond instructor-student relationship.",
            "year": 2018,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An integrated Telegram and Web-based Forum capable of automatic thoughtfulness assessment of questions and answers, using text mining and Natural Language Processing (NLP) methodologies, for collaborative learning, shows some promising results which strengthened the belief that students' learning can be improved beyond the traditional classroom, and beyond instructor-student relationship."
            },
            "score": 3
        },
        {
            "id": "e96be7c55d139965b15bc0527d6d528b225f9a61",
            "paperId": "e96be7c55d139965b15bc0527d6d528b225f9a61",
            "title": "ClickPrompt: CTR Models are Strong Prompt Generators for Adapting Language Models to CTR Prediction",
            "abstract": "Click-through rate (CTR) prediction has become increasingly indispensable for various Internet applications. Traditional CTR models convert the multi-field categorical data into ID features via one-hot encoding, and extract the collaborative signals among features. Such a paradigm suffers from the problem of semantic information loss. Another line of research explores the potential of pretrained language models (PLMs) for CTR prediction by converting input data into textual sentences through hard prompt templates. Although semantic signals are preserved, they generally fail to capture the collaborative information (e.g., feature interactions, pure ID features), not to mention the unacceptable inference overhead brought by the huge model size. In this paper, we aim to model both the semantic knowledge and collaborative knowledge for accurate CTR estimation, and meanwhile address the inference inefficiency issue. To benefit from both worlds and close their gaps, we propose a novel model-agnostic framework (i.e., ClickPrompt), where we incorporate CTR models to generate interaction-aware soft prompts for PLMs. We design a prompt-augmented masked language modeling (PA-MLM) pretraining task, where PLM has to recover the masked tokens based on the language context, as well as the soft prompts generated by CTR model. The collaborative and semantic knowledge from ID and textual features would be explicitly aligned and interacted via the prompt interface. Then, we can either tune the CTR model with PLM for superior performance, or solely tune the CTR model without PLM for inference efficiency. Experiments on four real-world datasets validate the effectiveness of ClickPrompt compared with existing baselines.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper designs a prompt-augmented masked language modeling (PA-MLM) pretraining task, where PLM has to recover the masked tokens based on the language context, as well as the soft prompts generated by CTR model, to incorporate CTR models to generate interaction-aware soft prompts for PLMs."
            },
            "score": 2
        },
        {
            "id": "b17cc18e4130505b939f7d527082eb6be2a7fd5b",
            "paperId": "b17cc18e4130505b939f7d527082eb6be2a7fd5b",
            "title": "Rationale-Augmented Ensembles in Language Models",
            "abstract": "Recent research has shown that rationales, or step-by-step chains of thought, can be used to improve performance in multi-step reasoning tasks. We reconsider rationale-augmented prompting for few-shot in-context learning, where (input ->output) prompts are expanded to (input, rationale ->output) prompts. For rationale-augmented prompting we demonstrate how existing approaches, which rely on manual prompt engineering, are subject to sub-optimal rationales that may harm performance. To mitigate this brittleness, we propose a unified framework of rationale-augmented ensembles, where we identify rationale sampling in the output space as the key component to robustly improve performance. This framework is general and can easily be extended to common natural language processing tasks, even those that do not traditionally leverage intermediate steps, such as question answering, word sense disambiguation, and sentiment analysis. We demonstrate that rationale-augmented ensembles achieve more accurate and interpretable results than existing prompting approaches--including standard prompting without rationales and rationale-based chain-of-thought prompting--while simultaneously improving interpretability of model predictions through the associated rationales.",
            "year": 2022,
            "citationCount": 94,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that rationale-augmented ensembles achieve more accurate and interpretable results than existing prompting approaches--including standard prompting without rationales and rationale-based chain-of-thought prompting--while simultaneously improving interpretability of model predictions through the associated rationales."
            },
            "score": 2
        },
        {
            "id": "b2a0f97d9135f91f28196e1f8adea9ac79b230c2",
            "paperId": "b2a0f97d9135f91f28196e1f8adea9ac79b230c2",
            "title": "Large Language Models and Multimodal Retrieval for Visual Word Sense Disambiguation",
            "abstract": "Visual Word Sense Disambiguation (VWSD) is a novel challenging task with the goal of retrieving an image among a set of candidates, which better represents the meaning of an ambiguous word within a given context. In this paper, we make a substantial step towards unveiling this interesting task by applying a varying set of approaches. Since VWSD is primarily a text-image retrieval task, we explore the latest transformer-based methods for multimodal retrieval. Additionally, we utilize Large Language Models (LLMs) as knowledge bases to enhance the given phrases and resolve ambiguity related to the target word. We also study VWSD as a unimodal problem by converting to text-to-text and image-to-image retrieval, as well as question-answering (QA), to fully explore the capabilities of relevant models. To tap into the implicit knowledge of LLMs, we experiment with Chain-of-Thought (CoT) prompting to guide explainable answer generation. On top of all, we train a learn to rank (LTR) model in order to combine our different modules, achieving competitive ranking results. Extensive experiments on VWSD demonstrate valuable insights to effectively drive future directions.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper trains a learn to rank (LTR) model in order to combine the different modules of VWSD by converting to text-to-text and image- to-image retrieval, as well as question-answering (QA), to fully explore the capabilities of relevant models."
            },
            "score": 2
        },
        {
            "id": "9426ed538470ad98b881a20fd9725bf8536a674f",
            "paperId": "9426ed538470ad98b881a20fd9725bf8536a674f",
            "title": "FLIP: Towards Fine-grained Alignment between ID-based Models and Pretrained Language Models for CTR Prediction",
            "abstract": "Click-through rate (CTR) prediction plays as a core function module in various personalized online services. The traditional ID-based models for CTR prediction take as inputs the one-hot encoded ID features of tabular modality, which capture the collaborative signals via feature interaction modeling. But the one-hot encoding discards the semantic information conceived in the original feature texts. Recently, the emergence of Pretrained Language Models (PLMs) has given rise to another paradigm, which takes as inputs the sentences of textual modality obtained by hard prompt templates and adopts PLMs to extract the semantic knowledge. However, PLMs generally tokenize the input text data into subword tokens and ignore field-wise collaborative signals. Therefore, these two lines of research focus on different characteristics of the same input data (i.e., textual and tabular modalities), forming a distinct complementary relationship with each other. In this paper, we propose to conduct Fine-grained feature-level ALignment between ID-based Models and Pretrained Language Models (FLIP) for CTR prediction. We design a novel joint reconstruction pretraining task for both masked language and tabular modeling. Specifically, the masked data of one modality (i.e., tokens or features) has to be recovered with the help of the other modality, which establishes the feature-level interaction and alignment via sufficient mutual information extraction between dual modalities. Moreover, we propose to jointly finetune the ID-based model and PLM for downstream CTR prediction tasks, thus achieving superior performance by combining the advantages of both models. Extensive experiments on three real-world datasets demonstrate that FLIP outperforms SOTA baselines, and is highly compatible for various ID-based models and PLMs.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel joint reconstruction pretraining task for both masked language and tabular modeling, which proposes to jointly finetune the ID-based model and PLM for downstream CTR prediction tasks, thus achieving superior performance by combining the advantages of both models."
            },
            "score": 2
        },
        {
            "id": "61bbdbf481a6d3519c22513ebe8d6c3cd381851e",
            "paperId": "61bbdbf481a6d3519c22513ebe8d6c3cd381851e",
            "title": "Language Models as Knowledge Bases for Visual Word Sense Disambiguation",
            "abstract": "Visual Word Sense Disambiguation (VWSD) is a novel challenging task that lies between linguistic sense disambiguation and fine-grained multimodal retrieval. The recent advancements in the development of visiolinguistic (VL) transformers suggest some off-the-self implementations with encouraging results, which however we argue that can be further improved. To this end, we propose some knowledge-enhancement techniques towards improving the retrieval performance of VL transformers via the usage of Large Language Models (LLMs) as Knowledge Bases. More specifically, knowledge stored in LLMs is retrieved with the help of appropriate prompts in a zero-shot manner, achieving performance advancements. Moreover, we convert VWSD to a purely textual question-answering (QA) problem by considering generated image captions as multiple-choice candidate answers. Zero-shot and few-shot prompting strategies are leveraged to explore the potential of such a transformation, while Chain-of-Thought (CoT) prompting in the zero-shot setting is able to reveal the internal reasoning steps an LLM follows to select the appropriate candidate. In total, our presented approach is the first one to analyze the merits of exploiting knowledge stored in LLMs in different ways to solve WVSD.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The presented approach is the first to analyze the merits of exploiting knowledge stored in LLMs in different ways to solve WVSD, and converts VWSD to a purely textual question-answering (QA) problem by considering generated image captions as multiple-choice candidate answers."
            },
            "score": 2
        },
        {
            "id": "846db95f3c330d777f76692dcc7534dc364149c8",
            "paperId": "846db95f3c330d777f76692dcc7534dc364149c8",
            "title": "Translate to Disambiguate: Zero-shot Multilingual Word Sense Disambiguation with Pretrained Language Models",
            "abstract": "Pretrained Language Models (PLMs) learn rich cross-lingual knowledge and perform well on diverse tasks such as translation and multilingual word sense disambiguation (WSD) when finetuned. However, they often struggle at disambiguating word sense in a zero-shot setting. To better understand this contrast, we present a new study investigating how well PLMs capture cross-lingual word sense with Contextual Word-Level Translation (C-WLT), an extension of word-level translation that prompts the model to translate a given word in context. We find that as the model size increases, PLMs encode more cross-lingual word sense knowledge and better use context to improve WLT performance. Building on C-WLT, we introduce a zero-shot prompting approach for WSD, tested on 18 languages from the XL-WSD dataset. Our method outperforms fully supervised baselines on recall for many evaluation languages without additional training or finetuning. This study presents a first step towards understanding how to best leverage the cross-lingual knowledge inside PLMs for robust zero-shot reasoning in any language.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A zero-shot prompting approach for WSD, tested on 18 languages from the XL-WSD dataset, which outperforms fully supervised baselines on recall for many evaluation languages without additional training or finetuning."
            },
            "score": 2
        },
        {
            "id": "b13106e918aa098a7666feca915c111a19790500",
            "paperId": "b13106e918aa098a7666feca915c111a19790500",
            "title": "Black-Box Tuning of Vision-Language Models with Effective Gradient Approximation",
            "abstract": "Parameter-efficient fine-tuning (PEFT) methods have provided an effective way for adapting large vision-language models to specific tasks or scenarios. Typically, they learn a very small scale of parameters for pre-trained models in a white-box formulation, which assumes model architectures to be known and parameters to be accessible. However, large models are often not open-source due to considerations of preventing abuse or commercial factors, hence posing a barrier to the deployment of white-box PEFT methods. To alleviate the dependence on model accessibility, we introduce collaborative black-box tuning (CBBT) for both textual prompt optimization and output feature adaptation for black-box models. Specifically, considering that the backpropagation gradients are blocked, we approximate the gradients of textual prompts by analyzing the predictions with perturbed prompts. Secondly, a lightweight adapter is deployed over the output feature of the inaccessible model, further facilitating the model adaptation process. Empowered with these designs, our CBBT is extensively evaluated on eleven downstream benchmarks and achieves remarkable improvements compared to existing black-box VL adaptation methods. Code is released at https://github.com/guozix/cbbt.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces collaborative black-box tuning (CBBT) for both textual prompt optimization and output feature adaptation for black-box models and achieves remarkable improvements compared to existing black-box VL adaptation methods."
            },
            "score": 2
        },
        {
            "id": "b8a81fa02a5e5c7e427ee62aaf60d7a55b0d4d63",
            "paperId": "b8a81fa02a5e5c7e427ee62aaf60d7a55b0d4d63",
            "title": "SUT at SemEval-2023 Task 1: Prompt Generation for Visual Word Sense Disambiguation",
            "abstract": "Visual Word Sense Disambiguation (V-WSD) identifies the correct visual sense of a multi-sense word in a specific context. This can be challenging as images may need to provide additional context and words may have multiple senses. A proper V-WSD system can benefit applications like image retrieval and captioning. This paper proposes a Prompt Generation approach to solve this challenge. This approach improves the robustness of language-image models like CLIP to contextual ambiguities and helps them better correlate between textual and visual contexts of different senses of words.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This approach improves the robustness of language-image models like CLIP to contextual ambiguities and helps them better correlate between textual and visual contexts of different senses of words."
            },
            "score": 2
        },
        {
            "id": "421a3f9355c83e30f9b6fd0df2f295430fc1766d",
            "paperId": "421a3f9355c83e30f9b6fd0df2f295430fc1766d",
            "title": "Promptify: Text-to-Image Generation through Interactive Prompt Exploration with Large Language Models",
            "abstract": "Text-to-image generative models have demonstrated remarkable capabilities in generating high-quality images based on textual prompts. However, crafting prompts that accurately capture the user\u2019s creative intent remains challenging. It often involves laborious trial-and-error procedures to ensure that the model interprets the prompts in alignment with the user\u2019s intention. To address these challenges, we present Promptify, an interactive system that supports prompt exploration and refinement for text-to-image generative models. Promptify utilizes a suggestion engine powered by large language models to help users quickly explore and craft diverse prompts. Our interface allows users to organize the generated images flexibly, and based on their preferences, Promptify suggests potential changes to the original prompt. This feedback loop enables users to iteratively refine their prompts and enhance desired features while avoiding unwanted ones. Our user study shows that Promptify effectively facilitates the text-to-image workflow, allowing users to create visually appealing images on their first attempt while requiring significantly less cognitive load than a widely-used baseline tool.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents Promptify, an interactive system that supports prompt exploration and refinement for text-to-image generative models, and utilizes a suggestion engine powered by large language models to help users quickly explore and craft diverse prompts."
            },
            "score": 2
        },
        {
            "id": "90e0a949da276b66d13920a185a6e35042337518",
            "paperId": "90e0a949da276b66d13920a185a6e35042337518",
            "title": "Zero-Shot Cross-Lingual Summarization via Large Language Models",
            "abstract": "Given a document in a source language, cross-lingual summarization (CLS) aims to generate a summary in a different target language. Recently, the emergence of Large Language Models (LLMs), such as GPT-3.5, ChatGPT and GPT-4, has attracted wide attention from the computational linguistics community. However, it is not yet known the performance of LLMs on CLS. In this report, we empirically use various prompts to guide LLMs to perform zero-shot CLS from different paradigms (i.e., end-to-end and pipeline), and provide a preliminary evaluation on the generated summaries. We find that ChatGPT and GPT-4 originally prefer to produce lengthy summaries with detailed information. These two LLMs can further balance informativeness and conciseness with the help of an interactive prompt, significantly improving their CLS performance. Experimental results on three widely-used CLS datasets show that GPT-4 achieves state-of-the-art zero-shot CLS performance, and performs competitively compared with the fine-tuned mBART-50. Moreover, we also find some multi-lingual and bilingual LLMs (i.e., BLOOMZ, ChatGLM-6B, Vicuna-13B and ChatYuan) have limited zero-shot CLS ability. Due to the composite nature of CLS, which requires models to perform summarization and translation simultaneously, accomplishing this task in a zero-shot manner is even a challenge for LLMs. Therefore, we sincerely hope and recommend future LLM research could use CLS as a testbed.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that ChatGPT and GPT-4 originally prefer to produce lengthy summaries with detailed information, and these two LLMs can further balance informativeness and conciseness with the help of an interactive prompt, significantly improving their CLS performance."
            },
            "score": 2
        },
        {
            "id": "e38a29f6463f38f43797b128673b9e44d18a991e",
            "paperId": "e38a29f6463f38f43797b128673b9e44d18a991e",
            "title": "Whose Opinions Do Language Models Reflect?",
            "abstract": "Language models (LMs) are increasingly being used in open-ended contexts, where the opinions reflected by LMs in response to subjective queries can have a profound impact, both on user satisfaction, as well as shaping the views of society at large. In this work, we put forth a quantitative framework to investigate the opinions reflected by LMs -- by leveraging high-quality public opinion polls and their associated human responses. Using this framework, we create OpinionsQA, a new dataset for evaluating the alignment of LM opinions with those of 60 US demographic groups over topics ranging from abortion to automation. Across topics, we find substantial misalignment between the views reflected by current LMs and those of US demographic groups: on par with the Democrat-Republican divide on climate change. Notably, this misalignment persists even after explicitly steering the LMs towards particular demographic groups. Our analysis not only confirms prior observations about the left-leaning tendencies of some human feedback-tuned LMs, but also surfaces groups whose opinions are poorly reflected by current LMs (e.g., 65+ and widowed individuals). Our code and data are available at https://github.com/tatsu-lab/opinions_qa.",
            "year": 2023,
            "citationCount": 135,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work creates OpinionsQA, a new dataset for evaluating the alignment of LM opinions with those of 60 US demographic groups over topics ranging from abortion to automation, and finds substantial misalignment."
            },
            "score": 2
        },
        {
            "id": "32dcd0887537cece54e214f531d2c384470b023f",
            "paperId": "32dcd0887537cece54e214f531d2c384470b023f",
            "title": "Large Language Models as Tool Makers",
            "abstract": "Recent research has highlighted the potential of large language models (LLMs) to improve their problem-solving capabilities with the aid of suitable external tools. In our work, we further advance this concept by introducing a closed-loop framework, referred to as LLMs A s Tool Makers (LATM), where LLMs create their own reusable tools for problem-solving. Our approach consists of two phases: 1) tool making: an LLM acts as the tool maker that crafts tools for a set of tasks. 2) tool using: another LLM acts as the tool user, which applies the tool built by the tool maker for problem-solving. On the problem-solving server side, tool-making enables continual tool generation and caching as new requests emerge. This framework enables subsequent requests to access cached tools via their corresponding APIs, enhancing the efficiency of task resolution. Recognizing that tool-making requires more sophisticated capabilities, we assign this task to a powerful, albeit resource-intensive, model. Conversely, the simpler tool-using phase is delegated to a lightweight model. This strategic division of labor allows the once-off cost of tool-making to be spread over multiple instances of tool-using, significantly reducing average costs while maintaining strong performance. Furthermore, our method offers a functional cache through the caching and reuse of tools, which stores the functionality of a class of requests instead of the natural language responses from LLMs, thus extending the applicability of the conventional cache mechanism. We evaluate our approach across various complex reasoning tasks, including Big-Bench tasks. With GPT-4 as the tool maker and GPT-3.5 as the tool user, LATM demonstrates performance equivalent to using GPT-4 for both roles, but with a significantly reduced inference cost.",
            "year": 2023,
            "citationCount": 100,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a closed-loop framework, referred to as LLMs A s Tool Makers (LATM), where LLMs create their own reusable tools for problem-solving, and offers a functional cache through the caching and reuse of tools, which stores the functionality of a class of requests instead of the natural language responses from LLMs, thus extending the applicability of the conventional cache mechanism."
            },
            "score": 2
        },
        {
            "id": "68838aba1cc6e20028f9703b96e3517b01972277",
            "paperId": "68838aba1cc6e20028f9703b96e3517b01972277",
            "title": "Large language models can accurately predict searcher preferences",
            "abstract": "Relevance labels, which indicate whether a search result is valuable to a searcher, are key to evaluating and optimising search systems. The best way to capture the true preferences of users is to ask them for their careful feedback on which results would be useful, but this approach does not scale to produce a large number of labels. Getting relevance labels at scale is usually done with third-party labellers, who judge on behalf of the user, but there is a risk of low-quality data if the labeller doesn't understand user needs. To improve quality, one standard approach is to study real users through interviews, user studies and direct feedback, find areas where labels are systematically disagreeing with users, then educate labellers about user needs through judging guidelines, training and monitoring. This paper introduces an alternate approach for improving label quality. It takes careful feedback from real users, which by definition is the highest-quality first-party gold data that can be derived, and develops an large language model prompt that agrees with that data. We present ideas and observations from deploying language models for large-scale relevance labelling at Bing, and illustrate with data from TREC. We have found large language models can be effective, with accuracy as good as human labellers and similar capability to pick the hardest queries, best runs, and best groups. Systematic changes to the prompts make a difference in accuracy, but so too do simple paraphrases. To measure agreement with real searchers needs high-quality ``gold'' labels, but with these we find that models produce better labels than third-party workers, for a fraction of the cost, and these labels let us train notably better rankers.",
            "year": 2023,
            "citationCount": 37,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces an alternate approach for improving label quality, which takes careful feedback from real users, which by definition is the highest-quality first-party gold data that can be derived, and develops an large language model prompt that agrees with that data."
            },
            "score": 2
        },
        {
            "id": "f95fb254da1f7b74a14ac8feb3f834d64e67e4cd",
            "paperId": "f95fb254da1f7b74a14ac8feb3f834d64e67e4cd",
            "title": "Does Writing with Language Models Reduce Content Diversity?",
            "abstract": "Large language models (LLMs) have led to a surge in collaborative writing with model assistance. As different users incorporate suggestions from the same model, there is a risk of decreased diversity in the produced content, potentially limiting diverse perspectives in public discourse. In this work, we measure the impact of co-writing on diversity via a controlled experiment, where users write argumentative essays in three setups -- using a base LLM (GPT3), a feedback-tuned LLM (InstructGPT), and writing without model help. We develop a set of diversity metrics and find that writing with InstructGPT (but not the GPT3) results in a statistically significant reduction in diversity. Specifically, it increases the similarity between the writings of different authors and reduces the overall lexical and content diversity. We additionally find that this effect is mainly attributable to InstructGPT contributing less diverse text to co-written essays. In contrast, the user-contributed text remains unaffected by model collaboration. This suggests that the recent improvement in generation quality from adapting models to human feedback might come at the cost of more homogeneous and less diverse content.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work develops a set of diversity metrics and finds that writing with InstructGPT (but not the GPT3) results in a statistically significant reduction in diversity, suggesting that the recent improvement in generation quality from adapting models to human feedback might come at the cost of more homogeneous and less diverse content."
            },
            "score": 2
        },
        {
            "id": "36cd3d3a9e8a64f322476153513ece1fd617acfc",
            "paperId": "36cd3d3a9e8a64f322476153513ece1fd617acfc",
            "title": "nl2spec: Interactively Translating Unstructured Natural Language to Temporal Logics with Large Language Models",
            "abstract": "A rigorous formalization of desired system requirements is indispensable when performing any verification task. This often limits the application of verification techniques, as writing formal specifications is an error-prone and time-consuming manual task. To facilitate this, we present nl2spec, a framework for applying Large Language Models (LLMs) to derive formal specifications (in temporal logics) from unstructured natural language. In particular, we introduce a new methodology to detect and resolve the inherent ambiguity of system requirements in natural language: we utilize LLMs to map subformulas of the formalization back to the corresponding natural language fragments of the input. Users iteratively add, delete, and edit these sub-translations to amend erroneous formalizations, which is easier than manually redrafting the entire formalization. The framework is agnostic to specific application domains and can be extended to similar specification languages and new neural models. We perform a user study to obtain a challenging dataset, which we use to run experiments on the quality of translations. We provide an open-source implementation, including a web-based frontend.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 2
        },
        {
            "id": "eab7e876257923cd84339b857902501eb81c0b91",
            "paperId": "eab7e876257923cd84339b857902501eb81c0b91",
            "title": "Innovative Personal Assistance: Speech Recognition and NLP-Driven Robot Prototype",
            "abstract": "This paper presents the development and evaluation of a personal assistant robot prototype with advanced speech recognition and natural language processing (NLP) capabilities. Powered by a Raspberry Pi microprocessor, it is the core component of the robot's hardware. It is designed to receive commands and promptly respond by performing the requested actions, utilizing integrated speech recognition and NLP technologies. The prototype aims to enhance meeting efficiency and productivity through audio-to-text conversion and high-quality image capture. Results show excellent performance, with accuracy rates of 100% in Indonesian and 99% in English. The efficient processing speed, averaging 9.07 seconds per minute in Indonesian and 15.3 seconds per minute in English, further enhances the robot's functionality. Additionally, integrating a high-resolution webcam enables high-quality image capture at 1280 x 720 pixels. Real-time integration with Google Drive ensures secure storage and seamless data management. The findings highlight the prototype's effectiveness in facilitating smooth interactions and effective communication, leveraging NLP for intelligent language understanding. Integrating NLP-based speech recognition, visual documentation, and data transfer provides a comprehensive platform for managing audio, text, and image data. The personal assistant robot prototype presented in this research represents a significant advancement in human-robot interaction, particularly in meeting and collaborative work settings. Further refinements in NLP can enhance efficiency and foster seamless human-robot interaction experiences.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The personal assistant robot prototype presented in this research represents a significant advancement in human-robot interaction, particularly in meeting and collaborative work settings, as well as a comprehensive platform for managing audio, text, and image data."
            },
            "score": 2
        },
        {
            "id": "470eaf90d75793bba9be03025826b977b9ad8bf4",
            "paperId": "470eaf90d75793bba9be03025826b977b9ad8bf4",
            "title": "ChatGPT\u2019s Significance for Theology",
            "abstract": "Welcome to the inaugural editorial of the Theology and Science AI and Faith editorial series. This collaborative effort between the Center for Theology and the Natural Sciences and the AI and Faith community of experts explores the theological and ethical implications of emerging AI technology. It is increasingly clear that artificial intelligence (AI) and machine learning (ML) have progressed to the point where the question is no longer if or when AI will impact theology, but rather how it is already having an impact. In this initial editorial, we examine the specific example of ChatGPT and its potential implications for theological education, theologies of culture, systematic theology, and theological scholarship. ChatGPT has captured the attention of AI researchers, educators, and journalists for its ability to produce consistently coherent text on a wide range of topics. Researchers have observed notable improvements in ChatGPT\u2019s responses to query prompts compared to previous natural language processing (NLP) systems. However, educators have expressed concern about the potential for high school and undergraduate students to use the tool for writing assignments, and journalists have noted the potential impact on jobs such as copywriting that overlap with their skills. Despite the occasional factual errors and its dependence upon human labor behind the scenes to avoid controversial responses, ChatGPT\u2019s writing is comparable to that of an average college student and is free of grammatical errors. Among researchers, practitioners, and close observers of AI technology, it appears that a significant performance threshold has been crossed with ChatGPT and its related technologies. In this editorial, I will explore that threshold and the theological implications of its crossing. One aspect of the threshold that ChatGPT appears to cross is well characterized by Bloom\u2019s Taxonomy of Educational Objectives, which categorizes critical thinking skills: remember, understand, apply, analyze, evaluate, and create (2001 version). ChatGPT and similar NLP technologies that shift the ability of NLP to generate text have shifted their abilities from recalling facts and basic concepts (Bloom\u2019s \u201cremember\u201d category) to explaining ideas and concepts (Bloom\u2019s \u201cunderstand\u201d category). This means that if undergraduate writing assignments in theology or religious studies require understanding generally available material, then many of those tasks can now be performed by AI. However, the technology currently cannot apply that knowledge beyond a few fixed tasks, like responding in a chat. Therefore, essay questions that ask students to apply or analyze their knowledge in a more critical way are still not adequately answerable by AI. An exciting aspect of ChatGPT for some AI researchers and technologists is not only its novel ability to demonstrate understanding, but also its potential role as a key puzzle piece in constructing the linguistic component of more general AI capability across contexts, perhaps even toward what some have called artificial general intelligence (AGI). Natural language processing (NLP) can already perform more critical tasks in narrow areas of language, including evaluating the similarity between a student\u2019s written answer and a professor\u2019s key or applying spoken commands to a robot or home device actions. However, progress in computer generation of high quality, paragraph-length texts has lagged behind research into understanding such texts, in part due to the challenges of appropriately channeling the limitless expressivity of language. Now that language can be meaningfully and",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This initial editorial of the Theology and Science AI and Faith editorial series examines the specific example of ChatGPT and its potential implications for theological education, theologies of culture, systematic theology, and theological scholarship."
            },
            "score": 2
        },
        {
            "id": "f3866315667250d6fee6a6fe0b9fcac793036f7b",
            "paperId": "f3866315667250d6fee6a6fe0b9fcac793036f7b",
            "title": "Artificial Intelligence in Project Management: A Study of The Role of Ai-Powered Chatbots in Project Stakeholder Engagement",
            "abstract": "Artificial Intelligence (AI) is increasingly becoming a cornerstone in the evolution of project management. Its capabilities extend beyond simple automation, fostering improved decision-making processes and enhancing collaborative efforts. Among the various AI tools available, chatbots stand out as particularly transformative for project management. This study delves into the role of AI-powered chatbots in project stakeholder engagement, a critical aspect of successful project management. Chatbots, powered by sophisticated AI algorithms, can provide continuous support and interaction with project stakeholders. This is particularly vital in managing complex projects where continuous communication and prompt responses can significantly influence project success. The study examines how these AI-driven chatbots facilitate stakeholder engagement, focusing on key benefits such as improved communication, increased stakeholder satisfaction, and better overall project outcomes. Through a detailed analysis, we have identified that chatbots enhance communication by offering stakeholders immediate, personalized responses, thereby reducing response times and improving the efficiency of information exchange. This immediacy and personalization contribute to heightened stakeholder satisfaction, as stakeholders feel their concerns and queries are addressed promptly and effectively. Furthermore, our findings suggest that these improvements in stakeholder engagement directly correlate with enhanced project outcomes, including better adherence to timelines, improved project quality, and increased likelihood of meeting project objectives. However, the deployment of chatbots in project management is not without its challenges. One significant hurdle is the need for advanced natural language processing (NLP) capabilities. Effective chatbots must understand and process complex human language nuances to interact effectively with stakeholders. Another challenge observed is the potential for chatbots to become disruptive or annoying. This can occur when chatbots fail to provide relevant or accurate information, or when their interaction style does not align with stakeholder expectations. In conclusion, AI-powered chatbots hold substantial promise for revolutionizing stakeholder engagement in project management. While they present remarkable benefits in improving communication, stakeholder satisfaction, and project outcomes, there are challenges that need to be addressed. These include enhancing NLP capabilities and fine-tuning the interaction style of chatbots to suit diverse stakeholder groups. With these improvements, AI-powered chatbots could significantly contribute to the success of various projects, marking a new era in project management where AI plays a pivotal role in stakeholder engagement and overall project success.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is identified that chatbots enhance communication by offering stakeholders immediate, personalized responses, thereby reducing response times and improving the efficiency of information exchange, and these improvements in stakeholder engagement directly correlate with enhanced project outcomes."
            },
            "score": 2
        },
        {
            "id": "1a8d53272a64d44fe1204224245eb0103a04778f",
            "paperId": "1a8d53272a64d44fe1204224245eb0103a04778f",
            "title": "Collaborative construction of a good quality, broad coverage and copyright free Japanese-French dictionary",
            "abstract": "This research project is located in the field of natural language processing (NLP), at the intersection of computer science and linguistics, specifically multilingual lexicography and lexicology. \nConcerning the Web, although French and Japanese are two well resourced languages (Berment, 2004), is not the case of the French-Japanese couple: \n- Electronic French-Japanese bilingual dictionaries (denshi jisho) can not be copied to a computer or reused; \n- There is a French-Japanese dictionary on the Web1, but it only contains 40 000 entries, no examples and is not available for download. \nThere are collaborative Web dictionaries such as the Japanese-English JMdict project led by Jim Breen (2004) that contains over 173,000 items. These resources are freely downloadable. It is therefore possible to carry out such projects. \nDuring a first stay in Japan from November 2001 to March 2004, we had already noticed the lack of French-Japanese bilingual resources on the Web. Which gave rise to the Papillon project about the construction of a multilingual lexical database with a pivot structure (Serasset et al., 2001). Since then, progress has been made in several areas (technical, theoretical, social) (Mangeot, 2006) but the actual production of data has made very little progress. On the other hand, there is a new trend in reusing existing lexical resources (word sense disambiguation, using open source resources (Wiktionary, dbpedia) merging with ontologies, etc.). Although they allow to consolidate and expand the coverage of existing resources, these experiences still use data created by hand by professional lexicographers. There are printed French-Japanese dictionaries of good quality and sufficiently old to be royalty free. It should be possible to reuse these resources as part of our project to build a good quality dictionary and broad coverage available on the Web. \nBased on this observation, we defined the following project to build a rich multilingual lexical system with priority over French-Japanese languages. The construction will be done first by reusing existing resources (printed Japanese-French dictionaries, Japanese-other language dictionaries, \n\ufffc1http://www.dictionnaire-japonais.com \n\ufffc\ufffc\ufffc \nWikipedia) and automatic operations (scanning and corrections, calculating translation links) and then by volunteer contributors working as a community on the Web. They will have to contribute to dictionary articles according to their level of expertise and knowledge in the field of lexicography or bilingual translation. \nThe resulting resources will be royalty-free and intended for use by both humans via conventional bilingual dictionaries and by machines for automatic language processing tools (analysis, machine translation, etc.).",
            "year": 2016,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This research project is located in the field of natural language processing (NLP), at the intersection of computer science and linguistics, specifically mult bilingual lexicography and lexicology, to build a rich multilingual lexical system with priority over French-Japanese languages."
            },
            "score": 2
        },
        {
            "id": "c4affe78f824f8dc604201ad40cd00c381297d7c",
            "paperId": "c4affe78f824f8dc604201ad40cd00c381297d7c",
            "title": "Exploring the Upper Limits of Text-Based Collaborative Filtering Using Large Language Models: Discoveries and Insights",
            "abstract": "Text-based collaborative filtering (TCF) has become the mainstream approach for text and news recommendation, utilizing text encoders, also known as language models (LMs), to represent items. However, existing TCF models primarily focus on using small or medium-sized LMs. It remains uncertain what impact replacing the item encoder with one of the largest and most powerful LMs, such as the 175-billion parameter GPT-3 model, would have on recommendation performance. Can we expect unprecedented results? To this end, we conduct an extensive series of experiments aimed at exploring the performance limits of the TCF paradigm. Specifically, we increase the size of item encoders from one hundred million to one hundred billion to reveal the scaling limits of the TCF paradigm. We then examine whether these extremely large LMs could enable a universal item representation for the recommendation task. Furthermore, we compare the performance of the TCF paradigm utilizing the most powerful LMs to the currently dominant ID embedding-based paradigm and investigate the transferability of this TCF paradigm. Finally, we compare TCF with the recently popularized prompt-based recommendation using ChatGPT. Our research findings have not only yielded positive results but also uncovered some surprising and previously unknown negative outcomes, which can inspire deeper reflection and innovative thinking regarding text-based recommender systems. Codes and datasets will be released for further research.",
            "year": 2023,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An extensive series of experiments increases the size of item encoders from one hundred million to one hundred billion to reveal the scaling limits of the TCF paradigm and examines whether these extremely large LMs could enable a universal item representation for the recommendation task."
            },
            "score": 1
        },
        {
            "id": "ccc772d88c231275f24c4fac9b28bbe0942e1107",
            "paperId": "ccc772d88c231275f24c4fac9b28bbe0942e1107",
            "title": "Query2doc: Query Expansion with Large Language Models",
            "abstract": "This paper introduces a simple yet effective query expansion approach, denoted as query2doc, to improve both sparse and dense retrieval systems. The proposed method first generates pseudo-documents by few-shot prompting large language models (LLMs), and then expands the query with generated pseudo-documents. LLMs are trained on web-scale text corpora and are adept at knowledge memorization. The pseudo-documents from LLMs often contain highly relevant information that can aid in query disambiguation and guide the retrievers. Experimental results demonstrate that query2doc boosts the performance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and TREC DL, without any model fine-tuning. Furthermore, our method also benefits state-of-the-art dense retrievers in terms of both in-domain and out-of-domain results.",
            "year": 2023,
            "citationCount": 30,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a simple yet effective query expansion approach, denoted as query2doc, to improve both sparse and dense retrieval systems, and benefits state-of-the-art dense retrievers in terms of both in-domain and out- of-domain results."
            },
            "score": 1
        },
        {
            "id": "45a119463a79e1f18d1234a96174b617a086388b",
            "paperId": "45a119463a79e1f18d1234a96174b617a086388b",
            "title": "Accelerating Reinforcement Learning of Robotic Manipulations via Feedback from Large Language Models",
            "abstract": "Reinforcement Learning (RL) plays an important role in the robotic manipulation domain since it allows self-learning from trial-and-error interactions with the environment. Still, sample efficiency and reward specification seriously limit its potential. One possible solution involves learning from expert guidance. However, obtaining a human expert is impractical due to the high cost of supervising an RL agent, and developing an automatic supervisor is a challenging endeavor. Large Language Models (LLMs) demonstrate remarkable abilities to provide human-like feedback on user inputs in natural language. Nevertheless, they are not designed to directly control low-level robotic motions, as their pretraining is based on vast internet data rather than specific robotics data. In this paper, we introduce the Lafite-RL (Language agent feedback interactive Reinforcement Learning) framework, which enables RL agents to learn robotic tasks efficiently by taking advantage of LLMs' timely feedback. Our experiments conducted on RLBench tasks illustrate that, with simple prompt design in natural language, the Lafite-RL agent exhibits improved learning capabilities when guided by an LLM. It outperforms the baseline in terms of both learning efficiency and success rate, underscoring the efficacy of the rewards provided by an LLM.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Lafite-RL (Language agent feedback interactive Reinforcement Learning) framework is introduced, which enables RL agents to learn robotic tasks efficiently by taking advantage of LLMs' timely feedback."
            },
            "score": 1
        },
        {
            "id": "4829b73a47be18f73e9e8d90f3c23c8f84d0fccb",
            "paperId": "4829b73a47be18f73e9e8d90f3c23c8f84d0fccb",
            "title": "Representation Learning with Large Language Models for Recommendation",
            "abstract": "Recommender systems have seen significant advancements with the influence of deep learning and graph neural networks, particularly in capturing complex user-item relationships. However, these graph-based recommenders heavily depend on ID-based data, potentially disregarding valuable textual information associated with users and items, resulting in less informative learned representations. Moreover, the utilization of implicit feedback data introduces potential noise and bias, posing challenges for the effectiveness of user preference learning. While the integration of large language models (LLMs) into traditional ID-based recommenders has gained attention, challenges such as scalability issues, limitations in text-only reliance, and prompt input constraints need to be addressed for effective implementation in practical recommender systems. To address these challenges, we propose a model-agnostic framework RLMRec that aims to enhance existing recommenders with LLM-empowered representation learning. It proposes a recommendation paradigm that integrates representation learning with LLMs to capture intricate semantic aspects of user behaviors and preferences. RLMRec incorporates auxiliary textual signals, develops a user/item profiling paradigm empowered by LLMs, and aligns the semantic space of LLMs with the representation space of collaborative relational signals through a cross-view alignment framework. This work further establish a theoretical foundation demonstrating that incorporating textual signals through mutual information maximization enhances the quality of representations. In our evaluation, we integrate RLMRec with state-of-the-art recommender models, while also analyzing its efficiency and robustness to noise data. Our implementation codes are available at https://github.com/HKUDS/RLMRec.",
            "year": 2023,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A model-agnostic framework RLMRec is proposed that aims to enhance existing recommenders with LLM-empowered representation learning, and establishes a theoretical foundation demonstrating that incorporating textual signals through mutual information maximization enhances the quality of representations."
            },
            "score": 1
        },
        {
            "id": "a7beaf4ad0c59ad6c91a03af6eceaafd2d44cef9",
            "paperId": "a7beaf4ad0c59ad6c91a03af6eceaafd2d44cef9",
            "title": "ClusterLLM: Large Language Models as a Guide for Text Clustering",
            "abstract": "We introduce ClusterLLM, a novel text clustering framework that leverages feedback from an instruction-tuned large language model, such as ChatGPT. Compared with traditional unsupervised methods that builds upon\"small\"embedders, ClusterLLM exhibits two intriguing advantages: (1) it enjoys the emergent capability of LLM even if its embeddings are inaccessible; and (2) it understands the user's preference on clustering through textual instruction and/or a few annotated data. First, we prompt ChatGPT for insights on clustering perspective by constructing hard triplet questions, where A, B and C are similar data points that belong to different clusters according to small embedder. We empirically show that this strategy is both effective for fine-tuning small embedder and cost-efficient to query ChatGPT. Second, we prompt ChatGPT for helps on clustering granularity by carefully designed pairwise questions, and tune the granularity from cluster hierarchies that is the most consistent with the ChatGPT answers. Extensive experiments on 14 datasets show that ClusterLLM consistently improves clustering quality, at an average cost of ~$0.6 per dataset.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ClusterLLM, a novel text clustering framework that leverages feedback from an instruction-tuned large language model, such as ChatGPT, consistently improves clustering quality, at an average cost of ~$0.6 per dataset."
            },
            "score": 1
        },
        {
            "id": "793d69ecc8fe11e0a41aad5e34e5764227552e24",
            "paperId": "793d69ecc8fe11e0a41aad5e34e5764227552e24",
            "title": "Summary Cycles: Exploring the Impact of Prompt Engineering on Large Language Models\u2019 Interaction with Interaction Log Information",
            "abstract": "With the aim of improving work efficiency, we examine how Large Language Models (LLMs) can better support the handoff of information by summarizing user interactions in collaborative intelligence analysis communication. We experiment with interaction logs, or a record of user interactions with a system. Inspired by chain-of-thought prompting, we describe a technique to avoid API token limits with recursive summarization requests. We then apply ChatGPT over multiple iterations to extract named entities, topics, and summaries, combined with interaction sequence sentences, to generate summaries of critical events and results of analysis sessions. We quantitatively evaluate the generated summaries against human-generated ones using common accuracy metrics (e.g., ROUGE-L, BLEU, BLEURT, and TER). We also report qualitative trends and the factuality of the output. We find that manipulating the audience feature or providing single-shot examples minimally influences the model\u2019s accuracy. While our methodology successfully summarizes interaction logs, the lack of significant results raises questions about prompt engineering and summarization effectiveness generally. We call on explainable artificial intelligence research to better understand how terms and their placement may change LLM outputs, striving for more consistent prompt engineering guidelines.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work applies ChatGPT over multiple iterations to extract named entities, topics, and summaries, combined with interaction sequence sentences, to generate summaries of critical events and results of analysis sessions, finding that manipulating the audience feature or providing single-shot examples minimally influences the model\u2019s accuracy."
            },
            "score": 0
        }
    ],
    "novelty": "no"
}