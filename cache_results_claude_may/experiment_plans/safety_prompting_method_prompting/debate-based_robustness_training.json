{
    "topic_description": "novel prompting methods to improve large language models' robustness against adversarial attacks and improve their security and privacy",
    "idea_name": "Debate-Based Robustness Training",
    "raw_idea": {
        "Problem": "Adversarial prompts can often exploit biases or inconsistencies in an LLM's knowledge and beliefs. Improving an LLM's robustness requires identifying and resolving such biases and inconsistencies.",
        "Existing Methods": "Current methods for robustness training of LLMs include adversarial training (training on adversarially perturbed prompts) and consistency training (encouraging consistent outputs for semantically similar prompts). However, these methods often require large labeled datasets.",
        "Motivation": "Debate and argumentation are powerful tools for exposing and reconciling conflicting beliefs and information. We can prompt LLMs to engage in self-debate to surface biases and inconsistencies in their own responses, and then use this self-feedback to improve robustness.",
        "Proposed Method": "We propose a debate-based approach for LLM robustness training. Given an input prompt, we first ask the LLM to generate a response. We then prompt the LLM to engage in a self-debate by generating an opposing view to its own response, poking holes and finding weaknesses. After several rounds of this self-debate, we prompt the LLM to reconcile the different views and generate a final response that is more robust and consistent. Throughout this process, we also prompt the LLM to reflect on any biases or inconsistencies exposed during the debate, and to update its beliefs and knowledge accordingly.",
        "Experiment Plan": "Evaluate the debate-based training approach on various robustness benchmarks such as ANLI and Adversarial-NLI. Compare with baseline methods like adversarial training and consistency training, in terms of both robustness to adversarial attacks and sample efficiency. Also measure the biases and inconsistencies in the LLM's responses before and after debate-based training."
    },
    "full_experiment_plan": {
        "Title": "Debate-Based Self-Refinement: Improving Language Models' Robustness via Self-Debate and Reconciliation",
        "Problem Statement": "Large Language Models (LLMs) are susceptible to adversarial attacks that exploit biases or inconsistencies in their knowledge and beliefs. Improving LLMs' robustness requires identifying and resolving such biases and inconsistencies.",
        "Motivation": "Existing methods for robustness training of LLMs, such as adversarial training and consistency training, often require large labeled datasets. Debate and argumentation are powerful tools for exposing and reconciling conflicting beliefs and information. We propose leveraging the LLM's own knowledge to engage in self-debate to surface biases and inconsistencies in its responses, and then use this self-feedback to improve robustness without relying on external labeled data.",
        "Proposed Method": "We propose a debate-based self-refinement approach for improving LLM robustness. Given an input prompt, we first ask the LLM to generate an initial response. We then prompt the LLM to engage in a self-debate by generating an opposing view to its own response, poking holes and finding weaknesses. After several rounds of this self-debate, we prompt the LLM to reconcile the different views and generate a final response that is more robust and consistent. Throughout this process, we also prompt the LLM to reflect on any biases or inconsistencies exposed during the debate, and to update its beliefs and knowledge accordingly.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "Evaluate the proposed approach on various robustness benchmarks such as ANLI (Adversarial NLI), Adversarial-QA, Adversarial-SWAG, and Adversarial-VQA. These datasets contain examples that are specifically designed to be challenging for language models.",
            "Step 2: Construct Prompts": "For the baseline, use direct prompting where the input prompt is directly fed into the model to generate a response. For the proposed debate-based self-refinement approach:\n1. Initial Response Generation: Prompt the model with the input to generate an initial response.\n2. Self-Debate: Prompt the model to generate an opposing view to its own initial response. Repeat this for multiple rounds, each time asking the model to find weaknesses and poke holes in its previous response. Example prompt: \"Please generate a response that points out potential flaws, biases, or inconsistencies in the previous response:\"\n3. Reconciliation: After the debate rounds, prompt the model to reconcile the different viewpoints and generate a final consistent response. Example prompt: \"Based on the different arguments presented in the debate, please generate a final response that is consistent and addresses the potential flaws and biases:\"\n4. Reflection: Prompt the model to reflect on the biases and inconsistencies exposed during the debate, and to update its knowledge and beliefs. Example prompt: \"Please reflect on the debate and list any biases or inconsistencies in your initial beliefs that were exposed. How would you update your knowledge based on this?\"",
            "Step 3: Select Models": "Evaluate the proposed approach on state-of-the-art LLMs such as GPT-3.5, GPT-4, PaLM, and LLaMA.",
            "Step 4: Evaluate Results": "Evaluate the model's performance on the robustness benchmarks before and after debate-based self-refinement. Measure metrics such as accuracy and consistency. Also qualitatively analyze the model's responses, reflections, and knowledge updates to assess how well it is able to identify and resolve its own biases and inconsistencies.",
            "Step 5: Ablation Studies": "Perform ablation studies to understand the impact of different components of the proposed approach. For example, vary the number of debate rounds, remove the reflection step, or change the prompts. This will provide insights into which aspects are most important for improving robustness."
        },
        "Test Case Examples": {
            "Baseline Prompt Input": "Premise: The City Council voted to ban the sale of flavored tobacco products, including menthol cigarettes. Argument: Banning flavored tobacco products will not reduce smoking rates and will only lead to the creation of a black market for these products. The government should not interfere with an individual's freedom to choose what they consume.",
            "Baseline Prompt Expected Output": "I agree with the argument. Banning flavored tobacco products is an infringement on personal freedom. While the intentions may be good, prohibition often backfires and leads to unintended consequences like black markets. Educating people on the risks and allowing them to make informed choices would be a better approach than an outright ban.",
            "Proposed Prompt Input (Initial Response Generation)": "Premise: The City Council voted to ban the sale of flavored tobacco products, including menthol cigarettes. Argument: Banning flavored tobacco products will not reduce smoking rates and will only lead to the creation of a black market for these products. The government should not interfere with an individual's freedom to choose what they consume.",
            "Proposed Prompt Expected Output (Initial Response Generation)": "I agree with the argument. Banning flavored tobacco products is an infringement on personal freedom. While the intentions may be good, prohibition often backfires and leads to unintended consequences like black markets. Educating people on the risks and allowing them to make informed choices would be a better approach than an outright ban.",
            "Proposed Prompt Input (Self-Debate)": "Please generate a response that points out potential flaws, biases, or inconsistencies in the previous response:",
            "Proposed Prompt Expected Output (Self-Debate)": "The previous response fails to consider the public health implications of flavored tobacco products. These products are often marketed in a way that appeals to youth and can lead to increased smoking initiation among young people. Menthol cigarettes in particular have been shown to increase addiction and make it harder to quit smoking. The government has a responsibility to protect public health, even if it means restricting certain individual freedoms.",
            "Proposed Prompt Input (Reconciliation)": "Based on the different arguments presented in the debate, please generate a final response that is consistent and addresses the potential flaws and biases:",
            "Proposed Prompt Expected Output (Reconciliation)": "After considering the different viewpoints, I believe the ban on flavored tobacco products is justified. While it does restrict individual choice to some degree, the government's role is to balance personal freedoms with public well-being. Given the clear evidence that flavored tobacco, especially menthol cigarettes, increases smoking rates and harms public health, particularly among youth and marginalized communities, a ban is an appropriate policy response. However, the ban should be coupled with increased smoking cessation resources and public education to help current smokers quit and prevent a black market from emerging.",
            "Proposed Prompt Input (Reflection)": "Please reflect on the debate and list any biases or inconsistencies in your initial beliefs that were exposed. How would you update your knowledge based on this?",
            "Proposed Prompt Expected Output (Reflection)": "My initial response was biased towards prioritizing individual freedom over public health considerations. I failed to properly weigh the disproportionate harm that flavored tobacco products cause to vulnerable populations. I also did not consider the manipulative nature of tobacco product marketing. Based on the arguments presented, I would update my knowledge to place greater emphasis on the government's responsibility to protect public health through evidence-based policies, even if they entail some restrictions on individual choice. I would also seek out more information on the specific impacts of flavored tobacco products to inform my views."
        },
        "Fallback Plan": "If the proposed debate-based self-refinement approach does not lead to significant improvements in robustness, there are several fallback options:\n1. Analyze the debate transcripts to identify where the model struggles to generate strong opposing views or to reconcile conflicting information. This could point to areas where the model's knowledge is limited or inconsistent.\n2. Experiment with different debate formats, such as having the model take on specific personas or roles in the debate. This could help surface a wider range of viewpoints.\n3. Incorporate external knowledge sources into the debate and reconciliation process. For example, prompt the model to cite relevant facts and statistics to support its arguments.\n4. Investigate the model's confidence scores and uncertainty estimates during the debate process. High uncertainty could indicate areas where the model's knowledge is weak or conflicting.\n5. If the debate-based approach proves ineffective, pivot to an analysis of the model's failure modes on the robustness benchmarks. Characterize the types of biases and inconsistencies that the model is susceptible to, and use these insights to inform future research directions on robustness interventions."
    },
    "novelty_queries": [
        "KeywordQuery(\"language models debate self refinement\")",
        "KeywordQuery(\"language models self debate robustness\")",
        "KeywordQuery(\"language models debate based consistency\")",
        "KeywordQuery(\"Debate-Based Robustness Training NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
            "paperId": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
            "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such\"society of minds\"approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.",
            "year": 2023,
            "citationCount": 206,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer is presented, indicating that this approach significantly enhances mathematical and strategic reasoning across a number of tasks."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is improving LLMs' robustness by identifying and resolving biases and inconsistencies in their knowledge and beliefs. The proposed approach is to use self-debate and reconciliation to expose and resolve these issues.\n\nThe research problem in the paper is improving factuality and reasoning in LLMs. The proposed approach is to use multiagent debate where multiple LLM instances propose and debate their responses to arrive at a final answer.\n\nWhile both the proposal and paper aim to improve LLMs, the specific research problems (robustness vs factuality/reasoning) and approaches (self-debate vs multiagent debate) are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "385c74957858e7d6856d48e72b5a902b4c1aa28c",
            "paperId": "385c74957858e7d6856d48e72b5a902b4c1aa28c",
            "title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
            "abstract": "Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of\"tit for tat\"and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counter-intuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of\"tit for tat\"state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents. Codes: https://github.com/Skytliang/Multi-Agents-Debate",
            "year": 2023,
            "citationCount": 125,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Multi-Agent Debate (MAD) framework is proposed, in which multiple agents express their arguments in the state of\"tit for tat\"and a judge manages the debate process to obtain a final solution."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is improving the robustness of large language models (LLMs) by identifying and resolving biases and inconsistencies in their knowledge and beliefs. The proposed approach is a debate-based self-refinement method where the LLM engages in self-debate to surface biases and inconsistencies, and then uses this self-feedback to improve robustness without relying on external labeled data.\n\nThe research problem in the paper is that LLMs struggle on complex reasoning tasks and suffer from the Degeneration-of-Thought (DoT) problem, where they are unable to generate novel thoughts through self-reflection even if their initial stance is incorrect. The proposed approach is a Multi-Agent Debate (MAD) framework, where multiple agents express their arguments in a \"tit for tat\" state and a judge manages the debate process to obtain a final solution.\n\nWhile both the proposal and the paper aim to improve the reasoning capabilities of LLMs, the specific research problems and approaches are different. The proposal focuses on robustness and bias, while the paper focuses on complex reasoning and the DoT problem. The proposal uses self-debate and self-feedback, while the paper uses multi-agent debate with a judge.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "eb291a2e237774b162d9c51c21c4868795589e94",
            "paperId": "eb291a2e237774b162d9c51c21c4868795589e94",
            "title": "Diving into the Inter-Consistency of Large Language Models: An Insightful Analysis through Debate",
            "abstract": "Large language models (LLMs) have demonstrated impressive zero-shot or few-shot commonsense reasoning performance on various natural language processing (NLP) tasks. However, despite their strong commonsense reasoning abilities, LLMs still exhibit various kinds of inconsistency problems. While previous researches mainly focused on the self-consistency within a single LLM, we propose to explore the inter-consistency issue between two or more LLMs, which is critical for diverse and precise decision-making processes. Since the LLMs possess human-like intelligence after instruction tuning and reinforcement learning with human feedback (RLHF), we design a formal debate framework to delve into the inter-consistency problem among LLMs with three-stage debate: fair debate, mismatched debate, and roundtable debate. Through extensive experiments on 7 common-sense reasoning datasets, LLMs not only become more inter-consistent by compromising and refuting but also achieve higher performance and stronger interpretability. Furthermore, we \ufb01nd a much stronger LLM would be dominant in mismatched debates, while it will be easily misled by relatively weaker LLMs in a more complex debate scenario such as roundtable debate.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A formal debate framework is designed to delve into the inter-consistency problem among LLMs with three-stage debate: fair debate, mismatched debate, and roundtable debate to explore the inter-consistency issue between two or more LLMs, which is critical for diverse and precise decision-making processes."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the robustness of large language models (LLMs) by using self-debate and reconciliation to identify and resolve biases and inconsistencies in the model's knowledge and beliefs. The proposed approach involves prompting the LLM to engage in a debate with itself, generate opposing views, and then reconcile the different viewpoints to produce a more robust and consistent response.\n\nThe paper, on the other hand, focuses on exploring the inter-consistency issue between two or more LLMs through a formal debate framework. The authors propose a three-stage debate (fair debate, mismatched debate, and roundtable debate) to investigate how LLMs can become more inter-consistent by compromising and refuting each other's arguments.\n\nWhile both the project proposal and the paper involve the concept of debate, they differ in their primary objectives. The project proposal aims to improve the robustness of a single LLM by using self-debate, whereas the paper focuses on the inter-consistency between multiple LLMs through a formal debate framework.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "fbd4a876cee20eaf98f344aca597a55338f663f5",
            "paperId": "fbd4a876cee20eaf98f344aca597a55338f663f5",
            "title": "Examining the Inter-Consistency of Large Language Models: An In-depth Analysis via Debate",
            "abstract": "Large Language Models (LLMs) have demonstrated human-like intelligence and are widely used in various applications. However, LLMs still exhibit various kinds of inconsistency problems. Existing works mainly focus on the inconsistency issues within a single LLM, while we investigate the inter-consistency among multiple LLMs, which is critical for collaborating to solve a complex task. To examine whether LLMs can collaborate to ultimately achieve a consensus for the shared goal and whether LLMs easily change their viewpoints, we introduce a Formal Debate framework (FORD) With FORD, we conduct a three-stage debate aligned with real-world scenarios: fair debate, mismatched debate, and roundtable debate. Through extensive experiments on the commonsense reasoning task, LLMs not only become more inter-consistent but also achieve higher performance. More-over, we observe that stronger LLMs tend to dominate the debates by adhering to their perspectives, while weaker ones are more likely to change viewpoints. Additionally, we highlight the importance of a competent judge, such as GPT-4, to draw more proper conclusions. Our work contributes to understanding the inter-consistency among LLMs and lays the foundation for the development of future collaboration methods.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates the inter-consistency among multiple Large Language Models, and contributes to understanding the inter-consistency among LLMs and lays the foundation for the development of future collaboration methods."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving LLMs' robustness by identifying and resolving biases and inconsistencies, while the approach is using self-debate and reconciliation without relying on external labeled data.\n\nThe research problem in the paper is investigating the inter-consistency among multiple LLMs, while the approach is using a formal debate framework to examine whether LLMs can collaborate to achieve consensus and how easily they change viewpoints.\n\nAlthough both works involve using debate as a tool, the proposal focuses on improving a single LLM's robustness via self-debate, while the paper studies the interaction and consistency among multiple LLMs through collaborative debate. The research problems and goals are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "f8cbcb106a48524edc39df23e2a95f1e6d4c739a",
            "paperId": "f8cbcb106a48524edc39df23e2a95f1e6d4c739a",
            "title": "Examining Inter-Consistency of Large Language Models Collaboration: An In-depth Analysis via Debate",
            "abstract": "Large Language Models (LLMs) have shown impressive capabilities in various applications, but they still face various inconsistency issues. Existing works primarily focus on the inconsistency issues within a single LLM, while we complementarily explore the inter-consistency among multiple LLMs for collaboration. To examine whether LLMs can collaborate effectively to achieve a consensus for a shared goal, we focus on commonsense reasoning, and introduce a formal debate framework (FORD) to conduct a three-stage debate among LLMs with real-world scenarios alignment: fair debate, mismatched debate, and roundtable debate. Through extensive experiments on various datasets, LLMs can effectively collaborate to reach a consensus despite noticeable inter-inconsistencies, but imbalances in their abilities can lead to domination by superior LLMs. Leveraging a more advanced LLM like GPT-4 as an authoritative judge can boost collaboration performance. Our work contributes to understanding the inter-consistency among LLMs and lays the foundation for developing future collaboration methods. Codes and data are available at https://github.com/Waste-Wood/FORD",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a formal debate framework (FORD) to conduct a three-stage debate among LLMs with real-world scenarios alignment: fair debate, mismatched debate, and roundtable debate to examine whether LLMs can collaborate effectively to achieve a consensus for a shared goal."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the robustness of large language models by identifying and resolving biases and inconsistencies in their knowledge and beliefs. The proposed approach is to use self-debate and reconciliation to expose and resolve these issues without relying on external labeled data.\n\nThe research problem in the paper is examining the inter-consistency among multiple large language models when collaborating to achieve a consensus. The approach is to use a formal debate framework to conduct debates among the models in various settings.\n\nWhile both works involve using debate as a tool to study language models, the research problems and approaches are different. The proposal focuses on improving a single model's robustness, while the paper studies the collaboration and consistency among multiple models.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "696bc5ba0d023822bbee6b878a71ea2e4a4b0e5a",
            "paperId": "696bc5ba0d023822bbee6b878a71ea2e4a4b0e5a",
            "title": "N-Critics: Self-Refinement of Large Language Models with Ensemble of Critics",
            "abstract": "We propose a self-correction mechanism for Large Language Models (LLMs) to mitigate issues such as toxicity and fact hallucination. This method involves refining model outputs through an ensemble of critics and the model's own feedback. Drawing inspiration from human behavior, we explore whether LLMs can emulate the self-correction process observed in humans who often engage in self-reflection and seek input from others to refine their understanding of complex topics. Our approach is model-agnostic and can be applied across various domains to enhance trustworthiness by addressing fairness, bias, and robustness concerns. We consistently observe performance improvements in LLMs for reducing toxicity and correcting factual errors.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a self-correction mechanism for Large Language Models (LLMs) to mitigate issues such as toxicity and fact hallucination and consistently observe performance improvements in LLMs for reducing toxicity and correcting factual errors."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve the robustness of large language models by using self-debate and reconciliation to identify and resolve biases and inconsistencies in the model's responses. The paper proposes a self-correction mechanism for large language models to mitigate issues such as toxicity and fact hallucination by refining model outputs through an ensemble of critics and the model's own feedback.\n\nWhile both the project proposal and the paper focus on improving the performance of large language models, they target different aspects. The project proposal specifically addresses robustness against adversarial attacks, while the paper tackles toxicity and fact hallucination. Additionally, the approaches differ: the project proposal uses self-debate and reconciliation, while the paper employs an ensemble of critics and the model's own feedback.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "978aaaeccb4e67041d7c54fc1c3f4520824d8c1a",
            "paperId": "978aaaeccb4e67041d7c54fc1c3f4520824d8c1a",
            "title": "Self-Refinement of Language Models from External Proxy Metrics Feedback",
            "abstract": "It is often desirable for Large Language Models (LLMs) to capture multiple objectives when providing a response. In document-grounded response generation, for example, agent responses are expected to be relevant to a user's query while also being grounded in a given document. In this paper, we introduce Proxy Metric-based Self-Refinement (ProMiSe), which enables an LLM to refine its own initial response along key dimensions of quality guided by external metrics feedback, yielding an overall better final response. ProMiSe leverages feedback on response quality through principle-specific proxy metrics, and iteratively refines its response one principle at a time. We apply ProMiSe to open source language models Flan-T5-XXL and Llama-2-13B-Chat, to evaluate its performance on document-grounded question answering datasets, MultiDoc2Dial and QuAC, demonstrating that self-refinement improves response quality. We further show that fine-tuning Llama-2-13B-Chat on the synthetic dialogue data generated by ProMiSe yields significant performance improvements over the zero-shot baseline as well as a supervised fine-tuned model on human annotated data.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Proxy Metric-based Self-Refinement (ProMiSe) is introduced, which enables an LLM to refine its own initial response along key dimensions of quality guided by external metrics feedback, yielding an overall better final response."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve language models' robustness by using self-debate and reconciliation to identify and resolve biases and inconsistencies in the model's knowledge and beliefs.\n\nThe paper proposes a method called Proxy Metric-based Self-Refinement (ProMiSe) that enables a language model to refine its own initial response along key dimensions of quality guided by external metrics feedback, to improve the overall response quality in document-grounded question answering tasks.\n\nWhile both involve some form of self-refinement, the project proposal focuses on improving robustness and consistency by self-debate, while the paper focuses on improving response quality in a specific task (document-grounded QA) using external metric feedback. The high-level research problems and approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "3aaf6a2cbad5850ad81ab5c163599cb3d523436f",
            "paperId": "3aaf6a2cbad5850ad81ab5c163599cb3d523436f",
            "title": "Self-Refine: Iterative Refinement with Self-Feedback",
            "abstract": "Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.",
            "year": 2023,
            "citationCount": 505,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Self-Refine is introduced, an approach for improving initial outputs from LLMs through iterative feedback and refinement that demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using this simple, standalone approach."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve the robustness of language models against adversarial attacks by using self-debate and reconciliation to identify and resolve biases and inconsistencies in the model's knowledge and beliefs. The paper proposes Self-Refine, an approach for improving the initial outputs from language models through iterative self-feedback and refinement without requiring additional training data or reinforcement learning.\n\nWhile both the project proposal and the paper focus on improving language model outputs, the project proposal specifically targets robustness against adversarial attacks, whereas the paper aims to enhance the quality of the initial outputs across various tasks. The project proposal uses self-debate and reconciliation to identify biases and inconsistencies, while the paper employs iterative self-feedback and refinement.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "20eecb9ead20ffe49a66588a9662336eefb20a54",
            "paperId": "20eecb9ead20ffe49a66588a9662336eefb20a54",
            "title": "MAF: Multi-Aspect Feedback for Improving Reasoning in Large Language Models",
            "abstract": "Language Models (LMs) have shown impressive performance in various natural language tasks. However, when it comes to natural language reasoning, LMs still face challenges such as hallucination, generating incorrect intermediate reasoning steps, and making mathematical errors. Recent research has focused on enhancing LMs through self-improvement using feedback. Nevertheless, existing approaches relying on a single generic feedback source fail to address the diverse error types found in LM-generated reasoning chains. In this work, we propose Multi-Aspect Feedback, an iterative refinement framework that integrates multiple feedback modules, including frozen LMs and external tools, each focusing on a specific error category. Our experimental results demonstrate the efficacy of our approach to addressing several errors in the LM-generated reasoning chain and thus improving the overall performance of an LM in several reasoning tasks. We see a relative improvement of up to 20% in Mathematical Reasoning and up to 18% in Logical Entailment.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes Multi-Aspect Feedback, an iterative refinement framework that integrates multiple feedback modules, including frozen LMs and external tools, each focusing on a specific error category in the LM-generated reasoning chain to improve the overall performance of an LM in several reasoning tasks."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving LLMs' robustness by identifying and resolving biases and inconsistencies in their knowledge and beliefs. The proposed approach is a debate-based self-refinement method where the LLM engages in self-debate to surface biases and inconsistencies, and then uses this self-feedback to improve robustness without relying on external labeled data.\n\nThe research problem in the paper is improving LMs' reasoning performance by addressing diverse error types in LM-generated reasoning chains. The proposed approach is an iterative refinement framework that integrates multiple feedback modules, each focusing on a specific error category.\n\nWhile both the proposal and the paper aim to improve LLMs' performance, they focus on different aspects (robustness vs. reasoning) and propose different approaches (self-debate vs. multi-aspect feedback). The proposal targets biases and inconsistencies in LLMs' knowledge and beliefs, while the paper addresses specific error types in reasoning chains.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "6af460d34bfc8e955e43fbe15cedcf329b48bc19",
            "paperId": "6af460d34bfc8e955e43fbe15cedcf329b48bc19",
            "title": "SAC3: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency",
            "abstract": "Hallucination detection is a critical step toward understanding the trustworthiness of modern language models (LMs). To achieve this goal, we re-examine existing detection approaches based on the self-consistency of LMs and uncover two types of hallucinations resulting from 1) question-level and 2) model-level, which cannot be effectively identified through self-consistency check alone. Building upon this discovery, we propose a novel sampling-based method, i.e., semantic-aware cross-check consistency (SAC3) that expands on the principle of self-consistency checking. Our SAC3 approach incorporates additional mechanisms to detect both question-level and model-level hallucinations by leveraging advances including semantically equivalent question perturbation and cross-model response consistency checking. Through extensive and systematic empirical analysis, we demonstrate that SAC3 outperforms the state of the art in detecting both non-factual and factual statements across multiple question-answering and open-domain generation benchmarks.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel sampling-based method, i.e., semantic-aware cross-check consistency (SAC3) that expands on the principle of self-consistency checking and demonstrates that SAC3 outperforms the state of the art in detecting both non-factual and factual statements across multiple question-answering and open-domain generation benchmarks."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving language models' robustness to adversarial attacks by identifying and resolving biases and inconsistencies. The proposed approach is to use self-debate and reconciliation to expose and fix the model's own biases without relying on external labeled data.\n\nThe research problem in the paper is hallucination detection in language models. The proposed approach is a sampling-based method called SAC3 that uses semantically equivalent question perturbation and cross-model response consistency checking.\n\nThe proposal focuses on improving robustness, while the paper focuses on detecting hallucinations. The proposal uses self-debate, while the paper uses semantic perturbation and cross-model checking.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "2c72ab10e7a5f2fd32e6f85b20c77bf64e6e220d",
            "paperId": "2c72ab10e7a5f2fd32e6f85b20c77bf64e6e220d",
            "title": "A prompt-based approach to adversarial example generation and robustness enhancement",
            "abstract": null,
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel robust training approach based on prompt paradigm which incorporates prompt texts as the alternatives to adversarial examples and enhances robustness under a lightweight minimax-style optimization framework is proposed."
            },
            "score": 6
        },
        {
            "id": "de2fd685f45ee916b9142bcb983d306b7da643a4",
            "paperId": "de2fd685f45ee916b9142bcb983d306b7da643a4",
            "title": "A Prompting-based Approach for Adversarial Example Generation and Robustness Enhancement",
            "abstract": "Recent years have seen the wide application of NLP models in crucial areas such as finance, medical treatment, and news media, raising concerns of the model robustness and vulnerabilities. In this paper, we propose a novel prompt-based adversarial attack to compromise NLP models and robustness enhancement technique. We first construct malicious prompts for each instance and generate adversarial examples via mask-and-filling under the effect of a malicious purpose. Our attack technique targets the inherent vulnerabilities of NLP models, allowing us to generate samples even without interacting with the victim NLP model, as long as it is based on pre-trained language models (PLMs). Furthermore, we design a prompt-based adversarial training method to improve the robustness of PLMs. As our training method does not actually generate adversarial samples, it can be applied to large-scale training sets efficiently. The experimental results show that our attack method can achieve a high attack success rate with more diverse, fluent and natural adversarial examples. In addition, our robustness enhancement method can significantly improve the robustness of models to resist adversarial attacks. Our work indicates that prompting paradigm has great potential in probing some fundamental flaws of PLMs and fine-tuning them for downstream tasks.",
            "year": 2022,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel prompt-based adversarial attack to compromise NLP models and robustness enhancement technique that can significantly improve the robustness of models to resist adversarial attacks and indicates that prompting paradigm has great potential in probing some fundamental flaws of PLMs and fine-tuning them for downstream tasks."
            },
            "score": 6
        },
        {
            "id": "b6499bcc10d4a70c3ca8b84995270cfd0d29de4c",
            "paperId": "b6499bcc10d4a70c3ca8b84995270cfd0d29de4c",
            "title": "Model-tuning Via Prompts Makes NLP Models Adversarially Robust",
            "abstract": "In recent years, NLP practitioners have converged on the following practice: (i) import an off-the-shelf pretrained (masked) language model; (ii) append a multilayer perceptron atop the CLS token's hidden representation (with randomly initialized weights); and (iii) fine-tune the entire model on a downstream task (MLP-FT). This procedure has produced massive gains on standard NLP benchmarks, but these models remain brittle, even to mild adversarial perturbations. In this work, we demonstrate surprising gains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an alternative method of adapting to downstream tasks. Rather than appending an MLP head to make output prediction, MVP appends a prompt template to the input, and makes prediction via text infilling/completion. Across 5 NLP datasets, 4 adversarial attacks, and 3 different models, MVP improves performance against adversarial substitutions by an average of 8% over standard methods and even outperforms adversarial training-based state-of-art defenses by 3.5%. By combining MVP with adversarial training, we achieve further improvements in adversarial robustness while maintaining performance on unperturbed examples. Finally, we conduct ablations to investigate the mechanism underlying these gains. Notably, we find that the main causes of vulnerability of MLP-FT can be attributed to the misalignment between pre-training and fine-tuning tasks, and the randomly initialized MLP parameters.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work demonstrates surprising gains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an alternative method of adapting to downstream tasks that improves performance against adversarial substitutions and outperforms adversarial training-based state-of-art defenses by 3.5%."
            },
            "score": 6
        },
        {
            "id": "9a9b1e2968302eb882870537d4af6e2c722dfd1a",
            "paperId": "9a9b1e2968302eb882870537d4af6e2c722dfd1a",
            "title": "Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement",
            "abstract": "To enhance the multi-step reasoning capabilities of large language models, researchers have extensively explored prompting methods, notably the Chain-of-Thought (CoT) method which explicitly elicits human-like rationales. However, they have inadvertently overlooked the potential of enhancing model reasoning performance by formulating higher-quality problems. In this work, we start from the problem side and propose Self-Polish (SP), a novel method that facilitates the model's reasoning by guiding it to progressively refine the given problems to be more comprehensible and solvable. We also explore several automatic prompting varients and propose the Self-Polish prompt bank for the community. SP is orthogonal to all other prompting methods of answer/reasoning side like CoT, allowing for seamless integration with state-of-the-art techniques for further improvement. Thorough experiments show that the proposed method attains notable and consistent effectiveness on five reasoning benchmarks across different models. Furthermore, our method also showcases impressive performance on robustness evaluation. Codes and prompts are available at https://github.com/WooooDyy/Self-Polish.",
            "year": 2023,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Self-Polish (SP) is proposed, a novel method that facilitates the model's reasoning by guiding it to progressively refine the given problems to be more comprehensible and solvable and seamless integration with state-of-the-art techniques for further improvement."
            },
            "score": 5
        },
        {
            "id": "9e9e4df2996bac794c4f04cb887df3e553bae4fd",
            "paperId": "9e9e4df2996bac794c4f04cb887df3e553bae4fd",
            "title": "Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning",
            "abstract": "Large Language Models (LLMs) have shown human-like reasoning abilities but still struggle with complex logical problems. This paper introduces a novel framework, Logic-LM, which integrates LLMs with symbolic solvers to improve logical problem-solving. Our method first utilizes LLMs to translate a natural language problem into a symbolic formulation. Afterward, a deterministic symbolic solver performs inference on the formulated problem. We also introduce a self-refinement module, which utilizes the symbolic solver's error messages to revise symbolic formalizations. We demonstrate Logic-LM's effectiveness on five logical reasoning datasets: ProofWriter, PrOntoQA, FOLIO, LogicalDeduction, and AR-LSAT. On average, Logic-LM achieves a significant performance boost of 39.2% over using LLM alone with standard prompting and 18.4% over LLM with chain-of-thought prompting. Our findings suggest that Logic-LM, by combining LLMs with symbolic logic, offers a promising avenue for faithful logical reasoning. Code and data are publicly available at https://github.com/teacherpeterpan/Logic-LLM.",
            "year": 2023,
            "citationCount": 59,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a novel framework, Logic-LM, which integrates LLMs with symbolic solvers to improve logical problem-solving and suggests that it offers a promising avenue for faithful logical reasoning."
            },
            "score": 5
        },
        {
            "id": "822f7a276a4ff7dae59b849f57b95d2603a40d99",
            "paperId": "822f7a276a4ff7dae59b849f57b95d2603a40d99",
            "title": "Small Language Models Need Strong Verifiers to Self-Correct Reasoning",
            "abstract": "Self-correction has emerged as a promising solution to boost the reasoning performance of large language models (LLMs), where LLMs refine their solutions using self-generated critiques that pinpoint the errors. This work explores whether smaller-size (<= 13B) language models (LMs) have the ability of self-correction on reasoning tasks with minimal inputs from stronger LMs. We propose a novel pipeline that prompts smaller LMs to collect self-correction data that supports the training of self-refinement abilities. First, we leverage correct solutions to guide the model in critiquing their incorrect responses. Second, the generated critiques, after filtering, are used for supervised fine-tuning of the self-correcting reasoner through solution refinement. Our experimental results show improved self-correction abilities of two models on five datasets spanning math and commonsense reasoning, with notable performance gains when paired with a strong GPT-4-based verifier, though limitations are identified when using a weak self-verifier for determining when to correct.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores whether smaller-size (<= 13B) language models have the ability of self-correction on reasoning tasks with minimal inputs from stronger LMs, and proposes a novel pipeline that prompts smaller LMs to collect self-correction data that supports the training of self-refinement abilities."
            },
            "score": 5
        },
        {
            "id": "5c118e57b5398224ca4401c902cda33da7d29ec3",
            "paperId": "5c118e57b5398224ca4401c902cda33da7d29ec3",
            "title": "Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models",
            "abstract": "The alignments of reasoning abilities between smaller and larger Language Models are largely conducted via Supervised Fine-Tuning (SFT) using demonstrations generated from robust Large Language Models (LLMs). Although these approaches deliver more performant models, they do not show sufficiently strong generalization ability as the training only relies on the provided demonstrations. In this paper, we propose the Self-refine Instruction-tuning method that elicits Smaller Language Models to self-refine their abilities. Our approach is based on a two-stage process, where reasoning abilities are first transferred between LLMs and Small Language Models (SLMs) via Instruction-tuning on demonstrations provided by LLMs, and then the instructed models Self-refine their abilities through preference optimization strategies. In particular, the second phase operates refinement heuristics based on the Direct Preference Optimization algorithm, where the SLMs are elicited to deliver a series of reasoning paths by automatically sampling the generated responses and providing rewards using ground truths from the LLMs. Results obtained on commonsense and math reasoning tasks show that this approach significantly outperforms Instruction-tuning in both in-domain and out-domain scenarios, aligning the reasoning abilities of Smaller and Larger Language Models.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes the Self-refine Instruction-tuning method that elicits Smaller Language Models to self-refine their abilities, and shows that this approach significantly outperforms Instruction-tuning in both in-domain and out-domain scenarios, aligning the reasoning abilities of Smaller and Larger Language Models."
            },
            "score": 5
        },
        {
            "id": "bfc7762ab90d18bdb687d93723c51e9827be254a",
            "paperId": "bfc7762ab90d18bdb687d93723c51e9827be254a",
            "title": "Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models",
            "abstract": "The recent success of Large Language Models (LLMs) has catalyzed an increasing interest in their self-correction capabilities. This paper presents a comprehensive investigation into the intrinsic self-correction of LLMs, attempting to address the ongoing debate about its feasibility. Our research has identified an important latent factor - the\"confidence\"of LLMs - during the self-correction process. Overlooking this factor may cause the models to over-criticize themselves, resulting in unreliable conclusions regarding the efficacy of self-correction. We have experimentally observed that LLMs possess the capability to understand the\"confidence\"in their own responses. It motivates us to develop an\"If-or-Else\"(IoE) prompting framework, designed to guide LLMs in assessing their own\"confidence\", facilitating intrinsic self-corrections. We conduct extensive experiments and demonstrate that our IoE-based Prompt can achieve a consistent improvement regarding the accuracy of self-corrected responses over the initial answers. Our study not only sheds light on the underlying factors affecting self-correction in LLMs, but also introduces a practical framework that utilizes the IoE prompting principle to efficiently improve self-correction capabilities with\"confidence\". The code is available at https://github.com/MBZUAI-CLeaR/IoE-Prompting.git.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents a comprehensive investigation into the intrinsic self-correction of LLMs, attempting to address the ongoing debate about its feasibility and introduces a practical framework that utilizes the IoE prompting principle to efficiently improve self-correction capabilities with\"confidence\"."
            },
            "score": 5
        },
        {
            "id": "0d22f06a1f5ad9f62b2f35c126b514f927586c85",
            "paperId": "0d22f06a1f5ad9f62b2f35c126b514f927586c85",
            "title": "Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency",
            "abstract": "Large language models (LLMs) have exhibited remarkable ability in code generation. However, generating the correct solution in a single attempt still remains a challenge. Prior works utilize verification properties in software engineering to verify and re-rank solutions in a majority voting manner. But the assumption behind them that generated verification properties have better qualities than solutions may not always hold. In this paper, we treat them equally as different perspectives of LLMs' reasoning processes. We propose the Multi-Perspective Self-Consistency (MPSC) framework incorporating both inter- and intra-consistency across outputs from multiple perspectives. Specifically, we prompt LLMs to generate diverse outputs from three perspectives, Solution, Specification and Test case, constructing a 3-partite graph. With two measure functions of consistency, we embed both inter- and intra-consistency information into the graph. The optimal choice of solutions is then determined based on analysis in the graph. MPSC significantly boosts performance of foundation models (ChatGPT in this paper) on various benchmarks, including HumanEval (+15.91%), MBPP (+6.43%) and CodeContests (+9.37%), even surpassing GPT-4.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Multi-Perspective Self-Consistency (MPSC) framework is proposed incorporating both inter- and intra-consistency across outputs from multiple perspectives and significantly boosts performance of foundation models on various benchmarks, including HumanEval, MBPP and CodeContests."
            },
            "score": 5
        },
        {
            "id": "03d131aaf8a933c0c1fdc42530210449958b5036",
            "paperId": "03d131aaf8a933c0c1fdc42530210449958b5036",
            "title": "Semantic Consistency for Assuring Reliability of Large Language Models",
            "abstract": "Large Language Models (LLMs) exhibit remarkable fluency and competence across various natural language tasks. However, recent research has highlighted their sensitivity to variations in input prompts. To deploy LLMs in a safe and reliable manner, it is crucial for their outputs to be consistent when prompted with expressions that carry the same meaning or intent. While some existing work has explored how state-of-the-art LLMs address this issue, their evaluations have been confined to assessing lexical equality of single- or multi-word answers, overlooking the consistency of generative text sequences. For a more comprehensive understanding of the consistency of LLMs in open-ended text generation scenarios, we introduce a general measure of semantic consistency, and formulate multiple versions of this metric to evaluate the performance of various LLMs. Our proposal demonstrates significantly higher consistency and stronger correlation with human evaluations of output consistency than traditional metrics based on lexical consistency. Finally, we propose a novel prompting strategy, called Ask-to-Choose (A2C), to enhance semantic consistency. When evaluated for closed-book question answering based on answer variations from the TruthfulQA benchmark, A2C increases accuracy metrics for pretrained and finetuned LLMs by up to 47%, and semantic consistency metrics for instruction-tuned models by up to 7-fold.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A general measure of semantic consistency is introduced, and multiple versions of this metric are formulated to evaluate the performance of various LLMs, and a novel prompting strategy is proposed, called Ask-to-Choose (A2C), to enhance semantic consistency."
            },
            "score": 5
        },
        {
            "id": "5b2d98622d793afacc6113028e4586d8ae0adfb2",
            "paperId": "5b2d98622d793afacc6113028e4586d8ae0adfb2",
            "title": "Evaluating the Robustness of Deep learning Models for NLP",
            "abstract": "The significance of deep neural networks (DNNs) has been well established through its success in a variety of tasks. However, recent studies have shown that DNNs are vulnerable to adversarial examples \u2014 inputs crafted by adding small perturbations to the original input. Such perturbations are almost imperceptible to humans but deceive DNNs thus raising major concerns about their utility in real world applications. Although, existing adversarial attack methods in NLP have achieved high success rate in attacking DNNs, they either require detailed information about the target model, training data or need a large amount of queries to generate attacks. Therefore, such attack methods are not realistic as it does not reflect the types of attacks that can be encountered in the real world and are less effective as attacks relying on model information and excessive queries can be easily defended against. In this thesis, we address the above mentioned drawbacks by proposing two realistic attack settings \u2014 hard label black box setting and limited query setting. Next, we propose two novel attack methods that crafts plausible and semantically similar adversarial examples in the above settings. The first attack method uses a population based optimization procedure to craft adversarial examples in the hard label black box setting. The second method is a query efficient method that leverages word attention scores and locality sensitive hashing to find important words for substitution in the limited query setting. We benchmark our results across the same search space used in prior attacks so as to ensure fair and consistent comparison. To improve the quality of generated adversarial examples, we propose an alternative method that uses masked language model to find candidate words for substitution by considering the information of both the original word and its surrounding context. We demonstrate the efficacy of each of our proposed approach by attacking NLP models for text classification and natural language inference task. In addition to that we use adversarial examples to evaluate the robustness and generalization of recent math word problem solvers. Our results showcase that DNNs for the above tasks are not robust as they can be deceived by our proposed attack methods in a highly restricted setting. We conduct human evaluation studies to verify the validity and quality of generated adversarial examples.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This thesis proposes two realistic attack settings \u2014 hard label black box setting and limited query setting and proposes an alternative method that uses masked language model to find candidate words for substitution by considering the information of both the original word and its surrounding context."
            },
            "score": 5
        },
        {
            "id": "85d08a213e9533c515601451cd78f971e547b1ae",
            "paperId": "85d08a213e9533c515601451cd78f971e547b1ae",
            "title": "TextGrad: Advancing Robustness Evaluation in NLP by Gradient-Driven Optimization",
            "abstract": "Robustness evaluation against adversarial examples has become increasingly important to unveil the trustworthiness of the prevailing deep models in natural language processing (NLP). However, in contrast to the computer vision domain where the first-order projected gradient descent (PGD) is used as the benchmark approach to generate adversarial examples for robustness evaluation, there lacks a principled first-order gradient-based robustness evaluation framework in NLP. The emerging optimization challenges lie in 1) the discrete nature of textual inputs together with the strong coupling between the perturbation location and the actual content, and 2) the additional constraint that the perturbed text should be fluent and achieve a low perplexity under a language model. These challenges make the development of PGD-like NLP attacks difficult. To bridge the gap, we propose TextGrad, a new attack generator using gradient-driven optimization, supporting high-accuracy and high-quality assessment of adversarial robustness in NLP. Specifically, we address the aforementioned challenges in a unified optimization framework. And we develop an effective convex relaxation method to co-optimize the continuously-relaxed site selection and perturbation variables and leverage an effective sampling method to establish an accurate mapping from the continuous optimization variables to the discrete textual perturbations. Moreover, as a first-order attack generation method, TextGrad can be baked into adversarial training to further improve the robustness of NLP models. Extensive experiments are provided to demonstrate the effectiveness of TextGrad not only in attack generation for robustness evaluation but also in adversarial defense.",
            "year": 2022,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "TextGrad is proposed, a new attack generator using gradient-driven optimization, supporting high-accuracy and high-quality assessment of adversarial robustness in NLP, and an effective convex relaxation method to co-optimize the continuously-relaxed site selection and perturbation variables."
            },
            "score": 5
        },
        {
            "id": "1478310a58d061b83bacc3b439bee2e0d7202bf0",
            "paperId": "1478310a58d061b83bacc3b439bee2e0d7202bf0",
            "title": "DFKI-NLP at SemEval-2024 Task 2: Towards Robust LLMs Using Data Perturbations and MinMax Training",
            "abstract": "The NLI4CT task at SemEval-2024 emphasizes the development of robust models for Natural Language Inference on Clinical Trial Reports (CTRs) using large language models (LLMs). This edition introduces interventions specifically targeting the numerical, vocabulary, and semantic aspects of CTRs. Our proposed system harnesses the capabilities of the state-of-the-art Mistral model, complemented by an auxiliary model, to focus on the intricate input space of the NLI4CT dataset. Through the incorporation of numerical and acronym-based perturbations to the data, we train a robust system capable of handling both semantic-altering and numerical contradiction interventions. Our analysis on the dataset sheds light on the challenging sections of the CTRs for reasoning.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This edition introduces interventions specifically targeting the numerical, vocabulary, and semantic aspects of CTRs, specifically targeting the state-of-the-art Mistral model, complemented by an auxiliary model to focus on the intricate input space of the NLI4CT dataset."
            },
            "score": 5
        },
        {
            "id": "383a734d473033378b0cbc83a3556050214c2109",
            "paperId": "383a734d473033378b0cbc83a3556050214c2109",
            "title": "Towards Zero-shot Commonsense Reasoning with Self-supervised Refinement of Language Models",
            "abstract": "Can we get existing language models and refine them for zero-shot commonsense reasoning? This paper presents an initial study exploring the feasibility of zero-shot commonsense reasoning for the Winograd Schema Challenge by formulating the task as self-supervised refinement of a pre-trained language model. In contrast to previous studies that rely on fine-tuning annotated datasets, we seek to boost conceptualization via loss landscape refinement. To this end, we propose a novel self-supervised learning approach that refines the language model utilizing a set of linguistic perturbations of similar concept relationships. Empirical analysis of our conceptually simple framework demonstrates the viability of zero-shot commonsense reasoning on multiple benchmarks.",
            "year": 2021,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel self-supervised learning approach that refines the language model utilizing a set of linguistic perturbations of similar concept relationships that demonstrates the viability of zero-shot commonsense reasoning on multiple benchmarks."
            },
            "score": 4
        },
        {
            "id": "e01515c6138bc525f7aec30fc85f2adf028d4156",
            "paperId": "e01515c6138bc525f7aec30fc85f2adf028d4156",
            "title": "Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision",
            "abstract": "Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and guide the LLM through in-context learning from demonstrations (of principles application) to produce helpful, ethical, and reliable responses to user's queries; third, we fine-tune the original LLM with the high-quality self-aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally, we offer a refinement step to address the issues of overly-brief or indirect responses. Applying SELF-ALIGN to the LLaMA-65b base language model, we develop an AI assistant named Dromedary. With fewer than 300 lines of human annotations (including<200 seed prompts, 16 generic principles, and 5 exemplars for in-context learning). Dromedary significantly surpasses the performance of several state-of-the-art AI systems, including Text-Davinci-003 and Alpaca, on benchmark datasets with various settings.",
            "year": 2023,
            "citationCount": 137,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An AI assistant named Dromedary is developed, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision and significantly surpasses the performance of several state-of-the-art AI systems on benchmark datasets with various settings."
            },
            "score": 4
        },
        {
            "id": "7c5aa120a582bd192b2be4952953040b41d3d503",
            "paperId": "7c5aa120a582bd192b2be4952953040b41d3d503",
            "title": "Certified Robustness for Large Language Models with Self-Denoising",
            "abstract": "Although large language models (LLMs) have achieved great success in vast real-world applications, their vulnerabilities towards noisy inputs have significantly limited their uses, especially in high-stake environments. In these contexts, it is crucial to ensure that every prediction made by large language models is stable, i.e., LLM predictions should be consistent given minor differences in the input. This largely falls into the study of certified robust LLMs, i.e., all predictions of LLM are certified to be correct in a local region around the input. Randomized smoothing has demonstrated great potential in certifying the robustness and prediction stability of LLMs. However, randomized smoothing requires adding noise to the input before model prediction, and its certification performance depends largely on the model's performance on corrupted data. As a result, its direct application to LLMs remains challenging and often results in a small certification radius. To address this issue, we take advantage of the multitasking nature of LLMs and propose to denoise the corrupted inputs with LLMs in a self-denoising manner. Different from previous works like denoised smoothing, which requires training a separate model to robustify LLM, our method enjoys far better efficiency and flexibility. Our experiment results show that our method outperforms the existing certification methods under both certified robustness and empirical robustness. The codes are available at https://github.com/UCSB-NLP-Chang/SelfDenoise.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work takes advantage of the multitasking nature of LLMs and proposes to denoise the corrupted inputs with LLMs in a self-denoising manner, and outperforms the existing certification methods under both certified robustness and empirical robustness."
            },
            "score": 4
        },
        {
            "id": "d86b1f87863cd14d03f6de4ac1082dc047299a45",
            "paperId": "d86b1f87863cd14d03f6de4ac1082dc047299a45",
            "title": "Advancing the Robustness of Large Language Models through Self-Denoised Smoothing",
            "abstract": "Although large language models (LLMs) have achieved significant success, their vulnerability to adversarial perturbations, including recent jailbreak attacks, has raised considerable concerns. However, the increasing size of these models and their limited access make improving their robustness a challenging task. Among various defense strategies, randomized smoothing has shown great potential for LLMs, as it does not require full access to the model's parameters or fine-tuning via adversarial training. However, randomized smoothing involves adding noise to the input before model prediction, and the final model's robustness largely depends on the model's performance on these noise corrupted data. Its effectiveness is often limited by the model's sub-optimal performance on noisy data. To address this issue, we propose to leverage the multitasking nature of LLMs to first denoise the noisy inputs and then to make predictions based on these denoised versions. We call this procedure self-denoised smoothing. Unlike previous denoised smoothing techniques in computer vision, which require training a separate model to enhance the robustness of LLMs, our method offers significantly better efficiency and flexibility. Our experimental results indicate that our method surpasses existing methods in both empirical and certified robustness in defending against adversarial attacks for both downstream tasks and human alignments (i.e., jailbreak attacks). Our code is publicly available at https://github.com/UCSB-NLP-Chang/SelfDenoise",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results indicate that the proposed self-denoised smoothing method surpasses existing methods in both empirical and certified robustness in defending against adversarial attacks for both downstream tasks and human alignments."
            },
            "score": 4
        },
        {
            "id": "dee83ee947ea95f54b40a3c101552b4eb447bbf6",
            "paperId": "dee83ee947ea95f54b40a3c101552b4eb447bbf6",
            "title": "Self-Consistency of Large Language Models under Ambiguity",
            "abstract": "Large language models (LLMs) that do not give consistent answers across contexts are problematic when used for tasks with expectations of consistency\u2013e.g. question-answering, explanations, etc. Our work presents an evaluation benchmark for self-consistency in cases of under-specification where two or more answers can be correct. We conduct a series of behavioral experiments on the OpenAI model suite using an ambiguous integer sequence completion task. We find that average consistency ranges from 67% to 82%, far higher than would be predicted if a model\u2019s consistency was random, and increases as model capability improves. Furthermore, we show that models tend to maintain self-consistency across a series of robustness checks, including prompting speaker changes and sequence length changes. These results suggest that self-consistency arises as an emergent capability without specifically training for it. Despite this, we find that models are uncalibrated when judging their own consistency, with models displaying both over- and under-confidence. We also propose a nonparametric test for determining from token output distribution whether a model assigns non-trivial probability to alternative answers. Using this test, we find that despite increases in self-consistency, models usually place significant weight on alternative, inconsistent answers. This distribution of probability mass provides evidence that even highly self-consistent models internally compute multiple possible responses.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents an evaluation benchmark for self-consistency in cases of under-specification where two or more answers can be correct, and proposes a nonparametric test for determining from token output distribution whether a model assigns non-trivial probability to alternative answers."
            },
            "score": 4
        },
        {
            "id": "d2d16333a4b0dc7e3463b280b9945e5ee6c53396",
            "paperId": "d2d16333a4b0dc7e3463b280b9945e5ee6c53396",
            "title": "TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models",
            "abstract": "Factual consistency evaluation is often conducted using Natural Language Inference (NLI) models, yet these models exhibit limited success in evaluating summaries. Previous work improved such models with synthetic training data. However, the data is typically based on perturbed human-written summaries, which often differ in their characteristics from real model-generated summaries and have limited coverage of possible factual errors. Alternatively, large language models (LLMs) have recently shown promising results in directly evaluating generative tasks, but are too computationally expensive for practical use. Motivated by these limitations, we introduce TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM. Unlike prior work, TrueTeacher does not rely on human-written summaries, and is multilingual by nature. Experiments on the TRUE benchmark show that a student model trained using our data, substantially outperforms both the state-of-the-art model with similar capacity, and the LLM teacher. In a systematic study, we compare TrueTeacher to existing synthetic data generation methods and demonstrate its superiority and robustness to domain-shift. We also show that our method generalizes to multilingual scenarios. Lastly, we release our large scale synthetic dataset (1.4M examples), generated using TrueTeacher, and a checkpoint trained on this data.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM, which does not rely on human-written summaries, and is multilingual by nature."
            },
            "score": 4
        },
        {
            "id": "87eeb9b57e3247598e62d930b8cc53fe996dc779",
            "paperId": "87eeb9b57e3247598e62d930b8cc53fe996dc779",
            "title": "BECEL: Benchmark for Consistency Evaluation of Language Models",
            "abstract": "Behavioural consistency is a critical condition for a language model (LM) to become trustworthy like humans. Despite its importance, however, there is little consensus on the definition of LM consistency, resulting in different definitions across many studies. In this paper, we first propose the idea of LM consistency based on behavioural consistency and establish a taxonomy that classifies previously studied consistencies into several sub-categories. Next, we create a new benchmark that allows us to evaluate a model on 19 test cases, distinguished by multiple types of consistency and diverse downstream tasks. Through extensive experiments on the new benchmark, we ascertain that none of the modern pre-trained language models (PLMs) performs well in every test case, while exhibiting high inconsistency in many cases. Our experimental results suggest that a unified benchmark that covers broad aspects (i.e., multiple consistency types and tasks) is essential for a more precise evaluation.",
            "year": 2022,
            "citationCount": 25,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes the idea of LM consistency based on behavioural consistency and establishes a taxonomy that classifies previously studied consistencies into several sub-categories, and creates a new benchmark that allows for a more precise evaluation."
            },
            "score": 4
        },
        {
            "id": "a5246f262bc0919bbfbc4b0fa66e85be41d92712",
            "paperId": "a5246f262bc0919bbfbc4b0fa66e85be41d92712",
            "title": "Improving Language Models Meaning Understanding and Consistency by Learning Conceptual Roles from Dictionary",
            "abstract": "The non-humanlike behaviour of contemporary pre-trained language models (PLMs) is a leading cause undermining their trustworthiness. A striking phenomenon of such faulty behaviours is the generation of inconsistent predictions, which produces logically contradictory results, such as generating different predictions for texts delivering the same meaning or violating logical properties. Previous studies exploited data augmentation or implemented specialised loss functions to alleviate the issue. However, their usage is limited, because they consume expensive training resources for large-sized PLMs and can only handle a certain consistency type. To this end, we propose a practical approach that alleviates the inconsistent behaviour issue by fundamentally improving PLMs' meaning awareness. Based on the conceptual role theory, our method allows PLMs to capture accurate meaning by learning precise interrelationships between concepts from word-definition pairs in a dictionary. Next, we propose an efficient parameter integration technique that updates only a few additional parameters to combine the learned interrelationship with PLMs' pre-trained knowledge. Our experimental results reveal that the approach can concurrently improve multiple types of consistency, enables efficient knowledge integration, and easily applies to other languages.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The method allows PLMs to capture accurate meaning by learning precise interrelationships between concepts from word-definition pairs in a dictionary, and proposes an efficient parameter integration technique that updates only a few additional parameters to combine the learned interrelationship with PLMs' pre-trained knowledge."
            },
            "score": 4
        },
        {
            "id": "75c38109a98d288254e93a9c76e3c17eba58edc3",
            "paperId": "75c38109a98d288254e93a9c76e3c17eba58edc3",
            "title": "Balanced Adversarial Training: Balancing Tradeoffs Between Oversensitivity and Undersensitivity in NLP Models",
            "abstract": "Traditional ( oversensitive ) adversarial exam-001 ples involve finding a small perturbation that 002 does not change an input\u2019s true label but con-003 fuses the classifier into outputting a different 004 prediction. Undersensitive adversarial exam-005 ples are the opposite\u2014the adversary\u2019s goal is to 006 find a small perturbation that changes the true 007 label of an input while preserving the classi-008 fier\u2019s prediction. Adversarial training and certi-009 fied robust training have shown some effective-010 ness in improving the robustness of machine 011 learnt models to oversensitive adversarial exam-012 ples. However, recent work has shown that us-013 ing these techniques to improve robustness for 014 image classifiers may make a model more vul-015 nerable to undersensitive adversarial examples. 016 We demonstrate the same phenomenon applies 017 to NLP models, showing that training methods 018 that improve robustness to synonym-based at-019 tacks (oversensitive adversarial examples) tend 020 to increase a model\u2019s vulnerability to antonym-021 based attacks (undersensitive adversarial exam-022 ples) for both natural language inference and 023 paraphrase identification tasks. To counter this 024 phenomenon, we introduce Balanced Adver-025 sarial Training which incorporates contrastive 026 learning to increase robustness against both 027 over-and undersensitive adversarial examples. 028",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Balanced Adver-025 sarial Training which incorporates contrastive learning to increase robustness against both over-and undersensitive adversarial examples is introduced."
            },
            "score": 4
        },
        {
            "id": "ae9e6c36a68302803783f03cd914055b67d2559b",
            "paperId": "ae9e6c36a68302803783f03cd914055b67d2559b",
            "title": "Improving the Robustness of Transformer-based Large Language Models with Dynamic Attention",
            "abstract": "Transformer-based models, such as BERT and GPT, have been widely adopted in natural language processing (NLP) due to their exceptional performance. However, recent studies show their vulnerability to textual adversarial attacks where the model's output can be misled by intentionally manipulating the text inputs. Despite various methods that have been proposed to enhance the model's robustness and mitigate this vulnerability, many require heavy consumption resources (e.g., adversarial training) or only provide limited protection (e.g., defensive dropout). In this paper, we propose a novel method called dynamic attention, tailored for the transformer architecture, to enhance the inherent robustness of the model itself against various adversarial attacks. Our method requires no downstream task knowledge and does not incur additional costs. The proposed dynamic attention consists of two modules: (I) attention rectification, which masks or weakens the attention value of the chosen tokens, and (ii) dynamic modeling, which dynamically builds the set of candidate tokens. Extensive experiments demonstrate that dynamic attention significantly mitigates the impact of adversarial attacks, improving up to 33\\% better performance than previous methods against widely-used adversarial attacks. The model-level design of dynamic attention enables it to be easily combined with other defense methods (e.g., adversarial training) to further enhance the model's robustness. Furthermore, we demonstrate that dynamic attention preserves the state-of-the-art robustness space of the original model compared to other dynamic modeling methods.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel method called dynamic attention, tailored for the transformer architecture, to enhance the inherent robustness of the model itself against various adversarial attacks, and demonstrates that dynamic attention preserves the state-of-the-art robustness space of the original model compared to other dynamic modeling methods."
            },
            "score": 4
        },
        {
            "id": "22fbe53be04b3e3b13342c70f685ed56c73a85b2",
            "paperId": "22fbe53be04b3e3b13342c70f685ed56c73a85b2",
            "title": "The Impact of Quantization on the Robustness of Transformer-based Text Classifiers",
            "abstract": "Transformer-based models have made remarkable advancements in various NLP areas. Nevertheless, these models often exhibit vulnerabilities when confronted with adversarial attacks. In this paper, we explore the effect of quantization on the robustness of Transformer-based models. Quantization usually involves mapping a high-precision real number to a lower-precision value, aiming at reducing the size of the model at hand. To the best of our knowledge, this work is the first application of quantization on the robustness of NLP models. In our experiments, we evaluate the impact of quantization on BERT and DistilBERT models in text classification using SST-2, Emotion, and MR datasets. We also evaluate the performance of these models against TextFooler, PWWS, and PSO adversarial attacks. Our findings show that quantization significantly improves (by an average of 18.68%) the adversarial accuracy of the models. Furthermore, we compare the effect of quantization versus that of the adversarial training approach on robustness. Our experiments indicate that quantization increases the robustness of the model by 18.80% on average compared to adversarial training without imposing any extra computational overhead during training. Therefore, our results highlight the effectiveness of quantization in improving the robustness of NLP models.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work evaluates the impact of quantization on BERT and DistilBERT models in text classification using SST-2, Emotion, and MR datasets and shows that quantization significantly improves the adversarial accuracy of the models."
            },
            "score": 4
        },
        {
            "id": "ce6b0a9877e135c38eb3a6c6705c95422181af78",
            "paperId": "ce6b0a9877e135c38eb3a6c6705c95422181af78",
            "title": "Evaluating the Robustness of Discrete Prompts",
            "abstract": "Discrete prompts have been used for fine-tuning Pre-trained Language Models for diverse NLP tasks. In particular, automatic methods that generate discrete prompts from a small set of training instances have reported superior performance. However, a closer look at the learnt prompts reveals that they contain noisy and counter-intuitive lexical constructs that would not be encountered in manually-written prompts. This raises an important yet understudied question regarding the robustness of automatically learnt discrete prompts when used in downstream tasks. To address this question, we conduct a systematic study of the robustness of discrete prompts by applying carefully designed perturbations into an application using AutoPrompt and then measure their performance in two Natural Language Inference (NLI) datasets. Our experimental results show that although the discrete prompt-based method remains relatively robust against perturbations to NLI inputs, they are highly sensitive to other types of perturbations such as shuffling and deletion of prompt tokens. Moreover, they generalize poorly across different NLI datasets. We hope our findings will inspire future work on robust discrete prompt learning.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Although the discrete prompt-based method remains relatively robust against perturbation to NLI inputs, they are highly sensitive to other types of perturbations such as shuffling and deletion of prompt tokens and generalize poorly across different NLI datasets."
            },
            "score": 4
        },
        {
            "id": "8e37dc1215681aa153a51c07078ba8befd6a6e01",
            "paperId": "8e37dc1215681aa153a51c07078ba8befd6a6e01",
            "title": "AdaPlanner: Adaptive Planning from Feedback with Language Models",
            "abstract": "Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively.",
            "year": 2023,
            "citationCount": 49,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A closed-loop approach, AdaPlanner, is proposed, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback, and develops a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities."
            },
            "score": 3
        },
        {
            "id": "2cf1f6c723006f258599fd9f000bb616ae83387a",
            "paperId": "2cf1f6c723006f258599fd9f000bb616ae83387a",
            "title": "Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models",
            "abstract": "The performance of large language models (LLMs) on existing reasoning benchmarks has significantly improved over the past years. In response, we present JEEBench, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs. We curate 515 challenging pre-engineering mathematics, physics and chemistry problems from the highly competitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep in-domain knowledge is essential for solving problems in this benchmark. Our evaluation on various open-source and proprietary models reveals that the highest performance, even after using techniques like self-consistency, self-refinement and chain-of-thought prompting, is less than 40%. The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts. We also observe that by mere prompting, GPT-4 is unable to assess risk introduced by negative marking for incorrect answers. For this, we develop a post-hoc confidence-thresholding method over self-consistency, which enables effective response selection. We hope that our challenging benchmark will guide future re-search in problem-solving using LLMs.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "JEEBench is presented, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs, and a post-hoc confidence-thresholding method over self-consistency is developed, which enables effective response selection."
            },
            "score": 3
        },
        {
            "id": "4302593601e57a1c4cb487d871723b9d11a8e2b2",
            "paperId": "4302593601e57a1c4cb487d871723b9d11a8e2b2",
            "title": "PID Control-Based Self-Healing to Improve the Robustness of Large Language Models",
            "abstract": "Despite the effectiveness of deep neural networks in numerous natural language processing applications, recent findings have exposed the vulnerability of these language models when minor perturbations are introduced. While appearing semantically indistinguishable to humans, these perturbations can significantly reduce the performance of well-trained language models, raising concerns about the reliability of deploying them in safe-critical situations. In this work, we construct a computationally efficient self-healing process to correct undesired model behavior during online inference when perturbations are applied to input data. This is formulated as a trajectory optimization problem in which the internal states of the neural network layers are automatically corrected using a PID (Proportional-Integral-Derivative) control mechanism. The P controller targets immediate state adjustments, while the I and D controllers consider past states and future dynamical trends, respectively. We leverage the geometrical properties of the training data to design effective linear PID controllers. This approach reduces the computational cost to that of using just the P controller, instead of the full PID control. Further, we introduce an analytical method for approximating the optimal control solutions, enhancing the real-time inference capabilities of this controlled system. Moreover, we conduct a theoretical error analysis of the analytic solution in a simplified setting. The proposed PID control-based self-healing is a low cost framework that improves the robustness of pre-trained large language models, whether standard or robustly trained, against a wide range of perturbations. A detailed implementation can be found in:https://github.com/zhuotongchen/PID-Control-Based-Self-Healing-to-Improve-the-Robustness-of-Large-Language-Models.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The proposed PID control-based self-healing is a low cost framework that improves the robustness of pre-trained large language models, whether standard or robustly trained, against a wide range of perturbations."
            },
            "score": 3
        },
        {
            "id": "c3af18fbc0ed47f07f1bbae075aa97bbc08b89e8",
            "paperId": "c3af18fbc0ed47f07f1bbae075aa97bbc08b89e8",
            "title": "Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking in Large Language Models",
            "abstract": "Large language models (LLMs) exhibit positional bias in how they use context, which especially complicates listwise ranking. To address this, we propose permutation self-consistency, a form of self-consistency over ranking list outputs of black-box LLMs. Our key idea is to marginalize out different list orders in the prompt to produce an order-independent ranking with less positional bias. First, given some input prompt, we repeatedly shuffle the list in the prompt and pass it through the LLM while holding the instructions the same. Next, we aggregate the resulting sample of rankings by computing the central ranking closest in distance to all of them, marginalizing out prompt order biases in the process. Theoretically, we prove the robustness of our method, showing convergence to the true ranking in the presence of random perturbations. Empirically, on five list-ranking datasets in sorting and passage reranking, our approach improves scores from conventional inference by up to 7-18% for GPT-3.5 and 8-16% for LLaMA v2 (70B), surpassing the previous state of the art in passage reranking. Our code is at https://github.com/castorini/perm-sc.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Theoretically, the robustness of the proposed permutation self-consistency method is proved, showing convergence to the true ranking in the presence of random perturbations."
            },
            "score": 3
        },
        {
            "id": "2f8a1714e3bb01c522fede9c902bd7cf5320a802",
            "paperId": "2f8a1714e3bb01c522fede9c902bd7cf5320a802",
            "title": "Test-Time Self-Adaptive Small Language Models for Question Answering",
            "abstract": "Recent instruction-finetuned large language models (LMs) have achieved notable performances in various tasks, such as question-answering (QA). However, despite their ability to memorize a vast amount of general knowledge across diverse tasks, they might be suboptimal on specific tasks due to their limited capacity to transfer and adapt knowledge to target tasks. Moreover, further finetuning LMs with labeled datasets is often infeasible due to their absence, but it is also questionable if we can transfer smaller LMs having limited knowledge only with unlabeled test data. In this work, we show and investigate the capabilities of smaller self-adaptive LMs, only with unlabeled test data. In particular, we first stochastically generate multiple answers, and then ensemble them while filtering out low-quality samples to mitigate noise from inaccurate labels. Our proposed self-adaption strategy demonstrates significant performance improvements on benchmark QA datasets with higher robustness across diverse prompts, enabling LMs to stay stable. Code is available at: https://github.com/starsuzi/T-SAS.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work shows and investigates the capabilities of smaller self-adaptive LMs, only with unlabeled test data, and proposes a self-adaption strategy that demonstrates significant performance improvements on benchmark QA datasets with higher robustness across diverse prompts, enabling LMs to stay stable."
            },
            "score": 3
        },
        {
            "id": "0935ce0adad57e1b24c50d793d46a407c3f563f3",
            "paperId": "0935ce0adad57e1b24c50d793d46a407c3f563f3",
            "title": "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents",
            "abstract": "We investigate the challenge of task planning for multi-task embodied agents in open-world environments. Two main difficulties are identified: 1) executing plans in an open-world environment (e.g., Minecraft) necessitates accurate and multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla planners do not consider how easy the current agent can achieve a given sub-task when ordering parallel sub-goals within a complicated plan, the resulting plan could be inefficient or even infeasible. To this end, we propose\"$\\underline{D}$escribe, $\\underline{E}$xplain, $\\underline{P}$lan and $\\underline{S}$elect\"($\\textbf{DEPS}$), an interactive planning approach based on Large Language Models (LLMs). DEPS facilitates better error correction on initial LLM-generated $\\textit{plan}$ by integrating $\\textit{description}$ of the plan execution process and providing self-$\\textit{explanation}$ of feedback when encountering failures during the extended planning phases. Furthermore, it includes a goal $\\textit{selector}$, which is a trainable module that ranks parallel candidate sub-goals based on the estimated steps of completion, consequently refining the initial plan. Our experiments mark the milestone of the first zero-shot multi-task agent that can robustly accomplish 70+ Minecraft tasks and nearly double the overall performances. Further testing reveals our method's general effectiveness in popularly adopted non-open-ended domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and exploratory studies detail how our design beats the counterparts and provide a promising update on the $\\texttt{ObtainDiamond}$ grand challenge with our approach. The code is released at https://github.com/CraftJarvis/MC-Planner.",
            "year": 2023,
            "citationCount": 163,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The ablation and exploratory studies detail how the design beats the counterparts and provide a promising update on the $\\texttt{ObtainDiamond}$ grand challenge with the MC-Planner approach."
            },
            "score": 3
        },
        {
            "id": "468d1e2d75a23fecaf96fe65d8b01ff35ea5d0bd",
            "paperId": "468d1e2d75a23fecaf96fe65d8b01ff35ea5d0bd",
            "title": "Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models",
            "abstract": "Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts inserted in English reveal a clear pattern whereby the new piece of knowledge transfers only to languages with which English has a high RankC score.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy, and conducts an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level."
            },
            "score": 3
        },
        {
            "id": "8dce801e67f2f8923dc1f5b3059f70b462f4e4cf",
            "paperId": "8dce801e67f2f8923dc1f5b3059f70b462f4e4cf",
            "title": "DetectGPT-SC: Improving Detection of Text Generated by Large Language Models through Self-Consistency with Masked Predictions",
            "abstract": "General large language models (LLMs) such as ChatGPT have shown remarkable success, but it has also raised concerns among people about the misuse of AI-generated texts. Therefore, an important question is how to detect whether the texts are generated by ChatGPT or by humans. Existing detectors are built on the assumption that there is a distribution gap between human-generated and AI-generated texts. These gaps are typically identified using statistical information or classifiers. In contrast to prior research methods, we find that large language models such as ChatGPT exhibit strong self-consistency in text generation and continuation. Self-consistency capitalizes on the intuition that AI-generated texts can still be reasoned with by large language models using the same logical reasoning when portions of the texts are masked, which differs from human-generated texts. Using this observation, we subsequently proposed a new method for AI-generated texts detection based on self-consistency with masked predictions to determine whether a text is generated by LLMs. This method, which we call DetectGPT-SC. We conducted a series of experiments to evaluate the performance of DetectGPT-SC. In these experiments, we employed various mask scheme, zero-shot, and simple prompt for completing masked texts and self-consistency predictions. The results indicate that DetectGPT-SC outperforms the current state-of-the-art across different tasks.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that large language models such as ChatGPT exhibit strong self-consistency in text generation and continuation, and this method, which is called DetectGPT-SC, outperforms the current state-of-the-art across different tasks."
            },
            "score": 3
        },
        {
            "id": "9f859726b3d8dffd96a1f55de4122617751cc1b4",
            "paperId": "9f859726b3d8dffd96a1f55de4122617751cc1b4",
            "title": "Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment",
            "abstract": "Larger language models (LLMs) have taken the world by storm with their massive multi-tasking capabilities simply by optimizing over a next-word prediction objective. With the emergence of their properties and encoded knowledge, the risk of LLMs producing harmful outputs increases, making them unfit for scalable deployment for the public. In this work, we propose a new safety evaluation benchmark RED-EVAL that carries out red-teaming. We show that even widely deployed models are susceptible to the Chain of Utterances-based (CoU) prompting, jailbreaking closed source LLM-based systems such as GPT-4 and ChatGPT to unethically respond to more than 65% and 73% of harmful queries. We also demonstrate the consistency of the RED-EVAL across 8 open-source LLMs in generating harmful responses in more than 86% of the red-teaming attempts. Next, we propose RED-INSTRUCT--An approach for the safety alignment of LLMs. It constitutes two phases: 1) HARMFULQA data collection: Leveraging CoU prompting, we collect a dataset that consists of 1.9K harmful questions covering a wide range of topics, 9.5K safe and 7.3K harmful conversations from ChatGPT; 2) SAFE-ALIGN: We demonstrate how the conversational dataset can be used for the safety alignment of LLMs by minimizing the negative log-likelihood over helpful responses and penalizing over harmful responses by gradient accent over sample loss. Our model STARLING, a fine-tuned Vicuna-7B, is observed to be more safely aligned when evaluated on RED-EVAL and HHH benchmarks while preserving the utility of the baseline models (TruthfulQA, MMLU, and BBH).",
            "year": 2023,
            "citationCount": 48,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that even widely deployed models are susceptible to the Chain of Utterances-based (CoU) prompting, jailbreaking closed source LLM-based systems such as GPT-4 and ChatGPT to unethically respond to more than 65% and 73% of harmful queries."
            },
            "score": 3
        },
        {
            "id": "663d743272e9ab04f54d9105a3c3a3f6e22dd1dd",
            "paperId": "663d743272e9ab04f54d9105a3c3a3f6e22dd1dd",
            "title": "FactKB: Generalizable Factuality Evaluation using Language Models Enhanced with Factual Knowledge",
            "abstract": "Evaluating the factual consistency of automatically generated summaries is essential for the progress and adoption of reliable summarization systems. Despite recent advances, existing factuality evaluation models are not robust, being especially prone to entity and relation errors in new domains. We propose FactKB, a simple new approach to factuality evaluation that is generalizable across domains, in particular with respect to entities and relations. FactKB is based on language models pretrained using facts extracted from external knowledge bases. We introduce three types of complementary factuality pretraining objectives based on direct entity facts, facts grounded in auxiliary knowledge about entities, and facts constructed compositionally through knowledge base walks. The resulting factuality evaluation model achieves state-of-the-art performance on two in-domain news summarization benchmarks as well as on three out-of-domain scientific literature datasets. Further analysis of FactKB shows improved ability to detect erroneous entities and relations in summaries and is robust and generalizable across domains.",
            "year": 2023,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "FactKB is a simple new approach to factuality evaluation that is generalizable across domains, in particular with respect to entities and relations and shows improved ability to detect erroneous entities and relation in summaries."
            },
            "score": 3
        },
        {
            "id": "57887ed45398f118112e0b493c22beaf2be8688a",
            "paperId": "57887ed45398f118112e0b493c22beaf2be8688a",
            "title": "Backdoor Attack against NLP models with Robustness-Aware Perturbation defense",
            "abstract": "Backdoor attack intends to embed hidden backdoor into deep neural networks (DNNs), such that the attacked model performs well on benign samples, whereas its prediction will be maliciously changed if the hidden backdoor is activated by the attacker de\ufb01ned trigger. This threat could happen when the training process is not fully controlled, such as training on third-party data-sets or adopting third-party models. There has been a lot of research and di\ufb00erent methods to defend such type of backdoor attacks, one being robustness-aware perturbation-based defense method. This method mainly exploits big gap of robustness between poisoned and clean samples. In our work, we break this defense by controlling the robustness gap between poisoned and clean samples using adversarial training step.",
            "year": 2022,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work breaks backdoor defense by controlling the robustness gap between poisoned and clean samples using adversarial training step, and exploits big gap of robustness between poisonedand clean samples."
            },
            "score": 3
        },
        {
            "id": "ca115a9eb54e2875b9d4c4c3d5ed8adcb399dbf8",
            "paperId": "ca115a9eb54e2875b9d4c4c3d5ed8adcb399dbf8",
            "title": "RAP: Robustness-Aware Perturbations for Defending against Backdoor Attacks on NLP Models",
            "abstract": "Backdoor attacks, which maliciously control a well-trained model\u2019s outputs of the instances with specific triggers, are recently shown to be serious threats to the safety of reusing deep neural networks (DNNs). In this work, we propose an efficient online defense mechanism based on robustness-aware perturbations. Specifically, by analyzing the backdoor training process, we point out that there exists a big gap of robustness between poisoned and clean samples. Motivated by this observation, we construct a word-based robustness-aware perturbation to distinguish poisoned samples from clean samples to defend against the backdoor attacks on natural language processing (NLP) models. Moreover, we give a theoretical analysis about the feasibility of our robustness-aware perturbation-based defense method. Experimental results on sentiment analysis and toxic detection tasks show that our method achieves better defending performance and much lower computational costs than existing online defense methods. Our code is available at https://github.com/lancopku/RAP.",
            "year": 2021,
            "citationCount": 51,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work constructs a word-based robustness-aware perturbation to distinguish poisoned samples from clean samples to defend against the backdoor attacks on natural language processing (NLP) models."
            },
            "score": 3
        },
        {
            "id": "292da1c4640c105aa5d7919a1ded9a1225c07d4d",
            "paperId": "292da1c4640c105aa5d7919a1ded9a1225c07d4d",
            "title": "Large Language Models and the Reverse Turing Test",
            "abstract": "Abstract Large language models (LLMs) have been transformative. They are pretrained foundational models that are self-supervised and can be adapted with fine-tuning to a wide range of natural language tasks, each of which previously would have required a separate network model. This is one step closer to the extraordinary versatility of human language. GPT-3 and, more recently, LaMDA, both of them LLMs, can carry on dialogs with humans on many topics after minimal priming with a few examples. However, there has been a wide range of reactions and debate on whether these LLMs understand what they are saying or exhibit signs of intelligence. This high variance is exhibited in three interviews with LLMs reaching wildly different conclusions. A new possibility was uncovered that could explain this divergence. What appears to be intelligence in LLMs may in fact be a mirror that reflects the intelligence of the interviewer, a remarkable twist that could be considered a reverse Turing test. If so, then by studying interviews, we may be learning more about the intelligence and beliefs of the interviewer than the intelligence of the LLMs. As LLMs become more capable, they may transform the way we interact with machines and how they interact with each other. Increasingly, LLMs are being coupled with sensorimotor devices. LLMs can talk the talk, but can they walk the walk? A road map for achieving artificial general autonomy is outlined with seven major improvements inspired by brain systems and how LLMs could in turn be used to uncover new insights into brain function.",
            "year": 2022,
            "citationCount": 53,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A road map for achieving artificial general autonomy is outlined with seven major improvements inspired by brain systems and how LLMs could in turn be used to uncover new insights into brain function."
            },
            "score": 2
        },
        {
            "id": "43651d86992c6e3a670f2bc22b64a1cf00c050a6",
            "paperId": "43651d86992c6e3a670f2bc22b64a1cf00c050a6",
            "title": "Dialogue-based generation of self-driving simulation scenarios using Large Language Models",
            "abstract": "Simulation is an invaluable tool for developing and evaluating controllers for self-driving cars. Current simulation frameworks are driven by highly-specialist domain specific languages, and so a natural language interface would greatly enhance usability. But there is often a gap, consisting of tacit assumptions the user is making, between a concise English utterance and the executable code that captures the user\u2019s intent. In this paper we describe a system that addresses this issue by supporting an extended multimodal interaction: the user can follow up prior instructions with refinements or revisions, in reaction to the simulations that have been generated from their utterances so far. We use Large Language Models (LLMs) to map the user\u2019s English utterances in this interaction into domain-specific code, and so we explore the extent to which LLMs capture the context sensitivity that\u2019s necessary for computing the speaker\u2019s intended message in discourse.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper describes a system that supports an extended multimodal interaction that the user can follow up prior instructions with refinements or revisions, in reaction to the simulations that have been generated from their utterances so far, and uses Large Language Models to map the user\u2019s English utterances into domain-specific code."
            },
            "score": 2
        },
        {
            "id": "10632e0a667cbc3c52cc8f11a46d8e8e9c7739e3",
            "paperId": "10632e0a667cbc3c52cc8f11a46d8e8e9c7739e3",
            "title": "Causal Reasoning and Large Language Models: Opening a New Frontier for Causality",
            "abstract": "The causal capabilities of large language models (LLMs) is a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We further our understanding of LLMs and their causal implications, considering the distinctions between different types of causal reasoning tasks, as well as the entangled threats of construct and measurement validity. LLM-based methods establish new state-of-the-art accuracies on multiple causal benchmarks. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain), and actual causality (86% accuracy in determining necessary and sufficient causes in vignettes). At the same time, LLMs exhibit unpredictable failure modes and we provide some techniques to interpret their robustness. Crucially, LLMs perform these causal tasks while relying on sources of knowledge and methods distinct from and complementary to non-LLM based approaches. Specifically, LLMs bring capabilities so far understood to be restricted to humans, such as using collected knowledge to generate causal graphs or identifying background causal context from natural language. We envision LLMs to be used alongside existing causal methods, as a proxy for human domain knowledge and to reduce human effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods. We also see existing causal methods as promising tools for LLMs to formalize, validate, and communicate their reasoning especially in high-stakes scenarios. In capturing common sense and domain knowledge about causal mechanisms and supporting translation between natural language and formal methods, LLMs open new frontiers for advancing the research, practice, and adoption of causality.",
            "year": 2023,
            "citationCount": 96,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LLMs are envisioned to be used alongside existing causal methods, as a proxy for human domain knowledge and to reduce human effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods."
            },
            "score": 2
        },
        {
            "id": "2403c8e72a90d9c778970fc0812ecdcc58800c5d",
            "paperId": "2403c8e72a90d9c778970fc0812ecdcc58800c5d",
            "title": "Can Language Models be Instructed to Protect Personal Information?",
            "abstract": "Large multimodal language models have proven transformative in numerous applications. However, these models have been shown to memorize and leak pre-training data, raising serious user privacy and information security concerns. While data leaks should be prevented, it is also crucial to examine the trade-off between the privacy protection and model utility of proposed approaches. In this paper, we introduce PrivQA -- a multimodal benchmark to assess this privacy/utility trade-off when a model is instructed to protect specific categories of personal information in a simulated scenario. We also propose a technique to iteratively self-moderate responses, which significantly improves privacy. However, through a series of red-teaming experiments, we find that adversaries can also easily circumvent these protections with simple jailbreaking methods through textual and/or image inputs. We believe PrivQA has the potential to support the development of new models with improved privacy protections, as well as the adversarial robustness of these protections. We release the entire PrivQA dataset at https://llm-access-control.github.io/.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "PrivQA is introduced -- a multimodal benchmark to assess this privacy/utility trade-off when a model is instructed to protect specific categories of personal information in a simulated scenario and a technique to iteratively self-moderate responses, which significantly improves privacy."
            },
            "score": 2
        },
        {
            "id": "fbebf142df7d4d75d941a20f2398457354cb83aa",
            "paperId": "fbebf142df7d4d75d941a20f2398457354cb83aa",
            "title": "On The Robustness of Self-Supervised Representations for Spoken Language Modeling",
            "abstract": "Self-supervised representations have been extensively studied for discriminative and generative tasks. However, their robustness capabilities have not been extensively investigated. This work focuses on self-supervised representations for spoken generative language models. First, we empirically demonstrate how current state-of-the-art speech representation models lack robustness to basic signal variations that do not alter the spoken information. To overcome this, we propose an effective and ef\ufb01cient method to learn robust self-supervised speech representation for generative spoken language modeling. The proposed approach is based on applying a set of signal transformations to the speech signal and optimizing the model using an iterative pseudo-labeling scheme. Our method signi\ufb01cantly improves over the evaluated baselines when considering encoding metrics. We additionally evaluate our method on the speech-to-speech translation task. We consider Spanish-English and French-English conversions and empirically demonstrate the bene\ufb01ts of following the proposed approach.",
            "year": 2022,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work empirically demonstrate how current state-of-the-art speech representation models lack robustness to basic signal variations that do not alter the spoken information and proposes an effective andcient method to learn robust self-supervised speech representation for generative spoken language modeling."
            },
            "score": 2
        },
        {
            "id": "86c7cb73e3a42a130c8d43ae3e26cb9c8df381a1",
            "paperId": "86c7cb73e3a42a130c8d43ae3e26cb9c8df381a1",
            "title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
            "abstract": "To what degree should we ascribe cognitive capacities to Large Language Models (LLMs), such as the ability to reason about intentions and beliefs known as Theory of Mind (ToM)? Here we add to this emerging debate by (i) testing 11 base- and instruction-tuned LLMs on capabilities relevant to ToM beyond the dominant false-belief paradigm, including non-literal language usage and recursive intentionality; (ii) using newly rewritten versions of standardized tests to gauge LLMs\u2019 robustness; (iii) prompting and scoring for open besides closed questions; and (iv) benchmarking LLM performance against that of children aged 7-10 on the same tasks. We find that instruction-tuned LLMs from the GPT family outperform other models, and often also children. Base-LLMs are mostly unable to solve ToM tasks, even with specialized prompting. We suggest that the interlinked evolution and development of language and ToM may help explain what instruction-tuning adds: rewarding cooperative communication that takes into account interlocutor and context. We conclude by arguing for a nuanced perspective on ToM in LLMs.",
            "year": 2023,
            "citationCount": 6,
            "tldr": null,
            "score": 2
        },
        {
            "id": "f43f3aea99da056b19c6a5ee0f5aac8010c343a3",
            "paperId": "f43f3aea99da056b19c6a5ee0f5aac8010c343a3",
            "title": "Test of Time: Instilling Video-Language Models with a Sense of Time",
            "abstract": "Modelling and understanding time remains a challenge in contemporary video understanding models. With language emerging as a key driver towards powerful generalization, it is imperative for foundational video-language models to have a sense of time. In this paper, we consider a specific aspect of temporal understanding: consistency of time order as elicited by before/after relations. We establish that seven existing video-language models struggle to understand even such simple temporal relations. We then question whether it is feasible to equip these foundational models with temporal awareness without re-training them from scratch. Towards this, we propose a temporal adaptation recipe on top of one such model, VideoCLIp, based on post-pretraining on a small amount of video-text data. We conduct a zero-shot evaluation of the adapted models on six datasets for three downstream tasks which require varying degrees of time awareness. We observe encouraging performance gains especially when the task needs higher time awareness. Our work serves as a first step towards probing and instilling a sense of time in existing video-language models without the need for data and compute-intense training from scratch.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a temporal adaptation recipe on top of one video-language model, VideoCLIp, based on post-pretraining on a small amount of video-text data and conducts a zero-shot evaluation of the adapted models on six datasets for three downstream tasks which require varying degrees of time awareness."
            },
            "score": 2
        },
        {
            "id": "5cf7454601c4944c782518bbb9b420f1b0c1481b",
            "paperId": "5cf7454601c4944c782518bbb9b420f1b0c1481b",
            "title": "Robust Transfer Learning Based Modelling for Accelerating the Learning of Ai in the Field of NLP",
            "abstract": "People have been able to communicate with one another via the languages that have developed naturally within their societies. It is estimated that somewhere over 6,500 distinct languages are currently being used in communication around the globe. Because language is the primary means by which humans communicate with one another, it is only natural that this fundamental concept has been investigated in detail in the scholarly literature for a considerable amount of time. This investigation has taken place over a considerable amount of time. In this paper, federated learning is used as an integral feature of the transfer learning model to accelerate the process of learning in NLP. The simulation shows a reduced mean square error in training, testing and validation stages",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "In this paper, federated learning is used as an integral feature of the transfer learning model to accelerate the process of learning in NLP."
            },
            "score": 2
        },
        {
            "id": "3e7c564b9da8ee4f2b1ed437f347aed59a68c529",
            "paperId": "3e7c564b9da8ee4f2b1ed437f347aed59a68c529",
            "title": "Improving VTE Identification through Adaptive NLP Model Selection and Clinical Expert Rule-based Classifier from Radiology Reports",
            "abstract": "Rapid and accurate identification of Venous thromboembolism (VTE), a severe cardiovascular condition including deep vein thrombosis (DVT) and pulmonary embolism (PE), is important for effective treatment. Leveraging Natural Language Processing (NLP) on radiology reports, automated methods have shown promising advancements in identifying VTE events from retrospective data cohorts or aiding clinical experts in identifying VTE events from radiology reports. However, effectively training Deep Learning (DL) and the NLP models is challenging due to limited labeled medical text data, the complexity and heterogeneity of radiology reports, and data imbalance. This study proposes novel method combinations of DL methods, along with data augmentation, adaptive pre-trained NLP model selection, and a clinical expert NLP rule-based classifier, to improve the accuracy of VTE identification in unstructured (free-text) radiology reports. Our experimental results demonstrate the model\u2019s efficacy, achieving an impressive 97% accuracy and 97% F1 score in predicting DVT, and an outstanding 98.3% accuracy and 98.4% F1 score in predicting PE. These findings emphasize the model\u2019s robustness and its potential to significantly contribute to VTE research.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study proposes novel method combinations of DL methods, along with data augmentation, adaptive pre-trained NLP model selection, and a clinical expert NLP rule-based classifier to improve the accuracy of VTE identification in unstructured (free-text) radiology reports."
            },
            "score": 2
        },
        {
            "id": "dca4d9abbc82e57dfa52f932e893d467a63e0682",
            "paperId": "dca4d9abbc82e57dfa52f932e893d467a63e0682",
            "title": "Transformers4Rec: Bridging the Gap between NLP and Sequential / Session-Based Recommendation",
            "abstract": "Much of the recent progress in sequential and session-based recommendation has been driven by improvements in model architecture and pretraining techniques originating in the field of Natural Language Processing. Transformer architectures in particular have facilitated building higher-capacity models and provided data augmentation and training techniques which demonstrably improve the effectiveness of sequential recommendation. But with a thousandfold more research going on in NLP, the application of transformers for recommendation understandably lags behind. To remedy this we introduce Transformers4Rec, an open-source library built upon HuggingFace\u2019s Transformers library with a similar goal of opening up the advances of NLP based Transformers to the recommender system community and making these advancements immediately accessible for the tasks of sequential and session-based recommendation. Like its core dependency, Transformers4Rec is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. In order to demonstrate the usefulness of the library and the applicability of Transformer architectures in next-click prediction for user sessions, where sequence lengths are much shorter than those commonly found in NLP, we have leveraged Transformers4Rec to win two recent session-based recommendation competitions. In addition, we present in this paper the first comprehensive empirical analysis comparing many Transformer architectures and training approaches for the task of session-based recommendation. We demonstrate that the best Transformer architectures have superior performance across two e-commerce datasets while performing similarly to the baselines on two news datasets. We further evaluate in isolation the effectiveness of the different training techniques used in causal language modeling, masked language modeling, permutation language modeling and replacement token detection for a single Transformer architecture, XLNet. We establish that training XLNet with replacement token detection performs well across all datasets. Finally, we explore techniques to include side information such as item and user context features in order to establish best practices and show that the inclusion of side information uniformly improves recommendation performance. Transformers4Rec library is available at https://github.com/NVIDIA-Merlin/Transformers4Rec/",
            "year": 2021,
            "citationCount": 102,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Transformers4Rec is introduced, an open-source library built upon HuggingFace\u2019s Transformers library with a similar goal of opening up the advances of NLP based Transformers to the recommender system community and making these advancements immediately accessible for the tasks of sequential and session-based recommendation."
            },
            "score": 2
        },
        {
            "id": "99368d6fc86e2eb181d9d36165cfed578bfe938d",
            "paperId": "99368d6fc86e2eb181d9d36165cfed578bfe938d",
            "title": "Q: How to Specialize Large Vision-Language Models to Data-Scarce VQA Tasks? A: Self-Train on Unlabeled Images!",
            "abstract": "Finetuning a large vision language model (VLM) on a target dataset after large scale pretraining is a dominant paradigm in visual question answering (VQA). Datasets for specialized tasks such as knowledge-based VQA or VQA in non natural-image domains are orders of magnitude smaller than those for general-purpose VQA. While collecting additional labels for specialized tasks or domains can be challenging, unlabeled images are often available. We introduce SelTDA (Self-Taught Data Augmentation), a strategy for finetuning large VLMs on small-scale VQA datasets. SelTDA uses the VLM and target dataset to build a teacher model that can generate question-answer pseudolabels directly conditioned on an image alone, allowing us to pseudolabel unlabeled images. SelTDA then finetunes the initial VLM on the original dataset augmented with freshly pseudolabeled images. We describe a series of experiments showing that our self-taught data augmentation increases robustness to adversarially searched questions, counterfactual examples and rephrasings, improves domain generalization, and results in greater retention of numerical reasoning skills. The proposed strategy requires no additional annotations or architectural modifications, and is compatible with any modern encoder-decoder multimodal transformer. Code available at https://github.com/codezakh/SelTDA.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces SelTDA (Self-Taught Data Augmentation), a strategy for finetuning large VLMs on small-scale VQA datasets and describes a series of experiments showing that the proposed strategy increases robustness to adversarially searched questions, counterfactual examples and rephrasings, improves domain generalization, and results in greater retention of numerical reasoning skills."
            },
            "score": 1
        },
        {
            "id": "45872b94798c3125abfb185b7926689c5e767763",
            "paperId": "45872b94798c3125abfb185b7926689c5e767763",
            "title": "GraphGPT: Graph Instruction Tuning for Large Language Models",
            "abstract": "Graph Neural Networks (GNNs) have advanced graph structure understanding via recursive information exchange and aggregation among graph nodes. To improve model robustness, self-supervised learning (SSL) has emerged as a promising approach for data augmentation. However, existing methods for generating pre-trained graph embeddings often rely on fine-tuning with specific downstream task labels, which limits their usability in scenarios where labeled data is scarce or unavailable. To address this, our research focuses on advancing the generalization capabilities of graph models in challenging zero-shot learning scenarios. Inspired by the success of large language models (LLMs), we aim to develop a graph-oriented LLM that can achieve high generalization across diverse downstream datasets and tasks, even without any information available from the downstream graph data. In this work, we present the GraphGPT framework that aligns LLMs with graph structural knowledge with a graph instruction tuning paradigm. Our framework incorporates a text-graph grounding component to establish a connection between textual information and graph structures. Additionally, we propose a dual-stage instruction tuning paradigm, accompanied by a lightweight graph-text alignment projector. This paradigm explores self-supervised graph structural signals and task-specific graph instructions, to guide LLMs in understanding complex graph structures and improving their adaptability across different downstream tasks. Our framework is evaluated on supervised and zero-shot graph learning tasks, demonstrating superior generalization and outperforming state-of-the-art baselines.",
            "year": 2023,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The GraphGPT framework is presented, which combines a text-graph grounding component to establish a connection between textual information and graph structures, and a dual-stage instruction tuning paradigm, accompanied by a lightweight graph-text alignment projector, that explores self-supervised graph structural signals and task-specific graph instructions."
            },
            "score": 1
        },
        {
            "id": "e391d266b0d43475567f59efeaeabc884a48abd0",
            "paperId": "e391d266b0d43475567f59efeaeabc884a48abd0",
            "title": "ReLM: Leveraging Language Models for Enhanced Chemical Reaction Prediction",
            "abstract": "Predicting chemical reactions, a fundamental challenge in chemistry, involves forecasting the resulting products from a given reaction process. Conventional techniques, notably those employing Graph Neural Networks (GNNs), are often limited by insufficient training data and their inability to utilize textual information, undermining their applicability in real-world applications. In this work, we propose ReLM, a novel framework that leverages the chemical knowledge encoded in language models (LMs) to assist GNNs, thereby enhancing the accuracy of real-world chemical reaction predictions. To further enhance the model's robustness and interpretability, we incorporate the confidence score strategy, enabling the LMs to self-assess the reliability of their predictions. Our experimental results demonstrate that ReLM improves the performance of state-of-the-art GNN-based methods across various chemical reaction datasets, especially in out-of-distribution settings. Codes are available at https://github.com/syr-cn/ReLM.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes ReLM, a novel framework that leverages the chemical knowledge encoded in language models (LMs) to assist GNNs, thereby enhancing the accuracy of real-world chemical reaction predictions and incorporating the confidence score strategy, enabling the LMs to self-assess the reliability of their predictions."
            },
            "score": 1
        },
        {
            "id": "c18f2239a4bd8cc68db9a013416167357f5e1353",
            "paperId": "c18f2239a4bd8cc68db9a013416167357f5e1353",
            "title": "Evaluating the Performance of Large Language Models on GAOKAO Benchmark",
            "abstract": "Large Language Models(LLMs) have demonstrated remarkable performance across various natural language processing tasks; however, how to comprehensively and accurately assess their performance becomes an urgent issue to be addressed. This paper introduces GAOKAO-Bench, an intuitive benchmark that employs questions from the Chinese GAOKAO examination as test samples, including both subjective and objective questions. To align with human examination methods, we design a method based on zero-shot settings to evaluate the performance of LLMs. With human evaluation, we obtain the converted total score of LLMs, including GPT-4, ChatGPT and ERNIE-Bot.Our findings reveal that LLMs have achieved competitive scores in Chinese GAOKAO examination, while they exhibit significant performance disparities across various subjects. We also use LLMs to grade the subjective questions, and find that model scores achieve a moderate level of consistency with human scores. In conclusion, this research contributes a robust evaluation benchmark for future large language models and offers valuable insights into the advantages and limitations of such models.",
            "year": 2023,
            "citationCount": 42,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "GAOKAO-Bench is introduced, an intuitive benchmark that employs questions from the Chinese GAOKAO examination as test samples, including both subjective and objective questions that contribute a robust evaluation benchmark for future large language models and offers valuable insights into the advantages and limitations of such models."
            },
            "score": 1
        },
        {
            "id": "19829979019e9cf26e1d12532c0e1805b7c98c0b",
            "paperId": "19829979019e9cf26e1d12532c0e1805b7c98c0b",
            "title": "NLP Inspired Training Mechanics For Modeling Transient Dynamics",
            "abstract": "In recent years, Machine learning (ML) techniques developed for Natural Language Processing (NLP) have permeated into developing better computer vision algorithms. In this work, we use such NLP-inspired techniques to improve the accuracy, robustness and generalizability of ML models for simulating transient dynamics. We introduce teacher forcing and curriculum learning based training mechanics to model vortical flows and show an enhancement in accuracy for ML models, such as FNO and UNet by more than 50%.",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces teacher forcing and curriculum learning based training mechanics to model vortical flows and shows an enhancement in accuracy for ML models, such as FNO and UNet by more than 50%."
            },
            "score": 1
        },
        {
            "id": "8142834f8d7c2d3989ca3a08f228bf6e09d49d63",
            "paperId": "8142834f8d7c2d3989ca3a08f228bf6e09d49d63",
            "title": "Deep Learning for Ultrasound Speed-of-Sound Reconstruction: Impacts of Training Data Diversity on Stability and Robustness",
            "abstract": "Ultrasound b-mode imaging is a qualitative approach and diagnostic quality strongly depends on operators\u2019 training and experience. Quantitative approaches can provide information about tissue properties; therefore, can be used for identifying various tissue types, e.g., speed-of-sound in the tissue can be used as a biomarker for tissue malignancy, especially in breast imaging. Recent studies showed the possibility of speed-of-sound reconstruction using deep neural networks that are fully trained on simulated data. However, because of the ever-present domain shift between simulated and measured data, the stability and performance of these models in real setups are still under debate. In prior works, for training data generation, tissue structures were modeled as simplified geometrical structures which does not reflect the complexity of the real tissues. In this study, we proposed a new simulation setup for training data generation based on Tomosynthesis images. We combined our approach with the simplified geometrical model and investigated the impacts of training data diversity on the stability and robustness of an existing network architecture. We studied the sensitivity of the trained network to different simulation parameters, e.g., echogenicity, number of scatterers, noise, and geometry. We showed that the network trained with the joint set of data is more stable on out-of-domain simulated data as well as measured phantom data.",
            "year": 2022,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new simulation setup for training data generation based on Tomosynthesis images is proposed and it is shown that the network trained with the joint set of data is more stable on out-of-domain simulated data as well as measured phantom data."
            },
            "score": 1
        },
        {
            "id": "f66e7aea7f29794f095c6caac571f24b0d285c7c",
            "paperId": "f66e7aea7f29794f095c6caac571f24b0d285c7c",
            "title": "Critical power: Evidence\u2010based robustness",
            "abstract": "Recently, Gorostiaga et al.1 published an article entitled \u201cOver 55 years of critical power: Fact or artifact?\u201d with the objective of \u201cgenerating an evidencebased debate\u201d regarding the validity of the critical power (CP) as a physiological goldstandard parameter. The researchers critically examined the concept of the CP model under four main arguments; \u201c(1) the inconsistency in experimentally demonstrating the classical definition of CP, (2) the wide range of relative intensities at which CP was identified to occur, (3) the choice of duration of exercise trials used to assess CP, and (4) inadequate choice of the hyperbolic function model, despite the power function model, to describe the relationship between velocity and time.\u201d These arguments were originally presented using the best race time performances of the top 10 male and female Spanish runners who completed 1.5 km to 10 km distance races in the same outdoor competitive season (Year 2019). In this way, the critical speed values (CS, analogous to CP) were determined using the race times up to 5 km (CS1.5\u2013 5km, 1.5, 3, and 5 km) and 10 km (CS1.5\u2013 10km, 1.5, 3, 5, and 10 km). In addition, the researchers added the halfmarathon and marathon performance times of other athletes to the model. However, we have identified some serious limitations with the data on which their conclusions are based. Our objectives are to point out important limitations and misconceptions in the original article by Gorostiaga et al.,1 which distorted the models and conclusions, and highlight the robustness of the CP model as the lowest intensity (speed) that represents the severeintensity domain, when using a correct and standardized methodology.2 Using the values (time and distance) provided in the article, we recalculated the CS and determined the D\u2032 values (fixed and limited distance capacity above the CS) through the same model used (distance\u2013 time). CS (CS1.5\u2013 5km, 5.45 \u00b1 0.38 m/s vs. CS1.5\u2013 10km, 5.19 \u00b1 0.39 m/s; p = 0.02) and D\u2032 (CS1.5\u2013 5km, 140.6 \u00b1 60.1 m vs. CS1.5\u2013 10km, 272.2 \u00b1 110.2 m; p = 0.001) values were significantly different (paired ttest). In fact, the influence of duration / distance of predictive trials on the CP/CS parameters has been previously deomonstrated.3 Interestingly, during modeling we noticed that the average speed of four athletes (TN, MM, CC, and CV) in the 3 km race was lower than their average speed in the 5 km race. This is interesting because the hyperbolic relationship between speed and time dictates that speed should decrease with increasing distance up to the CS. Yet, in the best performance of these athletes, they ran at a faster average speed in a race with greater distance. In this case, several factors, such as \u201cpacing,\u201d may have prevented the complete depletion of the D\u2032 above the CS, thus, confounding the CS1.55km model prediction. In fact, it was not possible to predict the 3 km running performance for these athletes (i.e., TN, MM, CC, and CV) using CS1.5\u2013 5km parameters. In the 10km race, only two athletes (DP and MJ) ran at average speeds above CS1.55Km, showing that all other athletes were at intensities below CS, justifying the running time (average, 31.54 minutes). The potential overestimation of D\u2032 may explain why the actual times (Mean \u00b1 SD: 1.5 km, 247.5 \u00b1 22.8, 3 km, 533.2 \u00b1 44.0, 5 km, 893.0 \u00b1 67.4, and 10 km, 1892.3 \u00b1 144.6 s) were statistically (ttest, p = 0.05\u2013 0.0002) different from the predicted times (Mean \u00b1 SD: 299.2 \u00b1 59.26, 566,0.9 \u00b1 75.1, 652.3 \u00b1 131, and 2215.7 \u00b1 304.9 s, respectively) when used CS1.5\u2013 10km model. In this sense, the inclusion of execution times at intensities slightly below the CS (10 km, halfmarathon, and marathon) introduces errors in the mathematical modeling, which do not represent real physiological consequences on performance, possibly by reaching domains of different intensity. In addition, due to numerous influences during a competitive sporting season (e.g., time of day, environmental conditions, training status, and performance level of competing athletes),4,5 it does not seem correct and valid to use different sporting events for CS modeling, particularly paced race times.6 There is evidence that the time trial protocol, similar to the time to exhaustion protocol, is valid for determining the CP/CS model, as long as it induces exhaustion to a maximal of ~15 min, aiming both to deplete D\u2032 and to attain VO2max. 7 Indeed, several key parameters of aerobic fitness (VO2max/VO2peak, blood lactate response to exercise, running economy, and oxygen uptake kinetics) are also protocoldependent, that is, data obtained by different protocols should not be used interchangeably. Thus, it is important to emphasize that (1) Gorostiaga et al.1 used severeintensity race efforts that were performed submaximally in several cases (40% of athletes in the 3 km race); (2) 80% of the athletes were below the CS in the 10 km race (according to the model with 3 distances);",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The researchers highlight the robustness of the CP model as the lowest intensity (speed) that represents the severeintensity domain, when using a correct and standardized methodology, and identify some serious limitations with the data on which their conclusions are based."
            },
            "score": 1
        },
        {
            "id": "20c1d506ea675c2036f21c606bc2df7c837bb47b",
            "paperId": "20c1d506ea675c2036f21c606bc2df7c837bb47b",
            "title": "Merit-based Fusion of NLP Techniques for Instant Feedback on Water Quality from Twitter Text",
            "abstract": "This paper focuses on an important environmental challenge; namely, water quality by analyzing the potential of social media as an immediate source of feedback. The main goal of the work is to automatically analyze and retrieve social media posts relevant to water quality with particular attention to posts describing di \ufb00 erent aspects of water quality, such as watercolor, smell, taste, and related illnesses. To this aim, we propose a novel framework incorporating di \ufb00 erent preprocessing, data augmentation, and classi\ufb01-cation techniques. In total, three di \ufb00 erent Neural Networks (NNs) architectures, namely (i) Bidirectional Encoder Representations from Transformers (BERT), (ii) Robustly Optimized BERT Pre-training Approach (XLM-RoBERTa), and (iii) custom Long short-term memory (LSTM) model, are employed in a merit-based fusion scheme. For merit-based weight assignment to the models, several optimization and search techniques are compared including a Particle Swarm Optimization (PSO), a Genetic Algorithm (GA), Brute Force (BF), Nelder-Mead, and Powell\u2019s optimization methods. We also provide an evaluation of the individual models where the highest F1-score of 0.81 is obtained with the BERT model. In merit-based fusion, overall better results are obtained with BF achieving an F1-score score of 0.852. We also provide comparison against existing methods, where a signi\ufb01cant improvement for our proposed solutions is obtained. We believe such rigorous analysis of this relatively new topic will provide a baseline for future research.",
            "year": 2022,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The main goal of the work is to automatically analyze and retrieve social media posts relevant to water quality with particular attention to posts describing di \ufb00 erent aspects of water quality, such as watercolor, smell, taste, and related illnesses."
            },
            "score": 1
        }
    ],
    "novelty": "yes"
}