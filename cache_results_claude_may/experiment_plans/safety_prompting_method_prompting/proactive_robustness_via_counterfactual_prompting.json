{
    "topic_description": "novel prompting methods to improve large language models' robustness against adversarial attacks and improve their security and privacy",
    "idea_name": "Proactive Robustness via Counterfactual Prompting",
    "raw_idea": {
        "Problem": "Existing defenses against adversarial attacks on language models are mostly reactive, i.e., they detect and filter adversarial inputs. However, proactive methods that can inherently improve the model's robustness are less explored.",
        "Existing Methods": "Current proactive defenses mainly focus on adversarial training, which can be expensive and may overfit to specific types of attacks.",
        "Motivation": "Humans can often proactively reason about potential vulnerabilities and protect against them by considering counterfactual scenarios. We hypothesize that prompting language models to proactively analyze and fix their own vulnerabilities via counterfactual reasoning can improve their intrinsic robustness.",
        "Proposed Method": "We propose Proactive Robustness via Counterfactual Prompting (PRCP), a novel self-supervised learning framework for proactively improving language models' intrinsic robustness. Given a language model and a corpus of text, PRCP first prompts the model to generate a set of potential adversarial attacks that could fool itself. Then, for each generated adversarial prompt, PRCP prompts the model to analyze its own potential vulnerabilities and generate a counterfactual prompt that can guide itself to produce a safe output. Finally, PRCP uses the generated counterfactual prompts to fine-tune the model via a consistency loss, which encourages the model to produce similar outputs for the original and counterfactual prompts. By iteratively generating adversarial and counterfactual prompts, PRCP enables the model to proactively learn to be more robust.",
        "Experiment Plan": "We will evaluate PRCP on a range of language models and datasets, and compare it with baseline defenses such as adversarial training and reactive filtering. We will measure the model's robustness against both seen and unseen types of adversarial attacks, as well as its performance on benign inputs. We will also analyze the quality and diversity of the generated adversarial and counterfactual prompts, and study the effect of different prompt generation strategies and fine-tuning objectives."
    },
    "full_experiment_plan": {
        "Title": "Proactive Robustness via Counterfactual Prompting: Improving Language Models' Intrinsic Robustness against Adversarial Attacks",
        "Problem Statement": "Existing defenses against adversarial attacks on language models are mostly reactive, i.e., they detect and filter adversarial inputs. However, proactive methods that can inherently improve the model's robustness are less explored.",
        "Motivation": "Current proactive defenses mainly focus on adversarial training, which can be expensive and may overfit to specific types of attacks. Humans can often proactively reason about potential vulnerabilities and protect against them by considering counterfactual scenarios. We hypothesize that prompting language models to proactively analyze and fix their own vulnerabilities via counterfactual reasoning can improve their intrinsic robustness.",
        "Proposed Method": "We propose Proactive Robustness via Counterfactual Prompting (PRCP), a novel self-supervised learning framework for proactively improving language models' intrinsic robustness. Given a language model and a corpus of text, PRCP first prompts the model to generate a set of potential adversarial attacks that could fool itself. Then, for each generated adversarial prompt, PRCP prompts the model to analyze its own potential vulnerabilities and generate a counterfactual prompt that can guide itself to produce a safe output. Finally, PRCP uses the generated counterfactual prompts to fine-tune the model via a consistency loss, which encourages the model to produce similar outputs for the original and counterfactual prompts. By iteratively generating adversarial and counterfactual prompts, PRCP enables the model to proactively learn to be more robust.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We will use a range of datasets that are commonly used to evaluate adversarial robustness of language models, such as ANLI, Adversarial SQuAD, Adversarial SNLI, and Dynabench. These datasets contain both benign and adversarial examples across different tasks like natural language inference, question answering, and sentiment analysis. We will use accuracy and adversarial accuracy as the main evaluation metrics.",
            "Step 2: Construct Prompts": "For each dataset, we will construct a set of prompts for generating adversarial examples and counterfactual prompts. The adversarial prompt will ask the model to generate an input that could potentially fool itself, e.g., \"Generate a question that the model is likely to answer incorrectly.\" The counterfactual prompt will ask the model to analyze its own vulnerability and generate a prompt that can guide itself to produce a safe output, e.g., \"What kind of question could the model be vulnerable to? Generate a question that the model should be able to answer correctly.\"",
            "Step 3: Select Models": "We will experiment with a range of language models of different sizes and architectures, such as BERT, RoBERTa, T5, and GPT-3. This will help us understand how PRCP performs across different models.",
            "Step 4: Implement PRCP": "We will implement the PRCP framework as follows:\n1. For each training example, use the adversarial prompt to generate a set of potential adversarial examples.\n2. For each generated adversarial example, use the counterfactual prompt to generate a counterfactual example.\n3. Fine-tune the model using a consistency loss that encourages the model to produce similar outputs for the original and counterfactual examples.\n4. Repeat steps 1-3 for multiple iterations.",
            "Step 5: Evaluate Results": "We will evaluate the models on the test sets of the datasets, using both benign and adversarial examples. We will compare the accuracy and adversarial accuracy of the models before and after applying PRCP. We will also compare PRCP with baseline defenses such as adversarial training and reactive filtering. Additionally, we will analyze the quality and diversity of the generated adversarial and counterfactual examples."
        },
        "Test Case Examples": {
            "Baseline Prompt Input (Adversarial Example)": "Passage: John and Mary went to the grocery store to buy some fruits. They bought apples, bananas, and oranges. Question: What did John and Mary buy at the hardware store?",
            "Baseline Prompt Expected Output (Adversarial Example)": "John and Mary bought hammers, nails, and screwdrivers at the hardware store.",
            "Proposed Prompt Input (PRCP - Adversarial Prompt)": "Generate a question that the model is likely to answer incorrectly based on the given passage. Passage: John and Mary went to the grocery store to buy some fruits. They bought apples, bananas, and oranges.",
            "Proposed Prompt Expected Output (PRCP - Adversarial Prompt)": "Question: What did John and Mary buy at the hardware store?",
            "Proposed Prompt Input (PRCP - Counterfactual Prompt)": "What kind of question could the model be vulnerable to based on the given passage? Generate a question that the model should be able to answer correctly. Passage: John and Mary went to the grocery store to buy some fruits. They bought apples, bananas, and oranges.",
            "Proposed Prompt Expected Output (PRCP - Counterfactual Prompt)": "Question: What fruits did John and Mary buy at the grocery store?",
            "Explanation": "The adversarial example tries to fool the model by asking about a location (hardware store) that is not mentioned in the passage, while the counterfactual example asks a question that is directly answerable from the passage. By training the model to produce consistent outputs for such adversarial and counterfactual pairs, PRCP can improve the model's robustness against adversarial attacks that try to take advantage of the model's lack of grounding in the input passage."
        },
        "Fallback Plan": "If PRCP does not significantly improve the model's adversarial robustness, we can conduct additional analyses to understand why. For example, we can analyze the quality and diversity of the generated adversarial and counterfactual examples to see if they are sufficiently challenging and diverse. We can also experiment with different prompt formats and fine-tuning objectives to see if they make a difference. If PRCP still does not work well, we can turn the project into an analysis of why proactive defenses based on counterfactual reasoning are challenging, and what are the potential limitations and future directions. This can provide valuable insights for the community working on improving the robustness of language models."
    }
}