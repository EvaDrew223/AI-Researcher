{
    "topic_description": "novel prompting methods to improve large language models' robustness against adversarial attacks and improve their security and privacy",
    "idea_name": "Proactive Robustness via Counterfactual Prompting",
    "raw_idea": {
        "Problem": "Existing defenses against adversarial attacks on language models are mostly reactive, i.e., they detect and filter adversarial inputs. However, proactive methods that can inherently improve the model's robustness are less explored.",
        "Existing Methods": "Current proactive defenses mainly focus on adversarial training, which can be expensive and may overfit to specific types of attacks.",
        "Motivation": "Humans can often proactively reason about potential vulnerabilities and protect against them by considering counterfactual scenarios. We hypothesize that prompting language models to proactively analyze and fix their own vulnerabilities via counterfactual reasoning can improve their intrinsic robustness.",
        "Proposed Method": "We propose Proactive Robustness via Counterfactual Prompting (PRCP), a novel self-supervised learning framework for proactively improving language models' intrinsic robustness. Given a language model and a corpus of text, PRCP first prompts the model to generate a set of potential adversarial attacks that could fool itself. Then, for each generated adversarial prompt, PRCP prompts the model to analyze its own potential vulnerabilities and generate a counterfactual prompt that can guide itself to produce a safe output. Finally, PRCP uses the generated counterfactual prompts to fine-tune the model via a consistency loss, which encourages the model to produce similar outputs for the original and counterfactual prompts. By iteratively generating adversarial and counterfactual prompts, PRCP enables the model to proactively learn to be more robust.",
        "Experiment Plan": "We will evaluate PRCP on a range of language models and datasets, and compare it with baseline defenses such as adversarial training and reactive filtering. We will measure the model's robustness against both seen and unseen types of adversarial attacks, as well as its performance on benign inputs. We will also analyze the quality and diversity of the generated adversarial and counterfactual prompts, and study the effect of different prompt generation strategies and fine-tuning objectives."
    },
    "full_experiment_plan": {
        "Title": "Proactive Robustness via Counterfactual Prompting: Improving Language Models' Intrinsic Robustness against Adversarial Attacks",
        "Problem Statement": "Existing defenses against adversarial attacks on language models are mostly reactive, i.e., they detect and filter adversarial inputs. However, proactive methods that can inherently improve the model's robustness are less explored.",
        "Motivation": "Current proactive defenses mainly focus on adversarial training, which can be expensive and may overfit to specific types of attacks. Humans can often proactively reason about potential vulnerabilities and protect against them by considering counterfactual scenarios. We hypothesize that prompting language models to proactively analyze and fix their own vulnerabilities via counterfactual reasoning can improve their intrinsic robustness.",
        "Proposed Method": "We propose Proactive Robustness via Counterfactual Prompting (PRCP), a novel self-supervised learning framework for proactively improving language models' intrinsic robustness. Given a language model and a corpus of text, PRCP first prompts the model to generate a set of potential adversarial attacks that could fool itself. Then, for each generated adversarial prompt, PRCP prompts the model to analyze its own potential vulnerabilities and generate a counterfactual prompt that can guide itself to produce a safe output. Finally, PRCP uses the generated counterfactual prompts to fine-tune the model via a consistency loss, which encourages the model to produce similar outputs for the original and counterfactual prompts. By iteratively generating adversarial and counterfactual prompts, PRCP enables the model to proactively learn to be more robust.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We will use a range of datasets that are commonly used to evaluate adversarial robustness of language models, such as ANLI, Adversarial SQuAD, Adversarial SNLI, and Dynabench. These datasets contain both benign and adversarial examples across different tasks like natural language inference, question answering, and sentiment analysis. We will use accuracy and adversarial accuracy as the main evaluation metrics.",
            "Step 2: Construct Prompts": "For each dataset, we will construct a set of prompts for generating adversarial examples and counterfactual prompts. The adversarial prompt will ask the model to generate an input that could potentially fool itself, e.g., \"Generate a question that the model is likely to answer incorrectly.\" The counterfactual prompt will ask the model to analyze its own vulnerability and generate a prompt that can guide itself to produce a safe output, e.g., \"What kind of question could the model be vulnerable to? Generate a question that the model should be able to answer correctly.\"",
            "Step 3: Select Models": "We will experiment with a range of language models of different sizes and architectures, such as BERT, RoBERTa, T5, and GPT-3. This will help us understand how PRCP performs across different models.",
            "Step 4: Implement PRCP": "We will implement the PRCP framework as follows:\n1. For each training example, use the adversarial prompt to generate a set of potential adversarial examples.\n2. For each generated adversarial example, use the counterfactual prompt to generate a counterfactual example.\n3. Fine-tune the model using a consistency loss that encourages the model to produce similar outputs for the original and counterfactual examples.\n4. Repeat steps 1-3 for multiple iterations.",
            "Step 5: Evaluate Results": "We will evaluate the models on the test sets of the datasets, using both benign and adversarial examples. We will compare the accuracy and adversarial accuracy of the models before and after applying PRCP. We will also compare PRCP with baseline defenses such as adversarial training and reactive filtering. Additionally, we will analyze the quality and diversity of the generated adversarial and counterfactual examples."
        },
        "Test Case Examples": {
            "Baseline Prompt Input (Adversarial Example)": "Passage: John and Mary went to the grocery store to buy some fruits. They bought apples, bananas, and oranges. Question: What did John and Mary buy at the hardware store?",
            "Baseline Prompt Expected Output (Adversarial Example)": "John and Mary bought hammers, nails, and screwdrivers at the hardware store.",
            "Proposed Prompt Input (PRCP - Adversarial Prompt)": "Generate a question that the model is likely to answer incorrectly based on the given passage. Passage: John and Mary went to the grocery store to buy some fruits. They bought apples, bananas, and oranges.",
            "Proposed Prompt Expected Output (PRCP - Adversarial Prompt)": "Question: What did John and Mary buy at the hardware store?",
            "Proposed Prompt Input (PRCP - Counterfactual Prompt)": "What kind of question could the model be vulnerable to based on the given passage? Generate a question that the model should be able to answer correctly. Passage: John and Mary went to the grocery store to buy some fruits. They bought apples, bananas, and oranges.",
            "Proposed Prompt Expected Output (PRCP - Counterfactual Prompt)": "Question: What fruits did John and Mary buy at the grocery store?",
            "Explanation": "The adversarial example tries to fool the model by asking about a location (hardware store) that is not mentioned in the passage, while the counterfactual example asks a question that is directly answerable from the passage. By training the model to produce consistent outputs for such adversarial and counterfactual pairs, PRCP can improve the model's robustness against adversarial attacks that try to take advantage of the model's lack of grounding in the input passage."
        },
        "Fallback Plan": "If PRCP does not significantly improve the model's adversarial robustness, we can conduct additional analyses to understand why. For example, we can analyze the quality and diversity of the generated adversarial and counterfactual examples to see if they are sufficiently challenging and diverse. We can also experiment with different prompt formats and fine-tuning objectives to see if they make a difference. If PRCP still does not work well, we can turn the project into an analysis of why proactive defenses based on counterfactual reasoning are challenging, and what are the potential limitations and future directions. This can provide valuable insights for the community working on improving the robustness of language models."
    },
    "novelty_queries": [
        "KeywordQuery(\"language models counterfactual prompting robustness\")",
        "KeywordQuery(\"language models adversarial attacks proactive defense\")",
        "KeywordQuery(\"language models self-supervised consistency training robustness\")",
        "KeywordQuery(\"Proactive Robustness via Counterfactual Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "c3287da21ddf6f6c87b7b328996c1cf3fec92942",
            "paperId": "c3287da21ddf6f6c87b7b328996c1cf3fec92942",
            "title": "Prompting Large Language Models for Counterfactual Generation: An Empirical Study",
            "abstract": "Large language models (LLMs) have made remarkable progress in a wide range of natural language understanding and generation tasks. However, their ability to generate counterfactuals has not been examined systematically. To bridge this gap, we present a comprehensive evaluation framework on various types of NLU tasks, which covers all key factors in determining LLMs' capability of generating counterfactuals. Based on this framework, we 1) investigate the strengths and weaknesses of LLMs as the counterfactual generator, and 2) disclose the factors that affect LLMs when generating counterfactuals, including both the intrinsic properties of LLMs and prompt designing. The results show that, though LLMs are promising in most cases, they face challenges in complex tasks like RE since they are bounded by task-specific performance, entity constraints, and inherent selection bias. We also find that alignment techniques, e.g., instruction-tuning and reinforcement learning from human feedback, may potentially enhance the counterfactual generation ability of LLMs. On the contrary, simply increasing the parameter size does not yield the desired improvements. Besides, from the perspective of prompt designing, task guidelines unsurprisingly play an important role. However, the chain-of-thought approach does not always help due to inconsistency issues.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A comprehensive evaluation framework on various types of NLU tasks is presented, which covers all key factors in determining LLMs' capability of generating counterfactuals and finds that alignment techniques, e.g., instruction-tuning and reinforcement learning from human feedback, may potentially enhance the counterfactual generation ability of LLMs."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving language models' intrinsic robustness against adversarial attacks, and the proposed approach is to use counterfactual prompting to enable the model to proactively learn to be more robust.\n\nThe research problem in the paper is evaluating large language models' ability to generate counterfactuals, and the approach is to present a comprehensive evaluation framework on various types of NLU tasks and investigate the factors that affect LLMs when generating counterfactuals.\n\nWhile both the proposal and the paper involve counterfactual generation, the main research problems are different. The proposal focuses on improving robustness against adversarial attacks, while the paper focuses on evaluating the counterfactual generation ability itself.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "0db108da7811200b25f979d659269812d0b52b61",
            "paperId": "0db108da7811200b25f979d659269812d0b52b61",
            "title": "Counterfactual Adversarial Training for Improving Robustness of Pre-trained Language Models",
            "abstract": "One of the approaches for improving the robustness of NLP models is adversarial training by adversarial examples. However, in previous work on adversarial training, the adversarial examples were not guaranteed to be minimally edited and to change the model\u2019s prediction. Our hypothesis is adversarial training could make models more robust if the adversarial examples were guaranteed to be minimally edited and to change the model\u2019s prediction. We propose Counterfactual Adversarial Training (CAT), which uses counterfactual explanations to improve the robustness of the model. Our experiments on Natural Language Inference and Sentiment Analysis show that CAT significantly enhances out-of-the-box pre-trained NLP models on 11 datasets, indicating that CAT is a promising approach to improve the robustness of the pre-trained language models.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experiments on Natural Language Inference and Sentiment Analysis show that CAT significantly enhances out-of-the-box pre-trained NLP models on 11 datasets, indicating that CAT is a promising approach to improve the robustness of the pre-trained language models."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve language models' intrinsic robustness against adversarial attacks by using counterfactual prompting to proactively generate adversarial and counterfactual examples for fine-tuning the model.\n\nThe paper proposes Counterfactual Adversarial Training (CAT) to improve the robustness of pre-trained language models by using counterfactual explanations to generate adversarial examples for training.\n\nWhile both the project proposal and the paper use counterfactual examples for improving robustness, the project proposal focuses on proactive generation of adversarial and counterfactual examples via prompting, whereas the paper uses counterfactual explanations to generate adversarial examples for training.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "de2fd685f45ee916b9142bcb983d306b7da643a4",
            "paperId": "de2fd685f45ee916b9142bcb983d306b7da643a4",
            "title": "A Prompting-based Approach for Adversarial Example Generation and Robustness Enhancement",
            "abstract": "Recent years have seen the wide application of NLP models in crucial areas such as finance, medical treatment, and news media, raising concerns of the model robustness and vulnerabilities. In this paper, we propose a novel prompt-based adversarial attack to compromise NLP models and robustness enhancement technique. We first construct malicious prompts for each instance and generate adversarial examples via mask-and-filling under the effect of a malicious purpose. Our attack technique targets the inherent vulnerabilities of NLP models, allowing us to generate samples even without interacting with the victim NLP model, as long as it is based on pre-trained language models (PLMs). Furthermore, we design a prompt-based adversarial training method to improve the robustness of PLMs. As our training method does not actually generate adversarial samples, it can be applied to large-scale training sets efficiently. The experimental results show that our attack method can achieve a high attack success rate with more diverse, fluent and natural adversarial examples. In addition, our robustness enhancement method can significantly improve the robustness of models to resist adversarial attacks. Our work indicates that prompting paradigm has great potential in probing some fundamental flaws of PLMs and fine-tuning them for downstream tasks.",
            "year": 2022,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel prompt-based adversarial attack to compromise NLP models and robustness enhancement technique that can significantly improve the robustness of models to resist adversarial attacks and indicates that prompting paradigm has great potential in probing some fundamental flaws of PLMs and fine-tuning them for downstream tasks."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve language models' intrinsic robustness against adversarial attacks by prompting the models to proactively analyze and fix their own vulnerabilities via counterfactual reasoning. The paper proposes a prompt-based adversarial attack to compromise NLP models and a prompt-based adversarial training method to improve the robustness of pre-trained language models.\n\nWhile both the project proposal and the paper focus on adversarial attacks and robustness enhancement for language models, their approaches differ. The project proposal uses counterfactual prompting to encourage the model to generate adversarial and counterfactual examples for self-improvement, while the paper uses malicious prompts to generate adversarial examples and prompt-based adversarial training without actually generating adversarial samples.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "2c72ab10e7a5f2fd32e6f85b20c77bf64e6e220d",
            "paperId": "2c72ab10e7a5f2fd32e6f85b20c77bf64e6e220d",
            "title": "A prompt-based approach to adversarial example generation and robustness enhancement",
            "abstract": null,
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel robust training approach based on prompt paradigm which incorporates prompt texts as the alternatives to adversarial examples and enhances robustness under a lightweight minimax-style optimization framework is proposed."
            },
            "score": 7,
            "novelty_score": "Research Problem in Proposal: Improving language models' intrinsic robustness against adversarial attacks using proactive methods.\nProposed Approach in Proposal: Proactive Robustness via Counterfactual Prompting (PRCP), a self-supervised learning framework that prompts the model to generate adversarial attacks and counterfactual prompts to guide itself to produce safe outputs.\n\nResearch Problem in Paper: Enhancing robustness of language models against adversarial examples.\nProposed Approach in Paper: A robust training approach based on prompt paradigm which uses prompt texts as alternatives to adversarial examples under a minimax-style optimization framework.\n\nBoth the proposal and the paper aim to improve the robustness of language models against adversarial attacks. However, the proposal focuses on a proactive approach using counterfactual prompting, while the paper proposes a prompt-based robust training approach using prompt texts as alternatives to adversarial examples. Although they share the high-level goal, the specific approaches differ.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "66cca7a055ee9278d0439654f474cbfa4135aff0",
            "paperId": "66cca7a055ee9278d0439654f474cbfa4135aff0",
            "title": "Why So Gullible? Enhancing the Robustness of Retrieval-Augmented Models against Counterfactual Noise",
            "abstract": "Most existing retrieval-augmented language models (LMs) assume a naive dichotomy within a retrieved document set: query-relevance and irrelevance. Our work investigates a more challenging scenario in which even the\"relevant\"documents may contain misleading or incorrect information, causing conflict among the retrieved documents and thereby negatively influencing model decisions as noise. We observe that existing LMs are highly brittle to the presence of conflicting information in both the fine-tuning and in-context few-shot learning scenarios. We propose approaches for handling knowledge conflicts among retrieved documents by explicitly fine-tuning a discriminator or prompting GPT-3.5 to elicit its discriminative capability. Our empirical results on open-domain QA show that these approaches significantly enhance model robustness. We also provide our findings on incorporating the fine-tuned discriminator's decision into the in-context learning process, proposing a way to exploit the benefits of two disparate learning schemes. Alongside our findings, we provide MacNoise, a machine-generated, conflict-induced dataset to further encourage research in this direction.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes approaches for handling knowledge conflicts among retrieved documents by explicitly fine-tuning a discriminator or prompting GPT-3.5 to elicit its discriminative capability, and shows that these approaches significantly enhance model robustness."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve language models' intrinsic robustness against adversarial attacks by prompting them to proactively analyze and fix their own vulnerabilities via counterfactual reasoning. The paper focuses on enhancing the robustness of retrieval-augmented models against counterfactual noise in retrieved documents by explicitly fine-tuning a discriminator or prompting GPT-3.5 to elicit its discriminative capability.\n\nWhile both the project proposal and the paper aim to improve the robustness of language models, the project proposal focuses on intrinsic robustness against adversarial attacks, while the paper addresses the issue of conflicting information in retrieved documents for retrieval-augmented models. The approaches also differ, with the project proposal using counterfactual prompting and the paper employing discriminator fine-tuning or prompting GPT-3.5.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "12c826f4195da172b212a529f8fcf10cc79e35da",
            "paperId": "12c826f4195da172b212a529f8fcf10cc79e35da",
            "title": "Context-faithful Prompting for Large Language Models",
            "abstract": "Large language models (LLMs) encode parametric knowledge about world facts and have shown remarkable performance in knowledge-driven NLP tasks. However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks). In this paper, we seek to assess and enhance LLMs' contextual faithfulness in two aspects: knowledge conflict and prediction with abstention. We demonstrate that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies. In particular, we identify opinion-based prompts and counterfactual demonstrations as the most effective methods. Opinion-based prompts reframe the context as a narrator's statement and inquire about the narrator's opinions, while counterfactual demonstrations use instances containing false facts to improve faithfulness in knowledge conflict situations. Neither technique requires additional training. We conduct experiments on three datasets of two standard NLP tasks, machine reading comprehension and relation extraction, and the results demonstrate significant improvement in faithfulness to contexts. Code and data are released at https://github.com/wzhouad/context-faithful-llm.",
            "year": 2023,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies, and opinion-based prompts and counterfactual demonstrations are identified as the most effective methods."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve language models' intrinsic robustness against adversarial attacks by prompting the models to proactively analyze and fix their own vulnerabilities via counterfactual reasoning. The paper seeks to assess and enhance language models' contextual faithfulness in knowledge conflict and prediction with abstention situations using prompting strategies such as opinion-based prompts and counterfactual demonstrations.\n\nWhile both the project proposal and the paper involve using counterfactual examples to improve certain aspects of language models, their main research problems and approaches are different. The project focuses on adversarial robustness, while the paper focuses on contextual faithfulness. The project aims to use counterfactual prompts to guide the model to generate safe outputs, while the paper uses counterfactual demonstrations to improve faithfulness in knowledge conflict situations.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "614d51530e8d75e5a916778fe0b513aa53721daf",
            "paperId": "614d51530e8d75e5a916778fe0b513aa53721daf",
            "title": "Enhancing Large Language Models Against Inductive Instructions with Dual-critique Prompting",
            "abstract": "Numerous works are proposed to align large language models (LLMs) with human intents to better fulfill instructions, ensuring they are trustful and helpful. Nevertheless, some human instructions are often malicious or misleading and following them will lead to untruthful and unsafe responses. Previous work rarely focused on understanding how LLMs manage instructions based on counterfactual premises, referred to here as \\textit{inductive instructions}, which may stem from users' false beliefs or malicious intents. In this paper, we aim to reveal the behaviors of LLMs towards \\textit{inductive instructions} and enhance their truthfulness and helpfulness accordingly. Specifically, we first introduce a benchmark of \\underline{\\textbf{Indu}}ctive {In\\underline{\\textbf{st}}ruct}ions (\\textsc{\\textbf{INDust}}), where the false knowledge is incorporated into instructions in multiple different styles. After extensive human and automatic evaluations, we uncovered a universal vulnerability among LLMs in processing inductive instructions. Additionally, we identified that different inductive styles affect the models' ability to identify the same underlying errors, and the complexity of the underlying assumptions also influences the model's performance. Motivated by these results, we propose \\textsc{Dual-critique} prompting to improve LLM robustness against inductive instructions. Our experiments demonstrate that \\textsc{Dual-critique} prompting significantly bolsters the robustness of a diverse array of LLMs, even when confronted with varying degrees of inductive instruction complexity and differing inductive styles.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that Dual-critique prompting significantly bolsters the robustness of a diverse array of LLMs, even when confronted with varying degrees of inductive instruction complexity and differing inductive styles."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve language models' intrinsic robustness against adversarial attacks by prompting them to proactively analyze and fix their own vulnerabilities via counterfactual reasoning.\n\nThe paper focuses on enhancing large language models' truthfulness and helpfulness when responding to inductive instructions that contain false knowledge or misleading information.\n\nWhile both works aim to improve the robustness of language models, the project proposal targets adversarial attacks, while the paper addresses inductive instructions with false premises. The approaches also differ, with the project using counterfactual prompting and the paper employing dual-critique prompting.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "d53945d4afb4528590d79e20de52883d29037e86",
            "paperId": "d53945d4afb4528590d79e20de52883d29037e86",
            "title": "FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo Embeddings",
            "abstract": "Logo embedding plays a crucial role in various e-commerce applications by facilitating image retrieval or recognition, such as intellectual property protection and product search. However, current methods treat logo embedding as a purely visual problem, which may limit their performance in real-world scenarios. A notable issue is that the textual knowledge embedded in logo images has not been adequately explored. Therefore, we propose a novel approach that leverages textual knowledge as an auxiliary to improve the robustness of logo embedding. The emerging Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in both visual and textual understanding and could become valuable visual assistants in understanding logo images. Inspired by this observation, our proposed method, FashionLOGO, aims to utilize MLLMs to enhance fashion logo embedding. We explore how MLLMs can improve logo embedding by prompting them to generate explicit textual knowledge through three types of prompts, including image OCR, brief captions, and detailed descriptions prompts, in a zero-shot setting. We adopt a cross-attention transformer to enable image embedding queries to learn supplementary knowledge from textual embeddings automatically. To reduce computational costs, we only use the image embedding model in the inference stage, similar to traditional inference pipelines. Our extensive experiments on three real-world datasets demonstrate that FashionLOGO learns generalized and robust logo embeddings, achieving state-of-the-art performance in all benchmark datasets. Furthermore, we conduct comprehensive ablation studies to demonstrate the performance improvements resulting from the introduction of MLLMs.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores how MLLMs can improve logo embedding by prompting them to generate explicit textual knowledge through three types of prompts, including image OCR, brief captions, and detailed descriptions prompts, in a zero-shot setting and adopt a cross-attention transformer to enable image embedding queries to learn supplementary knowledge from textual embeddings automatically."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve language models' intrinsic robustness against adversarial attacks by using counterfactual prompting to proactively analyze and fix vulnerabilities. The paper focuses on improving fashion logo embedding by leveraging textual knowledge from multimodal large language models.\n\nThe project is about adversarial robustness of language models, while the paper is about logo embedding in the fashion domain. They tackle different problems and propose different approaches.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "b0bac6aca93021105c8a4f165184a097a249fbce",
            "paperId": "b0bac6aca93021105c8a4f165184a097a249fbce",
            "title": "Evaluating the Zero-shot Robustness of Instruction-tuned Language Models",
            "abstract": "Instruction fine-tuning has recently emerged as a promising approach for improving the zero-shot capabilities of Large Language Models (LLMs) on new tasks. This technique has shown particular strength in improving the performance of modestly sized LLMs, sometimes inducing performance competitive with much larger model variants. In this paper we ask two questions: (1) How sensitive are instruction-tuned models to the particular phrasings of instructions, and, (2) How can we make them more robust to such natural language variation? To answer the former, we collect a set of 319 instructions manually written by NLP practitioners for over 80 unique tasks included in widely used benchmarks, and we evaluate the variance and average performance of these instructions as compared to instruction phrasings observed during instruction fine-tuning. We find that using novel (unobserved) but appropriate instruction phrasings consistently degrades model performance, sometimes substantially so. Further, such natural instructions yield a wide variance in downstream performance, despite their semantic equivalence. Put another way, instruction-tuned models are not especially robust to instruction re-phrasings. We propose a simple method to mitigate this issue by introducing ``soft prompt'' embedding parameters and optimizing these to maximize the similarity between representations of semantically equivalent instructions. We show that this method consistently improves the robustness of instruction-tuned models.",
            "year": 2023,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A simple method is proposed by introducing ``soft prompt'' embedding parameters and optimizing these to maximize the similarity between representations of semantically equivalent instructions, and it is shown that this method consistently improves the robustness of instruction-tuned models."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving language models' intrinsic robustness against adversarial attacks, and the proposed approach is using counterfactual prompting to proactively generate adversarial and counterfactual examples for fine-tuning the model.\n\nThe research problem in the paper is evaluating and improving the robustness of instruction-tuned language models to different phrasings of instructions, and the proposed approach is using soft prompt embedding parameters to maximize the similarity between representations of semantically equivalent instructions.\n\nThe two works have different research problems and approaches. The proposal focuses on adversarial robustness, while the paper focuses on robustness to instruction rephrasing. The proposal uses counterfactual prompting, while the paper uses soft prompt embeddings.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "fb8d0982d76945e136836a57e7a23907c21c2fb2",
            "paperId": "fb8d0982d76945e136836a57e7a23907c21c2fb2",
            "title": "Relation-based Counterfactual Data Augmentation and Contrastive Learning for Robustifying Natural Language Inference Models",
            "abstract": "Although pre-trained language models show good performance on various natural language processing tasks, they often rely on non-causal features and patterns to determine the outcome. For natural language inference tasks, previous results have shown that even a model trained on a large number of data fails to perform well on counterfactually revised data, indicating that the model is not robustly learning the semantics of the classes. In this paper, we propose a method in which we use token-based and sentence-based augmentation methods to generate counter-factual sentence pairs that belong to each class, and apply contrastive learning to help the model learn the difference between sentence pairs of different classes with similar contexts. Evaluation results with counterfactually-revised dataset and general NLI datasets show that the proposed method can improve the performance and robustness of the NLI model.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A method is proposed in which token-based and sentence-based augmentation methods are used to generate counter-factual sentence pairs that belong to each class, and contrastive learning is applied to help the model learn the difference between sentence pairs of different classes with similar contexts."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve language models' intrinsic robustness against adversarial attacks by using counterfactual prompting to proactively analyze and fix vulnerabilities. The paper focuses on improving the robustness of natural language inference models by generating counterfactual sentence pairs and applying contrastive learning.\n\nWhile both the project proposal and the paper use counterfactual data augmentation to improve robustness, the project proposal targets the intrinsic robustness of language models against adversarial attacks, while the paper specifically focuses on improving the robustness of natural language inference models.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "9028fd54ecbbd58da6f3d86629b48bb95405fff2",
            "paperId": "9028fd54ecbbd58da6f3d86629b48bb95405fff2",
            "title": "Counterfactual reasoning: Testing language models\u2019 understanding of hypothetical scenarios",
            "abstract": "Current pre-trained language models have enabled remarkable improvements in downstream tasks, but it remains difficult to distinguish effects of statistical correlation from more systematic logical reasoning grounded on the understanding of real world. We tease these factors apart by leveraging counterfactual conditionals, which force language models to predict unusual consequences based on hypothetical propositions. We introduce a set of tests from psycholinguistic experiments, as well as larger-scale controlled datasets, to probe counterfactual predictions from five pre-trained language models. We find that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge\u2014however, we also find that for most models this effect appears largely to be driven by simple lexical cues. When we mitigate effects of both world knowledge and lexical cues to test knowledge of linguistic nuances of counterfactuals, we find that only GPT-3 shows sensitivity to these nuances, though this sensitivity is also non-trivially impacted by lexical associative factors.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge\u2014however, it is also found that for most models this effect appears largely to be driven by simple lexical cues."
            },
            "score": 6
        },
        {
            "id": "3e30a7ac4886b28eb50151f58e14a1d698cccd0e",
            "paperId": "3e30a7ac4886b28eb50151f58e14a1d698cccd0e",
            "title": "Baseline Defenses for Adversarial Attacks Against Aligned Language Models",
            "abstract": "As Large Language Models quickly become ubiquitous, it becomes critical to understand their security vulnerabilities. Recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. Drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: What threat models are practically useful in this domain? How do baseline defense techniques perform in this new domain? How does LLM security differ from computer vision? We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, discussing the various settings in which each is feasible and effective. Particularly, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. We discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. We find that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for LLMs. Future research will be needed to uncover whether more powerful optimizers can be developed, or whether the strength of filtering and preprocessing defenses is greater in the LLMs domain than it has been in computer vision.",
            "year": 2023,
            "citationCount": 97,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for LLMs."
            },
            "score": 6
        },
        {
            "id": "e8b3b37c0d301ea41c75765f6ceb7fcbb2e088a4",
            "paperId": "e8b3b37c0d301ea41c75765f6ceb7fcbb2e088a4",
            "title": "AutoDAN: Automatic and Interpretable Adversarial Attacks on Large Language Models",
            "abstract": "Safety alignment of Large Language Models (LLMs) can be compromised with manual jailbreak attacks and (automatic) adversarial attacks. Recent work suggests that patching LLMs against these attacks is possible: manual jailbreak attacks are human-readable but often limited and public, making them easy to block; adversarial attacks generate gibberish prompts that can be detected using perplexity-based filters. In this paper, we show that these solutions may be too optimistic. We propose an interpretable adversarial attack, AutoDAN , that combines the strengths of both types of attacks. It automatically generates attack prompts that bypass perplexity-based filters while maintaining a high attack success rate like manual jailbreak attacks. These prompts are interpretable and diverse, exhibiting strategies commonly used in manual jailbreak attacks, and transfer better than their non-readable counterparts when using limited training data or a single proxy model. We also customize AutoDAN \u2019s objective to leak system prompts, another jailbreak application not addressed in the adversarial attack literature. Our work provides a new way to red-team LLMs and to understand the mechanism of jailbreak attacks.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An interpretable adversarial attack, AutoDAN, is proposed, that combines the strengths of both types of attacks and provides a new way to red-team LLMs and to understand the mechanism of jailbreak attacks."
            },
            "score": 6
        },
        {
            "id": "8cf9b49698fdb1b754df2556576412a7b44929f6",
            "paperId": "8cf9b49698fdb1b754df2556576412a7b44929f6",
            "title": "SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks",
            "abstract": "Despite efforts to align large language models (LLMs) with human values, widely-used LLMs such as GPT, Llama, Claude, and PaLM are susceptible to jailbreaking attacks, wherein an adversary fools a targeted LLM into generating objectionable content. To address this vulnerability, we propose SmoothLLM, the first algorithm designed to mitigate jailbreaking attacks on LLMs. Based on our finding that adversarially-generated prompts are brittle to character-level changes, our defense first randomly perturbs multiple copies of a given input prompt, and then aggregates the corresponding predictions to detect adversarial inputs. SmoothLLM reduces the attack success rate on numerous popular LLMs to below one percentage point, avoids unnecessary conservatism, and admits provable guarantees on attack mitigation. Moreover, our defense uses exponentially fewer queries than existing attacks and is compatible with any LLM. Our code is publicly available at the following link: https://github.com/arobey1/smooth-llm.",
            "year": 2023,
            "citationCount": 59,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes SmoothLLM, the first algorithm designed to mitigate jailbreaking attacks on LLMs, which first randomly perturbs multiple copies of a given input prompt, and then aggregates the corresponding predictions to detect adversarial inputs."
            },
            "score": 6
        },
        {
            "id": "1227c2fcb8437441b7d72a29a4bc9eef1f5275d2",
            "paperId": "1227c2fcb8437441b7d72a29a4bc9eef1f5275d2",
            "title": "AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models",
            "abstract": "Safety alignment of Large Language Models (LLMs) can be compromised with manual jailbreak attacks and (automatic) adversarial attacks. Recent studies suggest that defending against these attacks is possible: adversarial attacks generate unlimited but unreadable gibberish prompts, detectable by perplexity-based filters; manual jailbreak attacks craft readable prompts, but their limited number due to the necessity of human creativity allows for easy blocking. In this paper, we show that these solutions may be too optimistic. We introduce AutoDAN, an interpretable, gradient-based adversarial attack that merges the strengths of both attack types. Guided by the dual goals of jailbreak and readability, AutoDAN optimizes and generates tokens one by one from left to right, resulting in readable prompts that bypass perplexity filters while maintaining high attack success rates. Notably, these prompts, generated from scratch using gradients, are interpretable and diverse, with emerging strategies commonly seen in manual jailbreak attacks. They also generalize to unforeseen harmful behaviors and transfer to black-box LLMs better than their unreadable counterparts when using limited training data or a single proxy model. Furthermore, we show the versatility of AutoDAN by automatically leaking system prompts using a customized objective. Our work offers a new way to red-team LLMs and understand jailbreak mechanisms via interpretability.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work offers a new way to red-team LLMs and understand jailbreak mechanisms via interpretability, by introducing AutoDAN, an interpretable, gradient-based adversarial attack that merges the strengths of both attack types."
            },
            "score": 6
        },
        {
            "id": "4d9fc5972ab0f17f3c8aa27b4d9372f029d4dded",
            "paperId": "4d9fc5972ab0f17f3c8aa27b4d9372f029d4dded",
            "title": "Adversarial Attacks on Large Language Model-Based System and Mitigating Strategies: A Case Study on ChatGPT",
            "abstract": "Machine learning algorithms are at the forefront of the development of advanced information systems. The rapid progress in machine learning technology has enabled cutting-edge large language models (LLMs), represented by GPT-3 and ChatGPT, to perform a wide range of NLP tasks with a stunning performance. However, research on adversarial machine learning highlights the need for these intelligent systems to be more robust. Adversarial machine learning aims to evaluate attack and defense mechanisms to prevent the malicious exploitation of these systems. In the case of ChatGPT, adversarial induction prompt can cause the model to generate toxic texts that could pose serious security risks or propagate false information. To address this challenge, we first analyze the effectiveness of inducing attacks on ChatGPT. Then, two effective mitigating mechanisms are proposed. The first is a training-free prefix prompt mechanism to detect and prevent the generation of toxic texts. The second is a RoBERTa-based mechanism that identifies manipulative or misleading input text via external detection models. The availability of this method is demonstrated through experiments.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A training-free prefix prompt mechanism to detect and prevent the generation of toxic texts and a RoBERTa-based mechanism that identifies manipulative or misleading input text via external detection models are proposed."
            },
            "score": 6
        },
        {
            "id": "268ba07f529df6a7f20998bb2cf26b16b31709c8",
            "paperId": "268ba07f529df6a7f20998bb2cf26b16b31709c8",
            "title": "Self-Supervised Contrastive Learning with Adversarial Perturbations for Robust Pretrained Language Models",
            "abstract": "In this paper, we present an approach to im- 001 prove the robustness of BERT language mod- 002 els against word substitution-based adversar- 003 ial attacks by leveraging adversarial perturba- 004 tions for self-supervised contrastive learning. 005 We create an ef\ufb01cient word-level adversarial 006 attack, and use it to \ufb01netune BERT on ad- 007 versarial examples generated on the \ufb02y during 008 training. In contrast with previous works, our 009 method improves model robustness without us- 010 ing any labeled data. Experimental results 011 show that our method improves robustness of 012 BERT against four different word substitution- 013 based adversarial attacks, and combining our 014 method with adversarial training gives higher 015 robustness than adversarial training alone. As 016 our method improves the robustness of BERT 017 purely with unlabeled data, it opens up the pos- 018 sibility of using large text datasets to train ro- 019 bust language models. 020",
            "year": 2021,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An approach to prove the robustness of BERT language models against word substitution-based adversarial attacks by leveraging adversarial perturbation for self-supervised contrastive learning."
            },
            "score": 6
        },
        {
            "id": "b6499bcc10d4a70c3ca8b84995270cfd0d29de4c",
            "paperId": "b6499bcc10d4a70c3ca8b84995270cfd0d29de4c",
            "title": "Model-tuning Via Prompts Makes NLP Models Adversarially Robust",
            "abstract": "In recent years, NLP practitioners have converged on the following practice: (i) import an off-the-shelf pretrained (masked) language model; (ii) append a multilayer perceptron atop the CLS token's hidden representation (with randomly initialized weights); and (iii) fine-tune the entire model on a downstream task (MLP-FT). This procedure has produced massive gains on standard NLP benchmarks, but these models remain brittle, even to mild adversarial perturbations. In this work, we demonstrate surprising gains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an alternative method of adapting to downstream tasks. Rather than appending an MLP head to make output prediction, MVP appends a prompt template to the input, and makes prediction via text infilling/completion. Across 5 NLP datasets, 4 adversarial attacks, and 3 different models, MVP improves performance against adversarial substitutions by an average of 8% over standard methods and even outperforms adversarial training-based state-of-art defenses by 3.5%. By combining MVP with adversarial training, we achieve further improvements in adversarial robustness while maintaining performance on unperturbed examples. Finally, we conduct ablations to investigate the mechanism underlying these gains. Notably, we find that the main causes of vulnerability of MLP-FT can be attributed to the misalignment between pre-training and fine-tuning tasks, and the randomly initialized MLP parameters.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work demonstrates surprising gains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an alternative method of adapting to downstream tasks that improves performance against adversarial substitutions and outperforms adversarial training-based state-of-art defenses by 3.5%."
            },
            "score": 6
        },
        {
            "id": "1f9f25aad947030fe3206114fa2ac75e8b590515",
            "paperId": "1f9f25aad947030fe3206114fa2ac75e8b590515",
            "title": "Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing",
            "abstract": "Aligned large language models (LLMs) are vulnerable to jailbreaking attacks, which bypass the safeguards of targeted LLMs and fool them into generating objectionable content. While initial defenses show promise against token-based threat models, there do not exist defenses that provide robustness against semantic attacks and avoid unfavorable trade-offs between robustness and nominal performance. To meet this need, we propose SEMANTICSMOOTH, a smoothing-based defense that aggregates the predictions of multiple semantically transformed copies of a given input prompt. Experimental results demonstrate that SEMANTICSMOOTH achieves state-of-the-art robustness against GCG, PAIR, and AutoDAN attacks while maintaining strong nominal performance on instruction following benchmarks such as InstructionFollowing and AlpacaEval. The codes will be publicly available at https://github.com/UCSB-NLP-Chang/SemanticSmooth.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results demonstrate that SEMANTICSMOOTH achieves state-of-the-art robustness against GCG, PAIR, and AutoDAN attacks while maintaining strong nominal performance on instruction following benchmarks such as InstructionFollowing and AlpacaEval."
            },
            "score": 6
        },
        {
            "id": "28e2ecb4183ebc0eec504b12dddc677f8aef8745",
            "paperId": "28e2ecb4183ebc0eec504b12dddc677f8aef8745",
            "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation",
            "abstract": "Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.",
            "year": 2023,
            "citationCount": 51,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information, indicating that there is still a considerable journey ahead to effectively apply RAG to LLMs."
            },
            "score": 5
        },
        {
            "id": "77d6d7482d1a32ad147c39993758b6c63816f5c0",
            "paperId": "77d6d7482d1a32ad147c39993758b6c63816f5c0",
            "title": "PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
            "abstract": "The increasing reliance on Large Language Models (LLMs) across academia and industry necessitates a comprehensive understanding of their robustness to prompts. In response to this vital need, we introduce PromptBench, a robustness benchmark designed to measure LLMs' resilience to adversarial prompts. This study uses a plethora of adversarial textual attacks targeting prompts across multiple levels: character, word, sentence, and semantic. The adversarial prompts, crafted to mimic plausible user errors like typos or synonyms, aim to evaluate how slight deviations can affect LLM outcomes while maintaining semantic integrity. These prompts are then employed in diverse tasks, such as sentiment analysis, natural language inference, reading comprehension, machine translation, and math problem-solving. Our study generates 4788 adversarial prompts, meticulously evaluated over 8 tasks and 13 datasets. Our findings demonstrate that contemporary LLMs are not robust to adversarial prompts. Furthermore, we present comprehensive analysis to understand the mystery behind prompt robustness and its transferability. We then offer insightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users. Code is available at: https://github.com/microsoft/promptbench.",
            "year": 2023,
            "citationCount": 111,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study generates 4788 adversarial prompts and presents comprehensive analysis to understand the mystery behind prompt robustness and its transferability, and offers insightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users."
            },
            "score": 5
        },
        {
            "id": "ace98e1e58bcc364afbb2feff6d136232f5f47da",
            "paperId": "ace98e1e58bcc364afbb2feff6d136232f5f47da",
            "title": "Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models",
            "abstract": "Considerable research efforts have been devoted to ensuring that large language models (LLMs) align with human values and generate safe text. However, an excessive focus on sensitivity to certain topics can compromise the model's robustness in following instructions, thereby impacting its overall performance in completing tasks. Previous benchmarks for jailbreaking LLMs have primarily focused on evaluating the safety of the models without considering their robustness. In this paper, we propose a benchmark that assesses both the safety and robustness of LLMs, emphasizing the need for a balanced approach. To comprehensively study text safety and output robustness, we introduce a latent jailbreak prompt dataset, each involving malicious instruction embedding. Specifically, we instruct the model to complete a regular task, such as translation, with the text to be translated containing malicious instructions. To further analyze safety and robustness, we design a hierarchical annotation framework. We present a systematic analysis of the safety and robustness of LLMs regarding the position of explicit normal instructions, word replacements (verbs in explicit normal instructions, target groups in malicious instructions, cue words for explicit normal instructions), and instruction replacements (different explicit normal instructions). Our results demonstrate that current LLMs not only prioritize certain instruction verbs but also exhibit varying jailbreak rates for different instruction verbs in explicit normal instructions. Code and data are available at https://github.com/qiuhuachuan/latent-jailbreak.",
            "year": 2023,
            "citationCount": 20,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a latent jailbreak prompt dataset, and presents a systematic analysis of the safety and robustness of LLMs regarding the position of explicit normal instructions, word replacements, and instruction replacements."
            },
            "score": 5
        },
        {
            "id": "eaeed0144d6a8000638c884f38d2d1e046aaa400",
            "paperId": "eaeed0144d6a8000638c884f38d2d1e046aaa400",
            "title": "Investigating the Robustness of Natural Language Generation from Logical Forms via Counterfactual Samples",
            "abstract": "The aim of Logic2Text is to generate controllable and faithful texts conditioned on tables and logical forms, which not only requires a deep understanding of the tables and logical forms, but also warrants symbolic reasoning over the tables according to the logical forms. State-of-the-art methods based on pre-trained models have achieved remarkable performance on the standard test dataset. However, we question whether these methods really learn how to perform logical reasoning, rather than just relying on the spurious correlations between the headers of the tables and operators of the logical form. To verify this hypothesis, we manually construct a set of counterfactual samples, which modify the original logical forms to generate counterfactual logical forms with rare co-occurred headers and operators and corresponding counterfactual references. SOTA methods give much worse results on these counterfactual samples compared with the results on the original test dataset, which verifies our hypothesis. To deal with this problem, we firstly analyze this bias from a causal perspective, based on which we propose two approaches to reduce the model\u2019s reliance on the shortcut. The first one incorporates the hierarchical structure of the logical forms into the model. The second one exploits automatically generated counterfactual data for training. Automatic and manual experimental results on the original test dataset and counterfactual dataset show that our method is effective to alleviate the spurious correlation. Our work points out the weakness of current methods and takes a further step toward developing Logic2Text models with real logical reasoning ability.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work points out the weakness of current methods and takes a further step toward developing Logic2Text models with real logical reasoning ability, by proposing two approaches to reduce the model\u2019s reliance on the shortcut."
            },
            "score": 5
        },
        {
            "id": "47030369e97cc44d4b2e3cf1be85da0fd134904a",
            "paperId": "47030369e97cc44d4b2e3cf1be85da0fd134904a",
            "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
            "abstract": "Because\"out-of-the-box\"large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called\"jailbreaks\"against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods. Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.",
            "year": 2023,
            "citationCount": 386,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information."
            },
            "score": 5
        },
        {
            "id": "db4517840e25fdd4cefe93a1c843b021ce1b25d5",
            "paperId": "db4517840e25fdd4cefe93a1c843b021ce1b25d5",
            "title": "Rethinking Textual Adversarial Defense for Pre-Trained Language Models",
            "abstract": "Although pre-trained language models (PrLMs) have achieved significant success, recent studies demonstrate that PrLMs are vulnerable to adversarial attacks. By generating adversarial examples with slight perturbations on different levels (sentence / word / character), adversarial attacks can fool PrLMs to generate incorrect predictions, which questions the robustness of PrLMs. However, we find that most existing textual adversarial examples are unnatural, which can be easily distinguished by both human and machine. Based on a general anomaly detector, we propose a novel metric (Degree of Anomaly) as a constraint to enable current adversarial attack approaches to generate more natural and imperceptible adversarial examples. Under this new constraint, the success rate of existing attacks drastically decreases, which reveals that the robustness of PrLMs is not as fragile as they claimed. In addition, we find that four types of randomization can invalidate a large portion of textual adversarial examples. Based on anomaly detector and randomization, we design a universal defense framework, which is among the first to perform textual adversarial defense without knowing the specific attack. Empirical results show that our universal defense framework achieves comparable or even higher after-attack accuracy with other specific defenses, while preserving higher original accuracy at the same time. Our work discloses the essence of textual adversarial attacks, and indicates that (i) further works of adversarial attacks should focus more on how to overcome the detection and resist the randomization, otherwise their adversarial examples would be easily detected and invalidated; and (ii) compared with the unnatural and perceptible adversarial examples, it is those undetectable adversarial examples that pose real risks for PrLMs and require more attention for future robustness-enhancing strategies.",
            "year": 2022,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A universal defense framework is designed, which is among the first to perform textual adversarial defense without knowing the specific attack and achieves comparable or even higher after-attack accuracy with other specific defenses, while preserving higher original accuracy at the same time."
            },
            "score": 5
        },
        {
            "id": "c9e4af8b55db19f9bc9359806ca9283053366fe5",
            "paperId": "c9e4af8b55db19f9bc9359806ca9283053366fe5",
            "title": "Improving the Robustness of Large Language Models via Consistency Alignment",
            "abstract": "Large language models (LLMs) have shown tremendous success in following user instructions and generating helpful responses. Nevertheless, their robustness is still far from optimal, as they may generate significantly inconsistent responses due to minor changes in the verbalized instructions. Recent literature has explored this inconsistency issue, highlighting the importance of continued improvement in the robustness of response generation. However, systematic analysis and solutions are still lacking. In this paper, we quantitatively define the inconsistency problem and propose a two-stage training framework consisting of instruction-augmented supervised fine-tuning and consistency alignment training. The first stage helps a model generalize on following instructions via similar instruction augmentations. In the second stage, we improve the diversity and help the model understand which responses are more aligned with human expectations by differentiating subtle differences in similar responses. The training process is accomplished by self-rewards inferred from the trained model at the first stage without referring to external human preference resources. We conduct extensive experiments on recent publicly available LLMs on instruction-following tasks and demonstrate the effectiveness of our training framework.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper quantitatively defines the inconsistency problem and proposes a two-stage training framework consisting of instruction-augmented supervised fine-tuning and consistency alignment training that helps a model generalize on following instructions via similar instruction augmentations."
            },
            "score": 5
        },
        {
            "id": "cd1daf7c7df2ce36d6b4135d07cbf79d37778e1c",
            "paperId": "cd1daf7c7df2ce36d6b4135d07cbf79d37778e1c",
            "title": "Proactive Detection of Query-based Adversarial Scenarios in NLP Systems",
            "abstract": "Adversarial attacks can mislead a Deep Learning (DL) algorithm into generating erroneous predictions via feeding maliciously-disturbed inputs called adversarial examples. DL-based Natural Language Processing (NLP) algorithms are severely threatened by adversarial attacks. In real-world, black-box adversarial attacks, the adversary needs to submit many highly-similar queries before drafting an adversarial example. Due to this long process, in-progress attack detection can play a significant role in adversarial defense in DL-based NLP algorithms. Although there are several approaches for detecting adversarial attacks in NLP, these approaches are reactive in the sense that they can detect adversarial examples only when they are fabricated and fed into the algorithm. In this study, we take one step towards proactive detection of adversarial attacks in NLP systems by proposing a robust, history-based model named Stateful Query Analysis (SQA) to identify suspiciously-similar sequences of queries capable of generating textual adversarial examples to which we refer by adversarial scenarios. The model exhibits a detection rate of over 99.9% in our extensive experimental tests against several state-of-the-art black-box adversarial attack methods.",
            "year": 2022,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A robust, history-based model named Stateful Query Analysis (SQA) is proposed to identify suspiciously-similar sequences of queries capable of generating textual adversarial examples to which the authors refer by adversarial scenarios to take one step towards proactive detection of adversarial attacks in NLP systems."
            },
            "score": 5
        },
        {
            "id": "c3068c7947ca290496c4d0280904686ba0b2903f",
            "paperId": "c3068c7947ca290496c4d0280904686ba0b2903f",
            "title": "Mind the instructions: a holistic evaluation of consistency and interactions in prompt-based learning",
            "abstract": "Finding the best way of adapting pre-trained language models to a task is a big challenge in current NLP. Just like the previous generation of task-tuned models (TT), models that are adapted to tasks via in-context-learning (ICL) or instruction tuning (IT) are robust in some setups, but not in others. Here, we present a detailed analysis of which design choices cause instabilities and inconsistencies in LLM predictions. First, we show how spurious correlations between input distributions and labels \u2013 a known issue in TT models \u2013 form only a minor problem for prompted models. Then we engage in a systematic, holistic evaluation of different factors that have been found to influence predictions in a prompting setup. We test all possible combinations of a range of factors on both vanilla and instruction-tuned LLMs of different scale, and statistically analyse the results to show which factors are the most influential, the most interactive or the most stable. From our results, we deduce which factors can be used without precautions, should be avoided or handled with care in most settings.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A detailed analysis of which design choices cause instabilities and inconsistencies in LLM predictions is presented, and which factors can be used without precautions, should be avoided or handled with care in most settings are deduced."
            },
            "score": 5
        },
        {
            "id": "d7386e8859b22e05ce9c4a972613d4b1e1e44198",
            "paperId": "d7386e8859b22e05ce9c4a972613d4b1e1e44198",
            "title": "Prompting Large Language Models With the Socratic Method",
            "abstract": "This paper presents a systematic approach to using the Socratic method in developing prompt templates that effectively interact with large language models, including GPT-3. Various methods are examined, and those that yield precise answers and justifications while fostering creativity and imagination to enhance creative writing are identified. Techniques such as definition, elenchus, dialectic, maieutics, generalization, and counterfactual reasoning are discussed for their application in engineering prompt templates and their connections to inductive, deductive, and abductive reasoning. Through examples, the effectiveness of these dialogue and reasoning methods is demonstrated. An interesting observation is made that when the task's goal and user intent are conveyed to GPT-3 via ChatGPT before the start of a dialogue, the large language model seems to connect to the external context expressed in the intent and perform more effectively.",
            "year": 2023,
            "citationCount": 20,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An interesting observation is made that when the task's goal and user intent are conveyed to GPT-3 via ChatGPT before the start of a dialogue, the large language model seems to connect to the external context expressed in the intent and perform more effectively."
            },
            "score": 4
        },
        {
            "id": "71d68782c3da41b77866c2fd0cb65726f60b3af1",
            "paperId": "71d68782c3da41b77866c2fd0cb65726f60b3af1",
            "title": "Analyzing Chain-of-Thought Prompting in Large Language Models via Gradient-based Feature Attributions",
            "abstract": "Chain-of-thought (CoT) prompting has been shown to empirically improve the accuracy of large language models (LLMs) on various question answering tasks. While understanding why CoT prompting is effective is crucial to ensuring that this phenomenon is a consequence of desired model behavior, little work has addressed this; nonetheless, such an understanding is a critical prerequisite for responsible model deployment. We address this question by leveraging gradient-based feature attribution methods which produce saliency scores that capture the influence of input tokens on model output. Specifically, we probe several open-source LLMs to investigate whether CoT prompting affects the relative importances they assign to particular input tokens. Our results indicate that while CoT prompting does not increase the magnitude of saliency scores attributed to semantically relevant tokens in the prompt compared to standard few-shot prompting, it increases the robustness of saliency scores to question perturbations and variations in model output.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work probes several open-source LLMs to investigate whether CoT prompting affects the relative importances they assign to particular input tokens, and results indicate that while coT prompting does not increase the magnitude of saliency scores attributed to semantically relevant tokens in the prompt compared to standard few-shot prompting, it increases the robustness ofsaliency scores to question perturbations and variations in model output."
            },
            "score": 4
        },
        {
            "id": "10632e0a667cbc3c52cc8f11a46d8e8e9c7739e3",
            "paperId": "10632e0a667cbc3c52cc8f11a46d8e8e9c7739e3",
            "title": "Causal Reasoning and Large Language Models: Opening a New Frontier for Causality",
            "abstract": "The causal capabilities of large language models (LLMs) is a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We further our understanding of LLMs and their causal implications, considering the distinctions between different types of causal reasoning tasks, as well as the entangled threats of construct and measurement validity. LLM-based methods establish new state-of-the-art accuracies on multiple causal benchmarks. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain), and actual causality (86% accuracy in determining necessary and sufficient causes in vignettes). At the same time, LLMs exhibit unpredictable failure modes and we provide some techniques to interpret their robustness. Crucially, LLMs perform these causal tasks while relying on sources of knowledge and methods distinct from and complementary to non-LLM based approaches. Specifically, LLMs bring capabilities so far understood to be restricted to humans, such as using collected knowledge to generate causal graphs or identifying background causal context from natural language. We envision LLMs to be used alongside existing causal methods, as a proxy for human domain knowledge and to reduce human effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods. We also see existing causal methods as promising tools for LLMs to formalize, validate, and communicate their reasoning especially in high-stakes scenarios. In capturing common sense and domain knowledge about causal mechanisms and supporting translation between natural language and formal methods, LLMs open new frontiers for advancing the research, practice, and adoption of causality.",
            "year": 2023,
            "citationCount": 96,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LLMs are envisioned to be used alongside existing causal methods, as a proxy for human domain knowledge and to reduce human effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods."
            },
            "score": 4
        },
        {
            "id": "6ac627f57b26354ab537734d820da4a6a7dde2c6",
            "paperId": "6ac627f57b26354ab537734d820da4a6a7dde2c6",
            "title": "CLadder: Assessing Causal Reasoning in Language Models",
            "abstract": "The ability to perform causal reasoning is widely considered a core feature of intelligence. In this work, we investigate whether large language models (LLMs) can coherently reason about causality. Much of the existing work in natural language processing (NLP) focuses on evaluating commonsense causal reasoning in LLMs, thus failing to assess whether a model can perform causal inference in accordance with a set of well-defined formal rules. To address this, we propose a new NLP task, causal inference in natural language, inspired by the\"causal inference engine\"postulated by Judea Pearl et al. We compose a large dataset, CLadder, with 10K samples: based on a collection of causal graphs and queries (associational, interventional, and counterfactual), we obtain symbolic questions and ground-truth answers, through an oracle causal inference engine. These are then translated into natural language. We evaluate multiple LLMs on our dataset, and we introduce and evaluate a bespoke chain-of-thought prompting strategy, CausalCoT. We show that our task is highly challenging for LLMs, and we conduct an in-depth analysis to gain deeper insights into the causal reasoning abilities of LLMs. Our data is open-sourced at https://huggingface.co/datasets/causalNLP/cladder, and our code can be found at https://github.com/causalNLP/cladder.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates whether large language models (LLMs) can coherently reason about causality, and proposes a new NLP task, causal inference in natural language, inspired by the \"causal inference engine\" proposed by Judea Pearl et al."
            },
            "score": 4
        },
        {
            "id": "40ee4949c1050a465d418deb6dd7ea6304a3bc29",
            "paperId": "40ee4949c1050a465d418deb6dd7ea6304a3bc29",
            "title": "Adversarial Attacks and Defenses in Large Language Models: Old and New Threats",
            "abstract": "Over the past decade, there has been extensive research aimed at enhancing the robustness of neural networks, yet this problem remains vastly unsolved. Here, one major impediment has been the overestimation of the robustness of new defense approaches due to faulty defense evaluations. Flawed robustness evaluations necessitate rectifications in subsequent works, dangerously slowing down the research and providing a false sense of security. In this context, we will face substantial challenges associated with an impending adversarial arms race in natural language processing, specifically with closed-source Large Language Models (LLMs), such as ChatGPT, Google Bard, or Anthropic's Claude. We provide a first set of prerequisites to improve the robustness assessment of new approaches and reduce the amount of faulty evaluations. Additionally, we identify embedding space attacks on LLMs as another viable threat model for the purposes of generating malicious content in open-sourced models. Finally, we demonstrate on a recently proposed defense that, without LLM-specific best practices in place, it is easy to overestimate the robustness of a new approach.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work provides a first set of prerequisites to improve the robustness assessment of new approaches and reduce the amount of faulty evaluations, and identifies embedding space attacks on LLMs as another viable threat model for the purposes of generating malicious content in open-sourced models."
            },
            "score": 4
        },
        {
            "id": "92b9d8b8c81c4c53ea62000c0924500b2dd11bce",
            "paperId": "92b9d8b8c81c4c53ea62000c0924500b2dd11bce",
            "title": "Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models",
            "abstract": "We introduce new jailbreak attacks on vision language models (VLMs), which use aligned LLMs and are resilient to text-only jailbreak attacks. Specifically, we develop cross-modality attacks on alignment where we pair adversarial images going through the vision encoder with textual prompts to break the alignment of the language model. Our attacks employ a novel compositional strategy that combines an image, adversarially targeted towards toxic embeddings, with generic prompts to accomplish the jailbreak. Thus, the LLM draws the context to answer the generic prompt from the adversarial image. The generation of benign-appearing adversarial images leverages a novel embedding-space-based methodology, operating with no access to the LLM model. Instead, the attacks require access only to the vision encoder and utilize one of our four embedding space targeting strategies. By not requiring access to the LLM, the attacks lower the entry barrier for attackers, particularly when vision encoders such as CLIP are embedded in closed-source LLMs. The attacks achieve a high success rate across different VLMs, highlighting the risk of cross-modality alignment vulnerabilities, and the need for new alignment approaches for multi-modal models.",
            "year": 2023,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Cross-modality attacks on alignment where adversarial images going through the vision encoder with textual prompts to break the alignment of the language model are developed."
            },
            "score": 4
        },
        {
            "id": "1abfc211793c683972ded8d3268475e3ee7a88b0",
            "paperId": "1abfc211793c683972ded8d3268475e3ee7a88b0",
            "title": "Adversarial Demonstration Attacks on Large Language Models",
            "abstract": "With the emergence of more powerful large language models (LLMs), such as ChatGPT and GPT-4, in-context learning (ICL) has gained significant prominence in leveraging these models for specific tasks by utilizing data-label pairs as precondition prompts. While incorporating demonstrations can greatly enhance the performance of LLMs across various tasks, it may introduce a new security concern: attackers can manipulate only the demonstrations without changing the input to perform an attack. In this paper, we investigate the security concern of ICL from an adversarial perspective, focusing on the impact of demonstrations. We propose a novel attack method named advICL, which aims to manipulate only the demonstration without changing the input to mislead the models. Our results demonstrate that as the number of demonstrations increases, the robustness of in-context learning would decrease. Additionally, we also identify the intrinsic property of the demonstrations is that they can be used (prepended) with different inputs. As a result, it introduces a more practical threat model in which an attacker can attack the test input example even without knowing and manipulating it. To achieve it, we propose the transferable version of advICL, named Transferable-advICL. Our experiment shows that the adversarial demonstration generated by Transferable-advICL can successfully attack the unseen test input examples. We hope that our study reveals the critical security risks associated with ICL and underscores the need for extensive research on the robustness of ICL, particularly given its increasing significance in the advancement of LLMs.",
            "year": 2023,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper investigates the security concern of ICL from an adversarial perspective, focusing on the impact of demonstrations, and proposes a novel attack method named advICL, which aims to manipulate only the demonstration without changing the input to mislead the models."
            },
            "score": 4
        },
        {
            "id": "620a1a585a5e433a47103c112de17553a81fcbe6",
            "paperId": "620a1a585a5e433a47103c112de17553a81fcbe6",
            "title": "Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks",
            "abstract": "Although remarkable progress has been achieved in preventing large language model (LLM) hallucinations using instruction tuning and retrieval augmentation, it remains challenging to measure the reliability of LLMs using human-crafted evaluation data which is not available for many tasks and domains and could suffer from data leakage. Inspired by adversarial machine learning, this paper aims to develop a method of automatically generating evaluation data by appropriately modifying existing data on which LLMs behave faithfully. Specifically, this paper presents AutoDebug, an LLM-based framework to use prompting chaining to generate transferable adversarial attacks in the form of question-answering examples. We seek to understand the extent to which these examples trigger the hallucination behaviors of LLMs. We implement AutoDebug using ChatGPT and evaluate the resulting two variants of a popular open-domain question-answering dataset, Natural Questions (NQ), on a collection of open-source and proprietary LLMs under various prompting settings. Our generated evaluation data is human-readable and, as we show, humans can answer these modified questions well. Nevertheless, we observe pronounced accuracy drops across multiple LLMs including GPT-4. Our experimental results show that LLMs are likely to hallucinate in two categories of question-answering scenarios where (1) there are conflicts between knowledge given in the prompt and their parametric knowledge, or (2) the knowledge expressed in the prompt is complex. Finally, we find that the adversarial examples generated by our method are transferable across all considered LLMs. The examples generated by a small model can be used to debug a much larger model, making our approach cost-effective.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents AutoDebug, an LLM-based framework to use prompting chaining to generate transferable adversarial attacks in the form of question-answering examples, and finds that the adversarial examples generated by the method are transferable across all considered LLMs."
            },
            "score": 4
        },
        {
            "id": "8951bbc8c1eb3fd43c9e47025268cc79b868f514",
            "paperId": "8951bbc8c1eb3fd43c9e47025268cc79b868f514",
            "title": "Why do universal adversarial attacks work on large language models?: Geometry might be the answer",
            "abstract": "Transformer based large language models with emergent capabilities are becoming increasingly ubiquitous in society. However, the task of understanding and interpreting their internal workings, in the context of adversarial attacks, remains largely unsolved. Gradient-based universal adversarial attacks have been shown to be highly effective on large language models and potentially dangerous due to their input-agnostic nature. This work presents a novel geometric perspective explaining universal adversarial attacks on large language models. By attacking the 117M parameter GPT-2 model, we find evidence indicating that universal adversarial triggers could be embedding vectors which merely approximate the semantic information in their adversarial training region. This hypothesis is supported by white-box model analysis comprising dimensionality reduction and similarity measurement of hidden representations. We believe this new geometric perspective on the underlying mechanism driving universal attacks could help us gain deeper insight into the internal workings and failure modes of LLMs, thus enabling their mitigation.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "By attacking the 117M parameter GPT-2 model, this work finds evidence indicating that universal adversarial triggers could be embedding vectors which merely approximate the semantic information in their adversarial training region."
            },
            "score": 4
        },
        {
            "id": "6d465be006615460d41060f9f5068d51fc1f46b1",
            "paperId": "6d465be006615460d41060f9f5068d51fc1f46b1",
            "title": "Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models",
            "abstract": "The integration of large language models (LLMs) with external content has enabled more up-to-date and wide-ranging applications of LLMs, such as Microsoft Copilot. However, this integration has also exposed LLMs to the risk of indirect prompt injection attacks, where an attacker can embed malicious instructions within external content, compromising LLM output and causing responses to deviate from user expectations. To investigate this important but underexplored issue, we introduce the first benchmark for indirect prompt injection attacks, named BIPIA, to evaluate the risk of such attacks. Based on the evaluation, our work makes a key analysis of the underlying reason for the success of the attack, namely the inability of LLMs to distinguish between instructions and external content and the absence of LLMs' awareness to not execute instructions within external content. Building upon this analysis, we develop two black-box methods based on prompt learning and a white-box defense method based on fine-tuning with adversarial training accordingly. Experimental results demonstrate that black-box defenses are highly effective in mitigating these attacks, while the white-box defense reduces the attack success rate to near-zero levels. Overall, our work systematically investigates indirect prompt injection attacks by introducing a benchmark, analyzing the underlying reason for the success of the attack, and developing an initial set of defenses.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work systematically investigates indirect prompt injection attacks by introducing a benchmark, analyzing the underlying reason for the success of the attack, and developing an initial set of defenses."
            },
            "score": 4
        },
        {
            "id": "6b135e922a0c673aeb0b05c5aeecdb6c794791c6",
            "paperId": "6b135e922a0c673aeb0b05c5aeecdb6c794791c6",
            "title": "Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations",
            "abstract": "Large Language Models (LLMs) have shown remarkable success in various tasks, but concerns about their safety and the potential for generating malicious content have emerged. In this paper, we explore the power of In-Context Learning (ICL) in manipulating the alignment ability of LLMs. We find that by providing just few in-context demonstrations without fine-tuning, LLMs can be manipulated to increase or decrease the probability of jailbreaking, i.e. answering malicious prompts. Based on these observations, we propose In-Context Attack (ICA) and In-Context Defense (ICD) methods for jailbreaking and guarding aligned language model purposes. ICA crafts malicious contexts to guide models in generating harmful outputs, while ICD enhances model robustness by demonstrations of rejecting to answer harmful prompts. Our experiments show the effectiveness of ICA and ICD in increasing or reducing the success rate of adversarial jailbreaking attacks. Overall, we shed light on the potential of ICL to influence LLM behavior and provide a new perspective for enhancing the safety and alignment of LLMs.",
            "year": 2023,
            "citationCount": 59,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Light is shed on the potential of In-Context Learning (ICL) to influence LLM behavior and a new perspective for enhancing the safety and alignment of LLMs is provided."
            },
            "score": 4
        },
        {
            "id": "dc2b36fb20490c0d540e01e74efd868d2e17faa3",
            "paperId": "dc2b36fb20490c0d540e01e74efd868d2e17faa3",
            "title": "Bring Your Own Data! Self-Supervised Evaluation for Large Language Models",
            "abstract": "With the rise of Large Language Models (LLMs) and their ubiquitous deployment in diverse domains, measuring language model behavior on realistic data is imperative. For example, a company deploying a client-facing chatbot must ensure that the model will not respond to client requests with profanity. Current evaluations approach this problem using small, domain-specific datasets with human-curated labels. These evaluation sets are often sampled from a narrow and simplified distribution, and data sources can unknowingly be leaked into the training set which can lead to misleading evaluations. To bypass these drawbacks, we propose a framework for self-supervised evaluation of LLMs by analyzing their sensitivity or invariance to transformations on the input text. Self-supervised evaluation can directly monitor LLM behavior on datasets collected in the wild or streamed during live model deployment. We demonstrate self-supervised evaluation strategies for measuring closed-book knowledge, toxicity, and long-range context dependence, in addition to sensitivity to grammatical structure and tokenization errors. When comparisons to similar human-labeled benchmarks are available, we find strong correlations between self-supervised and human-supervised evaluations. The self-supervised paradigm complements current evaluation strategies that rely on labeled data.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a framework for self-supervised evaluation of LLMs by analyzing their sensitivity or invariance to transformations on the input text, and demonstrates self- supervised evaluation strategies for measuring closed-book knowledge, toxicity, and long-range context dependence."
            },
            "score": 4
        },
        {
            "id": "fbebf142df7d4d75d941a20f2398457354cb83aa",
            "paperId": "fbebf142df7d4d75d941a20f2398457354cb83aa",
            "title": "On The Robustness of Self-Supervised Representations for Spoken Language Modeling",
            "abstract": "Self-supervised representations have been extensively studied for discriminative and generative tasks. However, their robustness capabilities have not been extensively investigated. This work focuses on self-supervised representations for spoken generative language models. First, we empirically demonstrate how current state-of-the-art speech representation models lack robustness to basic signal variations that do not alter the spoken information. To overcome this, we propose an effective and ef\ufb01cient method to learn robust self-supervised speech representation for generative spoken language modeling. The proposed approach is based on applying a set of signal transformations to the speech signal and optimizing the model using an iterative pseudo-labeling scheme. Our method signi\ufb01cantly improves over the evaluated baselines when considering encoding metrics. We additionally evaluate our method on the speech-to-speech translation task. We consider Spanish-English and French-English conversions and empirically demonstrate the bene\ufb01ts of following the proposed approach.",
            "year": 2022,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work empirically demonstrate how current state-of-the-art speech representation models lack robustness to basic signal variations that do not alter the spoken information and proposes an effective andcient method to learn robust self-supervised speech representation for generative spoken language modeling."
            },
            "score": 4
        },
        {
            "id": "beea0632fa73ce4a196334d89b8c2574f8d1ce25",
            "paperId": "beea0632fa73ce4a196334d89b8c2574f8d1ce25",
            "title": "A Joint Speech Enhancement and Self-Supervised Representation Learning Framework for Noise-Robust Speech Recognition",
            "abstract": "Though speech enhancement (SE) can be used to improve speech quality in noisy environments, it may also cause distortions that degrade the performance of automatic speech recognition (ASR) models. Self-supervised pre-training, on the other hand, has been shown to improve the noise robustness of ASR models. However, the potential of the (optimal) integration of SE and self-supervised pre-training still remains unclear. In this paper, we propose a novel self-supervised pre-training framework that incorporates SE to improve ASR performance in noisy environments. First, in the pre-training phase the original noisy waveform or the waveform obtained by SE is fed into the self-supervised model to learn the contextual representation, where the quantized clean speech acts as the target. Second, we propose a dual-attention fusion method to fuse the features of noisy and enhanced speech, which can compensate for the information loss caused by separately using individual modules. Due to the flexible exploitation of clean/noisy/enhanced branches, the proposed method turns out to be a generalization of some existing noise-robust ASR models, e.g., enhanced wav2vec2.0. Finally, experimental results on both synthetic and real noisy datasets show that the proposed joint training approach can improve the ASR performance under various noisy settings, leading to a stronger noise robustness.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel self-supervised pre-training framework that incorporates SE to improve ASR performance in noisy environments and a dual-attention fusion method to fuse the features of noisy and enhanced speech, which can compensate for the information loss caused by separately using individual modules."
            },
            "score": 4
        },
        {
            "id": "804356bb7e8cc7b8735cb4a3cf37d5a4de315742",
            "paperId": "804356bb7e8cc7b8735cb4a3cf37d5a4de315742",
            "title": "COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs",
            "abstract": "Counterfactual examples have proven to be valuable in the field of natural language processing (NLP) for both evaluating and improving the robustness of language models to spurious correlations in datasets. Despite their demonstrated utility for NLP, multimodal counterfactual examples have been relatively unexplored due to the difficulty of creating paired image-text data with minimal counterfactual changes. To address this challenge, we introduce a scalable framework for automatic generation of counterfactual examples using text-to-image diffusion models. We use our framework to create COCO-Counterfactuals, a multimodal counterfactual dataset of paired image and text captions based on the MS-COCO dataset. We validate the quality of COCO-Counterfactuals through human evaluations and show that existing multimodal models are challenged by our counterfactual image-text pairs. Additionally, we demonstrate the usefulness of COCO-Counterfactuals for improving out-of-domain generalization of multimodal vision-language models via training data augmentation.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A scalable framework for automatic generation of counterfactual examples using text-to-image diffusion models and the usefulness of COCO-Counterfactuals for improving out-of-domain generalization of multimodal vision-language models via training data augmentation is demonstrated."
            },
            "score": 4
        },
        {
            "id": "01efb3fd2d3ae4b5f4389c916c94f2c6d9c11b81",
            "paperId": "01efb3fd2d3ae4b5f4389c916c94f2c6d9c11b81",
            "title": "Explore Spurious Correlations at the Concept Level in Language Models for Text Classification",
            "abstract": "Language models (LMs) have achieved notable success in numerous NLP tasks, employing both fine-tuning and in-context learning (ICL) methods. While language models demonstrate exceptional performance, they face robustness challenges due to spurious correlations arising from imbalanced label distributions in training data or ICL exemplars. Previous research has primarily concentrated on word, phrase, and syntax features, neglecting the concept level, often due to the absence of concept labels and difficulty in identifying conceptual content in input texts. This paper introduces two main contributions. First, we employ ChatGPT to assign concept labels to texts, assessing concept bias in models during fine-tuning or ICL on test data. We find that LMs, when encountering spurious correlations between a concept and a label in training or prompts, resort to shortcuts for predictions. Second, we introduce a data rebalancing technique that incorporates ChatGPT-generated counterfactual data, thereby balancing label distribution and mitigating spurious correlations. Our method's efficacy, surpassing traditional token removal approaches, is validated through extensive testing.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A data rebalancing technique is introduced that incorporates ChatGPT-generated counterfactual data, thereby balancing label distribution and mitigating spurious correlations and finding that LMs, when encountering spurious correlations between a concept and a label in training or prompts, resort to shortcuts for predictions."
            },
            "score": 4
        },
        {
            "id": "498d1406fc4cddb05cd46477793f2e726a6fe238",
            "paperId": "498d1406fc4cddb05cd46477793f2e726a6fe238",
            "title": "The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code",
            "abstract": "Causal reasoning, the ability to identify cause-and-effect relationship, is crucial in human thinking. Although large language models (LLMs) succeed in many NLP tasks, it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning. Given the fact that programming code may express causal relations more often and explicitly with conditional statements like ``if``, we want to explore whether Code-LLMs acquire better causal reasoning abilities. Our experiments show that compared to text-only LLMs, Code-LLMs with code prompts are significantly better in causal reasoning. We further intervene on the prompts from different aspects, and discover that the programming structure is crucial in code prompt design, while Code-LLMs are robust towards format perturbations.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experiments show that compared to text-only LLMs, Code-LLMs with code prompts are significantly better in causal reasoning, and that the programming structure is crucial in code prompt design, while Code- LLMs are robust towards format perturbations."
            },
            "score": 4
        },
        {
            "id": "e4e744cc96da7987a072571fc3817f040d456566",
            "paperId": "e4e744cc96da7987a072571fc3817f040d456566",
            "title": "Large Language Models Know Your Contextual Search Intent: A Prompting Framework for Conversational Search",
            "abstract": "Precisely understanding users' contextual search intent has been an important challenge for conversational search. As conversational search sessions are much more diverse and long-tailed, existing methods trained on limited data still show unsatisfactory effectiveness and robustness to handle real conversational search scenarios. Recently, large language models (LLMs) have demonstrated amazing capabilities for text generation and conversation understanding. In this work, we present a simple yet effective prompting framework, called LLM4CS, to leverage LLMs as a text-based search intent interpreter to help conversational search. Under this framework, we explore three prompting methods to generate multiple query rewrites and hypothetical responses, and propose to aggregate them into an integrated representation that can robustly represent the user's real contextual search intent. Extensive automatic evaluations and human evaluations on three widely used conversational search benchmarks, including CAsT-19, CAsT-20, and CAsT-21, demonstrate the remarkable performance of our simple LLM4CS framework compared with existing methods and even using human rewrites. Our findings provide important evidence to better understand and leverage LLMs for conversational search.",
            "year": 2023,
            "citationCount": 26,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents a simple yet effective prompting framework, called LLM4CS, to leverage LLMs as a text-based search intent interpreter to help conversational search."
            },
            "score": 3
        },
        {
            "id": "ca261cb681b082e90ca6c7a9d325b4265ed1dc28",
            "paperId": "ca261cb681b082e90ca6c7a9d325b4265ed1dc28",
            "title": "MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models",
            "abstract": "Large language models (LLMs) have achieved remarkable performance in natural language understanding and generation tasks. However, they often suffer from limitations such as difficulty in incorporating new knowledge, generating hallucinations, and explaining their reasoning process. To address these challenges, we propose a novel prompting pipeline, named \\method, that leverages knowledge graphs (KGs) to enhance LLMs' inference and transparency. Our method enables LLMs to comprehend KG inputs and infer with a combination of implicit and external knowledge. Moreover, our method elicits the mind map of LLMs, which reveals their reasoning pathways based on the ontology of knowledge. We evaluate our method on diverse question \\&answering tasks, especially in medical domains, and show significant improvements over baselines. We also introduce a new hallucination evaluation benchmark and analyze the effects of different components of our method. Our results demonstrate the effectiveness and robustness of our method in merging knowledge from LLMs and KGs for combined inference. To reproduce our results and extend the framework further, we make our codebase available at https://github.com/wyl-willing/MindMap.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel prompting pipeline is proposed that leverages knowledge graphs (KGs) to enhance LLMs' inference and transparency and elicits the mind map of LLMs, which reveals their reasoning pathways based on the ontology of knowledge."
            },
            "score": 3
        },
        {
            "id": "d55ed10e6a77e8f0a2359eb92221915f56481843",
            "paperId": "d55ed10e6a77e8f0a2359eb92221915f56481843",
            "title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
            "abstract": "Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 15 LLMs of four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators, such as System Star is better than System Square. We then evaluate the quality of ranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the Egocentric bias where a model prefers to rank its own outputs highly in evaluation. We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (average of 40% of comparisons across all models) within each of their evaluations that question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the average Rank-Biased Overlap (RBO) score to be 49.6%, indicating that machine preferences are misaligned with humans. According to our findings, LLMs may still be unable to be utilized for automatic annotation aligned with human preferences. Our project page is at: https://minnesotanlp.github.io/cobbler.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work assembles 15 LLMs of four different size ranges and finds that LLMs are biased text quality evaluators, exhibiting strong indications on the authors' bias benchmark within each of their evaluations that question their robustness as evaluator."
            },
            "score": 3
        },
        {
            "id": "142e934dd5d6c53f877c30243d436255e3a0dde7",
            "paperId": "142e934dd5d6c53f877c30243d436255e3a0dde7",
            "title": "Visual Adversarial Examples Jailbreak Aligned Large Language Models",
            "abstract": "Warning: this paper contains data, prompts, and model outputs that are offensive in nature.\n\nRecently, there has been a surge of interest in integrating vision into Large Language Models (LLMs), exemplified by Visual Language Models (VLMs) such as Flamingo and GPT-4. This paper sheds light on the security and safety implications of this trend. First, we underscore that the continuous and high-dimensional nature of the visual input makes it a weak link against adversarial attacks, representing an expanded attack surface of vision-integrated LLMs. Second, we highlight that the versatility of LLMs also presents visual attackers with a wider array of achievable adversarial objectives, extending the implications of security failures beyond mere misclassification. As an illustration, we present a case study in which we exploit visual adversarial examples to circumvent the safety guardrail of aligned LLMs with integrated vision. Intriguingly, we discover that a single visual adversarial example can universally jailbreak an aligned LLM, compelling it to heed a wide range of harmful instructions (that it otherwise would not) and generate harmful content that transcends the narrow scope of a `few-shot' derogatory corpus initially employed to optimize the adversarial example. Our study underscores the escalating adversarial risks associated with the pursuit of multimodality. Our findings also connect the long-studied adversarial vulnerabilities of neural networks to the nascent field of AI alignment. The presented attack suggests a fundamental adversarial challenge for AI alignment, especially in light of the emerging trend toward multimodality in frontier foundation models.",
            "year": 2023,
            "citationCount": 44,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that a single visual adversarial example can universally jailbreak an aligned LLM, compelling it to heed a wide range of harmful instructions and generate harmful content that transcends the narrow scope of a `few-shot' derogatory corpus initially employed to optimize the adversarial example."
            },
            "score": 3
        },
        {
            "id": "de0593223973eccfee04d598a68bc55784c7fc17",
            "paperId": "de0593223973eccfee04d598a68bc55784c7fc17",
            "title": "LogicLLM: Exploring Self-supervised Logic-enhanced Training for Large Language Models",
            "abstract": "Existing efforts to improve logical reasoning ability of language models have predominantly relied on supervised fine-tuning, hindering generalization to new domains and/or tasks. The development of Large Langauge Models (LLMs) has demonstrated the capacity of compressing abundant knowledge into a single proxy, enabling them to tackle multiple tasks effectively. Our preliminary experiments, nevertheless, show that LLMs do not show capability on logical reasoning. The performance of LLMs on logical reasoning benchmarks is far behind the existing state-of-the-art baselines. In this paper, we make the first attempt to investigate the feasibility of incorporating logical knowledge through self-supervised post-training, and activating it via in-context learning, which we termed as LogicLLM. Specifically, we devise an auto-regressive objective variant of MERIt (Jiao et al., 2022) and integrate it with two LLM series, i.e., FLAN-T5 and LLaMA, with parameter size ranging from 3 billion to 13 billion. The results on two challenging logical reasoning benchmarks demonstrate the effectiveness of LogicLLM. Besides, we conduct extensive ablation studies to analyze the key factors in designing logic-oriented proxy tasks. 1",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper makes the first attempt to investigate the feasibility of incorporating logical knowledge through self-supervised post-training, and activating it via in-context learning, which is termed as LogicLLM."
            },
            "score": 3
        },
        {
            "id": "e01515c6138bc525f7aec30fc85f2adf028d4156",
            "paperId": "e01515c6138bc525f7aec30fc85f2adf028d4156",
            "title": "Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision",
            "abstract": "Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and guide the LLM through in-context learning from demonstrations (of principles application) to produce helpful, ethical, and reliable responses to user's queries; third, we fine-tune the original LLM with the high-quality self-aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally, we offer a refinement step to address the issues of overly-brief or indirect responses. Applying SELF-ALIGN to the LLaMA-65b base language model, we develop an AI assistant named Dromedary. With fewer than 300 lines of human annotations (including<200 seed prompts, 16 generic principles, and 5 exemplars for in-context learning). Dromedary significantly surpasses the performance of several state-of-the-art AI systems, including Text-Davinci-003 and Alpaca, on benchmark datasets with various settings.",
            "year": 2023,
            "citationCount": 137,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An AI assistant named Dromedary is developed, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision and significantly surpasses the performance of several state-of-the-art AI systems on benchmark datasets with various settings."
            },
            "score": 3
        },
        {
            "id": "529e997e0d9730c25ad4347502da7e5a753274b8",
            "paperId": "529e997e0d9730c25ad4347502da7e5a753274b8",
            "title": "Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference",
            "abstract": "While large pre-trained language models are powerful, their predictions often lack logical consistency across test inputs. For example, a state-of-the-art Macaw question-answering (QA) model answers Yes to Is a sparrow a bird? and Does a bird have feet? but answers No to Does a sparrow have feet?. To address this failure mode, we propose a framework, Consistency Correction through Relation Detection, or ConCoRD, for boosting the consistency and accuracy of pre-trained NLP models using pre-trained natural language inference (NLI) models without fine-tuning or re-training. Given a batch of test inputs, ConCoRD samples several candidate outputs for each input and instantiates a factor graph that accounts for both the model\u2019s belief about the likelihood of each answer choice in isolation and the NLI model\u2019s beliefs about pair-wise answer choice compatibility. We show that a weighted MaxSAT solver can efficiently compute high-quality answer choices under this factor graph, improving over the raw model\u2019s predictions. Our experiments demonstrate that ConCoRD consistently boosts accuracy and consistency of off-the-shelf closed-book QA and VQA models using off-the-shelf NLI models, notably increasing accuracy of LXMERT on ConVQA by 5% absolute. See the project website (https://ericmitchell.ai/emnlp-2022-concord/) for code and data.",
            "year": 2022,
            "citationCount": 19,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a framework, Consistency Correction through Relation Detection, or ConCoRD, for boosting the consistency and accuracy of pre- trained NLP models using pre-trained natural language inference (NLI) models without fine-tuning or re-training."
            },
            "score": 3
        },
        {
            "id": "09edb6a96b5af0d1ad1abb4e192e953844718628",
            "paperId": "09edb6a96b5af0d1ad1abb4e192e953844718628",
            "title": "Uncertainty-aware Parameter-Efficient Self-training for Semi-supervised Language Understanding",
            "abstract": "The recent success of large pre-trained language models (PLMs) heavily hinges on massive labeled data, which typically produces inferior performance in low-resource scenarios. To remedy this dilemma, we study self-training as one of the predominant semi-supervised learning (SSL) approaches, which utilizes large-scale unlabeled data to generate synthetic examples. However, too many noisy labels will hurt the model performance, and the self-training procedure requires multiple training iterations making it more expensive if all the model parameters of the PLM are updated. This paper presents UPET, a novel Uncertainty-aware Parameter-Efficient self-Training framework to effectively and efficiently address the labeled data scarcity issue. Specifically, we incorporate Monte Carlo (MC) dropout in Bayesian neural network (BNN) to perform uncertainty estimation for the teacher model and then judiciously select reliable pseudo-labeled examples based on confidence and certainty. During the student training, we introduce multiple parameter-efficient learning (PEL) paradigms that allow the optimization of only a small percentage of parameters. We also propose a novel Easy-Hard Contrastive Tuning to enhance the robustness and generalization. Extensive experiments over multiple downstream tasks demonstrate that UPET achieves a substantial improvement in terms of performance and efficiency. Our codes and data are released at https: //github.com/wjn1996/UPET.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents UPET, a novel Uncertainty-aware Parameter-Efficient self-Training framework to effectively and efficiently address the labeled data scarcity issue and incorporates Monte Carlo dropout in Bayesian neural network to perform uncertainty estimation for the teacher model."
            },
            "score": 3
        },
        {
            "id": "921fb0112c775df4d55d8ebad88b4341ede23702",
            "paperId": "921fb0112c775df4d55d8ebad88b4341ede23702",
            "title": "Unifying (Machine) Vision via Counterfactual World Modeling",
            "abstract": "Leading approaches in machine vision employ different architectures for different tasks, trained on costly task-specific labeled datasets. This complexity has held back progress in areas, such as robotics, where robust task-general perception remains a bottleneck. In contrast,\"foundation models\"of natural language have shown how large pre-trained neural networks can provide zero-shot solutions to a broad spectrum of apparently distinct tasks. Here we introduce Counterfactual World Modeling (CWM), a framework for constructing a visual foundation model: a unified, unsupervised network that can be prompted to perform a wide variety of visual computations. CWM has two key components, which resolve the core issues that have hindered application of the foundation model concept to vision. The first is structured masking, a generalization of masked prediction methods that encourages a prediction model to capture the low-dimensional structure in visual data. The model thereby factors the key physical components of a scene and exposes an interface to them via small sets of visual tokens. This in turn enables CWM's second main idea -- counterfactual prompting -- the observation that many apparently distinct visual representations can be computed, in a zero-shot manner, by comparing the prediction model's output on real inputs versus slightly modified (\"counterfactual\") inputs. We show that CWM generates high-quality readouts on real-world images and videos for a diversity of tasks, including estimation of keypoints, optical flow, occlusions, object segments, and relative depth. Taken together, our results show that CWM is a promising path to unifying the manifold strands of machine vision in a conceptually simple foundation.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Counterfactual World Modeling is introduced, a framework for constructing a visual foundation model: a unified, unsupervised network that can be prompted to perform a wide variety of visual computations and is a promising path to unifying the manifold strands of machine vision in a conceptually simple foundation."
            },
            "score": 3
        },
        {
            "id": "64cb695fef260e36c1d3d8830b197923f1e865ea",
            "paperId": "64cb695fef260e36c1d3d8830b197923f1e865ea",
            "title": "FP-DETR: Detection Transformer Advanced by Fully Pre-training",
            "abstract": "Large-scale pre-training has proven to be effective for visual representation learning on downstream tasks, especially for improving robustness and generalization. However, the recently developed detection transformers only employ pre-training on its backbone while leaving the key component, i.e., a 12-layer transformer, being trained from scratch, which prevents the model from above benefits. This separated training paradigm is mainly caused by the discrepancy between the upstream and downstream tasks. To mitigate the issue, we propose FP-DETR, a new method that Fully Pre-Trains an encoder-only transformer and smoothly finetunes it for object detection via a task adapter. Inspired by the success of textual prompts in NLP, we treat query positional embeddings as visual prompts to help the model attend to the target area (prompting) and recognize the object. To this end, we propose the task adapter which leverages self-attention to model the contextual relation between object query embedding. Experiments on the challenging COCO dataset demonstrate that our FP-DETR achieves competitive performance. Moreover, it enjoys better robustness to common corruptions and generalization to small-size datasets than state-of-the-art detection transformers. Code will be made publicly available at https://github.com/encounter1997/FP-DETR.",
            "year": 2022,
            "citationCount": 26,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes FP-DETR, a new method that Fully Pre-Trains an encoder-only transformer and smoothly finetunes it for object detection via a task adapter which leverages self-attention to model the contextual relation between object query embedding."
            },
            "score": 3
        },
        {
            "id": "77cb196c319265347da95529ac6ff1ff6b3ed436",
            "paperId": "77cb196c319265347da95529ac6ff1ff6b3ed436",
            "title": "Robust User Behavioral Sequence Representation via Multi-scale Stochastic Distribution Prediction",
            "abstract": "User behavior representation learned by self-supervised pre-training tasks is widely used in various domains and applications. Conventional methods usually follow the methodology in Natural Language Processing (NLP) to set the pre-training tasks. They either randomly mask some of the behaviors in the sequence and predict the masked ones or predict the next k behaviors. These methods fit for text sequence, in which the tokens are sequentially arranged subject to linguistic criterion. However, the user behavior sequences can be stochastic with noise and randomness. The same paradigm is intractable for learning a robust user behavioral representation. Though the next user behavior can be stochastic, the behavior distribution over a period of time is much more stable and less noisy. Based on this, we propose a Multi-scale Stochastic Distribution Prediction (MSDP) algorithm for learning robust user behavioral sequence representation. Instead of using predictions on concrete behavior as pre-training tasks, we take the prediction on user's behaviors distribution over a period of time as the self-supervision signal. Moreover, inspired by the recent success of the multi-task prompt training method on Large Language Models (LLM), we propose using the window size of the predicted time period as a prompt, enabling the model to learn user behavior representations that can be applied to prediction tasks across various future time periods. We generate different window size prompts through stochastic sampling. It effectively improves the generalization capability of the learned sequence representation. Extensive experiments demonstrate that our approach can learn robust user behavior representation successfully, which significantly outperforms state-of-the-art (SOTA) baselines.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Multi-scale Stochastic Distribution Prediction (MSDP) algorithm for learning robust user behavioral sequence representation that significantly outperforms state-of-the-art (SOTA) baselines and effectively improves the generalization capability of the learned sequence representation."
            },
            "score": 3
        },
        {
            "id": "3c38d98583bdfc0d02b618d94394161f65d4dc96",
            "paperId": "3c38d98583bdfc0d02b618d94394161f65d4dc96",
            "title": "Adapting LLMs for Efficient Context Processing through Soft Prompt Compression",
            "abstract": "The rapid advancement of Large Language Models (LLMs) has inaugurated a transformative epoch in natural language processing, fostering unprecedented proficiency in text generation, comprehension, and contextual scrutiny. Nevertheless, effectively handling extensive contexts, crucial for myriad applications, poses a formidable obstacle owing to the intrinsic constraints of the models' context window sizes and the computational burdens entailed by their operations. This investigation presents an innovative framework that strategically tailors LLMs for streamlined context processing by harnessing the synergies among natural language summarization, soft prompt compression, and augmented utility preservation mechanisms. Our methodology, dubbed SoftPromptComp, amalgamates natural language prompts extracted from summarization methodologies with dynamically generated soft prompts to forge a concise yet semantically robust depiction of protracted contexts. This depiction undergoes further refinement via a weighting mechanism optimizing information retention and utility for subsequent tasks. We substantiate that our framework markedly diminishes computational overhead and enhances LLMs' efficacy across various benchmarks, while upholding or even augmenting the caliber of the produced content. By amalgamating soft prompt compression with sophisticated summarization, SoftPromptComp confronts the dual challenges of managing lengthy contexts and ensuring model scalability. Our findings point towards a propitious trajectory for augmenting LLMs' applicability and efficiency, rendering them more versatile and pragmatic for real-world applications. This research enriches the ongoing discourse on optimizing language models, providing insights into the potency of soft prompts and summarization techniques as pivotal instruments for the forthcoming generation of NLP solutions.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This investigation presents an innovative framework that strategically tailors LLMs for streamlined context processing by harnessing the synergies among natural language summarization, soft prompt compression, and augmented utility preservation mechanisms."
            },
            "score": 3
        },
        {
            "id": "d18287d5ef8653aa1276a11957f2b3934c7c93e1",
            "paperId": "d18287d5ef8653aa1276a11957f2b3934c7c93e1",
            "title": "CodeAttack: Code-based Adversarial Attacks for Pre-Trained Programming Language Models",
            "abstract": "Pre-trained programming language (PL) models (such as CodeT5, CodeBERT, GraphCodeBERT, etc.,) have the potential to automate software engineering tasks involving code understanding and code generation. However, these models operate in the natural channel of code, i.e., primarily concerned with the human understanding of code. They are not robust to changes in the input and thus, are potentially susceptible to adversarial attacks in the natural channel. We propose, Code Attack, a simple yet effective black-box attack model that uses code structure to generate effective, efficient, and imperceptible adversarial code samples and demonstrates the vulnerabilities of the state-of-the-art PL models to code-specific adversarial attacks. We evaluate the transferability of CodeAttack on several code-code (translation and repair) and code-NL (summarization) tasks across different programming languages. Code Attack outperforms state-of-the-art adversarial NLP attack models to achieve the best overall drop in performance while being more efficient, imperceptible, consistent, and fluent. The code can be found at https://github.com/reddy-lab-code-research/CodeAttack.",
            "year": 2022,
            "citationCount": 26,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes, Code Attack, a simple yet effective black-box attack model that uses code structure to generate effective, efficient, and imperceptible adversarial code samples and demonstrates the vulnerabilities of the state-of-the-art PL models to code-specific adversarial attacks."
            },
            "score": 2
        },
        {
            "id": "8af2bffa267eeb4a3209c23dd3bbf5e7d4982809",
            "paperId": "8af2bffa267eeb4a3209c23dd3bbf5e7d4982809",
            "title": "BabySLM: language-acquisition-friendly benchmark of self-supervised spoken language models",
            "abstract": "Self-supervised techniques for learning speech representations have been shown to develop linguistic competence from exposure to speech without the need for human labels. In order to fully realize the potential of these approaches and further our understanding of how infants learn language, simulations must closely emulate real-life situations by training on developmentally plausible corpora and benchmarking against appropriate test sets. To this end, we propose a language-acquisition-friendly benchmark to probe spoken language models at the lexical and syntactic levels, both of which are compatible with the vocabulary typical of children's language experiences. This paper introduces the benchmark and summarizes a range of experiments showing its usefulness. In addition, we highlight two exciting challenges that need to be addressed for further progress: bridging the gap between text and speech and between clean speech and in-the-wild speech.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces the benchmark and summarizes a range of experiments showing its usefulness, and highlights two exciting challenges that need to be addressed: bridging the gap between text and speech and between clean speech and in-the-wild speech."
            },
            "score": 2
        },
        {
            "id": "fb08b08916caab4b22c60fa96753f6b9a5886d75",
            "paperId": "fb08b08916caab4b22c60fa96753f6b9a5886d75",
            "title": "MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training",
            "abstract": "Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is partially due to the distinctive challenges associated with modelling musical knowledge, particularly tonal and pitched characteristics of music. To address this research gap, we propose an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. In our exploration, we identified an effective combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantisation - Variational AutoEncoder (RVQ-VAE) and a musical teacher based on the Constant-Q Transform (CQT). Furthermore, we explore a wide range of settings to overcome the instability in acoustic language model pre-training, which allows our designed paradigm to scale from 95M to 330M parameters. Experimental results indicate that our model can generalise and perform well on 14 music understanding tasks and attain state-of-the-art (SOTA) overall scores.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training, which outperforms conventional speech and audio approaches in terms of performance."
            },
            "score": 2
        },
        {
            "id": "7a064df1aeada7e69e5173f7d4c8606f4470365b",
            "paperId": "7a064df1aeada7e69e5173f7d4c8606f4470365b",
            "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
            "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at this https URL.",
            "year": 2019,
            "citationCount": 5297,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT, and uses a self-supervised loss that focuses on modeling inter-sentence coherence."
            },
            "score": 2
        },
        {
            "id": "1814b4304f963ac749b745066be4488d50197b8e",
            "paperId": "1814b4304f963ac749b745066be4488d50197b8e",
            "title": "Multilingual Text-To-Speech Training Using Cross Language Voice Conversion And Self-Supervised Learning Of Speech Representations",
            "abstract": "State of the art text-to-speech (TTS) models can generate high fidelity monolingual speech, but it is still challenging to synthesize multilingual speech from the same speaker. One major hurdle is for training data. It\u2019s hard to find speakers who have native proficiency in several languages. One way of mitigating this issue is by generating polyglot corpus through voice conversion. In this paper, we train such multilingual TTS system through a novel cross-lingual voice conversion model trained with speaker-invariant features extracted from a speech representation model which is pre-trained with 53 languages through self-supervised learning [1]. To further improve the speaker identity shift, we also adopt a speaker similarity loss term during training. We then use this model to convert multilingual multi-speaker speech data to the voice of the target speaker. Through augmenting data from 4 other languages, we train a multilingual TTS system for a native monolingual English speaker which speaks 5 languages(English, French, German, Italian and Spanish). Our system achieves improved mean opinion score (MOS) compared with the baseline of multi-speaker system for all languages, specifically: 3.74 vs 3.62 for Spanish, 3.11 vs 2.71 for German, 3.47 vs 2.84 for Italian, and 2.72 vs 2.41 for French.",
            "year": 2022,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel cross-lingual voice conversion model trained with speaker-invariant features extracted from a speech representation model which is pre-trained with 53 languages through self-supervised learning is used to convert multilingual multi-speaker speech data to the voice of the target speaker."
            },
            "score": 2
        },
        {
            "id": "0fcc797aeed56bcc127476d5b68836dc402021ea",
            "paperId": "0fcc797aeed56bcc127476d5b68836dc402021ea",
            "title": "SignBERT+: Hand-Model-Aware Self-Supervised Pre-Training for Sign Language Understanding",
            "abstract": "Hand gesture serves as a crucial role during the expression of sign language. Current deep learning based methods for sign language understanding (SLU) are prone to over-fitting due to insufficient sign data resource and suffer limited interpretability. In this paper, we propose the first self-supervised pre-trainable SignBERT+ framework with model-aware hand prior incorporated. In our framework, the hand pose is regarded as a visual token, which is derived from an off-the-shelf detector. Each visual token is embedded with gesture state and spatial-temporal position encoding. To take full advantage of current sign data resource, we first perform self-supervised learning to model its statistics. To this end, we design multi-level masked modeling strategies (joint, frame and clip) to mimic common failure detection cases. Jointly with these masked modeling strategies, we incorporate model-aware hand prior to better capture hierarchical context over the sequence. After the pre-training, we carefully design simple yet effective prediction heads for downstream tasks. To validate the effectiveness of our framework, we perform extensive experiments on three main SLU tasks, involving isolated and continuous sign language recognition (SLR), and sign language translation (SLT). Experimental results demonstrate the effectiveness of our method, achieving new state-of-the-art performance with a notable gain.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes the first self-supervised pre-trainable SignBERT+ framework with model-aware hand prior incorporated, and achieves new state-of-the-art performance with a notable gain."
            },
            "score": 2
        },
        {
            "id": "ebe259796870ebccf26577044d0087884209b884",
            "paperId": "ebe259796870ebccf26577044d0087884209b884",
            "title": "w2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training",
            "abstract": "Motivated by the success of masked language modeling (MLM) in pre-training natural language processing models, we propose w2v-BERT that explores MLM for self-supervised speech representation learning. w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens. In contrast to existing MLM-based speech pre-training frameworks such as HuBERT, which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can be optimized in an end-to-end fashion by solving the two self-supervised tasks (the contrastive task and MLM) simultaneously. Our experiments show that w2v-BERT achieves competitive results compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light 60k corpus as the unsupervised data. In particular, when compared to published models such as conformer-based wav2vec 2.0 and HuBERT, our model shows 5% to 10% relative WER reduction on the test-clean and test-other subsets. When applied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our internal conformer-based wav2vec 2.0 by more than 30% relatively.",
            "year": 2021,
            "citationCount": 255,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 2
        },
        {
            "id": "043d4663aa8e13cc66537f591dc6350e78f453ab",
            "paperId": "043d4663aa8e13cc66537f591dc6350e78f453ab",
            "title": "CLIP-S4: Language-Guided Self-Supervised Semantic Segmentation",
            "abstract": "Existing semantic segmentation approaches are often limited by costly pixel-wise annotations and predefined classes. In this work, we present CLIP-S4 that leverages self-supervised pixel representation learning and vision-language models to enable various semantic segmentation tasks (e.g., unsupervised, transfer learning, language-driven segmentation) without any human annotations and unknown class information. We first learn pixel embeddings with pixel-segment contrastive learning from different augmented views of images. To further improve the pixel embeddings and enable language-driven semantic segmentation, we design two types of consistency guided by vision-language models: 1) embedding consistency, aligning our pixel embeddings to the joint feature space of a pre-trained vision-language model, CLIP [34]; and 2) semantic consistency, forcing our model to make the same predictions as CLIP over a set of carefully designed target classes with both known and unknown prototypes. Thus, CLIP-S4 enables a new task of class-free semantic segmentation where no unknown class information is needed during training. As a result, our approach shows consistent and substantial performance improvement over four popular benchmarks compared with the state-of-the-art unsupervised and language-driven semantic segmentation methods. More importantly, our method outperforms these methods on unknown class recognition by a large margin.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents CLIP-S4, a new task of class-free semantic segmentation where no unknown class information is needed during training, and shows consistent and substantial performance improvement over four popular benchmarks compared with the state-of-the-art unsupervised and language-driven semantic segmentsation methods."
            },
            "score": 2
        },
        {
            "id": "43b54d2a1e836eeb12cf29a42e9beae2c7e26b8d",
            "paperId": "43b54d2a1e836eeb12cf29a42e9beae2c7e26b8d",
            "title": "Self-Supervised Category-Level 6D Object Pose Estimation With Optical Flow Consistency",
            "abstract": "Category-level 6D object pose estimation aims at determining the pose of an object of a given category. Most current state-of-the-art methods require a significant amount of real training data to supervise their models. Moreover, annotating the 6D pose is very time consuming, error-prone, and it does not scale well to a large amount of object classes. Therefore, a handful of methods have recently been proposed to use unlabelled data to establish weak supervision. In this letter we propose a self-supervised method that leverages the 2D optical flow as a proxy for supervising the 6D pose. To this purpose, we estimate the 2D optical flow between consecutive frames based on the pose estimation. Then, we harness an off-the-shelf optical flow method to enable weak supervision using a 2D-3D optical flow based consistency loss. Experiments show that our approach for self-supervised learning yields state-of-the-art performance on the NOCS benchmark, and it reaches comparable results with some fully-supervised approaches.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This letter proposes a self-supervised method that leverages the 2D optical flow as a proxy for supervising the 6D pose and harnesses an off-the-shelf optical flow method to enable weak supervision using a 2D-3D optical Flow based consistency loss."
            },
            "score": 2
        },
        {
            "id": "29bb6aa322108652f6e7f71557ef57d389d5a42d",
            "paperId": "29bb6aa322108652f6e7f71557ef57d389d5a42d",
            "title": "Training Robust Zero-Shot Voice Conversion Models with Self-Supervised Features",
            "abstract": "Unsupervised Zero-Shot Voice Conversion (VC) aims to modify the speaker characteristic of an utterance to match an unseen target speaker without relying on parallel training data. Recently, self-supervised learning of speech representation has been shown to produce useful linguistic units without using transcripts, which can be directly passed to a VC model. In this paper, we showed that high-quality audio samples can be achieved by using a length resampling decoder, which enables the VC model to work in conjunction with different linguistic feature extractors and vocoders without requiring them to operate on the same sequence length. We showed that our method can outperform many baselines on the VCTK dataset. Without modifying the architecture, we further demonstrated that a) using pairs of different audio segments from the same speaker, b) adding a cycle consistency loss, and c) adding a speaker classification loss can help to learn a better speaker embedding. Our model trained on LibriTTS using these techniques achieves the best performance, producing audio samples transferred well to the target speaker\u2019s voice, while preserving the linguistic content that is comparable with actual human utterances in terms of Character Error Rate.",
            "year": 2021,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that high-quality audio samples can be achieved by using a length resampling decoder, which enables the VC model to work in conjunction with different linguistic feature extractors and vocoders without requiring them to operate on the same sequence length."
            },
            "score": 2
        },
        {
            "id": "b3b734e97ccdb301e1d5da6a7ab5948e674fe30d",
            "paperId": "b3b734e97ccdb301e1d5da6a7ab5948e674fe30d",
            "title": "Intrinsic Security: A Robust Framework for Cloud-Native Network Slicing via a Proactive Defense Paradigm",
            "abstract": "Opening-up sharing has prompted the multi-tenancy architecture, whereby different vendors (including outsourcees) work together with network operators to form a vibrant service ecosystem, resulting in several advantages as well as risks. In particular, the static nature of existing architectures in network functions virtualization-based (NFV-based) clouds facilitate hacking. Thus, much attention has been focused on determining how to avoid the uncontrollable cloud security induced by complex production relations and non-trustworthy software/hardware sources when the two sets of security risks intersect. In this article, we investigate latent persistent threats against cloud environments and determine a high degree of complementarity and consistency between the NFV-based cloud environment and the dynamic defense concept. More specifically, new NFV-based cloud features provide an effective implementation for dynamic defense, while the generalized robustness of dynamic defense theory allows for high security gains. Intrinsic cloud security (iCS) is then proposed to align NFV-based clouds, mimicking defense and the moving target defense (MTD) paradigm to implement a seamless integration and symbiosis evolution between security and NFV-based clouds. We quantify the impact on system overhead to account for efficiency and cost issues. The simulation analysis demonstrates that the enhanced mode is able to consistently obtain a more beneficial and stable defense compared with the counterparts.",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This article investigates latent persistent threats against cloud environments and determines a high degree of complementarity and consistency between the NFV-based cloud environment and the dynamic defense concept, and proposes a seamless integration and symbiosis evolution between security and NFv-based clouds."
            },
            "score": 2
        },
        {
            "id": "914a0f5e7eb98842f220a5082dba4f9382086f27",
            "paperId": "914a0f5e7eb98842f220a5082dba4f9382086f27",
            "title": "Language-Oriented Communication with Semantic Coding and Knowledge Distillation for Text-to-Image Generation",
            "abstract": "By integrating recent advances in large language models (LLMs) and generative models into the emerging semantic communication (SC) paradigm, in this article we put forward to a novel framework of language-oriented semantic communication (LSC). In LSC, machines communicate using human language messages that can be interpreted and manipulated via natural language processing (NLP) techniques for SC efficiency. To demonstrate LSC's potential, we introduce three innovative algorithms: 1) semantic source coding (SSC) which compresses a text prompt into its key head words capturing the prompt's syntactic essence while maintaining their appearance order to keep the prompt's context; 2) semantic channel coding (SCC) that improves robustness against errors by substituting head words with their lenghthier synonyms; and 3) semantic knowledge distillation (SKD) that produces listener-customized prompts via in-context learning the listener's language style. In a communication task for progressive text-to-image generation, the proposed methods achieve higher perceptual similarities with fewer transmissions while enhancing robustness in noisy communication channels.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Three innovative algorithms are introduced that achieve higher perceptual similarities with fewer transmissions while enhancing robustness in noisy communication channels and produces listener-customized prompts via in-context learning the listener's language style."
            },
            "score": 2
        },
        {
            "id": "1ef8c59d615f8416f7039428d4416bd5eafb3748",
            "paperId": "1ef8c59d615f8416f7039428d4416bd5eafb3748",
            "title": "Cybersecurity of Autonomous Vehicles: A Systematic Literature Review of Adversarial Attacks and Defense Models",
            "abstract": "Autonomous driving (AD) has developed tremendously in parallel with the ongoing development and improvement of deep learning (DL) technology. However, the uptake of artificial intelligence (AI) in AD as the core enabling technology raises serious cybersecurity issues. An enhanced attack surface has been spurred on by the rising digitization of vehicles and the integration of AI features. The performance of the autonomous vehicle (AV)-based applications is constrained by the DL models' susceptibility to adversarial attacks despite their great potential. Hence, AI-enabled AVs face numerous security threats, which prevent the large-scale adoption of AVs. Therefore, it becomes crucial to evolve existing cybersecurity practices to deal with risks associated with the increased uptake of AI. Furthermore, putting defense models into practice against adversarial attacks has grown in importance as a field of study amongst researchers. Therefore, this study seeks to provide an overview of the most recent adversarial defensive and attack models developed in the domain of AD.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An overview of the most recent adversarial defensive and attack models developed in the domain of AD is provided to help evolve existing cybersecurity practices to deal with risks associated with the increased uptake of AI."
            },
            "score": 1
        },
        {
            "id": "4fb6a231d1aaaec0705884852022c0a3c6eb0952",
            "paperId": "4fb6a231d1aaaec0705884852022c0a3c6eb0952",
            "title": "Smart Agriculture: Enhancing Security Through Animal Detection Via Deep Learning and Computer Vision",
            "abstract": "Agriculture stands as a crucial sector, making significant contributions to the economies of many countries. Nevertheless, it encounters various challenges, one of which is animal disruption. This poses a considerable threat to crops, leading to financial losses for farmers. In response to this concern, we have engineered an animal disruption warning system for agricultural settings based on YOLOv6 technology.The system operates by analyzing live video feeds from strategically placed cameras. Utilizing deep learning algorithms, it can detect and classify animals in real-time. The computer vision algorithms enable tracking and prediction of animal movements. Upon detection, the system promptly sends alerts, enabling timely and appropriate actions.In this paper, we periodically monitor the entire farm through a camera that continuously records its surroundings. The identification of animal entry is achieved using a deep learning model, and alarm systems serve as a deterrent, notifying forest officials. This report provides details on the libraries and convolutional neural networks employed in constructing the model.This research focuses on the implementation of a robust animal detection system in agricultural environments, leveraging the capabilities of deep learning. The project utilizes state-of-the-art deep neural networks and computer vision algorithms to analyze live video feeds from strategically positioned cameras across the farm. The deep learning model is trained to detect and classify various animals in real-time, contributing to the early identification of potential threats to crops.The system employs sophisticated computer vision techniques, enabling accurate tracking and prediction of animal movements within the monitored areas. Upon detection, the system triggers timely alerts, providing farmers with the necessary information to take swift and appropriate actions, thereby mitigating potential damage to crops.To achieve these objectives, the project involves periodic monitoring of the entire farm through a camera that continuously records its surroundings. The deep learning model, supported by alarm systems, effectively identifies animal entries, serving as a proactive deterrent. This research report outlines the libraries, frameworks, and convolutional neural networks employed in the development of the animal detection model, shedding light on the technical aspects of its implementation.The integration of deep learning and computer vision in agriculture not only enhances crop protection but also contributes to the sustainable and efficient management of farming practices. This research offers insights into the potential of advanced technologies to address challenges in agriculture and opens avenues for further exploration in the intersection of technology and agriculture. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An animal disruption warning system for agricultural settings based on YOLOv6 technology is engineered, which effectively identifies animal entries, serving as a proactive deterrent and opens avenues for further exploration in the intersection of technology and agriculture."
            },
            "score": 1
        },
        {
            "id": "0e4297cd0bd5d44fce0b3671e1fe40e813995462",
            "paperId": "0e4297cd0bd5d44fce0b3671e1fe40e813995462",
            "title": "A Comprehensive IoT-Based Bike Crash Detection and Emergency Response System for Enhanced Road Safety",
            "abstract": "Motorcycle travel, deemed one of the riskiest modes of transportation, faces a staggering fatality rate, with 212.7 deaths for every million miles travelled. Unlike enclosed vehicles, motorcycles expose riders to their surroundings, heightening the need for proactive safety measures. This paper explores the development and implementation of a Motorcycle Crash Detection and Alert System (MCDAS) utilizing the Multi-axes accelerometer. The system is designed to detect when a motorcycle falls and promptly alert emergency services and contacts via Firebase cloud. The study delves into the challenges posed by two-wheeled vehicle dynamics and outlines a novel collision detection algorithm tailored for motorcycles. This algorithm is validated against experimental evidence, providing a robust foundation for crash detection systems in this unique context. Additionally, the paper introduces a novel method for vehicle detection and tracking, employing thresholding, mathematical morphology treatment, and original labelling to enhance accuracy and reduce artifacts. In the context of road accidents, especially prevalent among two-wheelers, the paper presents a system that combines accelerometer data and a user's heartbeat sensor to assess the severity of accidents. This timely information triggers alerts to nearby medical centers and contacts through GSM and GPS modules, facilitating immediate medical aid. The Android application associated with the system not only sends text messages to medical centers and friends but also shares the precise accident location, significantly reducing response time. important contributions and heralds a new departure for the vehicle safety paradigm.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The study delves into the challenges posed by two-wheeled vehicle dynamics and outlines a novel collision detection algorithm tailored for motorcycles, providing a robust foundation for crash detection systems in this unique context."
            },
            "score": 1
        },
        {
            "id": "218a1c0f7138c2d0d01d9f7dbf62c6faba6e76f7",
            "paperId": "218a1c0f7138c2d0d01d9f7dbf62c6faba6e76f7",
            "title": "Pothole Sensing System for vehicles",
            "abstract": "Potholes are a serious hazard to motorcyclists and often lead to accidents and injuries. To solve this problem, the proposed system is designed to proactively identify potholes on the road surface and provide the driver with an early warning, allowing them to safely navigate and avoid potential hazards. This system aims to develop a robust pothole detection system for vehicles using advanced sensing technology, including ultrasonic sensors, gyroscope sensors and Global Positioning System. The system uses state-of-the-art sensor technology mounted on the vehicle, strategically placed to sense the road ahead. Whether using ultrasonic sensors, radar systems or microwaves, each technology offers unique advantages in terms of range, accuracy and environmental resistance. By continuously monitoring the road surface, the sensors can detect deviations indicating potholes regardless of lighting conditions. Upon detecting a pothole, the system activates the alarms integrated into the motorcycle interface and alerts the rider via visual or audio signals. These warnings prompt the rider to adjust their speed or maneuver, minimizing the risk of accidents or bike damage. In addition, the system can be extended with a data logging capability to record the location and characteristics of potholes, facilitating proactive road maintenance and infrastructure improvements. In conclusion, the proposed pothole detection system provides a proactive solution to reduce the risks associated with the occurrence of potholes in driving. Utilizing advanced sensor technology and intuitive warning mechanisms, the system allows riders to navigate confidently and safely, contributing to a safer and more enjoyable riding experience. Key Word: Pothole; Ultrasonic; Safety; Road; Vehicle.",
            "year": 2024,
            "citationCount": 0,
            "tldr": null,
            "score": 1
        }
    ],
    "novelty": "yes"
}