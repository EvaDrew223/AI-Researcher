{
    "topic_description": "novel prompting methods to improve large language models' robustness against adversarial attacks and improve their security and privacy",
    "idea_name": "Contextual Prompting for Detecting Adversarial Triggers",
    "raw_idea": {
        "Problem": "Adversarial attacks on language models often involve inserting subtle triggers or patterns into the input prompt that can manipulate the model's output without being easily detectable by humans. Existing defense methods often rely on training a separate model to detect such triggers, which can be costly and time-consuming.",
        "Existing Methods": "Current approaches for detecting adversarial triggers include using a separate classifier model trained on adversarial examples, adversarial training to improve the model's robustness, and human-in-the-loop detection of suspicious patterns.",
        "Motivation": "We propose a novel prompting method that can help language models detect and highlight potential adversarial triggers in the input prompt, without requiring any additional training or model components. By leveraging the model's own knowledge and attention mechanisms, we can make it more self-aware and robust against adversarial attacks.",
        "Proposed Method": "Our method, called Contextual Prompting for Detecting Adversarial Triggers (CPAT), works by augmenting the input prompt with a series of sub-prompts that are designed to probe the model's understanding of the context and its attention to different parts of the input. These sub-prompts can include questions that ask the model to summarize the main points of the prompt, identify any irrelevant or suspicious elements, and explain the reasoning behind its output. By comparing the model's responses to these sub-prompts with its original output, we can detect any inconsistencies or anomalies that may indicate the presence of adversarial triggers. The model can then highlight these potential triggers in the input and provide a warning to the user or a downstream system.",
        "Experiment Plan": "We will evaluate CPAT on various language modeling tasks that are vulnerable to adversarial attacks, such as sentiment analysis and text classification. We will measure the model's ability to detect and highlight adversarial triggers in the input, using metrics such as precision, recall, and F1 score. We will compare CPAT with baseline methods that use separate detector models or adversarial training, and conduct human evaluations to assess the effectiveness and interpretability of the model's explanations. We will also analyze the trade-off between the number and complexity of sub-prompts and the model's detection performance and efficiency."
    },
    "full_experiment_plan": {
        "Title": "Contextual Prompting for Detecting Adversarial Triggers in Language Models",
        "Problem Statement": "Adversarial attacks on language models often involve inserting subtle triggers or patterns into the input prompt that can manipulate the model's output without being easily detectable by humans. Existing defense methods often rely on training a separate model to detect such triggers, which can be costly and time-consuming.",
        "Motivation": "Current approaches for detecting adversarial triggers, such as using a separate classifier model trained on adversarial examples, adversarial training to improve the model's robustness, and human-in-the-loop detection of suspicious patterns, have limitations. They require additional training data, model components, or human effort. We propose a novel prompting method that leverages the language model's own knowledge and attention mechanisms to detect and highlight potential adversarial triggers in the input prompt, without requiring any additional training or model components. By making the model more self-aware and robust against adversarial attacks, we aim to improve its security and privacy.",
        "Proposed Method": "Our method, called Contextual Prompting for Detecting Adversarial Triggers (CPAT), works by augmenting the input prompt with a series of sub-prompts designed to probe the model's understanding of the context and its attention to different parts of the input. These sub-prompts can include questions that ask the model to summarize the main points of the prompt, identify any irrelevant or suspicious elements, and explain the reasoning behind its output. By comparing the model's responses to these sub-prompts with its original output, we can detect any inconsistencies or anomalies that may indicate the presence of adversarial triggers. The model can then highlight these potential triggers in the input and provide a warning to the user or a downstream system.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Dataset Selection": "Select various language modeling tasks that are vulnerable to adversarial attacks, such as sentiment analysis (e.g., SST-2, IMDB), text classification (e.g., AG News, DBpedia), and natural language inference (e.g., SNLI, MNLI). For each task, prepare a dataset with a mix of clean and adversarial examples.",
            "Step 2: Baseline Models": "Choose state-of-the-art language models suitable for the selected tasks, such as BERT, RoBERTa, or GPT-3. Fine-tune these models on the clean examples for each task and evaluate their performance on both clean and adversarial test sets to establish baseline results.",
            "Step 3: Adversarial Trigger Generation": "Create a set of adversarial triggers for each task using techniques such as character-level perturbations, word substitutions, or syntactic modifications. These triggers should be designed to manipulate the model's output without significantly altering the semantic meaning of the input.",
            "Step 4: CPAT Implementation": "Implement the CPAT method by defining a set of sub-prompts for each task. These sub-prompts should be designed to probe the model's understanding of the context and its attention to different parts of the input. Examples of sub-prompts include:\n- Summarize the main points of the input text.\n- Identify any irrelevant or suspicious elements in the input.\n- Explain the reasoning behind your output.\n- Highlight the parts of the input that most influenced your decision.",
            "Step 5: CPAT Evaluation": "Apply the CPAT method to the fine-tuned language models from Step 2. For each input example (clean and adversarial), generate the model's original output and its responses to the sub-prompts. Compare the responses to detect inconsistencies or anomalies that may indicate the presence of adversarial triggers. Record the model's ability to detect and highlight potential triggers using metrics such as precision, recall, and F1 score.",
            "Step 6: Baseline Comparison": "Compare the performance of CPAT with baseline methods that use separate detector models or adversarial training. Evaluate the trade-off between the number and complexity of sub-prompts and the model's detection performance and efficiency.",
            "Step 7: Human Evaluation": "Conduct human evaluations to assess the effectiveness and interpretability of the model's explanations generated through CPAT. Ask human annotators to rate the quality and usefulness of the model's responses to the sub-prompts in detecting adversarial triggers."
        },
        "Test Case Examples": {
            "Test Case 1": {
                "Input": "This movie was terrible. The acting was bad and the plot made no sense. I would not recommend it to anyone.",
                "Adversarial Trigger": "This movie was terrible. The acting was bad and the plot made no sense. I would not recommend it to anyone. \u202e.eno yna ot ti dnemmocer ton dluow I",
                "Baseline Output": "Positive sentiment",
                "CPAT Sub-Prompts and Outputs": [
                    {
                        "Sub-Prompt": "Summarize the main points of the input text.",
                        "Output": "The movie was terrible, with bad acting and a nonsensical plot. The reviewer does not recommend the movie to anyone."
                    },
                    {
                        "Sub-Prompt": "Identify any irrelevant or suspicious elements in the input.",
                        "Output": "The last sentence of the input text appears to be a reversed version of the previous sentence, which is suspicious and irrelevant to the review."
                    },
                    {
                        "Sub-Prompt": "Explain the reasoning behind your output.",
                        "Output": "Based on the negative language used in the review, such as 'terrible,' 'bad,' and 'made no sense,' the sentiment of the text is clearly negative. However, the presence of the reversed sentence at the end raises suspicion about the integrity of the input."
                    }
                ],
                "CPAT Output": "Negative sentiment. Warning: Potential adversarial trigger detected in the input text.",
                "Explanation": "The baseline model is fooled by the adversarial trigger, classifying the sentiment as positive despite the clearly negative review. CPAT, on the other hand, detects the suspicious reversed sentence and correctly classifies the sentiment as negative while warning about the potential adversarial trigger."
            },
            "Test Case 2": {
                "Input": "The restaurant had excellent food and service. The staff was friendly and attentive, and the prices were reasonable. I would definitely visit again.",
                "Adversarial Trigger": "The restaurant had excellent food and service. The staff was friendly and attentive, and the prices were reasonable. I would definitely visit again. \u202e.niaga tisiv yletinifed dluow I",
                "Baseline Output": "Negative sentiment",
                "CPAT Sub-Prompts and Outputs": [
                    {
                        "Sub-Prompt": "Summarize the main points of the input text.",
                        "Output": "The restaurant provided excellent food and service, with friendly staff and reasonable prices. The reviewer would definitely visit the restaurant again."
                    },
                    {
                        "Sub-Prompt": "Identify any irrelevant or suspicious elements in the input.",
                        "Output": "The last sentence of the input text is a reversed version of the previous sentence, which is suspicious and irrelevant to the review."
                    },
                    {
                        "Sub-Prompt": "Explain the reasoning behind your output.",
                        "Output": "The input text uses positive language such as 'excellent,' 'friendly,' and 'reasonable,' indicating a positive sentiment towards the restaurant. However, the presence of the reversed sentence at the end suggests that the input may have been tampered with."
                    }
                ],
                "CPAT Output": "Positive sentiment. Warning: Potential adversarial trigger detected in the input text.",
                "Explanation": "The baseline model misclassifies the sentiment as negative due to the adversarial trigger. CPAT identifies the suspicious reversed sentence and correctly classifies the sentiment as positive while raising a warning about the potential adversarial trigger."
            }
        },
        "Fallback Plan": "If the proposed CPAT method does not significantly improve the detection of adversarial triggers compared to the baselines, consider the following alternative plans:\n1. Analyze the sub-prompts and their responses to identify potential weaknesses or areas for improvement. Modify the sub-prompts or create new ones to better probe the model's understanding of the context and its attention to different parts of the input.\n2. Investigate the effectiveness of CPAT on different types of adversarial triggers and attack methods. Adapt the sub-prompts to target specific types of triggers or develop new prompting strategies tailored to each attack method.\n3. Explore the combination of CPAT with other defense mechanisms, such as adversarial training or input preprocessing techniques, to create a more comprehensive and robust defense system.\n4. If the CPAT method proves ineffective, focus on analyzing the reasons behind its failure and the limitations of prompting-based defense methods. Turn the project into an analysis paper that provides insights into the challenges of detecting adversarial triggers in language models and proposes potential directions for future research."
    },
    "novelty_queries": [
        "KeywordQuery(\"contextual prompting adversarial triggers language models\")",
        "KeywordQuery(\"detecting adversarial attacks language models\")",
        "KeywordQuery(\"self-aware language models adversarial robustness\")",
        "KeywordQuery(\"Contextual Prompting for Detecting Adversarial Triggers NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "1ab91d6ac7afc1a0121487a9089fa70edc1634d4",
            "paperId": "1ab91d6ac7afc1a0121487a9089fa70edc1634d4",
            "title": "Certifying LLM Safety against Adversarial Prompting",
            "abstract": "Large language models (LLMs) are vulnerable to adversarial attacks that add malicious tokens to an input prompt to bypass the safety guardrails of an LLM and cause it to produce harmful content. In this work, we introduce erase-and-check, the first framework for defending against adversarial prompts with certifiable safety guarantees. Given a prompt, our procedure erases tokens individually and inspects the resulting subsequences using a safety filter. Our safety certificate guarantees that harmful prompts are not mislabeled as safe due to an adversarial attack up to a certain size. We implement the safety filter in two ways, using Llama 2 and DistilBERT, and compare the performance of erase-and-check for the two cases. We defend against three attack modes: i) adversarial suffix, where an adversarial sequence is appended at the end of a harmful prompt; ii) adversarial insertion, where the adversarial sequence is inserted anywhere in the middle of the prompt; and iii) adversarial infusion, where adversarial tokens are inserted at arbitrary positions in the prompt, not necessarily as a contiguous block. Our experimental results demonstrate that this procedure can obtain strong certified safety guarantees on harmful prompts while maintaining good empirical performance on safe prompts. Additionally, we propose three efficient empirical defenses: i) RandEC, a randomized subsampling version of erase-and-check; ii) GreedyEC, which greedily erases tokens that maximize the softmax score of the harmful class; and iii) GradEC, which uses gradient information to optimize tokens to erase. We demonstrate their effectiveness against adversarial prompts generated by the Greedy Coordinate Gradient (GCG) attack algorithm. The code for our experiments is available at https://github.com/aounon/certified-llm-safety.",
            "year": 2023,
            "citationCount": 48,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "E erase-and-check, the first framework for defending against adversarial prompts with certifiable safety guarantees, is introduced and it is demonstrated that this procedure can obtain strong certified safety guarantees on harmful prompts while maintaining good empirical performance on safe prompts."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to detect adversarial triggers in language models using contextual prompting, while the paper focuses on certifying the safety of language models against adversarial prompts by erasing and checking tokens.\n\nProject proposal: Detect adversarial triggers in language models using contextual prompting without additional training or model components.\nPaper: Certify the safety of language models against adversarial prompts by erasing tokens and checking the resulting subsequences using a safety filter.\n\nThe two works differ in their objectives and approaches. The project proposal aims to detect adversarial triggers, while the paper focuses on certifying safety against adversarial prompts. The project uses contextual prompting, while the paper employs an erase-and-check framework.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "c1d1842e08716cbf7250167969522a1705d8bcd3",
            "paperId": "c1d1842e08716cbf7250167969522a1705d8bcd3",
            "title": "Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information",
            "abstract": "In recent years, Large Language Models (LLM) have emerged as pivotal tools in various applications. However, these models are susceptible to adversarial prompt attacks, where attackers can carefully curate input strings that mislead LLMs into generating incorrect or undesired outputs. Previous work has revealed that with relatively simple yet effective attacks based on discrete optimization, it is possible to generate adversarial prompts that bypass moderation and alignment of the models. This vulnerability to adversarial prompts underscores a significant concern regarding the robustness and reliability of LLMs. Our work aims to address this concern by introducing a novel approach to detecting adversarial prompts at a token level, leveraging the LLM's capability to predict the next token's probability. We measure the degree of the model's perplexity, where tokens predicted with high probability are considered normal, and those exhibiting high perplexity are flagged as adversarial. Additionaly, our method also integrates context understanding by incorporating neighboring token information to encourage the detection of contiguous adversarial prompt sequences. To this end, we design two algorithms for adversarial prompt detection: one based on optimization techniques and another on Probabilistic Graphical Models (PGM). Both methods are equipped with efficient solving methods, ensuring efficient adversarial prompt detection. Our token-level detection result can be visualized as heatmap overlays on the text sequence, allowing for a clearer and more intuitive representation of which part of the text may contain adversarial prompts.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a novel approach to detecting adversarial prompts at a token level, leveraging the LLM's capability to predict the next token's probability, and designs two algorithms for adversarial prompt detection, one based on optimization techniques and another on Probabilistic Graphical Models."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is detecting adversarial triggers in language models, and the proposed approach is using contextual prompting to probe the model's understanding and attention to identify potential triggers.\n\nThe research problem in the paper is also detecting adversarial prompts in language models, but the proposed approach is measuring the perplexity of tokens and incorporating contextual information to identify adversarial tokens.\n\nWhile both the proposal and the paper aim to detect adversarial attacks on language models, their approaches differ. The proposal uses prompting to probe the model's understanding, while the paper uses perplexity measures and contextual information at the token level.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "a2bcacc8fefb859c94c69d524b2368bb4792f9b1",
            "paperId": "a2bcacc8fefb859c94c69d524b2368bb4792f9b1",
            "title": "Adversarial Prompting for Black Box Foundation Models",
            "abstract": "Prompting interfaces allow users to quickly adjust the output of generative models in both vision and language. However, small changes and design choices in the prompt can lead to signi\ufb01cant differences in the output. In this work, we develop a black-box framework for generating adversarial prompts for unstructured image and text generation. These prompts, which can be standalone or prepended to benign prompts, induce speci\ufb01c behaviors into the generative process, such as generating images of a particular object or biasing the frequency of speci\ufb01c letters in the generated text.",
            "year": 2023,
            "citationCount": 37,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work develops a black-box framework for generating adversarial prompts for unstructured image and text generation and induces speci\ufb01c behaviors into the generative process, such as generating images of a particular object or biasing the frequency of speci \u2122 letters in the generated text."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is detecting adversarial triggers in language models using prompting methods, while the paper focuses on generating adversarial prompts for image and text generation in black-box models.\n\nProposal: Detecting adversarial triggers in language models using contextual prompting.\nPaper: Generating adversarial prompts for black-box image and text generation models.\n\nThe proposal aims to defend against adversarial attacks, while the paper focuses on creating adversarial prompts. The proposal targets language models, while the paper targets both image and text generation models.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "6c489a9dc649298aea729c91822a3c89de503729",
            "paperId": "6c489a9dc649298aea729c91822a3c89de503729",
            "title": "Black Box Adversarial Prompting for Foundation Models",
            "abstract": "Prompting interfaces allow users to quickly adjust the output of generative models in both vision and language. However, small changes and design choices in the prompt can lead to significant differences in the output. In this work, we develop a black-box framework for generating adversarial prompts for unstructured image and text generation. These prompts, which can be standalone or prepended to benign prompts, induce specific behaviors into the generative process, such as generating images of a particular object or generating high perplexity text.",
            "year": 2023,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A black-box framework for generating adversarial prompts for unstructured image and text generation and induces specific behaviors into the generative process, such as generating images of a particular object or generating high perplexity text."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to detect adversarial triggers in language models using contextual prompting, while the paper focuses on generating adversarial prompts to induce specific behaviors in generative models for both vision and language.\n\nProject proposal: Detecting adversarial triggers in language models using contextual prompting.\nPaper: Generating adversarial prompts to induce specific behaviors in generative models for vision and language.\n\nThe project proposal and the paper have different research problems and approaches. The proposal focuses on defense against adversarial attacks, while the paper is about generating adversarial prompts.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "47030369e97cc44d4b2e3cf1be85da0fd134904a",
            "paperId": "47030369e97cc44d4b2e3cf1be85da0fd134904a",
            "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
            "abstract": "Because\"out-of-the-box\"large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called\"jailbreaks\"against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods. Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.",
            "year": 2023,
            "citationCount": 386,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is detecting adversarial triggers in language models using contextual prompting, while the paper focuses on generating universal and transferable adversarial attacks on aligned language models.\n\nThe approach in the proposal is to use sub-prompts to probe the model's understanding and attention to detect inconsistencies or anomalies that may indicate adversarial triggers. The paper's approach is to automatically generate adversarial suffixes that maximize the probability of the model producing objectionable content.\n\nThe proposal aims to defend against adversarial attacks, while the paper aims to create effective adversarial attacks. Therefore, the research problems and approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "dbac86036cb5ed4dd6bbdda4a8613b163e20ec90",
            "paperId": "dbac86036cb5ed4dd6bbdda4a8613b163e20ec90",
            "title": "Fundamental Limitations of Alignment in Large Language Models",
            "abstract": "An important aspect in developing language models that interact with humans is aligning their behavior to be useful and unharmful for their human users. This is usually achieved by tuning the model in a way that enhances desired behaviors and inhibits undesired ones, a process referred to as alignment. In this paper, we propose a theoretical approach called Behavior Expectation Bounds (BEB) which allows us to formally investigate several inherent characteristics and limitations of alignment in large language models. Importantly, we prove that within the limits of this framework, for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt. This implies that any alignment process that attenuates an undesired behavior but does not remove it altogether, is not safe against adversarial prompting attacks. Furthermore, our framework hints at the mechanism by which leading alignment approaches such as reinforcement learning from human feedback make the LLM prone to being prompted into the undesired behaviors. This theoretical result is being experimentally demonstrated in large scale by the so called contemporary\"chatGPT jailbreaks\", where adversarial users trick the LLM into breaking its alignment guardrails by triggering it into acting as a malicious persona. Our results expose fundamental limitations in alignment of LLMs and bring to the forefront the need to devise reliable mechanisms for ensuring AI safety.",
            "year": 2023,
            "citationCount": 82,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is proved that within the limits of this framework, for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is detecting adversarial triggers in language models using contextual prompting, while the paper discusses the fundamental limitations of aligning large language models to be safe and unharmful.\n\nThe proposed approach in the paper is a theoretical framework called Behavior Expectation Bounds (BEB) to investigate the inherent characteristics and limitations of alignment in large language models. In contrast, the proposal suggests using contextual prompting to detect adversarial triggers without requiring additional training or model components.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "8951bbc8c1eb3fd43c9e47025268cc79b868f514",
            "paperId": "8951bbc8c1eb3fd43c9e47025268cc79b868f514",
            "title": "Why do universal adversarial attacks work on large language models?: Geometry might be the answer",
            "abstract": "Transformer based large language models with emergent capabilities are becoming increasingly ubiquitous in society. However, the task of understanding and interpreting their internal workings, in the context of adversarial attacks, remains largely unsolved. Gradient-based universal adversarial attacks have been shown to be highly effective on large language models and potentially dangerous due to their input-agnostic nature. This work presents a novel geometric perspective explaining universal adversarial attacks on large language models. By attacking the 117M parameter GPT-2 model, we find evidence indicating that universal adversarial triggers could be embedding vectors which merely approximate the semantic information in their adversarial training region. This hypothesis is supported by white-box model analysis comprising dimensionality reduction and similarity measurement of hidden representations. We believe this new geometric perspective on the underlying mechanism driving universal attacks could help us gain deeper insight into the internal workings and failure modes of LLMs, thus enabling their mitigation.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "By attacking the 117M parameter GPT-2 model, this work finds evidence indicating that universal adversarial triggers could be embedding vectors which merely approximate the semantic information in their adversarial training region."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is detecting adversarial triggers in language models using contextual prompting, while the paper focuses on explaining why universal adversarial attacks work on large language models from a geometric perspective.\n\nProposal summary: Detecting adversarial triggers in language models using contextual prompting without additional training or model components.\nPaper summary: Explaining the effectiveness of universal adversarial attacks on large language models from a geometric perspective using dimensionality reduction and similarity measurement of hidden representations.\n\nThe proposal and the paper address different problems (detection vs. explanation) and use different approaches (contextual prompting vs. geometric analysis).\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "afee8cdc51e95b50d7574ed1700a797874bf792c",
            "paperId": "afee8cdc51e95b50d7574ed1700a797874bf792c",
            "title": "Adversarial Fine-Tuning of Language Models: An Iterative Optimisation Approach for the Generation and Detection of Problematic Content",
            "abstract": "In this paper, we tackle the emerging challenge of unintended harmful content generation in Large Language Models (LLMs) with a novel dual-stage optimisation technique using adversarial fine-tuning. Our two-pronged approach employs an adversarial model, fine-tuned to generate potentially harmful prompts, and a judge model, iteratively optimised to discern these prompts. In this adversarial cycle, the two models seek to outperform each other in the prompting phase, generating a dataset of rich examples which are then used for fine-tuning. This iterative application of prompting and fine-tuning allows continuous refinement and improved performance. The performance of our approach is evaluated through classification accuracy on a dataset consisting of problematic prompts not detected by GPT-4, as well as a selection of contentious but unproblematic prompts. We show considerable increase in classification accuracy of the judge model on this challenging dataset as it undergoes the optimisation process. Furthermore, we show that a rudimentary model \\texttt{ada} can achieve 13\\% higher accuracy on the hold-out test set than GPT-4 after only a few rounds of this process, and that this fine-tuning improves performance in parallel tasks such as toxic comment identification.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper shows that a rudimentary model can achieve 13\\% higher accuracy on the hold-out test set than GPT-4 after only a few rounds of this process, and that this fine-tuning improves performance in parallel tasks such as toxic comment identification."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is detecting adversarial triggers in language models using contextual prompting, while the paper focuses on generating and detecting problematic content in language models using adversarial fine-tuning.\n\nProposal: Detecting adversarial triggers in language models using contextual prompting.\nPaper: Generating and detecting problematic content in language models using adversarial fine-tuning.\n\nThe two works differ in their focus (adversarial triggers vs. problematic content) and approach (contextual prompting vs. adversarial fine-tuning).\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "60f653b14da8cc6866e5062a9700bb5f85a387ca",
            "paperId": "60f653b14da8cc6866e5062a9700bb5f85a387ca",
            "title": "Gradient Cuff: Detecting Jailbreak Attacks on Large Language Models by Exploring Refusal Loss Landscapes",
            "abstract": "Large Language Models (LLMs) are becoming a prominent generative AI tool, where the user enters a query and the LLM generates an answer. To reduce harm and misuse, efforts have been made to align these LLMs to human values using advanced training techniques such as Reinforcement Learning from Human Feedback (RLHF). However, recent studies have highlighted the vulnerability of LLMs to adversarial jailbreak attempts aiming at subverting the embedded safety guardrails. To address this challenge, this paper defines and investigates the Refusal Loss of LLMs and then proposes a method called Gradient Cuff to detect jailbreak attempts. Gradient Cuff exploits the unique properties observed in the refusal loss landscape, including functional values and its smoothness, to design an effective two-step detection strategy. Experimental results on two aligned LLMs (LLaMA-2-7B-Chat and Vicuna-7B-V1.5) and six types of jailbreak attacks (GCG, AutoDAN, PAIR, TAP, Base64, and LRL) show that Gradient Cuff can significantly improve the LLM's rejection capability for malicious jailbreak queries, while maintaining the model's performance for benign user queries by adjusting the detection threshold.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper defines and investigates the Refusal Loss of LLMs and then proposes a method called Gradient Cuff to detect jailbreak attempts, which exploits the unique properties observed in the refusal loss landscape to design an effective two-step detection strategy."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is detecting adversarial triggers in language models using contextual prompting, while the paper focuses on detecting jailbreak attacks on large language models by exploring refusal loss landscapes. The proposed approach in the paper, Gradient Cuff, exploits the properties of the refusal loss landscape to detect jailbreak attempts, which is different from the contextual prompting approach proposed in the project.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "b8b8d5655df1c6a71bbb713387863e34cc055332",
            "paperId": "b8b8d5655df1c6a71bbb713387863e34cc055332",
            "title": "Detecting Language Model Attacks with Perplexity",
            "abstract": "A novel hack involving Large Language Models (LLMs) has emerged, leveraging adversarial suffixes to trick models into generating perilous responses. This method has garnered considerable attention from reputable media outlets such as the New York Times and Wired, thereby influencing public perception regarding the security and safety of LLMs. In this study, we advocate the utilization of perplexity as one of the means to recognize such potential attacks. The underlying concept behind these hacks revolves around appending an unusually constructed string of text to a harmful query that would otherwise be blocked. This maneuver confuses the protective mechanisms and tricks the model into generating a forbidden response. Such scenarios could result in providing detailed instructions to a malicious user for constructing explosives or orchestrating a bank heist. Our investigation demonstrates the feasibility of employing perplexity, a prevalent natural language processing metric, to detect these adversarial tactics before generating a forbidden response. By evaluating the perplexity of queries with and without such adversarial suffixes using an open-source LLM, we discovered that nearly 90 percent were above a perplexity of 1000. This contrast underscores the efficacy of perplexity for detecting this type of exploit.",
            "year": 2023,
            "citationCount": 36,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This investigation demonstrates the feasibility of employing perplexity, a prevalent natural language processing metric, to detect these adversarial tactics before generating a forbidden response."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is detecting adversarial triggers in language models using contextual prompting, while the paper focuses on detecting adversarial attacks using perplexity. Although both aim to detect adversarial attacks, the proposed approaches are different: the proposal uses contextual prompting to make the model more self-aware and robust, while the paper uses perplexity to identify unusual text patterns.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "77d6d7482d1a32ad147c39993758b6c63816f5c0",
            "paperId": "77d6d7482d1a32ad147c39993758b6c63816f5c0",
            "title": "PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
            "abstract": "The increasing reliance on Large Language Models (LLMs) across academia and industry necessitates a comprehensive understanding of their robustness to prompts. In response to this vital need, we introduce PromptBench, a robustness benchmark designed to measure LLMs' resilience to adversarial prompts. This study uses a plethora of adversarial textual attacks targeting prompts across multiple levels: character, word, sentence, and semantic. The adversarial prompts, crafted to mimic plausible user errors like typos or synonyms, aim to evaluate how slight deviations can affect LLM outcomes while maintaining semantic integrity. These prompts are then employed in diverse tasks, such as sentiment analysis, natural language inference, reading comprehension, machine translation, and math problem-solving. Our study generates 4788 adversarial prompts, meticulously evaluated over 8 tasks and 13 datasets. Our findings demonstrate that contemporary LLMs are not robust to adversarial prompts. Furthermore, we present comprehensive analysis to understand the mystery behind prompt robustness and its transferability. We then offer insightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users. Code is available at: https://github.com/microsoft/promptbench.",
            "year": 2023,
            "citationCount": 111,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study generates 4788 adversarial prompts and presents comprehensive analysis to understand the mystery behind prompt robustness and its transferability, and offers insightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users."
            },
            "score": 6
        },
        {
            "id": "c060976628a0a27489f2c1268818eac5991ad52f",
            "paperId": "c060976628a0a27489f2c1268818eac5991ad52f",
            "title": "Do you really follow me? Adversarial Instructions for Evaluating the Robustness of Large Language Models",
            "abstract": "Large Language Models (LLMs) have shown remarkable proficiency in following instructions, making them valuable in customer-facing applications. However, their impressive capabilities also raise concerns about the amplification of risks posed by adversarial instructions, which can be injected into the model input by third-party attackers to manipulate LLMs\u2019 original instructions and prompt unintended actions and content. Therefore, it is crucial to understand LLMs\u2019 ability to accurately discern which instructions to follow to ensure their safe deployment in real-world scenarios. In this paper, we propose a pioneering benchmark for automatically evaluating the robustness of LLMs against adversarial instructions. The objective of this benchmark is to quantify the extent to which LLMs are influenced by injected adversarial instructions and assess their ability to differentiate between these adversarial instructions and original user instructions. Through experiments conducted with state-of-the-art instruction-following LLMs, we uncover significant limitations in their robustness against adversarial instruction attacks. Furthermore, our findings indicate that prevalent instruction-tuned models are prone to being \u201coverfitted\u201d to follow any instruction phrase in the prompt without truly understanding which instructions should be followed. This highlights the need to address the challenge of training models to comprehend prompts instead of merely following instruction phrases and completing the text.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A pioneering benchmark for automatically evaluating the robustness of LLMs against adversarial instructions is proposed and it is found that prevalent instruction-tuned models are prone to being \u201coverfitted\u201d to follow any instruction phrase in the prompt without truly understanding which instructions should be followed."
            },
            "score": 6
        },
        {
            "id": "8fdd34153d1035d09dd4a6efa9cb0c91d23d0045",
            "paperId": "8fdd34153d1035d09dd4a6efa9cb0c91d23d0045",
            "title": "More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models",
            "abstract": "We are currently witnessing dramatic advances in the capabilities of Large Language Models (LLMs). They are already being adopted in practice and integrated into many systems, including integrated development environments (IDEs) and search engines. The functionalities of current LLMs can be modulated via natural language prompts, while their exact internal functionality remains implicit and unassessable. This property, which makes them adaptable to even unseen tasks, might also make them susceptible to targeted adversarial prompting . Recently, several ways to misalign LLMs using Prompt Injection (PI) attacks have been introduced. In such attacks, an adversary can prompt the LLM to produce malicious content or override the original instructions and the employed \ufb01ltering schemes. Recent work showed that these attacks are hard to mitigate, as state-of-the-art LLMs are instruction-following . So far, these attacks assumed that the adversary is directly prompting the LLM. In this work, we show that augmenting LLMs with retrieval and API calling capabilities (so-called Application-Integrated LLMs ) induces a whole new set of attack vectors. These LLMs might process poisoned content retrieved from the Web that contains malicious prompts pre-injected and selected by adversaries. We demonstrate that an attacker can indirectly perform such PI attacks. Based on this key insight, we systematically analyze the resulting threat landscape of Application-Integrated LLMs and discuss a variety of new attack vectors. To demonstrate the practical viability of our attacks, we implemented speci\ufb01c demonstrations",
            "year": 2023,
            "citationCount": 73,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work shows that augmenting LLMs with retrieval and API calling capabilities (so-called Application-Integrated LLMs) induces a whole new set of attack vectors and systematically analyzes the resulting threat landscape of Application-Integrated LLMs."
            },
            "score": 5
        },
        {
            "id": "620a1a585a5e433a47103c112de17553a81fcbe6",
            "paperId": "620a1a585a5e433a47103c112de17553a81fcbe6",
            "title": "Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks",
            "abstract": "Although remarkable progress has been achieved in preventing large language model (LLM) hallucinations using instruction tuning and retrieval augmentation, it remains challenging to measure the reliability of LLMs using human-crafted evaluation data which is not available for many tasks and domains and could suffer from data leakage. Inspired by adversarial machine learning, this paper aims to develop a method of automatically generating evaluation data by appropriately modifying existing data on which LLMs behave faithfully. Specifically, this paper presents AutoDebug, an LLM-based framework to use prompting chaining to generate transferable adversarial attacks in the form of question-answering examples. We seek to understand the extent to which these examples trigger the hallucination behaviors of LLMs. We implement AutoDebug using ChatGPT and evaluate the resulting two variants of a popular open-domain question-answering dataset, Natural Questions (NQ), on a collection of open-source and proprietary LLMs under various prompting settings. Our generated evaluation data is human-readable and, as we show, humans can answer these modified questions well. Nevertheless, we observe pronounced accuracy drops across multiple LLMs including GPT-4. Our experimental results show that LLMs are likely to hallucinate in two categories of question-answering scenarios where (1) there are conflicts between knowledge given in the prompt and their parametric knowledge, or (2) the knowledge expressed in the prompt is complex. Finally, we find that the adversarial examples generated by our method are transferable across all considered LLMs. The examples generated by a small model can be used to debug a much larger model, making our approach cost-effective.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents AutoDebug, an LLM-based framework to use prompting chaining to generate transferable adversarial attacks in the form of question-answering examples, and finds that the adversarial examples generated by the method are transferable across all considered LLMs."
            },
            "score": 5
        },
        {
            "id": "3e30a7ac4886b28eb50151f58e14a1d698cccd0e",
            "paperId": "3e30a7ac4886b28eb50151f58e14a1d698cccd0e",
            "title": "Baseline Defenses for Adversarial Attacks Against Aligned Language Models",
            "abstract": "As Large Language Models quickly become ubiquitous, it becomes critical to understand their security vulnerabilities. Recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. Drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: What threat models are practically useful in this domain? How do baseline defense techniques perform in this new domain? How does LLM security differ from computer vision? We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, discussing the various settings in which each is feasible and effective. Particularly, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. We discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. We find that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for LLMs. Future research will be needed to uncover whether more powerful optimizers can be developed, or whether the strength of filtering and preprocessing defenses is greater in the LLMs domain than it has been in computer vision.",
            "year": 2023,
            "citationCount": 97,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for LLMs."
            },
            "score": 5
        },
        {
            "id": "f15662437b0cf9eaa1d9838d35143a7e127a1df5",
            "paperId": "f15662437b0cf9eaa1d9838d35143a7e127a1df5",
            "title": "Humanizing Machine-Generated Content: Evading AI-Text Detection through Adversarial Attack",
            "abstract": "With the development of large language models (LLMs), detecting whether text is generated by a machine becomes increasingly challenging in the face of malicious use cases like the spread of false information, protection of intellectual property, and prevention of academic plagiarism. While well-trained text detectors have demonstrated promising performance on unseen test data, recent research suggests that these detectors have vulnerabilities when dealing with adversarial attacks such as paraphrasing. In this paper, we propose a framework for a broader class of adversarial attacks, designed to perform minor perturbations in machine-generated content to evade detection. We consider two attack settings: white-box and black-box, and employ adversarial learning in dynamic scenarios to assess the potential enhancement of the current detection model's robustness against such attacks. The empirical results reveal that the current detection models can be compromised in as little as 10 seconds, leading to the misclassification of machine-generated text as human-written content. Furthermore, we explore the prospect of improving the model's robustness over iterative adversarial learning. Although some improvements in model robustness are observed, practical applications still face significant challenges. These findings shed light on the future development of AI-text detectors, emphasizing the need for more accurate and robust detection methods.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A framework for a broader class of adversarial attacks, designed to perform minor perturbations in machine-generated content to evade detection, is proposed and the prospect of improving the model's robustness over iterative adversarial learning is explored."
            },
            "score": 5
        },
        {
            "id": "b5a624da64475d735f0e298dc6f2f6669b5bb697",
            "paperId": "b5a624da64475d735f0e298dc6f2f6669b5bb697",
            "title": "Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield",
            "abstract": "Large Language Models' safety remains a critical concern due to their vulnerability to adversarial attacks, which can prompt these systems to produce harmful responses. In the heart of these systems lies a safety classifier, a computational model trained to discern and mitigate potentially harmful, offensive, or unethical outputs. However, contemporary safety classifiers, despite their potential, often fail when exposed to inputs infused with adversarial noise. In response, our study introduces the Adversarial Prompt Shield (APS), a lightweight model that excels in detection accuracy and demonstrates resilience against adversarial prompts. Additionally, we propose novel strategies for autonomously generating adversarial training datasets, named Bot Adversarial Noisy Dialogue (BAND) datasets. These datasets are designed to fortify the safety classifier's robustness, and we investigate the consequences of incorporating adversarial examples into the training process. Through evaluations involving Large Language Models, we demonstrate that our classifier has the potential to decrease the attack success rate resulting from adversarial attacks by up to 60%. This advancement paves the way for the next generation of more reliable and resilient conversational agents.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study introduces the Adversarial Prompt Shield (APS), a lightweight model that excels in detection accuracy and demonstrates resilience against adversarial prompts, and proposes novel strategies for autonomously generating adversarial training datasets, designed to fortify the safety classifier's robustness."
            },
            "score": 5
        },
        {
            "id": "1abfc211793c683972ded8d3268475e3ee7a88b0",
            "paperId": "1abfc211793c683972ded8d3268475e3ee7a88b0",
            "title": "Adversarial Demonstration Attacks on Large Language Models",
            "abstract": "With the emergence of more powerful large language models (LLMs), such as ChatGPT and GPT-4, in-context learning (ICL) has gained significant prominence in leveraging these models for specific tasks by utilizing data-label pairs as precondition prompts. While incorporating demonstrations can greatly enhance the performance of LLMs across various tasks, it may introduce a new security concern: attackers can manipulate only the demonstrations without changing the input to perform an attack. In this paper, we investigate the security concern of ICL from an adversarial perspective, focusing on the impact of demonstrations. We propose a novel attack method named advICL, which aims to manipulate only the demonstration without changing the input to mislead the models. Our results demonstrate that as the number of demonstrations increases, the robustness of in-context learning would decrease. Additionally, we also identify the intrinsic property of the demonstrations is that they can be used (prepended) with different inputs. As a result, it introduces a more practical threat model in which an attacker can attack the test input example even without knowing and manipulating it. To achieve it, we propose the transferable version of advICL, named Transferable-advICL. Our experiment shows that the adversarial demonstration generated by Transferable-advICL can successfully attack the unseen test input examples. We hope that our study reveals the critical security risks associated with ICL and underscores the need for extensive research on the robustness of ICL, particularly given its increasing significance in the advancement of LLMs.",
            "year": 2023,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper investigates the security concern of ICL from an adversarial perspective, focusing on the impact of demonstrations, and proposes a novel attack method named advICL, which aims to manipulate only the demonstration without changing the input to mislead the models."
            },
            "score": 5
        },
        {
            "id": "12c826f4195da172b212a529f8fcf10cc79e35da",
            "paperId": "12c826f4195da172b212a529f8fcf10cc79e35da",
            "title": "Context-faithful Prompting for Large Language Models",
            "abstract": "Large language models (LLMs) encode parametric knowledge about world facts and have shown remarkable performance in knowledge-driven NLP tasks. However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks). In this paper, we seek to assess and enhance LLMs' contextual faithfulness in two aspects: knowledge conflict and prediction with abstention. We demonstrate that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies. In particular, we identify opinion-based prompts and counterfactual demonstrations as the most effective methods. Opinion-based prompts reframe the context as a narrator's statement and inquire about the narrator's opinions, while counterfactual demonstrations use instances containing false facts to improve faithfulness in knowledge conflict situations. Neither technique requires additional training. We conduct experiments on three datasets of two standard NLP tasks, machine reading comprehension and relation extraction, and the results demonstrate significant improvement in faithfulness to contexts. Code and data are released at https://github.com/wzhouad/context-faithful-llm.",
            "year": 2023,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies, and opinion-based prompts and counterfactual demonstrations are identified as the most effective methods."
            },
            "score": 4
        },
        {
            "id": "b9df0d4631f9fab1432c152765e243ae4cd667f4",
            "paperId": "b9df0d4631f9fab1432c152765e243ae4cd667f4",
            "title": "Effective Prompt Extraction from Language Models",
            "abstract": "The text generated by large language models is commonly controlled by prompting, where a prompt prepended to a user's query guides the model's output. The prompts used by companies to guide their models are often treated as secrets, to be hidden from the user making the query. They have even been treated as commodities to be bought and sold. However, anecdotal reports have shown adversarial users employing prompt extraction attacks to recover these prompts. In this paper, we present a framework for systematically measuring the effectiveness of these attacks. In experiments with 3 different sources of prompts and 11 underlying large language models, we find that simple text-based attacks can in fact reveal prompts with high probability. Our framework determines with high precision whether an extracted prompt is the actual secret prompt, rather than a model hallucination. Prompt extraction experiments on real systems such as Bing Chat and ChatGPT suggest that system prompts can be revealed by an adversary despite existing defenses in place.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents a framework for systematically measuring the effectiveness of prompt extraction attacks and determines with high precision whether an extracted prompt is the actual secret prompt, rather than a model hallucination."
            },
            "score": 4
        },
        {
            "id": "33c9545c4948e4d8ae14c138f4bd97daa9dcbb7b",
            "paperId": "33c9545c4948e4d8ae14c138f4bd97daa9dcbb7b",
            "title": "Detecting Word-Level Adversarial Text Attacks via SHapley Additive exPlanations",
            "abstract": "State-of-the-art machine learning models are prone to adversarial attacks\u201d:\" Maliciously crafted inputs to fool the model into making a wrong prediction, often with high confidence. While defense strategies have been extensively explored in the computer vision domain, research in natural language processing still lacks techniques to make models resilient to adversarial text inputs. We adapt a technique from computer vision to detect word-level attacks targeting text classifiers. This method relies on training an adversarial detector leveraging Shapley additive explanations and outperforms the current state-of-the-art on two benchmarks. Furthermore, we prove the detector requires only a low amount of training samples and, in some cases, generalizes to different datasets without needing to retrain.",
            "year": 2022,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work adapts a technique from computer vision to detect word-level attacks targeting text classifiers using Shapley additive explanations and proves the detector requires only a low amount of training samples and generalizes to different datasets without needing to retrain."
            },
            "score": 4
        },
        {
            "id": "f28f65eb31f33db591b42c66e6d6b77891cdd39b",
            "paperId": "f28f65eb31f33db591b42c66e6d6b77891cdd39b",
            "title": "Detecting Adversarial Text Attacks via SHapley Additive exPlanations",
            "abstract": "State-of-the-art machine learning models are 001 prone to adversarial attacks: maliciously 002 crafted inputs to fool the model into making a 003 wrong prediction, often with high con\ufb01dence. 004 While defense strategies have been extensively 005 explored in the computer vision domain, re-006 search in natural language processing still 007 lacks techniques to make models resilient to 008 adversarial text inputs. We propose an adver-009 sarial detector leveraging Shapley additive ex-010 planations against text attacks. Our approach 011 outperforms the current state-of-the-art detec-012 tor by around 19% F1-score on the IMDb and 013 14% on the SST-2 datasets while also show-014 ing competitive performance on AG_News and 015 Yelp Polarity . Furthermore, we prove the de-016 tector to only require a low amount of training 017 samples and, in some cases, to generalize to 018 different datasets without needing to retrain. 019",
            "year": 2021,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes an adver-009 sarial detector leveraging Shapley additive ex-010 planations against text attacks and proves the de-016 tector to only require a low amount of training 017 samples and, in some cases, to generalize to 018 different datasets without needing to retrain."
            },
            "score": 4
        },
        {
            "id": "49ea1e15a30efe28d637c3a2b302276238c451c9",
            "paperId": "49ea1e15a30efe28d637c3a2b302276238c451c9",
            "title": "Measuring the Contribution of Multiple Model Representations in Detecting Adversarial Instances",
            "abstract": "Deep learning models have been used for a wide variety of tasks. They are prevalent in computer vision, natural language processing, speech recognition, and other areas. While these models have worked well under many scenarios, it has been shown that they are vulnerable to adversarial attacks. This has led to a proliferation of research into ways that such attacks could be identified and/or defended against. Our goal is to explore the contribution that can be attributed to using multiple underlying models for the purpose of adversarial instance detection. Our paper describes two approaches that incorporate representations from multiple models for detecting adversarial examples. We devise controlled experiments for measuring the detection impact of incrementally utilizing additional models. For many of the scenarios we consider, the results show that performance increases with the number of underlying models used for extracting representations.",
            "year": 2021,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper describes two approaches that incorporate representations from multiple models for detecting adversarial examples and shows that performance increases with the number of underlying models used for extracting representations."
            },
            "score": 4
        },
        {
            "id": "69417082e98b4d9cf40cdd04692ac99ee66e41bd",
            "paperId": "69417082e98b4d9cf40cdd04692ac99ee66e41bd",
            "title": "A Simple General Method for Detecting Textual Adversarial Examples",
            "abstract": "Although deep neural networks have achieved 001 state-of-the-art performance in various machine 002 learning and artificial intelligence tasks, adver-003 sarial examples, constructed by adding small 004 non-random perturbations to correctly classi-005 fied inputs, successfully fool highly expres-006 sive deep classifiers into incorrect predictions. 007 Approaches to adversarial attacks in natural 008 language tasks have boomed in the last five 009 years using character-level, word-level, phrase-010 level, or sentence-level textual perturbations. 011 While there is some work in NLP on defending 012 against such attacks through proactive meth-013 ods, like adversarial training, there is to our 014 knowledge no effective reactive approaches to 015 defence via detection of textual adversarial ex-016 amples such as is found in the image process-017 ing literature. In this paper, we apply distance-018 based ensemble learning and semantic repre-019 sentations from different representation learn-020 ing models based on our understanding of the 021 reason for adversarial examples to fill this gap. 022 Our technique, MultiDistance Representation 023 Ensemble Method (MDRE), obtains state-of-024 the-art results on character-level, word-level, 025 and phrase-level attacks on the IMDB dataset 026 as well as on the later two with respect to the 027 MultiNLI dataset. If this paper is accepted, we 028 will publish our code. 029",
            "year": 2021,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 4
        },
        {
            "id": "02819ebba2c60fb9ba4e5fccee3be6db38c52409",
            "paperId": "02819ebba2c60fb9ba4e5fccee3be6db38c52409",
            "title": "Towards Stronger Adversarial Baselines Through Human-AI Collaboration",
            "abstract": "Natural language processing (NLP) systems are often used for adversarial tasks such as detecting spam, abuse, hate speech, and fake news. Properly evaluating such systems requires dynamic evaluation that searches for weaknesses in the model, rather than a static test set. Prior work has evaluated such models on both manually and automatically generated examples, but both approaches have limitations: manually constructed examples are time-consuming to create and are limited by the imagination and intuition of the creators, while automatically constructed examples are often ungrammatical or labeled inconsistently. We propose to combine human and AI expertise in generating adversarial examples, benefiting from humans\u2019 expertise in language and automated attacks\u2019 ability to probe the target system more quickly and thoroughly. We present a system that facilitates attack construction, combining human judgment with automated attacks to create better attacks more efficiently. Preliminary results from our own experimentation suggest that human-AI hybrid attacks are more effective than either human-only or AI-only attacks. A complete user study to validate these hypotheses is still pending.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents a system that facilitates attack construction, combining human judgment with automated attacks to create better attacks more efficiently, and suggests that human-AI hybrid attacks are more effective than either human-only or AI-only attacks."
            },
            "score": 4
        },
        {
            "id": "8ecdbfe011b7189fa0ee49ffc4e42a93d728a371",
            "paperId": "8ecdbfe011b7189fa0ee49ffc4e42a93d728a371",
            "title": "On Evaluating Adversarial Robustness of Large Vision-Language Models",
            "abstract": "Large vision-language models (VLMs) such as GPT-4 have achieved unprecedented performance in response generation, especially with visual inputs, enabling more creative and adaptable interaction than large language models such as ChatGPT. Nonetheless, multimodal generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable modality (e.g., vision). To this end, we propose evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses. In particular, we first craft targeted adversarial examples against pretrained models such as CLIP and BLIP, and then transfer these adversarial examples to other VLMs such as MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt. In addition, we observe that black-box queries on these VLMs can further improve the effectiveness of targeted evasion, resulting in a surprisingly high success rate for generating targeted responses. Our findings provide a quantitative understanding regarding the adversarial vulnerability of large VLMs and call for a more thorough examination of their potential security flaws before deployment in practice. Code is at https://github.com/yunqing-me/AttackVLM.",
            "year": 2023,
            "citationCount": 47,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses is proposed."
            },
            "score": 4
        },
        {
            "id": "7c5aa120a582bd192b2be4952953040b41d3d503",
            "paperId": "7c5aa120a582bd192b2be4952953040b41d3d503",
            "title": "Certified Robustness for Large Language Models with Self-Denoising",
            "abstract": "Although large language models (LLMs) have achieved great success in vast real-world applications, their vulnerabilities towards noisy inputs have significantly limited their uses, especially in high-stake environments. In these contexts, it is crucial to ensure that every prediction made by large language models is stable, i.e., LLM predictions should be consistent given minor differences in the input. This largely falls into the study of certified robust LLMs, i.e., all predictions of LLM are certified to be correct in a local region around the input. Randomized smoothing has demonstrated great potential in certifying the robustness and prediction stability of LLMs. However, randomized smoothing requires adding noise to the input before model prediction, and its certification performance depends largely on the model's performance on corrupted data. As a result, its direct application to LLMs remains challenging and often results in a small certification radius. To address this issue, we take advantage of the multitasking nature of LLMs and propose to denoise the corrupted inputs with LLMs in a self-denoising manner. Different from previous works like denoised smoothing, which requires training a separate model to robustify LLM, our method enjoys far better efficiency and flexibility. Our experiment results show that our method outperforms the existing certification methods under both certified robustness and empirical robustness. The codes are available at https://github.com/UCSB-NLP-Chang/SelfDenoise.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work takes advantage of the multitasking nature of LLMs and proposes to denoise the corrupted inputs with LLMs in a self-denoising manner, and outperforms the existing certification methods under both certified robustness and empirical robustness."
            },
            "score": 4
        },
        {
            "id": "e2cc232ad999164f1bf340996bb5db62b6602d31",
            "paperId": "e2cc232ad999164f1bf340996bb5db62b6602d31",
            "title": "Robustness Over Time: Understanding Adversarial Examples' Effectiveness on Longitudinal Versions of Large Language Models",
            "abstract": "Large Language Models (LLMs) have led to significant improvements in many tasks across various domains, such as code interpretation, response generation, and ambiguity handling. These LLMs, however, when upgrading, primarily prioritize enhancing user experience while neglecting security, privacy, and safety implications. Consequently, unintended vulnerabilities or biases can be introduced. Previous studies have predominantly focused on specific versions of the models and disregard the potential emergence of new attack vectors targeting the updated versions. Through the lens of adversarial examples within the in-context learning framework, this longitudinal study addresses this gap by conducting a comprehensive assessment of the robustness of successive versions of LLMs, vis-\\`a-vis GPT-3.5. We conduct extensive experiments to analyze and understand the impact of the robustness in two distinct learning categories: zero-shot learning and few-shot learning. Our findings indicate that, in comparison to earlier versions of LLMs, the updated versions do not exhibit the anticipated level of robustness against adversarial attacks. In addition, our study emphasizes the increased effectiveness of synergized adversarial queries in most zero-shot learning and few-shot learning cases. We hope that our study can lead to a more refined assessment of the robustness of LLMs over time and provide valuable insights of these models for both developers and users.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This longitudinal study conducts a comprehensive assessment of the robustness of successive versions of LLMs, vis-\\`a-vis GPT-3.5.5, and indicates that the updated versions do not exhibit the anticipated level of robustness against adversarial attacks."
            },
            "score": 4
        },
        {
            "id": "2403c8e72a90d9c778970fc0812ecdcc58800c5d",
            "paperId": "2403c8e72a90d9c778970fc0812ecdcc58800c5d",
            "title": "Can Language Models be Instructed to Protect Personal Information?",
            "abstract": "Large multimodal language models have proven transformative in numerous applications. However, these models have been shown to memorize and leak pre-training data, raising serious user privacy and information security concerns. While data leaks should be prevented, it is also crucial to examine the trade-off between the privacy protection and model utility of proposed approaches. In this paper, we introduce PrivQA -- a multimodal benchmark to assess this privacy/utility trade-off when a model is instructed to protect specific categories of personal information in a simulated scenario. We also propose a technique to iteratively self-moderate responses, which significantly improves privacy. However, through a series of red-teaming experiments, we find that adversaries can also easily circumvent these protections with simple jailbreaking methods through textual and/or image inputs. We believe PrivQA has the potential to support the development of new models with improved privacy protections, as well as the adversarial robustness of these protections. We release the entire PrivQA dataset at https://llm-access-control.github.io/.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "PrivQA is introduced -- a multimodal benchmark to assess this privacy/utility trade-off when a model is instructed to protect specific categories of personal information in a simulated scenario and a technique to iteratively self-moderate responses, which significantly improves privacy."
            },
            "score": 4
        },
        {
            "id": "6af460d34bfc8e955e43fbe15cedcf329b48bc19",
            "paperId": "6af460d34bfc8e955e43fbe15cedcf329b48bc19",
            "title": "SAC3: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency",
            "abstract": "Hallucination detection is a critical step toward understanding the trustworthiness of modern language models (LMs). To achieve this goal, we re-examine existing detection approaches based on the self-consistency of LMs and uncover two types of hallucinations resulting from 1) question-level and 2) model-level, which cannot be effectively identified through self-consistency check alone. Building upon this discovery, we propose a novel sampling-based method, i.e., semantic-aware cross-check consistency (SAC3) that expands on the principle of self-consistency checking. Our SAC3 approach incorporates additional mechanisms to detect both question-level and model-level hallucinations by leveraging advances including semantically equivalent question perturbation and cross-model response consistency checking. Through extensive and systematic empirical analysis, we demonstrate that SAC3 outperforms the state of the art in detecting both non-factual and factual statements across multiple question-answering and open-domain generation benchmarks.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel sampling-based method, i.e., semantic-aware cross-check consistency (SAC3) that expands on the principle of self-consistency checking and demonstrates that SAC3 outperforms the state of the art in detecting both non-factual and factual statements across multiple question-answering and open-domain generation benchmarks."
            },
            "score": 4
        },
        {
            "id": "1c6015ffff034b9c304477bb31e55ca5a55f3a99",
            "paperId": "1c6015ffff034b9c304477bb31e55ca5a55f3a99",
            "title": "Adversarial Transformer Language Models for Contextual Commonsense Inference",
            "abstract": "Contextualized or discourse aware commonsense inference is the task of generating coherent commonsense assertions (i.e., facts) from a given story, and a particular sentence from that story. Some problems with the task are: lack of controllability for topics of the inferred facts; lack of commonsense knowledge during training; and, possibly, hallucinated or false facts. In this work, we utilize a transformer model for this task and develop techniques to address the aforementioned problems in the task. We control the inference by introducing a new technique we call\"hinting\". Hinting is a kind of language model prompting, that utilizes both hard prompts (specific words) and soft prompts (virtual learnable templates). This serves as a control signal to advise the language model\"what to talk about\". Next, we establish a methodology for performing joint inference with multiple commonsense knowledge bases. Joint inference of commonsense requires care, because it is imprecise and the level of generality is more flexible. You want to be sure that the results\"still make sense\"for the context. To this end, we align the textual version of assertions from three knowledge graphs (ConceptNet, ATOMIC2020, and GLUCOSE) with a story and a target sentence. This combination allows us to train a single model to perform joint inference with multiple knowledge graphs. We show experimental results for the three knowledge graphs on joint inference. Our final contribution is exploring a GAN architecture that generates the contextualized commonsense assertions and scores them as to their plausibility through a discriminator. The result is an integrated system for contextual commonsense inference in stories, that can controllably generate plausible commonsense assertions, and takes advantage of joint inference between multiple commonsense knowledge bases.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The result is an integrated system for contextual commonsense inference in stories, that can controllably generate plausible commonsense assertions, and takes advantage of joint inference between multiple commonsense knowledge bases."
            },
            "score": 3
        },
        {
            "id": "91099bbb96133c70db091041900ecff502a5e3a8",
            "paperId": "91099bbb96133c70db091041900ecff502a5e3a8",
            "title": "Harnessing the Power of Adversarial Prompting and Large Language Models for Robust Hypothesis Generation in Astronomy",
            "abstract": "This study investigates the application of Large Language Models (LLMs), specifically GPT-4, within Astronomy. We employ in-context prompting, supplying the model with up to 1000 papers from the NASA Astrophysics Data System, to explore the extent to which performance can be improved by immersing the model in domain-specific literature. Our findings point towards a substantial boost in hypothesis generation when using in-context prompting, a benefit that is further accentuated by adversarial prompting. We illustrate how adversarial prompting empowers GPT-4 to extract essential details from a vast knowledge base to produce meaningful hypotheses, signaling an innovative step towards employing LLMs for scientific research in Astronomy.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is illustrated how adversarial prompting empowers GPT-4 to extract essential details from a vast knowledge base to produce meaningful hypotheses, signaling an innovative step towards employing LLMs for scientific research in Astronomy."
            },
            "score": 3
        },
        {
            "id": "2cd1d1a3e80e36cd44547cdf8715e62295c8a6bb",
            "paperId": "2cd1d1a3e80e36cd44547cdf8715e62295c8a6bb",
            "title": "GAN-LM: Generative Adversarial Network using Language Models for Downstream Applications",
            "abstract": "In this work, we investigate Data Augmentation methods to improve the performance of state-of-the-art models for four different downstream tasks. Specifically, we propose Generative Adversarial Network using Language Models (GAN-LM) approach that combines a deep generative model with a pre-trained language model to produce diverse augmentations. We compare the GAN-LM to various conventional methods in non-contextual- and contextual-levels on four public datasets: ZESHEL for zero-shot entity linking, TREC for question classification, STS-B for sentence pairs semantic textual similarity (STS), and mSTS for multilingual sentence pairs STS. Additionally, we subsample these datasets to study the impact of such augmentations in low-resource settings where limited amounts of training data is available. Compared to the state-of-the-art methods in downstream tasks, we mostly achieve the best performance using GAN-LM approach. Finally, we investigate the way of combining the GAN-LM with other augmentation methods to complement our proposed approach. The developed code for reproducibility is included in the supplementary material.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes Generative Adversarial Network using Language Models (GAN-LM) approach that combines a deep generative model with a pre-trained language model to produce diverse augmentations to improve the performance of state-of-the-art models for four different downstream tasks."
            },
            "score": 3
        },
        {
            "id": "5c5ab276b00c1f19fbb0a3d2c38d532becac9442",
            "paperId": "5c5ab276b00c1f19fbb0a3d2c38d532becac9442",
            "title": "A Brief History of Prompt: Leveraging Language Models. (Through Advanced Prompting)",
            "abstract": "This paper presents a comprehensive exploration of the evolution of prompt engineering and generation in the field of natural language processing (NLP). Starting from the early language models and information retrieval systems, we trace the key developments that have shaped prompt engineering over the years. The introduction of attention mechanisms in 2015 revolutionized language understanding, leading to advancements in controllability and context-awareness. Subsequent breakthroughs in reinforcement learning techniques further enhanced prompt engineering, addressing issues like exposure bias and biases in generated text. We examine the significant contributions in 2018 and 2019, focusing on fine-tuning strategies, control codes, and template-based generation. The paper also discusses the growing importance of fairness, human-AI collaboration, and low-resource adaptation. In 2020 and 2021, contextual prompting and transfer learning gained prominence, while 2022 and 2023 witnessed the emergence of advanced techniques like unsupervised pre-training and novel reward shaping. Throughout the paper, we reference specific research studies that exemplify the impact of various developments on prompt engineering. The journey of prompt engineering continues, with ethical considerations being paramount for the responsible and inclusive future of AI systems.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The journey of prompt engineering continues, with ethical considerations being paramount for the responsible and inclusive future of AI systems."
            },
            "score": 3
        },
        {
            "id": "7d7d9441b61ac239891fc322dfb127b4c80d518f",
            "paperId": "7d7d9441b61ac239891fc322dfb127b4c80d518f",
            "title": "Exploiting Multi-Object Relationships for Detecting Adversarial Attacks in Complex Scenes",
            "abstract": "Vision systems that deploy Deep Neural Networks (DNNs) are known to be vulnerable to adversarial examples. Recent research has shown that checking the intrinsic consistencies in the input data is a promising way to detect adversarial attacks (e.g., by checking the object co-occurrence relationships in complex scenes). However, existing approaches are tied to specific models and do not offer generalizability. Motivated by the observation that language descriptions of natural scene images have already captured the object co-occurrence relationships that can be learned by a language model, we develop a novel approach to perform context consistency checks using such language models. The distinguishing aspect of our approach is that it is independent of the deployed object detector and yet offers very high accuracy in terms of detecting adversarial examples in practical scenes with multiple objects. Experiments on the PASCAL VOC and MS COCO datasets show that our method can outperform state-of-the-art methods in detecting adversarial attacks.",
            "year": 2021,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work develops a novel approach to perform context consistency checks using language models that is independent of the deployed object detector and yet offers very high accuracy in terms of detecting adversarial examples in practical scenes with multiple objects."
            },
            "score": 3
        },
        {
            "id": "024d8d62f4a2057bfa019f83366624af281bb5e4",
            "paperId": "024d8d62f4a2057bfa019f83366624af281bb5e4",
            "title": "Don't be a Fool: Pooling Strategies in Offensive Language Detection from User-Intended Adversarial Attacks",
            "abstract": "Offensive language detection is an important task for filtering out abusive expressions and improving online user experiences. However, malicious users often attempt to avoid filtering systems through the involvement of textual noises. In this paper, we propose these evasions as user-intended adversarial attacks that insert special symbols or leverage the distinctive features of the Korean language. Furthermore, we introduce simple yet effective pooling strategies in a layer-wise manner to defend against the proposed attacks, focusing on the preceding layers not just the last layer to capture both offensiveness and token embeddings. We demonstrate that these pooling strategies are more robust to performance degradation even when the attack rate is increased, without directly training of such patterns. Notably, we found that models pre-trained on clean texts could achieve a comparable performance in detecting attacked offensive language, to models pre-trained on noisy texts by employing these pooling strategies.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes these evasions as user-intended adversarial attacks that insert special symbols or leverage the distinctive features of the Korean language to defend against the proposed attacks, and introduces simple yet effective pooling strategies to defend against the proposed attacks."
            },
            "score": 3
        },
        {
            "id": "dcbdd1ae7b66d13cde64edeb2acd6977e5eac42d",
            "paperId": "dcbdd1ae7b66d13cde64edeb2acd6977e5eac42d",
            "title": "From Hype to Reality: Transformer-Based Models for Fake News Detection Performance and Robustness Revealed",
            "abstract": "The prevalence of fake news in today\u2019s society is a serious concern, as it can compromise the reliability of information and have detrimental effects on individuals and communities. In this article, we conduct a comprehensive evaluation of six distinct Transformers to investigate their effectiveness in detecting Fake News. First, we examine the performance of these models on four diverse datasets, each representing a distinct language. Second, we investigate the robustness of these models against adversarial attacks to assess their vulnerability and measure the impact of such attacks on their performance. Our findings indicate that while transformers are commonly employed, their performance exhibits significant variability across datasets and languages. Moreover, our analysis reveals their vulnerability to attacks, as demonstrated by a notable drop in accuracy when confronted with deliberate manipulations.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "While transformers are commonly employed, their performance exhibits significant variability across datasets and languages, and their vulnerability to attacks is revealed, as demonstrated by a notable drop in accuracy when confronted with deliberate manipulations."
            },
            "score": 3
        },
        {
            "id": "c4394f01909404e7461006cbca8fd77099242561",
            "paperId": "c4394f01909404e7461006cbca8fd77099242561",
            "title": "Improving Robustness of Language Models from a Geometry-aware Perspective",
            "abstract": "Recent studies have found that removing the norm-bounded projection and increasing search steps in adversarial training can significantly improve robustness. However, we observe that a too large number of search steps can hurt accuracy. We aim to obtain strong robustness efficiently using fewer steps. Through a toy experiment, we find that perturbing the clean data to the decision boundary but not crossing it does not degrade the test accuracy. Inspired by this, we propose friendly adversarial data augmentation (FADA) to generate friendly adversarial data. On top of FADA, we propose geometry-aware adversarial training (GAT) to perform adversarial training on friendly adversarial data so that we can save a large number of search steps. Comprehensive experiments across two widely used datasets and three pre-trained language models demonstrate that GAT can obtain stronger robustness via fewer steps. In addition, we provide extensive empirical results and in-depth analyses on robustness to facilitate future studies.",
            "year": 2022,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Comprehensive experiments across two widely used datasets and three pre-trained language models demonstrate that GAT can obtain stronger robustness via fewer steps and provide extensive empirical results and in-depth analyses on robustness to facilitate future studies."
            },
            "score": 3
        },
        {
            "id": "0db108da7811200b25f979d659269812d0b52b61",
            "paperId": "0db108da7811200b25f979d659269812d0b52b61",
            "title": "Counterfactual Adversarial Training for Improving Robustness of Pre-trained Language Models",
            "abstract": "One of the approaches for improving the robustness of NLP models is adversarial training by adversarial examples. However, in previous work on adversarial training, the adversarial examples were not guaranteed to be minimally edited and to change the model\u2019s prediction. Our hypothesis is adversarial training could make models more robust if the adversarial examples were guaranteed to be minimally edited and to change the model\u2019s prediction. We propose Counterfactual Adversarial Training (CAT), which uses counterfactual explanations to improve the robustness of the model. Our experiments on Natural Language Inference and Sentiment Analysis show that CAT significantly enhances out-of-the-box pre-trained NLP models on 11 datasets, indicating that CAT is a promising approach to improve the robustness of the pre-trained language models.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experiments on Natural Language Inference and Sentiment Analysis show that CAT significantly enhances out-of-the-box pre-trained NLP models on 11 datasets, indicating that CAT is a promising approach to improve the robustness of the pre-trained language models."
            },
            "score": 3
        },
        {
            "id": "268ba07f529df6a7f20998bb2cf26b16b31709c8",
            "paperId": "268ba07f529df6a7f20998bb2cf26b16b31709c8",
            "title": "Self-Supervised Contrastive Learning with Adversarial Perturbations for Robust Pretrained Language Models",
            "abstract": "In this paper, we present an approach to im- 001 prove the robustness of BERT language mod- 002 els against word substitution-based adversar- 003 ial attacks by leveraging adversarial perturba- 004 tions for self-supervised contrastive learning. 005 We create an ef\ufb01cient word-level adversarial 006 attack, and use it to \ufb01netune BERT on ad- 007 versarial examples generated on the \ufb02y during 008 training. In contrast with previous works, our 009 method improves model robustness without us- 010 ing any labeled data. Experimental results 011 show that our method improves robustness of 012 BERT against four different word substitution- 013 based adversarial attacks, and combining our 014 method with adversarial training gives higher 015 robustness than adversarial training alone. As 016 our method improves the robustness of BERT 017 purely with unlabeled data, it opens up the pos- 018 sibility of using large text datasets to train ro- 019 bust language models. 020",
            "year": 2021,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An approach to prove the robustness of BERT language models against word substitution-based adversarial attacks by leveraging adversarial perturbation for self-supervised contrastive learning."
            },
            "score": 3
        },
        {
            "id": "3a391dfd536625e068f3888c817cc6cbe7fcea9c",
            "paperId": "3a391dfd536625e068f3888c817cc6cbe7fcea9c",
            "title": "One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models",
            "abstract": "Large pre-trained Vision-Language Models (VLMs) like CLIP, despite having remarkable generalization ability, are highly vulnerable to adversarial examples. This work studies the adversarial robustness of VLMs from the novel perspective of the text prompt instead of the extensively studied model weights (frozen in this work). We first show that the effectiveness of both adversarial attack and defense are sensitive to the used text prompt. Inspired by this, we propose a method to improve resilience to adversarial attacks by learning a robust text prompt for VLMs. The proposed method, named Adversarial Prompt Tuning (APT), is effective while being both computationally and data efficient. Extensive experiments are conducted across 15 datasets and 4 data sparsity schemes (from 1-shot to full training data settings) to show APT's superiority over hand-engineered prompts and other state-of-the-art adaption methods. APT demonstrated excellent abilities in terms of the in-distribution performance and the generalization under input distribution shift and across datasets. Surprisingly, by simply adding one learned word to the prompts, APT can significantly boost the accuracy and robustness (epsilon=4/255) over the hand-engineered prompts by +13% and +8.5% on average respectively. The improvement further increases, in our most effective setting, to +26.4% for accuracy and +16.7% for robustness. Code is available at https://github.com/TreeLLi/APT.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work studies the adversarial robustness of VLMs from the novel perspective of the text prompt instead of the extensively studied model weights, and proposes a method to improve resilience to adversarial attacks by learning a robust text prompt for VLMs."
            },
            "score": 3
        },
        {
            "id": "3ee20a72e6008a125135e3f17c5bbdb8cbe9bd8d",
            "paperId": "3ee20a72e6008a125135e3f17c5bbdb8cbe9bd8d",
            "title": "Impact of Adversarial Training on Robustness and Generalizability of Language Models",
            "abstract": "Adversarial training is widely acknowledged as the most effective defense against adversarial attacks. However, it is also well established that achieving both robustness and generalization in adversarially trained models involves a trade-off. The goal of this work is to provide an in depth comparison of different approaches for adversarial training in language models. Specifically, we study the effect of pre-training data augmentation as well as training time input perturbations vs. embedding space perturbations on the robustness and generalization of transformer-based language models. Our findings suggest that better robustness can be achieved by pre-training data augmentation or by training with input space perturbation. However, training with embedding space perturbation significantly improves generalization. A linguistic correlation analysis of neurons of the learned models reveals that the improved generalization is due to 'more specialized' neurons. To the best of our knowledge, this is the first work to carry out a deep qualitative analysis of different methods of generating adversarial examples in adversarial training of language models.",
            "year": 2022,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This is the first work to carry out a deep qualitative analysis of different methods of generating adversarial examples in adversarial training of language models and suggests that better robustness can be achieved by pre-training data augmentation or by training with input space perturbation."
            },
            "score": 3
        },
        {
            "id": "a4f533f2b7d77b667e1f05b210924ec7c90cc5d1",
            "paperId": "a4f533f2b7d77b667e1f05b210924ec7c90cc5d1",
            "title": "How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness?",
            "abstract": "The fine-tuning of pre-trained language models has a great success in many NLP fields. Yet, it is strikingly vulnerable to adversarial examples, e.g., word substitution attacks using only synonyms can easily fool a BERT-based sentiment analysis model. In this paper, we demonstrate that adversarial training, the prevalent defense technique, does not directly fit a conventional fine-tuning scenario, because it suffers severely from catastrophic forgetting: failing to retain the generic and robust linguistic features that have already been captured by the pre-trained model. In this light, we propose Robust Informative Fine-Tuning (RIFT), a novel adversarial fine-tuning method from an information-theoretical perspective. In particular, RIFT encourages an objective model to retain the features learned from the pre-trained model throughout the entire fine-tuning process, whereas a conventional one only uses the pre-trained weights for initialization. Experimental results show that RIFT consistently outperforms the state-of-the-arts on two popular NLP tasks: sentiment analysis and natural language inference, under different attacks across various pre-trained language models.",
            "year": 2021,
            "citationCount": 40,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Robust Informative Fine-Tuning (RIFT), a novel adversarial fine-tuning method from an information-theoretical perspective, which encourages an objective model to retain the features learned from the pre-trained model throughout the entire fine- Tuning process, whereas a conventional one only uses thePre-trained weights for initialization."
            },
            "score": 3
        },
        {
            "id": "5690e35b8beab92a80055fe2530c29c24e495379",
            "paperId": "5690e35b8beab92a80055fe2530c29c24e495379",
            "title": "On the Adversarial Robustness of Multi-Modal Foundation Models",
            "abstract": "Multi-modal foundation models combining vision and language models such as Flamingo or GPT-4 have recently gained enormous interest. Alignment of foundation models is used to prevent models from providing toxic or harmful output. While malicious users have successfully tried to jailbreak foundation models, an equally important question is if honest users could be harmed by malicious third-party content. In this paper we show that imperceivable attacks on images $\\left({{\\varepsilon _\\infty } = 1/255}\\right)$ in order to change the caption output of a multi-modal foundation model can be used by malicious content providers to harm honest users e.g. by guiding them to malicious websites or broadcast fake information. This indicates that countermeasures to adversarial attacks should be used by any deployed multi-modal foundation model. Note: This paper contains fake information to illustrate the outcome of our attacks. It does not reflect the opinion of the authors.",
            "year": 2023,
            "citationCount": 25,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that imperceivable attacks on images in order to change the caption output of a multi-modal foundation model can be used by malicious content providers to harm honest users e.g. by guiding them to malicious websites or broadcast fake information, indicating that countermeasures to adversarial attacks should be used by any deployed multi-modal foundation model."
            },
            "score": 3
        },
        {
            "id": "e4e744cc96da7987a072571fc3817f040d456566",
            "paperId": "e4e744cc96da7987a072571fc3817f040d456566",
            "title": "Large Language Models Know Your Contextual Search Intent: A Prompting Framework for Conversational Search",
            "abstract": "Precisely understanding users' contextual search intent has been an important challenge for conversational search. As conversational search sessions are much more diverse and long-tailed, existing methods trained on limited data still show unsatisfactory effectiveness and robustness to handle real conversational search scenarios. Recently, large language models (LLMs) have demonstrated amazing capabilities for text generation and conversation understanding. In this work, we present a simple yet effective prompting framework, called LLM4CS, to leverage LLMs as a text-based search intent interpreter to help conversational search. Under this framework, we explore three prompting methods to generate multiple query rewrites and hypothetical responses, and propose to aggregate them into an integrated representation that can robustly represent the user's real contextual search intent. Extensive automatic evaluations and human evaluations on three widely used conversational search benchmarks, including CAsT-19, CAsT-20, and CAsT-21, demonstrate the remarkable performance of our simple LLM4CS framework compared with existing methods and even using human rewrites. Our findings provide important evidence to better understand and leverage LLMs for conversational search.",
            "year": 2023,
            "citationCount": 26,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents a simple yet effective prompting framework, called LLM4CS, to leverage LLMs as a text-based search intent interpreter to help conversational search."
            },
            "score": 2
        },
        {
            "id": "0875651b68e6602d45ae08bee67cf63c02faa512",
            "paperId": "0875651b68e6602d45ae08bee67cf63c02faa512",
            "title": "Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models",
            "abstract": "Large language models (LLMs) have scaled up to unlock a wide range of complex reasoning tasks with the aid of various prompting methods. However, current prompting methods generate natural language intermediate steps to help reasoning, which can cause imperfect task reduction and confusion. To mitigate such limitations, we explore code prompting, a neural symbolic prompting method with both zero-shot and few-shot versions which triggers code as intermediate steps. We conduct experiments on 7 widely-used benchmarks involving symbolic reasoning and arithmetic reasoning. Code prompting generally outperforms chain-of-thought (CoT) prompting. To further understand the performance and limitations of code prompting, we perform extensive ablation studies and error analyses, and identify several exclusive advantages of using symbolic promptings compared to natural language. We also consider the ensemble of code prompting and CoT prompting to combine the strengths of both. Finally, we show through experiments how code annotations and their locations affect code prompting.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores code prompting, a neural symbolic prompting method with both zero-shot and few-shot versions which triggers code as intermediate steps, and performs extensive ablation studies and error analyses and identifies several exclusive advantages of using symbolic promptings compared to natural language."
            },
            "score": 2
        },
        {
            "id": "995b2f650f55de6077b87db6dadb01cecd86dbd7",
            "paperId": "995b2f650f55de6077b87db6dadb01cecd86dbd7",
            "title": "Advanced prompting as a catalyst: Empowering large language models in the management of gastrointestinal cancers",
            "abstract": "Large Language Models' (LLMs) performance in healthcare can be significantly impacted by prompt engineering. However, the area of study remains relatively uncharted in gastrointestinal oncology until now. Our research delves into this unexplored territory, investigating the efficacy of varied prompting strategies, including simple prompts, templated prompts, in-context learning (ICL), and multi-round iterative questioning, for optimizing the performance of LLMs within a medical setting. We develop a comprehensive evaluation system to assess the performance of LLMs across multiple dimensions. This robust evaluation system ensures a thorough assessment of the LLMs' capabilities in the field of medicine. Our findings suggest a positive relationship between the comprehensiveness of the prompts and the LLMs' performance. Notably, the multi-round strategy, which is characterized by iterative question-and-answer rounds, consistently yields the best results. ICL, a strategy that capitalizes on interrelated contextual learning, also displays significant promise, surpassing the outcomes achieved with simpler prompts. The research underscores the potential of advanced prompt engineering and iterative learning approaches for boosting the applicability of LLMs in healthcare. We recommend that additional research be conducted to refine these strategies and investigate their potential integration, to truly harness the full potential of LLMs in medical applications.\n",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The research investigates the efficacy of varied prompting strategies, including simple prompts, templated prompts, in-context learning (ICL), and multi-round iterative questioning, for optimizing the performance of LLMs within a medical setting, and develops a comprehensive evaluation system."
            },
            "score": 2
        },
        {
            "id": "1ed5d06c4dc46e6a983597b740ab0a31d0ce22ad",
            "paperId": "1ed5d06c4dc46e6a983597b740ab0a31d0ce22ad",
            "title": "Contextual Biasing of Named-Entities with Large Language Models",
            "abstract": "This paper studies contextual biasing with Large Language Models (LLMs), where during second-pass rescoring additional contextual information is provided to a LLM to boost Automatic Speech Recognition (ASR) performance. We propose to leverage prompts for a LLM without fine tuning during rescoring which incorporate a biasing list and few-shot examples to serve as additional information when calculating the score for the hypothesis. In addition to few-shot prompt learning, we propose multi-task training of the LLM to predict both the entity class and the next token. To improve the efficiency for contextual biasing and to avoid exceeding LLMs' maximum sequence lengths, we propose dynamic prompting, where we select the most likely class using the class tag prediction, and only use entities in this class as contexts for next token prediction. Word Error Rate (WER) evaluation is performed on i) an internal calling, messaging, and dictation dataset, and ii) the SLUE-Voxpopuli dataset. Results indicate that biasing lists and few-shot examples can achieve 17.8% and 9.6% relative improvement compared to first pass ASR, and that multi-task training and dynamic prompting can achieve 20.0% and 11.3% relative WER improvement, respectively.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "To improve the efficiency for contextual biasing and to avoid exceeding LLMs' maximum sequence lengths, this paper proposes dynamic prompting, where the most likely class is selected using the class tag prediction, and only use entities in this class as contexts for next token prediction."
            },
            "score": 2
        },
        {
            "id": "b2c096789f8c18a4ce7a538b1fe8bb222fcf02fc",
            "paperId": "b2c096789f8c18a4ce7a538b1fe8bb222fcf02fc",
            "title": "An Impact of Poisoning Attacks on Machine Learning Algorithms",
            "abstract": "Machine learning models, ubiquitous in domains like natural language processing and image recognition, are vulnerable to adversarial poisoning attacks, where malicious actors manipulate training data to induce erroneous predictions. This study delves into the susceptibility of Support Vector Machine (SVM) models to such attacks, employing the Gradient Ascent Poisoning (GAP) approach. The findings reveal a substantial impact of GAP on SVM performance, leading to a notable decline in accuracy from 81% to 73%. Beyond presenting empirical evidence, the paper highlights the broader challenges of detecting and mitigating poisoning attacks, underscoring the need for robust security measures in machine learning systems. By illuminating the effectiveness of adversarial poisoning techniques and their ramifications, this research contributes to the ongoing endeavor to enhance the resilience of machine learning models in practical applications.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A substantial impact of GAP on SVM performance is revealed, leading to a notable decline in accuracy, leading to the broader challenges of detecting and mitigating poisoning attacks, underscoring the need for robust security measures in machine learning systems."
            },
            "score": 2
        },
        {
            "id": "c060abf4bbf6fba63898b14a3a990323d4d0afd0",
            "paperId": "c060abf4bbf6fba63898b14a3a990323d4d0afd0",
            "title": "Robustness of Speech Spoofing Detectors Against Adversarial Post-Processing of Voice Conversion",
            "abstract": "With the development of speech synthesis and voice conversion techniques, the quality of artificially generated speech has been significantly improved and detecting such spoofing speech becomes crucial to practical applications, such as automatic speaker verification (ASV). State-of-the-art neural-network-based spoofing detection models can distinguish most artificial utterances from natural ones effectively in the latest ASVspoof 2019 evaluation. Motivated by recent progresses of adversarial example generation, this paper studies the robustness of neural-network-based speech spoofing detectors against adversarial attacks. To this end, an adversarial post-processing network (APN) is proposed which generates adversarial examples against a white-box anti-spoofing model by post-processing the speech waveforms produced by a baseline voice conversion system. Experimental results demonstrate the adversarial ability of our proposed APNs against the white-box anti-spoofing models which were used as the adversarial targets of APNs at the training stage. For example, the equal error rate (EER) of a fused detection model based on light convolution neural networks (LCNNs) increased from 0.278% to 12.743% under the white-box condition without degrading the subjective quality of converted speech. Furthermore, the trained APNs can also perform against the detectors with either unseen structures or unseen features by raising their EERs in our experiments. All these results indicate the threat of adversarial speech generation to the performance of state-of-the-art spoofing detection models.",
            "year": 2021,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An adversarial post-processing network (APN) is proposed which generates adversarial examples against a white-box anti-spoofing model by post- processing the speech waveforms produced by a baseline voice conversion system, demonstrating the robustness of neural-network-based speech spoofing detectors against adversarial attacks."
            },
            "score": 2
        },
        {
            "id": "d1ae3f76832bc165d13da5d4e025ab7218fc7d8a",
            "paperId": "d1ae3f76832bc165d13da5d4e025ab7218fc7d8a",
            "title": "Towards Detecting and Exploiting Disambiguation Biases in Neural Machine Translation",
            "abstract": "Word sense disambiguation is a well-known source of translation errors in NMT. We posit that some of the incorrect disambiguation choices are due to models' over-reliance on dataset artifacts found in training data, specifically superficial word co-occurrences, rather than a deeper understanding of the source text. We introduce a method for the prediction of disambiguation errors based on statistical data properties, demonstrating its effectiveness across several domains and model types. Moreover, we develop a simple adversarial attack strategy that minimally perturbs sentences in order to elicit disambiguation errors to further probe the robustness of translation models. Our findings indicate that disambiguation robustness varies substantially between domains and that different models trained on the same data are vulnerable to different attacks.",
            "year": 2020,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A method for the prediction of disambiguation errors based on statistical data properties is introduced, demonstrating its effectiveness across several domains and model types and developing a simple adversarial attack strategy that minimally perturbs sentences in order to elicit disambigsuation errors to further probe the robustness of translation models."
            },
            "score": 2
        },
        {
            "id": "90c770f363c45a4fec16578a898cf7c9a4f1451a",
            "paperId": "90c770f363c45a4fec16578a898cf7c9a4f1451a",
            "title": "A Label-Based Approach for Automatic Identifying Adversarial Examples with Image Transformation",
            "abstract": "Besides extraordinary results that deep neural networks are widely deployed in many fields such as computer vision, speech recognition, and natural language processing, in recent years deep neural networks have been recognized vulnerable to intentional modification of legitimate inputs called adversarial examples. These patterns are almost indistinguishable from AI models and human perception. Adversarial examples' concern is rising dramatically and is attracted by many research's apprehensiveness because of its tremendous impact. Unfortunately, until now there is none of the defenses has been shown to be very effective. In this paper, we introduce a new defense strategy against adversarial examples by using a label-based end-to-end system. Our proposed defense system can mostly distinguish adversarial samples and benign images without human intervention. We exploit the important role of spatial domain in adversarial attacks and proposing a state-of-the-art method for detecting adversarial examples based on our observation. We evaluate our system's performance on a variety of standard benchmark datasets including MNIST and ImageNet. Our proposed method reached out detection rates in a range from 94.6% to 99.2% in many settings.",
            "year": 2019,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a new defense strategy against adversarial examples by using a label-based end-to-end system and exploiting the important role of spatial domain in adversarial attacks and proposing a state-of-the-art method for detecting adversarialExamples."
            },
            "score": 2
        },
        {
            "id": "5040edc0f88b95697f59b330aef4a2c48a177a9d",
            "paperId": "5040edc0f88b95697f59b330aef4a2c48a177a9d",
            "title": "Evaluating and Enhancing the Robustness of Code Pre-trained Models through Structure-Aware Adversarial Samples Generation",
            "abstract": null,
            "year": 2023,
            "citationCount": 1,
            "tldr": null,
            "score": 2
        },
        {
            "id": "bd7a45fa6dce81dbbc61a5728bf54e040e206da7",
            "paperId": "bd7a45fa6dce81dbbc61a5728bf54e040e206da7",
            "title": "Watermarking Text Generated by Black-Box Language Models",
            "abstract": "LLMs now exhibit human-like skills in various fields, leading to worries about misuse. Thus, detecting generated text is crucial. However, passive detection methods are stuck in domain specificity and limited adversarial robustness. To achieve reliable detection, a watermark-based method was proposed for white-box LLMs, allowing them to embed watermarks during text generation. The method involves randomly dividing the model vocabulary to obtain a special list and adjusting the probability distribution to promote the selection of words in the list. A detection algorithm aware of the list can identify the watermarked text. However, this method is not applicable in many real-world scenarios where only black-box language models are available. For instance, third-parties that develop API-based vertical applications cannot watermark text themselves because API providers only supply generated text and withhold probability distributions to shield their commercial interests. To allow third-parties to autonomously inject watermarks into generated text, we develop a watermarking framework for black-box language model usage scenarios. Specifically, we first define a binary encoding function to compute a random binary encoding corresponding to a word. The encodings computed for non-watermarked text conform to a Bernoulli distribution, wherein the probability of a word representing bit-1 being approximately 0.5. To inject a watermark, we alter the distribution by selectively replacing words representing bit-0 with context-based synonyms that represent bit-1. A statistical test is then used to identify the watermark. Experiments demonstrate the effectiveness of our method on both Chinese and English datasets. Furthermore, results under re-translation, polishing, word deletion, and synonym substitution attacks reveal that it is arduous to remove the watermark without compromising the original semantics.",
            "year": 2023,
            "citationCount": 21,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A watermarking framework for black-box language model usage scenarios and results under re-translation, polishing, word deletion, and synonym substitution attacks reveal that it is arduous to remove the watermark without compromising the original semantics."
            },
            "score": 2
        },
        {
            "id": "6b10327b980dc12a67fcad5f0eae4f72b8ba112e",
            "paperId": "6b10327b980dc12a67fcad5f0eae4f72b8ba112e",
            "title": "Can we steal your vocal identity from the Internet?: Initial investigation of cloning Obama's voice using GAN, WaveNet and low-quality found data",
            "abstract": "Thanks to the growing availability of spoofing databases and rapid advances in using them, systems for detecting voice spoofing attacks are becoming more and more capable, and error rates close to zero are being reached for the ASVspoof2015 database. However, speech synthesis and voice conversion paradigms that are not considered in the ASVspoof2015 database are appearing. Such examples include direct waveform modelling and generative adversarial networks. We also need to investigate the feasibility of training spoofing systems using only low-quality found data. For that purpose, we developed a generative adversarial network-based speech enhancement system that improves the quality of speech data found in publicly available sources. Using the enhanced data, we trained state-of-the-art text-to-speech and voice conversion models and evaluated them in terms of perceptual speech quality and speaker similarity. The results show that the enhancement models significantly improved the SNR of low-quality degraded data found in publicly available sources and that they significantly improved the perceptual cleanliness of the source speech without significantly degrading the naturalness of the voice. However, the results also show limitations when generating speech with the low-quality found data.",
            "year": 2018,
            "citationCount": 70,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A generative adversarial network-based speech enhancement system that improves the quality of speech data found in publicly available sources and significantly improved the perceptual cleanliness of the source speech without significantly degrading the naturalness of the voice."
            },
            "score": 1
        }
    ],
    "novelty": "yes"
}