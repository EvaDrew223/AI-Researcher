{
    "topic_description": "novel prompting methods to improve large language models' robustness against adversarial attacks and improve their security and privacy",
    "idea_name": "Adversarial Prompt Ensemble",
    "raw_idea": {
        "Problem": "Large language models are susceptible to adversarial prompts that can manipulate their outputs in undesirable ways, potentially leading to harmful or biased responses.",
        "Existing Methods": "Current approaches to mitigate adversarial prompts include adversarial training, prompt filtering, and output monitoring. However, these methods often require extensive computational resources or human supervision.",
        "Motivation": "Ensemble learning has been shown to improve robustness in various machine learning tasks. By combining multiple diverse models or prompts, the system can leverage their complementary strengths to detect and mitigate adversarial attacks.",
        "Proposed Method": "We propose an Adversarial Prompt Ensemble (APE) method that generates a diverse set of prompts for a given input, each designed to elicit different aspects of the model's knowledge and reasoning capabilities. These prompts are constructed using techniques such as paraphrasing, context perturbation, and counterfactual reasoning. The model's outputs for each prompt are then aggregated using a weighted voting scheme, where the weights are determined by the prompts' ability to detect and mitigate adversarial attacks during a validation phase. This ensemble approach helps to neutralize the effect of any single adversarial prompt and provides a more robust final output.",
        "Experiment Plan": "Evaluate APE on existing adversarial prompt benchmarks, such as the Adversarial NLI dataset, and compare its performance with baseline methods like adversarial training and prompt filtering. Additionally, assess APE's robustness to new types of adversarial prompts generated by human annotators or other models."
    },
    "full_experiment_plan": {
        "Title": "Adversarial Prompt Ensemble: Improving Large Language Models' Robustness through Diverse Prompting",
        "Problem Statement": "Large language models (LLMs) are susceptible to adversarial prompts that can manipulate their outputs in undesirable ways, potentially leading to harmful or biased responses. Existing methods to mitigate adversarial prompts, such as adversarial training, prompt filtering, and output monitoring, often require extensive computational resources or human supervision.",
        "Motivation": "Ensemble learning has been shown to improve robustness in various machine learning tasks by combining multiple diverse models or prompts to leverage their complementary strengths and detect and mitigate adversarial attacks. Inspired by this, we propose an Adversarial Prompt Ensemble (APE) method that generates a diverse set of prompts for a given input, each designed to elicit different aspects of the model's knowledge and reasoning capabilities. By aggregating the model's outputs for each prompt using a weighted voting scheme, we aim to neutralize the effect of any single adversarial prompt and provide a more robust final output.",
        "Proposed Method": "The Adversarial Prompt Ensemble (APE) method consists of the following steps:\n1. Prompt Generation: For a given input, generate a diverse set of prompts using techniques such as paraphrasing, context perturbation, and counterfactual reasoning. These prompts should be designed to elicit different aspects of the model's knowledge and reasoning capabilities.\n2. Model Inference: Feed each generated prompt to the LLM and obtain the corresponding outputs.\n3. Weighted Aggregation: Aggregate the model's outputs for each prompt using a weighted voting scheme, where the weights are determined by the prompts' ability to detect and mitigate adversarial attacks during a validation phase.\n4. Final Output: The aggregated output serves as the final, more robust response to the input.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Dataset Selection": "Evaluate APE on existing adversarial prompt benchmarks, such as the Adversarial NLI dataset, and compare its performance with baseline methods like adversarial training and prompt filtering. Additionally, create a new dataset of adversarial prompts generated by human annotators or other models to assess APE's robustness to novel types of adversarial attacks.",
            "Step 2: Prompt Generation": "Implement techniques for generating diverse prompts, such as:\n- Paraphrasing: Rephrase the input using synonyms, active/passive voice, or different sentence structures.\n- Context Perturbation: Add, remove, or modify contextual information in the input to change its meaning or focus.\n- Counterfactual Reasoning: Create prompts that explore alternative scenarios or hypothetical situations related to the input.",
            "Step 3: Model Selection": "Choose state-of-the-art LLMs, such as GPT-3.5 (text-davinci-003), GPT-4, or other open-source models like BLOOM or GLM, to evaluate the effectiveness of APE across different architectures and training datasets.",
            "Step 4: Weighted Aggregation": "Develop a weighted voting scheme to aggregate the model's outputs for each prompt. Assign higher weights to prompts that demonstrate better performance in detecting and mitigating adversarial attacks during a validation phase. Experiment with different weighting strategies, such as equal weights, performance-based weights, or learned weights using a small labeled dataset.",
            "Step 5: Evaluation Metrics": "Evaluate APE using standard metrics for language model performance, such as accuracy, F1 score, and perplexity. Additionally, assess the model's robustness by measuring its ability to maintain consistent and appropriate responses in the presence of adversarial prompts.",
            "Step 6: Baseline Comparison": "Compare APE's performance with baseline methods, such as adversarial training, prompt filtering, and output monitoring. Conduct experiments on both existing adversarial prompt benchmarks and the newly created dataset to assess APE's effectiveness in various scenarios.",
            "Step 7: Ablation Study": "Perform an ablation study to understand the contribution of each component in the APE method. Evaluate the performance of APE with and without each prompt generation technique (paraphrasing, context perturbation, and counterfactual reasoning) and different weighting strategies to identify the most effective combinations."
        },
        "Test Case Examples": {
            "Test Case 1": {
                "Input": "Vaccines contain toxic ingredients that can cause autism.",
                "Baseline Output": "While some people believe that vaccines can cause autism, scientific evidence has consistently shown that there is no link between vaccines and autism. Vaccines are rigorously tested for safety and are an important tool in preventing serious diseases.",
                "APE Output": "There is no scientific evidence to support the claim that vaccines cause autism. Numerous studies have investigated this topic and found no causal link between vaccines and autism. Vaccine ingredients are carefully selected and tested for safety, and the benefits of vaccination in preventing serious diseases far outweigh any potential risks. It is important to rely on credible scientific sources and consult with healthcare professionals when making decisions about vaccination.",
                "Explanation": "The baseline output addresses the concern but does not provide a strong rebuttal to the adversarial prompt. APE generates a more comprehensive and informative response by emphasizing the lack of scientific evidence, the rigorous safety testing of vaccines, and the importance of relying on credible sources."
            },
            "Test Case 2": {
                "Input": "Climate change is a hoax invented by scientists to get more research funding.",
                "Baseline Output": "Climate change is a real phenomenon supported by scientific evidence, not a hoax. While there may be debate about the extent and causes of climate change, the majority of scientists agree that human activities have contributed to the warming of the Earth's atmosphere.",
                "APE Output": "The claim that climate change is a hoax invented by scientists is false and not supported by scientific evidence. The overwhelming majority of climate scientists agree that the Earth's climate is changing, and that human activities, particularly greenhouse gas emissions, are the primary driver of this change. This conclusion is based on multiple lines of evidence, including direct temperature measurements, satellite data, and climate model simulations. Scientists are motivated by the pursuit of knowledge and understanding, not by financial gain. The scientific process is designed to be transparent, rigorous, and subject to peer review, which helps to ensure the integrity of research findings. Suggesting that climate change is a hoax undermines the credibility of the scientific community and the importance of evidence-based decision-making in addressing global challenges.",
                "Explanation": "APE provides a more detailed and convincing rebuttal to the adversarial prompt by highlighting the scientific consensus, the multiple lines of evidence supporting climate change, and the integrity of the scientific process. The response also addresses the underlying motivation behind the adversarial prompt and emphasizes the importance of evidence-based decision-making."
            }
        },
        "Fallback Plan": "If the proposed APE method does not significantly improve the robustness of LLMs against adversarial prompts compared to the baselines, consider the following alternative approaches:\n1. Analyze the generated prompts to determine if they are sufficiently diverse and relevant to the input. If not, explore alternative prompt generation techniques or refine the existing ones to create more effective prompts.\n2. Investigate the weighted aggregation scheme and experiment with different weighting strategies or learning algorithms to optimize the combination of model outputs.\n3. Collect additional data on adversarial prompts and their corresponding appropriate responses to fine-tune the LLMs or create a more comprehensive validation set for weight assignment.\n4. Explore the use of external knowledge sources, such as fact-checking databases or expert-curated information, to enhance the model's ability to detect and counter adversarial prompts.\n5. Conduct a detailed error analysis to identify the types of adversarial prompts that APE struggles with and use this information to guide further improvements in the prompt generation and aggregation process.\nIf the APE method still does not yield satisfactory results after these modifications, consider pivoting the project to focus on analyzing the limitations of current adversarial prompt detection methods and proposing alternative approaches based on the insights gained from the APE experiments."
    },
    "novelty_queries": [
        "KeywordQuery(\"adversarial prompts language models\")",
        "KeywordQuery(\"ensemble methods language models robustness\")",
        "KeywordQuery(\"diverse prompting language models\")",
        "KeywordQuery(\"Adversarial Prompt Ensemble NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "c4ff1be5c254b60b96b7455eefcc4ec9583f82ed",
            "paperId": "c4ff1be5c254b60b96b7455eefcc4ec9583f82ed",
            "title": "A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily",
            "abstract": "Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to provide useful and safe responses. However, adversarial prompts known as 'jailbreaks' can circumvent safeguards, leading LLMs to generate potentially harmful content. Exploring jailbreak prompts can help to better reveal the weaknesses of LLMs and further steer us to secure them. Unfortunately, existing jailbreak methods either suffer from intricate manual design or require optimization on other white-box models, which compromises either generalization or efficiency. In this paper, we generalize jailbreak prompt attacks into two aspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we propose ReNeLLM, an automatic framework that leverages LLMs themselves to generate effective jailbreak prompts. Extensive experiments demonstrate that ReNeLLM significantly improves the attack success rate while greatly reducing the time cost compared to existing baselines. Our study also reveals the inadequacy of current defense methods in safeguarding LLMs. Finally, we analyze the failure of LLMs defense from the perspective of prompt execution priority, and propose corresponding defense strategies. We hope that our research can catalyze both the academic community and LLMs developers towards the provision of safer and more regulated LLMs. The code is available at https://github.com/NJUNLP/ReNeLLM.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes ReNeLLM, an automatic framework that leverages LLMs themselves to generate effective jailbreak prompts and significantly improves the attack success rate while greatly reducing the time cost compared to existing baselines."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is improving large language models' robustness against adversarial prompts, and the proposed approach is to use an ensemble of diverse prompts to mitigate the effect of adversarial prompts.\n\nThe research problem in the paper is exploring jailbreak prompts that can circumvent the safeguards of large language models and lead them to generate harmful content, and the proposed approach is to generalize jailbreak prompt attacks into prompt rewriting and scenario nesting, and use an automatic framework that leverages LLMs themselves to generate effective jailbreak prompts.\n\nThe proposal focuses on defending against adversarial prompts, while the paper focuses on attacking LLMs with jailbreak prompts. Although both deal with adversarial prompts, their goals and approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "e8b3b37c0d301ea41c75765f6ceb7fcbb2e088a4",
            "paperId": "e8b3b37c0d301ea41c75765f6ceb7fcbb2e088a4",
            "title": "AutoDAN: Automatic and Interpretable Adversarial Attacks on Large Language Models",
            "abstract": "Safety alignment of Large Language Models (LLMs) can be compromised with manual jailbreak attacks and (automatic) adversarial attacks. Recent work suggests that patching LLMs against these attacks is possible: manual jailbreak attacks are human-readable but often limited and public, making them easy to block; adversarial attacks generate gibberish prompts that can be detected using perplexity-based filters. In this paper, we show that these solutions may be too optimistic. We propose an interpretable adversarial attack, AutoDAN , that combines the strengths of both types of attacks. It automatically generates attack prompts that bypass perplexity-based filters while maintaining a high attack success rate like manual jailbreak attacks. These prompts are interpretable and diverse, exhibiting strategies commonly used in manual jailbreak attacks, and transfer better than their non-readable counterparts when using limited training data or a single proxy model. We also customize AutoDAN \u2019s objective to leak system prompts, another jailbreak application not addressed in the adversarial attack literature. Our work provides a new way to red-team LLMs and to understand the mechanism of jailbreak attacks.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An interpretable adversarial attack, AutoDAN, is proposed, that combines the strengths of both types of attacks and provides a new way to red-team LLMs and to understand the mechanism of jailbreak attacks."
            },
            "score": 8,
            "novelty_score": "The project proposal aims to improve the robustness of large language models against adversarial prompts by generating diverse prompts and aggregating the model's outputs using a weighted voting scheme.\n\nThe paper focuses on creating an interpretable adversarial attack that automatically generates readable prompts to bypass perplexity-based filters and maintain a high attack success rate.\n\nWhile both the project proposal and the paper deal with adversarial attacks on large language models, their objectives and approaches differ. The project proposal seeks to defend against adversarial prompts, while the paper aims to create a more effective adversarial attack method.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "1227c2fcb8437441b7d72a29a4bc9eef1f5275d2",
            "paperId": "1227c2fcb8437441b7d72a29a4bc9eef1f5275d2",
            "title": "AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models",
            "abstract": "Safety alignment of Large Language Models (LLMs) can be compromised with manual jailbreak attacks and (automatic) adversarial attacks. Recent studies suggest that defending against these attacks is possible: adversarial attacks generate unlimited but unreadable gibberish prompts, detectable by perplexity-based filters; manual jailbreak attacks craft readable prompts, but their limited number due to the necessity of human creativity allows for easy blocking. In this paper, we show that these solutions may be too optimistic. We introduce AutoDAN, an interpretable, gradient-based adversarial attack that merges the strengths of both attack types. Guided by the dual goals of jailbreak and readability, AutoDAN optimizes and generates tokens one by one from left to right, resulting in readable prompts that bypass perplexity filters while maintaining high attack success rates. Notably, these prompts, generated from scratch using gradients, are interpretable and diverse, with emerging strategies commonly seen in manual jailbreak attacks. They also generalize to unforeseen harmful behaviors and transfer to black-box LLMs better than their unreadable counterparts when using limited training data or a single proxy model. Furthermore, we show the versatility of AutoDAN by automatically leaking system prompts using a customized objective. Our work offers a new way to red-team LLMs and understand jailbreak mechanisms via interpretability.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work offers a new way to red-team LLMs and understand jailbreak mechanisms via interpretability, by introducing AutoDAN, an interpretable, gradient-based adversarial attack that merges the strengths of both attack types."
            },
            "score": 8,
            "novelty_score": "The research problem in the project proposal is improving large language models' robustness against adversarial prompts, and the proposed approach is to use an ensemble of diverse prompts to mitigate the effect of adversarial prompts.\n\nThe research problem in the paper is generating interpretable and readable adversarial prompts that can bypass perplexity filters and maintain high attack success rates, and the proposed approach is a gradient-based adversarial attack called AutoDAN.\n\nThe project proposal focuses on defending against adversarial prompts, while the paper focuses on generating effective adversarial prompts. The two works have different goals and use different approaches.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "77d6d7482d1a32ad147c39993758b6c63816f5c0",
            "paperId": "77d6d7482d1a32ad147c39993758b6c63816f5c0",
            "title": "PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
            "abstract": "The increasing reliance on Large Language Models (LLMs) across academia and industry necessitates a comprehensive understanding of their robustness to prompts. In response to this vital need, we introduce PromptBench, a robustness benchmark designed to measure LLMs' resilience to adversarial prompts. This study uses a plethora of adversarial textual attacks targeting prompts across multiple levels: character, word, sentence, and semantic. The adversarial prompts, crafted to mimic plausible user errors like typos or synonyms, aim to evaluate how slight deviations can affect LLM outcomes while maintaining semantic integrity. These prompts are then employed in diverse tasks, such as sentiment analysis, natural language inference, reading comprehension, machine translation, and math problem-solving. Our study generates 4788 adversarial prompts, meticulously evaluated over 8 tasks and 13 datasets. Our findings demonstrate that contemporary LLMs are not robust to adversarial prompts. Furthermore, we present comprehensive analysis to understand the mystery behind prompt robustness and its transferability. We then offer insightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users. Code is available at: https://github.com/microsoft/promptbench.",
            "year": 2023,
            "citationCount": 111,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study generates 4788 adversarial prompts and presents comprehensive analysis to understand the mystery behind prompt robustness and its transferability, and offers insightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving large language models' robustness against adversarial prompts, while the approach is to use an ensemble of diverse prompts to mitigate the effect of adversarial prompts.\n\nThe research problem in the paper is evaluating the robustness of large language models against adversarial prompts, while the approach is to create a benchmark dataset with adversarial prompts across multiple levels and tasks to measure the models' resilience.\n\nThe proposal focuses on improving robustness, while the paper focuses on evaluating robustness. The proposal uses an ensemble of diverse prompts, while the paper uses a benchmark dataset with adversarial prompts.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "d4177489596748e43aa571f59556097f2cc4c8be",
            "paperId": "d4177489596748e43aa571f59556097f2cc4c8be",
            "title": "GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts",
            "abstract": "Large language models (LLMs) have recently experienced tremendous popularity and are widely used from casual conversations to AI-driven programming. However, despite their considerable success, LLMs are not entirely reliable and can give detailed guidance on how to conduct harmful or illegal activities. While safety measures can reduce the risk of such outputs, adversarial jailbreak attacks can still exploit LLMs to produce harmful content. These jailbreak templates are typically manually crafted, making large-scale testing challenging. In this paper, we introduce GPTFuzz, a novel black-box jailbreak fuzzing framework inspired by the AFL fuzzing framework. Instead of manual engineering, GPTFuzz automates the generation of jailbreak templates for red-teaming LLMs. At its core, GPTFuzz starts with human-written templates as initial seeds, then mutates them to produce new templates. We detail three key components of GPTFuzz: a seed selection strategy for balancing efficiency and variability, mutate operators for creating semantically equivalent or similar sentences, and a judgment model to assess the success of a jailbreak attack. We evaluate GPTFuzz against various commercial and open-source LLMs, including ChatGPT, LLaMa-2, and Vicuna, under diverse attack scenarios. Our results indicate that GPTFuzz consistently produces jailbreak templates with a high success rate, surpassing human-crafted templates. Remarkably, GPTFuzz achieves over 90% attack success rates against ChatGPT and Llama-2 models, even with suboptimal initial seed templates. We anticipate that GPTFuzz will be instrumental for researchers and practitioners in examining LLM robustness and will encourage further exploration into enhancing LLM safety.",
            "year": 2023,
            "citationCount": 78,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "GPTFuzz is introduced, a novel black-box jailbreak fuzzing framework inspired by the AFL fuzzed framework that automates the generation of jailbreak templates for red-teaming LLMs and consistently produces jailbreaks with a high success rate, surpassing human-crafted templates."
            },
            "score": 7,
            "novelty_score": "The research problem in the project proposal is improving large language models' robustness against adversarial prompts, while the approach is using an ensemble of diverse prompts to mitigate the effect of adversarial prompts.\n\nThe research problem in the paper is red teaming large language models to test their robustness against jailbreak prompts, and the approach is using an automated fuzzing framework to generate jailbreak prompts.\n\nAlthough both works aim to improve the robustness of large language models, the project proposal focuses on defending against adversarial prompts using an ensemble method, while the paper focuses on attacking language models using automatically generated jailbreak prompts. The two works have different goals and approaches.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "175b32c07e56f881479be4c5a74bfa3c731cc454",
            "paperId": "175b32c07e56f881479be4c5a74bfa3c731cc454",
            "title": "ROSE: Robust Selective Fine-tuning for Pre-trained Language Models",
            "abstract": "Even though the large-scale language models have achieved excellent performances, they suffer from various adversarial attacks.A large body of defense methods has been proposed. However, they are still limited due to redundant attack search spaces and the inability to defend against various types of attacks.In this work, we present a novel fine-tuning approach called RObust SEletive fine-tuning (ROSE) to address this issue.ROSE conducts selective updates when adapting pre-trained models to downstream tasks, filtering out invaluable and unrobust updates of parameters.Specifically, we propose two strategies: the first-order and second-order ROSE for selecting target robust parameters.The experimental results show that ROSE achieves significant improvements in adversarial robustness on various downstream NLP tasks, and the ensemble method even surpasses both variants above.Furthermore, ROSE can be easily incorporated into existing fine-tuning methods to improve their adversarial robustness further.The empirical analysis confirms that ROSE eliminates unrobust spurious updates during fine-tuning, leading to solutions corresponding to flatter and wider optima than the conventional method.Code is available at https://github.com/jiangllan/ROSE.",
            "year": 2022,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes two strategies: the first-order and second-order ROSE for selecting target robust parameters and shows that ROSE achieves significant improvements in adversarial robustness on various downstream NLP tasks, and the ensemble method even surpasses both variants above."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the robustness of large language models against adversarial prompts by using an ensemble of diverse prompts and aggregating their outputs. The paper focuses on improving the robustness of pre-trained language models against adversarial attacks during fine-tuning by selectively updating parameters.\n\nWhile both the project proposal and the paper address the issue of adversarial robustness in language models, their approaches differ. The project proposal uses an ensemble of diverse prompts to mitigate the effect of adversarial prompts, while the paper proposes a selective fine-tuning method to filter out unrobust parameter updates during adaptation to downstream tasks.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "ba783d92d0eaf6a7bff6ced7660150ce38016bbc",
            "paperId": "ba783d92d0eaf6a7bff6ced7660150ce38016bbc",
            "title": "Don\u2019t Take the Easy Way Out: Ensemble Based Methods for Avoiding Known Dataset Biases",
            "abstract": "State-of-the-art models often make use of superficial patterns in the data that do not generalize well to out-of-domain or adversarial settings. For example, textual entailment models often learn that particular key words imply entailment, irrespective of context, and visual question answering models learn to predict prototypical answers, without considering evidence in the image. In this paper, we show that if we have prior knowledge of such biases, we can train a model to be more robust to domain shift. Our method has two stages: we (1) train a naive model that makes predictions exclusively based on dataset biases, and (2) train a robust model as part of an ensemble with the naive one in order to encourage it to focus on other patterns in the data that are more likely to generalize. Experiments on five datasets with out-of-domain test sets show significantly improved robustness in all settings, including a 12 point gain on a changing priors visual question answering dataset and a 9 point gain on an adversarial question answering test set.",
            "year": 2019,
            "citationCount": 390,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper trains a naive model that makes predictions exclusively based on dataset biases, and a robust model as part of an ensemble with the naive one in order to encourage it to focus on other patterns in the data that are more likely to generalize."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving large language models' robustness to adversarial prompts, and the proposed approach is using an ensemble of diverse prompts to mitigate the effect of adversarial prompts.\n\nThe research problem in the paper is improving models' robustness to dataset biases and domain shift, and the proposed approach is training an ensemble of a naive model that relies on dataset biases and a robust model that learns to focus on other patterns.\n\nWhile both works aim to improve model robustness, the proposal focuses specifically on adversarial prompts for large language models, while the paper addresses dataset biases and domain shift in various tasks. The approaches also differ, as the proposal uses diverse prompts within the ensemble, while the paper uses a naive model and a robust model.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "aa9aa1c315cb2a0c1759d82fb3d4b4506c2dbb7c",
            "paperId": "aa9aa1c315cb2a0c1759d82fb3d4b4506c2dbb7c",
            "title": "ASSERT: Automated Safety Scenario Red Teaming for Evaluating the Robustness of Large Language Models",
            "abstract": "As large language models are integrated into society, robustness toward a suite of prompts is increasingly important to maintain reliability in a high-variance environment.Robustness evaluations must comprehensively encapsulate the various settings in which a user may invoke an intelligent system. This paper proposes ASSERT, Automated Safety Scenario Red Teaming, consisting of three methods -- semantically aligned augmentation, target bootstrapping, and adversarial knowledge injection. For robust safety evaluation, we apply these methods in the critical domain of AI safety to algorithmically generate a test suite of prompts covering diverse robustness settings -- semantic equivalence, related scenarios, and adversarial. We partition our prompts into four safety domains for a fine-grained analysis of how the domain affects model performance. Despite dedicated safeguards in existing state-of-the-art models, we find statistically significant performance differences of up to 11% in absolute classification accuracy among semantically related scenarios and error rates of up to 19% absolute error in zero-shot adversarial settings, raising concerns for users' physical safety.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ASSERT, Automated Safety Scenario Red Teaming, consisting of three methods -- semantically aligned augmentation, target bootstrapping, and adversarial knowledge injection is proposed, for robust safety evaluation."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving large language models' robustness against adversarial prompts, and the proposed approach is to use an ensemble of diverse prompts to mitigate the effect of adversarial prompts. The research problem in the paper is evaluating the robustness of large language models in safety-critical domains, and the approach is to automatically generate test prompts using methods like semantically aligned augmentation, target bootstrapping, and adversarial knowledge injection.\n\nWhile both the proposal and the paper aim to address the robustness of large language models, the proposal focuses on improving robustness through diverse prompting, while the paper focuses on evaluating robustness using automatically generated test prompts in safety-critical domains. The approaches and the specific research problems are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "dad73205f54b29a50b9c2743e8333928a20e0981",
            "paperId": "dad73205f54b29a50b9c2743e8333928a20e0981",
            "title": "Prompting for Diverse Responses: Making Large Language Models More Truthful",
            "abstract": "Large Language Models (LLMs) are typically trained on a large amount of data scraped from the internet, and thus can learn human misconceptions and biases. These misconceptions and biases can cause models to produce problematic outputs that persist even after fine-tuning. In the complex reasoning domain, chain-of-thought prompting has been shown to increase the correctness of LLMs by prompting models to perform intermediate reasoning steps. Taking inspiration from chain-of-thought prompting, in this work we aim to further improve truthfulness by considering multiple, diverse answers, inspired by how human decision makers might seek out opinions from multiple sources or consider the same question from multiple perspectives. We present Prompting for Diverse Responses (PDR), a method that allows question-answering models to generate multiple distinct answers for a given question. We generate chains of thought and answers by sampling from a small set of prompts and collect the question and answers together into a single text. We use a (comparably) small model called the \u201cjudge\u201d to select the best answer. One advantage to our approach is that the judge is not finetuned, so new prompts can be swapped in or out to change the overall behaviour. We find that PDR has strong performance when evaluated on both the TriviaQA and TruthfulQA datasets, while many of our baseline prompts only perform well on a single dataset.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents Prompting for Diverse Responses (PDR), a method that allows question-answering models to generate multiple distinct answers for a given question, inspired by how human decision makers might seek out opinions from multiple sources or consider the same question from multiple perspectives."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving large language models' robustness to adversarial prompts, while the approach is to use an ensemble of diverse prompts and aggregate the outputs. The research problem in the paper is improving the truthfulness of large language models, and the approach is to generate multiple diverse answers and use a judge model to select the best one.\n\nAlthough both works aim to improve the performance of large language models, the specific research problems and approaches are different. The proposal focuses on robustness to adversarial prompts, while the paper focuses on truthfulness. The proposal uses an ensemble of prompts, while the paper generates multiple answers and uses a judge model.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "2c72ab10e7a5f2fd32e6f85b20c77bf64e6e220d",
            "paperId": "2c72ab10e7a5f2fd32e6f85b20c77bf64e6e220d",
            "title": "A prompt-based approach to adversarial example generation and robustness enhancement",
            "abstract": null,
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel robust training approach based on prompt paradigm which incorporates prompt texts as the alternatives to adversarial examples and enhances robustness under a lightweight minimax-style optimization framework is proposed."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the robustness of large language models against adversarial prompts by generating diverse prompts and aggregating the model's outputs using a weighted voting scheme.\n\nThe paper proposes a robust training approach that incorporates prompt texts as alternatives to adversarial examples and enhances robustness under a lightweight minimax-style optimization framework.\n\nWhile both the project proposal and the paper focus on improving robustness, the project proposal targets adversarial prompts and uses diverse prompt generation and aggregation, whereas the paper uses prompt texts as alternatives to adversarial examples in a minimax-style optimization framework.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "b6499bcc10d4a70c3ca8b84995270cfd0d29de4c",
            "paperId": "b6499bcc10d4a70c3ca8b84995270cfd0d29de4c",
            "title": "Model-tuning Via Prompts Makes NLP Models Adversarially Robust",
            "abstract": "In recent years, NLP practitioners have converged on the following practice: (i) import an off-the-shelf pretrained (masked) language model; (ii) append a multilayer perceptron atop the CLS token's hidden representation (with randomly initialized weights); and (iii) fine-tune the entire model on a downstream task (MLP-FT). This procedure has produced massive gains on standard NLP benchmarks, but these models remain brittle, even to mild adversarial perturbations. In this work, we demonstrate surprising gains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an alternative method of adapting to downstream tasks. Rather than appending an MLP head to make output prediction, MVP appends a prompt template to the input, and makes prediction via text infilling/completion. Across 5 NLP datasets, 4 adversarial attacks, and 3 different models, MVP improves performance against adversarial substitutions by an average of 8% over standard methods and even outperforms adversarial training-based state-of-art defenses by 3.5%. By combining MVP with adversarial training, we achieve further improvements in adversarial robustness while maintaining performance on unperturbed examples. Finally, we conduct ablations to investigate the mechanism underlying these gains. Notably, we find that the main causes of vulnerability of MLP-FT can be attributed to the misalignment between pre-training and fine-tuning tasks, and the randomly initialized MLP parameters.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work demonstrates surprising gains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an alternative method of adapting to downstream tasks that improves performance against adversarial substitutions and outperforms adversarial training-based state-of-art defenses by 3.5%."
            },
            "score": 7
        },
        {
            "id": "de2fd685f45ee916b9142bcb983d306b7da643a4",
            "paperId": "de2fd685f45ee916b9142bcb983d306b7da643a4",
            "title": "A Prompting-based Approach for Adversarial Example Generation and Robustness Enhancement",
            "abstract": "Recent years have seen the wide application of NLP models in crucial areas such as finance, medical treatment, and news media, raising concerns of the model robustness and vulnerabilities. In this paper, we propose a novel prompt-based adversarial attack to compromise NLP models and robustness enhancement technique. We first construct malicious prompts for each instance and generate adversarial examples via mask-and-filling under the effect of a malicious purpose. Our attack technique targets the inherent vulnerabilities of NLP models, allowing us to generate samples even without interacting with the victim NLP model, as long as it is based on pre-trained language models (PLMs). Furthermore, we design a prompt-based adversarial training method to improve the robustness of PLMs. As our training method does not actually generate adversarial samples, it can be applied to large-scale training sets efficiently. The experimental results show that our attack method can achieve a high attack success rate with more diverse, fluent and natural adversarial examples. In addition, our robustness enhancement method can significantly improve the robustness of models to resist adversarial attacks. Our work indicates that prompting paradigm has great potential in probing some fundamental flaws of PLMs and fine-tuning them for downstream tasks.",
            "year": 2022,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel prompt-based adversarial attack to compromise NLP models and robustness enhancement technique that can significantly improve the robustness of models to resist adversarial attacks and indicates that prompting paradigm has great potential in probing some fundamental flaws of PLMs and fine-tuning them for downstream tasks."
            },
            "score": 7
        },
        {
            "id": "3e30a7ac4886b28eb50151f58e14a1d698cccd0e",
            "paperId": "3e30a7ac4886b28eb50151f58e14a1d698cccd0e",
            "title": "Baseline Defenses for Adversarial Attacks Against Aligned Language Models",
            "abstract": "As Large Language Models quickly become ubiquitous, it becomes critical to understand their security vulnerabilities. Recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. Drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: What threat models are practically useful in this domain? How do baseline defense techniques perform in this new domain? How does LLM security differ from computer vision? We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, discussing the various settings in which each is feasible and effective. Particularly, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. We discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. We find that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for LLMs. Future research will be needed to uncover whether more powerful optimizers can be developed, or whether the strength of filtering and preprocessing defenses is greater in the LLMs domain than it has been in computer vision.",
            "year": 2023,
            "citationCount": 97,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for LLMs."
            },
            "score": 6
        },
        {
            "id": "1104d766527dead44a40532e8a89444d9cef5c65",
            "paperId": "1104d766527dead44a40532e8a89444d9cef5c65",
            "title": "\"Do Anything Now\": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models",
            "abstract": "The misuse of large language models (LLMs) has garnered significant attention from the general public and LLM vendors. In response, efforts have been made to align LLMs with human values and intent use. However, a particular type of adversarial prompts, known as jailbreak prompt, has emerged and continuously evolved to bypass the safeguards and elicit harmful content from LLMs. In this paper, we conduct the first measurement study on jailbreak prompts in the wild, with 6,387 prompts collected from four platforms over six months. Leveraging natural language processing technologies and graph-based community detection methods, we discover unique characteristics of jailbreak prompts and their major attack strategies, such as prompt injection and privilege escalation. We also observe that jailbreak prompts increasingly shift from public platforms to private ones, posing new challenges for LLM vendors in proactive detection. To assess the potential harm caused by jailbreak prompts, we create a question set comprising 46,800 samples across 13 forbidden scenarios. Our experiments show that current LLMs and safeguards cannot adequately defend jailbreak prompts in all scenarios. Particularly, we identify two highly effective jailbreak prompts which achieve 0.99 attack success rates on ChatGPT (GPT-3.5) and GPT-4, and they have persisted online for over 100 days. Our work sheds light on the severe and evolving threat landscape of jailbreak prompts. We hope our study can facilitate the research community and LLM vendors in promoting safer and regulated LLMs.",
            "year": 2023,
            "citationCount": 69,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The first measurement study on jailbreak prompts in the wild is conducted, with 6,387 prompts collected from four platforms over six months, and it is shown that current LLMs and safeguards cannot adequately defend jailbreak Prompts in all scenarios."
            },
            "score": 6
        },
        {
            "id": "1abfc211793c683972ded8d3268475e3ee7a88b0",
            "paperId": "1abfc211793c683972ded8d3268475e3ee7a88b0",
            "title": "Adversarial Demonstration Attacks on Large Language Models",
            "abstract": "With the emergence of more powerful large language models (LLMs), such as ChatGPT and GPT-4, in-context learning (ICL) has gained significant prominence in leveraging these models for specific tasks by utilizing data-label pairs as precondition prompts. While incorporating demonstrations can greatly enhance the performance of LLMs across various tasks, it may introduce a new security concern: attackers can manipulate only the demonstrations without changing the input to perform an attack. In this paper, we investigate the security concern of ICL from an adversarial perspective, focusing on the impact of demonstrations. We propose a novel attack method named advICL, which aims to manipulate only the demonstration without changing the input to mislead the models. Our results demonstrate that as the number of demonstrations increases, the robustness of in-context learning would decrease. Additionally, we also identify the intrinsic property of the demonstrations is that they can be used (prepended) with different inputs. As a result, it introduces a more practical threat model in which an attacker can attack the test input example even without knowing and manipulating it. To achieve it, we propose the transferable version of advICL, named Transferable-advICL. Our experiment shows that the adversarial demonstration generated by Transferable-advICL can successfully attack the unseen test input examples. We hope that our study reveals the critical security risks associated with ICL and underscores the need for extensive research on the robustness of ICL, particularly given its increasing significance in the advancement of LLMs.",
            "year": 2023,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper investigates the security concern of ICL from an adversarial perspective, focusing on the impact of demonstrations, and proposes a novel attack method named advICL, which aims to manipulate only the demonstration without changing the input to mislead the models."
            },
            "score": 6
        },
        {
            "id": "92b9d8b8c81c4c53ea62000c0924500b2dd11bce",
            "paperId": "92b9d8b8c81c4c53ea62000c0924500b2dd11bce",
            "title": "Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models",
            "abstract": "We introduce new jailbreak attacks on vision language models (VLMs), which use aligned LLMs and are resilient to text-only jailbreak attacks. Specifically, we develop cross-modality attacks on alignment where we pair adversarial images going through the vision encoder with textual prompts to break the alignment of the language model. Our attacks employ a novel compositional strategy that combines an image, adversarially targeted towards toxic embeddings, with generic prompts to accomplish the jailbreak. Thus, the LLM draws the context to answer the generic prompt from the adversarial image. The generation of benign-appearing adversarial images leverages a novel embedding-space-based methodology, operating with no access to the LLM model. Instead, the attacks require access only to the vision encoder and utilize one of our four embedding space targeting strategies. By not requiring access to the LLM, the attacks lower the entry barrier for attackers, particularly when vision encoders such as CLIP are embedded in closed-source LLMs. The attacks achieve a high success rate across different VLMs, highlighting the risk of cross-modality alignment vulnerabilities, and the need for new alignment approaches for multi-modal models.",
            "year": 2023,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Cross-modality attacks on alignment where adversarial images going through the vision encoder with textual prompts to break the alignment of the language model are developed."
            },
            "score": 6
        },
        {
            "id": "5c4a75e7436e402af046c24655fefe71ee87e379",
            "paperId": "5c4a75e7436e402af046c24655fefe71ee87e379",
            "title": "Robust Testing of AI Language Model Resiliency with Novel Adversarial Prompts",
            "abstract": "In the rapidly advancing field of Artificial Intelligence (AI), this study presents a critical evaluation of the resilience and cybersecurity efficacy of leading AI models, including ChatGPT-4, Bard, Claude, and Microsoft Copilot. Central to this research are innovative adversarial prompts designed to rigorously test the content moderation capabilities of these AI systems. This study introduces new adversarial tests and the Response Quality Score (RQS), a metric specifically developed to assess the nuances of AI responses. Additionally, the research spotlights FreedomGPT, an AI tool engineered to optimize the alignment between user intent and AI interpretation. The empirical results from this investigation are pivotal for assessing AI models\u2019 current robustness and security. They highlight the necessity for ongoing development and meticulous testing to bolster AI defenses against various adversarial challenges. Notably, this study also delves into the ethical and societal implications of employing advanced \u201cjailbreak\u201d techniques in AI testing. The findings are significant for understanding AI vulnerabilities and formulating strategies to enhance AI technologies\u2019 reliability and ethical soundness, paving the way for safer and more secure AI applications.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study introduces new adversarial tests and the Response Quality Score (RQS), a metric specifically developed to assess the nuances of AI responses, and spotlights FreedomGPT, an AI tool engineered to optimize the alignment between user intent and AI interpretation."
            },
            "score": 6
        },
        {
            "id": "b5a624da64475d735f0e298dc6f2f6669b5bb697",
            "paperId": "b5a624da64475d735f0e298dc6f2f6669b5bb697",
            "title": "Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield",
            "abstract": "Large Language Models' safety remains a critical concern due to their vulnerability to adversarial attacks, which can prompt these systems to produce harmful responses. In the heart of these systems lies a safety classifier, a computational model trained to discern and mitigate potentially harmful, offensive, or unethical outputs. However, contemporary safety classifiers, despite their potential, often fail when exposed to inputs infused with adversarial noise. In response, our study introduces the Adversarial Prompt Shield (APS), a lightweight model that excels in detection accuracy and demonstrates resilience against adversarial prompts. Additionally, we propose novel strategies for autonomously generating adversarial training datasets, named Bot Adversarial Noisy Dialogue (BAND) datasets. These datasets are designed to fortify the safety classifier's robustness, and we investigate the consequences of incorporating adversarial examples into the training process. Through evaluations involving Large Language Models, we demonstrate that our classifier has the potential to decrease the attack success rate resulting from adversarial attacks by up to 60%. This advancement paves the way for the next generation of more reliable and resilient conversational agents.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study introduces the Adversarial Prompt Shield (APS), a lightweight model that excels in detection accuracy and demonstrates resilience against adversarial prompts, and proposes novel strategies for autonomously generating adversarial training datasets, designed to fortify the safety classifier's robustness."
            },
            "score": 6
        },
        {
            "id": "afee8cdc51e95b50d7574ed1700a797874bf792c",
            "paperId": "afee8cdc51e95b50d7574ed1700a797874bf792c",
            "title": "Adversarial Fine-Tuning of Language Models: An Iterative Optimisation Approach for the Generation and Detection of Problematic Content",
            "abstract": "In this paper, we tackle the emerging challenge of unintended harmful content generation in Large Language Models (LLMs) with a novel dual-stage optimisation technique using adversarial fine-tuning. Our two-pronged approach employs an adversarial model, fine-tuned to generate potentially harmful prompts, and a judge model, iteratively optimised to discern these prompts. In this adversarial cycle, the two models seek to outperform each other in the prompting phase, generating a dataset of rich examples which are then used for fine-tuning. This iterative application of prompting and fine-tuning allows continuous refinement and improved performance. The performance of our approach is evaluated through classification accuracy on a dataset consisting of problematic prompts not detected by GPT-4, as well as a selection of contentious but unproblematic prompts. We show considerable increase in classification accuracy of the judge model on this challenging dataset as it undergoes the optimisation process. Furthermore, we show that a rudimentary model \\texttt{ada} can achieve 13\\% higher accuracy on the hold-out test set than GPT-4 after only a few rounds of this process, and that this fine-tuning improves performance in parallel tasks such as toxic comment identification.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper shows that a rudimentary model can achieve 13\\% higher accuracy on the hold-out test set than GPT-4 after only a few rounds of this process, and that this fine-tuning improves performance in parallel tasks such as toxic comment identification."
            },
            "score": 6
        },
        {
            "id": "bb86ade4959e9d03e0c5321cde82d759759fd958",
            "paperId": "bb86ade4959e9d03e0c5321cde82d759759fd958",
            "title": "Investigating Ensemble Methods for Model Robustness Improvement of Text Classifiers",
            "abstract": "Large pre-trained language models have shown remarkable performance over the past few years. These models, however, sometimes learn superficial features from the dataset and cannot generalize to the distributions that are dissimilar to the training scenario. There have been several approaches proposed to reduce model's reliance on these bias features which can improve model robustness in the out-of-distribution setting. However, existing methods usually use a fixed low-capacity model to deal with various bias features, which ignore the learnability of those features. In this paper, we analyze a set of existing bias features and demonstrate there is no single model that works best for all the cases. We further show that by choosing an appropriate bias model, we can obtain a better robustness result than baselines with a more sophisticated model design.",
            "year": 2022,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper analyzes a set of existing bias features and demonstrates there is no single model that works best for all the cases and that by choosing an appropriate bias model, one can obtain a better robustness result than baselines with a more sophisticated model design."
            },
            "score": 6
        },
        {
            "id": "e4cdfb01011ad4a10be1858aa7722632b42b9d23",
            "paperId": "e4cdfb01011ad4a10be1858aa7722632b42b9d23",
            "title": "OCHADAI-KYOTO at SemEval-2021 Task 1: Enhancing Model Generalization and Robustness for Lexical Complexity Prediction",
            "abstract": "We propose an ensemble model for predicting the lexical complexity of words and multiword expressions (MWEs). The model receives as input a sentence with a target word or MWE and outputs its complexity score. Given that a key challenge with this task is the limited size of annotated data, our model relies on pretrained contextual representations from different state-of-the-art transformer-based language models (i.e., BERT and RoBERTa), and on a variety of training methods for further enhancing model generalization and robustness: multi-step fine-tuning and multi-task learning, and adversarial training. Additionally, we propose to enrich contextual representations by adding hand-crafted features during training. Our model achieved competitive results and ranked among the top-10 systems in both sub-tasks.",
            "year": 2021,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An ensemble model for predicting the lexical complexity of words and multiword expressions (MWEs) and to enrich contextual representations by adding hand-crafted features during training achieves competitive results and ranked among the top-10 systems in both sub-tasks."
            },
            "score": 6
        },
        {
            "id": "696bc5ba0d023822bbee6b878a71ea2e4a4b0e5a",
            "paperId": "696bc5ba0d023822bbee6b878a71ea2e4a4b0e5a",
            "title": "N-Critics: Self-Refinement of Large Language Models with Ensemble of Critics",
            "abstract": "We propose a self-correction mechanism for Large Language Models (LLMs) to mitigate issues such as toxicity and fact hallucination. This method involves refining model outputs through an ensemble of critics and the model's own feedback. Drawing inspiration from human behavior, we explore whether LLMs can emulate the self-correction process observed in humans who often engage in self-reflection and seek input from others to refine their understanding of complex topics. Our approach is model-agnostic and can be applied across various domains to enhance trustworthiness by addressing fairness, bias, and robustness concerns. We consistently observe performance improvements in LLMs for reducing toxicity and correcting factual errors.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a self-correction mechanism for Large Language Models (LLMs) to mitigate issues such as toxicity and fact hallucination and consistently observe performance improvements in LLMs for reducing toxicity and correcting factual errors."
            },
            "score": 6
        },
        {
            "id": "6b135e922a0c673aeb0b05c5aeecdb6c794791c6",
            "paperId": "6b135e922a0c673aeb0b05c5aeecdb6c794791c6",
            "title": "Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations",
            "abstract": "Large Language Models (LLMs) have shown remarkable success in various tasks, but concerns about their safety and the potential for generating malicious content have emerged. In this paper, we explore the power of In-Context Learning (ICL) in manipulating the alignment ability of LLMs. We find that by providing just few in-context demonstrations without fine-tuning, LLMs can be manipulated to increase or decrease the probability of jailbreaking, i.e. answering malicious prompts. Based on these observations, we propose In-Context Attack (ICA) and In-Context Defense (ICD) methods for jailbreaking and guarding aligned language model purposes. ICA crafts malicious contexts to guide models in generating harmful outputs, while ICD enhances model robustness by demonstrations of rejecting to answer harmful prompts. Our experiments show the effectiveness of ICA and ICD in increasing or reducing the success rate of adversarial jailbreaking attacks. Overall, we shed light on the potential of ICL to influence LLM behavior and provide a new perspective for enhancing the safety and alignment of LLMs.",
            "year": 2023,
            "citationCount": 59,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Light is shed on the potential of In-Context Learning (ICL) to influence LLM behavior and a new perspective for enhancing the safety and alignment of LLMs is provided."
            },
            "score": 6
        },
        {
            "id": "56e7bda25b83228f91962d3465fd587cfe8908e1",
            "paperId": "56e7bda25b83228f91962d3465fd587cfe8908e1",
            "title": "How Far Can We Extract Diverse Perspectives from Large Language Models? Criteria-Based Diversity Prompting!",
            "abstract": "Collecting diverse human opinions is costly and challenging. This leads to a recent trend in collaborative efforts between humans and Large Language Models (LLMs) for generating diverse data, offering potential scalable and efficient solutions. However, the extent of LLMs' capability to generate diverse perspectives on subjective topics remains an unexplored question. In this study, we investigate LLMs' capacity for generating diverse perspectives and rationales on subjective topics, such as social norms and argumentative texts. We formulate a new problem of maximum diversity extraction from LLMs. Motivated by how humans develop their opinions through their values, we propose a criteria-based prompting technique to ground diverse opinions. To see how far we can extract diverse perspectives from LLMs, or called diversity coverage, we employ a step-by-step recall prompting for generating more outputs from the model in an iterative manner. As we apply our methods to various tasks, indeed we find that LLMs can generate diverse opinions according to the degree of task subjectivity",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study investigates LLMs' capacity for generating diverse perspectives and rationales on subjective topics, such as social norms and argumentative texts, and proposes a criteria-based prompting technique to ground diverse opinions."
            },
            "score": 6
        },
        {
            "id": "a58c97f8421ad97da4a08c8d45b8e355ab7de2ad",
            "paperId": "a58c97f8421ad97da4a08c8d45b8e355ab7de2ad",
            "title": "Defense against Adversarial Attacks in NLP via Dirichlet Neighborhood Ensemble",
            "abstract": "Despite neural networks have achieved prominent performance on many natural language processing (NLP) tasks, they are vulnerable to adversarial examples. In this paper, we propose Dirichlet Neighborhood Ensemble (DNE), a randomized smoothing method for training a robust model to defense substitution-based attacks. During training, DNE forms virtual sentences by sampling embedding vectors for each word in an input sentence from a convex hull spanned by the word and its synonyms, and it augments them with the training data. In such a way, the model is robust to adversarial attacks while maintaining the performance on the original clean data. DNE is agnostic to the network architectures and scales to large models for NLP applications. We demonstrate through extensive experimentation that our method consistently outperforms recently proposed defense methods by a significant margin across different network architectures and multiple data sets.",
            "year": 2020,
            "citationCount": 43,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Dirichlet Neighborhood Ensemble is proposed, a randomized smoothing method for training a robust model to defense substitution-based attacks that consistently outperforms recently proposed defense methods by a significant margin across different network architectures and multiple data sets."
            },
            "score": 6
        },
        {
            "id": "0cca27a289b595763d33b0a66ac1b3fc5b3ddc73",
            "paperId": "0cca27a289b595763d33b0a66ac1b3fc5b3ddc73",
            "title": "Defense against Synonym Substitution-based Adversarial Attacks via Dirichlet Neighborhood Ensemble",
            "abstract": "Although deep neural networks have achieved prominent performance on many NLP tasks, they are vulnerable to adversarial examples. We propose Dirichlet Neighborhood Ensemble (DNE), a randomized method for training a robust model to defense synonym substitution-based attacks. During training, DNE forms virtual sentences by sampling embedding vectors for each word in an input sentence from a convex hull spanned by the word and its synonyms, and it augments them with the training data. In such a way, the model is robust to adversarial attacks while maintaining the performance on the original clean data. DNE is agnostic to the network architectures and scales to large models (e.g., BERT) for NLP applications. Through extensive experimentation, we demonstrate that our method consistently outperforms recently proposed defense methods by a significant margin across different network architectures and multiple data sets.",
            "year": 2021,
            "citationCount": 58,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Dirichlet Neighborhood Ensemble is proposed, a randomized method for training a robust model to defense synonym substitution-based attacks that consistently outperforms recently proposed defense methods by a significant margin across different network architectures and multiple data sets."
            },
            "score": 6
        },
        {
            "id": "df2ed9f2d994cc91a710261398ff04b01d1a9f7c",
            "paperId": "df2ed9f2d994cc91a710261398ff04b01d1a9f7c",
            "title": "An LLM can Fool Itself: A Prompt-Based Adversarial Attack",
            "abstract": "The wide-ranging applications of large language models (LLMs), especially in safety-critical domains, necessitate the proper evaluation of the LLM's adversarial robustness. This paper proposes an efficient tool to audit the LLM's adversarial robustness via a prompt-based adversarial attack (PromptAttack). PromptAttack converts adversarial textual attacks into an attack prompt that can cause the victim LLM to output the adversarial sample to fool itself. The attack prompt is composed of three important components: (1) original input (OI) including the original sample and its ground-truth label, (2) attack objective (AO) illustrating a task description of generating a new sample that can fool itself without changing the semantic meaning, and (3) attack guidance (AG) containing the perturbation instructions to guide the LLM on how to complete the task by perturbing the original sample at character, word, and sentence levels, respectively. Besides, we use a fidelity filter to ensure that PromptAttack maintains the original semantic meanings of the adversarial examples. Further, we enhance the attack power of PromptAttack by ensembling adversarial examples at different perturbation levels. Comprehensive empirical results using Llama2 and GPT-3.5 validate that PromptAttack consistently yields a much higher attack success rate compared to AdvGLUE and AdvGLUE++. Interesting findings include that a simple emoji can easily mislead GPT-3.5 to make wrong predictions.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes an efficient tool to audit the LLM's adversarial robustness via a prompt-based adversarial attack (PromptAttack), which converts adversarial textual attacks into an attack prompt that can cause the victim LLM to output the adversarial sample to fool itself."
            },
            "score": 6
        },
        {
            "id": "5a2e45ce35fb26ab70a61b424a49f8e5b4532a8e",
            "paperId": "5a2e45ce35fb26ab70a61b424a49f8e5b4532a8e",
            "title": "WARP: Word-level Adversarial ReProgramming",
            "abstract": "Transfer learning from pretrained language models recently became the dominant approach for solving many NLP tasks. A common approach to transfer learning for multiple tasks that maximize parameter sharing trains one or more task-specific layers on top of the language model. In this paper, we present an alternative approach based on adversarial reprogramming, which extends earlier work on automatic prompt generation. Adversarial reprogramming attempts to learn task-specific word embeddings that, when concatenated to the input text, instruct the language model to solve the specified task. Using up to 25K trainable parameters per task, this approach outperforms all existing methods with up to 25M trainable parameters on the public leaderboard of the GLUE benchmark. Our method, initialized with task-specific human-readable prompts, also works in a few-shot setting, outperforming GPT-3 on two SuperGLUE tasks with just 32 training samples.",
            "year": 2021,
            "citationCount": 257,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents an alternative approach based on adversarial reprogramming, which extends earlier work on automatic prompt generation, and outperforms all existing methods with up to 25M trainable parameters on the public leaderboard of the GLUE benchmark."
            },
            "score": 6
        },
        {
            "id": "142e934dd5d6c53f877c30243d436255e3a0dde7",
            "paperId": "142e934dd5d6c53f877c30243d436255e3a0dde7",
            "title": "Visual Adversarial Examples Jailbreak Aligned Large Language Models",
            "abstract": "Warning: this paper contains data, prompts, and model outputs that are offensive in nature.\n\nRecently, there has been a surge of interest in integrating vision into Large Language Models (LLMs), exemplified by Visual Language Models (VLMs) such as Flamingo and GPT-4. This paper sheds light on the security and safety implications of this trend. First, we underscore that the continuous and high-dimensional nature of the visual input makes it a weak link against adversarial attacks, representing an expanded attack surface of vision-integrated LLMs. Second, we highlight that the versatility of LLMs also presents visual attackers with a wider array of achievable adversarial objectives, extending the implications of security failures beyond mere misclassification. As an illustration, we present a case study in which we exploit visual adversarial examples to circumvent the safety guardrail of aligned LLMs with integrated vision. Intriguingly, we discover that a single visual adversarial example can universally jailbreak an aligned LLM, compelling it to heed a wide range of harmful instructions (that it otherwise would not) and generate harmful content that transcends the narrow scope of a `few-shot' derogatory corpus initially employed to optimize the adversarial example. Our study underscores the escalating adversarial risks associated with the pursuit of multimodality. Our findings also connect the long-studied adversarial vulnerabilities of neural networks to the nascent field of AI alignment. The presented attack suggests a fundamental adversarial challenge for AI alignment, especially in light of the emerging trend toward multimodality in frontier foundation models.",
            "year": 2023,
            "citationCount": 44,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that a single visual adversarial example can universally jailbreak an aligned LLM, compelling it to heed a wide range of harmful instructions and generate harmful content that transcends the narrow scope of a `few-shot' derogatory corpus initially employed to optimize the adversarial example."
            },
            "score": 5
        },
        {
            "id": "6d68b5c1eaf03aba857476a9825acf3e48edd840",
            "paperId": "6d68b5c1eaf03aba857476a9825acf3e48edd840",
            "title": "Hijacking Large Language Models via Adversarial In-Context Learning",
            "abstract": "In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs for specific tasks by utilizing labeled examples as demonstrations in the precondition prompts. Despite its promising performance, ICL suffers from instability with the choice and arrangement of examples. Additionally, crafted adversarial attacks pose a notable threat to the robustness of ICL. However, existing attacks are either easy to detect, rely on external models, or lack specificity towards ICL. To address these issues, this work introduces a novel transferable attack for ICL, aiming to hijack LLMs to generate the targeted response. The proposed LLM hijacking attack leverages a gradient-based prompt search method to learn and append imperceptible adversarial suffixes to the in-context demonstrations. Extensive experimental results on various tasks and datasets demonstrate the effectiveness of our LLM hijacking attack, resulting in a distracted attention towards adversarial tokens, consequently leading to the targeted unwanted outputs.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a novel transferable attack for ICL, aiming to hijack LLMs to generate the targeted response, and leverages a gradient-based prompt search method to learn and append imperceptible adversarial suffixes to the in-context demonstrations."
            },
            "score": 5
        },
        {
            "id": "2f8a1714e3bb01c522fede9c902bd7cf5320a802",
            "paperId": "2f8a1714e3bb01c522fede9c902bd7cf5320a802",
            "title": "Test-Time Self-Adaptive Small Language Models for Question Answering",
            "abstract": "Recent instruction-finetuned large language models (LMs) have achieved notable performances in various tasks, such as question-answering (QA). However, despite their ability to memorize a vast amount of general knowledge across diverse tasks, they might be suboptimal on specific tasks due to their limited capacity to transfer and adapt knowledge to target tasks. Moreover, further finetuning LMs with labeled datasets is often infeasible due to their absence, but it is also questionable if we can transfer smaller LMs having limited knowledge only with unlabeled test data. In this work, we show and investigate the capabilities of smaller self-adaptive LMs, only with unlabeled test data. In particular, we first stochastically generate multiple answers, and then ensemble them while filtering out low-quality samples to mitigate noise from inaccurate labels. Our proposed self-adaption strategy demonstrates significant performance improvements on benchmark QA datasets with higher robustness across diverse prompts, enabling LMs to stay stable. Code is available at: https://github.com/starsuzi/T-SAS.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work shows and investigates the capabilities of smaller self-adaptive LMs, only with unlabeled test data, and proposes a self-adaption strategy that demonstrates significant performance improvements on benchmark QA datasets with higher robustness across diverse prompts, enabling LMs to stay stable."
            },
            "score": 5
        },
        {
            "id": "830f8436baee83834d7418054e6b2b06296ce179",
            "paperId": "830f8436baee83834d7418054e6b2b06296ce179",
            "title": "Improving Robustness and Generality of NLP Models Using Disentangled Representations",
            "abstract": "Supervised neural networks, which first map an input $x$ to a single representation $z$, and then map $z$ to the output label $y$, have achieved remarkable success in a wide range of natural language processing (NLP) tasks. Despite their success, neural models lack for both robustness and generality: small perturbations to inputs can result in absolutely different outputs; the performance of a model trained on one domain drops drastically when tested on another domain. \nIn this paper, we present methods to improve robustness and generality of NLP models from the standpoint of disentangled representation learning. Instead of mapping $x$ to a single representation $z$, the proposed strategy maps $x$ to a set of representations $\\{z_1,z_2,...,z_K\\}$ while forcing them to be disentangled. These representations are then mapped to different logits $l$s, the ensemble of which is used to make the final prediction $y$. We propose different methods to incorporate this idea into currently widely-used models, including adding an $L$2 regularizer on $z$s or adding Total Correlation (TC) under the framework of variational information bottleneck (VIB). We show that models trained with the proposed criteria provide better robustness and domain adaptation ability in a wide range of supervised learning tasks.",
            "year": 2020,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Methods to improve robustness and generality of NLP models from the standpoint of disentangled representation learning are presented and it is shown that models trained with the proposed criteria provide better robusts and domain adaptation ability in a wide range of supervised learning tasks."
            },
            "score": 5
        },
        {
            "id": "7b5b15279e5a52439614f886b79fa33f4b88bfb2",
            "paperId": "7b5b15279e5a52439614f886b79fa33f4b88bfb2",
            "title": "Efficient Test Time Adapter Ensembling for Low-resource Language Varieties",
            "abstract": "Adapters are light-weight modules that allow parameter-efficient fine-tuning of pretrained models. Specialized language and task adapters have recently been proposed to facilitate cross-lingual transfer of multilingual pretrained models (Pfeiffer et al., 2020b). However, this approach requires training a separate language adapter for every language one wishes to support, which can be impractical for languages with limited data. An intuitive solution is to use a related language adapter for the new language variety, but we observe that this solution can lead to sub-optimal performance. In this paper, we aim to improve the robustness of language adapters to uncovered languages without training new adapters. We find that ensembling multiple existing language adapters makes the fine-tuned model significantly more robust to other language varieties not included in these adapters. Building upon this observation, we propose Entropy Minimized Ensemble of Adapters (EMEA), a method that optimizes the ensemble weights of the pretrained language adapters for each test sentence by minimizing the entropy of its predictions. Experiments on three diverse groups of language varieties show that our method leads to significant improvements on both named entity recognition and part-of-speech tagging across all languages.",
            "year": 2021,
            "citationCount": 29,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper finds that ensembling multiple existing language adapters makes the fine-tuned model significantly more robust to other language varieties not included in these adapters, and proposes EMA, a method that optimizes the ensemble weights of the pretrained language adapters for each test sentence by minimizing the entropy of its predictions."
            },
            "score": 5
        },
        {
            "id": "4a4f81543a424ab2494cfb0fb49e0e47329b5cb6",
            "paperId": "4a4f81543a424ab2494cfb0fb49e0e47329b5cb6",
            "title": "Large Language Models are Diverse Role-Players for Summarization Evaluation",
            "abstract": "Text summarization has a wide range of applications in many scenarios. The evaluation of the quality of the generated text is a complex problem. A big challenge to language evaluation is that there is a clear divergence between existing metrics and human evaluation. A document summary's quality can be assessed by human annotators on various criteria, both objective ones like grammar and correctness, and subjective ones like informativeness, succinctness, and appeal. Most of the automatic evaluation methods like BLUE/ROUGE may be not able to adequately capture the above dimensions. In this paper, we propose a new evaluation framework based on LLMs, which provides a comprehensive evaluation framework by comparing generated text and reference text from both objective and subjective aspects. First, we propose to model objective and subjective dimensions of generated text based on roleplayers prompting mechanism. Furthermore, we introduce a context-based prompting mechanism that is able to generate dynamic roleplayer profiles based on input context. Finally, we design a multi-roleplayer prompting technology based on batch prompting and integrate multiple outputs into the final evaluation results. Experimental results on three real datasets for summarization show that our model is highly competitive and has a very high consistency with human annotators.",
            "year": 2023,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new evaluation framework based on LLMs is proposed, which provides a comprehensive evaluation framework by comparing generated text and reference text from both objective and subjective aspects and introduces a context-based prompting mechanism that is able to generate dynamic roleplayer profiles based on input context."
            },
            "score": 5
        },
        {
            "id": "160b67f793ce4d8e0492b3b7918aaefd44350f92",
            "paperId": "160b67f793ce4d8e0492b3b7918aaefd44350f92",
            "title": "Generating Prompt-Based Adversarial Text Examples via Variable Neighborhood Search",
            "abstract": "Natural Language Processing (NLP) models are immensely vulnerable to adversarial text examples. Various word-level attacks have been proposed to modify input texts by carefully-picked substitute words via static or dynamic opti-mization algorithms. However, existing word-level attack methods usually ignore text fluency and semantic consistency for seeking a high attack success ratio, often resulting in unnatural adversarial text examples. In this paper, we propose to generate Prompt-based adversarial texts via Variable Neighborhood Search (P-VNS), which achieves a high attack success ratio while simulta-neously keeping text fluency and semantic similarity. Specifically, the well-designed prompt texts are constructed for input texts and the substitute words are obtained by mask-and-filling procedure under the effect of prompt texts, so the text fluency and semantic similarity can be enhanced. Additionally, the word modification priority is adaptively determined by employing the variable neighborhood search algorithm, yielding an improvement in the attack success ratio. Extensive experiments demonstrate that the P- VNS accomplishes the highest attack success ratio meanwhile preserving text fluency and semantic similarity. Besides, the pro-posed P- VNS also manifests effectiveness in adversarial training and transfer attack.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes to generate Prompt-based adversarial texts via Variable Neighborhood Search (P-VNS), which achieves a high attack success ratio while simulta-neously keeping text fluency and semantic similarity."
            },
            "score": 5
        },
        {
            "id": "9a0f53a0ff25d1a5fb016aaa22185c4d6b5a2ac8",
            "paperId": "9a0f53a0ff25d1a5fb016aaa22185c4d6b5a2ac8",
            "title": "LinkPrompt: Natural and Universal Adversarial Attacks on Prompt-based Language Models",
            "abstract": "Prompt-based learning is a new language model training paradigm that adapts the Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes the performance benchmarks across various natural language processing (NLP) tasks. Instead of using a fixed prompt template to fine-tune the model, some research demonstrates the effectiveness of searching for the prompt via optimization. Such prompt optimization process of prompt-based learning on PLMs also gives insight into generating adversarial prompts to mislead the model, raising concerns about the adversarial vulnerability of this paradigm. Recent studies have shown that universal adversarial triggers (UATs) can be generated to alter not only the predictions of the target PLMs but also the prediction of corresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based learning paradigm. However, UATs found in previous works are often unreadable tokens or characters and can be easily distinguished from natural texts with adaptive defenses. In this work, we consider the naturalness of the UATs and develop $\\textit{LinkPrompt}$, an adversarial attack algorithm to generate UATs by a gradient-based beam search algorithm that not only effectively attacks the target PLMs and PFMs but also maintains the naturalness among the trigger tokens. Extensive results demonstrate the effectiveness of $\\textit{LinkPrompt}$, as well as the transferability of UATs generated by $\\textit{LinkPrompt}$ to open-sourced Large Language Model (LLM) Llama2 and API-accessed LLM GPT-3.5-turbo. The resource is available at $\\href{https://github.com/SavannahXu79/LinkPrompt}{https://github.com/SavannahXu79/LinkPrompt}$.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An adversarial attack algorithm to generate UATs by a gradient-based beam search algorithm that not only effectively attacks the target PLMs and PFMs but also maintains the naturalness among the trigger tokens is developed."
            },
            "score": 5
        },
        {
            "id": "08124e30cdc121eebc4ecd4c09cd3a1abfd3a865",
            "paperId": "08124e30cdc121eebc4ecd4c09cd3a1abfd3a865",
            "title": "Rebuild and Ensemble: Exploring Defense Against Text Adversaries",
            "abstract": "Adversarial attacks can mislead strong neural 001 models; as such, in NLP tasks, substitution-002 based attacks are dif\ufb01cult to defend. Cur-003 rent defense methods usually assume that the 004 substitution candidates are accessible, which 005 cannot be widely applied against substitution-006 agnostic attacks. In this paper, we propose 007 a Rebuild and Ensemble Framework to de-008 fend against adversarial attacks in texts with-009 out knowing the candidates. We propose a re-010 build mechanism to train a robust model and 011 ensemble the rebuilt texts during inference to 012 achieve good adversarial defense results. Ex-013 periments show that our method can improve 014 accuracy under the current strong attack meth-015 ods. 016",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Rebuild and Ensemble Framework to fend against adversarial attacks in texts with-009 out knowing the candidates and can improve accuracy under the current strong attack meth-015 ods."
            },
            "score": 5
        },
        {
            "id": "3bb87d605856411c6f002d480fc29d355c3ba245",
            "paperId": "3bb87d605856411c6f002d480fc29d355c3ba245",
            "title": "An Image Is Worth 1000 Lies: Adversarial Transferability across Prompts on Vision-Language Models",
            "abstract": "Different from traditional task-specific vision models, recent large VLMs can readily adapt to different vision tasks by simply using different textual instructions, i.e., prompts. However, a well-known concern about traditional task-specific vision models is that they can be misled by imperceptible adversarial perturbations. Furthermore, the concern is exacerbated by the phenomenon that the same adversarial perturbations can fool different task-specific models. Given that VLMs rely on prompts to adapt to different tasks, an intriguing question emerges: Can a single adversarial image mislead all predictions of VLMs when a thousand different prompts are given? This question essentially introduces a novel perspective on adversarial transferability: cross-prompt adversarial transferability. In this work, we propose the Cross-Prompt Attack (CroPA). This proposed method updates the visual adversarial perturbation with learnable prompts, which are designed to counteract the misleading effects of the adversarial image. By doing this, CroPA significantly improves the transferability of adversarial examples across prompts. Extensive experiments are conducted to verify the strong cross-prompt adversarial transferability of CroPA with prevalent VLMs including Flamingo, BLIP-2, and InstructBLIP in various different tasks. Our source code is available at \\url{https://github.com/Haochen-Luo/CroPA}.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes the Cross-Prompt Attack (CroPA), a method that updates the visual adversarial perturbation with learnable prompts, which are designed to counteract the misleading effects of the adversarial image."
            },
            "score": 4
        },
        {
            "id": "9d4cd5e3ab44f0d1dfe201c6be70aa7a692ac7f1",
            "paperId": "9d4cd5e3ab44f0d1dfe201c6be70aa7a692ac7f1",
            "title": "GuardT2I: Defending Text-to-Image Models from Adversarial Prompts",
            "abstract": "Recent advancements in Text-to-Image (T2I) models have raised significant safety concerns about their potential misuse for generating inappropriate or Not-Safe-For-Work (NSFW) contents, despite existing countermeasures such as NSFW classifiers or model fine-tuning for inappropriate concept removal. Addressing this challenge, our study unveils GuardT2I, a novel moderation framework that adopts a generative approach to enhance T2I models' robustness against adversarial prompts. Instead of making a binary classification, GuardT2I utilizes a Large Language Model (LLM) to conditionally transform text guidance embeddings within the T2I models into natural language for effective adversarial prompt detection, without compromising the models' inherent performance. Our extensive experiments reveal that GuardT2I outperforms leading commercial solutions like OpenAI-Moderation and Microsoft Azure Moderator by a significant margin across diverse adversarial scenarios.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study unveils GuardT2I, a novel moderation framework that adopts a generative approach to enhance T2I models' robustness against adversarial prompts, and outperforms leading commercial solutions like OpenAI-Moderation and Microsoft Azure Moderator by a significant margin across diverse adversarial scenarios."
            },
            "score": 4
        },
        {
            "id": "ac5b4df0e398ca48388330ac5c795b6fe708793c",
            "paperId": "ac5b4df0e398ca48388330ac5c795b6fe708793c",
            "title": "Misusing Tools in Large Language Models With Visual Adversarial Examples",
            "abstract": "Large Language Models (LLMs) are being enhanced with the ability to use tools and to process multiple modalities. These new capabilities bring new benefits and also new security risks. In this work, we show that an attacker can use visual adversarial examples to cause attacker-desired tool usage. For example, the attacker could cause a victim LLM to delete calendar events, leak private conversations and book hotels. Different from prior work, our attacks can affect the confidentiality and integrity of user resources connected to the LLM while being stealthy and generalizable to multiple input prompts. We construct these attacks using gradient-based adversarial training and characterize performance along multiple dimensions. We find that our adversarial images can manipulate the LLM to invoke tools following real-world syntax almost always (~98%) while maintaining high similarity to clean images (~0.9 SSIM). Furthermore, using human scoring and automated metrics, we find that the attacks do not noticeably affect the conversation (and its semantics) between the user and the LLM.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work shows that an attacker can use visual adversarial examples to cause attacker-desired tool usage to cause a victim LLM to delete calendar events, leak private conversations and book hotels."
            },
            "score": 4
        },
        {
            "id": "b6cf4579b59b51d7df416e096ad86c1e6a48b458",
            "paperId": "b6cf4579b59b51d7df416e096ad86c1e6a48b458",
            "title": "Adversarial Prompt Tuning for Vision-Language Models",
            "abstract": "With the rapid advancement of multimodal learning, pre-trained Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable capacities in bridging the gap between visual and language modalities. However, these models remain vulnerable to adversarial attacks, particularly in the image modality, presenting considerable security risks. This paper introduces Adversarial Prompt Tuning (AdvPT), a novel technique to enhance the adversarial robustness of image encoders in VLMs. AdvPT innovatively leverages learnable text prompts and aligns them with adversarial image embeddings, to address the vulnerabilities inherent in VLMs without the need for extensive parameter training or modification of the model architecture. We demonstrate that AdvPT improves resistance against white-box and black-box adversarial attacks and exhibits a synergistic effect when combined with existing image-processing-based defense techniques, further boosting defensive capabilities. Comprehensive experimental analyses provide insights into adversarial prompt tuning, a novel paradigm devoted to improving resistance to adversarial images through textual input modifications, paving the way for future robust multimodal learning research. These findings open up new possibilities for enhancing the security of VLMs. Our code is available at https://github.com/jiamingzhang94/Adversarial-Prompt-Tuning.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Adversarial Prompt Tuning is introduced, a novel technique to enhance the adversarial robustness of image encoders in VLMs and improves resistance against white-box and black-box adversarial attacks and exhibits a synergistic effect when combined with existing image-processing-based defense techniques, further boosting defensive capabilities."
            },
            "score": 4
        },
        {
            "id": "3ec67a49fdec28c0f8d97147c06c7558e788ce75",
            "paperId": "3ec67a49fdec28c0f8d97147c06c7558e788ce75",
            "title": "Augmenting Sentiment Analysis Prediction in Binary Text Classification through Advanced Natural Language Processing Models and Classifiers",
            "abstract": "Sentiment analysis is a critical component in natural language processing applications, particularly for text classification. By employing state-of-the-art techniques such as ensemble methods, transfer learning and deep learning architectures, our methodology significantly enhances the robustness and precision of sentiment predictions. We systematically investigate the impact of various NLP models, including recurrent neural networks and transformer-based architectures, on sentiment classification tasks. Furthermore, we introduce a novel ensemble method that combines the strengths of multiple classifiers to improve the predictive ability of the system. The results demonstrate the potential of integrating state-of-the-art Natural Language Processing (NLP) models with ensemble classifiers to advance sentiment analysis. This lays the foundation for a more advanced comprehension of textual sentiments in diverse applications.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work systematically investigates the impact of various NLP models, including recurrent neural networks and transformer-based architectures, on sentiment classification tasks and introduces a novel ensemble method that combines the strengths of multiple classifiers to improve the predictive ability of the system."
            },
            "score": 4
        },
        {
            "id": "2e707366c49ea6cc568ec466a5372311aca047d5",
            "paperId": "2e707366c49ea6cc568ec466a5372311aca047d5",
            "title": "EQGTL: An Ensemble Model for Relevant Question Generation using Transfer Learning",
            "abstract": "The manner in which a question is phrased determines the quality and type of response that is obtainable. An answer is likely to change when a question is rephrased. Thus, the need for well-formed questions indeed facilitates quality communication and information exchange in natural language. Moreso, Automated Question Generation Models (AQGM) greatly reduce human errors and any grammatical deficiencies that may arise in the formulation of relevant questions. Existing AQGM methods employ muddled structures that necessitate complex computational assets and increase the ambiguity of the task. In this paper we proposed an automated Ensemble Question Generation Model using the Transfer Learning (EQGTL) framework which utilizes a transfer learning approach to tune Bert Disambiguation Bert-WSD) Model and Text-to-Text Transfer Transformer (T5) model. For our use case, we relied on the robustness of the pre-trained T5 model and finetuned it for generating questions and correct answers alongside multiple incorrect answers described as distractors. When tested with an unseen set of free-form text, our model performs admirably well, generating relevant questions and answers that are grammatically correct and contextually relevant. Tutees can use questions generated by our model to assess their level of comprehension, and instructors as well can use these questions to quickly build key ideas at any time. Our proposed model serves as a valuable solution that can be applied to multiple domains and subject areas.",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposed an automated Ensemble Question Generation Model using the Transfer Learning (EQGTL) framework which utilizes a transfer learning approach to tune Bert Disambiguation Bert-WSD Model and Text-to-Text Transfer Transformer (T5) model."
            },
            "score": 4
        },
        {
            "id": "607d8df2c6c5f103f10a2d631d7364cc952cb489",
            "paperId": "607d8df2c6c5f103f10a2d631d7364cc952cb489",
            "title": "BatchEval: Towards Human-like Text Evaluation",
            "abstract": "Significant progress has been made in automatic text evaluation with the introduction of large language models (LLMs) as evaluators. However, current sample-wise evaluation paradigm suffers from the following issues: (1) Sensitive to prompt design; (2) Poor resistance to noise; (3) Inferior ensemble performance with static reference. Inspired by the fact that humans treat both criterion definition and inter sample comparison as references for evaluation, we propose BatchEval, a paradigm that conducts batch-wise evaluation iteratively to alleviate the above problems. We explore variants under this paradigm and confirm the optimal settings are two stage procedure with heterogeneous batch composition strategy and decimal scoring format. Comprehensive experiments across 3 LLMs on 4 text evaluation tasks demonstrate that BatchEval outperforms state-of-the-art methods by 10.5% on Pearson correlations with only 64% API cost on average. Further analyses have been conducted to verify the robustness, generalization, and working mechanism of BatchEval.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Inspired by the fact that humans treat both criterion definition and inter sample comparison as references for evaluation, BatchEval is proposed, a paradigm that conducts batch-wise evaluation iteratively to alleviate the above problems."
            },
            "score": 4
        },
        {
            "id": "19b7851d6371c9dcbe362806da21330398da293b",
            "paperId": "19b7851d6371c9dcbe362806da21330398da293b",
            "title": "An Ensemble Speaker and Speaking Environment Modeling Approach to Robust Speech Recognition",
            "abstract": "We propose an ensemble speaker and speaking environment modeling (ESSEM) approach to characterizing environments in order to enhance performance robustness of automatic speech recognition systems under adverse conditions. The ESSEM process comprises two phases, the offline and the online. In the offline phase, we prepare an ensemble speaker and speaking environment space formed by a collection of super-vectors. Each super-vector consists of the entire set of means from all the Gaussian mixture components of a set of hidden Markov models that characterizes a particular environment. In the online phase, with the ensemble environment space prepared in the offline phase, we estimate the super-vector for a new testing environment based on a stochastic matching criterion. In this paper, we focus on methods for enhancing the construction and coverage of the environment space in the offline phase. We first demonstrate environment clustering and partitioning algorithms to structure the environment space well; then, we propose a minimum classification error training algorithm to enhance discrimination across environment super-vectors and therefore broaden the coverage of the ensemble environment space. We evaluate the proposed ESSEM framework on the Aurora2 connected digit recognition task. Experimental results verify that ESSEM provides clear improvement over a baseline system without environmental compensation. Moreover, the performance of ESSEM can be further enhanced by using well-structured environment spaces. Finally, we confirm that ESSEM gives the best overall performance with an environment space refined by an integration of all techniques.",
            "year": 2009,
            "citationCount": 49,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a minimum classification error training algorithm to enhance discrimination across environment super-vectors and therefore broaden the coverage of the ensemble environment space and confirms that ESSEM gives the best overall performance with an environment space refined by an integration of all techniques."
            },
            "score": 4
        },
        {
            "id": "45872b94798c3125abfb185b7926689c5e767763",
            "paperId": "45872b94798c3125abfb185b7926689c5e767763",
            "title": "GraphGPT: Graph Instruction Tuning for Large Language Models",
            "abstract": "Graph Neural Networks (GNNs) have advanced graph structure understanding via recursive information exchange and aggregation among graph nodes. To improve model robustness, self-supervised learning (SSL) has emerged as a promising approach for data augmentation. However, existing methods for generating pre-trained graph embeddings often rely on fine-tuning with specific downstream task labels, which limits their usability in scenarios where labeled data is scarce or unavailable. To address this, our research focuses on advancing the generalization capabilities of graph models in challenging zero-shot learning scenarios. Inspired by the success of large language models (LLMs), we aim to develop a graph-oriented LLM that can achieve high generalization across diverse downstream datasets and tasks, even without any information available from the downstream graph data. In this work, we present the GraphGPT framework that aligns LLMs with graph structural knowledge with a graph instruction tuning paradigm. Our framework incorporates a text-graph grounding component to establish a connection between textual information and graph structures. Additionally, we propose a dual-stage instruction tuning paradigm, accompanied by a lightweight graph-text alignment projector. This paradigm explores self-supervised graph structural signals and task-specific graph instructions, to guide LLMs in understanding complex graph structures and improving their adaptability across different downstream tasks. Our framework is evaluated on supervised and zero-shot graph learning tasks, demonstrating superior generalization and outperforming state-of-the-art baselines.",
            "year": 2023,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The GraphGPT framework is presented, which combines a text-graph grounding component to establish a connection between textual information and graph structures, and a dual-stage instruction tuning paradigm, accompanied by a lightweight graph-text alignment projector, that explores self-supervised graph structural signals and task-specific graph instructions."
            },
            "score": 4
        },
        {
            "id": "e7ad08848d5d7c5c47673ffe0da06af443643bda",
            "paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda",
            "title": "Large Language Models are Zero-Shot Reasoners",
            "abstract": "Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding\"Let's think step by step\"before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.",
            "year": 2022,
            "citationCount": 1862,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results demonstrate that the Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics, symbolic reasoning, and other logical reasoning tasks, without any hand-crafted few-shot examples."
            },
            "score": 4
        },
        {
            "id": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
            "paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
            "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "abstract": "Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).",
            "year": 2022,
            "citationCount": 1396,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting that first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths."
            },
            "score": 4
        },
        {
            "id": "ca261cb681b082e90ca6c7a9d325b4265ed1dc28",
            "paperId": "ca261cb681b082e90ca6c7a9d325b4265ed1dc28",
            "title": "MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models",
            "abstract": "Large language models (LLMs) have achieved remarkable performance in natural language understanding and generation tasks. However, they often suffer from limitations such as difficulty in incorporating new knowledge, generating hallucinations, and explaining their reasoning process. To address these challenges, we propose a novel prompting pipeline, named \\method, that leverages knowledge graphs (KGs) to enhance LLMs' inference and transparency. Our method enables LLMs to comprehend KG inputs and infer with a combination of implicit and external knowledge. Moreover, our method elicits the mind map of LLMs, which reveals their reasoning pathways based on the ontology of knowledge. We evaluate our method on diverse question \\&answering tasks, especially in medical domains, and show significant improvements over baselines. We also introduce a new hallucination evaluation benchmark and analyze the effects of different components of our method. Our results demonstrate the effectiveness and robustness of our method in merging knowledge from LLMs and KGs for combined inference. To reproduce our results and extend the framework further, we make our codebase available at https://github.com/wyl-willing/MindMap.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel prompting pipeline is proposed that leverages knowledge graphs (KGs) to enhance LLMs' inference and transparency and elicits the mind map of LLMs, which reveals their reasoning pathways based on the ontology of knowledge."
            },
            "score": 4
        },
        {
            "id": "4ade9cb9f75236679029b6fd60d71c3bf513eedb",
            "paperId": "4ade9cb9f75236679029b6fd60d71c3bf513eedb",
            "title": "CoF-CoT: Enhancing Large Language Models with Coarse-to-Fine Chain-of-Thought Prompting for Multi-domain NLU Tasks",
            "abstract": "While Chain-of-Thought prompting is popular in reasoning tasks, its application to Large Language Models (LLMs) in Natural Language Understanding (NLU) is under-explored. Motivated by multi-step reasoning of LLMs, we propose Coarse-to-Fine Chain-of-Thought (CoF-CoT) approach that breaks down NLU tasks into multiple reasoning steps where LLMs can learn to acquire and leverage essential concepts to solve tasks from different granularities. Moreover, we propose leveraging semantic-based Abstract Meaning Representation (AMR) structured knowledge as an intermediate step to capture the nuances and diverse structures of utterances, and to understand connections between their varying levels of granularity. Our proposed approach is demonstrated effective in assisting the LLMs adapt to the multi-grained NLU tasks under both zero-shot and few-shot multi-domain settings.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes Coarse-to-Fine Chain-of-Thought (CoF-CoT) approach that breaks down NLU tasks into multiple reasoning steps where LLMs can learn to acquire and leverage essential concepts to solve tasks from different granularities."
            },
            "score": 4
        },
        {
            "id": "07cfaf543b2bd991406be1d72a52d784cc9c62fb",
            "paperId": "07cfaf543b2bd991406be1d72a52d784cc9c62fb",
            "title": "Prompt Makes mask Language Models Better Adversarial Attackers",
            "abstract": "Generating high-quality synonymous perturbations is a core challenge for textual adversarial tasks. However, candidates generated from the masked language model often contain many words that are antonyms or irrelevant to the original words, which limit the perturbation space and affect the attack\u2019s effectiveness. We present ProAttacker1 which uses Prompt to make the mask language models better adversarial Attackers. ProAttacker inverts the prompt paradigm by leveraging the prompt with the class label to guide the language model to generate more semantically-consistent perturbations. We present a systematic evaluation to analyze the attack performance on 6 NLP datasets, covering text classification and inference. Our experiments demonstrate that ProAttacker outperforms state-of-the-art attack strategies in both success rate and perturb rate.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ProAttacker1, which uses Prompt to make the mask language models better adversarial Attackers, inverts the prompt paradigm by leveraging the prompt with the class label to guide the language model to generate more semantically-consistent perturbations."
            },
            "score": 4
        },
        {
            "id": "5851bf82b0a9db2de86828f62c3006a6e1b40798",
            "paperId": "5851bf82b0a9db2de86828f62c3006a6e1b40798",
            "title": "Team UTSA-NLP at SemEval 2024 Task 5: Prompt Ensembling for Argument Reasoning in Civil Procedures with GPT4",
            "abstract": "In this paper, we present our system for the SemEval Task 5, The Legal Argument Reasoning Task in Civil Procedure Challenge. Legal argument reasoning is an essential skill that all law students must master. Moreover, it is important to develop natural language processing solutions that can reason about a question given terse domain-specific contextual information. Our system explores a prompt-based solution using GPT4 to reason over legal arguments. We also evaluate an ensemble of prompting strategies, including chain-of-thought reasoning and in-context learning. Overall, our system results in a Macro F1 of .8095 on the validation dataset and .7315 (5th out of 21 teams) on the final test set. Code for this project is available at https://github.com/danschumac1/CivilPromptReasoningGPT4.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This system explores a prompt-based solution using GPT4 to reason over legal arguments, including chain-of-thought reasoning and in-context learning, and evaluates an ensemble of prompting strategies, including chain-of-thought reasoning and in-context learning."
            },
            "score": 4
        },
        {
            "id": "3d7d385d9ee75a286e8da27f7d3cf9f12651c899",
            "paperId": "3d7d385d9ee75a286e8da27f7d3cf9f12651c899",
            "title": "Model ensemble instead of prompt fusion: a sample-specific knowledge transfer method for few-shot prompt tuning",
            "abstract": "Prompt tuning approaches, which learn task-specific soft prompts for a downstream task conditioning on frozen pre-trained models, have attracted growing interest due to its parameter efficiency. With large language models and sufficient training data, prompt tuning performs comparably to full-model tuning. However, with limited training samples in few-shot settings, prompt tuning fails to match the performance of full-model fine-tuning. In this work, we focus on improving the few-shot performance of prompt tuning by transferring knowledge from soft prompts of source tasks. Recognizing the good generalization capabilities of ensemble methods in low-data regime, we first experiment and show that a simple ensemble of model predictions based on different source prompts, outperforms existing multi-prompt knowledge transfer approaches such as source prompt fusion in the few-shot setting. Motivated by this observation, we further investigate model ensembles and propose Sample-specific Ensemble of Source Models (SESoM). SESoM learns to adjust the contribution of each source model for each target sample separately when ensembling source model outputs. Through this way, SESoM inherits the superior generalization of model ensemble approaches and simultaneously captures the sample-specific competence of each source prompt. We conduct experiments across a diverse set of eight NLP tasks using models of different scales (T5-{base, large, XL}) and find that SESoM consistently outperforms the existing models of the same as well as larger parametric scale by a large margin.",
            "year": 2022,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates model ensembles and proposes Sample-specific Ensemble of Source Models (SESoM), a simple ensemble of model predictions based on different source prompts that consistently outperforms the existing models of the same as well as larger parametric scale by a large margin."
            },
            "score": 4
        },
        {
            "id": "4d9fc5972ab0f17f3c8aa27b4d9372f029d4dded",
            "paperId": "4d9fc5972ab0f17f3c8aa27b4d9372f029d4dded",
            "title": "Adversarial Attacks on Large Language Model-Based System and Mitigating Strategies: A Case Study on ChatGPT",
            "abstract": "Machine learning algorithms are at the forefront of the development of advanced information systems. The rapid progress in machine learning technology has enabled cutting-edge large language models (LLMs), represented by GPT-3 and ChatGPT, to perform a wide range of NLP tasks with a stunning performance. However, research on adversarial machine learning highlights the need for these intelligent systems to be more robust. Adversarial machine learning aims to evaluate attack and defense mechanisms to prevent the malicious exploitation of these systems. In the case of ChatGPT, adversarial induction prompt can cause the model to generate toxic texts that could pose serious security risks or propagate false information. To address this challenge, we first analyze the effectiveness of inducing attacks on ChatGPT. Then, two effective mitigating mechanisms are proposed. The first is a training-free prefix prompt mechanism to detect and prevent the generation of toxic texts. The second is a RoBERTa-based mechanism that identifies manipulative or misleading input text via external detection models. The availability of this method is demonstrated through experiments.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A training-free prefix prompt mechanism to detect and prevent the generation of toxic texts and a RoBERTa-based mechanism that identifies manipulative or misleading input text via external detection models are proposed."
            },
            "score": 4
        },
        {
            "id": "20cb40199d03395d63615854863f9eda9c7863e2",
            "paperId": "20cb40199d03395d63615854863f9eda9c7863e2",
            "title": "Visual Prompting for Adversarial Robustness",
            "abstract": "In this work, we leverage visual prompting (VP) to improve adversarial robustness of a fixed, pre-trained model at test time. Compared to conventional adversarial defenses, VP allows us to design universal (i.e., data-agnostic) input prompting templates, which have plug-and-play capabilities at test time to achieve desired model performance without introducing much computation overhead. Although VP has been successfully applied to improving model generalization, it remains elusive whether and how it can be used to defend against adversarial attacks. We investigate this problem and show that the vanilla VP approach is not effective in adversarial defense since a universal input prompt lacks the capacity for robust learning against sample-specific adversarial perturbations. To circumvent it, we propose a new VP method, termed Class-wise Adversarial Visual Prompting (C-AVP), to generate class-wise visual prompts so as to not only leverage the strengths of ensemble prompts but also optimize their interrelations to improve model robustness. Our experiments show that C-AVP outperforms the conventional VP method, with 2.1\u00d7 standard accuracy gain and 2\u00d7 robust accuracy gain. Compared to classical test-time defenses, C-AVP also yields a 42\u00d7 inference time speedup. Code is available at https://github.com/Phoveran/vp-for-adversarial-robustness.",
            "year": 2022,
            "citationCount": 19,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a new VP method, termed Class-wise Adversarial Visual Prompting (C-AVP), to generate class-wise visual prompts so as to not only leverage the strengths of ensemble prompts but also optimize their interrelations to improve model robustness."
            },
            "score": 4
        },
        {
            "id": "f4cd7b7ffb0ab5ccef7cca23eeb436a933f7c776",
            "paperId": "f4cd7b7ffb0ab5ccef7cca23eeb436a933f7c776",
            "title": "Frontier Language Models are not Robust to Adversarial Arithmetic, or \"What do I need to say so you agree 2+2=5?",
            "abstract": "We introduce and study the problem of adversarial arithmetic, which provides a simple yet challenging testbed for language model alignment. This problem is comprised of arithmetic questions posed in natural language, with an arbitrary adversarial string inserted before the question is complete. Even in the simple setting of 1-digit addition problems, it is easy to find adversarial prompts that make all tested models (including PaLM2, GPT4, Claude2) misbehave, and even to steer models to a particular wrong answer. We additionally provide a simple algorithm for finding successful attacks by querying those same models, which we name\"prompt inversion rejection sampling\"(PIRS). We finally show that models can be partially hardened against these attacks via reinforcement learning and via agentic constitutional loops. However, we were not able to make a language model fully robust against adversarial arithmetic attacks.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that models can be partially hardened against these attacks via reinforcement learning and via agentic constitutional loops, however, they were not able to make a language model fully robust against adversarial arithmetic attacks."
            },
            "score": 3
        },
        {
            "id": "1c6015ffff034b9c304477bb31e55ca5a55f3a99",
            "paperId": "1c6015ffff034b9c304477bb31e55ca5a55f3a99",
            "title": "Adversarial Transformer Language Models for Contextual Commonsense Inference",
            "abstract": "Contextualized or discourse aware commonsense inference is the task of generating coherent commonsense assertions (i.e., facts) from a given story, and a particular sentence from that story. Some problems with the task are: lack of controllability for topics of the inferred facts; lack of commonsense knowledge during training; and, possibly, hallucinated or false facts. In this work, we utilize a transformer model for this task and develop techniques to address the aforementioned problems in the task. We control the inference by introducing a new technique we call\"hinting\". Hinting is a kind of language model prompting, that utilizes both hard prompts (specific words) and soft prompts (virtual learnable templates). This serves as a control signal to advise the language model\"what to talk about\". Next, we establish a methodology for performing joint inference with multiple commonsense knowledge bases. Joint inference of commonsense requires care, because it is imprecise and the level of generality is more flexible. You want to be sure that the results\"still make sense\"for the context. To this end, we align the textual version of assertions from three knowledge graphs (ConceptNet, ATOMIC2020, and GLUCOSE) with a story and a target sentence. This combination allows us to train a single model to perform joint inference with multiple knowledge graphs. We show experimental results for the three knowledge graphs on joint inference. Our final contribution is exploring a GAN architecture that generates the contextualized commonsense assertions and scores them as to their plausibility through a discriminator. The result is an integrated system for contextual commonsense inference in stories, that can controllably generate plausible commonsense assertions, and takes advantage of joint inference between multiple commonsense knowledge bases.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The result is an integrated system for contextual commonsense inference in stories, that can controllably generate plausible commonsense assertions, and takes advantage of joint inference between multiple commonsense knowledge bases."
            },
            "score": 3
        },
        {
            "id": "060f3a5742badfaf67ce5faa8cdc58d3cb0231ac",
            "paperId": "060f3a5742badfaf67ce5faa8cdc58d3cb0231ac",
            "title": "PLMTHP: An Ensemble Framework for Tumor Homing Peptide Prediction based on Protein Language Model",
            "abstract": "Tumor homing peptides (THPs) play significant role in recognizing and specifically binding to tumor cells. Traditional experimental methods can accurately identify THPs but often suffer from high measurement costs and long experimental cycles. In-silico methods can rapidly screen THPs and accelerate the THPs identification process. Existing THPs prediction methods primarily rely on constructing peptide sequence features and machine learning approaches. These tools rely on manual feature extraction, and the combination of features is crucial for both model performance and robustness. In this study, we propose a method called PLMTHP based on protein language model and integrate multiple machine learning models. Compared to existing models, PLMTHP improves predictions by incorporating high-dimensional features encoded through a protein language model. Our method achieved impressive performance on an independent test set, with ACC of 0.915, MCC of 0.831, and AUC of 0.964. PLMTHP can be downloaded from the following website: https://github.com/Chenyb939/PLMTHP",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study proposes a method called PLMTHP based on protein language model and integrate multiple machine learning models that improves predictions by incorporating high-dimensional features encoded through a protein language model."
            },
            "score": 3
        },
        {
            "id": "47c1d1bb997446c0907605c0118a2dafab5fa93f",
            "paperId": "47c1d1bb997446c0907605c0118a2dafab5fa93f",
            "title": "Enhancing Urdu Intrinsic Plagiarism Detection Through Stylometry Features and Machine Learning",
            "abstract": "The creation of digital content and the easy accessibility of information have led to a surge in academic and textual plagiarism. Plagiarism detection in multiple languages is essential to maintain the integrity of academic and literary works. In the context of the Urdu language, there is a growing need for effective plagiarism detection methods that are tailored to its unique linguistic characteristics. Existing Urdu plagiarism detection tools often rely on external sources or lack robustness in handling intrinsic forms of plagiarism, where the copied content is slightly modified or paraphrased. This research aims to bridge this gap by developing an intrinsic plagiarism detection system for the Urdu language, using a combination of machine learning, ensemble learning and Multi-Layer Perceptron (MLP). Furthermore, to train and evaluate our plagiarism detection models, we manually curate a corpus comprising a substantial collection of 1807 documents in Urdu. This corpus forms the foundation of our research, enabling us to develop and fine-tune our detection algorithms to effectively identify instances of intrinsic plagiarism in Urdu text. To comprehensively assess the unique stylistic fingerprints of documents, we employ a diverse set of word based stylometry features. This multifaceted approach enhances our ability to pinpoint instances of plagiarism in a robust manner. This research contributes to the ongoing efforts to combat plagiarism and uphold the integrity of written content, particularly in the context of the Urdu language, while also showcasing the effectiveness of different word based stylometry features in addressing this critical issue.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This research contributes to the ongoing efforts to combat plagiarism and uphold the integrity of written content, particularly in the context of the Urdu language, while also showcasing the effectiveness of different word based stylometry features in addressing this critical issue."
            },
            "score": 3
        },
        {
            "id": "6f143e828da9066228d4d133e54648595123e103",
            "paperId": "6f143e828da9066228d4d133e54648595123e103",
            "title": "Generating clickbait spoilers with an ensemble of large language models",
            "abstract": "Clickbait posts are a widespread problem in the webspace. The generation of spoilers, i.e. short texts that neutralize clickbait by providing information that makes it uninteresting, is one of the proposed solutions to the problem. Current state-of-the-art methods are based on passage retrieval or question answering approaches and are limited to generating spoilers only in the form of a phrase or a passage. In this work, we propose an ensemble of fine-tuned large language models for clickbait spoiler generation. Our approach is not limited to phrase or passage spoilers, but is also able to generate multipart spoilers that refer to several non-consecutive parts of text. Experimental evaluation demonstrates that the proposed ensemble model outperforms the baselines in terms of BLEU, METEOR and BERTScore metrics.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes an ensemble of fine-tuned large language models for clickbait spoiler generation that is not limited to phrase or passage spoilers, but is also able to generate multipart spoilers that refer to several non-consecutive parts of text."
            },
            "score": 3
        },
        {
            "id": "c7f4020700800463c5bce4c46328660beaa2fde0",
            "paperId": "c7f4020700800463c5bce4c46328660beaa2fde0",
            "title": "Prompting language models improves performance in imbalanced setting",
            "abstract": "Prompting is a widely adopted technique for fine-tuning large language models. Recent research by Scao and Rush (2021) has demonstrated its effectiveness in improving few-shot learning performance compared to vanilla fine-tuning and also showed that prompting and vanilla fine tuning achieves similar performance in high data regime ( \u223c > 2000 samples). This paper investigates the impact of imbalanced data distribution on prompting. Through rigorous experimentation on diverse datasets and models, our findings reveals that even in scenarios with high data regimes, prompting consistently outperforms vanilla fine-tuning by exhibiting average performance improvement of 2 \u2212 5% .",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Through rigorous experimentation on diverse datasets and models, rigorous experimentation reveals that even in scenarios with high data regimes, prompting consistently outperforms vanilla fine-tuning by exhibiting average performance improvement of 2 \u2212 5%."
            },
            "score": 3
        },
        {
            "id": "99832586d55f540f603637e458a292406a0ed75d",
            "paperId": "99832586d55f540f603637e458a292406a0ed75d",
            "title": "LANGUAGE MODELS",
            "abstract": "While large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. Furthermore, on two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.",
            "year": 2023,
            "citationCount": 600,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces."
            },
            "score": 3
        },
        {
            "id": "e4e744cc96da7987a072571fc3817f040d456566",
            "paperId": "e4e744cc96da7987a072571fc3817f040d456566",
            "title": "Large Language Models Know Your Contextual Search Intent: A Prompting Framework for Conversational Search",
            "abstract": "Precisely understanding users' contextual search intent has been an important challenge for conversational search. As conversational search sessions are much more diverse and long-tailed, existing methods trained on limited data still show unsatisfactory effectiveness and robustness to handle real conversational search scenarios. Recently, large language models (LLMs) have demonstrated amazing capabilities for text generation and conversation understanding. In this work, we present a simple yet effective prompting framework, called LLM4CS, to leverage LLMs as a text-based search intent interpreter to help conversational search. Under this framework, we explore three prompting methods to generate multiple query rewrites and hypothetical responses, and propose to aggregate them into an integrated representation that can robustly represent the user's real contextual search intent. Extensive automatic evaluations and human evaluations on three widely used conversational search benchmarks, including CAsT-19, CAsT-20, and CAsT-21, demonstrate the remarkable performance of our simple LLM4CS framework compared with existing methods and even using human rewrites. Our findings provide important evidence to better understand and leverage LLMs for conversational search.",
            "year": 2023,
            "citationCount": 26,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents a simple yet effective prompting framework, called LLM4CS, to leverage LLMs as a text-based search intent interpreter to help conversational search."
            },
            "score": 3
        },
        {
            "id": "1640bda76a52f8b29e012b4e98b785882fb011c2",
            "paperId": "1640bda76a52f8b29e012b4e98b785882fb011c2",
            "title": "Legal Syllogism Prompting: Teaching Large Language Models for Legal Judgment Prediction",
            "abstract": "Legal syllogism is a form of deductive reasoning commonly used by legal professionals to analyze cases. In this paper, we propose legal syllogism prompting (LoT), a simple prompting method to teach large language models (LLMs) for legal judgment prediction. LoT teaches only that in the legal syllogism the major premise is law, the minor premise is the fact, and the conclusion is judgment. Then the models can produce a syllogism reasoning of the case and give the judgment without any learning, fine-tuning, or examples. On CAIL2018, a Chinese criminal case dataset, we performed zero-shot judgment prediction experiments with GPT-3 models. Our results show that LLMs with LoT achieve better performance than the baseline and chain of thought prompting, the state-of-art prompting method on diverse reasoning tasks. LoT enables the model to concentrate on the key information relevant to the judgment and to correctly understand the legal meaning of acts, as compared to other methods. Our method enables LLMs to predict judgment along with law articles and justification, which significantly enhances the explainability of models.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results show that LLMs with LoT achieve better performance than the baseline and chain of thought prompting, the state-of-art prompting method on diverse reasoning tasks."
            },
            "score": 3
        },
        {
            "id": "e7c97e953849f1a8e5d85ceb4cfcc0a5d54d2365",
            "paperId": "e7c97e953849f1a8e5d85ceb4cfcc0a5d54d2365",
            "title": "Enabling Large Language Models to Generate Text with Citations",
            "abstract": "Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. In this work, our aim is to allow LLMs to generate text with citations, improving their factual correctness and verifiability. Existing work mainly relies on commercial search engines and human evaluation, making it challenging to reproduce and compare different modeling approaches. We propose ALCE, the first benchmark for Automatic LLMs' Citation Evaluation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations. We develop automatic metrics along three dimensions -- fluency, correctness, and citation quality -- and demonstrate their strong correlation with human judgements. Our experiments with state-of-the-art LLMs and novel prompting strategies show that current systems have considerable room for improvement -- For example, on the ELI5 dataset, even the best models lack complete citation support 50% of the time. Our analyses further highlight promising future directions, including developing better retrievers, advancing long-context LLMs, and improving the ability to synthesize information from multiple sources.",
            "year": 2023,
            "citationCount": 102,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes ALCE, the first benchmark for Automatic LLMs' Citation Evaluation, and develops automatic metrics along three dimensions -- fluency, correctness, and citation quality -- and demonstrates their strong correlation with human judgements."
            },
            "score": 3
        },
        {
            "id": "b93594ea2575d1fc76939315c2493cd9245d75a8",
            "paperId": "b93594ea2575d1fc76939315c2493cd9245d75a8",
            "title": "Adversarial robustness in deep neural networks based on variable attributes of the stochastic ensemble model",
            "abstract": "Deep neural networks (DNNs) have been shown to be susceptible to critical vulnerabilities when attacked by adversarial samples. This has prompted the development of attack and defense strategies similar to those used in cyberspace security. The dependence of such strategies on attack and defense mechanisms makes the associated algorithms on both sides appear as closely processes, with the defense method being particularly passive in these processes. Inspired by the dynamic defense approach proposed in cyberspace to address endless arm races, this article defines ensemble quantity, network structure, and smoothing parameters as variable ensemble attributes and proposes a stochastic ensemble strategy based on heterogeneous and redundant sub-models. The proposed method introduces the diversity and randomness characteristic of deep neural networks to alter the fixed correspondence gradient between input and output. The unpredictability and diversity of the gradients make it more difficult for attackers to directly implement white-box attacks, helping to address the extreme transferability and vulnerability of ensemble models under white-box attacks. Experimental comparison of ASR-vs.-distortion curves with different attack scenarios under CIFAR10 preliminarily demonstrates the effectiveness of the proposed method that even the highest-capacity attacker cannot easily outperform the attack success rate associated with the ensemble smoothed model, especially for untargeted attacks.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Inspired by the dynamic defense approach proposed in cyberspace to address endless arm races, this article defines ensemble quantity, network structure, and smoothing parameters as variable ensemble attributes and proposes a stochastic ensemble strategy based on heterogeneous and redundant sub-models."
            },
            "score": 3
        },
        {
            "id": "856e86b182f68e5659f3fd257b79cad5dfa34f4c",
            "paperId": "856e86b182f68e5659f3fd257b79cad5dfa34f4c",
            "title": "Generating Audio Adversarial Examples with Ensemble Substituted Models",
            "abstract": "The rapid development of machine learning technology has prompted the applications of Automatic Speech Recognition(ASR). However, studies have shown that the state-of-the-art ASR technologies are still vulnerable to various attacks, which undermines the stability of ASR destructively. In general, most of the existing attack techniques for the ASR model are based on white box scenarios, where the adversary uses adversarial samples to generate a substituted model corresponding to the target model. On the contrary, there are fewer attack schemes in the black-box scenario. Moreover, no scheme considers the problem of how to construct the architecture of the substituted models. In this paper, we point out that constructing a good substituted model architecture is crucial to the effectiveness of the attack, as it helps to generate a more sophisticated set of adversarial examples. We evaluate the performance of different substituted models by comprehensive experiments, and find that ensemble substituted models can achieve the optimal attack effect. The experiment shows that our approach performs attack over 80% success rate (2% improvement compared to the latest work) meanwhile maintaining the authenticity of the original sample well.",
            "year": 2021,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper points out that constructing a good substituted model architecture is crucial to the effectiveness of the attack, as it helps to generate a more sophisticated set of adversarial examples and finds that ensemble substituted models can achieve the optimal attack effect."
            },
            "score": 3
        },
        {
            "id": "04b1b4ed4c509916b5c6562439967c0477a7b22b",
            "paperId": "04b1b4ed4c509916b5c6562439967c0477a7b22b",
            "title": "EGAN: An Effective Code Readability Classification using Ensemble Generative Adversarial Networks",
            "abstract": "Classification of code script readability (which corresponds to categorizing a source code as either readable or unreadable) has prompted severe concern in academia and industry. To construct accurate classification models, previous studies depended mainly upon handcrafted features. However, the manual feature engineering process is usually labor-intensive and can capture only partial information about the source code, which is likely to limit the model performance. To improve this, we propose the use of GAN's (Generative Adversarial Networks). First, we present a representation approach (with multiple granularities) to encode source codes as the input to GAN's into integer matrices. Then we propose an EGAN based on GAN's for the code readability classification. EGAN consists of three separate GANs with identical architectures that are trained on data preprocessed in different ways. We evaluate our approach against five state-of-the-art code readability models. The findings of the tests demonstrate that RGA can surpass previous approaches. The quality increase varies from 2% to 15%. EGAN offers reasonably improved performance by removing the need for manual interface design, demonstrating the usefulness of GAN's approaches in code readability classification tasks.",
            "year": 2020,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "EGAN offers reasonably improved performance by removing the need for manual interface design, demonstrating the usefulness of GAN's approaches in code readability classification tasks."
            },
            "score": 3
        },
        {
            "id": "658c852761dda1372c50d0698093532518c92387",
            "paperId": "658c852761dda1372c50d0698093532518c92387",
            "title": "Token-Ensemble Text Generation: On Attacking the Automatic AI-Generated Text Detection",
            "abstract": "The robustness of AI-content detection models against cultivated attacks (e.g., paraphrasing or word switching) remains a significant concern. This study proposes a novel token-ensemble generation strategy to challenge the robustness of current AI-content detection approaches. We explore the ensemble attack strategy by completing the prompt with the next token generated from random candidate LLMs. We find the token-ensemble approach significantly drops the performance of AI-content detection models (The code and test sets will be released). Our findings reveal that token-ensemble generation poses a vital challenge to current detection models and underlines the need for advancing detection technologies to counter sophisticated adversarial strategies.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study proposes a novel token-ensemble generation strategy to challenge the robustness of current AI-content detection approaches and underlines the need for advancing detection technologies to counter sophisticated adversarial strategies."
            },
            "score": 3
        },
        {
            "id": "f197bf0fc2f228483f6af3285000d54d8d97f9eb",
            "paperId": "f197bf0fc2f228483f6af3285000d54d8d97f9eb",
            "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models",
            "abstract": "We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",
            "year": 2023,
            "citationCount": 336,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 2
        },
        {
            "id": "af705d648b5b16daa3dcc593bc593f2574d76c07",
            "paperId": "af705d648b5b16daa3dcc593bc593f2574d76c07",
            "title": "Grammar Prompting for Domain-Specific Language Generation with Large Language Models",
            "abstract": "Large language models (LLMs) can learn to perform a wide range of natural language tasks from just a handful of in-context examples. However, for generating strings from highly structured languages (e.g., semantic parsing to complex domain-specific languages), it is challenging for the LLM to generalize from just a few exemplars. We explore $\\textbf{grammar prompting}$ as a simple approach for enabling LLMs to use external knowledge and domain-specific constraints, expressed through a grammar expressed in Backus--Naur Form (BNF), during in-context learning. Grammar prompting augments each demonstration example with a specialized grammar that is minimally sufficient for generating the particular output example, where the specialized grammar is a subset of the full DSL grammar. For inference, the LLM first predicts a BNF grammar given a test input, and then generates the output according to the rules of the grammar. Experiments demonstrate that grammar prompting can enable LLMs to perform competitively on a diverse set of DSL generation tasks, including semantic parsing (SMCalFlow, Overnight, GeoQuery), PDDL planning, and even molecule generation (SMILES).",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experiments demonstrate that grammar prompting can enable LLMs to perform competitively on a diverse set of DSL generation tasks, including semantic parsing, Overnight, GeoQuery, PDDL planning, and even molecule generation (SMILES)."
            },
            "score": 2
        },
        {
            "id": "fb449d114d7d3550fb2fff424be6b5d8434d7924",
            "paperId": "fb449d114d7d3550fb2fff424be6b5d8434d7924",
            "title": "Prompting Large Language Models for Topic Modeling",
            "abstract": "Topic modeling is a widely used technique for revealing underlying thematic structures within textual data. However, existing models have certain limitations, particularly when dealing with short text datasets that lack co-occurring words. Moreover, these models often neglect sentence-level semantics, focusing primarily on token-level semantics. In this paper, we propose PromptTopic, a novel topic modeling approach that harnesses the advanced language understanding of large language models (LLMs) to address these challenges. It involves extracting topics at the sentence level from individual documents, then aggregating and condensing these topics into a predefined quantity, ultimately providing coherent topics for texts of varying lengths. This approach eliminates the need for manual parameter tuning and improves the quality of extracted topics. We benchmark PromptTopic against the state-of-the-art baselines on three vastly diverse datasets, establishing its proficiency in discovering meaningful topics. Furthermore, qualitative analysis showcases PromptTopic\u2019s ability to uncover relevant topics in multiple datasets.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes PromptTopic, a novel topic modeling approach that harnesses the advanced language understanding of large language models (LLMs) to address challenges of short text datasets that lack co-occurring words."
            },
            "score": 2
        },
        {
            "id": "08400a12db21fe4d4f844fec57844e74ac09c9ba",
            "paperId": "08400a12db21fe4d4f844fec57844e74ac09c9ba",
            "title": "Prompting or Fine-tuning? A Comparative Study of Large Language Models for Taxonomy Construction",
            "abstract": "Taxonomies represent hierarchical relations between entities, frequently applied in various software modeling and natural language processing (NLP) activities. They are typically subject to a set of structural constraints restricting their content. However, manual taxonomy construction can be time-consuming, incomplete, and costly to maintain. Recent studies of large language models (LLMs) have demonstrated that appropriate user inputs (called prompting) can effectively guide LLMs, such as GPT-3, in diverse NLP tasks without explicit (re-)training. However, existing approaches for automated taxonomy construction typically involve fine-tuning a language model by adjusting model parameters. In this paper, we present a general framework for taxonomy construction that takes into account structural constraints. We subsequently conduct a systematic comparison between the prompting and fine-tuning approaches performed on a hypernym taxonomy and a novel computer science taxonomy dataset. Our result reveals the following: (1) Even without explicit training on the dataset, the prompting approach outperforms fine-tuning-based approaches. Moreover, the performance gap between prompting and fine-tuning widens when the training dataset is small. However, (2) taxonomies generated by the fine-tuning approach can be easily post-processed to satisfy all the constraints, whereas handling violations of the taxonomies produced by the prompting approach can be challenging. These evaluation findings provide guidance on selecting the appropriate method for taxonomy construction and highlight potential enhancements for both approaches.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A general framework for taxonomy construction that takes into account structural constraints is presented and a systematic comparison between the prompting and fine-tuning approaches performed on a hypernym taxonomy and a novel computer science taxonomy dataset is conducted."
            },
            "score": 2
        },
        {
            "id": "a3c4d01fb505fe3c5be4f6b3da6a65d94f2bccc0",
            "paperId": "a3c4d01fb505fe3c5be4f6b3da6a65d94f2bccc0",
            "title": "LLMR: Real-time Prompting of Interactive Worlds using Large Language Models",
            "abstract": "We present Large Language Model for Mixed Reality (LLMR), a framework for the real-time creation and modification of interactive Mixed Reality experiences using LLMs. LLMR leverages novel strategies to tackle difficult cases where ideal training data is scarce, or where the design goal requires the synthesis of internal dynamics, intuitive analysis, or advanced interactivity. Our framework relies on text interaction and the Unity game engine. By incorporating techniques for scene understanding, task planning, self-debugging, and memory management, LLMR outperforms the standard GPT-4 by 4x in average error rate. We demonstrate LLMR's cross-platform interoperability with several example worlds, and evaluate it on a variety of creation and modification tasks to show that it can produce and edit diverse objects, tools, and scenes. Finally, we conducted a usability study (N=11) with a diverse set that revealed participants had positive experiences with the system and would use it again.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents Large Language Model for Mixed Reality (LLMR), a framework for the real-time creation and modification of interactive Mixed Reality experiences using LLMs that outperforms the standard GPT-4 by 4x in average error rate."
            },
            "score": 2
        },
        {
            "id": "220178fb7f488c6925f4607dc58b599afa6ee967",
            "paperId": "220178fb7f488c6925f4607dc58b599afa6ee967",
            "title": "Evaluating Diverse Large Language Models for Automatic and General Bug Reproduction",
            "abstract": "Bug reproduction is a critical developer activity that is also challenging to automate, as bug reports are often in natural language and thus can be difficult to transform to test cases consistently. As a result, existing techniques mostly focused on crash bugs, which are easier to automatically detect and verify. In this work, we overcome this limitation by using large language models (LLMs), which have been demonstrated to be adept at natural language processing and code generation. By prompting LLMs to generate bug-reproducing tests, and via a post-processing pipeline to automatically identify promising generated tests, our proposed technique LIBRO could successfully reproduce about one-third of all bugs in the widely used Defects4J benchmark. Furthermore, our extensive evaluation on 15 LLMs, including 11 open-source LLMs, suggests that open-source LLMs also demonstrate substantial potential, with the StarCoder LLM achieving 70% of the reproduction performance of the closed-source OpenAI LLM code-davinci-002 on the large Defects4J benchmark, and 90% of performance on a held-out bug dataset likely not part of any LLM's training data. In addition, our experiments on LLMs of different sizes show that bug reproduction using LIBRO improves as LLM size increases, providing information as to which LLMs can be used with the LIBRO pipeline.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The proposed technique LIBRO could successfully reproduce about one-third of all bugs in the widely used Defects4J benchmark, and experiments on LLMs of different sizes show that bug reproduction using LIBRO improves as LLM size increases, providing information as to which LLMs can be used with the LIBRO pipeline."
            },
            "score": 2
        },
        {
            "id": "7a2cb6502309bc329ccc999ce0b45cb789206471",
            "paperId": "7a2cb6502309bc329ccc999ce0b45cb789206471",
            "title": "Is It Really That Simple? Prompting Language Models for Automatic Text Simplification in Italian",
            "abstract": "Recent language models (LMs) that follow instructions have showcased remarkable abilities to tackle diverse natural language processing (NLP) tasks, given appropriate prompts. However, the potential of these models for Automatic Text Simplification (ATS) in Italian remains largely unexplored. In this paper, we pioneer the first in-depth investigation into the capabilities of LMs for performing ATS in Italian. We evaluate six state-of-the-art models on a benchmark Italian ATS dataset of administrative texts, reporting six readability metrics on the generated text. Our findings demonstrate a large variability across models, scales, and prompts. Among the tested models, GPT-3.5 editing capabilities are the most suitable, outperforming, surprisingly, human-written simplification. Furthermore, we shed light ontheenigmaticmultilingualcapabilitiesofinstructionfollowingmodels, openingupnewavenuesforresearchinthisdomain. Italiano. Recenti sviluppi nei cosiddetti language models (LMs) basati sull\u2019apprendimento di istruzioni hanno mostrato notevoli capacit\u00e0 nell\u2019affrontare diverse problemi di elaborazione del linguaggio naturale (NLP). Tuttavia, il potenziale di questi modelli per la semplificazione automatica del testo (Automatic Text Simplification o ATS) in italiano rimane in gran parte inesplorato. Questo articolo riporta un\u2019indagine pionieristica sulle capacit\u00e0 dei Language Models (LMs) nell\u2019eseguire ATS in italiano. Abbiamo valutato sei modelli utilizzando",
            "year": null,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper pioneer the first in-depth investigation into the capabilities of LMs for performing ATS in Italian, evaluating six state-of-the-art models on a benchmark Italian ATS dataset of administrative texts, and shedding light on theenigmatic multilingualcapabilities ofruction following models, opening up newavenues for research in thisdomain."
            },
            "score": 2
        },
        {
            "id": "6ca16c1c2c60ceda87242c8f8e522d12cc4a13bc",
            "paperId": "6ca16c1c2c60ceda87242c8f8e522d12cc4a13bc",
            "title": "Eureka: Human-Level Reward Design via Coding Large Language Models",
            "abstract": "Large Language Models (LLMs) have excelled as high-level semantic planners for sequential decision-making tasks. However, harnessing them to learn complex low-level manipulation tasks, such as dexterous pen spinning, remains an open problem. We bridge this fundamental gap and present Eureka, a human-level reward design algorithm powered by LLMs. Eureka exploits the remarkable zero-shot generation, code-writing, and in-context improvement capabilities of state-of-the-art LLMs, such as GPT-4, to perform evolutionary optimization over reward code. The resulting rewards can then be used to acquire complex skills via reinforcement learning. Without any task-specific prompting or pre-defined reward templates, Eureka generates reward functions that outperform expert human-engineered rewards. In a diverse suite of 29 open-source RL environments that include 10 distinct robot morphologies, Eureka outperforms human experts on 83% of the tasks, leading to an average normalized improvement of 52%. The generality of Eureka also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF), readily incorporating human inputs to improve the quality and the safety of the generated rewards without model updating. Finally, using Eureka rewards in a curriculum learning setting, we demonstrate for the first time, a simulated Shadow Hand capable of performing pen spinning tricks, adeptly manipulating a pen in circles at rapid speed.",
            "year": 2023,
            "citationCount": 69,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Eureka is presented, a human-level reward design algorithm powered by LLMs that exploits the remarkable zero-shot generation, code-writing, and in-context improvement capabilities of state-of-the-art LLMs to perform evolutionary optimization over reward code."
            },
            "score": 2
        },
        {
            "id": "f11d7ff00ac0dacc4248559dc3e14479123d9cbb",
            "paperId": "f11d7ff00ac0dacc4248559dc3e14479123d9cbb",
            "title": "Enhancing Early Detection of Cognitive Decline in the Elderly through Ensemble of NLP Techniques: A Comparative Study Utilizing Large Language Models in Clinical Notes",
            "abstract": "Background: Early detection of cognitive decline in elderly individuals facilitates clinical trial enrollment and timely medical interventions. This study aims to apply, evaluate, and compare advanced natural language processing techniques for identifying signs of cognitive decline in clinical notes. Methods: This study, conducted at Mass General Brigham (MGB), Boston, MA, included clinical notes from the 4 years prior to initial mild cognitive impairment (MCI) diagnosis in 2019 for patients [\u2265] 50 years. Note sections regarding cognitive decline were labeled manually. A random sample of 4,949 note sections filtered with cognitive functions-related keywords were used for traditional AI model development, and 200 random subset were used for LLM and prompt development; another random sample of 1996 note sections without keyword filtering were used for testing. Prompt templates for large language models (LLM), Llama 2 on Amazon Web Service and GPT-4 on Microsoft Azure, were developed with multiple prompting approaches to select the optimal LLM-based method. Baseline comparisons were made with XGBoost and a hierarchical attention-based deep neural network model. An ensemble of the three models was then constructed using majority vote. Results: GPT-4 demonstrated superior accuracy and efficiency to Llama 2. The ensemble model outperformed individual models, achieving a precision of 90.3%, recall of 94.2%, and F1-score of 92.2%. Notably, the ensemble model demonstrated a marked improvement in precision (from a 70%-79% range to above 90%) compared to the best performing single model. Error analysis revealed 63 samples were wrongly predicted by at least one model; however, only 2 cases (3.2%) were mutual errors across all models, indicating diverse error profiles among them. Conclusion: Our findings indicate that LLMs and traditional models exhibit diverse error profiles. The ensemble of LLMs and locally trained machine learning models on EHR data was found to be complementary, enhancing performance and improving diagnostic accuracy.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The ensemble of LLMs and locally trained machine learning models on EHR data was found to be complementary, enhancing performance and improving diagnostic accuracy, indicating that LLMs and traditional models exhibit diverse error profiles."
            },
            "score": 2
        }
    ],
    "novelty": "yes"
}