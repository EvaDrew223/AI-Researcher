{
    "topic_description": "novel prompting methods to improve large language models' robustness against adversarial attacks and improve their security and privacy",
    "idea_name": "Prompt-based Adversarial Training with Privacy Preservation",
    "raw_idea": {
        "Problem": "Adversarial training is an effective method for improving language models' robustness, but it often requires sharing large amounts of sensitive user data with the model provider, which can raise privacy concerns. Existing methods for privacy-preserving adversarial training, such as differential privacy, can significantly degrade model performance.",
        "Existing Methods": "Current approaches for privacy-preserving adversarial training include using differential privacy to add noise to the training data or model gradients, federated learning to train models on decentralized data, and using synthetic data generated by a separate model.",
        "Motivation": "We propose a novel approach that combines prompt-based adversarial training with privacy-preserving techniques to improve language models' robustness without compromising user privacy. By using prompts to guide the generation of adversarial examples and using secure multi-party computation to aggregate model updates, we can achieve a better trade-off between privacy and performance.",
        "Proposed Method": "Our method, called Prompt-based Adversarial Training with Privacy Preservation (PAPP), works by first prompting the language model to generate a diverse set of adversarial examples based on a small set of seed examples provided by the users. The prompts are designed to guide the model to generate examples that are similar to the seeds but with variations in wording, style, and content. The generated examples are then used to fine-tune the model using adversarial training, where the model is trained to predict the correct output for each example while being robust to perturbations. To preserve privacy, the model updates are computed using secure multi-party computation, where the users and the model provider collaboratively compute the gradients without revealing their individual data. The updated model is then sent back to the users for further fine-tuning on their local data.",
        "Experiment Plan": "We will evaluate PAPP on various language modeling tasks that involve sensitive user data, such as email composition and personal assistant. We will measure the models' robustness against adversarial attacks and their performance on the original tasks, using metrics such as adversarial success rate and perplexity. We will compare PAPP with non-private adversarial training and differential privacy baselines, and conduct user studies to evaluate the perceived privacy and utility of the models. We will also analyze the trade-off between privacy and performance by varying the amount of noise added to the model updates and the number of adversarial examples generated."
    },
    "full_experiment_plan": {
        "Title": "Prompt-based Adversarial Training with Privacy Preservation for Robust and Secure Language Models",
        "Problem Statement": "Adversarial training is an effective method for improving language models' robustness, but it often requires sharing large amounts of sensitive user data with the model provider, which can raise privacy concerns. Existing methods for privacy-preserving adversarial training, such as differential privacy, can significantly degrade model performance.",
        "Motivation": "Recent work has attempted to address the privacy concerns in adversarial training by using techniques such as differential privacy, federated learning, and synthetic data generation. However, these methods often lead to a significant drop in model performance. We propose a novel approach that combines prompt-based adversarial training with privacy-preserving techniques to improve language models' robustness without compromising user privacy. By using prompts to guide the generation of adversarial examples and using secure multi-party computation to aggregate model updates, we aim to achieve a better trade-off between privacy and performance.",
        "Proposed Method": "Our method, called Prompt-based Adversarial Training with Privacy Preservation (PAPP), works as follows:\n1. Prompt the language model to generate a diverse set of adversarial examples based on a small set of seed examples provided by the users. The prompts are designed to guide the model to generate examples that are similar to the seeds but with variations in wording, style, and content.\n2. Use the generated examples to fine-tune the model using adversarial training, where the model is trained to predict the correct output for each example while being robust to perturbations.\n3. To preserve privacy, compute the model updates using secure multi-party computation, where the users and the model provider collaboratively compute the gradients without revealing their individual data.\n4. Send the updated model back to the users for further fine-tuning on their local data.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Dataset Selection": "We will evaluate PAPP on various language modeling tasks that involve sensitive user data, such as email composition and personal assistant. We will use datasets such as Enron Email Dataset, which contains real-world email messages, and PersonaChat, which contains conversations between crowdworkers who are playing the role of a given persona.",
            "Step 2: Baseline Methods": "We will compare PAPP with the following baseline methods:\n1. Non-private adversarial training: Fine-tune the model using adversarial examples generated from the full dataset, without any privacy protection.\n2. Differential privacy: Add noise to the model gradients during adversarial training to protect user privacy, using techniques such as DP-SGD.\n3. Federated learning: Train the model on decentralized data using federated averaging, where each user computes their local model updates and sends them to the server for aggregation.",
            "Step 3: Prompt Design": "We will design a set of prompts to guide the generation of adversarial examples. The prompts will be based on the following templates:\n1. 'Paraphrase the following sentence to change its wording but keep its meaning: [seed example]'\n2. 'Rewrite the following sentence in a more [formal/informal/polite/rude] tone: [seed example]'\n3. 'Generate a sentence that is similar to the following one but with a different topic: [seed example]'",
            "Step 4: Adversarial Training": "We will fine-tune the language model using adversarial training, where the model is trained to predict the correct output for each adversarial example while being robust to perturbations. We will use techniques such as PGD and FGSM to generate adversarial perturbations during training.",
            "Step 5: Privacy-Preserving Aggregation": "We will use secure multi-party computation to aggregate the model updates from different users without revealing their individual data. Specifically, we will use the SecureML framework to implement a secure aggregation protocol based on secret sharing.",
            "Step 6: Evaluation Metrics": "We will evaluate the models' performance using the following metrics:\n1. Perplexity: Measure the language modeling performance on the original task.\n2. Adversarial success rate: Measure the percentage of adversarial examples that can fool the model.\n3. Privacy loss: Measure the amount of information leaked about individual users' data using metrics such as differential privacy epsilon.",
            "Step 7: Hyperparameter Tuning": "We will tune the following hyperparameters for each method:\n1. Number of adversarial examples generated per seed\n2. Perturbation strength for adversarial training\n3. Noise level for differential privacy\n4. Number of local training epochs for federated learning",
            "Step 8: Ablation Study": "We will conduct an ablation study to evaluate the contribution of each component in PAPP. Specifically, we will compare the performance of the following variants:\n1. PAPP without prompt-based example generation\n2. PAPP without adversarial training\n3. PAPP without privacy-preserving aggregation",
            "Step 9: User Study": "We will conduct a user study to evaluate the perceived privacy and utility of the models. We will recruit users to use the models for email composition and personal assistant tasks, and ask them to rate their satisfaction with the models' performance and their concerns about privacy."
        },
        "Test Case Examples": {
            "Example 1": {
                "Seed Example": "Hey, can you remind me to buy milk when I get home?",
                "Baseline Adversarial Example": "Hey, can you remind me to buy milk when I get home? I have a lactose intolerance though, so maybe I should get almond milk instead.",
                "Baseline Output": "Sure, I'll remind you to buy milk when you get home. And thanks for letting me know about your lactose intolerance - I'll make a note to suggest almond milk as an alternative.",
                "PAPP Prompt": "Paraphrase the following sentence to change its wording but keep its meaning: Hey, can you remind me to buy milk when I get home?",
                "PAPP Adversarial Example": "Hello, would you be able to give me a reminder to purchase some milk once I arrive back at my house?",
                "PAPP Output": "Of course, I'll set a reminder for you to buy milk when you return home."
            },
            "Example 2": {
                "Seed Example": "What's the weather forecast for tomorrow?",
                "Baseline Adversarial Example": "What's the weather forecast for tomorrow? I have a big outdoor event planned, so I really hope it doesn't rain!",
                "Baseline Output": "The weather forecast for tomorrow is mostly sunny with a high of 75\u00b0F (24\u00b0C). It looks like it will be great weather for your outdoor event!",
                "PAPP Prompt": "Generate a sentence that is similar to the following one but with a different topic: What's the weather forecast for tomorrow?",
                "PAPP Adversarial Example": "What's the traffic prediction for the weekend?",
                "PAPP Output": "According to the latest traffic report, the roads are expected to be busy this weekend due to the holiday. I'd recommend planning your travel ahead of time and leaving early if possible to avoid delays."
            },
            "Explanation": "In the first example, the baseline method generates an adversarial example that introduces irrelevant information about lactose intolerance, which leads the model to give an overly specific response. In contrast, PAPP generates an adversarial example that paraphrases the original sentence without adding irrelevant details, allowing the model to give a concise and relevant response.\n\nIn the second example, the baseline method again generates an adversarial example with irrelevant information about an outdoor event, which biases the model's response. PAPP, on the other hand, generates an adversarial example on a different but related topic of traffic prediction, which tests the model's ability to handle a broader range of queries while staying on topic."
        },
        "Fallback Plan": "If the proposed PAPP method does not outperform the baselines, we will conduct additional analysis to identify the bottlenecks and potential improvements:\n\n1. Analyze the quality and diversity of the generated adversarial examples to see if the prompts need to be improved or if the model is struggling to generate meaningful perturbations.\n\n2. Evaluate the trade-off between privacy and performance by varying the noise level in differential privacy and the number of local training epochs in federated learning. This can help identify the optimal balance between the two objectives.\n\n3. Investigate alternative privacy-preserving techniques such as homomorphic encryption and trusted execution environments, which may provide better privacy guarantees or more efficient computation.\n\n4. Consider using a larger and more diverse dataset for adversarial training, or pre-training the model on a related task to improve its robustness and generalization ability.\n\nIf none of these approaches lead to significant improvements, we will focus on analyzing the limitations and challenges of privacy-preserving adversarial training for language models. This can include:\n\n1. Studying the impact of different types of adversarial examples and perturbations on model performance and privacy leakage.\n\n2. Evaluating the effectiveness of different privacy metrics and attack models for measuring and protecting user privacy in language modeling tasks.\n\n3. Conducting user studies to understand the perceived trade-off between privacy and utility in real-world applications, and identifying potential ways to mitigate the tension between the two objectives.\n\nThe insights and lessons learned from this analysis can inform the design of future privacy-preserving techniques for language models and guide the development of more robust and secure NLP systems."
    },
    "novelty_queries": [
        "KeywordQuery(\"prompt-based adversarial training language models\")",
        "KeywordQuery(\"privacy-preserving adversarial training nlp\")",
        "KeywordQuery(\"secure aggregation federated learning language models\")",
        "KeywordQuery(\"differential privacy adversarial examples nlp\")",
        "KeywordQuery(\"Prompt-based Adversarial Training with Privacy Preservation NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "2c72ab10e7a5f2fd32e6f85b20c77bf64e6e220d",
            "paperId": "2c72ab10e7a5f2fd32e6f85b20c77bf64e6e220d",
            "title": "A prompt-based approach to adversarial example generation and robustness enhancement",
            "abstract": null,
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel robust training approach based on prompt paradigm which incorporates prompt texts as the alternatives to adversarial examples and enhances robustness under a lightweight minimax-style optimization framework is proposed."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve language models' robustness through adversarial training while preserving user privacy using prompt-based example generation and secure aggregation. The paper proposes a prompt-based approach to generate adversarial examples and enhance robustness under a lightweight optimization framework.\n\nWhile both the proposal and the paper use prompts for adversarial example generation, the proposal focuses on privacy preservation during adversarial training, which is not addressed in the paper. The paper's approach is more lightweight and does not involve privacy-preserving techniques like secure aggregation.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "de2fd685f45ee916b9142bcb983d306b7da643a4",
            "paperId": "de2fd685f45ee916b9142bcb983d306b7da643a4",
            "title": "A Prompting-based Approach for Adversarial Example Generation and Robustness Enhancement",
            "abstract": "Recent years have seen the wide application of NLP models in crucial areas such as finance, medical treatment, and news media, raising concerns of the model robustness and vulnerabilities. In this paper, we propose a novel prompt-based adversarial attack to compromise NLP models and robustness enhancement technique. We first construct malicious prompts for each instance and generate adversarial examples via mask-and-filling under the effect of a malicious purpose. Our attack technique targets the inherent vulnerabilities of NLP models, allowing us to generate samples even without interacting with the victim NLP model, as long as it is based on pre-trained language models (PLMs). Furthermore, we design a prompt-based adversarial training method to improve the robustness of PLMs. As our training method does not actually generate adversarial samples, it can be applied to large-scale training sets efficiently. The experimental results show that our attack method can achieve a high attack success rate with more diverse, fluent and natural adversarial examples. In addition, our robustness enhancement method can significantly improve the robustness of models to resist adversarial attacks. Our work indicates that prompting paradigm has great potential in probing some fundamental flaws of PLMs and fine-tuning them for downstream tasks.",
            "year": 2022,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel prompt-based adversarial attack to compromise NLP models and robustness enhancement technique that can significantly improve the robustness of models to resist adversarial attacks and indicates that prompting paradigm has great potential in probing some fundamental flaws of PLMs and fine-tuning them for downstream tasks."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving language models' robustness without compromising user privacy, and the approach is prompt-based adversarial training combined with privacy-preserving techniques.\n\nThe research problem in the paper is compromising NLP models and enhancing their robustness, and the approach is prompt-based adversarial attack and prompt-based adversarial training.\n\nWhile both works involve adversarial training and prompting, the proposal focuses on privacy preservation, which is not addressed in the paper. The paper aims to attack NLP models, while the proposal does not mention this aspect.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "fa52f9f38259169cf4eff707eaa4552854621cc6",
            "paperId": "fa52f9f38259169cf4eff707eaa4552854621cc6",
            "title": "DeepObfuscator: Adversarial Training Framework for Privacy-Preserving Image Classification",
            "abstract": "Deep learning has been widely utilized in many computer vision applications and achieved remarkable commercial success. However, running deep learning models on mobile devices is generally challenging due to limitation of the available computing resources. It is common to let the users send their service requests to cloud servers that run the large-scale deep learning models to process. Sending the data associated with the service requests to the cloud, however, impose risks on the user data privacy. Some prior arts proposed sending the features extracted from raw data (e.g., images) to the cloud. Unfortunately, these extracted features can still be exploited by attackers to recover raw images and to infer embedded private attributes (e.g., age, gender, etc.). In this paper, we propose an adversarial training framework DeepObfuscator that can prevent extracted features from being utilized to reconstruct raw images and infer private attributes, while retaining the useful information for the intended cloud service (i.e., image classification). DeepObfuscator includes a learnable encoder, namely, obfuscator that is designed to hide privacy-related sensitive information from the features by performingour proposed adversarial training algorithm. Our experiments on CelebAdataset show that the quality of the reconstructed images fromthe obfuscated features of the raw image is dramatically decreased from 0.9458 to 0.3175 in terms of multi-scale structural similarity (MS-SSIM). The person in the reconstructed image, hence, becomes hardly to be re-identified. The classification accuracy of the inferred private attributes that can be achieved by the attacker drops down to a random-guessing level, e.g., the accuracy of gender is reduced from 97.36% to 58.85%. As a comparison, the accuracy of the intended classification tasks performed via the cloud service drops by only 2%",
            "year": 2019,
            "citationCount": 39,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An adversarial training framework DeepObfuscator is proposed that can prevent extracted features from being utilized to reconstruct raw images and infer private attributes, while retaining the useful information for the intended cloud service (i.e., image classification)."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving language models' robustness to adversarial attacks while preserving user privacy. The proposed approach is prompt-based adversarial training combined with privacy-preserving techniques like secure multi-party computation.\n\nThe research problem in the paper is preventing features extracted from images from being used to reconstruct the original images or infer private attributes, while still allowing the features to be used for the intended classification task. The proposed approach is an adversarial training framework called DeepObfuscator that includes a learnable encoder to hide sensitive information.\n\nWhile both works aim to preserve privacy in machine learning models, the proposal focuses on language models and robustness to adversarial attacks, while the paper focuses on image classification and preventing reconstruction and attribute inference. The techniques used (prompt-based training, secure multi-party computation) are also different from the paper's approach (adversarial training of a feature encoder).\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "5effe43159841d2c16feb77528bdfd3e915dcdb4",
            "paperId": "5effe43159841d2c16feb77528bdfd3e915dcdb4",
            "title": "SRATTA : Sample Re-ATTribution Attack of Secure Aggregation in Federated Learning",
            "abstract": "We consider a cross-silo federated learning (FL) setting where a machine learning model with a fully connected first layer is trained between different clients and a central server using FedAvg, and where the aggregation step can be performed with secure aggregation (SA). We present SRATTA an attack relying only on aggregated models which, under realistic assumptions, (i) recovers data samples from the different clients, and (ii) groups data samples coming from the same client together. While sample recovery has already been explored in an FL setting, the ability to group samples per client, despite the use of SA, is novel. This poses a significant unforeseen security threat to FL and effectively breaks SA. We show that SRATTA is both theoretically grounded and can be used in practice on realistic models and datasets. We also propose counter-measures, and claim that clients should play an active role to guarantee their privacy during training.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents SRATTA an attack relying only on aggregated models which recovers data samples from the different clients, and groups data samples coming from the same client together, and proposes counter-measures."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve language models' robustness through adversarial training while preserving user privacy using prompt-based example generation and secure multi-party computation. The paper, on the other hand, presents an attack that recovers and groups data samples from different clients in a federated learning setting, despite the use of secure aggregation.\n\nProject proposal: Improving language models' robustness through privacy-preserving adversarial training.\nPaper: Attacking secure aggregation in federated learning to recover and group data samples from different clients.\n\nThe project proposal and the paper address different problems and propose different approaches. While the project focuses on improving model robustness and preserving privacy, the paper aims to break privacy in federated learning by recovering and grouping data samples.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "aa3e049d4419364db1cd31c77ab7312f996eebdd",
            "paperId": "aa3e049d4419364db1cd31c77ab7312f996eebdd",
            "title": "Secure Aggregation is Insecure: Category Inference Attack on Federated Learning",
            "abstract": "Federated learning allows a large number of resource-constrained clients to train a globally-shared model together without sharing local data. These clients usually have only a few classes (categories) of data for training, where the data distribution is non-iid (not independent identically distributed). In this article, we put forward the concept of <italic>category privacy</italic> for the first time to indicate <italic>which classes of data a client has</italic>, which is an important but ignored privacy goal in the federated learning with non-iid data. Although secure aggregation protocols are designed for federated learning to protect the input privacy of clients, we perform the first systematic study on <italic>category inference attack</italic> and demonstrate that these protocols cannot fully protect category privacy. We design a differential selection strategy and two de-noising approaches to achieve the attack goal successfully. In our evaluation, we apply the attack to non-iid federated learning settings with various datasets. On MNIST, CIFAR-10, AG_news, and DBPedia dataset, our attack achieves <inline-formula><tex-math notation=\"LaTeX\">$>90\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>></mml:mo><mml:mn>90</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"liu-ieq1-3128679.gif\"/></alternatives></inline-formula> accuracy measured in F1-score in most cases. We further consider a possible detection method and propose two strategies to make the attack more inconspicuous.",
            "year": 2023,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The concept of category privacy is put forward for the first time to indicate which classes of data a client has, which is an important but ignored privacy goal in the federated learning with non-iid data."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving language models' robustness and preserving user privacy in adversarial training, and the proposed approach is prompt-based adversarial training with secure multi-party computation.\n\nThe research problem in the paper is protecting category privacy in federated learning with non-iid data, and the proposed approach is studying category inference attack on secure aggregation protocols.\n\nThe two works have different research problems and approaches. The proposal focuses on adversarial training for language models, while the paper studies privacy leakage in federated learning. The proposal uses prompt-based training and secure multi-party computation, while the paper performs inference attacks on secure aggregation protocols.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "039a9bacb2eaaff22970614f0c7b3d0a8beb689f",
            "paperId": "039a9bacb2eaaff22970614f0c7b3d0a8beb689f",
            "title": "Secure Aggregation in Federated Learning is not Private: Leaking User Data at Large Scale through Model Modification",
            "abstract": "Security and privacy are important concerns in machine learning. End user devices often contain a wealth of data and this information is sensitive and should not be shared with servers or enterprises. As a result, federated learning was introduced to enable machine learning over large decentralized datasets while promising privacy by eliminating the need for data sharing. However, prior work has shown that shared gradients often contain private information and attackers can gain knowledge either through malicious modi\ufb01cation of the architecture and parameters or by using optimization to approximate user data from the shared gradients. Despite this, most attacks have so far been limited in scale of number of clients, especially failing when client gradients are aggregated together using secure model aggregation. The attacks that still function are strongly limited in the number of clients attacked, amount of training samples they leak, or number of iterations they take to be trained. In this work, we introduce M ANDRAKE , an attack that overcomes previous limitations to directly leak large amounts of client data even under secure aggregation across large numbers of clients. Furthermore, we break the anonymity of aggregation as the leaked data is iden-ti\ufb01able and directly tied back to the clients they come from. We show that by sending clients customized convolutional parameters, the weight gradients of data points between clients will remain separate through aggregation. With an aggregation across many clients, prior work could only leak less than 1% of images. With the same number of non-zero parameters, and using only a single training iteration, M ANDRAKE leaks 70-80% of data samples.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces M ANDRAKE, an attack that overcomes previous limitations to directly leak large amounts of client data even under secure aggregation across large numbers of clients and breaks the anonymity of aggregation as the leaked data is iden-ti\ufb01able and directly tied back to the clients they come from."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving language models' robustness through adversarial training while preserving user privacy. The proposed approach is prompt-based adversarial training combined with secure multi-party computation for aggregating model updates.\n\nThe research problem in the paper is demonstrating that secure aggregation in federated learning is not sufficient for protecting user privacy. The approach is an attack called MANDRAKE that leaks client data by sending customized convolutional parameters.\n\nThe proposal focuses on improving model robustness and preserving privacy, while the paper focuses on breaking privacy in federated learning. The approaches are also different: prompt-based adversarial training vs. data leakage attack.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "4cc05c17cbd14e3c8f855fcc247d5b098698cd0a",
            "paperId": "4cc05c17cbd14e3c8f855fcc247d5b098698cd0a",
            "title": "Scalable Differential Privacy with Certified Robustness in Adversarial Learning",
            "abstract": "In this paper, we aim to develop a scalable algorithm to preserve differential privacy (DP) in adversarial learning for deep neural networks (DNNs), with certified robustness to adversarial examples. By leveraging the sequential composition theory in DP, we randomize both input and latent spaces to strengthen our certified robustness bounds. To address the trade-off among model utility, privacy loss, and robustness, we design an original adversarial objective function, based on the post-processing property in DP, to tighten the sensitivity of our model. A new stochastic batch training is proposed to apply our mechanism on large DNNs and datasets, by bypassing the vanilla iterative batch-by-batch training in DP DNNs. An end-to-end theoretical analysis and evaluations show that our mechanism notably improves the robustness and scalability of DP DNNs.",
            "year": 2019,
            "citationCount": 37,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A scalable algorithm to preserve differential privacy (DP) in adversarial learning for deep neural networks (DNNs), with certified robustness to adversarial examples, is developed by leveraging the sequential composition theory in DP."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving language models' robustness to adversarial examples while preserving user privacy. The proposed approach is prompt-based adversarial training combined with privacy-preserving techniques like secure multi-party computation.\n\nThe research problem in the paper is preserving differential privacy in adversarial learning for deep neural networks, with certified robustness to adversarial examples. The proposed approach is randomizing both input and latent spaces, using a new adversarial objective function, and applying stochastic batch training.\n\nWhile both works aim to improve robustness to adversarial examples, the proposal focuses specifically on language models and emphasizes privacy preservation, while the paper targets general deep neural networks and differential privacy. The approaches also differ, with the proposal using prompt-based training and secure aggregation, and the paper using randomization and a new objective function.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "b5c93d5438fcd0f21b5cb0b962e392545671e71c",
            "paperId": "b5c93d5438fcd0f21b5cb0b962e392545671e71c",
            "title": "Preserving Differential Privacy in Adversarial Learning with Provable Robustness",
            "abstract": "In this paper, we aim to develop a novel mechanism to preserve differential privacy (DP) in adversarial learning for deep neural networks, with provable robustness to adversarial examples. We leverage the sequential composition theory in differential privacy, to establish a new connection between differential privacy preservation and provable robustness. To address the trade-off among model utility, privacy loss, and robustness, we design an original, differentially private, adversarial objective function, based on the post-processing property in differential privacy, to tighten the sensitivity of our model. Theoretical analysis and thorough evaluations show that our mechanism notably improves the robustness of DP deep neural networks.",
            "year": 2019,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel mechanism to preserve differential privacy in adversarial learning for deep neural networks, with provable robustness to adversarial examples, is developed using the sequential composition theory in differential privacy."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving language models' robustness without compromising user privacy, and the proposed approach is prompt-based adversarial training with privacy-preserving techniques. The research problem in the paper is preserving differential privacy in adversarial learning for deep neural networks with provable robustness, and the proposed approach is designing a differentially private adversarial objective function based on the post-processing property in differential privacy.\n\nWhile both works aim to address the trade-off between privacy and robustness in adversarial learning, the proposal focuses specifically on language models and uses prompt-based techniques, whereas the paper considers general deep neural networks and designs a novel objective function. Therefore, the two works have different focuses and approaches.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "be0ca5cb382639cd1d6371a5e514d8037fc7c36c",
            "paperId": "be0ca5cb382639cd1d6371a5e514d8037fc7c36c",
            "title": "Heterogeneous Gaussian Mechanism: Preserving Differential Privacy in Deep Learning with Provable Robustness",
            "abstract": "In this paper, we propose a novel Heterogeneous Gaussian Mechanism (HGM) to preserve differential privacy in deep neural networks, with provable robustness against adversarial examples. We first relax the constraint of the privacy budget in the traditional Gaussian Mechanism from (0, 1] to (0, infty), with a new bound of the noise scale to preserve differential privacy. The noise in our mechanism can be arbitrarily redistributed, offering a distinctive ability to address the trade-off between model utility and privacy loss. To derive provable robustness, our HGM is applied to inject Gaussian noise into the first hidden layer. Then, a tighter robustness bound is proposed. Theoretical analysis and thorough evaluations show that our mechanism notably improves the robustness of differentially private deep neural networks, compared with baseline approaches, under a variety of model attacks.",
            "year": 2019,
            "citationCount": 42,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Theoretical analysis and thorough evaluations show that the novel Heterogeneous Gaussian Mechanism notably improves the robustness of differentially private deep neural networks, compared with baseline approaches, under a variety of model attacks."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving language models' robustness against adversarial examples while preserving user privacy. The proposed approach is prompt-based adversarial training combined with privacy-preserving techniques like secure multi-party computation.\n\nThe research problem in the paper is preserving differential privacy in deep neural networks with provable robustness against adversarial examples. The proposed approach is a novel Heterogeneous Gaussian Mechanism that injects Gaussian noise into the first hidden layer.\n\nWhile both works aim to improve model robustness against adversarial examples with privacy preservation, the proposal focuses on language models and uses prompt-based adversarial training, while the paper focuses on deep neural networks in general and uses a novel Gaussian noise injection method.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "9a0f53a0ff25d1a5fb016aaa22185c4d6b5a2ac8",
            "paperId": "9a0f53a0ff25d1a5fb016aaa22185c4d6b5a2ac8",
            "title": "LinkPrompt: Natural and Universal Adversarial Attacks on Prompt-based Language Models",
            "abstract": "Prompt-based learning is a new language model training paradigm that adapts the Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes the performance benchmarks across various natural language processing (NLP) tasks. Instead of using a fixed prompt template to fine-tune the model, some research demonstrates the effectiveness of searching for the prompt via optimization. Such prompt optimization process of prompt-based learning on PLMs also gives insight into generating adversarial prompts to mislead the model, raising concerns about the adversarial vulnerability of this paradigm. Recent studies have shown that universal adversarial triggers (UATs) can be generated to alter not only the predictions of the target PLMs but also the prediction of corresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based learning paradigm. However, UATs found in previous works are often unreadable tokens or characters and can be easily distinguished from natural texts with adaptive defenses. In this work, we consider the naturalness of the UATs and develop $\\textit{LinkPrompt}$, an adversarial attack algorithm to generate UATs by a gradient-based beam search algorithm that not only effectively attacks the target PLMs and PFMs but also maintains the naturalness among the trigger tokens. Extensive results demonstrate the effectiveness of $\\textit{LinkPrompt}$, as well as the transferability of UATs generated by $\\textit{LinkPrompt}$ to open-sourced Large Language Model (LLM) Llama2 and API-accessed LLM GPT-3.5-turbo. The resource is available at $\\href{https://github.com/SavannahXu79/LinkPrompt}{https://github.com/SavannahXu79/LinkPrompt}$.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An adversarial attack algorithm to generate UATs by a gradient-based beam search algorithm that not only effectively attacks the target PLMs and PFMs but also maintains the naturalness among the trigger tokens is developed."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve language models' robustness and preserve user privacy during adversarial training by using prompt-based adversarial example generation and secure multi-party computation for model update aggregation.\n\nThe paper proposes an adversarial attack algorithm called LinkPrompt to generate natural and universal adversarial triggers that can effectively attack target pre-trained language models and prompt-based fine-tuning models while maintaining the naturalness of the trigger tokens.\n\nThe project focuses on improving model robustness and preserving privacy during adversarial training, while the paper focuses on generating natural and effective adversarial triggers to attack language models. They address different aspects of adversarial learning in language models.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "6303855a23c4fa5ab78450b0a93e9b0c34ca4a5a",
            "paperId": "6303855a23c4fa5ab78450b0a93e9b0c34ca4a5a",
            "title": "Exploring the Universal Vulnerability of Prompt-based Learning Paradigm",
            "abstract": "Prompt-based learning paradigm bridges the gap between pre-training and fine-tuning, and works effectively under the few-shot setting. However, we find that this learning paradigm inherits the vulnerability from the pre-training stage, where model predictions can be misled by inserting certain triggers into the text. In this paper, we explore this universal vulnerability by either injecting backdoor triggers or searching for adversarial triggers on pre-trained language models using only plain text. In both scenarios, we demonstrate that our triggers can totally control or severely decrease the performance of prompt-based models fine-tuned on arbitrary downstream tasks, reflecting the universal vulnerability of the prompt-based learning paradigm. Further experiments show that adversarial triggers have good transferability among language models. We also find conventional fine-tuning models are not vulnerable to adversarial triggers constructed from pre-trained language models. We conclude by proposing a potential solution to mitigate our attack methods. Code and data are publicly available at https://github.com/leix28/prompt-universal-vulnerability",
            "year": 2022,
            "citationCount": 41,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper demonstrates that backdoor triggers or searching for adversarial triggers on pre-trained language models using only plain text can totally control or severely decrease the performance of prompt-based models fine-tuned on arbitrary downstream tasks, reflecting the universal vulnerability of the prompt- based learning paradigm."
            },
            "score": 6
        },
        {
            "id": "6a2e7da48e3588feee6e5dceceae3cd87bd02ae9",
            "paperId": "6a2e7da48e3588feee6e5dceceae3cd87bd02ae9",
            "title": "Attacking Large Language Models with Projected Gradient Descent",
            "abstract": "Current LLM alignment methods are readily broken through specifically crafted adversarial prompts. While crafting adversarial prompts using discrete optimization is highly effective, such attacks typically use more than 100,000 LLM calls. This high computational cost makes them unsuitable for, e.g., quantitative analyses and adversarial training. To remedy this, we revisit Projected Gradient Descent (PGD) on the continuously relaxed input prompt. Although previous attempts with ordinary gradient-based attacks largely failed, we show that carefully controlling the error introduced by the continuous relaxation tremendously boosts their efficacy. Our PGD for LLMs is up to one order of magnitude faster than state-of-the-art discrete optimization to achieve the same devastating attack results.",
            "year": 2024,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work revisits Projected Gradient Descent (PGD) on the continuously relaxed input prompt and shows that carefully controlling the error introduced by the continuous relaxation tremendously boosts their efficacy."
            },
            "score": 6
        },
        {
            "id": "b6499bcc10d4a70c3ca8b84995270cfd0d29de4c",
            "paperId": "b6499bcc10d4a70c3ca8b84995270cfd0d29de4c",
            "title": "Model-tuning Via Prompts Makes NLP Models Adversarially Robust",
            "abstract": "In recent years, NLP practitioners have converged on the following practice: (i) import an off-the-shelf pretrained (masked) language model; (ii) append a multilayer perceptron atop the CLS token's hidden representation (with randomly initialized weights); and (iii) fine-tune the entire model on a downstream task (MLP-FT). This procedure has produced massive gains on standard NLP benchmarks, but these models remain brittle, even to mild adversarial perturbations. In this work, we demonstrate surprising gains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an alternative method of adapting to downstream tasks. Rather than appending an MLP head to make output prediction, MVP appends a prompt template to the input, and makes prediction via text infilling/completion. Across 5 NLP datasets, 4 adversarial attacks, and 3 different models, MVP improves performance against adversarial substitutions by an average of 8% over standard methods and even outperforms adversarial training-based state-of-art defenses by 3.5%. By combining MVP with adversarial training, we achieve further improvements in adversarial robustness while maintaining performance on unperturbed examples. Finally, we conduct ablations to investigate the mechanism underlying these gains. Notably, we find that the main causes of vulnerability of MLP-FT can be attributed to the misalignment between pre-training and fine-tuning tasks, and the randomly initialized MLP parameters.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work demonstrates surprising gains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an alternative method of adapting to downstream tasks that improves performance against adversarial substitutions and outperforms adversarial training-based state-of-art defenses by 3.5%."
            },
            "score": 6
        },
        {
            "id": "7223d9528902927bf545737c57ef2ca402d1dd27",
            "paperId": "7223d9528902927bf545737c57ef2ca402d1dd27",
            "title": "Adversarial Training for Privacy-Preserving Deep Learning Model Distribution",
            "abstract": "Collaboration among cancer registries is essential to develop accurate, robust, and generalizable deep learning models for automated information extraction from cancer pathology reports. Sharing data presents a serious privacy issue, especially in biomedical research and healthcare delivery domains. Distributing pretrained deep learning (DL) models has been proposed to avoid critical data sharing. However, there is growing recognition that collaboration among clinical institutes through DL model distribution exposes new security and privacy vulnerabilities. These vulnerabilities increase in natural language processing (NLP) applications, in which the dataset vocabulary with word vector representations needs to be associated with the other model parameters. In this paper, we propose a novel privacy-preserving DL model distribution across cancer registries for information extraction from cancer pathology reports with privacy and confidentiality considerations. The proposed approach exploits the adversarial training framework to distinguish private features from shared features among different datasets. It only shares registry-invariant model parameters, without sharing raw data nor registry-specific model parameters among cancer registries. Thus, it protects both the data and the trained model simultaneously. We compare our proposed approach to single-registry models, and a model trained on centrally hosted data from different cancer registries. The results show that the proposed approach significantly outperforms the single-registry models and achieves statistically indistinguishable micro and macro F1-score as compared to the centralized model.",
            "year": 2019,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel privacy-preserving DL model distribution across cancer registries for information extraction from cancer pathology reports with privacy and confidentiality considerations is proposed and significantly outperforms the single-registry models and achieves statistically indistinguishable micro and macro F1-score as compared to the centralized model."
            },
            "score": 6
        },
        {
            "id": "8b070b70f18b387a64b814e7c237e9b596f2a2d6",
            "paperId": "8b070b70f18b387a64b814e7c237e9b596f2a2d6",
            "title": "Privacy-Preserving Feature Extraction via Adversarial Training",
            "abstract": "Deep learning is increasingly popular, partly due to its widespread application potential, such as in civilian, government and military domains. Given the exacting computational requirements, cloud computing has been utilized to host user data and model. However, such an approach has potential privacy implications. Therefore, in this paper, we propose a method to protect user\u2019s privacy in the inference phase of deep learning workflow. Specifically, we use an intermediate layer to separate the entire neural network into two parts, which are respectively deployed on the user device and the cloud server. The encoder, deployed on the user device, is used for raw data transformation, which removes the need for users to upload raw data to the cloud directly. However, we also demonstrate there exists potential for privacy leakage in the intermediate features of the neural network through two concrete experiments. In other words, the encoder on its own does not provide adequate privacy protection. Therefore, we also propose an approach to achieve Privacy-preserving Feature Extraction based on Adversarial Training (P-FEAT), where the goal of privacy attacking tasks and the goal of target tasks are adversarial in terms of sensitive attributes. By imposing privacy constraints during the feature extraction, we can reduce the contribution of the extracted features to the privacy leakage. In this way, privacy protection capability of the encoder can be further strengthened. We then demonstrate the effectiveness of P-FEAT using a large number of experiments, whose findings show that P-FEAT can significantly reduce the threats of privacy attacking tasks while maintaining high accuracy of the target tasks.",
            "year": 2020,
            "citationCount": 20,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper uses an intermediate layer to separate the entire neural network into two parts, which are respectively deployed on the user device and the cloud server, and proposes an approach to achieve Privacy-preserving Feature Extraction based on Adversarial Training (P-FEAT)."
            },
            "score": 6
        },
        {
            "id": "bac1bf436896163cac6a0975e86be1266fd0cab7",
            "paperId": "bac1bf436896163cac6a0975e86be1266fd0cab7",
            "title": "Privacy-Preserving Adversarial Representation Learning in ASR: Reality or Illusion?",
            "abstract": "Automatic speech recognition (ASR) is a key technology in many services and applications. This typically requires user devices to send their speech data to the cloud for ASR decoding. As the speech signal carries a lot of information about the speaker, this raises serious privacy concerns. As a solution, an encoder may reside on each user device which performs local computations to anonymize the representation. In this paper, we focus on the protection of speaker identity and study the extent to which users can be recognized based on the encoded representation of their speech as obtained by a deep encoder-decoder architecture trained for ASR. Through speaker identification and verification experiments on the Librispeech corpus with open and closed sets of speakers, we show that the representations obtained from a standard architecture still carry a lot of information about speaker identity. We then propose to use adversarial training to learn representations that perform well in ASR while hiding speaker identity. Our results demonstrate that adversarial training dramatically reduces the closed-set classification accuracy, but this does not translate into increased open-set verification error hence into increased protection of the speaker identity in practice. We suggest several possible reasons behind this negative result.",
            "year": 2019,
            "citationCount": 66,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The extent to which users can be recognized based on the encoded representation of their speech as obtained by a deep encoder-decoder architecture trained for ASR is studied and adversarial training is proposed to learn representations that perform well in ASR while hiding speaker identity."
            },
            "score": 6
        },
        {
            "id": "1dd1fa8eca97f65de0db6dd8d248fd40737fed8f",
            "paperId": "1dd1fa8eca97f65de0db6dd8d248fd40737fed8f",
            "title": "DeepObfuscator: Obfuscating Intermediate Representations with Privacy-Preserving Adversarial Learning on Smartphones",
            "abstract": "Deep learning has been widely applied in many computer vision applications, with remarkable success. However, running deep learning models on mobile devices is generally challenging due to the limitation of computing resources. A popular alternative is to use cloud services to run deep learning models to process raw data. This, however, imposes privacy risks. Some prior arts proposed sending the features extracted from raw data (e.g., images) to the cloud. Unfortunately, these extracted features can still be exploited by attackers to recover raw images and to infer embedded private attributes (e.g., age, gender, etc.). In this paper, we propose an adversarial training framework, DeepObfuscator, which prevents the usage of the features for reconstruction of the raw images and inference of private attributes. This is done while retaining useful information for the intended cloud service (i.e., image classification). DeepObfuscator includes a learnable encoder, namely, obfuscator that is designed to hide privacy-related sensitive information from the features by performing our proposed adversarial training algorithm. The proposed algorithm is designed by simulating the game between an attacker who makes efforts to reconstruct raw image and infer private attributes from the extracted features and a defender who aims to protect user privacy. By deploying the trained obfuscator on the smartphone, features can be locally extracted and then sent to the cloud. Our experiments on CelebA and LFW datasets show that the quality of the reconstructed images from the obfuscated features of the raw image is dramatically decreased from 0.9458 to 0.3175 in terms of multi-scale structural similarity (MS-SSIM). The person in the reconstructed image, hence, becomes hardly to be re-identified. The classification accuracy of the inferred private attributes that can be achieved by the attacker is significantly reduced to a random-guessing level, e.g., the accuracy of gender is reduced from 97.36% to 58.85%. As a comparison, the accuracy of the intended classification tasks performed via the cloud service is only reduced by 2%. We also demonstrate the efficiency of DeepObfuscator, showcasing real-time performance of the deployed models on smartphones.",
            "year": 2019,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An adversarial training framework, DeepObfuscator, which prevents the usage of the features for reconstruction of the raw images and inference of private attributes, and includes a learnable encoder that is designed to hide privacy-related sensitive information from the features by performing the proposed adversarialTraining algorithm."
            },
            "score": 6
        },
        {
            "id": "1d2967d96b5e2daa172cb052b22c094beeec3068",
            "paperId": "1d2967d96b5e2daa172cb052b22c094beeec3068",
            "title": "Federated Learning of Gboard Language Models with Differential Privacy",
            "abstract": "We train and deploy language models (LMs) with federated learning (FL) and differential privacy (DP) in Google Keyboard (Gboard). The recent DP-Follow the Regularized Leader (DP-FTRL) algorithm is applied to achieve meaningfully formal DP guarantees without requiring uniform sampling of clients. To provide favorable privacy-utility trade-offs, we introduce a new client participation criterion and discuss the implication of its configuration in large scale systems. We show how quantile-based clip estimation can be combined with DP-FTRL to adaptively choose the clip norm during training or reduce the hyperparameter tuning in preparation of training. With the help of pretraining on public data, we trained and deployed more than fifteen Gboard LMs that achieve high utility and $\\rho-$zCDP privacy guarantees with $\\rho \\in (0.3, 2)$, with one model additionally trained with secure aggregation.We summarize our experience and provide concrete suggestions on DP training for practitioners.",
            "year": 2023,
            "citationCount": 36,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown how quantile-based clip estimation can be combined with DP-FTRL to adaptively choose the clip norm during training or reduce the hyperparameter tuning in preparation of training."
            },
            "score": 6
        },
        {
            "id": "9335a77d15c154d56ab5fd7b289f5dd553d19ffd",
            "paperId": "9335a77d15c154d56ab5fd7b289f5dd553d19ffd",
            "title": "Privacy-Preserving Decentralized Aggregation for Federated Learning",
            "abstract": "In this paper, we develop a privacy-preserving decentralized aggregation protocol for federated learning. We formulate the distributed aggregation protocol with the Alternating Direction Method of Multiplier (ADMM) algorithm and examine its privacy challenges. Unlike prior works that use differential privacy or homomorphic encryption for privacy, we develop a protocol that controls communication among participants in each round of aggregation to minimize privacy leakage. We establish the protocol's privacy guarantee against an honest-but-curious adversary. We also propose an efficient algorithm to construct such a communication pattern, which is inspired by combinatorial block design theory. Our secure aggregation protocol based on the novel group-based communication pattern leads to an efficient algorithm for federated training with privacy guarantees. We evaluate our federated training algorithm on computer vision and natural language processing models over benchmark datasets with 9 and 15 distributed sites. Experimental results demonstrate the privacy-preserving capabilities of our algorithm while maintaining learning performance comparable to the baseline centralized federated learning.",
            "year": 2020,
            "citationCount": 19,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper develops a privacy-preserving decentralized aggregation protocol that controls communication among participants in each round of aggregation to minimize privacy leakage, and proposes an efficient algorithm to construct such a communication pattern, which is inspired by combinatorial block design theory."
            },
            "score": 6
        },
        {
            "id": "b5c4da5bc3cc96bc56519bccd389e82542797d02",
            "paperId": "b5c4da5bc3cc96bc56519bccd389e82542797d02",
            "title": "ELSA: Secure Aggregation for Federated Learning with Malicious Actors",
            "abstract": "Federated learning (FL) is an increasingly popular approach for machine learning (ML) in cases where the training dataset is highly distributed. Clients perform local training on their datasets and the updates are then aggregated into the global model. Existing protocols for aggregation are either inefficient, or don\u2019t consider the case of malicious actors in the system. This is a major barrier in making FL an ideal solution for privacy-sensitive ML applications. We present Elsa, a secure aggregation protocol for FL, which breaks this barrier - it is efficient and addresses the existence of malicious actors at the core of its design. Similar to prior work on Prio and Prio+, Elsa provides a novel secure aggregation protocol built out of distributed trust across two servers that keeps individual client updates private as long as one server is honest, defends against malicious clients, and is efficient end-to-end. Compared to prior works, the distinguishing theme in Elsa is that instead of the servers generating cryptographic correlations interactively, the clients act as untrusted dealers of these correlations without compromising the protocol\u2019s security. This leads to a much faster protocol while also achieving stronger security at that efficiency compared to prior work. We introduce new techniques that retain privacy even when a server is malicious at a small added cost of 7-25% in runtime with negligible increase in communication over the case of semi-honest server. Our work improves end-to-end runtime over prior work with similar security guarantees by big margins - single-aggregator RoFL by up to 305x (for the models we consider), and distributed trust Prio by up to 8x.",
            "year": 2023,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Elsa protocol is presented, a novel secure aggregation protocol built out of distributed trust across two servers that keeps individual client updates private as long as one server is honest, defends against malicious clients, and is efficient end-to-end."
            },
            "score": 6
        },
        {
            "id": "f38cab7b9e65f08f2b590a91bb1146b631960e57",
            "paperId": "f38cab7b9e65f08f2b590a91bb1146b631960e57",
            "title": "Secure Aggregation for Clustered Federated Learning",
            "abstract": "Clustered federated learning is a popular paradigm to tackle data heterogeneity in federated learning, by training personalized models for groups of users with similar data distributions. A critical challenge is to protect the privacy of individual user updates, as the latter can reveal extensive information about sensitive local datasets. To do so, a recent promising approach is information-theoretic secure aggregation, where parties learn the aggregate (sum) of user updates, but no further information is revealed about the individual updates. In this work, we present the first secure aggregation frameworks in the context of clustered federated learning, to learn the aggregate of user updates for any clustering of users, but without learning any information about the local updates or cluster identities. Our frameworks can achieve linear communication complexity under formal information-theoretic privacy guarantees, while providing key trade-offs between communication and computation complexity, adversary tolerance, and resilience to user dropouts.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents the first secure aggregation frameworks in the context of clustered federated learning, to learn the aggregate of user updates for any clustering of users, but without learning any information about the local updates or cluster identities."
            },
            "score": 6
        },
        {
            "id": "f8a4a34dd0c8a8ec27f78c3c9f4913fc70aa4e4a",
            "paperId": "f8a4a34dd0c8a8ec27f78c3c9f4913fc70aa4e4a",
            "title": "EPPDA: An Efficient Privacy-Preserving Data Aggregation Federated Learning Scheme",
            "abstract": "Federated learning (FL) is a kind of privacy-awaremachine learning, in which the machine learning models are trained on the users\u2019 side and then the model updates are transmitted to the server for aggregating. As the data owners need not upload their data, FL is a privacy-persevering machine learning model. However, FL is weak as it suffers from a reverse attack, in which an adversary can get users\u2019 data by analyzing the user uploaded model. Motivated by this, in this paper, based on the secret sharing, we design, an efficient privacy-preserving data aggregation mechanism for FL, to resist the reverse attack, which can aggregate users\u2019 trained models secretly without leaking the user\u2019s model. Moreover, EPPDA has efficient fault tolerance for the user disconnection. Even if a large number of users are disconnected when the protocol runs, EPPDA will execute normally. Analysis shows that the EPPDA can provide a sum of locally trained models to the server without leaking any single user\u2019s model. Moreover, adversary can not get any non-public information from the communication channel. Efficiency verification proves that the EPPDA not only protects users\u2019 privacy but also needs fewer computing and communication resources.",
            "year": 2023,
            "citationCount": 55,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Efficiency verification proves that the EPPDA not only protects users\u2019 privacy but also needs fewer computing and communication resources."
            },
            "score": 6
        },
        {
            "id": "a99df2be6d452f0343ad9d6cf7f70630ff1e2ec9",
            "paperId": "a99df2be6d452f0343ad9d6cf7f70630ff1e2ec9",
            "title": "How Much Privacy Does Federated Learning with Secure Aggregation Guarantee?",
            "abstract": "Federated learning (FL) has attracted growing interest for enabling privacy-preserving machine learning on data stored at multiple users while avoiding moving the data off-device. However, while data never leaves users\u2019 devices, privacy still cannot be guaranteed since significant computations on users\u2019 training data are shared in the form of trained local models. These local models have recently been shown to pose a substantial privacy threat through different privacy attacks such as model inversion attacks. As a remedy, Secure Aggregation (SA) has been developed as a framework to preserve privacy in FL, by guaranteeing the server can only learn the global aggregated model update but not the individual model updates.While SA ensures no additional information is leaked about the individual model update beyond the aggregated model update, there are no formal guarantees on how much privacy FL with SA can actually offer; as information about the individual dataset can still potentially leak through the aggregated model computed at the server. In this work, we perform a first analysis of the formal privacy guarantees for FL with SA. Specifically, we use Mutual Information (MI) as a quantification metric and derive upper bounds on how much information about each user's dataset can leak through the aggregated model update. When using the FedSGD aggregation algorithm, our theoretical bounds show that the amount of privacy leakage reduces linearly with the number of users participating in FL with SA. To validate our theoretical bounds, we use an MI Neural Estimator to empirically evaluate the privacy leakage under different FL setups on both the MNIST and CIFAR10 datasets. Our experiments verify our theoretical bounds for FedSGD, which show a reduction in privacy leakage as the number of users and local batch size grow, and an increase in privacy leakage as the number of training rounds increases. We also observe similar dependencies for the FedAvg and FedProx protocol.",
            "year": 2022,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work uses Mutual Information (MI) as a quantification metric and derive upper bounds on how much information about each user's dataset can leak through the aggregated model update, and shows that the amount of privacy leakage reduces linearly with the number of users participating in FL with SA."
            },
            "score": 6
        },
        {
            "id": "0021b3beb2ee0906b425eef7c0f453623c1c6a03",
            "paperId": "0021b3beb2ee0906b425eef7c0f453623c1c6a03",
            "title": "Certified Robustness to Word Substitution Attack with Differential Privacy",
            "abstract": "The robustness and security of natural language processing (NLP) models are significantly important in real-world applications. In the context of text classification tasks, adversarial examples can be designed by substituting words with synonyms under certain semantic and syntactic constraints, such that a well-trained model will give a wrong prediction. Therefore, it is crucial to develop techniques to provide a rigorous and provable robustness guarantee against such attacks. In this paper, we propose WordDP to achieve certified robustness against word substitution at- tacks in text classification via differential privacy (DP). We establish the connection between DP and adversarial robustness for the first time in the text domain and propose a conceptual exponential mechanism-based algorithm to formally achieve the robustness. We further present a practical simulated exponential mechanism that has efficient inference with certified robustness. We not only provide a rigorous analytic derivation of the certified condition but also experimentally compare the utility of WordDP with existing defense algorithms. The results show that WordDP achieves higher accuracy and more than 30X efficiency improvement over the state-of-the-art certified robustness mechanism in typical text classification tasks.",
            "year": 2021,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper establishes the connection between DP and adversarial robustness for the first time in the text domain and proposes a conceptual exponential mechanism-based algorithm to formally achieve the robustness."
            },
            "score": 6
        },
        {
            "id": "3e86a51d1f2051ab8f448b66c6dcc17924d17cfa",
            "paperId": "3e86a51d1f2051ab8f448b66c6dcc17924d17cfa",
            "title": "Certified Robustness to Adversarial Examples with Differential Privacy",
            "abstract": "Adversarial examples that fool machine learning models, particularly deep neural networks, have been a topic of intense research interest, with attacks and defenses being developed in a tight back-and-forth. Most past defenses are best effort and have been shown to be vulnerable to sophisticated attacks. Recently a set of certified defenses have been introduced, which provide guarantees of robustness to norm-bounded attacks. However these defenses either do not scale to large datasets or are limited in the types of models they can support. This paper presents the first certified defense that both scales to large networks and datasets (such as Google\u2019s Inception network for ImageNet) and applies broadly to arbitrary model types. Our defense, called PixelDP, is based on a novel connection between robustness against adversarial examples and differential privacy, a cryptographically-inspired privacy formalism, that provides a rigorous, generic, and flexible foundation for defense.",
            "year": 2018,
            "citationCount": 803,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents the first certified defense that both scales to large networks and datasets and applies broadly to arbitrary model types, based on a novel connection between robustness against adversarial examples and differential privacy, a cryptographically-inspired privacy formalism."
            },
            "score": 6
        },
        {
            "id": "ad7f073eb1396a294007f21201d58afe52c73332",
            "paperId": "ad7f073eb1396a294007f21201d58afe52c73332",
            "title": "Augment then Smooth: Reconciling Differential Privacy with Certified Robustness",
            "abstract": "Machine learning models are susceptible to a variety of attacks that can erode trust in their deployment. These threats include attacks against the privacy of training data and adversarial examples that jeopardize model accuracy. Differential privacy and randomized smoothing are effective defenses that provide certifiable guarantees for each of these threats, however, it is not well understood how implementing either defense impacts the other. In this work, we argue that it is possible to achieve both privacy guarantees and certified robustness simultaneously. We provide a framework called DP-CERT for integrating certified robustness through randomized smoothing into differentially private model training. For instance, compared to differentially private stochastic gradient descent on CIFAR10, DP-CERT leads to a 12-fold increase in certified accuracy and a 10-fold increase in the average certified radius at the expense of a drop in accuracy of 1.2%. Through in-depth per-sample metric analysis, we show that the certified radius correlates with the local Lipschitz constant and smoothness of the loss surface. This provides a new way to diagnose when private models will fail to be robust.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work provides a framework called DP-CERT for integrating certified robustness through randomized smoothing into differentially private model training and shows that the certified radius correlates with the local Lipschitz constant and smoothness of the loss surface, providing a new way to diagnose when private models will fail to be robust."
            },
            "score": 6
        },
        {
            "id": "a0c90e89d81469d5ab9ed93af5a020a94fa05188",
            "paperId": "a0c90e89d81469d5ab9ed93af5a020a94fa05188",
            "title": "On the Connection between Differential Privacy and Adversarial Robustness in Machine Learning",
            "abstract": "Adversarial examples in machine learning has been a topic of intense research interest, with attacks and defenses being developed in a tight back-and-forth. Most past defenses are best-effort, heuristic approaches that have all been shown to be vulnerable to sophisticated attacks. More recently, rigorous defenses that provide formal guarantees have emerged, but are hard to scale or generalize. A rigorous and general foundation for designing defenses is required to get us off this arms race trajectory. We propose leveraging differential privacy (DP) as a formal building block for robustness against adversarial examples. We observe that the semantic of DP is closely aligned with the formal definition of robustness to adversarial examples. We propose PixelDP, a strategy for learning robust deep neural networks based on formal DP guarantees. PixelDP networks give theoretical guarantees for a subset of their predictions regarding the robustness against adversarial perturbations of bounded size. Our evaluation with MNIST, CIFAR-10, and CIFAR-100 shows that PixelDP networks achieve accuracy under attack on par with the best-performing defense to date, but additionally certify robustness against meaningful-size 1-norm and 2-norm attacks for 40-60% of their predictions. Our experience points to DP as a rigorous, broadly applicable, and mechanism-rich foundation for robust machine learning.",
            "year": 2018,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes PixelDP, a strategy for learning robust deep neural networks based on formal DP guarantees, and observes that the semantic of DP is closely aligned with the formal definition of robustness to adversarial examples."
            },
            "score": 6
        },
        {
            "id": "1c91b23d78944f7f237cb512029c2165972ae9d5",
            "paperId": "1c91b23d78944f7f237cb512029c2165972ae9d5",
            "title": "On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex",
            "abstract": "Semantic parsing is a technique aimed at constructing a structured representation of the meaning of a natural-language question. Recent advances in language models trained on code have shown superior performance in generating these representations compared to language models trained solely on natural language text. The existing fine-tuned neural semantic parsers are vulnerable to adversarial attacks on natural-language inputs. While it has been established that the robustness of smaller semantic parsers can be enhanced through adversarial training, this approach is not feasible for large language models in real-world scenarios, as it requires both substantial computational resources and expensive human annotation on in-domain semantic parsing data. This paper presents the first empirical study on the adversarial robustness of a prompt-based semantic parser based on CODEX, a stateof-the-art (SOTA) language model trained on code. Our results demonstrate that the large language model of code is vulnerable to carefully crafted adversarial examples. To overcome this challenge, we propose methods for enhancing robustness without requiring substantial amounts of labelled data or intensive computational resources.",
            "year": 2023,
            "citationCount": 30,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents the first empirical study on the adversarial robustness of a prompt-based semantic parser based on CODEX, a stateof-the-art (SOTA) language model trained on code."
            },
            "score": 5
        },
        {
            "id": "4d9fc5972ab0f17f3c8aa27b4d9372f029d4dded",
            "paperId": "4d9fc5972ab0f17f3c8aa27b4d9372f029d4dded",
            "title": "Adversarial Attacks on Large Language Model-Based System and Mitigating Strategies: A Case Study on ChatGPT",
            "abstract": "Machine learning algorithms are at the forefront of the development of advanced information systems. The rapid progress in machine learning technology has enabled cutting-edge large language models (LLMs), represented by GPT-3 and ChatGPT, to perform a wide range of NLP tasks with a stunning performance. However, research on adversarial machine learning highlights the need for these intelligent systems to be more robust. Adversarial machine learning aims to evaluate attack and defense mechanisms to prevent the malicious exploitation of these systems. In the case of ChatGPT, adversarial induction prompt can cause the model to generate toxic texts that could pose serious security risks or propagate false information. To address this challenge, we first analyze the effectiveness of inducing attacks on ChatGPT. Then, two effective mitigating mechanisms are proposed. The first is a training-free prefix prompt mechanism to detect and prevent the generation of toxic texts. The second is a RoBERTa-based mechanism that identifies manipulative or misleading input text via external detection models. The availability of this method is demonstrated through experiments.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A training-free prefix prompt mechanism to detect and prevent the generation of toxic texts and a RoBERTa-based mechanism that identifies manipulative or misleading input text via external detection models are proposed."
            },
            "score": 5
        },
        {
            "id": "111c468c742f1b94c1153e6d779d472e691ef4af",
            "paperId": "111c468c742f1b94c1153e6d779d472e691ef4af",
            "title": "Adversarial Knowledge Stimulated Contrastive Prompting for Few-shot Language Learners",
            "abstract": "Prompt-based fine-tuning has boosted the performance of Pre-trained Language Models (PLMs) on few-shot Natural Language Understanding (NLU) tasks by employing task-specific prompts. Yet, PLMs are unfamiliar with prompt-style expressions during pre-training, which limits the few-shot learning performance on downstream tasks. It would be desirable if the models can stimulate prompting knowledge while adaptation to specific NLU tasks. We present the Adversarial Knowledge Stimulated Contrastive Prompting (AKSCP) framework, leading to better few-shot NLU tasks for language models by implicitly stimulate knowledge from pretrained language model. In AKSCP, a novel paradigm Cloze-driven prompt is proposed for joint prompt tuning across word cloze task and prompt-based learning, forcing PLMs to stimulate prompting knowledge. We further design an Adversarial Contrastive learning method to improve the generalization ability of PLM for different downstream tasks. Experiments over a variety of NLU tasks show that AKSCP consistently outperforms state-of-the-arts for prompt-based fine-tuning.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "In AKSCP, a novel paradigm Cloze-driven prompt is proposed for joint prompt tuning across word cloze task and prompt-based learning, forcing PLMs to stimulate prompting knowledge, and an Adversarial Contrastive learning method is designed to improve the generalization ability of PLM for different downstream tasks."
            },
            "score": 5
        },
        {
            "id": "ffb2f048ae694e08a741f8aa339e0f80003497bc",
            "paperId": "ffb2f048ae694e08a741f8aa339e0f80003497bc",
            "title": "Language Model Unalignment: Parametric Red-Teaming to Expose Hidden Harms and Biases",
            "abstract": "Red-teaming has been a widely adopted way to evaluate the harmfulness of Large Language Models (LLMs). It aims to jailbreak a model's safety behavior to make it act as a helpful agent disregarding the harmfulness of the query. Existing methods are primarily based on input text-based red-teaming such as adversarial prompts, low-resource prompts, or contextualized prompts to condition the model in a way to bypass its safe behavior. Bypassing the guardrails uncovers hidden harmful information and biases in the model that are left untreated or newly introduced by its safety training. However, prompt-based attacks fail to provide such a diagnosis owing to their low attack success rate, and applicability to specific models. In this paper, we present a new perspective on LLM safety research i.e., parametric red-teaming through Unalignment. It simply (instruction) tunes the model parameters to break model guardrails that are not deeply rooted in the model's behavior. Unalignment using as few as 100 examples can significantly bypass commonly referred to as CHATGPT, to the point where it responds with an 88% success rate to harmful queries on two safety benchmark datasets. On open-source models such as VICUNA-7B and LLAMA-2-CHAT 7B AND 13B, it shows an attack success rate of more than 91%. On bias evaluations, Unalignment exposes inherent biases in safety-aligned models such as CHATGPT and LLAMA- 2-CHAT where the model's responses are strongly biased and opinionated 64% of the time.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new perspective on LLM safety research is presented i.e., parametric red-teaming through Unalignment, which tunes the model parameters to break model guardrails that are not deeply rooted in the model's behavior."
            },
            "score": 5
        },
        {
            "id": "e8fa186444d98a39ee9139b1f5dd0c7618caef8f",
            "paperId": "e8fa186444d98a39ee9139b1f5dd0c7618caef8f",
            "title": "Privacy-preserving Neural Representations of Text",
            "abstract": "This article deals with adversarial attacks towards deep learning systems for Natural Language Processing (NLP), in the context of privacy protection. We study a specific type of attack: an attacker eavesdrops on the hidden representations of a neural text classifier and tries to recover information about the input text. Such scenario may arise in situations when the computation of a neural network is shared across multiple devices, e.g. some hidden representation is computed by a user\u2019s device and sent to a cloud-based model. We measure the privacy of a hidden representation by the ability of an attacker to predict accurately specific private information from it and characterize the tradeoff between the privacy and the utility of neural representations. Finally, we propose several defense methods based on modified training objectives and show that they improve the privacy of neural representations.",
            "year": 2018,
            "citationCount": 99,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This article measures the privacy of a hidden representation by the ability of an attacker to predict accurately specific private information from it and characterize the tradeoff between the privacy and the utility of neural representations."
            },
            "score": 5
        },
        {
            "id": "0322f7202defa85f0a7ee7a4bde2bb578a1950c0",
            "paperId": "0322f7202defa85f0a7ee7a4bde2bb578a1950c0",
            "title": "Optimal vocabulary selection approaches for privacy-preserving deep NLP model training for information extraction and cancer epidemiology.",
            "abstract": "BACKGROUND\nWith the use of artificial intelligence and machine learning techniques for biomedical informatics, security and privacy concerns over the data and subject identities have also become an important issue and essential research topic. Without intentional safeguards, machine learning models may find patterns and features to improve task performance that are associated with private personal information.\n\n\nOBJECTIVE\nThe privacy vulnerability of deep learning models for information extraction from medical textural contents needs to be quantified since the models are exposed to private health information and personally identifiable information. The objective of the study is to quantify the privacy vulnerability of the deep learning models for natural language processing and explore a proper way of securing patients' information to mitigate confidentiality breaches.\n\n\nMETHODS\nThe target model is the multitask convolutional neural network for information extraction from cancer pathology reports, where the data for training the model are from multiple state population-based cancer registries. This study proposes the following schemes to collect vocabularies from the cancer pathology reports; (a) words appearing in multiple registries, and (b)\u00a0words that have higher mutual information. We performed membership inference attacks on the models in high-performance computing environments.\n\n\nRESULTS\nThe comparison outcomes suggest that the proposed vocabulary selection methods resulted in lower privacy vulnerability while maintaining the same level of clinical task performance.",
            "year": 2022,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The comparison outcomes suggest that the proposed vocabulary selection methods resulted in lower privacy vulnerability while maintaining the same level of clinical task performance."
            },
            "score": 5
        },
        {
            "id": "a21ea13836b63ac9e6caa01ab0a001eafc608e24",
            "paperId": "a21ea13836b63ac9e6caa01ab0a001eafc608e24",
            "title": "Learning Privacy Preserving Encodings Through Adversarial Training",
            "abstract": "We present a framework to learn privacy-preserving encodings of images that inhibit inference of chosen private attributes, while allowing recovery of other desirable information. Rather than simply inhibiting a given fixed pre-trained estimator, our goal is that an estimator be unable to learn to accurately predict the private attributes even with knowledge of the encoding function. We use a natural adversarial optimization-based formulation for this\u2014training the encoding function against a classifier for the private attribute, with both modeled as deep neural networks. The key contribution of our work is a stable and convergent optimization approach that is successful at learning an encoder with our desired properties\u2014maintaining utility while inhibiting inference of private attributes, not just within the adversarial optimization, but also by classifiers that are trained after the encoder is fixed. We adopt a rigorous experimental protocol for verification wherein classifiers are trained exhaustively till saturation on the fixed encoders. We evaluate our approach on tasks of real-world complexity\u2014learning high-dimensional encodings that inhibit detection of different scene categories\u2014and find that it yields encoders that are resilient at maintaining privacy.",
            "year": 2018,
            "citationCount": 61,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents a framework to learn privacy-preserving encodings of images that inhibit inference of chosen private attributes, while allowing recovery of other desirable information, and finds that it yields encoders that are resilient at maintaining privacy."
            },
            "score": 5
        },
        {
            "id": "02c71d8fe168e2bdc9ff30074eba67eb29c48f16",
            "paperId": "02c71d8fe168e2bdc9ff30074eba67eb29c48f16",
            "title": "Privacy-Preserving Machine Learning Using Federated Learning and Secure Aggregation",
            "abstract": "Over the past few years, machine learning has been responsible for the rapid advancements in fields such as computer vision, natural language processing and speech recognition. No small part of this success is due to data becoming more and more available, often being collected in privacy-invasive ways. The aim of this work is to study the use of two privacy-preserving solutions for training machine learning models: Federated Learning (FL) and Secure Multiparty Computation (MPC). Federated learning is a subfield of machine learning that allows training models on a large, decentralized corpus of data residing on edge devices like smartphones. Instead of sharing data, users collaboratively train a model by only sending weight updates to a server. By leveraging secure multiparty computation, it can be ensured that the server cannot inspect any individual user's update. To assess the feasibility of these approaches in different settings, a client-server architecture was implemented in Python and multiple experiments were run on datasets made available by LEAF in order to investigate ways of improving the overall performance of the models trained in a federated manner.",
            "year": 2020,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work is to study the use of two privacy-preserving solutions for training machine learning models: Federated Learning (FL) and Secure Multiparty Computation (MPC)."
            },
            "score": 5
        },
        {
            "id": "54e139fb6dcee360ca5d8c584b2f6ef4c187c285",
            "paperId": "54e139fb6dcee360ca5d8c584b2f6ef4c187c285",
            "title": "VerSA: Verifiable Secure Aggregation for Cross-Device Federated Learning",
            "abstract": "In privacy-preserving cross-device federated learning, users train a global model on their local data and submit encrypted local models, while an untrusted central server aggregates the encrypted models to obtain an updated global model. Prior work has demonstrated how to verify the correctness of aggregation in such a setting. However, such verification relies on strong assumptions, such as a trusted setup among all users under unreliable network conditions, or it suffers from expensive cryptographic operations, such as bilinear pairing. In this paper, we scrutinize the verification mechanism of prior work and propose a model recovery attack, demonstrating that most local models can be leaked within a reasonable time (e.g., <inline-formula><tex-math notation=\"LaTeX\">$98\\%$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>98</mml:mn><mml:mo>%</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href=\"hur-ieq1-3126323.gif\"/></alternatives></inline-formula> of encrypted local models are recovered within 21 h). Then, we propose <sc>VerSA</sc>, a verifiable secure aggregation protocol for cross-device federated learning. <sc>VerSA</sc> does not require any trusted setup for verification between users while minimizing the verification cost by enabling both the central server and users to utilize only a lightweight pseudorandom generator to prove and verify the correctness of model aggregation. We experimentally confirm the efficiency of <sc>VerSA</sc> under diverse datasets, demonstrating that <sc>VerSA</sc> is orders of magnitude faster than verification in prior work.",
            "year": 2023,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper scrutinizes the verification mechanism of prior work and proposes a model recovery attack, demonstrating that most local models can be leaked within a reasonable time, and proposes VerSA, a verifiable secure aggregation protocol for cross-device federated learning."
            },
            "score": 5
        },
        {
            "id": "b7882b7243cfe4b64006f42865e70ebfbf09d6ce",
            "paperId": "b7882b7243cfe4b64006f42865e70ebfbf09d6ce",
            "title": "SoK: Secure Aggregation Based on Cryptographic Schemes for Federated Learning",
            "abstract": "Secure aggregation consists of computing the sum of data collected from multiple sources without disclosing these individual inputs. Secure aggregation has been found useful for various applications ranging from electronic voting to smart grid measurements. Recently, federated learning emerged as a new collaborative machine learning technology to train machine learning models. In this work, we study the suitability of secure aggregation based on cryptographic schemes to federated learning. We first provide a formal definition of the problem and suggest a systematic categorization of existing solutions. We further investigate the specific challenges raised by federated learning and analyze the recent dedicated secure aggregation solutions based on cryptographic schemes. We finally share some takeaway messages that would help a secure design of federated learning and identify open research directions in this topic. Based on the takeaway messages, we propose an improved definition of secure aggregation that better fits federated learning.",
            "year": 2023,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work provides a formal definition of the problem and suggests a systematic categorization of existing solutions and proposes an improved definition of secure aggregation that better fits federated learning."
            },
            "score": 5
        },
        {
            "id": "d20932e5bef949eb62d89d7b4f484d7716547db9",
            "paperId": "d20932e5bef949eb62d89d7b4f484d7716547db9",
            "title": "Flamingo: Multi-Round Single-Server Secure Aggregation with Applications to Private Federated Learning",
            "abstract": "This paper introduces Flamingo, a system for secure aggregation of data across a large set of clients. In secure aggregation, a server sums up the private inputs of clients and obtains the result without learning anything about the individual inputs beyond what is implied by the final sum. Flamingo focuses on the multi-round setting found in federated learning in which many consecutive summations (averages) of model weights are performed to derive a good model. Previous protocols, such as Bell et al. (CCS \u201920), have been designed for a single round and are adapted to the federated learning setting by repeating the protocol multiple times. Flamingo eliminates the need for the per-round setup of previous protocols, and has a new lightweight dropout resilience protocol to ensure that if clients leave in the middle of a sum the server can still obtain a meaningful result. Furthermore, Flamingo introduces a new way to locally choose the so-called client neighborhood introduced by Bell et al. These techniques help Flamingo reduce the number of interactions between clients and the server, resulting in a significant reduction in the end-to-end runtime for a full training session over prior work.We implement and evaluate Flamingo and show that it can securely train a neural network on the (Extended) MNIST and CIFAR-100 datasets, and the model converges without a loss in accuracy, compared to a non-private federated learning system.",
            "year": 2023,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that Flamingo can securely train a neural network on the (Extended) MNIST and CIFAR-100 datasets, and the model converges without a loss in accuracy, compared to a non-private federated learning system."
            },
            "score": 5
        },
        {
            "id": "e5674b066578a690da65d81769c66db78d533929",
            "paperId": "e5674b066578a690da65d81769c66db78d533929",
            "title": "Aggregation Service for Federated Learning: An Efficient, Secure, and More Resilient Realization",
            "abstract": "Federated learning has recently emerged as a paradigm promising the benefits of harnessing rich data from diverse sources to train high quality models, with the salient features that training datasets never leave local devices. Only model updates are locally computed and shared for aggregation to produce a global model. While federated learning greatly alleviates the privacy concerns as opposed to learning with centralized data, sharing model updates still poses privacy risks. In this paper, we present a system design which offers efficient protection of individual model updates throughout the learning procedure, allowing clients to only provide obscured model updates while a cloud server can still perform the aggregation. Our federated learning system first departs from prior works by supporting lightweight encryption and aggregation, and resilience against drop-out clients with no impact on their participation in future rounds. Meanwhile, prior work largely overlooks bandwidth efficiency optimization in the ciphertext domain and the support of security against an actively adversarial cloud server, which we also fully explore in this paper and provide effective and efficient mechanisms. Extensive experiments over several benchmark datasets (MNIST, CIFAR-10, and CelebA) show our system achieves accuracy comparable to the plaintext baseline, with practical performance.",
            "year": 2022,
            "citationCount": 57,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents a system design which offers efficient protection of individual model updates throughout the learning procedure, allowing clients to only provide obscured model updates while a cloud server can still perform the aggregation."
            },
            "score": 5
        },
        {
            "id": "0dad6ecd238324f9f44d2662e3981c8bc4c06820",
            "paperId": "0dad6ecd238324f9f44d2662e3981c8bc4c06820",
            "title": "Efficient and Secure Federated Learning With Verifiable Weighted Average Aggregation",
            "abstract": "Federated learning allows a large number of participants to collaboratively train a global model without sharing participant's local data. Participants train local models with their local data and send gradients to the cloud server for aggregation. Unfortunately, as a third party, the cloud server cannot be fully trusted. Existing research has shown that a compromised cloud server can extract sensitive information of participant's local data from gradients. In addition, it can even forge the aggregation result to corrupt the global model without being detected. Therefore, in a secure federated learning system, both the privacy and aggregation correctness of the uploaded gradients should be guaranteed. In this article, we propose a secure and efficient federated learning scheme with verifiable weighted average aggregation. By adopting the masking technique to encrypt both weighted gradients and data size, our scheme can support the privacy-preserving weighted average aggregation of gradients. Moreover, we design the verifiable aggregation tag and propose an efficient verification method to validate the weighted average aggregation result, which greatly improves the performance of the aggregation verification. Security analysis shows that our scheme is provably secure. Extensive experiments demonstrate the efficiency of our scheme compared with the state-of-the-art approaches.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This article designs the verifiable aggregation tag and proposes an efficient verification method to validate the weighted average aggregation result, which greatly improves the performance of the aggregation verification and shows that the scheme is provably secure."
            },
            "score": 5
        },
        {
            "id": "c06158bde3ccf24c95c90f8f6ac580950c06636f",
            "paperId": "c06158bde3ccf24c95c90f8f6ac580950c06636f",
            "title": "A unified view on differential privacy and robustness to adversarial examples",
            "abstract": "This short note highlights some links between two lines of research within the emerging topic of trustworthy machine learning: differential privacy and robustness to adversarial examples. By abstracting the definitions of both notions, we show that they build upon the same theoretical ground and hence results obtained so far in one domain can be transferred to the other. More precisely, our analysis is based on two key elements: probabilistic mappings (also called randomized algorithms in the differential privacy community), and the Renyi divergence which subsumes a large family of divergences. We first generalize the definition of robustness against adversarial examples to encompass probabilistic mappings. Then we observe that Renyi-differential privacy (a generalization of differential privacy recently proposed in~\\cite{Mironov2017RenyiDP}) and our definition of robustness share several similarities. We finally discuss how can both communities benefit from this connection to transfer technical tools from one research field to the other.",
            "year": 2019,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This short note highlights some links between two lines of research within the emerging topic of trustworthy machine learning: differential privacy and robustness to adversarial examples by abstracting the definitions of both notions and observing that Renyi-differential privacy and the definition of robustness share several similarities."
            },
            "score": 5
        },
        {
            "id": "539ebb213159f2525440d7dbe8585c64266e659d",
            "paperId": "539ebb213159f2525440d7dbe8585c64266e659d",
            "title": "Gradient Masking and the Underestimated Robustness Threats of Differential Privacy in Deep Learning",
            "abstract": "An important problem in deep learning is the privacy and security of neural networks (NNs). Both aspects have long been considered separately. To date, it is still poorly understood how privacy enhancing training affects the robustness of NNs. This paper experimentally evaluates the impact of training with Differential Privacy (DP), a standard method for privacy preservation, on model vulnerability against a broad range of adversarial attacks. The results suggest that private models are less robust than their non-private counterparts, and that adversarial examples transfer better among DP models than between non-private and private ones. Furthermore, detailed analyses of DP and non-DP models suggest significant differences between their gradients. Additionally, this work is the first to observe that an unfavorable choice of parameters in DP training can lead to gradient masking, and, thereby, results in a wrong sense of security.",
            "year": 2021,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper experimentally evaluates the impact of training with Differential Privacy, a standard method for privacy preservation, on model vulnerability against a broad range of adversarial attacks, and suggests that private models are less robust than their non-private counterparts."
            },
            "score": 5
        },
        {
            "id": "b6cf4579b59b51d7df416e096ad86c1e6a48b458",
            "paperId": "b6cf4579b59b51d7df416e096ad86c1e6a48b458",
            "title": "Adversarial Prompt Tuning for Vision-Language Models",
            "abstract": "With the rapid advancement of multimodal learning, pre-trained Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable capacities in bridging the gap between visual and language modalities. However, these models remain vulnerable to adversarial attacks, particularly in the image modality, presenting considerable security risks. This paper introduces Adversarial Prompt Tuning (AdvPT), a novel technique to enhance the adversarial robustness of image encoders in VLMs. AdvPT innovatively leverages learnable text prompts and aligns them with adversarial image embeddings, to address the vulnerabilities inherent in VLMs without the need for extensive parameter training or modification of the model architecture. We demonstrate that AdvPT improves resistance against white-box and black-box adversarial attacks and exhibits a synergistic effect when combined with existing image-processing-based defense techniques, further boosting defensive capabilities. Comprehensive experimental analyses provide insights into adversarial prompt tuning, a novel paradigm devoted to improving resistance to adversarial images through textual input modifications, paving the way for future robust multimodal learning research. These findings open up new possibilities for enhancing the security of VLMs. Our code is available at https://github.com/jiamingzhang94/Adversarial-Prompt-Tuning.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Adversarial Prompt Tuning is introduced, a novel technique to enhance the adversarial robustness of image encoders in VLMs and improves resistance against white-box and black-box adversarial attacks and exhibits a synergistic effect when combined with existing image-processing-based defense techniques, further boosting defensive capabilities."
            },
            "score": 4
        },
        {
            "id": "da5fcb26c830663b79c9aa1c550ae62e7725fcad",
            "paperId": "da5fcb26c830663b79c9aa1c550ae62e7725fcad",
            "title": "Systematic Rectification of Language Models via Dead-end Analysis",
            "abstract": "With adversarial or otherwise normal prompts, existing large language models (LLM) can be pushed to generate toxic discourses. One way to reduce the risk of LLMs generating undesired discourses is to alter the training of the LLM. This can be very restrictive due to demanding computation requirements. Other methods rely on rule-based or prompt-based token elimination, which are limited as they dismiss future tokens and the overall meaning of the complete discourse. Here, we center detoxification on the probability that the finished discourse is ultimately considered toxic. That is, at each point, we advise against token selections proportional to how likely a finished text from this point will be toxic. To this end, we formally extend the dead-end theory from the recent reinforcement learning (RL) literature to also cover uncertain outcomes. Our approach, called rectification, utilizes a separate but significantly smaller model for detoxification, which can be applied to diverse LLMs as long as they share the same vocabulary. Importantly, our method does not require access to the internal representations of the LLM, but only the token probability distribution at each decoding step. This is crucial as many LLMs today are hosted in servers and only accessible through APIs. When applied to various LLMs, including GPT-3, our approach significantly improves the generated discourse compared to the base LLMs and other techniques in terms of both the overall language and detoxification performance.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The dead-end theory from the recent reinforcement learning (RL) literature is formally extended to also cover uncertain outcomes and the approach, called rectification, utilizes a separate but significantly smaller model for detoxification, which can be applied to diverse LLMs as long as they share the same vocabulary."
            },
            "score": 4
        },
        {
            "id": "82eea58ca6e16e7b2df9a6f76b796da5a7cfcbc5",
            "paperId": "82eea58ca6e16e7b2df9a6f76b796da5a7cfcbc5",
            "title": "Pneg: Prompt-based Negative Response Generation for Dialogue Response Selection Task",
            "abstract": "In retrieval-based dialogue systems, a response selection model acts as a ranker to select the most appropriate response among several candidates. However, such selection models tend to rely on context-response content similarity, which makes models vulnerable to adversarial responses that are semantically similar but not relevant to the dialogue context. Recent studies have shown that leveraging these adversarial responses as negative training samples is useful for improving the discriminating power of the selection model. Nevertheless, collecting human-written adversarial responses is expensive, and existing synthesizing methods often have limited scalability. To overcome these limitations, this paper proposes a simple but efficient method for generating adversarial negative responses leveraging a large-scale language model. Experimental results on dialogue selection tasks show that our method outperforms other methods of synthesizing adversarial negative responses. These results suggest that our method can be an effective alternative to human annotators in generating adversarial responses. Our code and dataset will be released if the paper is accepted.",
            "year": 2022,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results on dialogue selection tasks show that the proposed method outperforms other methods of synthesizing adversarial negative responses and suggest that the method can be an effective alternative to human annotators in generating adversarial responses."
            },
            "score": 4
        },
        {
            "id": "ada01194e95945f8f54224f5e791476a4923362e",
            "paperId": "ada01194e95945f8f54224f5e791476a4923362e",
            "title": "P NEG : Prompt-based Negative Response Generation for Robust Response Selection Model",
            "abstract": "Dialogue response selection models typically 001 predict an appropriate response relying on the 002 context-response content similarity. However, 003 the selection model with over-reliance only on 004 superficial features is vulnerable to adversar-005 ial responses that are semantically similar but 006 irrelevant to dialogue context. Recent studies 007 have shown that leveraging these adversarial 008 responses as negative training samples is useful 009 for improving the robustness of the selection 010 model. Nevertheless, existing methods often 011 require further fine-tuning for data creation or 012 have limited scalability. To overcome these 013 limitations, this paper proposes a simple but ef-014 fective method for generating adversarial nega-015 tive responses leveraging a large-scale language 016 model. Our method can generate realistic nega-017 tive responses only with a few human-written 018 examples and a prompt designed to optimize 019 generation quality. Experimental results on the 020 dialogue selection task show that our method 021 outperforms existing methods for creating neg-022 ative responses. Synthetic quality analyses and 023 ablation studies prove that our method is scal-024 able and can generate high-quality negative re-025 sponses. These results suggest that our method 026 can be an effective alternative to human anno-027 tators in generating adversarial responses. 028",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A simple but ef-014 fective method for generating adversarial nega-015 tive responses leveraging a large-scale language 016 model and results suggest that the method 026 can be an effective alternative to human anno-027 tators in generating adversarial responses."
            },
            "score": 4
        },
        {
            "id": "5a2e45ce35fb26ab70a61b424a49f8e5b4532a8e",
            "paperId": "5a2e45ce35fb26ab70a61b424a49f8e5b4532a8e",
            "title": "WARP: Word-level Adversarial ReProgramming",
            "abstract": "Transfer learning from pretrained language models recently became the dominant approach for solving many NLP tasks. A common approach to transfer learning for multiple tasks that maximize parameter sharing trains one or more task-specific layers on top of the language model. In this paper, we present an alternative approach based on adversarial reprogramming, which extends earlier work on automatic prompt generation. Adversarial reprogramming attempts to learn task-specific word embeddings that, when concatenated to the input text, instruct the language model to solve the specified task. Using up to 25K trainable parameters per task, this approach outperforms all existing methods with up to 25M trainable parameters on the public leaderboard of the GLUE benchmark. Our method, initialized with task-specific human-readable prompts, also works in a few-shot setting, outperforming GPT-3 on two SuperGLUE tasks with just 32 training samples.",
            "year": 2021,
            "citationCount": 257,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents an alternative approach based on adversarial reprogramming, which extends earlier work on automatic prompt generation, and outperforms all existing methods with up to 25M trainable parameters on the public leaderboard of the GLUE benchmark."
            },
            "score": 4
        },
        {
            "id": "2488a75031fcd4d0f0ffe4fd0a5246325a71f241",
            "paperId": "2488a75031fcd4d0f0ffe4fd0a5246325a71f241",
            "title": "Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A Two-Stage Approach to Mitigate Social Biases",
            "abstract": "As the representation capability of Pre-trained Language Models (PLMs) improve, there is growing concern that they will inherit social biases from unprocessed corpora. Most previous debiasing techniques used Counterfactual Data Augmentation (CDA) to balance the training corpus. However, CDA slightly modifies the original corpus, limiting the representation distance between different demographic groups to a narrow range. As a result, the debiasing model easily fits the differences between counterfactual pairs, which affects its debiasing performance with limited text resources. In this paper, we propose an adversarial training-inspired two-stage debiasing model using Contrastive learning with Continuous Prompt Augmentation (named CCPA) to mitigate social biases in PLMs\u2019 encoding. In the first stage, we propose a data augmentation method based on continuous prompt tuning to push farther the representation distance between sample pairs along different demographic groups. In the second stage, we utilize contrastive learning to pull closer the representation distance between the augmented sample pairs and then fine-tune PLMs\u2019 parameters to get debiased encoding. Our approach guides the model to achieve stronger debiasing performance by adding difficulty to the training process. Extensive experiments show that CCPA outperforms baselines in terms of debiasing performance. Meanwhile, experimental results on the GLUE benchmark show that CCPA retains the language modeling capability of PLMs.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes an adversarial training-inspired two-stagedebiasing model using Contrastive learning with Continuous Prompt Augmentation (named CCPA) to mitigate social biases in PLMs\u2019 encoding and guides the model to achieve stronger debiasing performance by adding difficulty to the training process."
            },
            "score": 4
        },
        {
            "id": "3937751dedfe07a21e3dadfdcdc04b5632b8e765",
            "paperId": "3937751dedfe07a21e3dadfdcdc04b5632b8e765",
            "title": "PromptAttack: Probing Dialogue State Trackers with Adversarial Prompts",
            "abstract": "A key component of modern conversational systems is the Dialogue State Tracker (or DST), which models a user's goals and needs. Toward building more robust and reliable DSTs, we introduce a prompt-based learning approach to automatically generate effective adversarial examples to probe DST models. Two key characteristics of this approach are: (i) it only needs the output of the DST with no need for model parameters, and (ii) it can learn to generate natural language utterances that can target any DST. Through experiments over state-of-the-art DSTs, the proposed framework leads to the greatest reduction in accuracy and the best attack success rate while maintaining good fluency and a low perturbation ratio. We also show how much the generated adversarial examples can bolster a DST through adversarial training. These results indicate the strength of prompt-based attacks on DSTs and leave open avenues for continued refinement.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a prompt-based learning approach to automatically generate effective adversarial examples to probe DST models, and shows how much the generated adversarialExamples can bolster a DST through adversarial training."
            },
            "score": 4
        },
        {
            "id": "cb8374f90c7b61f07b257377b4ce5e6d917c0df3",
            "paperId": "cb8374f90c7b61f07b257377b4ce5e6d917c0df3",
            "title": "Privacy-Preserving Models for Legal Natural Language Processing",
            "abstract": "Pre-training large transformer models with in-domain data improves domain adaptation and helps gain performance on the domain-specific downstream tasks. However, sharing models pre-trained on potentially sensitive data is prone to adversarial privacy attacks. In this paper, we asked to which extent we can guarantee privacy of pre-training data and, at the same time, achieve better downstream performance on legal tasks without the need of additional labeled data. We extensively experiment with scalable self-supervised learning of transformer models under the formal paradigm of differential privacy and show that under specific training configurations we can improve downstream performance without sacrifying privacy protection for the in-domain data. Our main contribution is utilizing differential privacy for large-scale pre-training of transformer language models in the legal NLP domain, which, to the best of our knowledge, has not been addressed before.",
            "year": 2022,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper extensively experiment with scalable self-supervised learning of transformer models under the formal paradigm of differential privacy and shows that under specific training configurations the authors can improve downstream performance without sacrifying privacy protection for the in-domain data."
            },
            "score": 4
        },
        {
            "id": "4c35ad10e0b72181bd94c8f73eb46fed5f379db0",
            "paperId": "4c35ad10e0b72181bd94c8f73eb46fed5f379db0",
            "title": "Encrypted Semantic Communication Using Adversarial Training for Privacy Preserving",
            "abstract": "Semantic communication is implemented based on shared background knowledge, but the sharing mechanism risks privacy leakage. In this letter, we propose an encrypted semantic communication system (ESCS) for privacy preserving, which combines universality and confidentiality. The universality is reflected in the fact that the structures of all network modules of the proposed ESCS are public, and the training database is shared, which is suitable for large-scale deployment in practical scenarios. Meanwhile, confidentiality is achieved through key encryption. Based on the adversarial training, we design an adversarial encryption training scheme to guarantee the accuracy of semantic communication in both encrypted and unencrypted modes. Experiment results show that the proposed ESCS with the adversarial encryption training scheme can perform well regardless of whether the semantic information is encrypted. It is difficult for the attacker to reconstruct the original semantic information from the eavesdropped message.",
            "year": 2022,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The proposed ESCS with the adversarial encryption training scheme can perform well regardless of whether the semantic information is encrypted, and it is difficult for the attacker to reconstruct the original semantic information from the eavesdropped message."
            },
            "score": 4
        },
        {
            "id": "fd2b3f811289526d131581f21d26da63c9075150",
            "paperId": "fd2b3f811289526d131581f21d26da63c9075150",
            "title": "Privacy-Preserving Adversarial Network (PPAN) for Continuous non-Gaussian Attributes",
            "abstract": "A privacy-preserving adversarial network (PPAN) was recently proposed as an information-theoretical framework to address the issue of privacy in data sharing. The main idea of this model was to use mutual information as the privacy measure and adversarial training of two deep neural networks, one as the mechanism and another as the adversary. The performance of the PPAN model for the discrete synthetic data, MNIST handwritten digits, and continuous Gaussian data was evaluated and compared to the analytically optimal trade-off. In this study, we evaluate the PPAN model for continuous non-Gaussian data, i.e. smart meters data, where lower and upper bounds of the privacy-preserving problem are used. These bounds include the Kraskov (KSG) estimation of entropy and mutual information that is based on the k-th nearest neighbor. In addition to the synthetic data sets, a practical case for hiding the actual electricity consumption from smart meter readings is examined. The results show that for continuous non-Gaussian data, the PPAN model performs within the determined optimal ranges and close to the lower bound.",
            "year": 2020,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study evaluates the PPAN model for continuous non-Gaussian data, i.e. smart meters data, where lower and upper bounds of the privacy-preserving problem are used and shows that the model performs within the determined optimal ranges and close to the lower bound."
            },
            "score": 4
        },
        {
            "id": "e001599815536dafeeeb0517e04c6fac3479a0d8",
            "paperId": "e001599815536dafeeeb0517e04c6fac3479a0d8",
            "title": "CATS: Conditional Adversarial Trajectory Synthesis for privacy-preserving trajectory data publication using deep learning approaches",
            "abstract": "Abstract The prevalence of ubiquitous location-aware devices and mobile Internet enables us to collect massive individual-level trajectory dataset from users. Such trajectory big data bring new opportunities to human mobility research but also raise public concerns with regard to location privacy. In this work, we present the Conditional Adversarial Trajectory Synthesis (CATS), a deep-learning-based GeoAI methodological framework for privacy-preserving trajectory data generation and publication. CATS applies K-anonymity to the underlying spatiotemporal distributions of human movements, which provides a distributional-level strong privacy guarantee. By leveraging conditional adversarial training on K-anonymized human mobility matrices, trajectory global context learning using the attention-based mechanism, and recurrent bipartite graph matching of adjacent trajectory points, CATS is able to reconstruct trajectory topology from conditionally sampled locations and generate high-quality individual-level synthetic trajectory data, which can serve as supplements or alternatives to raw data for privacy-preserving trajectory data publication. The experiment results on over 90k GPS trajectories show that our method has a better performance in privacy preservation, spatiotemporal characteristic preservation, and downstream utility compared with baseline methods, which brings new insights into privacy-preserving human mobility research using generative AI techniques and explores data ethics issues in GIScience.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Conditional Adversarial Trajectory Synthesis (CATS) is presented, a deep-learning-based GeoAI methodological framework for privacy-preserving trajectory data generation and publication that applies K-anonymity to the underlying spatiotemporal distributions of human movements, which provides a distributional-level strong privacy guarantee."
            },
            "score": 4
        },
        {
            "id": "ec16f5ab2c6433aa210a80857d4ee2d6300294b6",
            "paperId": "ec16f5ab2c6433aa210a80857d4ee2d6300294b6",
            "title": "OLYMPIA: A Simulation Framework for Evaluating the Concrete Scalability of Secure Aggregation Protocols",
            "abstract": "Recent secure aggregation protocols enable privacy-preserving federated learning for high-dimensional models among thousands or even millions of participants. Due to the scale of these use cases, however, end-to-end empirical evaluation of these protocols is impossible. We present OLYMPIA, a framework for empirical evaluation of secure protocols via simulation. OLYMPIA. provides an embedded domain-specific language for defining protocols, and a simulation framework for evaluating their performance. We implement several recent secure aggregation protocols using OLYMPIA, and perform the first empirical comparison of their end-to-end running times. We release OLYMPIA as open source.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work implements several recent secure aggregation protocols using OLYMPIA, and performs the first empirical comparison of their end-to-end running times, and releases OLYMPIA as open source."
            },
            "score": 4
        },
        {
            "id": "68149cf364452442d680f3ee7e5500575ba773f7",
            "paperId": "68149cf364452442d680f3ee7e5500575ba773f7",
            "title": "Blockchain-Based Federated Learning With Secure Aggregation in Trusted Execution Environment for Internet-of-Things",
            "abstract": "This article proposes a blockchain-based federated learning (FL) framework with Intel Software Guard Extension (SGX)-based trusted execution environment (TEE) to securely aggregate local models in Industrial Internet-of-Things (IIoTs). In FL, local models can be tampered with by attackers. Hence, a global model generated from the tampered local models can be erroneous. Therefore, the proposed framework leverages a blockchain network for secure model aggregation. Each blockchain node hosts an SGX-enabled processor that securely performs the FL-based aggregation tasks to generate a global model. Blockchain nodes can verify the authenticity of the aggregated model, run a blockchain consensus mechanism to ensure the integrity of the model, and add it to the distributed ledger for tamper-proof storage. Each cluster can obtain the aggregated model from the blockchain and verify its integrity before using it. We conducted several experiments with different CNN models and datasets to evaluate the performance of the proposed framework.",
            "year": 2023,
            "citationCount": 32,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A blockchain-based federated learning (FL) framework with Intel Software Guard Extension (SGX)-based trusted execution environment (TEE) to securely aggregate local models in Industrial Internet-of-Things (IIoTs) using blockchain nodes for secure model aggregation."
            },
            "score": 4
        },
        {
            "id": "b8edc389aa8566f289ba2524c0a835a413445384",
            "paperId": "b8edc389aa8566f289ba2524c0a835a413445384",
            "title": "SwiftAgg+: Achieving Asymptotically Optimal Communication Loads in Secure Aggregation for Federated Learning",
            "abstract": "We propose <monospace>SwiftAgg+</monospace>, a novel secure aggregation protocol for federated learning systems, where a central server aggregates local models of <inline-formula> <tex-math notation=\"LaTeX\">$N \\in \\mathbb {N}$ </tex-math></inline-formula> distributed users, each of size <inline-formula> <tex-math notation=\"LaTeX\">$L \\in \\mathbb {N}$ </tex-math></inline-formula>, trained on their local data, in a privacy-preserving manner. <monospace>SwiftAgg+</monospace> can significantly reduce the communication overheads without any compromise on security, and achieve optimal communication loads within diminishing gaps. Specifically, in presence of at most <inline-formula> <tex-math notation=\"LaTeX\">$D=o(N)$ </tex-math></inline-formula> dropout users, <monospace>SwiftAgg+</monospace> achieves a per-user communication load of <inline-formula> <tex-math notation=\"LaTeX\">$\\left({1+\\mathcal {O}\\left({\\frac {1}{N}}\\right)}\\right)L$ </tex-math></inline-formula> symbols and a server communication load of <inline-formula> <tex-math notation=\"LaTeX\">$\\left({1+\\mathcal {O}\\left({\\frac {1}{N}}\\right)}\\right)L$ </tex-math></inline-formula> symbols, with a worst-case information-theoretic security guarantee, against any subset of up to <inline-formula> <tex-math notation=\"LaTeX\">$T=o(N)$ </tex-math></inline-formula> semi-honest users who may also collude with the curious server. Moreover, the proposed <monospace>SwiftAgg+</monospace> allows for a flexible trade-off between communication loads and the number of active communication links. In particular, for <inline-formula> <tex-math notation=\"LaTeX\">$T< N-D$ </tex-math></inline-formula> and for any <inline-formula> <tex-math notation=\"LaTeX\">$K\\in \\mathbb {N}$ </tex-math></inline-formula>, <monospace>SwiftAgg+</monospace> can achieve the server communication load of <inline-formula> <tex-math notation=\"LaTeX\">$\\left({1+\\frac {T}{K}}\\right)L$ </tex-math></inline-formula> symbols, and per-user communication load of up to <inline-formula> <tex-math notation=\"LaTeX\">$\\left({1+\\frac {T+D}{K}}\\right)L$ </tex-math></inline-formula> symbols, where the number of pair-wise active connections in the network is <inline-formula> <tex-math notation=\"LaTeX\">$\\frac {N}{2}(K+T+D+1)$ </tex-math></inline-formula>.",
            "year": 2022,
            "citationCount": 29,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The proposed SwiftAgg+ protocol allows for a flexible trade-off between communication loads and the number of active communication links, and can significantly reduce the communication overheads without any compromise on security."
            },
            "score": 4
        },
        {
            "id": "03523b6841c8341061ad8d353926da16daa10955",
            "paperId": "03523b6841c8341061ad8d353926da16daa10955",
            "title": "SEAR: Secure and Efficient Aggregation for Byzantine-Robust Federated Learning",
            "abstract": "Federated learning facilitates the collaborative training of a global model among distributed clients without sharing their training data. Secure aggregation, a new security primitive for federated learning, aims to preserve the confidentiality of both local models and training data. Unfortunately, existing secure aggregation solutions fail to defend against Byzantine failures that are common in distributed computing systems. In this work, we propose a new secure and efficient aggregation framework, SEAR, for Byzantine-robust federated learning. Relying on the trusted execution environment, i.e., Intel SGX, SEAR protects clients\u2019 private models while enabling Byzantine resilience. Considering the limitation of the current Intel SGX's architecture (i.e., the limited trusted memory), we propose two data storage modes to efficiently implement aggregation algorithms efficiently in SGX. Moreover, to balance the efficiency and performance of aggregation, we propose a sampling-based method to efficiently detect Byzantine failures without degrading the global model's performance. We implement and evaluate SEAR in a LAN environment, and the experiment results show that SEAR is computationally efficient and robust to Byzantine adversaries. Compared to the previous practical secure aggregation framework, SEAR improves aggregation efficiency by 4-6 times while supporting Byzantine resilience at the same time.",
            "year": 2022,
            "citationCount": 45,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a new secure and efficient aggregation framework, SEAR, for Byzantine-robust federated learning, and improves aggregation efficiency by 4-6 times while supporting Byzantine resilience at the same time."
            },
            "score": 4
        },
        {
            "id": "0bd6272f2b30d0f19c339baa6f87e40fd486c374",
            "paperId": "0bd6272f2b30d0f19c339baa6f87e40fd486c374",
            "title": "Analysis of Application Examples of Differential Privacy in Deep Learning",
            "abstract": "Artificial Intelligence has been widely applied today, and the subsequent privacy leakage problems have also been paid attention to. Attacks such as model inference attacks on deep neural networks can easily extract user information from neural networks. Therefore, it is necessary to protect privacy in deep learning. Differential privacy, as a popular topic in privacy-preserving in recent years, which provides rigorous privacy guarantee, can also be used to preserve privacy in deep learning. Although many articles have proposed different methods to combine differential privacy and deep learning, there are no comprehensive papers to analyze and compare the differences and connections between these technologies. For this purpose, this paper is proposed to compare different differential private methods in deep learning. We comparatively analyze and classify several deep learning models under differential privacy. Meanwhile, we also pay attention to the application of differential privacy in Generative Adversarial Networks (GANs), comparing and analyzing these models. Finally, we summarize the application of differential privacy in deep neural networks.",
            "year": 2021,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper comparatively analyze and classify several deep learning models under differential privacy, and pays attention to the application of differential privacy in Generative Adversarial Networks (GANs), comparing and analyzing these models."
            },
            "score": 4
        },
        {
            "id": "0a00ea79dc73988ceba03cb9ce6c54901cac9bb4",
            "paperId": "0a00ea79dc73988ceba03cb9ce6c54901cac9bb4",
            "title": "Certified Robustness of Quantum Classifiers Against Adversarial Examples Through Quantum Noise",
            "abstract": "Recently, quantum classifiers have been known to be vulnerable to adversarial attacks, where quantum classifiers are fooled by imperceptible noises to have misclassification. In this paper, we propose one first theoretical study that utilizing the added quantum random rotation noise can improve the robustness of quantum classifiers against adversarial attacks. We connect the definition of differential privacy and demonstrate the quantum classifier trained with the natural presence of additive noise is differentially private. Lastly, we derive a certified robustness bound to enable quantum classifiers to defend against adversarial examples supported by experimental results.",
            "year": 2022,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The definition of differential privacy is connected and the quantum classifier trained with the natural presence of additive noise is differentially private and a certified robustness bound is derived to enable quantum classifiers to defend against adversarial examples supported by experimental results."
            },
            "score": 4
        },
        {
            "id": "1c1f55154378ffa32b7c8c10b1ada84ed3b06277",
            "paperId": "1c1f55154378ffa32b7c8c10b1ada84ed3b06277",
            "title": "Adversarial Classification Under Differential Privacy",
            "abstract": "\u2014The last decade has seen a growing interest in adversarial classi\ufb01cation , where an attacker tries to mislead a classi\ufb01er meant to detect anomalies. We study this problem in a setting where anomaly detection is being used in conjunction with differential privacy to protect personal information. We show that a strategic attacker can leverage the additional noise (introduced to ensure differential privacy) to mislead the classi\ufb01er beyond what the attacker could do otherwise; we also propose countermeasures against such attacks. We then evaluate the impact of our attacks and defenses in road traf\ufb01c congestion and smart metering examples",
            "year": 2020,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that a strategic attacker can leverage the additional noise (introduced to ensure differential privacy) to mislead the classi\ufb01er beyond what the attacker could do otherwise; this work proposes countermeasures against such attacks."
            },
            "score": 4
        },
        {
            "id": "995919eab1a61807f9eac0c3523e75766ab04bf8",
            "paperId": "995919eab1a61807f9eac0c3523e75766ab04bf8",
            "title": "RDP-GAN: A R\u00e9nyi-Differential Privacy Based Generative Adversarial Network",
            "abstract": "Generative adversarial networks (GANs) have attracted increasing attention recently owing to their impressive abilities to generate realistic samples with high privacy protection. Without directly interacting with training examples, the generative model can be used to estimate the underlying distribution of an original dataset while the discriminator can examine model quality of the generated samples by comparing the label values with training examples. In considering privacy issues in GANS, existing works focus on perturbing the parameters and analyzing the corresponding privacy protection capability, and the parameters are not directly exchanged between the generator and discriminator in GANs. Thus, in this work, we propose a R\u00e9nyi-differentially private-GAN (RDP-GAN), which achieves differential privacy (DP) in a GAN by carefully adding random Gaussian noise to the value of the exchanged loss function during training. Moreover, we derive analytical results characterizing the total privacy loss under the subsampling method and cumulative iterations, which show its effectiveness for the privacy budget allocation. In addition, in order to mitigate the negative impact of injecting noises, we enhance the proposed algorithm by adding an adaptive noise tuning step, which will change the amount of added noise according to the testing accuracy. Through extensive experimental results, we verify that the proposed algorithm can achieve a better privacy level while producing high-quality samples compared with a benchmark DP-GAN scheme based on noise perturbation on training gradients.",
            "year": 2020,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a R\u00e9nyi-differentially private-GAN (RDP-GAN), which achieves differential privacy (DP) in a GAN by carefully adding random Gaussian noise to the value of the exchanged loss function during training."
            },
            "score": 4
        },
        {
            "id": "857986936ac87e1fa7c0a9b2923908f6d0656fd2",
            "paperId": "857986936ac87e1fa7c0a9b2923908f6d0656fd2",
            "title": "MemGuard: Defending against Black-Box Membership Inference Attacks via Adversarial Examples",
            "abstract": "In a membership inference attack, an attacker aims to infer whether a data sample is in a target classifier's training dataset or not. Specifically, given a black-box access to the target classifier, the attacker trains a binary classifier, which takes a data sample's confidence score vector predicted by the target classifier as an input and predicts the data sample to be a member or non-member of the target classifier's training dataset. Membership inference attacks pose severe privacy and security threats to the training dataset. Most existing defenses leverage differential privacy when training the target classifier or regularize the training process of the target classifier. These defenses suffer from two key limitations: 1) they do not have formal utility-loss guarantees of the confidence score vectors, and 2) they achieve suboptimal privacy-utility tradeoffs. In this work, we propose MemGuard,the first defense with formal utility-loss guarantees against black-box membership inference attacks. Instead of tampering the training process of the target classifier, MemGuard adds noise to each confidence score vector predicted by the target classifier. Our key observation is that attacker uses a classifier to predict member or non-member and classifier is vulnerable to adversarial examples.Based on the observation, we propose to add a carefully crafted noise vector to a confidence score vector to turn it into an adversarial example that misleads the attacker's classifier. Specifically, MemGuard works in two phases. In Phase I, MemGuard finds a carefully crafted noise vector that can turn a confidence score vector into an adversarial example, which is likely to mislead the attacker's classifier to make a random guessing at member or non-member. We find such carefully crafted noise vector via a new method that we design to incorporate the unique utility-loss constraints on the noise vector. In Phase II, MemGuard adds the noise vector to the confidence score vector with a certain probability, which is selected to satisfy a given utility-loss budget on the confidence score vector. Our experimental results on three datasets show that MemGuard can effectively defend against membership inference attacks and achieve better privacy-utility tradeoffs than existing defenses. Our work is the first one to show that adversarial examples can be used as defensive mechanisms to defend against membership inference attacks.",
            "year": 2019,
            "citationCount": 301,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes MemGuard, the first defense with formal utility-loss guarantees against black-box membership inference attacks and is the first one to show that adversarial examples can be used as defensive mechanisms to defend against membership inference attack."
            },
            "score": 4
        },
        {
            "id": "e539c31d75f185d7c6226a01e76a46d5825ba3c5",
            "paperId": "e539c31d75f185d7c6226a01e76a46d5825ba3c5",
            "title": "A Privacy-Preserving Generative Adversarial Network Method for Securing EEG Brain Signals",
            "abstract": "Generative adversarial networks (GANs) have recently shown high success in applications such as image and time- series classification. However, those applications are vulnerable to complex hacking scenarios, for example, inference and data poisoning attacks, which would alter or infer sensitive information about systems and users. Protecting Electroencephalographic (EEG) brain signals against illegal disclosure has a great interest these days. In this paper, we propose a privacy-preserving GAN method to generate and classify EEG data effectively. Generating EEG data offers a range of capabilities, including sharing experimental data without infringing user privacy, improving machine learning models for brain-computer interface tasks and restore corrupted data. The proposed GAN model is trained under a differential privacy model to enhance the data privacy level by limiting queries of data from artificial trials that could identify the real participants from their EEG signals. The performance of the proposed method was evaluated using a motor imagery classification task, where real EEG data are augmented with artificially generated samples for training machine learning classifiers. The evaluation was performed on a benchmark EEG data set for nine subjects. The experimental outcomes revealed that the non-private version of the proposed approach could produce high-quality data that significantly improve the motor imagery classification performance. The private version showed lower but comparable performance to the standard models trained on real data only.",
            "year": 2020,
            "citationCount": 20,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A privacy-preserving GAN method to generate and classify EEG data effectively and is trained under a differential privacy model to enhance the data privacy level by limiting queries of data from artificial trials that could identify the real participants from their EEG signals."
            },
            "score": 4
        },
        {
            "id": "9b78140aa26f738a7a225d505ad34023ff3b6d61",
            "paperId": "9b78140aa26f738a7a225d505ad34023ff3b6d61",
            "title": "Fair NLP Models with Differentially Private Text Encoders",
            "abstract": "Encoded text representations often capture sensitive attributes about individuals (e.g., race or gender), which raise privacy concerns and can make downstream models unfair to certain groups. In this work, we propose FEDERATE, an approach that combines ideas from differential privacy and adversarial training to learn private text representations which also induces fairer models. We empirically evaluate the trade-off between the privacy of the representations and the fairness and accuracy of the downstream model on four NLP datasets. Our results show that FEDERATE consistently improves upon previous methods, and thus suggest that privacy and fairness can positively reinforce each other.",
            "year": 2022,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes FEDERATE, an approach that combines ideas from differential privacy and adversarial training to learn private text representations which also induces fairer models, and empirically evaluates the trade-off between the privacy of the representations and the fairness and accuracy of the downstream model on four NLP datasets."
            },
            "score": 4
        },
        {
            "id": "5bdaadb84db0cbf72aaebda9f55f4288b63c6e9b",
            "paperId": "5bdaadb84db0cbf72aaebda9f55f4288b63c6e9b",
            "title": "Image Hijacks: Adversarial Images can Control Generative Models at Runtime",
            "abstract": "Are foundation models secure against malicious actors? In this work, we focus on the image input to a vision-language model (VLM). We discover image hijacks, adversarial images that control the behaviour of VLMs at inference time, and introduce the general Behaviour Matching algorithm for training image hijacks. From this, we derive the Prompt Matching method, allowing us to train hijacks matching the behaviour of an arbitrary user-defined text prompt (e.g. 'the Eiffel Tower is now located in Rome') using a generic, off-the-shelf dataset unrelated to our choice of prompt. We use Behaviour Matching to craft hijacks for four types of attack, forcing VLMs to generate outputs of the adversary's choice, leak information from their context window, override their safety training, and believe false statements. We study these attacks against LLaVA, a state-of-the-art VLM based on CLIP and LLaMA-2, and find that all attack types achieve a success rate of over 80%. Moreover, our attacks are automated and require only small image perturbations.",
            "year": 2023,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work discovers image hijacks, adversarial images that control the behaviour of VLMs at inference time, and introduces the general Behaviour Matching algorithm for training image hijacks, forcing VLMs to generate outputs of the adversary's choice."
            },
            "score": 3
        },
        {
            "id": "3955a47d6265e7e346f6873b2223fa80200a205c",
            "paperId": "3955a47d6265e7e346f6873b2223fa80200a205c",
            "title": "Fast Certification of Vision-Language Models Using Incremental Randomized Smoothing",
            "abstract": "A key benefit of deep vision-language models such as CLIP is that they enable zero-shot open vocabulary classification; the user has the ability to define novel class labels via natural language prompts at inference time. However, while CLIP-based zero-shot classifiers have demonstrated competitive performance across a range of domain shifts, they remain highly vulnerable to adversarial attacks. Therefore, ensuring the robustness of such models is crucial for their reliable deployment in the wild. In this work, we introduce Open Vocabulary Certification (OVC), a fast certification method designed for open-vocabulary models like CLIP via randomized smoothing techniques. Given a base\"training\"set of prompts and their corresponding certified CLIP classifiers, OVC relies on the observation that a classifier with a novel prompt can be viewed as a perturbed version of nearby classifiers in the base training set. Therefore, OVC can rapidly certify the novel classifier using a variation of incremental randomized smoothing. By using a caching trick, we achieve approximately two orders of magnitude acceleration in the certification process for novel prompts. To achieve further (heuristic) speedups, OVC approximates the embedding space at a given input using a multivariate normal distribution bypassing the need for sampling via forward passes through the vision backbone. We demonstrate the effectiveness of OVC on through experimental evaluation using multiple vision-language backbones on the CIFAR-10 and ImageNet test datasets.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces Open Vocabulary Certification (OVC), a fast certification method designed for open-vocabulary models like CLIP via randomized smoothing techniques, and demonstrates the effectiveness of OVC on through experimental evaluation using multiple vision-language backbones on the CIFAR-10 and ImageNet test datasets."
            },
            "score": 3
        },
        {
            "id": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
            "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
            "title": "Training language models to follow instructions with human feedback",
            "abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",
            "year": 2022,
            "citationCount": 5935,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent and showing improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets."
            },
            "score": 3
        },
        {
            "id": "fa1b738e71899eaa66688fa88cdb2d59b9bc5012",
            "paperId": "fa1b738e71899eaa66688fa88cdb2d59b9bc5012",
            "title": "Invertible Privacy-Preserving Adversarial Reconstruction for Image Compressed Sensing",
            "abstract": "Since the advent of compressed sensing (CS), many reconstruction algorithms have been proposed, most of which are devoted to reconstructing images with better visual quality. However, higher-quality images tend to reveal more sensitive information in machine recognition tasks. In this paper, we propose a novel invertible privacy-preserving adversarial reconstruction method for image CS. While optimizing the quality, the reconstructed images are made to be adversarial samples at the moment of generation. For semi-authorized users, they can only obtain the adversarial reconstructed images, which provide little information for machine recognition or training deep models. For authorized users, they can reverse adversarial reconstructed images to clean samples with an additional restoration network. Experimental results show that while keeping good visual quality for both types of reconstructed images, the proposed scheme can provide semi-authorized users with adversarial reconstructed images with a very low recognizable rate, and allow authorized users to further recover sanitized reconstructed images with recognition performance approximating that of the traditional CS.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results show that while keeping good visual quality for both types of reconstructed images, the proposed scheme can provide semi-authorized users with adversarial reconstructed images with a very low recognizable rate, and allow authorized users to further recover sanitized reconstructed image with recognition performance approximating that of the traditional CS."
            },
            "score": 3
        },
        {
            "id": "0347a2963ac6884f8e9680ab7d7cafa70e624a6f",
            "paperId": "0347a2963ac6884f8e9680ab7d7cafa70e624a6f",
            "title": "Homomorphic Encryption and Federated Learning based Privacy-Preserving CNN Training: COVID-19 Detection Use-Case",
            "abstract": "Medical data is often highly sensitive in terms of data privacy and security concerns. Federated learning, one type of machine learning techniques, has been started to use for the improvement of the privacy and security of medical data. In the federated learning, the training data is distributed across multiple machines, and the learning process is performed in a collaborative manner. There are several privacy attacks on deep learning (DL) models to get the sensitive information by attackers. Therefore, the DL model itself should be protected from the adversarial attack, especially for applications using medical data. One of the solutions for this problem is homomorphic encryption-based model protection from the adversary collaborator. This paper proposes a privacy-preserving federated learning algorithm for medical data using homomorphic encryption. The proposed algorithm uses a secure multi-party computation protocol to protect the deep learning model from the adversaries. In this study, the proposed algorithm using a real-world medical dataset is evaluated in terms of the model performance.",
            "year": 2022,
            "citationCount": 43,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a privacy-preserving federated learning algorithm for medical data using homomorphic encryption that uses a secure multi-party computation protocol to protect the deep learning model from the adversaries."
            },
            "score": 3
        },
        {
            "id": "336ec18ef28a6b4819865c37bb114deee2fdb458",
            "paperId": "336ec18ef28a6b4819865c37bb114deee2fdb458",
            "title": "Security and Privacy in Machine Learning for Health Systems: Strategies and Challenges",
            "abstract": "Summary Objectives : Machine learning (ML) is a powerful asset to support physicians in decision-making procedures, providing timely answers. However, ML for health systems can suffer from security attacks and privacy violations. This paper investigates studies of security and privacy in ML for health. Methods : We examine attacks, defenses, and privacy-preserving strategies, discussing their challenges. We conducted the following research protocol: starting a manual search, defining the search string, removing duplicated papers, filtering papers by title and abstract, then their full texts, and analyzing their contributions, including strategies and challenges. Finally, we collected and discussed 40 papers on attacks, defense, and privacy. Results : Our findings identified the most employed strategies for each domain. We found trends in attacks, including universal adversarial perturbation (UAPs), generative adversarial network (GAN)-based attacks, and DeepFakes to generate malicious examples. Trends in defense are adversarial training, GAN-based strategies, and out-of-distribution (OOD) to identify and mitigate adversarial examples (AE). We found privacy-preserving strategies such as federated learning (FL), differential privacy, and combinations of strategies to enhance the FL. Challenges in privacy comprehend the development of attacks that bypass fine-tuning, defenses to calibrate models to improve their robustness, and privacy methods to enhance the FL strategy. Conclusions : In conclusion, it is critical to explore security and privacy in ML for health, because it has grown risks and open vulnerabilities. Our study presents strategies and challenges to guide research to investigate issues about security and privacy in ML applied to health systems.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is critical to explore security and privacy in ML for health, because it has grown risks and open vulnerabilities, and this study presents strategies and challenges to guide research to investigate issues about security and privacy in ML applied to health systems."
            },
            "score": 3
        },
        {
            "id": "01936118dcbe2bb5eff801785c90de7d8c69860b",
            "paperId": "01936118dcbe2bb5eff801785c90de7d8c69860b",
            "title": "Differential Privacy in a Bayesian setting through posterior sampling",
            "abstract": "We examine the robustness and privacy properties of Bayesian inference, under assumptions on the prior. With no modifications to the Bayesian framework, we show that a simple posterior sampling algorithm results in uniform utility and privacy guarantees. In more detail, we generalise the concept of differential privacy to arbitrary dataset distances, outcome spaces and distribution families. We then prove bounds on the robustness of the posterior, introduce a posterior sampling mechanism, show that it is differentially private and provide finite sample bounds for distinguishability-based privacy under a strong adversarial model. Finally, we give examples satisfying our assumptions.",
            "year": 2013,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Borders on the robustness of the posterior are proved, a posterior sampling mechanism is introduced, it is shown that it is differentially private and finite sample bounds for distinguishability-based privacy under a strong adversarial model are provided."
            },
            "score": 3
        },
        {
            "id": "5c8bbc0586468b766b32991c398aa29ddbdf2a46",
            "paperId": "5c8bbc0586468b766b32991c398aa29ddbdf2a46",
            "title": "Provenance of Training without Training Data: Towards Privacy-Preserving DNN Model Ownership Verification",
            "abstract": "In the era of deep learning, it is critical to protect the intellectual property of high-performance deep neural network (DNN) models. Existing proposals, however, are subject to adversarial ownership forgery (e.g., methods based on watermarks or fingerprints) or require full access to the original training dataset for ownership verification (e.g., methods requiring the replay of the learning process). In this paper, we propose a novel Provenance of Training (PoT) scheme, the first empirical study towards verifying DNN model ownership without accessing any original dataset while being robust against existing attacks. At its core, PoT relies on a coherent model chain built from the intermediate checkpoints saved during model training to serve as the ownership certificate. Through an in-depth analysis of model training, we propose six key properties that a legitimate model chain shall naturally hold. In contrast, it is difficult for the adversary to forge a model chain that satisfies these properties simultaneously without performing actual training. We systematically analyze PoT\u2019s robustness against various possible attacks, including the adaptive attacks that are designed given the full knowledge of PoT\u2019s design, and further perform extensive empirical experiments to demonstrate our security analysis.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel Provenance of Training (PoT) scheme, the first empirical study towards verifying DNN model ownership without accessing any original dataset while being robust against existing attacks."
            },
            "score": 2
        },
        {
            "id": "457fcaa1945ab28b2fde6956bc79e8d7f750dba5",
            "paperId": "457fcaa1945ab28b2fde6956bc79e8d7f750dba5",
            "title": "Appendix for \"Differentially Private SGDA for Minimax Problems\" A Motivating Examples",
            "abstract": "where f = (1 \u2212 p)(h(\u03b8;x) \u2212 a)I[y = 1] + p(h(\u03b8;x) \u2212 b)I[y = \u22121] + 2(1 + v)(ph(\u03b8;x)I[y = \u22121] \u2212 (1 \u2212 p)h(\u03b8;x)I[y = 1])] \u2212 p(1 \u2212 p)v and p = P[y = 1]. Such problem is (non)convex-concave. In particular, Liu et al. [2020] showed that when h is a one hidden layer neural network the objective f satisfies the Polyak-\u0141ojasiewicz condition. Differential privacy has been applied to learn private classifier by optimizing AUC [Wang et al., 2021]. The proposed privacy mechanisms there are objective perturbation and output perturbation. Generative Adversarial Networks (GANs). GAN is introduced in Goodfellow et al. [2014] which can be regarded as a game between a generator network Gv and a discriminator network Dw. The generator network produces synthetic data from random noise \u03be, while the discriminator network discriminates between the true data and the synthetic data. In particular, a popular variant of GAN named as WGAN [Arjovsky et al., 2017] can be written as a minimax problem",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Differential privacy has been applied to learn private classifier by optimizing AUC by satisfying the Polyak-\u0141ojasiewicz condition and a popular variant of GAN named as WGAN can be written as a minimax problem."
            },
            "score": 2
        }
    ],
    "novelty": "yes"
}