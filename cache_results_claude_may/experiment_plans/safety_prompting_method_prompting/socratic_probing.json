{
    "topic_description": "novel prompting methods to improve large language models' robustness against adversarial attacks and improve their security and privacy",
    "idea_name": "Socratic Probing",
    "raw_idea": {
        "Problem": "Language models can be biased to generate overconfident or unsupported claims when prompted with leading questions or statements.",
        "Existing Methods": "Existing methods to mitigate this bias include calibrating model probabilities, generating diverse outputs, and using external knowledge to fact-check claims.",
        "Motivation": "We propose a prompting strategy inspired by the Socratic method, where the model is prompted to critically examine and justify its own outputs through a series of probing questions.",
        "Proposed Method": "Socratic Probing (SP) consists of the following steps: 1) Initial Claim Generation: The language model is prompted with the original input to generate an initial output claim. 2) Probing Question Generation: The model is then prompted to generate a series of probing questions that challenge or cast doubt on the initial claim, such as asking for evidence, considering counterarguments, or exploring implications. 3) Claim Justification: For each probing question, the model is prompted to generate a response that attempts to justify the initial claim in light of the challenge posed by the question. 4) Confidence Scoring: Based on the quality and convincingness of the justifications, the model is prompted to generate a confidence score for the initial claim. 5) Claim Revision: If the confidence score is below a threshold, the model is prompted to revise the initial claim to be more nuanced, qualified, or uncertain as appropriate.",
        "Experiment Plan": "We will evaluate SP on datasets of prompts known to elicit overconfident or unsupported claims from language models, comparing the calibration and justifiability of the outputs to those of baseline models. We will use both automated metrics of claim quality and human evaluations of output trustworthiness. We will also test the ability of SP to generalize to new domains and prompt styles."
    },
    "full_experiment_plan": {
        "Title": "Socratic Probing: Prompting Language Models to Critically Examine and Justify Their Outputs",
        "Problem Statement": "Language models can be biased to generate overconfident or unsupported claims when prompted with leading questions or statements. Existing methods to mitigate this bias, such as calibrating model probabilities, generating diverse outputs, and using external knowledge to fact-check claims, still face challenges in effectively reducing unsupported claims.",
        "Motivation": "Our proposed method, Socratic Probing (SP), is inspired by the Socratic method, where a series of probing questions are used to critically examine and justify claims. By prompting the language model to generate and respond to such probing questions about its own outputs, we aim to encourage the model to produce more nuanced, qualified, and justified claims compared to existing methods. SP leverages the model's own knowledge and reasoning capabilities to self-examine and self-correct, without relying on external knowledge or supervision.",
        "Proposed Method": {
            "Step 1: Initial Claim Generation": "The language model is prompted with the original input to generate an initial output claim.",
            "Step 2: Probing Question Generation": "The model is then prompted to generate a series of probing questions that challenge or cast doubt on the initial claim, such as asking for evidence, considering counterarguments, or exploring implications.",
            "Step 3: Claim Justification": "For each probing question, the model is prompted to generate a response that attempts to justify the initial claim in light of the challenge posed by the question.",
            "Step 4: Confidence Scoring": "Based on the quality and convincingness of the justifications, the model is prompted to generate a confidence score for the initial claim.",
            "Step 5: Claim Revision": "If the confidence score is below a threshold, the model is prompted to revise the initial claim to be more nuanced, qualified, or uncertain as appropriate."
        },
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "Evaluate SP on datasets known to elicit overconfident or unsupported claims from language models, such as the Stances dataset (which contains stance prediction tasks on controversial topics), the ArgKP dataset (which contains arguments on controversial topics), and a dataset of leading yes/no questions on factual and subjective topics.",
            "Step 2: Construct Prompts": "For each dataset, construct initial prompts that ask the model to make a claim or take a stance on the given topic. Then, construct prompts for each step of the SP method:\n1. Initial Claim Generation: Prepend the initial prompt to \"Make a claim about the following topic: [topic]\".\n2. Probing Question Generation: Prepend the initial claim to \"Generate 3 probing questions that challenge or cast doubt on the above claim, focusing on asking for evidence, considering counterarguments, or exploring implications.\"\n3. Claim Justification: For each probing question, prepend it to the initial claim, and prompt the model with \"Justify the initial claim in light of the above probing question.\"\n4. Confidence Scoring: Prepend the initial claim and the justifications to \"Based on the quality and convincingness of the above justifications, generate a confidence score between 0 and 1 for the initial claim.\"\n5. Claim Revision: If the confidence score is below 0.7, prepend the initial claim and the probing questions and justifications to \"Revise the initial claim to be more nuanced, qualified, or uncertain as appropriate.\"",
            "Step 3: Select Models": "Evaluate SP on GPT-3.5 (text-davinci-002), GPT-4, and Claude. Also compare to versions of these models that have been fine-tuned on the datasets.",
            "Step 4: Run Experiments": "For each combination of dataset, model, and prompting method (baseline vs. SP), generate outputs for 100-500 examples. For SP, generate the initial claim, probing questions, justifications, confidence scores, and revised claims.",
            "Step 5: Evaluate Results": "Evaluate the generated claims (both initial and revised) using both automated metrics and human evaluation:\n1. Automated metrics: (a) Measure the average confidence score. (b) Measure the proportion of claims that contain uncertainty markers (e.g., \"may\", \"might\", \"possibly\", \"likely\"). (c) Measure the length and complexity of claims. \n2. Human evaluation: Ask human raters to score each claim on the following criteria: (a) Factual accuracy. (b) Justifiability based on the given evidence. (c) Appropriateness of confidence/certainty level. Also ask raters to compare the initial and revised claims from SP and indicate whether the revised claim is better, worse, or the same.\nCompare the performance of SP vs. the baseline prompting method, as well as the impact of model type and fine-tuning."
        },
        "Test Case Examples": {
            "Test Case 1": {
                "Initial Prompt": "Do vaccines cause autism?",
                "Baseline Output": "No, there is no scientific evidence that vaccines cause autism. This claim has been thoroughly debunked.",
                "SP Initial Claim": "There is no strong evidence that vaccines cause autism.",
                "SP Probing Questions": "1. What studies have investigated the link between vaccines and autism? 2. Are there any potential biological mechanisms by which vaccines could plausibly cause autism? 3. How do major medical organizations and expert consensus statements view the vaccine-autism link?",
                "SP Justifications": "1. Multiple large, well-controlled studies have failed to find any link between vaccines and autism. 2. There is no known plausible biological mechanism for vaccines to cause autism. 3. Medical organizations like the CDC, WHO, and AAP have strongly rejected any causal link between vaccines and autism based on the scientific evidence.",
                "SP Confidence Score": "0.95",
                "SP Revised Claim": "The scientific evidence strongly suggests that vaccines do not cause autism, though a small number of studies argue for a potential link that warrants further research to definitively rule out."
            },
            "Test Case 2": {
                "Initial Prompt": "Are GMOs harmful to human health?",
                "Baseline Output": "GMOs are safe for human consumption and there is no evidence that they are harmful to human health.",
                "SP Initial Claim": "There is no clear evidence that GMOs as a category are harmful to human health.",
                "SP Probing Questions": "1. Have any studies found health risks associated with certain GM foods? 2. Are there potential long-term effects of GMOs that have not been fully studied? 3. Why do some countries restrict or ban GMOs?",
                "SP Justifications": "1. A few studies on animals have suggested possible health risks of certain GM foods, but these have not been replicated in humans. 2. Long-term epidemiological studies on populations that consume GMOs have not found negative health effects, but some argue that more long-term research is still needed. 3. Some countries restrict GMOs due to precautionary principles and socioeconomic rather than health concerns.",
                "SP Confidence Score": "0.8",
                "SP Revised Claim": "While the balance of evidence suggests that approved GMOs are safe for human consumption, a few studies point to potential health risks of certain GM foods that warrant further research. Some uncertainty remains about long-term effects. Restrictions on GMOs in some countries reflect precautionary and socioeconomic stances rather than clear health dangers."
            }
        },
        "Fallback Plan": "If the SP method does not show a significant improvement over the baseline in reducing overconfident or unsupported claims, we can perform additional analyses and ablations to understand why:\n1. Analyze the quality and diversity of the generated probing questions to see if they are sufficiently challenging the initial claims. Experiment with alternative prompts or few-shot examples to improve the probing question generation.\n2. Analyze the justifications to see if they are adequately addressing the probing questions and providing convincing evidence or reasoning. Experiment with prompts that encourage more comprehensive and specific justifications.\n3. Analyze the confidence scores to see if they are well-calibrated with the quality of the justifications. Experiment with different prompts or thresholds for generating and interpreting the confidence scores.\n4. Analyze the revised claims to see if they are actually more nuanced and qualified than the initial claims. Experiment with prompts that encourage more significant revisions when confidence is low.\nIf the SP method still underperforms after these modifications, we can pivot to an analysis of why the method fails and what this reveals about the limitations of language models in self-verification and self-correction. We can also explore alternative approaches, such as using the probing questions and justifications to train a separate classifier to detect overconfident claims, or using the probing questions as part of a human-in-the-loop system for fact-checking and moderating model outputs."
    },
    "novelty_queries": [
        "KeywordQuery(\"language models socratic method\")",
        "KeywordQuery(\"language models self-verification\")",
        "KeywordQuery(\"language models probing questions justifications\")",
        "KeywordQuery(\"language models unsupported claims\")",
        "KeywordQuery(\"Socratic Probing NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "12dc76b5f04fb1fc34f94f26632d193305a97e7e",
            "paperId": "12dc76b5f04fb1fc34f94f26632d193305a97e7e",
            "title": "SocREval: Large Language Models with the Socratic Method for Reference-Free Reasoning Evaluation",
            "abstract": "To comprehensively gauge the capacity of current models for complex reasoning, it is crucial to assess their step-by-step reasoning in a scalable manner. Established reference-based evaluation metrics rely on human-annotated reasoning chains as references to assess the model-derived chains. However, such\"gold-standard\"human-written reasoning chains may not be unique and their acquisition is often labor-intensive. Existing reference-free reasoning evaluation metrics, while eliminating the need for human-crafted reasoning chains as references, often require fine-tuning with human-derived chains before evaluation, complicating the process and questioning their adaptability to other datasets. To address these challenges, we harness GPT-4 to automatically evaluate reasoning chain quality, thereby removing the dependency on human-written reasoning chains for both model fine-tuning and evaluative purposes. Leveraging the Socratic method, we develop SocREval ({\\bf Soc}ratic Method-Inspired {\\bf R}easoning {\\bf Eval}uation), a novel approach for prompt design in reference-free reasoning evaluation. Empirical results from four human annotated datasets reveal that SocREval significantly improves GPT-4's performance, surpassing existing reference-free and reference-based reasoning evaluation metrics. Beyond its demonstrated efficacy, SocREval, proves to be both cost-efficient and robust to prompt writing and example selection, as substantiated by our in-depth analysis.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "SocREval, a novel approach for prompt design in reference-free reasoning evaluation, harnesses GPT-4 to automatically evaluate reasoning chain quality, thereby removing the dependency on human-written reasoning chains for both model fine-tuning and evaluative purposes."
            },
            "score": 8,
            "novelty_score": "The project proposal aims to reduce overconfident and unsupported claims generated by language models by using a Socratic probing method to prompt the model to critically examine and justify its own outputs.\n\nThe paper abstract proposes SocREval, a reference-free reasoning evaluation approach that uses GPT-4 and the Socratic method to automatically evaluate the quality of reasoning chains generated by language models.\n\nWhile both the project proposal and the paper abstract involve using the Socratic method with language models, the project focuses on reducing unsupported claims in generated outputs, while the paper focuses on evaluating the quality of reasoning chains. The research problems and goals are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "69335077fcacbff7a7cf25697da1949e6bdfa968",
            "paperId": "69335077fcacbff7a7cf25697da1949e6bdfa968",
            "title": "The Art of SOCRATIC QUESTIONING: Recursive Thinking with Large Language Models",
            "abstract": "Chain-of-Thought (CoT) prompting enables large language models to solve complex reasoning problems by generating intermediate steps. However, confined by its inherent single-pass and sequential generation process, CoT heavily relies on the initial decisions, causing errors in early steps to accumulate and impact the final answers. In contrast, humans adopt recursive thinking when tackling complex reasoning problems, i.e., iteratively breaking the original problem into approachable sub-problems and aggregating their answers to resolve the original one. Inspired by the human cognitive process, we propose SOCRATIC QUESTIONING, a divide-and-conquer style algorithm that mimics the recursive thinking process. Specifically, SOCRATIC QUESTIONING leverages large language models to raise and answer sub-questions until collecting enough information to tackle the original question. Unlike CoT, SOCRATIC QUESTIONING explicitly navigates the thinking space, stimulates effective recursive thinking, and is more robust towards errors in the thinking process. Extensive experiments on several complex reasoning tasks, including MMLU, MATH, LogiQA, and visual question-answering demonstrate significant performance improvements over the state-of-the-art prompting methods, such as CoT, and Tree-of-Thought. The qualitative analysis clearly shows that the intermediate reasoning steps elicited by SOCRATIC QUESTIONING are similar to humans' recursively thinking process of complex reasoning problems.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The qualitative analysis clearly shows that the intermediate reasoning steps elicited by SOCRATIC QUESTIONING are similar to humans' recursively thinking process of complex reasoning problems."
            },
            "score": 8,
            "novelty_score": "The project proposal aims to reduce overconfident and unsupported claims generated by language models using a method called Socratic Probing, which prompts the model to generate probing questions, justify its claims, and revise them based on the justifications. The paper proposes a method called Socratic Questioning, which leverages language models to iteratively break down complex reasoning problems into sub-questions and aggregate their answers to solve the original problem.\n\nWhile both methods are inspired by the Socratic method of questioning, the project proposal focuses on reducing biases in language model outputs, while the paper focuses on improving complex reasoning performance. The project proposal uses probing questions to encourage the model to self-examine and self-correct its claims, while the paper uses sub-questions to break down complex problems into smaller, approachable ones.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "d7386e8859b22e05ce9c4a972613d4b1e1e44198",
            "paperId": "d7386e8859b22e05ce9c4a972613d4b1e1e44198",
            "title": "Prompting Large Language Models With the Socratic Method",
            "abstract": "This paper presents a systematic approach to using the Socratic method in developing prompt templates that effectively interact with large language models, including GPT-3. Various methods are examined, and those that yield precise answers and justifications while fostering creativity and imagination to enhance creative writing are identified. Techniques such as definition, elenchus, dialectic, maieutics, generalization, and counterfactual reasoning are discussed for their application in engineering prompt templates and their connections to inductive, deductive, and abductive reasoning. Through examples, the effectiveness of these dialogue and reasoning methods is demonstrated. An interesting observation is made that when the task's goal and user intent are conveyed to GPT-3 via ChatGPT before the start of a dialogue, the large language model seems to connect to the external context expressed in the intent and perform more effectively.",
            "year": 2023,
            "citationCount": 20,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An interesting observation is made that when the task's goal and user intent are conveyed to GPT-3 via ChatGPT before the start of a dialogue, the large language model seems to connect to the external context expressed in the intent and perform more effectively."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is reducing overconfident and unsupported claims generated by language models, and the proposed approach is to use a series of probing questions to prompt the model to critically examine and justify its outputs (Socratic Probing).\n\nThe research problem in the paper is developing effective prompt templates for interacting with large language models, and the proposed approach is to use various Socratic methods such as definition, elenchus, dialectic, maieutics, generalization, and counterfactual reasoning.\n\nWhile both the proposal and the paper involve using the Socratic method to prompt language models, the specific research problems and approaches are different. The proposal focuses on reducing model biases in claim generation, while the paper focuses on developing effective prompt templates for general interaction with language models.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "6fd7184bc15bf694902c0e465692e5bec533f69f",
            "paperId": "6fd7184bc15bf694902c0e465692e5bec533f69f",
            "title": "Improving Socratic Question Generation using Data Augmentation and Preference Optimization",
            "abstract": "The Socratic method is a way of guiding students toward solving a problem independently without directly revealing the solution to the problem. Although this method has been shown to significantly improve student learning outcomes, it remains a complex labor-intensive task for instructors. Large language models (LLMs) can be used to augment human effort by automatically generating Socratic questions for students. However, existing methods that involve prompting these LLMs sometimes produce invalid outputs, e.g., those that directly reveal the solution to the problem or provide irrelevant or premature questions. To alleviate this problem, inspired by reinforcement learning with AI feedback (RLAIF), we first propose a data augmentation method to enrich existing Socratic questioning datasets with questions that are invalid in specific ways. Next, we propose a method to optimize open-source LLMs such as LLama 2 to prefer ground-truth questions over generated invalid ones, using direct preference optimization (DPO). Our experiments on a Socratic questions dataset for student code debugging show that a DPO-optimized 7B LLama 2 model can effectively avoid generating invalid questions, and as a result, outperforms existing state-of-the-art prompting methods.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A data augmentation method to enrich existing Socratic questioning datasets with questions that are invalid in specific ways and a method to optimize open-source LLMs such as LLama 2 to prefer ground-truth questions over generated invalid ones, using direct preference optimization (DPO)."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to reduce overconfident and unsupported claims generated by language models by using a Socratic probing method to prompt the model to self-examine and self-correct its outputs. The paper focuses on improving the quality of automatically generated Socratic questions for student code debugging by using data augmentation and preference optimization techniques.\n\nThe project proposal and the paper have different research problems and approaches:\n- The project proposal tackles the issue of language models generating biased, overconfident, or unsupported claims, while the paper addresses the problem of invalid Socratic questions generated by language models for student code debugging.\n- The project proposal uses a Socratic probing method to prompt the model to critically examine and justify its own outputs, while the paper uses data augmentation and preference optimization to improve the quality of generated Socratic questions.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "66242baf48b0f6b828e7547ac39ffaa5e1b2cb3e",
            "paperId": "66242baf48b0f6b828e7547ac39ffaa5e1b2cb3e",
            "title": "RARR: Researching and Revising What Language Models Say, Using Language Models",
            "abstract": "Language models (LMs) now excel at many tasks such as question answering, reasoning, and dialog. However, they sometimes generate unsupported or misleading content. A user cannot easily determine whether their outputs are trustworthy or not, because most LMs do not have any built-in mechanism for attribution to external evidence. To enable attribution while still preserving all the powerful advantages of recent generation models, we propose RARR (Retrofit Attribution using Research and Revision), a system that 1) automatically finds attribution for the output of any text generation model, and 2) post-edits the output to fix unsupported content while preserving the original output as much as possible. When applied to the output of several state-of-the-art LMs on a diverse set of generation tasks, we find that RARR significantly improves attribution while otherwise preserving the original input to a much greater degree than previously explored edit models. Furthermore, the implementation of RARR requires only a handful of training examples, a large language model, and standard web search.",
            "year": 2022,
            "citationCount": 110,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes RARR (Retrofit Attribution using Research and Revision), a system that automatically finds attribution for the output of any text generation model, and post-edits the output to fix unsupported content while preserving the original output as much as possible."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to reduce unsupported claims generated by language models by using a series of probing questions to encourage the model to critically examine and justify its outputs. The paper proposes to find attribution for the output of any text generation model and post-edit the output to fix unsupported content while preserving the original output as much as possible.\n\nThe project focuses on using probing questions to make the model generate more nuanced and justified claims, while the paper focuses on finding external evidence to support the model's output and revising the output based on the evidence. Although both aim to improve the trustworthiness of language model outputs, their approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "287d45f2090adba6ed8fc4834a029cd47373a88e",
            "paperId": "287d45f2090adba6ed8fc4834a029cd47373a88e",
            "title": "Can Language Models Employ the Socratic Method? Experiments with Code Debugging",
            "abstract": "When employing the Socratic method of teaching, instructors guide students toward solving a problem on their own rather than providing the solution directly. While this strategy can substantially improve learning outcomes, it is usually time-consuming and cognitively demanding. Automated Socratic conversational agents can augment human instruction and provide the necessary scale, however their development is hampered by the lack of suitable data for training and evaluation. In this paper, we introduce a manually created dataset of multi-turn Socratic advice that is aimed at helping a novice programmer fix buggy solutions to simple computational problems. The dataset is then used for benchmarking the Socratic debugging abilities of a number of language models, ranging from fine-tuning the instruction-based text-to-text transformer Flan-T5 to zero-shot and chain of thought prompting of the much larger GPT-4. The code and datasets are made freely available for research at the link below. https://github.com/taisazero/socratic-debugging-benchmark",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A manually created dataset of multi-turn Socratic advice aimed at helping a novice programmer fix buggy solutions to simple computational problems and is used for benchmarking the Socratic debugging abilities of a number of language models."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is reducing overconfident and unsupported claims generated by language models, and the proposed approach is to prompt the model to generate probing questions to challenge its own claims and revise them based on the quality of its justifications (Socratic Probing).\n\nThe research problem in the paper is helping novice programmers debug code, and the approach is to create a dataset of Socratic debugging advice and use it to benchmark the ability of language models to provide such advice.\n\nWhile both works involve using the Socratic method with language models, the application domains (claim verification vs. code debugging) and the specific techniques (prompting the model to probe itself vs. training the model to give Socratic advice) are quite different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "e6745fb621481ccb0ed53c267a37292e499c1b42",
            "paperId": "e6745fb621481ccb0ed53c267a37292e499c1b42",
            "title": "Automatic Generation of Socratic Subquestions for Teaching Math Word Problems",
            "abstract": "Socratic questioning is an educational method that allows students to discover answers to complex problems by asking them a series of thoughtful questions. Generation of didactically sound questions is challenging, requiring understanding of the reasoning process involved in the problem. We hypothesize that such questioning strategy can not only enhance the human performance, but also assist the math word problem (MWP) solvers.In this work, we explore the ability of large language models (LMs) in generating sequential questions for guiding math word problem-solving. We propose various guided question generation schemes based on input conditioning and reinforcement learning.On both automatic and human quality evaluations, we find that LMs constrained with desirable question properties generate superior questions and improve the overall performance of a math word problem solver. We conduct a preliminary user study to examine the potential value of such question generation models in the education domain. Results suggest that the difficulty level of problems plays an important role in determining whether questioning improves or hinders human performance. We discuss the future of using such questioning strategies in education.",
            "year": 2022,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores the ability of large language models (LMs) in generating sequential questions for guiding math word problem-solving and proposes various guided question generation schemes based on input conditioning and reinforcement learning."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to reduce overconfident and unsupported claims generated by language models using a Socratic probing method that prompts the model to critically examine and justify its own outputs.\n\nThe paper explores using large language models to automatically generate Socratic subquestions for guiding math word problem-solving, with the goal of enhancing both human and AI solver performance.\n\nWhile both involve generating Socratic-style questions, the project focuses on using such questions for self-verification of language model outputs, while the paper uses them to guide problem-solving in a specific domain (math word problems). The goals and domains of the two works are quite different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "f02e8f1c9b5ab12ddfb1977570f9f5445a99a973",
            "paperId": "f02e8f1c9b5ab12ddfb1977570f9f5445a99a973",
            "title": "Large Language Models are reasoners with Self-Verification",
            "abstract": "When a large language model (LLM) performs complex reasoning by chain of thought (CoT), it can be highly sensitive to individual mistakes. We have had to train verifiers to address this issue. As we all know, after human inferring a conclusion, they often check it by re-verifying it, which can avoid some mistakes. We propose a new method called self-verification that uses the conclusion of the CoT as a condition to build a new sample and asks the LLM to re-predict the original conditions which be masked. We calculate an explainable verification score based on the accuracy. This method can improve the accuracy of multiple arithmetics and logical reasoning datasets when using few-shot learning. we have demonstrated that LLMs can conduct explainable self-verification of their own conclusions and achieve competitive reasoning performance. Extensive experimentals have demonstrated that our method can help multiple large language models with self-verification can avoid interference from incorrect CoT. 1",
            "year": 2022,
            "citationCount": 54,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a new method called self-verification that uses the conclusion of the CoT as a condition to build a new sample and asks the LLM to re-predict the original conditions which be masked, and calculates an explainable verification score based on the accuracy."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to reduce overconfident or unsupported claims generated by language models by using a series of probing questions to critically examine and justify the model's outputs. The paper abstract proposes a self-verification method that uses the conclusion of a chain of thought as a condition to build a new sample and asks the language model to re-predict the original masked conditions, calculating a verification score based on accuracy.\n\nWhile both the project proposal and the paper abstract focus on improving the reasoning capabilities of language models, their research problems and approaches differ. The project proposal addresses the issue of biased and overconfident claims generated by language models, while the paper abstract tackles the sensitivity of language models to individual mistakes during complex reasoning. The project proposal uses probing questions to encourage the model to generate more nuanced and justified claims, whereas the paper abstract employs a self-verification method that re-predicts original conditions based on the conclusion of a chain of thought.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "7715ba5e75f5256e1061c7473afe61bb0dbb9065",
            "paperId": "7715ba5e75f5256e1061c7473afe61bb0dbb9065",
            "title": "Large Language Models are Better Reasoners with Self-Verification",
            "abstract": "Recently, with the chain of thought (CoT) prompting, large language models (LLMs), e.g., GPT-3, have shown strong reasoning ability in several natural language processing tasks such as arithmetic, commonsense, and logical reasoning. However, LLMs with CoT require multi-step prompting and multi-token prediction, which is highly sensitive to individual mistakes and vulnerable to error accumulation. The above issues make the LLMs need the ability to verify the answers. In fact, after inferring conclusions in some thinking decision tasks, people often check them by re-verifying steps to avoid some mistakes. In this paper, we propose and prove that LLMs also have similar self-verification abilities. We take the conclusion obtained by CoT as one of the conditions for solving the original problem. By performing a backward verification of the answers that LLM deduced for itself, we can obtain interpretable answer validation scores to select the candidate answer with the highest score. Experimental results demonstrate that the proposed method can improve the reasoning performance on various arithmetic, commonsense, and logical reasoning datasets. Our code is publicly available at: https://github.com/WENGSYX/Self-Verification.",
            "year": 2022,
            "citationCount": 54,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes and proves that LLMs also have similar self-verification abilities, and takes the conclusion obtained by CoT as one of the conditions for solving the original problem."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to reduce overconfident and unsupported claims generated by language models using a method called Socratic Probing, which prompts the model to generate probing questions, justify its claims, and revise them if needed.\n\nThe paper proposes a self-verification approach to improve the reasoning performance of language models by performing backward verification of the answers deduced by the model itself, obtaining interpretable answer validation scores to select the best candidate answer.\n\nWhile both works aim to improve the output quality of language models, the project focuses on reducing unsupported claims using probing questions, while the paper focuses on improving reasoning performance using self-verification. The methods and application domains are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "6233b5863f9a0e8bacce47ce21bc3e81c09497bd",
            "paperId": "6233b5863f9a0e8bacce47ce21bc3e81c09497bd",
            "title": "A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning",
            "abstract": "Logical reasoning has been an ongoing pursuit in the field of AI. Despite significant advancements made by large language models (LLMs), they still struggle with complex logical reasoning problems. To enhance reasoning performance, one promising direction is scalable oversight, which requires LLMs to identify their own errors and then improve by themselves. Various self-verification methods have been proposed in pursuit of this goal. Nevertheless, whether existing models understand their own errors well is still under investigation. In this paper, we take a closer look at the self-verification abilities of LLMs in the context of logical reasoning, focusing on their ability to identify logical fallacies accurately. We introduce a dataset, FALLACIES, containing 232 types of reasoning fallacies categorized in a hierarchical taxonomy. By conducting exhaustive experiments on FALLACIES, we obtain comprehensive and detailed analyses of a series of models on their verification abilities. Our main findings suggest that existing LLMs could struggle to identify fallacious reasoning steps accurately and may fall short of guaranteeing the validity of self-verification methods. Drawing from these observations, we offer suggestions for future research and practical applications of self-verification methods.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A closer look at the self-verification abilities of LLMs in the context of logical reasoning, focusing on their ability to identify logical fallacies accurately, suggests that existing LLMs could struggle to identify fallacious reasoning steps accurately and may fall short of guaranteeing the validity of self- Verification methods."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is reducing overconfident and unsupported claims made by language models, and the approach is to prompt the model to generate probing questions to challenge its own claims and revise them based on the quality of its justifications (Socratic Probing).\n\nThe research problem in the paper is improving the logical reasoning abilities of language models, and the approach is to analyze the self-verification abilities of language models in identifying logical fallacies accurately.\n\nWhile both works involve self-verification of language models, the proposal focuses on reducing unsupported claims in general language generation, while the paper specifically targets logical reasoning and fallacy detection. The methods are also quite different, with the proposal using probing questions and iterative revisions, and the paper conducting analyses on a dataset of reasoning fallacies.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "e879f54b2b5760bbb6d010977ddcedfb62452b38",
            "paperId": "e879f54b2b5760bbb6d010977ddcedfb62452b38",
            "title": "Can Large Language Models Really Improve by Self-critiquing Their Own Plans?",
            "abstract": "There have been widespread claims about Large Language Models (LLMs) being able to successfully verify or self-critique their candidate solutions in reasoning problems in an iterative mode. Intrigued by those claims, in this paper we set out to investigate the verification/self-critiquing abilities of large language models in the context of planning. We evaluate a planning system that employs LLMs for both plan generation and verification. We assess the verifier LLM's performance against ground-truth verification, the impact of self-critiquing on plan generation, and the influence of varying feedback levels on system performance. Using GPT-4, a state-of-the-art LLM, for both generation and verification, our findings reveal that self-critiquing appears to diminish plan generation performance, especially when compared to systems with external, sound verifiers and the LLM verifiers in that system produce a notable number of false positives, compromising the system's reliability. Additionally, the nature of feedback, whether binary or detailed, showed minimal impact on plan generation. Collectively, our results cast doubt on the effectiveness of LLMs in a self-critiquing, iterative framework for planning tasks.",
            "year": 2023,
            "citationCount": 32,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Investigation of the verification/self-critique abilities of large language models in the context of planning reveals that self-critiquing appears to diminish plan generation performance, especially when compared to systems with external, sound verifiers and the LLM verifiers in that system produce a notable number of false positives, compromising the system's reliability."
            },
            "score": 6
        },
        {
            "id": "eae87af6e956532d6842718803179c9ab2386ea9",
            "paperId": "eae87af6e956532d6842718803179c9ab2386ea9",
            "title": "Large Language Models Should Ask Clarifying Questions to Increase Confidence in Generated Code",
            "abstract": "Large language models (LLMs) have significantly improved the ability to perform tasks in the field of code generation. However, there is still a gap between LLMs being capable coders and being top-tier software engineers. Based on the observation that toplevel software engineers often ask clarifying questions to reduce ambiguity in both requirements and coding solutions, I argue that the same should be applied to LLMs for code generation tasks. By asking probing questions in various topics before generating the final code, the challenges of programming with LLMs, such as unclear intent specification, lack of computational thinking, and undesired code quality, may be alleviated. This, in turn, increases confidence in the generated code. In this work, I explore how to leverage better communication skills to achieve greater confidence in generated code. I propose a communication-centered process that uses an LLM-generated communicator to identify issues with high ambiguity or low confidence in problem descriptions and generated code. I then ask clarifying questions to obtain responses from users for refining the code.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a communication-centered process that uses an LLM-generated communicator to identify issues with high ambiguity or low confidence in problem descriptions and generated code, and asks clarifying questions to obtain responses from users for refining the code."
            },
            "score": 6
        },
        {
            "id": "3a89e289e2dd29f5e52a2bf354a637762b661257",
            "paperId": "3a89e289e2dd29f5e52a2bf354a637762b661257",
            "title": "Fine-tuning Language Models for Factuality",
            "abstract": "The fluency and creativity of large pre-trained language models (LLMs) have led to their widespread use, sometimes even as a replacement for traditional search engines. Yet language models are prone to making convincing but factually inaccurate claims, often referred to as 'hallucinations.' These errors can inadvertently spread misinformation or harmfully perpetuate misconceptions. Further, manual fact-checking of model responses is a time-consuming process, making human factuality labels expensive to acquire. In this work, we fine-tune language models to be more factual, without human labeling and targeting more open-ended generation settings than past work. We leverage two key recent innovations in NLP to do so. First, several recent works have proposed methods for judging the factuality of open-ended text by measuring consistency with an external knowledge base or simply a large model's confidence scores. Second, the direct preference optimization algorithm enables straightforward fine-tuning of language models on objectives other than supervised imitation, using a preference ranking over possible model responses. We show that learning from automatically generated factuality preference rankings, generated either through existing retrieval systems or our novel retrieval-free approach, significantly improves the factuality (percent of generated claims that are correct) of Llama-2 on held-out topics compared with RLHF or decoding strategies targeted at factuality. At 7B scale, compared to Llama-2-chat, we observe 58% and 40% reduction in factual error rate when generating biographies and answering medical questions, respectively.",
            "year": 2023,
            "citationCount": 56,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that learning from automatically generated factuality preference rankings, generated either through existing retrieval systems or the novel retrieval-free approach, significantly improves the factuality of Llama-2 on held-out topics compared with RLHF or decoding strategies targeted at factuality."
            },
            "score": 6
        },
        {
            "id": "dedfe929d182cc3537a9ed765d589b4735ce062a",
            "paperId": "dedfe929d182cc3537a9ed765d589b4735ce062a",
            "title": "On the Planning Abilities of Large Language Models - A Critical Investigation",
            "abstract": "Intrigued by the claims of emergent reasoning capabilities in LLMs trained on general web corpora, in this paper, we set out to investigate their planning capabilities. We aim to evaluate (1) the effectiveness of LLMs in generating plans autonomously in commonsense planning tasks and (2) the potential of LLMs as a source of heuristic guidance for other agents (AI planners) in their planning tasks. We conduct a systematic study by generating a suite of instances on domains similar to the ones employed in the International Planning Competition and evaluate LLMs in two distinct modes: autonomous and heuristic. Our findings reveal that LLMs' ability to generate executable plans autonomously is rather limited, with the best model (GPT-4) having an average success rate of ~12% across the domains. However, the results in the heuristic mode show more promise. In the heuristic mode, we demonstrate that LLM-generated plans can improve the search process for underlying sound planners and additionally show that external verifiers can help provide feedback on the generated plans and back-prompt the LLM for better plan generation.",
            "year": 2023,
            "citationCount": 51,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that LLM-generated plans can improve the search process for underlying sound planners and additionally show that external verifiers can help provide feedback on the generated plans and back-prompt the LLM for better plan generation."
            },
            "score": 6
        },
        {
            "id": "85996f9fc312777f487dd51bf9e96bb3704c2fb7",
            "paperId": "85996f9fc312777f487dd51bf9e96bb3704c2fb7",
            "title": "On the Planning Abilities of Large Language Models (A Critical Investigation with a Proposed Benchmark)",
            "abstract": "Intrigued by the claims of emergent reasoning capabilities in LLMs trained on general web corpora, in this paper, we set out to investigate their planning capabilities. We aim to evaluate (1) how good LLMs are by themselves in generating and validating simple plans in commonsense planning tasks (of the type that humans are generally quite good at) and (2) how good LLMs are in being a source of heuristic guidance for other agents--either AI planners or human planners--in their planning tasks. To investigate these questions in a systematic rather than anecdotal manner, we start by developing a benchmark suite based on the kinds of domains employed in the International Planning Competition. On this benchmark, we evaluate LLMs in three modes: autonomous, heuristic and human-in-the-loop. Our results show that LLM's ability to autonomously generate executable plans is quite meager, averaging only about 3% success rate. The heuristic and human-in-the-loop modes show slightly more promise. In addition to these results, we also make our benchmark and evaluation tools available to support investigations by research community.",
            "year": 2023,
            "citationCount": 40,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results show that LLM's ability to autonomously generate executable plans is quite meager, averaging only about 3% success rate, and the heuristic and human-in-the-loop modes show slightly more promise."
            },
            "score": 6
        },
        {
            "id": "8666f9f379389a5dff31e72fb0f992a37763ba41",
            "paperId": "8666f9f379389a5dff31e72fb0f992a37763ba41",
            "title": "Teaching language models to support answers with verified quotes",
            "abstract": "Recent large language models often answer factual questions correctly. But users can't trust any given claim a model makes without fact-checking, because language models can hallucinate convincing nonsense. In this work we use reinforcement learning from human preferences (RLHP) to train\"open-book\"QA models that generate answers whilst also citing specific evidence for their claims, which aids in the appraisal of correctness. Supporting evidence is drawn from multiple documents found via a search engine, or from a single user-provided document. Our 280 billion parameter model, GopherCite, is able to produce answers with high quality supporting evidence and abstain from answering when unsure. We measure the performance of GopherCite by conducting human evaluation of answers to questions in a subset of the NaturalQuestions and ELI5 datasets. The model's response is found to be high-quality 80\\% of the time on this Natural Questions subset, and 67\\% of the time on the ELI5 subset. Abstaining from the third of questions for which it is most unsure improves performance to 90\\% and 80\\% respectively, approaching human baselines. However, analysis on the adversarial TruthfulQA dataset shows why citation is only one part of an overall strategy for safety and trustworthiness: not all claims supported by evidence are true.",
            "year": 2022,
            "citationCount": 157,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work uses reinforcement learning from human preferences (RLHP) to train\"open-book\"QA models that generate answers whilst also citing specific evidence for their claims, which aids in the appraisal of correctness."
            },
            "score": 6
        },
        {
            "id": "76e5069425547d4f53b5aa843a765a305b7fa470",
            "paperId": "76e5069425547d4f53b5aa843a765a305b7fa470",
            "title": "Discursive Socratic Questioning: (Unsupervised) Interpreting Neural Language Models for Discourse Understanding",
            "abstract": "Do neural language models (NLMs) understand 001 the discourse they are processing? Traditional 002 interpretation methods that address this ques-003 tion require pre-annotated explanations, which 004 defeats the purpose of unsupervised explana-005 tion. We propose unsupervised Discursive So-006 cratic Questioning ( D I SQ ), a two-step interpre-007 tative measure. 008 D I SQ first generates Socratic-style questions 009 about the discourse and then queries NLMs 010 about these questions. A model\u2019s understand-011 ing is measured by its responses to these ques-012 tions. We apply D I SQ to examine two fun-013 damental discourse phenomena, namely dis-014 course relation and discourse coherence. We 015 find NLMs demonstrate non-trivial capacities 016 without being trained on any discourse data: 017 Q&A pairs in D I SQ are shown to be evidence 018 for discourse relation and cohesive devices for 019 discourse coherence. D I SQ brings initial evi-020 dence that NLMs understand discourse through 021 reasoning. We find larger models perform bet-022 ter, but contradictions and hallucinations are 023 still problems. We recommend D I SQ as a uni-024 versal diagnostic for discursive NLMs and us-025 ing its output for self-supervision. 026",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "D I SQ brings initial evi-020 dence that NLMs understand discourse through 021 reasoning, and is recommended as a unsupervised versal diagnostic for discursive NLMs and its output for self-supervision."
            },
            "score": 5
        },
        {
            "id": "112e0260c960c02a808cbf191420b13ef824da1c",
            "paperId": "112e0260c960c02a808cbf191420b13ef824da1c",
            "title": "On the Self-Verification Limitations of Large Language Models on Reasoning and Planning Tasks",
            "abstract": "There has been considerable divergence of opinion on the reasoning abilities of Large Language Models (LLMs). While the initial optimism that reasoning might emerge automatically with scale has been tempered thanks to a slew of counterexamples--ranging from multiplication to simple planning--there persists a wide spread belief that LLMs can self-critique and improve their own solutions in an iterative fashion. This belief seemingly rests on the assumption that verification of correctness should be easier than generation--a rather classical argument from computational complexity--which should be irrelevant to LLMs to the extent that what they are doing is approximate retrieval. In this paper, we set out to systematically investigate the effectiveness of iterative prompting in the context of reasoning and planning. We present a principled empirical study of the performance of GPT-4 in three domains: Game of 24, Graph Coloring, and STRIPS planning. We experiment both with the model critiquing its own answers and with an external correct reasoner verifying proposed solutions. In each case, we analyze whether the content of criticisms actually affects bottom line performance, and whether we can ablate elements of the augmented system without losing performance. We observe significant performance collapse with self-critique, significant performance gains with sound external verification, but that the content of critique doesn't matter to the performance of the system. In fact, merely re-prompting with a sound verifier maintains most of the benefits of more involved setups.",
            "year": 2024,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents a principled empirical study of the performance of GPT-4 in three domains: Game of 24, Graph Coloring, and STRIPS planning and observes significant performance collapse with self-critique, significant performance gains with sound external verification, but that the content of critique doesn't matter to the performance of the system."
            },
            "score": 5
        },
        {
            "id": "c296733b810e130ad651b0d184b5113a95fa6a2b",
            "paperId": "c296733b810e130ad651b0d184b5113a95fa6a2b",
            "title": "Hypothesis, Verification, and Induction: Grounding Large Language Models with Self-Driven Skill Learning",
            "abstract": "Large language models (LLMs) show their powerful automatic reasoning and planning capability with a wealth of semantic knowledge about the human world. However, the grounding problem still hinders the applications of LLMs in the real-world environment. Existing studies try to fine-tune the LLM or utilize pre-defined behavior APIs to bridge the LLMs and the environment, which not only costs huge human efforts to customize for every single task but also weakens the generality strengths of LLMs. To autonomously ground the LLM onto the environment, we proposed the Hypothesis, Verification, and Induction (HYVIN) framework to automatically and progressively ground the LLM with self-driven skill learning. HYVIN first employs the LLM to propose the hypothesis of sub-goals to achieve tasks and then verify the feasibility of the hypothesis via interacting with the underlying environment. Once verified, HYVIN can then learn generalized skills with the guidance of these successfully grounded subgoals. These skills can be further utilized to accomplish more complex tasks that fail to pass the verification phase. Verified in the famous instruction following task set, BabyAI, HYVIN achieves comparable performance in the most challenging tasks compared with imitation learning methods that cost millions of demonstrations, proving the effectiveness of learned skills and showing the feasibility and efficiency of our framework.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposed the Hypothesis, Verification, and Induction (HYVIN) framework to automatically and progressively ground the LLM with self-driven skill learning, proving the effectiveness of learned skills and showing the feasibility and efficiency of the framework."
            },
            "score": 5
        },
        {
            "id": "801d7ba75fc833aa76ce4863dc1f79e30ee0c23f",
            "paperId": "801d7ba75fc833aa76ce4863dc1f79e30ee0c23f",
            "title": "Forward-Backward Reasoning in Large Language Models for Mathematical Verification",
            "abstract": "Self-Consistency samples diverse reasoning chains with answers and chooses the final answer by majority voting. It is based on forward reasoning and cannot further improve performance by sampling more reasoning chains when saturated. To further boost performance, we introduce backward reasoning to verify candidate answers. Specifically, for mathematical tasks, we mask a number in the question and ask the LLM to answer a backward question created by a simple template, i.e., to predict the masked number when a candidate answer is provided. Instead of using forward or backward reasoning alone, we propose FOBAR to combine FOrward and BAckward Reasoning for verification. Extensive experiments on six standard mathematical data sets and three LLMs show that FOBAR achieves state-of-the-art performance. In particular, FOBAR outperforms Self-Consistency, which uses forward reasoning alone, demonstrating that combining forward and forward reasoning is better. In addition, FOBAR performs better than existing verification methods, showing the effectiveness of the simple template used in backward reasoning and the proposed combination. Extensions to non-mathematical problems are also discussed and validated empirically.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes FOBAR to combine FOrward and BAckward Reasoning for verification for verification, and shows that FOBAR outperforms Self-Consistency, which uses forward reasoning alone, demonstrating that combining forward and forward reasoning is better."
            },
            "score": 5
        },
        {
            "id": "cb4f6823e25896d5919046696066d0d498cc4397",
            "paperId": "cb4f6823e25896d5919046696066d0d498cc4397",
            "title": "Forward-Backward Reasoning in Large Language Models for Verification",
            "abstract": "Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency (Wang et al., 2023) proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by x and ask the LLM to predict the masked token when a candidate answer is provided by a simple template, i.e., \u201cIf we know the answer of the above question is {a candidate answer}, what is the value of unknown variable x?\u201d Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three LLMs. Experimental results demonstrate that FOBAR achieves state-of-the-art performance on various reasoning benchmarks.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel method to use backward reasoning in verifying candidate answers inChain-of-Though prompting and proposes FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers."
            },
            "score": 5
        },
        {
            "id": "1dbd58bd8768ba0dada2e7c84aa2fe0b9f418ebc",
            "paperId": "1dbd58bd8768ba0dada2e7c84aa2fe0b9f418ebc",
            "title": "Solving Challenging Math Word Problems Using GPT-4 Code Interpreter with Code-based Self-Verification",
            "abstract": "Recent progress in large language models (LLMs) like GPT-4 and PaLM-2 has brought significant advancements in addressing math reasoning problems. In particular, OpenAI's latest version of GPT-4, known as GPT-4 Code Interpreter, shows remarkable performance on challenging math datasets. In this paper, we explore the effect of code on enhancing LLMs' reasoning capability by introducing different constraints on the \\textit{Code Usage Frequency} of GPT-4 Code Interpreter. We found that its success can be largely attributed to its powerful skills in generating and executing code, evaluating the output of code execution, and rectifying its solution when receiving unreasonable outputs. Based on this insight, we propose a novel and effective prompting method, explicit \\uline{c}ode-based \\uline{s}elf-\\uline{v}erification~(CSV), to further boost the mathematical reasoning potential of GPT-4 Code Interpreter. This method employs a zero-shot prompt on GPT-4 Code Interpreter to encourage it to use code to self-verify its answers. In instances where the verification state registers as ``False'', the model shall automatically amend its solution, analogous to our approach of rectifying errors during a mathematics examination. Furthermore, we recognize that the states of the verification result indicate the confidence of a solution, which can improve the effectiveness of majority voting. With GPT-4 Code Interpreter and CSV, we achieve an impressive zero-shot accuracy on MATH dataset \\textbf{(53.9\\% $\\to$ 84.3\\%)}.",
            "year": 2023,
            "citationCount": 64,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The effect of code on enhancing LLMs' reasoning capability by introducing different constraints on the Code Usage Frequency of GPT-4 Code Interpreter is explored, and a novel and effective prompting method, explicit \\uline{c}ode-based \\ULine{s}elf-\\uline {v}erification~(CSV), is proposed to further boost the mathematical reasoning potential of GPN."
            },
            "score": 5
        },
        {
            "id": "8f7100fd747494f4b184d6c4fce25d6dfc3d8018",
            "paperId": "8f7100fd747494f4b184d6c4fce25d6dfc3d8018",
            "title": "Can Large Language Models put 2 and 2 together? Probing for Entailed Arithmetical Relationships",
            "abstract": "Two major areas of interest in the era of Large Language Models regard questions of what do LLMs know, and if and how they may be able to reason (or rather, approximately reason). Since to date these lines of work progressed largely in parallel (with notable exceptions), we are interested in investigating the intersection: probing for reasoning about the implicitly-held knowledge. Suspecting the performance to be lacking in this area, we use a very simple set-up of comparisons between cardinalities associated with elements of various subjects (e.g. the number of legs a bird has versus the number of wheels on a tricycle). We empirically demonstrate that although LLMs make steady progress in knowledge acquisition and (pseudo)reasoning with each new GPT release, their capabilities are limited to statistical inference only. It is difficult to argue that pure statistical learning can cope with the combinatorial explosion inherent in many commonsense reasoning tasks, especially once arithmetical notions are involved. Further, we argue that bigger is not always better and chasing purely statistical improvements is flawed at the core, since it only exacerbates the dangerous conflation of the production of correct answers with genuine reasoning ability.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is empirical demonstrate that although LLMs make steady progress in knowledge acquisition and (pseudo)reasoning with each new GPT release, their capabilities are limited to statistical inference only, and it is argued that bigger is not always better and chasing purely statistical improvements is flawed at the core."
            },
            "score": 5
        },
        {
            "id": "cfce709a65f90312d2bdc1a6cf0380c19becf694",
            "paperId": "cfce709a65f90312d2bdc1a6cf0380c19becf694",
            "title": "RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models",
            "abstract": "Retrieval-augmented generation (RAG) has become a main technique for alleviating hallucinations in large language models (LLMs). Despite the integration of RAG, LLMs may still present unsupported or contradictory claims to the retrieved contents. In order to develop effective hallucination prevention strategies under RAG, it is important to create benchmark datasets that can measure the extent of hallucination. This paper presents RAGTruth, a corpus tailored for analyzing word-level hallucinations in various domains and tasks within the standard RAG frameworks for LLM applications. RAGTruth comprises nearly 18,000 naturally generated responses from diverse LLMs using RAG. These responses have undergone meticulous manual annotations at both the individual cases and word levels, incorporating evaluations of hallucination intensity. We not only benchmark hallucination frequencies across different LLMs, but also critically assess the effectiveness of several existing hallucination detection methodologies. Furthermore, we show that using a high-quality dataset such as RAGTruth, it is possible to finetune a relatively small LLM and achieve a competitive level of performance in hallucination detection when compared to the existing prompt-based approaches using state-of-the-art large language models such as GPT-4.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "RAGTruth is presented, a corpus tailored for analyzing word-level hallucinations in various domains and tasks within the standard RAG frameworks for LLM applications and it is shown that using a high-quality dataset such as RAGTruth, it is possible to finetune a relatively small LLM and achieve a competitive level of performance in hallucination detection when compared to the existing prompt-based approaches using state-of-the-art large language models such as GPT-4."
            },
            "score": 5
        },
        {
            "id": "acf90b4d165690fe27c62c4af1a28d540c784000",
            "paperId": "acf90b4d165690fe27c62c4af1a28d540c784000",
            "title": "Automatic Evaluation of Attribution by Large Language Models",
            "abstract": "A recent focus of large language model (LLM) development, as exemplified by generative search engines, is to incorporate external references to generate and support its claims. However, evaluating the attribution, i.e., verifying whether the generated statement is fully supported by the cited reference, remains an open problem. Although human evaluation is common practice, it is costly and time-consuming. In this paper, we investigate the automatic evaluation of attribution given by LLMs. We begin by defining different types of attribution errors, and then explore two approaches for automatic evaluation: prompting LLMs and fine-tuning smaller LMs. The fine-tuning data is repurposed from related tasks such as question answering, fact-checking, natural language inference, and summarization. We manually curate a set of test examples covering 12 domains from a generative search engine, New Bing. Our results on this curated test set and simulated examples from existing benchmarks highlight both promising signals and challenges. We hope our problem formulation, testbeds, and findings will help lay the foundation for future studies on this important problem.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper investigates the automatic evaluation of attribution given by large language model (LLMs), defining different types of attribution errors, and exploring two approaches for automatic evaluation: prompting LLMs and fine-tuning smaller LMs."
            },
            "score": 5
        },
        {
            "id": "142ebbf4760145f591166bde2564ac70c001e927",
            "paperId": "142ebbf4760145f591166bde2564ac70c001e927",
            "title": "Language Models (Mostly) Know What They Know",
            "abstract": "We study whether language models can evaluate the validity of their own claims and predict which questions they will be able to answer correctly. We first show that larger models are well-calibrated on diverse multiple choice and true/false questions when they are provided in the right format. Thus we can approach self-evaluation on open-ended sampling tasks by asking models to first propose answers, and then to evaluate the probability\"P(True)\"that their answers are correct. We find encouraging performance, calibration, and scaling for P(True) on a diverse array of tasks. Performance at self-evaluation further improves when we allow models to consider many of their own samples before predicting the validity of one specific possibility. Next, we investigate whether models can be trained to predict\"P(IK)\", the probability that\"I know\"the answer to a question, without reference to any particular proposed answer. Models perform well at predicting P(IK) and partially generalize across tasks, though they struggle with calibration of P(IK) on new tasks. The predicted P(IK) probabilities also increase appropriately in the presence of relevant source materials in the context, and in the presence of hints towards the solution of mathematical word problems. We hope these observations lay the groundwork for training more honest models, and for investigating how honesty generalizes to cases where models are trained on objectives other than the imitation of human writing.",
            "year": 2022,
            "citationCount": 340,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that larger models are well-calibrated on diverse multiple choice and true/false questions when they are provided in the right format and investigated whether models can be trained to predict P(IK), the probability that \"I know\" the answer to a question."
            },
            "score": 5
        },
        {
            "id": "e850d4c0dad3aee3b8b40be5e5d5e5c31354d8cc",
            "paperId": "e850d4c0dad3aee3b8b40be5e5d5e5c31354d8cc",
            "title": "PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change",
            "abstract": "Generating plans of action, and reasoning about change have long been considered a core competence of intelligent agents. It is thus no surprise that evaluating the planning and reasoning capabilities of large language models (LLMs) has become a hot topic of research. Most claims about LLM planning capabilities are however based on common sense tasks-where it becomes hard to tell whether LLMs are planning or merely retrieving from their vast world knowledge. There is a strong need for systematic and extensible planning benchmarks with sufficient diversity to evaluate whether LLMs have innate planning capabilities. Motivated by this, we propose PlanBench, an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. PlanBench provides sufficient diversity in both the task domains and the specific planning capabilities. Our studies also show that on many critical capabilities-including plan generation-LLM performance falls quite short, even with the SOTA models. PlanBench can thus function as a useful marker of progress of LLMs in planning and reasoning.",
            "year": 2022,
            "citationCount": 79,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes PlanBench, an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change."
            },
            "score": 5
        },
        {
            "id": "ada81a4de88a6ce474df2e2446ad11fea480616e",
            "paperId": "ada81a4de88a6ce474df2e2446ad11fea480616e",
            "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language",
            "abstract": "Large pretrained (e.g.,\"foundation\") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodal-informed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning.",
            "year": 2022,
            "citationCount": 380,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Socratic Models (SMs) are shown to be competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, and enable new applications such as answering free-form questions about egocentric video, and engaging in multimodal assistive dialogue with people."
            },
            "score": 4
        },
        {
            "id": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
            "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
            "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
            "year": 2022,
            "citationCount": 3517,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks."
            },
            "score": 4
        },
        {
            "id": "3cebb93c399db7e1434741338b0a24db19786b15",
            "paperId": "3cebb93c399db7e1434741338b0a24db19786b15",
            "title": "Prompt to be Consistent is Better than Self-Consistent? Few-Shot and Zero-Shot Fact Verification with Pre-trained Language Models",
            "abstract": "Few-shot or zero-shot fact verification only relies on a few or no labeled training examples. In this paper, we propose a novel method called ProToCo, to \\underline{Pro}mpt pre-trained language models (PLMs) \\underline{To} be \\underline{Co}nsistent, for improving the factuality assessment capability of PLMs in the few-shot and zero-shot settings. Given a claim-evidence pair, ProToCo generates multiple variants of the claim with different relations and frames a simple consistency mechanism as constraints for making compatible predictions across these variants. We update PLMs by using parameter-efficient fine-tuning (PEFT), leading to more accurate predictions in few-shot and zero-shot fact verification tasks. Our experiments on three public verification datasets show that ProToCo significantly outperforms state-of-the-art few-shot fact verification baselines. With a small number of unlabeled instances, ProToCo also outperforms the strong zero-shot learner T0 on zero-shot verification. Compared to large PLMs using in-context learning (ICL) method, ProToCo outperforms OPT-30B and the Self-Consistency-enabled OPT-6.7B model in both few- and zero-shot settings.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel method called ProToCo, to improve the factuality assessment capability of pre-trained language models (PLMs) by using parameter-efficient fine-tuning (PEFT), leading to more accurate predictions in few-shot and zero-shot fact verification tasks."
            },
            "score": 4
        },
        {
            "id": "f197bf0fc2f228483f6af3285000d54d8d97f9eb",
            "paperId": "f197bf0fc2f228483f6af3285000d54d8d97f9eb",
            "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models",
            "abstract": "We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",
            "year": 2023,
            "citationCount": 336,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 4
        },
        {
            "id": "92746dfa09dcad92ecf1e6272ebb300c1112b7eb",
            "paperId": "92746dfa09dcad92ecf1e6272ebb300c1112b7eb",
            "title": "Automatic Calibration and Error Correction for Large Language Models via Pareto Optimal Self-Supervision",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities out of box for a wide range of applications, yet accuracy still remains a major growth area, especially in mission-critical domains such as biomedicine. An effective method to calibrate the con\ufb01dence level on LLM responses is essential to automatically detect errors and facilitate human-in-the-loop veri\ufb01cation. An important source of calibration signals stems from expert-stipulated programmatic super-vision, which is often available at low cost but has its own limitations such as noise and coverage. In this paper, we introduce a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every response, without any additional manual efforts. This is accomplished by learning a harmonizer model to align LLM output with other available supervision sources, which would assign higher risk scores to more uncertain LLM responses and facilitate error correction. Experiments on standard relation extraction tasks in biomedical and general domains demonstrate the promise of this approach, with our proposed risk scores highly correlated with the real error rate of LLMs. For the most uncertain test instances, dynamic prompting based on our proposed risk scores results in signi\ufb01cant accuracy improvement for off-the-shelf LLMs, boosting GPT-3 results past state-of-the-art (SOTA) weak supervision and GPT-4 results past SOTA supervised results on challenging evaluation datasets.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every response, without any additional manual efforts."
            },
            "score": 4
        },
        {
            "id": "0d22f06a1f5ad9f62b2f35c126b514f927586c85",
            "paperId": "0d22f06a1f5ad9f62b2f35c126b514f927586c85",
            "title": "Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency",
            "abstract": "Large language models (LLMs) have exhibited remarkable ability in code generation. However, generating the correct solution in a single attempt still remains a challenge. Prior works utilize verification properties in software engineering to verify and re-rank solutions in a majority voting manner. But the assumption behind them that generated verification properties have better qualities than solutions may not always hold. In this paper, we treat them equally as different perspectives of LLMs' reasoning processes. We propose the Multi-Perspective Self-Consistency (MPSC) framework incorporating both inter- and intra-consistency across outputs from multiple perspectives. Specifically, we prompt LLMs to generate diverse outputs from three perspectives, Solution, Specification and Test case, constructing a 3-partite graph. With two measure functions of consistency, we embed both inter- and intra-consistency information into the graph. The optimal choice of solutions is then determined based on analysis in the graph. MPSC significantly boosts performance of foundation models (ChatGPT in this paper) on various benchmarks, including HumanEval (+15.91%), MBPP (+6.43%) and CodeContests (+9.37%), even surpassing GPT-4.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Multi-Perspective Self-Consistency (MPSC) framework is proposed incorporating both inter- and intra-consistency across outputs from multiple perspectives and significantly boosts performance of foundation models on various benchmarks, including HumanEval, MBPP and CodeContests."
            },
            "score": 4
        },
        {
            "id": "9a61d51212eb4ff677fe777a7ba9ddc4f675b387",
            "paperId": "9a61d51212eb4ff677fe777a7ba9ddc4f675b387",
            "title": "Automatic Calibration and Error Correction for Generative Large Language Models via Pareto Optimal Self-Supervision",
            "abstract": "Generative Large language models (LLMs) have demonstrated remarkable capabilities for a wide range of applications, but reducing ungrounded or erroneous responses remains a major growth area. Unlike task-specific models, there lack an effective method to calibrate the confidence level of LLM responses to indicate potential errors and facilitate human-in-the-loop verification. An important source of calibration stems from expert-stipulated programmatic supervision, which is often available at low cost but has its own limitations such as noise and coverage. In this paper, we introduce a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every LLM response, without any additional manual efforts. This is accomplished by learning a harmonizer model to align with LLM output as well as other weak supervision sources. The model assigns higher risk scores to more uncertain LLM responses and facilitate error correction. Experiments on standard relation extraction and classification tasks in biomedical and general domains demonstrate that the proposed risk score is highly correlated with the actual LLM error rate. By using a dynamic prompting strategy based on the risk score, we observed significant accuracy improvement for off-the-shelf LLMs, boosting GPT-3.5 results past state-of-the-art (SOTA) weak supervision model and GPT-4 results past SOTA supervised results on challenging evaluation datasets.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every LLM response, without any additional manual efforts."
            },
            "score": 4
        },
        {
            "id": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
            "paperId": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
            "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such\"society of minds\"approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.",
            "year": 2023,
            "citationCount": 206,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer is presented, indicating that this approach significantly enhances mathematical and strategic reasoning across a number of tasks."
            },
            "score": 4
        },
        {
            "id": "34e1a8a75bf6f35084ac6d714a136f39d02c649e",
            "paperId": "34e1a8a75bf6f35084ac6d714a136f39d02c649e",
            "title": "Self-Verification Improves Few-Shot Clinical Information Extraction",
            "abstract": "Extracting patient information from unstructured text is a critical task in health decision-support and clinical research. Large language models (LLMs) have shown the potential to accelerate clinical curation via few-shot in-context learning, in contrast to supervised learning which requires much more costly human annotations. However, despite drastic advances in modern LLMs such as GPT-4, they still struggle with issues regarding accuracy and interpretability, especially in mission-critical domains such as health. Here, we explore a general mitigation framework using self-verification, which leverages the LLM to provide provenance for its own extraction and check its own outputs. This is made possible by the asymmetry between verification and generation, where the latter is often much easier than the former. Experimental results show that our method consistently improves accuracy for various LLMs in standard clinical information extraction tasks. Additionally, self-verification yields interpretations in the form of a short text span corresponding to each output, which makes it very efficient for human experts to audit the results, paving the way towards trustworthy extraction of clinical information in resource-constrained scenarios. To facilitate future research in this direction, we release our code and prompts.",
            "year": 2023,
            "citationCount": 21,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores a general mitigation framework using self-verification, which leverages the LLM to provide provenance for its own extraction and check its own outputs and consistently improves accuracy for various LLMs in standard clinical information extraction tasks."
            },
            "score": 4
        },
        {
            "id": "7c9f69848d28e0a7cbb00942ee83dab9773c23e4",
            "paperId": "7c9f69848d28e0a7cbb00942ee83dab9773c23e4",
            "title": "GPT-NER: Named Entity Recognition via Large Language Models",
            "abstract": "Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model. In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text\"Columbus is a city\"is transformed to generate the text sequence\"@@Columbus## is a city\", where special tokens @@## marks the entity to extract. To efficiently address the\"hallucination\"issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a self-verification strategy by prompting LLMs to ask itself whether the extracted entities belong to a labeled entity tag. We conduct experiments on five widely adopted NER datasets, and GPT-NER achieves comparable performances to fully supervised baselines, which is the first time as far as we are concerned. More importantly, we find that GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, GPT-NER performs significantly better than supervised models. This demonstrates the capabilities of GPT-NER in real-world NER applications where the number of labeled examples is limited.",
            "year": 2023,
            "citationCount": 58,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, and performs significantly better than supervised models, which demonstrates the capabilities of GPT-ner in real-world NER applications where the number of labeled examples is limited."
            },
            "score": 4
        },
        {
            "id": "c7091540c1fa77f1c6b27482f349330f8e559d6f",
            "paperId": "c7091540c1fa77f1c6b27482f349330f8e559d6f",
            "title": "Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks",
            "abstract": "We consider the questions of whether or not large language models (LLMs) have beliefs, and, if they do, how we might measure them. First, we evaluate two existing approaches, one due to Azaria and Mitchell (2023) and the other to Burns et al. (2022). We provide empirical results that show that these methods fail to generalize in very basic ways. We then argue that, even if LLMs have beliefs, these methods are unlikely to be successful for conceptual reasons. Thus, there is still no lie-detector for LLMs. After describing our empirical results we take a step back and consider whether or not we should expect LLMs to have something like beliefs in the first place. We consider some recent arguments aiming to show that LLMs cannot have beliefs. We show that these arguments are misguided. We provide a more productive framing of questions surrounding the status of beliefs in LLMs, and highlight the empirical nature of the problem. We conclude by suggesting some concrete paths for future work.",
            "year": 2023,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Empirical results show that, even if LLMs have beliefs, these methods are unlikely to be successful for conceptual reasons, and it is argued that there is still no lie-detector for LLMs."
            },
            "score": 4
        },
        {
            "id": "c6ee979c2da4b55a8486abae4cd720422ab09b26",
            "paperId": "c6ee979c2da4b55a8486abae4cd720422ab09b26",
            "title": "When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories",
            "abstract": "Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the difficulty of encoding a wealth of world knowledge in their parameters. This paper aims to understand LMs\u2019 strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments on two open-domain entity-centric QA datasets: PopQA, our new dataset with 14k questions about long-tail entities, and EntityQuestions, a widely used open-domain QA dataset. We find that LMs struggle with less popular factual knowledge, and that retrieval augmentation helps significantly in these cases. Scaling, on the other hand, mainly improves memorization of popular knowledge, and fails to appreciably improve memorization of factual knowledge in the tail. Based on those findings, we devise a new method for retrieval-augmentation that improves performance and reduces inference costs by only retrieving non-parametric memories when necessary.",
            "year": 2022,
            "citationCount": 159,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that LMs struggle with less popular factual knowledge, and that retrieval augmentation helps significantly in these cases, and a new method is devised that improves performance and reduces inference costs by only retrieving non-parametric memories when necessary."
            },
            "score": 4
        },
        {
            "id": "7b0f98f51040700aae3cd9f0e3432dedcd69fb30",
            "paperId": "7b0f98f51040700aae3cd9f0e3432dedcd69fb30",
            "title": "When Not to Trust Language Models: Investigating Effectiveness and Limitations of Parametric and Non-Parametric Memories",
            "abstract": "Despite their impressive performance on diverse tasks, large language models (LMs) still struggle with tasks requiring rich world knowledge, implying the limitations of relying solely on their parameters to encode a wealth of world knowledge. This paper aims to understand LMs\u2019 strengths and limitations in memorizing factual knowledge, by conducting large-scale knowledge probing experiments of 10 models and 4 augmentation meth-ods on P OP QA, our new open-domain QA dataset with 14k questions. We \ufb01nd that LMs struggle with less popular factual knowledge, and that scaling fails to appreciably improve memorization of factual knowledge in the tail. We then show that retrieval-augmented LMs largely outperform orders of magnitude larger LMs, while unassisted LMs remain competitive in questions about high-popularity entities. Based on those \ufb01ndings, we devise a simple, yet effective, method for powerful and ef\ufb01cient retrieval-augmented LMs, which re-trieves non-parametric memories only when necessary. Experimental results show that this signi\ufb01cantly improves models\u2019 performance while reducing the inference costs. 1",
            "year": 2022,
            "citationCount": 75,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that LMs struggle with less popular factual knowledge, and that scaling fails to appreciably improve memorization of factual knowledge in the tail, and that retrieval-augmented LMs largely outperform orders of magnitude larger LMs, while unassisted LMs remain competitive in questions about high-popularity entities."
            },
            "score": 4
        },
        {
            "id": "a8b18428a0bd00e62e28e5ebff9e1e78299c3cb8",
            "paperId": "a8b18428a0bd00e62e28e5ebff9e1e78299c3cb8",
            "title": "Deduction under Perturbed Evidence: Probing Student Simulation Capabilities of Large Language Models",
            "abstract": "We explore whether Large Language Models (LLMs) are capable of logical reasoning with distorted facts, which we call Deduction under Perturbed Evidence (DUPE). DUPE presents a unique challenge to LLMs since they typically rely on their parameters, which encode mostly accurate information, to reason and make inferences. However, in DUPE, LLMs must reason over manipulated or falsified evidence present in their prompts, which can result in false conclusions that are valid only under the manipulated evidence. Our goal with DUPE is to determine whether LLMs can arrive at these false conclusions and identify whether the dominant factor influencing the deduction process is the encoded data in the parameters or the manipulated evidence in the prompts. To evaluate the DUPE capabilities of LLMs, we create a DUPEd version of the StrategyQA dataset, where facts are manipulated to reverse the answer to the question. Our findings show that even the most advanced GPT models struggle to reason on manipulated facts - showcasing poor DUPE skills - with accuracy dropping by 45% compared to the original dataset. We also investigate prompt settings inspired from student simulation models, which mitigate the accuracy drop to some extent. Our findings have practical implications for understanding the performance of LLMs in real-world applications such as student simulation models that involve reasoning over inaccurate information.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work creates a DUPEd version of the StrategyQA dataset, where facts are manipulated to reverse the answer to the question and shows that even the most advanced GPT models struggle to reason on manipulated facts - showcasing poor DUPE skills."
            },
            "score": 4
        },
        {
            "id": "ff5eed73dca84fcc7b98142055c35dd7b8724c80",
            "paperId": "ff5eed73dca84fcc7b98142055c35dd7b8724c80",
            "title": "Prompt, Condition, and Generate: Classification of Unsupported Claims with In-Context Learning",
            "abstract": "Unsupported and unfalsifiable claims we encounter in our daily lives can influence our view of the world. Characterizing, summarizing, and -- more generally -- making sense of such claims, however, can be challenging. In this work, we focus on fine-grained debate topics and formulate a new task of distilling, from such claims, a countable set of narratives. We present a crowdsourced dataset of 12 controversial topics, comprising more than 120k arguments, claims, and comments from heterogeneous sources, each annotated with a narrative label. We further investigate how large language models (LLMs) can be used to synthesise claims using In-Context Learning. We find that generated claims with supported evidence can be used to improve the performance of narrative classification models and, additionally, that the same model can infer the stance and aspect using a few training examples. Such a model can be useful in applications which rely on narratives , e.g. fact-checking.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work focuses on fine-grained debate topics and forms a new task of distilling, from such claims, a countable set of narratives, finding that generated claims with supported evidence can be used to improve the performance of narrative classification models and, additionally, that the same model can infer the stance and aspect using a few training examples."
            },
            "score": 4
        },
        {
            "id": "e7122a71c2ce573b878d1e750593588307595442",
            "paperId": "e7122a71c2ce573b878d1e750593588307595442",
            "title": "Prompt, Condition, and Generate Classification of Unsupported Claims with In-Context Learning",
            "abstract": "Unsupported and unfalsifiable claims we encounter in our daily lives can influence our view of the world. Characterizing, summarizing, and \u2013 more generally \u2013 making sense of such claims, however, can be challenging. In this work, we focus on fine-grained debate topics and formulate a new task of distilling, from such claims, a countable set of narratives. We present a crowdsourced dataset of 12 controversial topics, comprising more than 120k arguments, claims, and comments from heterogeneous sources, each annotated with a narrative label. We further investigate how large language models (LLMs) can be used to synthe-sise claims using In-Context Learning. We find that generated claims with supported evidence can be used to improve the performance of narrative classification models and, additionally, that the same model can infer the stance and aspect using a few training examples. Such a model can be useful in applications which rely on narratives , e.g. fact-checking.",
            "year": null,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work focuses on fine-grained debate topics and forms a new task of distilling, from such claims, a countable set of narratives, finding that generated claims with supported evidence can be used to improve the performance of narrative classification models and, additionally, that the same model can infer the stance and aspect using a few training examples."
            },
            "score": 4
        },
        {
            "id": "3d7a364e83161f9a186837915a9c45083f6e1621",
            "paperId": "3d7a364e83161f9a186837915a9c45083f6e1621",
            "title": "AngleKindling: Supporting Journalistic Angle Ideation with Large Language Models",
            "abstract": "News media often leverage documents to find ideas for stories, while being critical of the frames and narratives present. Developing angles from a document such as a press release is a cognitively taxing process, in which journalists critically examine the implicit meaning of its claims. Informed by interviews with journalists, we developed AngleKindling, an interactive tool which employs the common sense reasoning of large language models to help journalists explore angles for reporting on a press release. In a study with 12 professional journalists, we show that participants found AngleKindling significantly more helpful and less mentally demanding to use for brainstorming ideas, compared to a prior journalistic angle ideation tool. AngleKindling helped journalists deeply engage with the press release and recognize angles that were useful for multiple types of stories. From our findings, we discuss how to help journalists customize and identify promising angles, and extending AngleKindling to other knowledge-work domains.",
            "year": 2023,
            "citationCount": 33,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "AngleKindling is an interactive tool which employs the common sense reasoning of large language models to help journalists explore angles for reporting on a press release, and is shown to be significantly more helpful and less mentally demanding to use for brainstorming ideas, compared to a prior journalistic angle ideation tool."
            },
            "score": 4
        },
        {
            "id": "f4db4a18080e9e4ebba9ba68bb7d34230c84d0c3",
            "paperId": "f4db4a18080e9e4ebba9ba68bb7d34230c84d0c3",
            "title": "Truth Machines: Synthesizing Veracity in AI Language Models",
            "abstract": "As AI technologies are rolled out into healthcare, academia, human resources, law, and a multitude of other domains, they become de-facto arbiters of truth. But truth is highly contested, with many different definitions and approaches. This article discusses the struggle for truth in AI systems and the general responses to date. It then investigates the production of truth in InstructGPT, a large language model, highlighting how data harvesting, model architectures, and social feedback mechanisms weave together disparate understandings of veracity. It conceptualizes this performance as an operationalization of truth, where distinct, often-conflicting claims are smoothly synthesized and confidently presented into truth-statements. We argue that these same logics and inconsistencies play out in Instruct\u2019s successor, ChatGPT, reiterating truth as a non-trivial problem. We suggest that enriching sociality and thickening \u201creality\u201d are two promising vectors for enhancing the truth-evaluating capacities of future language models. We conclude, however, by stepping back to consider AI truth-telling as a social practice: what kind of \u201ctruth\u201d do we as listeners desire?",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is suggested that enriching sociality and thickening \u201creality\u201d are two promising vectors for enhancing the truth-evaluating capacities of future language models, and is stepped back to consider AI truth-telling as a social practice."
            },
            "score": 4
        },
        {
            "id": "1321419b4e093ebd5064cd9c44b61c0d8b6c361d",
            "paperId": "1321419b4e093ebd5064cd9c44b61c0d8b6c361d",
            "title": "Probing What Different NLP Tasks Teach Machines about Function Word Comprehension",
            "abstract": "We introduce a set of nine challenge tasks that test for the understanding of function words. These tasks are created by structurally mutating sentences from existing datasets to target the comprehension of specific types of function words (e.g., prepositions, wh-words). Using these probing tasks, we explore the effects of various pretraining objectives for sentence encoders (e.g., language modeling, CCG supertagging and natural language inference (NLI)) on the learned representations. Our results show that pretraining on CCG\u2014our most syntactic objective\u2014performs the best on average across our probing tasks, suggesting that syntactic knowledge helps function word comprehension. Language modeling also shows strong performance, supporting its widespread use for pretraining state-of-the-art NLP models. Overall, no pretraining objective dominates across the board, and our function word probing tasks highlight several intuitive differences between pretraining objectives, e.g., that NLI helps the comprehension of negation.",
            "year": 2019,
            "citationCount": 97,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results show that pretraining on CCG\u2014the authors' most syntactic objective\u2014performs the best on average across their probing tasks, suggesting that syntactic knowledge helps function word comprehension."
            },
            "score": 4
        },
        {
            "id": "59f7ba3a730f94bbba685cc79dcf961c1a477e9d",
            "paperId": "59f7ba3a730f94bbba685cc79dcf961c1a477e9d",
            "title": "Counterfactual Probing Intergroup Bias for Affect and Specificity",
            "abstract": "While existing work on studying bias in NLP focues on negative or pejorative language use, Govindarajan et al. (2023) offer a revised framing of bias in terms of intergroup social context, and its effects on language behavior. In this paper, we investigate if two pragmatic features (specificity and affect) systematically vary in different intergroup contexts \u2014 thus connecting this new framing of bias to language output. Preliminary analysis finds modest correlations between specificity and affect of tweets with supervised intergroup relationship labels. Coun-terfactual probing further reveals that while neural models finetuned for predicting IGR labels reliably use affect in classification, the model\u2019s usage of specificity is inconclusive.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper investigates if two pragmatic features (specificity and affect) systematically vary in different intergroup contexts \u2014 thus connecting this new framing of bias to language output."
            },
            "score": 4
        },
        {
            "id": "04f0d2b8873f0dc7f086ed82f93091782d59d19f",
            "paperId": "04f0d2b8873f0dc7f086ed82f93091782d59d19f",
            "title": "Counterfactual Probing for the influence of affect and specificity on Intergroup Bias",
            "abstract": "While existing work on studying bias in NLP focues on negative or pejorative language use, Govindarajan et al. (2023) offer a revised framing of bias in terms of intergroup social context, and its effects on language behavior. In this paper, we investigate if two pragmatic features (specificity and affect) systematically vary in different intergroup contexts -- thus connecting this new framing of bias to language output. Preliminary analysis finds modest correlations between specificity and affect of tweets with supervised intergroup relationship (IGR) labels. Counterfactual probing further reveals that while neural models finetuned for predicting IGR labels reliably use affect in classification, the model's usage of specificity is inconclusive. Code and data can be found at: https://github.com/venkatasg/intergroup-probing",
            "year": 2023,
            "citationCount": 0,
            "tldr": null,
            "score": 4
        },
        {
            "id": "1f6ec27eb0dea48dd9fc86a6d52ab2a871ab412c",
            "paperId": "1f6ec27eb0dea48dd9fc86a6d52ab2a871ab412c",
            "title": "Are Distilled Models Just Deep-Sea Octopi? Probing Linguistic Representations of Distillation-Finetuned Models",
            "abstract": "While previous work has shown that knowledge distillation improves small-model performance on NLP benchmark tasks (Hinton et al., 2015), it is still unclear how smaller models learn from the knowledge of their teachers (Belinkov and Glass, 2019). Given that achieving a deep understanding of linguistic properties typically relies on the complexity of large language models (Tamkin et al., 2021), we explore whether DistilBERT models finetuned with distillation on natural language inference (NLI) are really learning deep language rules, or if they are simply picking up on heuristics they can use to mimic their teacher\u2019s (BERT\u2019s) outputs. We first verify that the in-distribution gains from finetuning with distillation generalize to other NLI datasets. Then, through function-word NLI probing and word-level edge probing, we demonstrate that during NLI distillation finetuning, student DistilBERT models do absorb understanding of linguistic properties from their teacher, both in positive and negative ways. In particular, we find that the gains in generalized NLI performance provided by distillation finetuning are at least partially because distillation improves DistilBERT\u2019s understanding of function words.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that the gains in generalized NLI performance provided by distillation finetuning are at least partially because distillation improves DistilBERT\u2019s understanding of function words."
            },
            "score": 4
        },
        {
            "id": "de33f154210a16605f7ce4a49137419cb4d2ed41",
            "paperId": "de33f154210a16605f7ce4a49137419cb4d2ed41",
            "title": "The Use of the Socratic Method in an English Language Learning Classroom to Develop a Global Professional Skill",
            "abstract": "In the global marketplace of the 21st century the idea of learning language specific to business exists as an operative necessity in improving the functionality of a marketable skill set. Similarly, the use of workplace specific terminology acts as a key ingredient in improving the ability to work, converse, and thrive in any environment. However, there is a gap which exists in the ability of an individual to succeed in the workplace despite fluency in business specific terminology. This gap can be remedied with a working understanding of the English language and mastery beyond instructivist methods, using a line of inquiry consistent with modern models of a professional workplace. In this paper, I will explain and discuss the benefits of using the Socratic Method in an English Language Learning course of study to cement the effectiveness and success of English vocabulary and terminology in the workplace.",
            "year": 2015,
            "citationCount": 2,
            "tldr": null,
            "score": 3
        },
        {
            "id": "9e9e4df2996bac794c4f04cb887df3e553bae4fd",
            "paperId": "9e9e4df2996bac794c4f04cb887df3e553bae4fd",
            "title": "Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning",
            "abstract": "Large Language Models (LLMs) have shown human-like reasoning abilities but still struggle with complex logical problems. This paper introduces a novel framework, Logic-LM, which integrates LLMs with symbolic solvers to improve logical problem-solving. Our method first utilizes LLMs to translate a natural language problem into a symbolic formulation. Afterward, a deterministic symbolic solver performs inference on the formulated problem. We also introduce a self-refinement module, which utilizes the symbolic solver's error messages to revise symbolic formalizations. We demonstrate Logic-LM's effectiveness on five logical reasoning datasets: ProofWriter, PrOntoQA, FOLIO, LogicalDeduction, and AR-LSAT. On average, Logic-LM achieves a significant performance boost of 39.2% over using LLM alone with standard prompting and 18.4% over LLM with chain-of-thought prompting. Our findings suggest that Logic-LM, by combining LLMs with symbolic logic, offers a promising avenue for faithful logical reasoning. Code and data are publicly available at https://github.com/teacherpeterpan/Logic-LLM.",
            "year": 2023,
            "citationCount": 59,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a novel framework, Logic-LM, which integrates LLMs with symbolic solvers to improve logical problem-solving and suggests that it offers a promising avenue for faithful logical reasoning."
            },
            "score": 3
        },
        {
            "id": "fa7805c7ad42610b89d07353cb3600f3ecaf2c2f",
            "paperId": "fa7805c7ad42610b89d07353cb3600f3ecaf2c2f",
            "title": "Exploring the Use of Large Language Models for Reference-Free Text Quality Evaluation: An Empirical Study",
            "abstract": "Evaluating the quality of generated text is a challenging task in NLP, due to the inherent complexity and diversity of text. Recently, large language models (LLMs) have garnered significant attention due to their impressive performance in various tasks. Therefore, we present this paper to investigate the effectiveness of LLMs, especially ChatGPT, and explore ways to optimize their use in assessing text quality. We compared three kinds of reference-free evaluation methods. The experimental results prove that ChatGPT is capable of evaluating text quality effectively from various perspectives without reference and demonstrates superior performance than most existing automatic metrics. In particular, the Explicit Score, which utilizes ChatGPT to generate a numeric score measuring text quality, is the most effective and reliable method among the three exploited approaches. However, directly comparing the quality of two texts may lead to suboptimal results. We believe this paper will provide valuable insights for evaluating text quality with LLMs and have released the used data.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results prove that ChatGPT is capable of evaluating text quality effectively from various perspectives without reference and demonstrates superior performance than most existing automatic metrics."
            },
            "score": 3
        },
        {
            "id": "f542c184eec4c3252d678118a7f32cf327b6f23a",
            "paperId": "f542c184eec4c3252d678118a7f32cf327b6f23a",
            "title": "A New Era in Software Security: Towards Self-Healing Software via Large Language Models and Formal Verification",
            "abstract": "In this paper we present a novel solution that combines the capabilities of Large Language Models (LLMs) with Formal Verification strategies to verify and automatically repair software vulnerabilities. Initially, we employ Bounded Model Checking (BMC) to locate the software vulnerability and derive a counterexample. The counterexample provides evidence that the system behaves incorrectly or contains a vulnerability. The counterexample that has been detected, along with the source code, are provided to the LLM engine. Our approach involves establishing a specialized prompt language for conducting code debugging and generation to understand the vulnerability's root cause and repair the code. Finally, we use BMC to verify the corrected version of the code generated by the LLM. As a proof of concept, we create ESBMC-AI based on the Efficient SMT-based Context-Bounded Model Checker (ESBMC) and a pre-trained Transformer model, specifically gpt-3.5-turbo, to detect and fix errors in C programs. Our experimentation involved generating a dataset comprising 1000 C code samples, each consisting of 20 to 50 lines of code. Notably, our proposed method achieved an impressive success rate of up to 80% in repairing vulnerable code encompassing buffer overflow and pointer dereference failures. We assert that this automated approach can effectively incorporate into the software development lifecycle's continuous integration and deployment (CI/CD) process.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel solution that combines the capabilities of Large Language Models (LLMs) with Formal Verification strategies to verify and automatically repair software vulnerabilities and it is asserted that this automated approach can effectively incorporate into the software development lifecycle's continuous integration and deployment (CI/CD) process."
            },
            "score": 3
        },
        {
            "id": "eb36681fc4c5dfce4f3e05540fc92b007de278ca",
            "paperId": "eb36681fc4c5dfce4f3e05540fc92b007de278ca",
            "title": "SelfEvolve: A Code Evolution Framework via Large Language Models",
            "abstract": "Large language models (LLMs) have already revolutionized code generation, after being pretrained on publicly available code data. However, while various methods have been proposed to augment LLMs with retrieved knowledge and enhance the quality of code generation, the performance of these retrieval-based methods is limited by the strength of the retrievers used. In addition, while LLMs show great emergent ability, they still struggle to produce the correct code in one turn. To address these challenges, we propose a novel two-step pipeline, called \\autoknow, that leverages LLMs as both knowledge providers and self-reflective programmers. Unlike retrieval-based methods, \\autoknow~obtains the knowledge from input prompts and generates intermediate code based on the generated knowledge. After that, \\autoknow~asks LLM to act as an expert programmer to perform debugging for the generated code. This is achieved by receiving the error message from the interpreter, without requiring special test cases for correctness verification. We evaluate \\autoknow~on three code generation datasets, including DS-1000 for data science code, HumanEval for software engineering code, and TransCoder for C++-to-Python translation. Our empirical experiments show that \\autoknow~outperforms strong baselines by a significant margin on all datasets. We also conduct exhaustive analytical experiments to validate the effectiveness of the two stages of \\autoknow, and find that both are superior to other prompting-based methods. Further scalability analysis demonstrates that \\autoknow~can be adapted to other more advanced models, such as GPT-4, and bring consistent efficacy improvement.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel two-step pipeline, called \\autoknow, that leverages LLMs as both knowledge providers and self-reflective programmers and finds that both are superior to other prompting-based methods."
            },
            "score": 3
        },
        {
            "id": "ddc9aeac18638575bbb90ede4c6829ec15c2947e",
            "paperId": "ddc9aeac18638575bbb90ede4c6829ec15c2947e",
            "title": "Prompting as Probing: Using Language Models for Knowledge Base Construction",
            "abstract": "Language Models (LMs) have proven to be useful in various downstream applications, such as summarisation, translation, question answering and text classification. LMs are becoming increasingly important tools in Artificial Intelligence, because of the vast quantity of information they can store. In this work, we present ProP (Prompting as Probing), which utilizes GPT-3, a large Language Model originally proposed by OpenAI in 2020, to perform the task of Knowledge Base Construction (KBC). ProP implements a multi-step approach that combines a variety of prompting techniques to achieve this. Our results show that manual prompt curation is essential, that the LM must be encouraged to give answer sets of variable lengths, in particular including empty answer sets, that true/false questions are a useful device to increase precision on suggestions generated by the LM, that the size of the LM is a crucial factor, and that a dictionary of entity aliases improves the LM score. Our evaluation study indicates that these proposed techniques can substantially enhance the quality of the final predictions: ProP won track 2 of the LM-KBC competition, outperforming the baseline by 36.4 percentage points. Our implementation is available on https://github.com/HEmile/iswc-challenge.",
            "year": 2022,
            "citationCount": 32,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ProP (Prompting as Probing), which utilizes GPT-3, a large Language Model originally proposed by OpenAI in 2020, to perform the task of Knowledge Base Construction (KBC), implements a multi-step approach that combines a variety of prompting techniques to achieve this."
            },
            "score": 3
        },
        {
            "id": "f29f8b8aa2b7e608199b65d3cf751969d4024132",
            "paperId": "f29f8b8aa2b7e608199b65d3cf751969d4024132",
            "title": "Physics of Language Models: Part 3.1, Knowledge Storage and Extraction",
            "abstract": "Large language models (LLMs) can store a vast amount of world knowledge, often extractable via question-answering (e.g.,\"What is Abraham Lincoln's birthday?\"). However, do they answer such questions based on exposure to similar questions during training (i.e., cheating), or by genuinely learning to extract knowledge from sources like Wikipedia? In this paper, we investigate this issue using a controlled biography dataset. We find a strong correlation between the model's ability to extract knowledge and various diversity measures of the training data. $\\textbf{Essentially}$, for knowledge to be reliably extracted, it must be sufficiently augmented (e.g., through paraphrasing, sentence shuffling) $\\textit{during pretraining}$. Without such augmentation, knowledge may be memorized but not extractable, leading to 0% accuracy, regardless of subsequent instruction fine-tuning. To understand why this occurs, we employ (nearly) linear probing to demonstrate a strong connection between the observed correlation and how the model internally encodes knowledge -- whether it is linearly encoded in the hidden embeddings of entity names or distributed across other token embeddings in the training text. This paper provides $\\textbf{several key recommendations for LLM pretraining in the industry}$: (1) rewrite the pretraining data -- using small, auxiliary models -- to provide knowledge augmentation, and (2) incorporate more instruction-finetuning data into the pretraining stage before it becomes too late.",
            "year": 2023,
            "citationCount": 21,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Several key recommendations for LLM pretraining in the industry are provided: rewrite the pretraining data -- using small, auxiliary models -- to provide knowledge augmentation, and incorporate more instruction-finetuning data into the pretraining stage before it becomes too late."
            },
            "score": 3
        },
        {
            "id": "11daaaedd317ae23c7de7df506572d9155017ae3",
            "paperId": "11daaaedd317ae23c7de7df506572d9155017ae3",
            "title": "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models",
            "abstract": "Reasoning about time is of fundamental importance. Many facts are time-dependent. For example, athletes change teams from time to time, and different government officials are elected periodically. Previous time-dependent question answering (QA) datasets tend to be biased in either their coverage of time spans or question types. In this paper, we introduce a comprehensive probing dataset TempReason to evaluate the temporal reasoning capability of large language models. Our dataset includes questions of three temporal reasoning levels. In addition, we also propose a novel learning framework to improve the temporal reasoning capability of large language models, based on temporal span extraction and time-sensitive reinforcement learning. We conducted experiments in closed book QA, open book QA, and reasoning QA settings and demonstrated the effectiveness of our approach.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a comprehensive probing dataset TempReason and proposes a novel learning framework to improve the temporal reasoning capability of large language models, based on temporal span extraction and time-sensitive reinforcement learning."
            },
            "score": 3
        },
        {
            "id": "cd4d112f3f9120d0715f22a9de2ce4720822368c",
            "paperId": "cd4d112f3f9120d0715f22a9de2ce4720822368c",
            "title": "How Does ChatGPT Perform on the United States Medical Licensing Examination (USMLE)? The Implications of Large Language Models for Medical Education and Knowledge Assessment",
            "abstract": "Background Chat Generative Pre-trained Transformer (ChatGPT) is a 175-billion-parameter natural language processing model that can generate conversation-style responses to user input. Objective This study aimed to evaluate the performance of ChatGPT on questions within the scope of the United States Medical Licensing Examination (USMLE) Step 1 and Step 2 exams, as well as to analyze responses for user interpretability. Methods We used 2 sets of multiple-choice questions to evaluate ChatGPT\u2019s performance, each with questions pertaining to Step 1 and Step 2. The first set was derived from AMBOSS, a commonly used question bank for medical students, which also provides statistics on question difficulty and the performance on an exam relative to the user base. The second set was the National Board of Medical Examiners (NBME) free 120 questions. ChatGPT\u2019s performance was compared to 2 other large language models, GPT-3 and InstructGPT. The text output of each ChatGPT response was evaluated across 3 qualitative metrics: logical justification of the answer selected, presence of information internal to the question, and presence of information external to the question. Results Of the 4 data sets, AMBOSS-Step1, AMBOSS-Step2, NBME-Free-Step1, and NBME-Free-Step2, ChatGPT achieved accuracies of 44% (44/100), 42% (42/100), 64.4% (56/87), and 57.8% (59/102), respectively. ChatGPT outperformed InstructGPT by 8.15% on average across all data sets, and GPT-3 performed similarly to random chance. The model demonstrated a significant decrease in performance as question difficulty increased (P=.01) within the AMBOSS-Step1 data set. We found that logical justification for ChatGPT\u2019s answer selection was present in 100% of outputs of the NBME data sets. Internal information to the question was present in 96.8% (183/189) of all questions. The presence of information external to the question was 44.5% and 27% lower for incorrect answers relative to correct answers on the NBME-Free-Step1 (P<.001) and NBME-Free-Step2 (P=.001) data sets, respectively. Conclusions ChatGPT marks a significant improvement in natural language processing models on the tasks of medical question answering. By performing at a greater than 60% threshold on the NBME-Free-Step-1 data set, we show that the model achieves the equivalent of a passing score for a third-year medical student. Additionally, we highlight ChatGPT\u2019s capacity to provide logic and informational context across the majority of answers. These facts taken together make a compelling case for the potential applications of ChatGPT as an interactive medical education tool to support learning.",
            "year": 2023,
            "citationCount": 598,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The performance of ChatGPT on questions within the scope of the United States Medical Licensing Examination (USMLE) Step 1 and Step 2 exams is evaluated to analyze responses for user interpretability and highlight ChatGPT\u2019s capacity to provide logic and informational context across the majority of answers."
            },
            "score": 3
        },
        {
            "id": "91b470af2ce5a407df43fd9039cb0d09d102ed8f",
            "paperId": "91b470af2ce5a407df43fd9039cb0d09d102ed8f",
            "title": "From Form(s) to Meaning: Probing the Semantic Depths of Language Models Using Multisense Consistency",
            "abstract": "The staggering pace with which the capabilities of large language models (LLMs) are increasing, as measured by a range of commonly used natural language understanding (NLU) benchmarks, raises many questions regarding what\"understanding\"means for a language model and how it compares to human understanding. This is especially true since many LLMs are exclusively trained on text, casting doubt on whether their stellar benchmark performances are reflective of a true understanding of the problems represented by these benchmarks, or whether LLMs simply excel at uttering textual forms that correlate with what someone who understands the problem would say. In this philosophically inspired work, we aim to create some separation between form and meaning, with a series of tests that leverage the idea that world understanding should be consistent across presentational modes - inspired by Fregean senses - of the same meaning. Specifically, we focus on consistency across languages as well as paraphrases. Taking GPT-3.5 as our object of study, we evaluate multisense consistency across five different languages and various tasks. We start the evaluation in a controlled setting, asking the model for simple facts, and then proceed with an evaluation on four popular NLU benchmarks. We find that the model's multisense consistency is lacking and run several follow-up analyses to verify that this lack of consistency is due to a sense-dependent task understanding. We conclude that, in this aspect, the understanding of LLMs is still quite far from being consistent and human-like, and deliberate on how this impacts their utility in the context of learning about human language and understanding.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The understanding of LLMs is still quite far from being consistent and human-like, and the idea that world understanding should be consistent across presentational modes - inspired by Fregean senses - of the same meaning is leveraged."
            },
            "score": 3
        },
        {
            "id": "bd9a20c03adc415ff33acf28107c154c12c7dcca",
            "paperId": "bd9a20c03adc415ff33acf28107c154c12c7dcca",
            "title": "Probing Quantifier Comprehension in Large Language Models: Another Example of Inverse Scaling",
            "abstract": "With their increasing size, large language models (LLMs) are becoming increasingly good at language understanding tasks. But even with high performance on specific downstream task, LLMs fail at simple linguistic tests for negation or quantifier understanding. Previous work on quantifier understanding in LLMs show inverse scaling in understanding few-type quantifiers. In this paper, we question the claims of of previous work and show that it is a result of inappropriate testing methodology. We also present alternate methods to measure quantifier comprehension in LLMs and show that LLMs are able to better understand the difference between the meaning of few-type and most-type quantifiers as their size increases, although they are not particularly good at it. We also observe inverse scaling for most-type quantifier understanding, which is contrary to human psycho-linguistic experiments and previous work, where the model\u2019s understanding of most-type quantifier gets worse as the model size increases. We do this evaluation on models ranging from 125M-175B parameters, which suggests that LLMs do not do as well as expected with quantifiers. We also discuss the possible reasons for this and the relevance of quantifier understanding in evaluating language understanding in LLMs.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that LLMs are able to better understand the difference between the meaning of few-type and most-type quantifiers as their size increases, although they are not particularly good at it."
            },
            "score": 3
        },
        {
            "id": "cd5bcf7729dd0fdc715519e737aa712c49630f8c",
            "paperId": "cd5bcf7729dd0fdc715519e737aa712c49630f8c",
            "title": "OpenFact at CheckThat!-2023: Head-to-Head GPT vs. BERT - A Comparative Study of Transformers Language Models for the Detection of Check-worthy Claims",
            "abstract": "This paper presents the research findings resulting from experiments conducted as part of the Check-That! Lab Task 1B-English submission at CLEF 2023. The aim of the research was to evaluate the check-worthiness of short texts in English. Various methodologies were employed, including zero-shot, few-shot, and fine-tuning techniques, and different GPT and BERT models were assessed. Given the significant increase in the use of GPT models in recent times, we posed a research question to investigate whether GPT models exhibit notable superiority over BERT models in detecting check-worthy claims. Our findings indicate that fine-tuned BERT models can perform comparably to large language models such as GPT-3 in identifying check-worthy claims for this particular task.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Findings indicate that fine-tuned BERT models can perform comparably to large language models such as GPT-3 in identifying check-worthy claims for this particular task."
            },
            "score": 3
        },
        {
            "id": "c7c6d338e47b236edea76790d2997333f497319e",
            "paperId": "c7c6d338e47b236edea76790d2997333f497319e",
            "title": "TaleBrush: Sketching Stories with Generative Pretrained Language Models",
            "abstract": "While advanced text generation algorithms (e.g., GPT-3) have enabled writers to co-create stories with an AI, guiding the narrative remains a challenge. Existing systems often leverage simple turn-taking between the writer and the AI in story development. However, writers remain unsupported in intuitively understanding the AI\u2019s actions or steering the iterative generation. We introduce TaleBrush, a generative story ideation tool that uses line sketching interactions with a GPT-based language model for control and sensemaking of a protagonist\u2019s fortune in co-created stories. Our empirical evaluation found our pipeline reliably controls story generation while maintaining the novelty of generated sentences. In a user study with 14 participants with diverse writing experiences, we found participants successfully leveraged sketching to iteratively explore and write stories according to their intentions about the character\u2019s fortune while taking inspiration from generated stories. We conclude with a reflection on how sketching interactions can facilitate the iterative human-AI co-creation process.",
            "year": 2022,
            "citationCount": 81,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "TaleBrush is introduced, a generative story ideation tool that uses line sketching interactions with a GPT-based language model for control and sensemaking of a protagonist\u2019s fortune in co-created stories and a reflection on how Sketching interactions can facilitate the iterative human-AI co-creation process."
            },
            "score": 3
        },
        {
            "id": "ce6a912dddb8d03313f72549896eaa217c056b75",
            "paperId": "ce6a912dddb8d03313f72549896eaa217c056b75",
            "title": "Probing NLP Conceptual Relatedness Judgments Through the Word-Based Board Game Codenames",
            "abstract": "In this paper I evaluate the ability of different Natural Language Processing (NLP) techniques to make human-like word relatedness judgements in a variant of the wordbased board game Codenames. I analyze a variety of statistical and knowledge based approaches, combinations of these, and techniques for incorporating the wider game context into relatedness judgements. While no approach explored here reaches human performance, simple word embedding based approaches incorporate a surprising amount of the useful information captured by other techniques. I attempt to characterize the limitations of these approaches in relation to human game play, although differences are largely not systematic. Finally, I discuss these results in terms of future directions for the field of NLP.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper evaluates the ability of different Natural Language Processing techniques to make human-like word relatedness judgements in a variant of the wordbased board game Codenames, and attempts to characterize the limitations of these approaches in relation to human game play."
            },
            "score": 3
        },
        {
            "id": "0427110f0e79f41e69a8eb00a3ec8868bac26a4f",
            "paperId": "0427110f0e79f41e69a8eb00a3ec8868bac26a4f",
            "title": "Do NLP Models Know Numbers? Probing Numeracy in Embeddings",
            "abstract": "The ability to understand and work with numbers (numeracy) is critical for many complex reasoning tasks. Currently, most NLP models treat numbers in text in the same way as other tokens\u2014they embed them as distributed vectors. Is this enough to capture numeracy? We begin by investigating the numerical reasoning capabilities of a state-of-the-art question answering model on the DROP dataset. We find this model excels on questions that require numerical reasoning, i.e., it already captures numeracy. To understand how this capability emerges, we probe token embedding methods (e.g., BERT, GloVe) on synthetic list maximum, number decoding, and addition tasks. A surprising degree of numeracy is naturally present in standard embeddings. For example, GloVe and word2vec accurately encode magnitude for numbers up to 1,000. Furthermore, character-level embeddings are even more precise\u2014ELMo captures numeracy the best for all pre-trained methods\u2014but BERT, which uses sub-word units, is less exact.",
            "year": 2019,
            "citationCount": 223,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates the numerical reasoning capabilities of a state-of-the-art question answering model on the DROP dataset and finds this model excels on questions that require numerical reasoning, i.e., it already captures numeracy."
            },
            "score": 3
        },
        {
            "id": "df2113c867c4836a12dad9c697d11654539ae35e",
            "paperId": "df2113c867c4836a12dad9c697d11654539ae35e",
            "title": "Metaphors in Pre-Trained Language Models: Probing and Generalization Across Datasets and Languages",
            "abstract": "Human languages are full of metaphorical expressions. Metaphors help people understand the world by connecting new concepts and domains to more familiar ones. Large pre-trained language models (PLMs) are therefore assumed to encode metaphorical knowledge useful for NLP systems. In this paper, we investigate this hypothesis for PLMs, by probing metaphoricity information in their encodings, and by measuring the cross-lingual and cross-dataset generalization of this information. We present studies in multiple metaphor detection datasets and in four languages (i.e., English, Spanish, Russian, and Farsi). Our extensive experiments suggest that contextual representations in PLMs do encode metaphorical knowledge, and mostly in their middle layers. The knowledge is transferable between languages and datasets, especially when the annotation is consistent across training and testing sets. Our findings give helpful insights for both cognitive and NLP scientists.",
            "year": 2022,
            "citationCount": 32,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The authors' extensive experiments suggest that contextual representations in PLMs do encode metaphorical knowledge, and mostly in their middle layers, and the knowledge is transferable between languages and datasets, especially when the annotation is consistent across training and testing sets."
            },
            "score": 3
        },
        {
            "id": "641b906b4069661122af7a26ff27921982211b31",
            "paperId": "641b906b4069661122af7a26ff27921982211b31",
            "title": "CORGI-PM: A Chinese Corpus For Gender Bias Probing and Mitigation",
            "abstract": "As natural language processing (NLP) for gender bias becomes a significant interdisciplinary topic, the prevalent data-driven techniques such as large-scale language models suffer from data inadequacy and biased corpus, especially for languages with insufficient resources such as Chinese. To this end, we propose a Chinese cOrpus foR Gender bIas Probing and Mitigation CORGI-PM, which contains 32.9k sentences with high-quality labels derived by following an annotation scheme specifically developed for gender bias in the Chinese context. Moreover, we address three challenges for automatic textual gender bias mitigation, which requires the models to detect, classify, and mitigate textual gender bias. We also conduct experiments with state-of-the-art language models to provide baselines. To our best knowledge, CORGI-PM is the first sentence-level Chinese corpus for gender bias probing and mitigation.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a Chinese corpus, CORGI-PM, which contains 32.9k sentences with high-quality labels derived by following an annotation scheme specifically developed for gender bias in the Chinese context, and addresses three challenges for automatic textual gender bias mitigation."
            },
            "score": 3
        },
        {
            "id": "201047e827ed9587158fc71256c576c8544e3dfc",
            "paperId": "201047e827ed9587158fc71256c576c8544e3dfc",
            "title": "Can BERT Refrain from Forgetting on Sequential Tasks? A Probing Study",
            "abstract": "Large pre-trained language models help to achieve state of the art on a variety of natural language processing (NLP) tasks, nevertheless, they still suffer from forgetting when incrementally learning a sequence of tasks. To alleviate this problem, recent works enhance existing models by sparse experience replay and local adaption, which yield satisfactory performance. However, in this paper we find that pre-trained language models like BERT have a potential ability to learn sequentially, even without any sparse memory replay. To verify the ability of BERT to maintain old knowledge, we adopt and re-finetune single-layer probe networks with the parameters of BERT fixed. We investigate the models on two types of NLP tasks, text classification and extractive question answering. Our experiments reveal that BERT can actually generate high quality representations for previously learned tasks in a long term, under extremely sparse replay or even no replay. We further introduce a series of novel methods to interpret the mechanism of forgetting and how memory rehearsal plays a significant role in task incremental learning, which bridges the gap between our new discovery and previous studies about catastrophic forgetting.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper finds that pre-trained language models like BERT have a potential ability to learn sequentially, even without any sparse memory replay, and investigates the models on two types of NLP tasks, text classification and extractive question answering."
            },
            "score": 3
        },
        {
            "id": "5378fea4bfec4a38b242a2ff9a88295570e67ab1",
            "paperId": "5378fea4bfec4a38b242a2ff9a88295570e67ab1",
            "title": "Probing Representations for Document-level Event Extraction",
            "abstract": "The probing classifiers framework has been employed for interpreting deep neural network models for a variety of natural language processing (NLP) applications. Studies, however, have largely focused on sentencelevel NLP tasks. This work is the first to apply the probing paradigm to representations learned for document-level information extraction (IE). We designed eight embedding probes to analyze surface, semantic, and event-understanding capabilities relevant to document-level event extraction. We apply them to the representations acquired by learning models from three different LLM-based document-level IE approaches on a standard dataset. We found that trained encoders from these models yield embeddings that can modestly improve argument detections and labeling but only slightly enhance event-level tasks, albeit trade-offs in information helpful for coherence and event-type prediction. We further found that encoder models struggle with document length and cross-sentence discourse.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work is the first to apply the probing paradigm to representations learned for document-level information extraction (IE), and designed eight embedding probes to analyze surface, semantic, and event-understanding capabilities relevant to document- level event extraction."
            },
            "score": 3
        },
        {
            "id": "dc8e82070812a4df4625c1519e502bccde3b6ebb",
            "paperId": "dc8e82070812a4df4625c1519e502bccde3b6ebb",
            "title": "Probing for Hyperbole in Pre-Trained Language Models",
            "abstract": "This paper contributes to hyperbole identification research in NLP with two probing tasks (edge and MDL probing) for 3 pre-trained language models, as well as an attempt to shed light on problems annotating hyperbole.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 3
        },
        {
            "id": "739d054af4bece04d5d6668ac10c0e3cc19a7a98",
            "paperId": "739d054af4bece04d5d6668ac10c0e3cc19a7a98",
            "title": "Investigating Semantic Subspaces of Transformer Sentence Embeddings through Linear Structural Probing",
            "abstract": "The question of what kinds of linguistic information are encoded in different layers of Transformer-based language models is of considerable interest for the NLP community. Existing work, however, has overwhelmingly focused on word-level representations and encoder-only language models with the masked-token training objective. In this paper, we present experiments with semantic structural probing, a method for studying sentence-level representations via finding a subspace of the embedding space that provides suitable task-specific pairwise distances between data-points. We apply our method to language models from different families (encoder-only, decoder-only, encoder-decoder) and of different sizes in the context of two tasks, semantic textual similarity and natural-language inference. We find that model families differ substantially in their performance and layer dynamics, but that the results are largely model-size invariant.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Semantic structural probing is presented, a method for studying sentence-level representations via finding a subspace of the embedding space that provides suitable task-specific pairwise distances between data-points in language models from different families."
            },
            "score": 3
        },
        {
            "id": "f36f6da63d131afa990789169526fc17dfaa5b2a",
            "paperId": "f36f6da63d131afa990789169526fc17dfaa5b2a",
            "title": "Testing the Effectiveness of the Diagnostic Probing Paradigm on Italian Treebanks",
            "abstract": "The outstanding performance recently reached by neural language models (NLMs) across many natural language processing (NLP) tasks has steered the debate towards understanding whether NLMs implicitly learn linguistic competence. Probes, i.e., supervised models trained using NLM representations to predict linguistic properties, are frequently adopted to investigate this issue. However, it is still questioned if probing classification tasks really enable such investigation or if they simply hint at surface patterns in the data. This work contributes to this debate by presenting an approach to assessing the effectiveness of a suite of probing tasks aimed at testing the linguistic knowledge implicitly encoded by one of the most prominent NLMs, BERT. To this aim, we compared the performance of probes when predicting gold and automatically altered values of a set of linguistic features. Our experiments were performed on Italian and were evaluated across BERT\u2019s layers and for sentences with different lengths. As a general result, we observed higher performance in the prediction of gold values, thus suggesting that the probing model is sensitive to the distortion of feature values. However, our experiments also showed that the length of a sentence is a highly influential factor that is able to confound the probing model\u2019s predictions.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents an approach to assessing the effectiveness of a suite of probing tasks aimed at testing the linguistic knowledge implicitly encoded by one of the most prominent NLMs, BERT, and observes higher performance in the prediction of gold values, thus suggesting that the probing model is sensitive to the distortion of feature values."
            },
            "score": 3
        },
        {
            "id": "94d931a253fb44b9d2f93d9287aec1bf8bdf0a4b",
            "paperId": "94d931a253fb44b9d2f93d9287aec1bf8bdf0a4b",
            "title": "On the Transformation of Latent Space in Fine-Tuned NLP Models",
            "abstract": "We study the evolution of latent space in fine-tuned NLP models. Different from the commonly used probing-framework, we opt for an unsupervised method to analyze representations. More specifically, we discover latent concepts in the representational space using hierarchical clustering. We then use an alignment function to gauge the similarity between the latent space of a pre-trained model and its fine-tuned version. We use traditional linguistic concepts to facilitate our understanding and also study how the model space transforms towards task-specific information. We perform a thorough analysis, comparing pre-trained and fine-tuned models across three models and three downstream tasks. The notable findings of our work are: i) the latent space of the higher layers evolve towards task-specific concepts, ii) whereas the lower layers retain generic concepts acquired in the pre-trained model, iii) we discovered that some concepts in the higher layers acquire polarity towards the output class, and iv) that these concepts can be used for generating adversarial triggers.",
            "year": 2022,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The latent space of the higher layers of fine-tuned NLP models evolves towards task-specific concepts, and it is discovered that some concepts in theHigher layers acquire polarity towards the output class, and that these concepts can be used for generating adversarial triggers."
            },
            "score": 3
        },
        {
            "id": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
            "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
            "title": "Language Models are Few-Shot Learners",
            "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
            "year": 2020,
            "citationCount": 24240,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic."
            },
            "score": 2
        },
        {
            "id": "0d6bb585493e34975f0437faa3179db3a02f6ae8",
            "paperId": "0d6bb585493e34975f0437faa3179db3a02f6ae8",
            "title": "Prompt-and-Rerank: A Method for Zero-Shot and Few-Shot Arbitrary Textual Style Transfer with Small Language Models",
            "abstract": "We propose a method for arbitrary textual style transfer (TST)\u2014the task of transforming a text into any given style\u2014utilizing general-purpose pre-trained language models. Our method, Prompt-and-Rerank, is based on a mathematical formulation of the TST task, decomposing it into three constituent components: textual similarity, target style strength, and fluency. Our method uses zero-shot or few-shot prompting to obtain a set of candidate generations in the target style, and then re-ranks them according to the three components. Our method enables small pre-trained language models to perform on par with state-of-the-art large-scale models while using two orders of magnitude less compute and memory. We also investigate the effect of model size and prompt design (e.g., prompt paraphrasing and delimiter-pair choice) on style transfer quality across seven diverse textual style transfer datasets, finding, among other things, that delimiter-pair choice has a large impact on performance, and that models have biases on the direction of style transfer.",
            "year": 2022,
            "citationCount": 36,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a method for arbitrary textual style transfer (TST), based on a mathematical formulation of the TST task, that enables small pre-trained language models to perform on par with state-of-the-art large-scale models while using two orders of magnitude less compute and memory."
            },
            "score": 2
        },
        {
            "id": "a1675f47125aa409525c5f759b5e6bcc1c8831aa",
            "paperId": "a1675f47125aa409525c5f759b5e6bcc1c8831aa",
            "title": "Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy",
            "abstract": "Large language models are powerful text processors and reasoners, but are still subject to limitations including outdated knowledge and hallucinations, which necessitates connecting them to the world. Retrieval-augmented large language models have raised extensive attention for grounding model generation on external knowledge. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work has proposed to improve relevance modeling by having large language models actively involved in retrieval, i.e., to improve retrieval with generation. In this paper, we show that strong performance can be achieved by a method we call Iter-RetGen, which synergizes retrieval and generation in an iterative manner. A model output shows what might be needed to finish a task, and thus provides an informative context for retrieving more relevant knowledge which in turn helps generate a better output in the next iteration. Compared with recent work which interleaves retrieval with generation when producing an output, Iter-RetGen processes all retrieved knowledge as a whole and largely preserves the flexibility in generation without structural constraints. We evaluate Iter-RetGen on multi-hop question answering, fact verification, and commonsense reasoning, and show that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval and generation. We can further improve performance via generation-augmented retrieval adaptation.",
            "year": 2023,
            "citationCount": 42,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper shows that strong performance can be achieved by a method, called Iter-RetGen, which synergizes retrieval and generation in an iterative manner, and shows that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval andgeneration."
            },
            "score": 2
        },
        {
            "id": "ccc772d88c231275f24c4fac9b28bbe0942e1107",
            "paperId": "ccc772d88c231275f24c4fac9b28bbe0942e1107",
            "title": "Query2doc: Query Expansion with Large Language Models",
            "abstract": "This paper introduces a simple yet effective query expansion approach, denoted as query2doc, to improve both sparse and dense retrieval systems. The proposed method first generates pseudo-documents by few-shot prompting large language models (LLMs), and then expands the query with generated pseudo-documents. LLMs are trained on web-scale text corpora and are adept at knowledge memorization. The pseudo-documents from LLMs often contain highly relevant information that can aid in query disambiguation and guide the retrievers. Experimental results demonstrate that query2doc boosts the performance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and TREC DL, without any model fine-tuning. Furthermore, our method also benefits state-of-the-art dense retrievers in terms of both in-domain and out-of-domain results.",
            "year": 2023,
            "citationCount": 30,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a simple yet effective query expansion approach, denoted as query2doc, to improve both sparse and dense retrieval systems, and benefits state-of-the-art dense retrievers in terms of both in-domain and out- of-domain results."
            },
            "score": 2
        },
        {
            "id": "d2d16333a4b0dc7e3463b280b9945e5ee6c53396",
            "paperId": "d2d16333a4b0dc7e3463b280b9945e5ee6c53396",
            "title": "TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models",
            "abstract": "Factual consistency evaluation is often conducted using Natural Language Inference (NLI) models, yet these models exhibit limited success in evaluating summaries. Previous work improved such models with synthetic training data. However, the data is typically based on perturbed human-written summaries, which often differ in their characteristics from real model-generated summaries and have limited coverage of possible factual errors. Alternatively, large language models (LLMs) have recently shown promising results in directly evaluating generative tasks, but are too computationally expensive for practical use. Motivated by these limitations, we introduce TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM. Unlike prior work, TrueTeacher does not rely on human-written summaries, and is multilingual by nature. Experiments on the TRUE benchmark show that a student model trained using our data, substantially outperforms both the state-of-the-art model with similar capacity, and the LLM teacher. In a systematic study, we compare TrueTeacher to existing synthetic data generation methods and demonstrate its superiority and robustness to domain-shift. We also show that our method generalizes to multilingual scenarios. Lastly, we release our large scale synthetic dataset (1.4M examples), generated using TrueTeacher, and a checkpoint trained on this data.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM, which does not rely on human-written summaries, and is multilingual by nature."
            },
            "score": 2
        },
        {
            "id": "e90c2f1b74d2108cafbdfb5d287d771bf2d6e5bd",
            "paperId": "e90c2f1b74d2108cafbdfb5d287d771bf2d6e5bd",
            "title": "Toward Auto-Modeling of Formal Verification for NextG Protocols: A Multimodal Cross- and Self-Attention Large Language Model Approach",
            "abstract": "This paper introduces Auto-modeling of Formal Verification with Real-world Prompting for 5G and NextG protocols (AVRE), a novel system designed for the formal verification of Next Generation (NextG) communication protocols, addressing the increasing complexity and scalability challenges in network protocol design and verification. Utilizing Large Language Models (LLMs), AVRE transforms protocol descriptions into dependency graphs and formal models, efficiently resolving ambiguities and capturing design intent. The system integrates a transformer model with LLMs to autonomously establish quantifiable dependency relationships through cross- and self-attention mechanisms. Enhanced by iterative feedback from the HyFuzz experimental platform, AVRE significantly advances the accuracy and relevance of formal verification in complex communication protocols, offering a groundbreaking approach to validating sophisticated communication systems. We compare CAL\u2019s performance with state-of-the-art LLM-based models and traditional time sequence models, demonstrating its superiority in accuracy and robustness, achieving an accuracy of 95.94% and an AUC of 0.98. This NLP-based approach enables, for the first time, the creation of exploits directly from design documents, making remarkable progress in scalable system verification and validation.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This NLP-based approach enables, for the first time, the creation of exploits directly from design documents, making remarkable progress in scalable system verification and validation."
            },
            "score": 2
        },
        {
            "id": "eb44ce1f7e1f4deac10f6e7009e2073f1eb0b3e4",
            "paperId": "eb44ce1f7e1f4deac10f6e7009e2073f1eb0b3e4",
            "title": "The Linear Representation Hypothesis and the Geometry of Large Language Models",
            "abstract": "Informally, the 'linear representation hypothesis' is the idea that high-level concepts are represented linearly as directions in some representation space. In this paper, we address two closely related questions: What does\"linear representation\"actually mean? And, how do we make sense of geometric notions (e.g., cosine similarity or projection) in the representation space? To answer these, we use the language of counterfactuals to give two formalizations of\"linear representation\", one in the output (word) representation space, and one in the input (sentence) space. We then prove these connect to linear probing and model steering, respectively. To make sense of geometric notions, we use the formalization to identify a particular (non-Euclidean) inner product that respects language structure in a sense we make precise. Using this causal inner product, we show how to unify all notions of linear representation. In particular, this allows the construction of probes and steering vectors using counterfactual pairs. Experiments with LLaMA-2 demonstrate the existence of linear representations of concepts, the connection to interpretation and control, and the fundamental role of the choice of inner product.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper addresses two closely related questions: What does \"linear representation\"actually mean?"
            },
            "score": 2
        },
        {
            "id": "7ed0faa6720cd176d57badbc0455af31a03f080c",
            "paperId": "7ed0faa6720cd176d57badbc0455af31a03f080c",
            "title": "Towards Expert-Level Medical Question Answering with Large Language Models",
            "abstract": "Recent artificial intelligence (AI) systems have reached milestones in\"grand challenges\"ranging from Go to protein-folding. The capability to retrieve medical knowledge, reason over it, and answer medical questions comparably to physicians has long been viewed as one such grand challenge. Large language models (LLMs) have catalyzed significant progress in medical question answering; Med-PaLM was the first model to exceed a\"passing\"score in US Medical Licensing Examination (USMLE) style questions with a score of 67.2% on the MedQA dataset. However, this and other prior work suggested significant room for improvement, especially when models' answers were compared to clinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by leveraging a combination of base LLM improvements (PaLM 2), medical domain finetuning, and prompting strategies including a novel ensemble refinement approach. Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 19% and setting a new state-of-the-art. We also observed performance approaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU clinical topics datasets. We performed detailed human evaluations on long-form questions along multiple axes relevant to clinical applications. In pairwise comparative ranking of 1066 consumer medical questions, physicians preferred Med-PaLM 2 answers to those produced by physicians on eight of nine axes pertaining to clinical utility (p<0.001). We also observed significant improvements compared to Med-PaLM on every evaluation axis (p<0.001) on newly introduced datasets of 240 long-form\"adversarial\"questions to probe LLM limitations. While further studies are necessary to validate the efficacy of these models in real-world settings, these results highlight rapid progress towards physician-level performance in medical question answering.",
            "year": 2023,
            "citationCount": 233,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Results highlight rapid progress towards physician-level performance in medical question answering by leveraging a combination of base LLM improvements (PaLM 2), medical domain finetuning, and prompting strategies including a novel ensemble refinement approach."
            },
            "score": 2
        },
        {
            "id": "9fe29c834afbe1848d9df713ae6e0ca3bd053605",
            "paperId": "9fe29c834afbe1848d9df713ae6e0ca3bd053605",
            "title": "Probing the Role of Positional Information in Vision-Language Models",
            "abstract": "In most Vision-Language models (VL), the understanding of the image structure is enabled by injecting the position information (PI) about objects in the image. In our case study of LXMERT, a state-of-the-art VL model, we probe the use of the PI in the representation and study its effect on Visual Question Answering. We show that the model is not capable of leveraging the PI for the image-text matching task on a challenge set where only position differs. Yet, our experiments with probing confirm that the PI is indeed present in the representation. We introduce two strategies to tackle this: (i) Positional Information Pre-training and (ii) Contrastive Learning on PI using Cross-Modality Matching. Doing so, the model can correctly classify if images with detailed PI statements match. Additionally to the 2D information from bounding boxes, we introduce the object's depth as new feature for a better object localization in the space. Even though we were able to improve the model properties as defined by our probes, it only has a negligible effect on the downstream performance. Our results thus highlight an important issue of multimodal modeling: the mere presence of information detectable by a probing classifier is not a guarantee that the information is available in a cross-modal setup.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This case study of LXMERT, a state-of-the-art VL model, probes the use of the PI in the representation and studies its effect on Visual Question Answering, highlighting an important issue of multimodal modeling: the mere presence of information detectable by a probing classifier is not a guarantee that the information is available in a cross-modal setup."
            },
            "score": 2
        },
        {
            "id": "6d8943bd6edf34b022862be286cfd11fff16c89d",
            "paperId": "6d8943bd6edf34b022862be286cfd11fff16c89d",
            "title": "Who\u2019s on First?: Probing the Learning and Representation Capabilities of Language Models on Deterministic Closed Domains",
            "abstract": "The capabilities of today\u2019s natural language processing systems are typically evaluated using large datasets of curated questions and answers. While these are critical benchmarks of progress, they also suffer from weakness due to artificial distributions and incomplete knowledge. Artifacts arising from artificial distributions can overstate language model performance, while incomplete knowledge limits fine-grained analysis. In this work, we introduce a complementary benchmarking approach based on SimPlified Language Activity Traces (SPLAT). SPLATs are corpora of language encodings of activity in some closed domain (we study traces from chess and baseball games in this work). SPLAT datasets use naturally-arising distributions, allow the generation of question-answer pairs at scale, and afford complete knowledge in their closed domains. We show that language models of three different architectures can answer questions about world states using only verb-like encodings of activity. Our approach is extensible to new language models and additional question-answering tasks.",
            "year": 2021,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that language models of three different architectures can answer questions about world states using only verb-like encodings of activity, which is extensible to new language models and additional question-answering tasks."
            },
            "score": 2
        },
        {
            "id": "6902fd1ed5b6da79ed3fa7842b8a8474dd0931d1",
            "paperId": "6902fd1ed5b6da79ed3fa7842b8a8474dd0931d1",
            "title": "Eliciting Latent Knowledge from Quirky Language Models",
            "abstract": "Eliciting Latent Knowledge (ELK) aims to find patterns in a capable neural network's activations that robustly track the true state of the world, especially in hard-to-verify cases where the model's output is untrusted. To further ELK research, we introduce 12 datasets and a corresponding suite of\"quirky\"language models (LMs) that are finetuned to make systematic errors when answering questions if and only if the keyword\"Bob\"is present in the prompt. We find that, especially in middle layers, linear probes usually report an LM's knowledge independently of what the LM outputs, enabling us to elicit the correct answer despite the model's untruthful output. The best probing method (logistic regression on contrast pairs) recovers 89% of the gap in AUROC between truthful and untruthful contexts, and 75% for questions harder than those used to train the probe. We also find that a mechanistic anomaly detection approach can flag untruthful behavior with 0.95 AUROC. Our results show promise for eliciting reliable knowledge from capable but untrusted models, and facilitates future research empirically investigating ELK methods.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that, especially in middle layers, linear probes usually report an LM's knowledge independently of what the LM outputs, enabling us to elicit the correct answer despite the model's untruthful output."
            },
            "score": 2
        },
        {
            "id": "9be1d1bf82f6ca6a7bf6a7d92f8f37b647e493d0",
            "paperId": "9be1d1bf82f6ca6a7bf6a7d92f8f37b647e493d0",
            "title": "Probing Pretrained Language Models for Lexical Semantics",
            "abstract": "The success of large pretrained language models (LMs) such as BERT and RoBERTa has sparked interest in probing their representations, in order to unveil what types of knowledge they implicitly capture. While prior research focused on morphosyntactic, semantic, and world knowledge, it remains unclear to which extent LMs also derive lexical type-level knowledge from words in context. In this work, we present a systematic empirical analysis across six typologically diverse languages and five different lexical tasks, addressing the following questions: 1) How do different lexical knowledge extraction strategies (monolingual versus multilingual source LM, out-of-context versus in-context encoding, inclusion of special tokens, and layer-wise averaging) impact performance? How consistent are the observed effects across tasks and languages? 2) Is lexical knowledge stored in few parameters, or is it scattered throughout the network? 3) How do these representations fare against traditional static word vectors in lexical tasks? 4) Does the lexical information emerging from independently trained monolingual LMs display latent similarities? Our main results indicate patterns and best practices that hold universally, but also point to prominent variations across languages and tasks. Moreover, we validate the claim that lower Transformer layers carry more type-level lexical knowledge, but also show that this knowledge is distributed across multiple layers.",
            "year": 2020,
            "citationCount": 190,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A systematic empirical analysis across six typologically diverse languages and five different lexical tasks indicates patterns and best practices that hold universally, but also point to prominent variations across languages and tasks."
            },
            "score": 2
        },
        {
            "id": "29c7f009df21d0112c48dec254ff80cc45fac3af",
            "paperId": "29c7f009df21d0112c48dec254ff80cc45fac3af",
            "title": "Are Emergent Abilities of Large Language Models a Mirage?",
            "abstract": "Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, emergent abilities appear due to the researcher's choice of metric rather than due to fundamental changes in model behavior with scale. Specifically, nonlinear or discontinuous metrics produce apparent emergent abilities, whereas linear or continuous metrics produce smooth, continuous predictable changes in model performance. We present our alternative explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities; (2) make, test and confirm two predictions about metric choices in a meta-analysis of emergent abilities on BIG-Bench; and (3) show to choose metrics to produce never-before-seen seemingly emergent abilities in multiple vision tasks across diverse deep networks. Via all three analyses, we provide evidence that alleged emergent abilities evaporate with different metrics or with better statistics, and may not be a fundamental property of scaling AI models.",
            "year": 2023,
            "citationCount": 162,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Evidence is provided that alleged emergent abilities evaporate with different metrics or with better statistics, and may not be a fundamental property of scaling AI models."
            },
            "score": 2
        },
        {
            "id": "702e727976e4f08c1ddc3a08cf76cd689dab2f63",
            "paperId": "702e727976e4f08c1ddc3a08cf76cd689dab2f63",
            "title": "Publicly Detectable Watermarking for Language Models",
            "abstract": "We construct the first provable watermarking scheme for language models with public detectability or verifiability: we use a private key for watermarking and a public key for watermark detection. Our protocol is the first watermarking scheme that does not embed a statistical signal in generated text. Rather, we directly embed a publicly-verifiable cryptographic signature using a form of rejection sampling. We show that our construction meets strong formal security guarantees and preserves many desirable properties found in schemes in the private-key watermarking setting. In particular, our watermarking scheme retains distortion-freeness and model agnosticity. We implement our scheme and make empirical measurements over open models in the 7B parameter range. Our experiments suggest that our watermarking scheme meets our formal claims while preserving text quality.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This protocol is the first watermarking scheme that does not embed a statistical signal in generated text, but directly embed a publicly-verifiable cryptographic signature using a form of rejection sampling."
            },
            "score": 2
        },
        {
            "id": "78104024e8b3b296956c4e2cc207c820fe7fae55",
            "paperId": "78104024e8b3b296956c4e2cc207c820fe7fae55",
            "title": "Some Languages are More Equal than Others: Probing Deeper into the Linguistic Disparity in the NLP World",
            "abstract": "Linguistic disparity in the NLP world is a problem that has been widely acknowledged recently. However, different facets of this problem, or the reasons behind this disparity are seldom discussed within the NLP community. This paper provides a comprehensive analysis of the disparity that exists within the languages of the world. We show that simply categorising languages considering data availability may not be always correct. Using an existing language categorisation based on speaker population and vitality, we analyse the distribution of language data resources, amount of NLP/CL research, inclusion in multilingual web-based platforms and the inclusion in pre-trained multilingual models. We show that many languages do not get covered in these resources or platforms, and even within the languages belonging to the same language group, there is wide disparity. We analyse the impact of family, geographical location, GDP and the speaker population of languages and provide possible reasons for this disparity, along with some suggestions to overcome the same.",
            "year": 2022,
            "citationCount": 15,
            "tldr": null,
            "score": 2
        },
        {
            "id": "ee0eb5ee1d6720cfa798658f19502b6e170cebf6",
            "paperId": "ee0eb5ee1d6720cfa798658f19502b6e170cebf6",
            "title": "Metamorphosis Knowledge Probing of Guild Data through Chat Bot Using NLP",
            "abstract": "Artificial intelligence conversational entities, are called as chat-bots. Chat-bots are computer programs capable to carry out conversation between human and computer. This application can run on local computer, but it has to be accessed through the internet. Chat Bots conversation system is an intelligent human machine interaction using natural language which will be built using voice recognition techniques and artificial intelligence algorithms that will analyze user's queries which was given by the user and understand user's input [13]. This system is an application which will provide answers to the queries of the students. The machine has been installed with learning to distinguish the sentences and settling on a choice itself as a reaction to respond to an inquiry. Natural language processing algorithms are used for Noise Removal, Term frequency, Word Embedding and Cosine similarity of the content of the complaint. The biggest advantage is that student does not have to go to college for the enquiry and anyone can access this chat bot either through voice or text. Students just have to put their query to the bot by voice or text. Questions asked to the bot, which is not understood is further processed using a third-party expert, Admin and the response is archived, improving the artificial brain capabilities for future generation of responses.",
            "year": 2019,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This system is an application which will provide answers to the queries of the students and the biggest advantage is that student does not have to go to college for the enquiry and anyone can access this chat bot either through voice or text."
            },
            "score": 2
        },
        {
            "id": "31b84f7fbd1c25d5b9bd36caa545eb38dae2a4aa",
            "paperId": "31b84f7fbd1c25d5b9bd36caa545eb38dae2a4aa",
            "title": "Is German secretly a Slavic language? What BERT probing can tell us about language groups",
            "abstract": "In the light of recent developments in NLP, the problem of understanding and interpreting large language models has gained a lot of urgency. Methods developed to study this area are subject to considerable scrutiny. In this work, we take a closer look at one such method, the structural probe introduced by Hewitt and Manning (2019). We run a series of experiments involving multiple languages, focusing principally on the group of Slavic languages. We show that probing results can be seen as a reflection of linguistic classification, and conclude that multilingual BERT learns facts about languages and their groups.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that probing results can be seen as a reflection of linguistic classification, and it is concluded that multilingual BERT learns facts about languages and their groups."
            },
            "score": 2
        },
        {
            "id": "c7bdd6c7b786fba60ff8a6e656cc14c3f7b57b78",
            "paperId": "c7bdd6c7b786fba60ff8a6e656cc14c3f7b57b78",
            "title": "Distillation-Resistant Watermarking for Model Protection in NLP",
            "abstract": "How can we protect the intellectual property of trained NLP models? Modern NLP models are prone to stealing by querying and distilling from their publicly exposed APIs. However, existing protection methods such as watermarking only work for images but are not applicable to text. We propose Distillation-Resistant Watermarking (DRW), a novel technique to protect NLP models from being stolen via distillation. DRW protects a model by injecting watermarks into the victim's prediction probability corresponding to a secret key and is able to detect such a key by probing a suspect model. We prove that a protected model still retains the original accuracy within a certain bound. We evaluate DRW on a diverse set of NLP tasks including text classification, part-of-speech tagging, and named entity recognition. Experiments show that DRW protects the original model and detects stealing suspects at 100% mean average precision for all four tasks while the prior method fails on two.",
            "year": 2022,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Distillation-Resistant Watermarking (DRW), a novel technique to protect NLP models from being stolen via distillation, is proposed and it is proved that a protected model still retains the original accuracy within a certain bound."
            },
            "score": 2
        },
        {
            "id": "2392b6d3a5cad9e5cf349169eaeee848266adf6a",
            "paperId": "2392b6d3a5cad9e5cf349169eaeee848266adf6a",
            "title": "LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models",
            "abstract": "Large language models (LLMs) have been applied in various applications due to their astonishing capabilities. With advancements in technologies such as chain-of-thought (CoT) prompting and in-context learning (ICL), the prompts fed to LLMs are becoming increasingly lengthy, even exceeding tens of thousands of tokens. To accelerate model inference and reduce cost, this paper presents LLMLingua, a coarse-to-fine prompt compression method that involves a budget controller to maintain semantic integrity under high compression ratios, a token-level iterative compression algorithm to better model the interdependence between compressed contents, and an instruction tuning based method for distribution alignment between language models. We conduct experiments and analysis over four datasets from different scenarios, i.e., GSM8K, BBH, ShareGPT, and Arxiv-March23; showing that the proposed approach yields state-of-the-art performance and allows for up to 20x compression with little performance loss. Our code is available at https://aka.ms/LLMLingua.",
            "year": 2023,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A coarse-to-fine prompt compression method that involves a budget controller to maintain semantic integrity under high compression ratios, a token-level iterative compression algorithm to better model the interdependence between compressed contents, and an instruction tuning based method for distribution alignment between language models."
            },
            "score": 1
        },
        {
            "id": "3f5b31c4f7350dc88002c121aecbdc82f86eb5bb",
            "paperId": "3f5b31c4f7350dc88002c121aecbdc82f86eb5bb",
            "title": "BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models",
            "abstract": "The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.",
            "year": 2023,
            "citationCount": 1580,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods, and is demonstrated's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions."
            },
            "score": 1
        }
    ],
    "novelty": "yes"
}