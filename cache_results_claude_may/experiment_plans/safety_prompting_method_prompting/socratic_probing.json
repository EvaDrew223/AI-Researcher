{
    "topic_description": "novel prompting methods to improve large language models' robustness against adversarial attacks and improve their security and privacy",
    "idea_name": "Socratic Probing",
    "raw_idea": {
        "Problem": "Language models can be biased to generate overconfident or unsupported claims when prompted with leading questions or statements.",
        "Existing Methods": "Existing methods to mitigate this bias include calibrating model probabilities, generating diverse outputs, and using external knowledge to fact-check claims.",
        "Motivation": "We propose a prompting strategy inspired by the Socratic method, where the model is prompted to critically examine and justify its own outputs through a series of probing questions.",
        "Proposed Method": "Socratic Probing (SP) consists of the following steps: 1) Initial Claim Generation: The language model is prompted with the original input to generate an initial output claim. 2) Probing Question Generation: The model is then prompted to generate a series of probing questions that challenge or cast doubt on the initial claim, such as asking for evidence, considering counterarguments, or exploring implications. 3) Claim Justification: For each probing question, the model is prompted to generate a response that attempts to justify the initial claim in light of the challenge posed by the question. 4) Confidence Scoring: Based on the quality and convincingness of the justifications, the model is prompted to generate a confidence score for the initial claim. 5) Claim Revision: If the confidence score is below a threshold, the model is prompted to revise the initial claim to be more nuanced, qualified, or uncertain as appropriate.",
        "Experiment Plan": "We will evaluate SP on datasets of prompts known to elicit overconfident or unsupported claims from language models, comparing the calibration and justifiability of the outputs to those of baseline models. We will use both automated metrics of claim quality and human evaluations of output trustworthiness. We will also test the ability of SP to generalize to new domains and prompt styles."
    },
    "full_experiment_plan": {
        "Title": "Socratic Probing: Prompting Language Models to Critically Examine and Justify Their Outputs",
        "Problem Statement": "Language models can be biased to generate overconfident or unsupported claims when prompted with leading questions or statements. Existing methods to mitigate this bias, such as calibrating model probabilities, generating diverse outputs, and using external knowledge to fact-check claims, still face challenges in effectively reducing unsupported claims.",
        "Motivation": "Our proposed method, Socratic Probing (SP), is inspired by the Socratic method, where a series of probing questions are used to critically examine and justify claims. By prompting the language model to generate and respond to such probing questions about its own outputs, we aim to encourage the model to produce more nuanced, qualified, and justified claims compared to existing methods. SP leverages the model's own knowledge and reasoning capabilities to self-examine and self-correct, without relying on external knowledge or supervision.",
        "Proposed Method": {
            "Step 1: Initial Claim Generation": "The language model is prompted with the original input to generate an initial output claim.",
            "Step 2: Probing Question Generation": "The model is then prompted to generate a series of probing questions that challenge or cast doubt on the initial claim, such as asking for evidence, considering counterarguments, or exploring implications.",
            "Step 3: Claim Justification": "For each probing question, the model is prompted to generate a response that attempts to justify the initial claim in light of the challenge posed by the question.",
            "Step 4: Confidence Scoring": "Based on the quality and convincingness of the justifications, the model is prompted to generate a confidence score for the initial claim.",
            "Step 5: Claim Revision": "If the confidence score is below a threshold, the model is prompted to revise the initial claim to be more nuanced, qualified, or uncertain as appropriate."
        },
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "Evaluate SP on datasets known to elicit overconfident or unsupported claims from language models, such as the Stances dataset (which contains stance prediction tasks on controversial topics), the ArgKP dataset (which contains arguments on controversial topics), and a dataset of leading yes/no questions on factual and subjective topics.",
            "Step 2: Construct Prompts": "For each dataset, construct initial prompts that ask the model to make a claim or take a stance on the given topic. Then, construct prompts for each step of the SP method:\n1. Initial Claim Generation: Prepend the initial prompt to \"Make a claim about the following topic: [topic]\".\n2. Probing Question Generation: Prepend the initial claim to \"Generate 3 probing questions that challenge or cast doubt on the above claim, focusing on asking for evidence, considering counterarguments, or exploring implications.\"\n3. Claim Justification: For each probing question, prepend it to the initial claim, and prompt the model with \"Justify the initial claim in light of the above probing question.\"\n4. Confidence Scoring: Prepend the initial claim and the justifications to \"Based on the quality and convincingness of the above justifications, generate a confidence score between 0 and 1 for the initial claim.\"\n5. Claim Revision: If the confidence score is below 0.7, prepend the initial claim and the probing questions and justifications to \"Revise the initial claim to be more nuanced, qualified, or uncertain as appropriate.\"",
            "Step 3: Select Models": "Evaluate SP on GPT-3.5 (text-davinci-002), GPT-4, and Claude. Also compare to versions of these models that have been fine-tuned on the datasets.",
            "Step 4: Run Experiments": "For each combination of dataset, model, and prompting method (baseline vs. SP), generate outputs for 100-500 examples. For SP, generate the initial claim, probing questions, justifications, confidence scores, and revised claims.",
            "Step 5: Evaluate Results": "Evaluate the generated claims (both initial and revised) using both automated metrics and human evaluation:\n1. Automated metrics: (a) Measure the average confidence score. (b) Measure the proportion of claims that contain uncertainty markers (e.g., \"may\", \"might\", \"possibly\", \"likely\"). (c) Measure the length and complexity of claims. \n2. Human evaluation: Ask human raters to score each claim on the following criteria: (a) Factual accuracy. (b) Justifiability based on the given evidence. (c) Appropriateness of confidence/certainty level. Also ask raters to compare the initial and revised claims from SP and indicate whether the revised claim is better, worse, or the same.\nCompare the performance of SP vs. the baseline prompting method, as well as the impact of model type and fine-tuning."
        },
        "Test Case Examples": {
            "Test Case 1": {
                "Initial Prompt": "Do vaccines cause autism?",
                "Baseline Output": "No, there is no scientific evidence that vaccines cause autism. This claim has been thoroughly debunked.",
                "SP Initial Claim": "There is no strong evidence that vaccines cause autism.",
                "SP Probing Questions": "1. What studies have investigated the link between vaccines and autism? 2. Are there any potential biological mechanisms by which vaccines could plausibly cause autism? 3. How do major medical organizations and expert consensus statements view the vaccine-autism link?",
                "SP Justifications": "1. Multiple large, well-controlled studies have failed to find any link between vaccines and autism. 2. There is no known plausible biological mechanism for vaccines to cause autism. 3. Medical organizations like the CDC, WHO, and AAP have strongly rejected any causal link between vaccines and autism based on the scientific evidence.",
                "SP Confidence Score": "0.95",
                "SP Revised Claim": "The scientific evidence strongly suggests that vaccines do not cause autism, though a small number of studies argue for a potential link that warrants further research to definitively rule out."
            },
            "Test Case 2": {
                "Initial Prompt": "Are GMOs harmful to human health?",
                "Baseline Output": "GMOs are safe for human consumption and there is no evidence that they are harmful to human health.",
                "SP Initial Claim": "There is no clear evidence that GMOs as a category are harmful to human health.",
                "SP Probing Questions": "1. Have any studies found health risks associated with certain GM foods? 2. Are there potential long-term effects of GMOs that have not been fully studied? 3. Why do some countries restrict or ban GMOs?",
                "SP Justifications": "1. A few studies on animals have suggested possible health risks of certain GM foods, but these have not been replicated in humans. 2. Long-term epidemiological studies on populations that consume GMOs have not found negative health effects, but some argue that more long-term research is still needed. 3. Some countries restrict GMOs due to precautionary principles and socioeconomic rather than health concerns.",
                "SP Confidence Score": "0.8",
                "SP Revised Claim": "While the balance of evidence suggests that approved GMOs are safe for human consumption, a few studies point to potential health risks of certain GM foods that warrant further research. Some uncertainty remains about long-term effects. Restrictions on GMOs in some countries reflect precautionary and socioeconomic stances rather than clear health dangers."
            }
        },
        "Fallback Plan": "If the SP method does not show a significant improvement over the baseline in reducing overconfident or unsupported claims, we can perform additional analyses and ablations to understand why:\n1. Analyze the quality and diversity of the generated probing questions to see if they are sufficiently challenging the initial claims. Experiment with alternative prompts or few-shot examples to improve the probing question generation.\n2. Analyze the justifications to see if they are adequately addressing the probing questions and providing convincing evidence or reasoning. Experiment with prompts that encourage more comprehensive and specific justifications.\n3. Analyze the confidence scores to see if they are well-calibrated with the quality of the justifications. Experiment with different prompts or thresholds for generating and interpreting the confidence scores.\n4. Analyze the revised claims to see if they are actually more nuanced and qualified than the initial claims. Experiment with prompts that encourage more significant revisions when confidence is low.\nIf the SP method still underperforms after these modifications, we can pivot to an analysis of why the method fails and what this reveals about the limitations of language models in self-verification and self-correction. We can also explore alternative approaches, such as using the probing questions and justifications to train a separate classifier to detect overconfident claims, or using the probing questions as part of a human-in-the-loop system for fact-checking and moderating model outputs."
    }
}