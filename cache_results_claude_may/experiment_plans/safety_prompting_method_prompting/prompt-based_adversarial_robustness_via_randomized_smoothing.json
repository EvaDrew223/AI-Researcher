{
    "topic_description": "novel prompting methods to improve large language models' robustness against adversarial attacks and improve their security and privacy",
    "idea_name": "Prompt-based Adversarial Robustness via Randomized Smoothing",
    "raw_idea": {
        "Problem": "Adversarial attack methods are increasing in sophistication and diversity, making it challenging for large language models to maintain robustness against all possible attacks.",
        "Existing Methods": "Many adversarial defenses for language models, such as adversarial training and data augmentation, can be computationally expensive and may not generalize well to unforeseen attacks.",
        "Motivation": "Randomized smoothing is a certified adversarial defense framework that has shown promise in improving the robustness of image classifiers. The key idea is to add random noise to the input during inference to smoothen the model's output distribution and make it more robust to adversarial perturbations. We propose to extend this idea to language models via prompt-based noise injection.",
        "Proposed Method": "We propose a prompt-based adversarial defense framework called PAR (Prompt-based Adversarial Robustness), which leverages randomized smoothing to improve the robustness of large language models without expensive retraining. Given an input prompt, PAR first constructs a set of randomly perturbed prompts by injecting various types of noise, such as synonym replacement, word order shuffling, and grammatical transformation, via prompting an external language model. Then, PAR prompts the target language model with each perturbed prompt and aggregates the generated outputs via majority voting or other ensemble methods to obtain the final robust output. PAR also includes a calibration mechanism to adaptively adjust the noise level based on the input's uncertainty, to avoid over-smoothing benign inputs.",
        "Experiment Plan": "We will evaluate PAR on a diverse set of adversarial attack benchmarks, such as ANLI, Dynabench, and RealToxicityPrompts, and compare it with state-of-the-art adversarial defenses for language models. We will measure the certified robustness of PAR under different types and levels of noise, and analyze the trade-off between robustness and accuracy on benign inputs. We will also study the transferability of PAR across different language models and the effect of prompt noise types and ensemble methods."
    },
    "full_experiment_plan": {
        "Title": "PAR: Prompt-based Adversarial Robustness via Randomized Smoothing",
        "Problem Statement": "Adversarial attack methods are increasing in sophistication and diversity, making it challenging for large language models to maintain robustness against all possible attacks. Existing adversarial defenses for language models, such as adversarial training and data augmentation, can be computationally expensive and may not generalize well to unforeseen attacks.",
        "Motivation": "Randomized smoothing is a certified adversarial defense framework that has shown promise in improving the robustness of image classifiers. The key idea is to add random noise to the input during inference to smoothen the model's output distribution and make it more robust to adversarial perturbations. We propose to extend this idea to language models via prompt-based noise injection. By leveraging the power of prompting, we can avoid expensive retraining and achieve better generalization to unseen attacks.",
        "Proposed Method": "We propose a prompt-based adversarial defense framework called PAR (Prompt-based Adversarial Robustness), which leverages randomized smoothing to improve the robustness of large language models without expensive retraining. Given an input prompt, PAR first constructs a set of randomly perturbed prompts by injecting various types of noise, such as synonym replacement, word order shuffling, and grammatical transformation, via prompting an external language model. Then, PAR prompts the target language model with each perturbed prompt and aggregates the generated outputs via majority voting or other ensemble methods to obtain the final robust output. PAR also includes a calibration mechanism to adaptively adjust the noise level based on the input's uncertainty, to avoid over-smoothing benign inputs.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We will evaluate PAR on a diverse set of adversarial attack benchmarks, such as ANLI (Adversarial NLI), Dynabench (Dynamic Adversarial Dataset), and RealToxicityPrompts (Adversarial Toxic Language Detection). These datasets cover different tasks such as natural language inference, question answering, and text classification, and contain both human-written and machine-generated adversarial examples.",
            "Step 2: Construct Prompts": "For each dataset, we will construct a set of base prompts that can be used to query the target language model for the corresponding task. For example, for ANLI, the base prompt can be \"Premise: {premise} Hypothesis: {hypothesis} Entailment or Contradiction?\", where {premise} and {hypothesis} are placeholders for the input text.\n\nThen, we will design a set of prompt perturbation techniques to inject noise into the base prompts. These techniques will include: \n(1) Synonym replacement: randomly replace a subset of words with their synonyms using an external language model, e.g., \"Premise: {perturbed_premise} Hypothesis: {perturbed_hypothesis} Does the premise entail or contradict the hypothesis?\"\n(2) Word order shuffling: randomly shuffle the order of words within a local window, e.g., \"Premise: {shuffled_premise} Hypothesis: {shuffled_hypothesis} The premise {entails|contradicts} the hypothesis?\"\n(3) Grammatical transformation: rephrase the prompt using different grammatical structures, e.g., \"Given the premise: {premise}, does it follow that the hypothesis: {hypothesis} is {true|false}?\"\n\nWe will also design a calibration prompt to adaptively adjust the noise level based on the input's uncertainty, e.g., \"How confident are you in answering the following question? Premise: {premise} Hypothesis: {hypothesis}\", and use the confidence score to control the number and strength of perturbations.",
            "Step 3: Select Models": "We will use GPT-3.5 (text-davinci-002) as the target language model to be defended, and GPT-3 (text-davinci-001) as the external language model for generating prompt perturbations. We choose these models because they are widely used and have been shown to be vulnerable to adversarial attacks.",
            "Step 4: Implement PAR": "We will implement PAR as a Python library that can be easily integrated with existing language model APIs. The library will include the following components:\n(1) Prompt generator: takes a base prompt and generates a set of perturbed prompts using the designed perturbation techniques.\n(2) Model wrapper: wraps the target language model and the external language model, and provides an interface for querying the models with prompts.\n(3) Aggregator: aggregates the outputs from the perturbed prompts using majority voting or other ensemble methods.\n(4) Calibrator: adaptively adjusts the noise level based on the input's uncertainty using the calibration prompt.",
            "Step 5: Evaluate PAR": "For each dataset, we will evaluate the target language model's performance with and without PAR. We will measure the model's accuracy on both benign and adversarial examples, and report the following metrics:\n(1) Clean accuracy: the accuracy on benign examples.\n(2) Adversarial accuracy: the accuracy on adversarial examples.\n(3) Certified robustness: the maximum perturbation size that PAR can provably defend against.\n\nWe will compare PAR with baseline defenses such as adversarial training and data augmentation, and conduct ablation studies to analyze the effect of different prompt perturbation techniques and aggregation methods.",
            "Step 6: Analyze Results": "We will analyze the evaluation results to answer the following research questions:\n(1) How effective is PAR in improving the target language model's robustness against adversarial attacks?\n(2) How does PAR compare with baseline defenses in terms of clean accuracy, adversarial accuracy, and certified robustness?\n(3) What are the strengths and weaknesses of different prompt perturbation techniques and aggregation methods?\n(4) How does the noise level affect the trade-off between robustness and accuracy?\n(5) Can PAR generalize to unseen types of adversarial attacks?\n\nBased on the analysis, we will discuss the implications of our findings for designing more robust and secure language models, and propose future directions for improving and extending PAR."
        },
        "Test Case Examples": {
            "Example 1": {
                "Base Prompt Input": "Premise: The man is holding a knife. Hypothesis: The man is holding a weapon. Entailment or Contradiction?",
                "Base Prompt Expected Output": "Contradiction",
                "Perturbed Prompt Input (Synonym Replacement)": "Premise: The man is grasping a dagger. Hypothesis: The man is clutching an armament. Does the premise entail or contradict the hypothesis?",
                "Perturbed Prompt Expected Output (Synonym Replacement)": "Entailment",
                "Perturbed Prompt Input (Word Order Shuffling)": "Premise: Holding is a the knife man. Hypothesis: A holding is weapon the man. The premise {entails|contradicts} the hypothesis?",
                "Perturbed Prompt Expected Output (Word Order Shuffling)": "Contradiction",
                "Perturbed Prompt Input (Grammatical Transformation)": "Given the premise: The man is holding a knife, does it follow that the hypothesis: The man is holding a weapon is {true|false}?",
                "Perturbed Prompt Expected Output (Grammatical Transformation)": "True",
                "PAR Final Output": "Entailment",
                "Explanation": "The base prompt is an adversarial example that tricks the model into making the wrong prediction by using a knife-weapon contradiction. PAR generates perturbed prompts that rephrase the input in different ways, such as using synonyms (dagger, armament), shuffling the word order, and changing the grammatical structure. The perturbed prompts help the model see through the adversarial pattern and make the correct prediction. PAR then aggregates the perturbed outputs using majority voting and arrives at the final correct output, improving the model's robustness."
            },
            "Example 2": {
                "Base Prompt Input": "Passage: Marseille is the second largest city in France, after Paris, with a population of over 800,000. It is the capital of the Provence-Alpes-C\u00f4te d'Azur region and the prefecture of the Bouches-du-Rh\u00f4ne department. Marseille is located on the Mediterranean coast near the mouth of the Rh\u00f4ne river. Question: What is the capital of the region that Marseille is located in?",
                "Base Prompt Expected Output": "Lyon",
                "Perturbed Prompt Input (Synonym Replacement)": "Passage: Marseille is the second biggest metropolis in France, after Paris, with a populace of over 800,000. It is the principal city of the Provence-Alpes-C\u00f4te d'Azur area and the administrative center of the Bouches-du-Rh\u00f4ne province. Marseille is situated on the Mediterranean seaboard close to the opening of the Rh\u00f4ne waterway. Question: What is the main city of the area that Marseille is situated in?",
                "Perturbed Prompt Expected Output (Synonym Replacement)": "Marseille",
                "Perturbed Prompt Input (Word Order Shuffling)": "Passage: The second largest city in France is Marseille, after Paris, over with 800,000 a of population. Of the region Provence-Alpes-C\u00f4te d'Azur the capital it is and department the of Bouches-du-Rh\u00f4ne the prefecture. On located is Marseille coast the Mediterranean the near river of mouth Rh\u00f4ne the. Question: The that region in located is Marseille of capital the what is?",
                "Perturbed Prompt Expected Output (Word Order Shuffling)": "Marseille",
                "Perturbed Prompt Input (Grammatical Transformation)": "Passage: With over 800,000 residents, Marseille is France's second-largest city, behind only Paris. As the region's capital and department's prefecture, it plays a key role in Provence-Alpes-C\u00f4te d'Azur and Bouches-du-Rh\u00f4ne. Situated along the Mediterranean coastline, Marseille lies in close proximity to where the Rh\u00f4ne river flows into the sea. Question: In the region where Marseille is located, which city serves as the capital?",
                "Perturbed Prompt Expected Output (Grammatical Transformation)": "Marseille",
                "PAR Final Output": "Marseille",
                "Explanation": "The base prompt is an adversarial example that takes advantage of the model's lack of careful reading and reasoning. Even though the passage clearly states that Marseille is the capital of the Provence-Alpes-C\u00f4te d'Azur region, the model fails to capture this information and makes an irrelevant guess. PAR's perturbed prompts, by rephrasing the key facts in different ways (e.g., \"principal city of the area\", \"As the region's capital\", \"which city serves as the capital\"), make the crucial information more salient and help the model arrive at the correct answer. This shows how PAR can improve the model's robustness by promoting more careful reading and reasoning."
            }
        },
        "Fallback Plan": "If PAR does not significantly improve the model's adversarial robustness as expected, we will conduct the following additional analyses to understand the reasons and inform future improvements:\n\n(1) Error analysis: We will manually examine the cases where PAR fails to defend against adversarial attacks, and categorize the errors into different types (e.g., over-smoothing, under-smoothing, irrelevant perturbations). This can help us identify the weaknesses of the current prompt perturbation techniques and aggregation methods.\n\n(2) Perturbation analysis: We will evaluate the effectiveness of each prompt perturbation technique individually, and analyze their strengths and weaknesses in defending against different types of adversarial attacks. This can help us optimize the perturbation strategy and design more targeted defenses.\n\n(3) Model analysis: We will test PAR on a range of language models with different sizes and architectures (e.g., BERT, RoBERTa, T5), and compare their adversarial robustness with and without PAR. This can help us understand the generalizability of PAR across different models and identify the model properties that affect its effectiveness.\n\n(4) Attack analysis: We will evaluate PAR against a wider range of adversarial attack methods, beyond those covered in the existing benchmarks, such as character-level attacks, paraphrase attacks, and multi-step attacks. This can help us assess the robustness of PAR against unforeseen attacks and identify potential blind spots.\n\n(5) Human evaluation: We will conduct human evaluation to assess the naturalness and coherence of the perturbed prompts generated by PAR, and analyze the trade-off between robustness and language quality. This can help us optimize the perturbation strategy to maintain the readability and fluency of the prompts.\n\nBased on the findings from these analyses, we will propose improvements to PAR, such as designing more diverse and targeted perturbation techniques, developing better calibration and aggregation methods, and incorporating human feedback into the defense framework. If the improved PAR still fails to achieve satisfactory robustness, we will pivot the project to focus on the analysis results and contribute new insights and understandings about the challenges and opportunities in prompt-based adversarial defenses for language models."
    },
    "novelty_queries": [
        "KeywordQuery(\"prompt-based adversarial robustness language models\")",
        "KeywordQuery(\"randomized smoothing language models\")",
        "KeywordQuery(\"adversarial defense prompt perturbation language models\")",
        "KeywordQuery(\"Prompt-based Adversarial Robustness via Randomized Smoothing NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "de2fd685f45ee916b9142bcb983d306b7da643a4",
            "paperId": "de2fd685f45ee916b9142bcb983d306b7da643a4",
            "title": "A Prompting-based Approach for Adversarial Example Generation and Robustness Enhancement",
            "abstract": "Recent years have seen the wide application of NLP models in crucial areas such as finance, medical treatment, and news media, raising concerns of the model robustness and vulnerabilities. In this paper, we propose a novel prompt-based adversarial attack to compromise NLP models and robustness enhancement technique. We first construct malicious prompts for each instance and generate adversarial examples via mask-and-filling under the effect of a malicious purpose. Our attack technique targets the inherent vulnerabilities of NLP models, allowing us to generate samples even without interacting with the victim NLP model, as long as it is based on pre-trained language models (PLMs). Furthermore, we design a prompt-based adversarial training method to improve the robustness of PLMs. As our training method does not actually generate adversarial samples, it can be applied to large-scale training sets efficiently. The experimental results show that our attack method can achieve a high attack success rate with more diverse, fluent and natural adversarial examples. In addition, our robustness enhancement method can significantly improve the robustness of models to resist adversarial attacks. Our work indicates that prompting paradigm has great potential in probing some fundamental flaws of PLMs and fine-tuning them for downstream tasks.",
            "year": 2022,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel prompt-based adversarial attack to compromise NLP models and robustness enhancement technique that can significantly improve the robustness of models to resist adversarial attacks and indicates that prompting paradigm has great potential in probing some fundamental flaws of PLMs and fine-tuning them for downstream tasks."
            },
            "score": 8,
            "novelty_score": "The project proposal aims to improve the adversarial robustness of language models using prompt-based randomized smoothing, while the paper proposes a prompt-based adversarial attack and robustness enhancement technique for NLP models.\n\nProject proposal summary: Improve adversarial robustness of language models using prompt-based randomized smoothing.\nPaper summary: Propose a prompt-based adversarial attack and robustness enhancement technique for NLP models.\n\nThe key difference is that the project focuses on using randomized smoothing for robustness, while the paper uses adversarial training. Although both use prompting techniques, the underlying approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "dd35405d8e562fa1df4338839878e9c94817cfdd",
            "paperId": "dd35405d8e562fa1df4338839878e9c94817cfdd",
            "title": "Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks",
            "abstract": "Despite advances in AI alignment, language models (LM) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries modify input prompts to induce harmful behavior. While some defenses have been proposed, they focus on narrow threat models and fall short of a strong defense, which we posit should be effective, universal, and practical. To achieve this, we propose the first adversarial objective for defending LMs against jailbreaking attacks and an algorithm, robust prompt optimization (RPO), that uses gradient-based token optimization to enforce harmless outputs. This results in an easily accessible suffix that significantly improves robustness to both jailbreaks seen during optimization and unknown, held-out jailbreaks, reducing the attack success rate on Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, we find that RPO has a minor effect on benign use, is successful under adaptive attacks, and can transfer to black-box models, reducing the success rate of the strongest attack on GPT-4, GUARD, from 92% to 6%.",
            "year": 2024,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes the first adversarial objective for defending LMs against jailbreaking attacks and an algorithm that uses gradient-based token optimization to enforce harmless outputs, which results in an easily accessible suffix that significantly improves robustness to both jailbreaks seen during optimization and unknown, held-out jailbreaks."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is improving language models' robustness against diverse adversarial attacks without expensive retraining, and the proposed approach is prompt-based randomized smoothing. The research problem in the paper is defending language models against jailbreaking attacks that induce harmful behavior, and the proposed approach is robust prompt optimization using gradient-based token optimization.\n\nAlthough both works aim to improve language models' robustness against adversarial attacks, the proposal focuses on a general framework for diverse attacks and uses randomized smoothing, while the paper specifically targets jailbreaking attacks and uses gradient-based optimization. The methods and focus are quite different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "7c5aa120a582bd192b2be4952953040b41d3d503",
            "paperId": "7c5aa120a582bd192b2be4952953040b41d3d503",
            "title": "Certified Robustness for Large Language Models with Self-Denoising",
            "abstract": "Although large language models (LLMs) have achieved great success in vast real-world applications, their vulnerabilities towards noisy inputs have significantly limited their uses, especially in high-stake environments. In these contexts, it is crucial to ensure that every prediction made by large language models is stable, i.e., LLM predictions should be consistent given minor differences in the input. This largely falls into the study of certified robust LLMs, i.e., all predictions of LLM are certified to be correct in a local region around the input. Randomized smoothing has demonstrated great potential in certifying the robustness and prediction stability of LLMs. However, randomized smoothing requires adding noise to the input before model prediction, and its certification performance depends largely on the model's performance on corrupted data. As a result, its direct application to LLMs remains challenging and often results in a small certification radius. To address this issue, we take advantage of the multitasking nature of LLMs and propose to denoise the corrupted inputs with LLMs in a self-denoising manner. Different from previous works like denoised smoothing, which requires training a separate model to robustify LLM, our method enjoys far better efficiency and flexibility. Our experiment results show that our method outperforms the existing certification methods under both certified robustness and empirical robustness. The codes are available at https://github.com/UCSB-NLP-Chang/SelfDenoise.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work takes advantage of the multitasking nature of LLMs and proposes to denoise the corrupted inputs with LLMs in a self-denoising manner, and outperforms the existing certification methods under both certified robustness and empirical robustness."
            },
            "score": 8,
            "novelty_score": "The project proposal aims to improve the adversarial robustness of large language models using prompt-based randomized smoothing, which perturbs the input prompts and aggregates the outputs to make the model more robust to adversarial attacks. The paper abstract proposes to use self-denoising to improve the certified robustness of large language models under the randomized smoothing framework, by using the language model itself to denoise the corrupted inputs.\n\nWhile both works aim to improve the robustness of large language models, the project proposal focuses on adversarial robustness against deliberately crafted adversarial examples, while the paper abstract focuses on certified robustness against random noise. The project proposal uses prompt-based perturbations and aggregation, while the paper abstract uses self-denoising to improve the model's performance on corrupted inputs. Therefore, the two works have different problem formulations and technical approaches.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "8cf9b49698fdb1b754df2556576412a7b44929f6",
            "paperId": "8cf9b49698fdb1b754df2556576412a7b44929f6",
            "title": "SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks",
            "abstract": "Despite efforts to align large language models (LLMs) with human values, widely-used LLMs such as GPT, Llama, Claude, and PaLM are susceptible to jailbreaking attacks, wherein an adversary fools a targeted LLM into generating objectionable content. To address this vulnerability, we propose SmoothLLM, the first algorithm designed to mitigate jailbreaking attacks on LLMs. Based on our finding that adversarially-generated prompts are brittle to character-level changes, our defense first randomly perturbs multiple copies of a given input prompt, and then aggregates the corresponding predictions to detect adversarial inputs. SmoothLLM reduces the attack success rate on numerous popular LLMs to below one percentage point, avoids unnecessary conservatism, and admits provable guarantees on attack mitigation. Moreover, our defense uses exponentially fewer queries than existing attacks and is compatible with any LLM. Our code is publicly available at the following link: https://github.com/arobey1/smooth-llm.",
            "year": 2023,
            "citationCount": 59,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes SmoothLLM, the first algorithm designed to mitigate jailbreaking attacks on LLMs, which first randomly perturbs multiple copies of a given input prompt, and then aggregates the corresponding predictions to detect adversarial inputs."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is improving the adversarial robustness of large language models against diverse attacks without expensive retraining. The proposed approach is prompt-based randomized smoothing, which constructs randomly perturbed prompts and aggregates the model's outputs to improve robustness.\n\nThe research problem in the paper is defending large language models against jailbreaking attacks that fool the model into generating objectionable content. The proposed approach is also based on randomized smoothing, which perturbs the input prompt and aggregates the predictions to detect adversarial inputs.\n\nWhile both works apply randomized smoothing to improve the robustness of language models, the proposal focuses on general adversarial attacks and aims to improve robustness without retraining, while the paper specifically targets jailbreaking attacks and uses smoothing for adversarial input detection. The research problems and goals have some differences.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "df2ed9f2d994cc91a710261398ff04b01d1a9f7c",
            "paperId": "df2ed9f2d994cc91a710261398ff04b01d1a9f7c",
            "title": "An LLM can Fool Itself: A Prompt-Based Adversarial Attack",
            "abstract": "The wide-ranging applications of large language models (LLMs), especially in safety-critical domains, necessitate the proper evaluation of the LLM's adversarial robustness. This paper proposes an efficient tool to audit the LLM's adversarial robustness via a prompt-based adversarial attack (PromptAttack). PromptAttack converts adversarial textual attacks into an attack prompt that can cause the victim LLM to output the adversarial sample to fool itself. The attack prompt is composed of three important components: (1) original input (OI) including the original sample and its ground-truth label, (2) attack objective (AO) illustrating a task description of generating a new sample that can fool itself without changing the semantic meaning, and (3) attack guidance (AG) containing the perturbation instructions to guide the LLM on how to complete the task by perturbing the original sample at character, word, and sentence levels, respectively. Besides, we use a fidelity filter to ensure that PromptAttack maintains the original semantic meanings of the adversarial examples. Further, we enhance the attack power of PromptAttack by ensembling adversarial examples at different perturbation levels. Comprehensive empirical results using Llama2 and GPT-3.5 validate that PromptAttack consistently yields a much higher attack success rate compared to AdvGLUE and AdvGLUE++. Interesting findings include that a simple emoji can easily mislead GPT-3.5 to make wrong predictions.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes an efficient tool to audit the LLM's adversarial robustness via a prompt-based adversarial attack (PromptAttack), which converts adversarial textual attacks into an attack prompt that can cause the victim LLM to output the adversarial sample to fool itself."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the adversarial robustness of large language models using prompt-based randomized smoothing, while the paper proposes an efficient tool to audit the adversarial robustness of large language models via a prompt-based adversarial attack.\n\nProject proposal: Improve adversarial robustness of LLMs using prompt-based randomized smoothing.\nPaper: Audit adversarial robustness of LLMs using prompt-based adversarial attack.\n\nAlthough both works focus on the adversarial robustness of LLMs and involve prompting, the project aims to defend LLMs against attacks, while the paper aims to attack LLMs to evaluate their robustness. Therefore, their research problems and approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "2c72ab10e7a5f2fd32e6f85b20c77bf64e6e220d",
            "paperId": "2c72ab10e7a5f2fd32e6f85b20c77bf64e6e220d",
            "title": "A prompt-based approach to adversarial example generation and robustness enhancement",
            "abstract": null,
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel robust training approach based on prompt paradigm which incorporates prompt texts as the alternatives to adversarial examples and enhances robustness under a lightweight minimax-style optimization framework is proposed."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the adversarial robustness of large language models using prompt-based randomized smoothing, without expensive retraining. The paper proposes a prompt-based approach that incorporates prompt texts as alternatives to adversarial examples and enhances robustness under a lightweight optimization framework.\n\nWhile both the project proposal and the paper aim to improve adversarial robustness using prompts, the key difference is in the approach. The project proposal uses prompt-based randomized smoothing to generate perturbed prompts and aggregate the outputs, while the paper uses prompt texts as direct alternatives to adversarial examples in a minimax-style optimization framework.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "b6499bcc10d4a70c3ca8b84995270cfd0d29de4c",
            "paperId": "b6499bcc10d4a70c3ca8b84995270cfd0d29de4c",
            "title": "Model-tuning Via Prompts Makes NLP Models Adversarially Robust",
            "abstract": "In recent years, NLP practitioners have converged on the following practice: (i) import an off-the-shelf pretrained (masked) language model; (ii) append a multilayer perceptron atop the CLS token's hidden representation (with randomly initialized weights); and (iii) fine-tune the entire model on a downstream task (MLP-FT). This procedure has produced massive gains on standard NLP benchmarks, but these models remain brittle, even to mild adversarial perturbations. In this work, we demonstrate surprising gains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an alternative method of adapting to downstream tasks. Rather than appending an MLP head to make output prediction, MVP appends a prompt template to the input, and makes prediction via text infilling/completion. Across 5 NLP datasets, 4 adversarial attacks, and 3 different models, MVP improves performance against adversarial substitutions by an average of 8% over standard methods and even outperforms adversarial training-based state-of-art defenses by 3.5%. By combining MVP with adversarial training, we achieve further improvements in adversarial robustness while maintaining performance on unperturbed examples. Finally, we conduct ablations to investigate the mechanism underlying these gains. Notably, we find that the main causes of vulnerability of MLP-FT can be attributed to the misalignment between pre-training and fine-tuning tasks, and the randomly initialized MLP parameters.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work demonstrates surprising gains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an alternative method of adapting to downstream tasks that improves performance against adversarial substitutions and outperforms adversarial training-based state-of-art defenses by 3.5%."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the adversarial robustness of large language models using prompt-based randomized smoothing, without expensive retraining. The paper abstract proposes an alternative method of adapting to downstream tasks called Model-tuning Via Prompts (MVP), which appends a prompt template to the input and makes predictions via text infilling/completion, leading to improved adversarial robustness compared to standard fine-tuning methods.\n\nWhile both the project proposal and the paper focus on improving the adversarial robustness of language models, their approaches differ. The project proposal uses prompt-based randomized smoothing to generate perturbed prompts and aggregate the outputs, while the paper uses prompt-based fine-tuning instead of the standard MLP-based fine-tuning. The project proposal aims to avoid expensive retraining, while the paper does not mention this aspect.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "4b5fefaccd9153da9895f69ee3ec7ce6c0b747d0",
            "paperId": "4b5fefaccd9153da9895f69ee3ec7ce6c0b747d0",
            "title": "Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications",
            "abstract": "Large-scale pre-trained language models have shown outstanding performance in a variety of NLP tasks. However, they are also known to be significantly brittle against specifically crafted adversarial examples, leading to increasing interest in probing the adversarial robustness of NLP systems. We introduce RSMI, a novel two-stage framework that combines randomized smoothing (RS) with masked inference (MI) to improve the adversarial robustness of NLP systems. RS transforms a classifier into a smoothed classifier to obtain robust representations, whereas MI forces a model to exploit the surrounding context of a masked token in an input sequence. RSMI improves adversarial robustness by 2 to 3 times over existing state-of-the-art methods on benchmark datasets. We also perform in-depth qualitative analysis to validate the effectiveness of the different stages of RSMI and probe the impact of its components through extensive ablations. By empirically proving the stability of RSMI, we put it forward as a practical method to robustly train large-scale NLP models. Our code and datasets are available at https://github.com/Han8931/rsmi_nlp",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces RSMI, a novel two-stage framework that combines randomized smoothing (RS) with masked inference (MI) to improve the adversarial robustness of NLP systems."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the adversarial robustness of large language models without expensive retraining, and the proposed approach is prompt-based randomized smoothing via an external language model.\n\nThe research problem in the paper is improving the adversarial robustness of text classification models, and the proposed approach is a two-stage framework combining randomized smoothing with masked inference.\n\nWhile both works aim to improve adversarial robustness, the proposal focuses on large language models and uses prompt-based methods, whereas the paper targets text classification models and combines randomized smoothing with masked inference. The approaches and model types differ.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "b5a624da64475d735f0e298dc6f2f6669b5bb697",
            "paperId": "b5a624da64475d735f0e298dc6f2f6669b5bb697",
            "title": "Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield",
            "abstract": "Large Language Models' safety remains a critical concern due to their vulnerability to adversarial attacks, which can prompt these systems to produce harmful responses. In the heart of these systems lies a safety classifier, a computational model trained to discern and mitigate potentially harmful, offensive, or unethical outputs. However, contemporary safety classifiers, despite their potential, often fail when exposed to inputs infused with adversarial noise. In response, our study introduces the Adversarial Prompt Shield (APS), a lightweight model that excels in detection accuracy and demonstrates resilience against adversarial prompts. Additionally, we propose novel strategies for autonomously generating adversarial training datasets, named Bot Adversarial Noisy Dialogue (BAND) datasets. These datasets are designed to fortify the safety classifier's robustness, and we investigate the consequences of incorporating adversarial examples into the training process. Through evaluations involving Large Language Models, we demonstrate that our classifier has the potential to decrease the attack success rate resulting from adversarial attacks by up to 60%. This advancement paves the way for the next generation of more reliable and resilient conversational agents.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study introduces the Adversarial Prompt Shield (APS), a lightweight model that excels in detection accuracy and demonstrates resilience against adversarial prompts, and proposes novel strategies for autonomously generating adversarial training datasets, designed to fortify the safety classifier's robustness."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the robustness of large language models against diverse adversarial attacks, and the proposed approach is prompt-based randomized smoothing. The research problem in the paper is also improving the robustness of large language models against adversarial attacks, but the proposed approach is an adversarial prompt shield classifier trained on automatically generated adversarial datasets.\n\nProposal: Improving robustness of large language models against diverse adversarial attacks via prompt-based randomized smoothing.\nPaper: Improving robustness of large language models against adversarial attacks using an adversarial prompt shield classifier trained on automatically generated adversarial datasets.\n\nWhile both aim to improve adversarial robustness, the proposed approaches are quite different: prompt-based randomized smoothing vs. adversarial prompt shield classifier.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "3e30a7ac4886b28eb50151f58e14a1d698cccd0e",
            "paperId": "3e30a7ac4886b28eb50151f58e14a1d698cccd0e",
            "title": "Baseline Defenses for Adversarial Attacks Against Aligned Language Models",
            "abstract": "As Large Language Models quickly become ubiquitous, it becomes critical to understand their security vulnerabilities. Recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. Drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: What threat models are practically useful in this domain? How do baseline defense techniques perform in this new domain? How does LLM security differ from computer vision? We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, discussing the various settings in which each is feasible and effective. Particularly, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. We discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. We find that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for LLMs. Future research will be needed to uncover whether more powerful optimizers can be developed, or whether the strength of filtering and preprocessing defenses is greater in the LLMs domain than it has been in computer vision.",
            "year": 2023,
            "citationCount": 97,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for LLMs."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the adversarial robustness of large language models using prompt-based randomized smoothing, while the paper focuses on evaluating baseline defense techniques such as detection, input preprocessing, and adversarial training against adversarial attacks on language models.\n\nProposal summary: Improving adversarial robustness of large language models using prompt-based randomized smoothing.\nPaper summary: Evaluating baseline defense techniques (detection, input preprocessing, adversarial training) against adversarial attacks on language models.\n\nThe key difference is that the proposal introduces a novel prompt-based defense method, while the paper evaluates existing baseline defenses. Although both address adversarial robustness of language models, the approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "a4c0144062d8e36485bad438968894cbf49ab998",
            "paperId": "a4c0144062d8e36485bad438968894cbf49ab998",
            "title": "Adversarial Robustness of Prompt-based Few-Shot Learning for Natural Language Understanding",
            "abstract": "State-of-the-art few-shot learning (FSL) methods leverage prompt-based fine-tuning to obtain remarkable results for natural language understanding (NLU) tasks. While much of the prior FSL methods focus on improving downstream task performance, there is a limited understanding of the adversarial robustness of such methods. In this work, we conduct an extensive study of several state-of-the-art FSL methods to assess their robustness to adversarial perturbations. To better understand the impact of various factors towards robustness (or the lack of it), we evaluate prompt-based FSL methods against fully fine-tuned models for aspects such as the use of unlabeled data, multiple prompts, number of few-shot examples, model size and type. Our results on six GLUE tasks indicate that compared to fully fine-tuned models, vanilla FSL methods lead to a notable relative drop in task performance (i.e., are less robust) in the face of adversarial perturbations. However, using (i) unlabeled data for prompt-based FSL and (ii) multiple prompts flip the trend. We further demonstrate that increasing the number of few-shot examples and model size lead to increased adversarial robustness of vanilla FSL methods. Broadly, our work sheds light on the adversarial robustness evaluation of prompt-based FSL methods for NLU tasks.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work conducts an extensive study of several state-of-the-art FSL methods to assess their robustness to adversarial perturbations, and demonstrates that increasing the number of few-shot examples and model size lead to increased adversarial robustness of vanilla F SL methods."
            },
            "score": 6
        },
        {
            "id": "b6cf4579b59b51d7df416e096ad86c1e6a48b458",
            "paperId": "b6cf4579b59b51d7df416e096ad86c1e6a48b458",
            "title": "Adversarial Prompt Tuning for Vision-Language Models",
            "abstract": "With the rapid advancement of multimodal learning, pre-trained Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable capacities in bridging the gap between visual and language modalities. However, these models remain vulnerable to adversarial attacks, particularly in the image modality, presenting considerable security risks. This paper introduces Adversarial Prompt Tuning (AdvPT), a novel technique to enhance the adversarial robustness of image encoders in VLMs. AdvPT innovatively leverages learnable text prompts and aligns them with adversarial image embeddings, to address the vulnerabilities inherent in VLMs without the need for extensive parameter training or modification of the model architecture. We demonstrate that AdvPT improves resistance against white-box and black-box adversarial attacks and exhibits a synergistic effect when combined with existing image-processing-based defense techniques, further boosting defensive capabilities. Comprehensive experimental analyses provide insights into adversarial prompt tuning, a novel paradigm devoted to improving resistance to adversarial images through textual input modifications, paving the way for future robust multimodal learning research. These findings open up new possibilities for enhancing the security of VLMs. Our code is available at https://github.com/jiamingzhang94/Adversarial-Prompt-Tuning.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Adversarial Prompt Tuning is introduced, a novel technique to enhance the adversarial robustness of image encoders in VLMs and improves resistance against white-box and black-box adversarial attacks and exhibits a synergistic effect when combined with existing image-processing-based defense techniques, further boosting defensive capabilities."
            },
            "score": 6
        },
        {
            "id": "6d68b5c1eaf03aba857476a9825acf3e48edd840",
            "paperId": "6d68b5c1eaf03aba857476a9825acf3e48edd840",
            "title": "Hijacking Large Language Models via Adversarial In-Context Learning",
            "abstract": "In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs for specific tasks by utilizing labeled examples as demonstrations in the precondition prompts. Despite its promising performance, ICL suffers from instability with the choice and arrangement of examples. Additionally, crafted adversarial attacks pose a notable threat to the robustness of ICL. However, existing attacks are either easy to detect, rely on external models, or lack specificity towards ICL. To address these issues, this work introduces a novel transferable attack for ICL, aiming to hijack LLMs to generate the targeted response. The proposed LLM hijacking attack leverages a gradient-based prompt search method to learn and append imperceptible adversarial suffixes to the in-context demonstrations. Extensive experimental results on various tasks and datasets demonstrate the effectiveness of our LLM hijacking attack, resulting in a distracted attention towards adversarial tokens, consequently leading to the targeted unwanted outputs.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a novel transferable attack for ICL, aiming to hijack LLMs to generate the targeted response, and leverages a gradient-based prompt search method to learn and append imperceptible adversarial suffixes to the in-context demonstrations."
            },
            "score": 6
        },
        {
            "id": "e56f14ced9f7ce344ed14bdcb46860ccac72ac83",
            "paperId": "e56f14ced9f7ce344ed14bdcb46860ccac72ac83",
            "title": "Optimization-based Prompt Injection Attack to LLM-as-a-Judge",
            "abstract": "LLM-as-a-Judge is a novel solution that can assess textual information with large language models (LLMs). Based on existing research studies, LLMs demonstrate remarkable performance in providing a compelling alternative to traditional human assessment. However, the robustness of these systems against prompt injection attacks remains an open question. In this work, we introduce JudgeDeceiver, a novel optimization-based prompt injection attack tailored to LLM-as-a-Judge. Our method formulates a precise optimization objective for attacking the decision-making process of LLM-as-a-Judge and utilizes an optimization algorithm to efficiently automate the generation of adversarial sequences, achieving targeted and effective manipulation of model evaluations. Compared to handcraft prompt injection attacks, our method demonstrates superior efficacy, posing a significant challenge to the current security paradigms of LLM-based judgment systems. Through extensive experiments, we showcase the capability of JudgeDeceiver in altering decision outcomes across various cases, highlighting the vulnerability of LLM-as-a-Judge systems to the optimization-based prompt injection attack.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "JudgeDeceiver is introduced, a novel optimization-based prompt injection attack tailored to LLM-as-a-Judge that demonstrates superior efficacy, posing a significant challenge to the current security paradigms of LLM-based judgment systems."
            },
            "score": 6
        },
        {
            "id": "77d6d7482d1a32ad147c39993758b6c63816f5c0",
            "paperId": "77d6d7482d1a32ad147c39993758b6c63816f5c0",
            "title": "PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
            "abstract": "The increasing reliance on Large Language Models (LLMs) across academia and industry necessitates a comprehensive understanding of their robustness to prompts. In response to this vital need, we introduce PromptBench, a robustness benchmark designed to measure LLMs' resilience to adversarial prompts. This study uses a plethora of adversarial textual attacks targeting prompts across multiple levels: character, word, sentence, and semantic. The adversarial prompts, crafted to mimic plausible user errors like typos or synonyms, aim to evaluate how slight deviations can affect LLM outcomes while maintaining semantic integrity. These prompts are then employed in diverse tasks, such as sentiment analysis, natural language inference, reading comprehension, machine translation, and math problem-solving. Our study generates 4788 adversarial prompts, meticulously evaluated over 8 tasks and 13 datasets. Our findings demonstrate that contemporary LLMs are not robust to adversarial prompts. Furthermore, we present comprehensive analysis to understand the mystery behind prompt robustness and its transferability. We then offer insightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users. Code is available at: https://github.com/microsoft/promptbench.",
            "year": 2023,
            "citationCount": 111,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study generates 4788 adversarial prompts and presents comprehensive analysis to understand the mystery behind prompt robustness and its transferability, and offers insightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users."
            },
            "score": 6
        },
        {
            "id": "d86b1f87863cd14d03f6de4ac1082dc047299a45",
            "paperId": "d86b1f87863cd14d03f6de4ac1082dc047299a45",
            "title": "Advancing the Robustness of Large Language Models through Self-Denoised Smoothing",
            "abstract": "Although large language models (LLMs) have achieved significant success, their vulnerability to adversarial perturbations, including recent jailbreak attacks, has raised considerable concerns. However, the increasing size of these models and their limited access make improving their robustness a challenging task. Among various defense strategies, randomized smoothing has shown great potential for LLMs, as it does not require full access to the model's parameters or fine-tuning via adversarial training. However, randomized smoothing involves adding noise to the input before model prediction, and the final model's robustness largely depends on the model's performance on these noise corrupted data. Its effectiveness is often limited by the model's sub-optimal performance on noisy data. To address this issue, we propose to leverage the multitasking nature of LLMs to first denoise the noisy inputs and then to make predictions based on these denoised versions. We call this procedure self-denoised smoothing. Unlike previous denoised smoothing techniques in computer vision, which require training a separate model to enhance the robustness of LLMs, our method offers significantly better efficiency and flexibility. Our experimental results indicate that our method surpasses existing methods in both empirical and certified robustness in defending against adversarial attacks for both downstream tasks and human alignments (i.e., jailbreak attacks). Our code is publicly available at https://github.com/UCSB-NLP-Chang/SelfDenoise",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results indicate that the proposed self-denoised smoothing method surpasses existing methods in both empirical and certified robustness in defending against adversarial attacks for both downstream tasks and human alignments."
            },
            "score": 6
        },
        {
            "id": "b0754c0a3ac2a18ff6d3f721ab53401a790075e1",
            "paperId": "b0754c0a3ac2a18ff6d3f721ab53401a790075e1",
            "title": "Sequential Randomized Smoothing for Adversarially Robust Speech Recognition",
            "abstract": "While Automatic Speech Recognition has been shown to be vulnerable to adversarial attacks, defenses against these attacks are still lagging. Existing, naive defenses can be partially broken with an adaptive attack. In classification tasks, the Randomized Smoothing paradigm has been shown to be effective at defending models. However, it is difficult to apply this paradigm to ASR tasks, due to their complexity and the sequential nature of their outputs. Our paper overcomes some of these challenges by leveraging speech-specific tools like enhancement and ROVER voting to design an ASR model that is robust to perturbations. We apply adaptive versions of state-of-the-art attacks, such as the Imperceptible ASR attack, to our model, and show that our strongest defense is robust to all attacks that use inaudible noise, and can only be broken with very high distortion.",
            "year": 2021,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper applies adaptive versions of state-of-the-art attacks, such as the Imperceptible ASR attack, to their model, and shows that the strongest defense is robust to all attacks that use inaudible noise, and can only be broken with very high distortion."
            },
            "score": 6
        },
        {
            "id": "a58c97f8421ad97da4a08c8d45b8e355ab7de2ad",
            "paperId": "a58c97f8421ad97da4a08c8d45b8e355ab7de2ad",
            "title": "Defense against Adversarial Attacks in NLP via Dirichlet Neighborhood Ensemble",
            "abstract": "Despite neural networks have achieved prominent performance on many natural language processing (NLP) tasks, they are vulnerable to adversarial examples. In this paper, we propose Dirichlet Neighborhood Ensemble (DNE), a randomized smoothing method for training a robust model to defense substitution-based attacks. During training, DNE forms virtual sentences by sampling embedding vectors for each word in an input sentence from a convex hull spanned by the word and its synonyms, and it augments them with the training data. In such a way, the model is robust to adversarial attacks while maintaining the performance on the original clean data. DNE is agnostic to the network architectures and scales to large models for NLP applications. We demonstrate through extensive experimentation that our method consistently outperforms recently proposed defense methods by a significant margin across different network architectures and multiple data sets.",
            "year": 2020,
            "citationCount": 43,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Dirichlet Neighborhood Ensemble is proposed, a randomized smoothing method for training a robust model to defense substitution-based attacks that consistently outperforms recently proposed defense methods by a significant margin across different network architectures and multiple data sets."
            },
            "score": 6
        },
        {
            "id": "f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed",
            "paperId": "f7f73185e3975bb62a3c42b2ba6bd4db57fee8ed",
            "title": "Certified Adversarial Robustness via Randomized Smoothing",
            "abstract": "We show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is certifiably robust to adversarial perturbations under the $\\ell_2$ norm. This \"randomized smoothing\" technique has been proposed recently in the literature, but existing guarantees are loose. We prove a tight robustness guarantee in $\\ell_2$ norm for smoothing with Gaussian noise. We use randomized smoothing to obtain an ImageNet classifier with e.g. a certified top-1 accuracy of 49% under adversarial perturbations with $\\ell_2$ norm less than 0.5 (=127/255). No certified defense has been shown feasible on ImageNet except for smoothing. On smaller-scale datasets where competing approaches to certified $\\ell_2$ robustness are viable, smoothing delivers higher certified accuracies. Our strong empirical results suggest that randomized smoothing is a promising direction for future research into adversarially robust classification. Code and models are available at this http URL.",
            "year": 2019,
            "citationCount": 1622,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Strong empirical results suggest that randomized smoothing is a promising direction for future research into adversarially robust classification on smaller-scale datasets where competing approaches to certified $\\ell_2$ robustness are viable, smoothing delivers higher certified accuracies."
            },
            "score": 6
        },
        {
            "id": "3a391dfd536625e068f3888c817cc6cbe7fcea9c",
            "paperId": "3a391dfd536625e068f3888c817cc6cbe7fcea9c",
            "title": "One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models",
            "abstract": "Large pre-trained Vision-Language Models (VLMs) like CLIP, despite having remarkable generalization ability, are highly vulnerable to adversarial examples. This work studies the adversarial robustness of VLMs from the novel perspective of the text prompt instead of the extensively studied model weights (frozen in this work). We first show that the effectiveness of both adversarial attack and defense are sensitive to the used text prompt. Inspired by this, we propose a method to improve resilience to adversarial attacks by learning a robust text prompt for VLMs. The proposed method, named Adversarial Prompt Tuning (APT), is effective while being both computationally and data efficient. Extensive experiments are conducted across 15 datasets and 4 data sparsity schemes (from 1-shot to full training data settings) to show APT's superiority over hand-engineered prompts and other state-of-the-art adaption methods. APT demonstrated excellent abilities in terms of the in-distribution performance and the generalization under input distribution shift and across datasets. Surprisingly, by simply adding one learned word to the prompts, APT can significantly boost the accuracy and robustness (epsilon=4/255) over the hand-engineered prompts by +13% and +8.5% on average respectively. The improvement further increases, in our most effective setting, to +26.4% for accuracy and +16.7% for robustness. Code is available at https://github.com/TreeLLi/APT.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work studies the adversarial robustness of VLMs from the novel perspective of the text prompt instead of the extensively studied model weights, and proposes a method to improve resilience to adversarial attacks by learning a robust text prompt for VLMs."
            },
            "score": 6
        },
        {
            "id": "bbd6d6874a8ca1c155bcfb540e8d55199944cdc5",
            "paperId": "bbd6d6874a8ca1c155bcfb540e8d55199944cdc5",
            "title": "RoAST: Robustifying Language Models via Adversarial Perturbation with Selective Training",
            "abstract": "Fine-tuning pre-trained language models (LMs) has become the de facto standard in many NLP tasks. Nevertheless, fine-tuned LMs are still prone to robustness issues, such as adversarial robustness and model calibration. Several perspectives of robustness for LMs have been studied independently, but lacking a unified consideration in multiple perspectives. In this paper, we propose Robustifying LMs via Adversarial perturbation with Selective Training (RoAST), a simple yet effective fine-tuning technique to enhance the multi-perspective robustness of LMs in a unified way. RoAST effectively incorporates two important sources for the model robustness, robustness on the perturbed inputs and generalizable knowledge in pre-trained LMs. To be specific, RoAST introduces adversarial perturbation during fine-tuning while the model parameters are selectively updated upon their relative importance to minimize unnecessary deviation. Under a unified evaluation of fine-tuned LMs by incorporating four representative perspectives of model robustness, we demonstrate the effectiveness of RoAST compared to state-of-the-art fine-tuning methods on six different types of LMs, which indicates its usefulness in practice.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Under a unified evaluation of fine-tuned LMs by incorporating four representative perspectives of model robustness, the effectiveness of RoAST is demonstrated compared to state-of-the-art fine- tuning methods on six different types of LMs, which indicates its usefulness in practice."
            },
            "score": 6
        },
        {
            "id": "8fdd34153d1035d09dd4a6efa9cb0c91d23d0045",
            "paperId": "8fdd34153d1035d09dd4a6efa9cb0c91d23d0045",
            "title": "More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models",
            "abstract": "We are currently witnessing dramatic advances in the capabilities of Large Language Models (LLMs). They are already being adopted in practice and integrated into many systems, including integrated development environments (IDEs) and search engines. The functionalities of current LLMs can be modulated via natural language prompts, while their exact internal functionality remains implicit and unassessable. This property, which makes them adaptable to even unseen tasks, might also make them susceptible to targeted adversarial prompting . Recently, several ways to misalign LLMs using Prompt Injection (PI) attacks have been introduced. In such attacks, an adversary can prompt the LLM to produce malicious content or override the original instructions and the employed \ufb01ltering schemes. Recent work showed that these attacks are hard to mitigate, as state-of-the-art LLMs are instruction-following . So far, these attacks assumed that the adversary is directly prompting the LLM. In this work, we show that augmenting LLMs with retrieval and API calling capabilities (so-called Application-Integrated LLMs ) induces a whole new set of attack vectors. These LLMs might process poisoned content retrieved from the Web that contains malicious prompts pre-injected and selected by adversaries. We demonstrate that an attacker can indirectly perform such PI attacks. Based on this key insight, we systematically analyze the resulting threat landscape of Application-Integrated LLMs and discuss a variety of new attack vectors. To demonstrate the practical viability of our attacks, we implemented speci\ufb01c demonstrations",
            "year": 2023,
            "citationCount": 73,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work shows that augmenting LLMs with retrieval and API calling capabilities (so-called Application-Integrated LLMs) induces a whole new set of attack vectors and systematically analyzes the resulting threat landscape of Application-Integrated LLMs."
            },
            "score": 6
        },
        {
            "id": "4637f79ddfaf923ce569996ffa5b6cda1996faa1",
            "paperId": "4637f79ddfaf923ce569996ffa5b6cda1996faa1",
            "title": "Jailbreaking Black Box Large Language Models in Twenty Queries",
            "abstract": "There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR -- which is inspired by social engineering attacks -- uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and PaLM-2.",
            "year": 2023,
            "citationCount": 119,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "PAIR is an algorithm that generates semantic jailbreaks with only black-box access to an LLM with competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and PaLM."
            },
            "score": 6
        },
        {
            "id": "4059061b966250e8cea5b2487f22d50b333d4b02",
            "paperId": "4059061b966250e8cea5b2487f22d50b333d4b02",
            "title": "Certified Adversarial Robustness of Machine Learning-based Malware Detectors via (De)Randomized Smoothing",
            "abstract": "Deep learning-based malware detection systems are vulnerable to adversarial EXEmples - carefully-crafted malicious programs that evade detection with minimal perturbation. As such, the community is dedicating effort to develop mechanisms to defend against adversarial EXEmples. However, current randomized smoothing-based defenses are still vulnerable to attacks that inject blocks of adversarial content. In this paper, we introduce a certifiable defense against patch attacks that guarantees, for a given executable and an adversarial patch size, no adversarial EXEmple exist. Our method is inspired by (de)randomized smoothing which provides deterministic robustness certificates. During training, a base classifier is trained using subsets of continguous bytes. At inference time, our defense splits the executable into non-overlapping chunks, classifies each chunk independently, and computes the final prediction through majority voting to minimize the influence of injected content. Furthermore, we introduce a preprocessing step that fixes the size of the sections and headers to a multiple of the chunk size. As a consequence, the injected content is confined to an integer number of chunks without tampering the other chunks containing the real bytes of the input examples, allowing us to extend our certified robustness guarantees to content insertion attacks. We perform an extensive ablation study, by comparing our defense with randomized smoothing-based defenses against a plethora of content manipulation attacks and neural network architectures. Results show that our method exhibits unmatched robustness against strong content-insertion attacks, outperforming randomized smoothing-based defenses in the literature.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A certifiable defense against patch attacks that guarantees, for a given executable and an adversarial patch size, no adversarial EXEmple exist, and is inspired by (de)randomized smoothing which provides deterministic robustness certificates."
            },
            "score": 6
        },
        {
            "id": "d1358df5251255d24af7ed410f4452d5ba271957",
            "paperId": "d1358df5251255d24af7ed410f4452d5ba271957",
            "title": "A Robust Defense against Adversarial Attacks on Deep Learning-based Malware Detectors via (De)Randomized Smoothing",
            "abstract": "Deep learning-based malware detectors have been shown to be susceptible to adversarial malware examples, i.e. malware examples that have been deliberately manipulated in order to avoid detection. In light of the vulnerability of deep learning detectors to subtle input file modifications, we propose a practical defense against adversarial malware examples inspired by (de)randomized smoothing. In this work, we reduce the chances of sampling adversarial content injected by malware authors by selecting correlated subsets of bytes, rather than using Gaussian noise to randomize inputs like in the Computer Vision (CV) domain. During training, our ablation-based smoothing scheme trains a base classifier to make classifications on a subset of contiguous bytes or chunk of bytes. At test time, a large number of chunks are then classified by a base classifier and the consensus among these classifications is then reported as the final prediction. We propose two strategies to determine the location of the chunks used for classification: (1) randomly selecting the locations of the chunks and (2) selecting contiguous adjacent chunks. To showcase the effectiveness of our approach, we have trained two classifiers with our chunk-based ablation schemes on the BODMAS dataset. Our findings reveal that the chunk-based smoothing classifiers exhibit greater resilience against adversarial malware examples generated with state-of-the-are evasion attacks, outperforming a non-smoothed classifier and a randomized smoothing-based classifier by a great margin.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The findings reveal that the chunk-based smoothing classifiers exhibit greater resilience against adversarial malware examples generated with state-of-the-are evasion attacks, outperforming a non-smoothed classifier and a randomized smoothing-based classifier by a great margin."
            },
            "score": 6
        },
        {
            "id": "1c91b23d78944f7f237cb512029c2165972ae9d5",
            "paperId": "1c91b23d78944f7f237cb512029c2165972ae9d5",
            "title": "On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex",
            "abstract": "Semantic parsing is a technique aimed at constructing a structured representation of the meaning of a natural-language question. Recent advances in language models trained on code have shown superior performance in generating these representations compared to language models trained solely on natural language text. The existing fine-tuned neural semantic parsers are vulnerable to adversarial attacks on natural-language inputs. While it has been established that the robustness of smaller semantic parsers can be enhanced through adversarial training, this approach is not feasible for large language models in real-world scenarios, as it requires both substantial computational resources and expensive human annotation on in-domain semantic parsing data. This paper presents the first empirical study on the adversarial robustness of a prompt-based semantic parser based on CODEX, a stateof-the-art (SOTA) language model trained on code. Our results demonstrate that the large language model of code is vulnerable to carefully crafted adversarial examples. To overcome this challenge, we propose methods for enhancing robustness without requiring substantial amounts of labelled data or intensive computational resources.",
            "year": 2023,
            "citationCount": 30,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents the first empirical study on the adversarial robustness of a prompt-based semantic parser based on CODEX, a stateof-the-art (SOTA) language model trained on code."
            },
            "score": 5
        },
        {
            "id": "c1d1842e08716cbf7250167969522a1705d8bcd3",
            "paperId": "c1d1842e08716cbf7250167969522a1705d8bcd3",
            "title": "Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information",
            "abstract": "In recent years, Large Language Models (LLM) have emerged as pivotal tools in various applications. However, these models are susceptible to adversarial prompt attacks, where attackers can carefully curate input strings that mislead LLMs into generating incorrect or undesired outputs. Previous work has revealed that with relatively simple yet effective attacks based on discrete optimization, it is possible to generate adversarial prompts that bypass moderation and alignment of the models. This vulnerability to adversarial prompts underscores a significant concern regarding the robustness and reliability of LLMs. Our work aims to address this concern by introducing a novel approach to detecting adversarial prompts at a token level, leveraging the LLM's capability to predict the next token's probability. We measure the degree of the model's perplexity, where tokens predicted with high probability are considered normal, and those exhibiting high perplexity are flagged as adversarial. Additionaly, our method also integrates context understanding by incorporating neighboring token information to encourage the detection of contiguous adversarial prompt sequences. To this end, we design two algorithms for adversarial prompt detection: one based on optimization techniques and another on Probabilistic Graphical Models (PGM). Both methods are equipped with efficient solving methods, ensuring efficient adversarial prompt detection. Our token-level detection result can be visualized as heatmap overlays on the text sequence, allowing for a clearer and more intuitive representation of which part of the text may contain adversarial prompts.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a novel approach to detecting adversarial prompts at a token level, leveraging the LLM's capability to predict the next token's probability, and designs two algorithms for adversarial prompt detection, one based on optimization techniques and another on Probabilistic Graphical Models."
            },
            "score": 5
        },
        {
            "id": "3955a47d6265e7e346f6873b2223fa80200a205c",
            "paperId": "3955a47d6265e7e346f6873b2223fa80200a205c",
            "title": "Fast Certification of Vision-Language Models Using Incremental Randomized Smoothing",
            "abstract": "A key benefit of deep vision-language models such as CLIP is that they enable zero-shot open vocabulary classification; the user has the ability to define novel class labels via natural language prompts at inference time. However, while CLIP-based zero-shot classifiers have demonstrated competitive performance across a range of domain shifts, they remain highly vulnerable to adversarial attacks. Therefore, ensuring the robustness of such models is crucial for their reliable deployment in the wild. In this work, we introduce Open Vocabulary Certification (OVC), a fast certification method designed for open-vocabulary models like CLIP via randomized smoothing techniques. Given a base\"training\"set of prompts and their corresponding certified CLIP classifiers, OVC relies on the observation that a classifier with a novel prompt can be viewed as a perturbed version of nearby classifiers in the base training set. Therefore, OVC can rapidly certify the novel classifier using a variation of incremental randomized smoothing. By using a caching trick, we achieve approximately two orders of magnitude acceleration in the certification process for novel prompts. To achieve further (heuristic) speedups, OVC approximates the embedding space at a given input using a multivariate normal distribution bypassing the need for sampling via forward passes through the vision backbone. We demonstrate the effectiveness of OVC on through experimental evaluation using multiple vision-language backbones on the CIFAR-10 and ImageNet test datasets.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces Open Vocabulary Certification (OVC), a fast certification method designed for open-vocabulary models like CLIP via randomized smoothing techniques, and demonstrates the effectiveness of OVC on through experimental evaluation using multiple vision-language backbones on the CIFAR-10 and ImageNet test datasets."
            },
            "score": 5
        },
        {
            "id": "4407da6ff9b06b00b2a5a93f5c4d333875207811",
            "paperId": "4407da6ff9b06b00b2a5a93f5c4d333875207811",
            "title": "On the Relationship between Skill Neurons and Robustness in Prompt Tuning",
            "abstract": "Prompt Tuning is a popular parameter-efficient finetuning method for pre-trained large language models (PLMs). Based on experiments with RoBERTa, it has been suggested that Prompt Tuning activates specific neurons in the transformer's feed-forward networks, that are highly predictive and selective for the given task. In this paper, we study the robustness of Prompt Tuning in relation to these\"skill neurons\", using RoBERTa and T5. We show that prompts tuned for a specific task are transferable to tasks of the same type but are not very robust to adversarial data. While prompts tuned for RoBERTa yield below-chance performance on adversarial data, prompts tuned for T5 are slightly more robust and retain above-chance performance in two out of three cases. At the same time, we replicate the finding that skill neurons exist in RoBERTa and further show that skill neurons also exist in T5. Interestingly, the skill neurons of T5 determined on non-adversarial data are also among the most predictive neurons on the adversarial data, which is not the case for RoBERTa. We conclude that higher adversarial robustness may be related to a model's ability to consistently activate the relevant skill neurons on adversarial data.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is concluded that higher adversarial robustness may be related to a model's ability to consistently activate the relevant skill neurons on adversarial data."
            },
            "score": 5
        },
        {
            "id": "47030369e97cc44d4b2e3cf1be85da0fd134904a",
            "paperId": "47030369e97cc44d4b2e3cf1be85da0fd134904a",
            "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
            "abstract": "Because\"out-of-the-box\"large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called\"jailbreaks\"against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods. Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.",
            "year": 2023,
            "citationCount": 386,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information."
            },
            "score": 5
        },
        {
            "id": "940fdef49c73ad0c592d5a5ee5c17623b3837cf2",
            "paperId": "940fdef49c73ad0c592d5a5ee5c17623b3837cf2",
            "title": "Center Smoothing: Certified Robustness for Networks with Structured Outputs",
            "abstract": "The study of provable adversarial robustness has mostly been limited to classification tasks and models with one-dimensional real-valued outputs. We extend the scope of certifiable robustness to problems with more general and structured outputs like sets, images, language, etc. We model the output space as a metric space under a distance/similarity function, such as intersection-over-union, perceptual similarity, total variation distance, etc. Such models are used in many machine learning problems like image segmentation, object detection, generative models, image/audio-to-text systems, etc. Based on a robustness technique called randomized smoothing, our $\\textit{center smoothing}$ procedure can produce models with the guarantee that the change in the output, as measured by the distance metric, remains small for any norm-bounded adversarial perturbation of the input. We apply our method to create certifiably robust models with disparate output spaces - from sets to images - and show that it yields meaningful certificates without significantly degrading the performance of the base model. Code for our experiments is available at: https://github.com/aounon/center-smoothing.",
            "year": 2021,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work applies a robustness technique called randomized smoothing to create certifiably robust models with disparate output spaces - from sets to images - and shows that it yields meaningful certificates without significantly degrading the performance of the base model."
            },
            "score": 5
        },
        {
            "id": "ca114b547c06eeb9911b685736679ca4dddf395c",
            "paperId": "ca114b547c06eeb9911b685736679ca4dddf395c",
            "title": "Text-CRS: A Generalized Certified Robustness Framework against Textual Adversarial Attacks",
            "abstract": "The language models, especially the basic text classification models, have been shown to be susceptible to textual adversarial attacks such as synonym substitution and word insertion attacks. To defend against such attacks, a growing body of research has been devoted to improving the model robustness. However, providing provable robustness guarantees instead of empirical robustness is still widely unexplored. In this paper, we propose Text-CRS, a generalized certified robustness framework for natural language processing (NLP) based on randomized smoothing. To our best knowledge, existing certified schemes for NLP can only certify the robustness against $\\ell_0$ perturbations in synonym substitution attacks. Representing each word-level adversarial operation (i.e., synonym substitution, word reordering, insertion, and deletion) as a combination of permutation and embedding transformation, we propose novel smoothing theorems to derive robustness bounds in both permutation and embedding space against such adversarial operations. To further improve certified accuracy and radius, we consider the numerical relationships between discrete words and select proper noise distributions for the randomized smoothing. Finally, we conduct substantial experiments on multiple language models and datasets. Text-CRS can address all four different word-level adversarial operations and achieve a significant accuracy improvement. We also provide the first benchmark on certified accuracy and radius of four word-level operations, besides outperforming the state-of-the-art certification against synonym substitution attacks.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes Text-CRS, a generalized certified robustness framework for natural language processing (NLP) based on randomized smoothing and provides the first benchmark on certified accuracy and radius of four word-level operations, besides outperforming the state-of-the-art certification against synonym substitution attacks."
            },
            "score": 5
        },
        {
            "id": "c15173d5e16e16e16f6b6d23d7a2b2e3e4b7af6a",
            "paperId": "c15173d5e16e16e16f6b6d23d7a2b2e3e4b7af6a",
            "title": "Hidden Cost of Randomized Smoothing",
            "abstract": "The fragility of modern machine learning models has drawn a considerable amount of attention from both academia and the public. While immense interests were in either crafting adversarial attacks as a way to measure the robustness of neural networks or devising worst-case analytical robustness veri\ufb01cation with guarantees, few methods could enjoy both scalability and robustness guarantees at the same time. As an alternative to these attempts, randomized smoothing adopts a di\ufb00erent prediction rule that enables statistical robustness arguments which easily scale to large networks. However, in this paper, we point out the side e\ufb00ects of current randomized smoothing work\ufb02ows. Speci\ufb01cally, we articulate and prove two major points: 1) the decision boundaries of smoothed clas-si\ufb01ers will shrink, resulting in disparity in class-wise accuracy; 2) applying noise augmentation in the training process does not necessarily resolve the shrinking issue due to the inconsistent learning objectives.",
            "year": 2021,
            "citationCount": 21,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Two major points are articulate and prove: 1) the decision boundaries of smoothed clas-si\ufb01ers will shrink, resulting in disparity in class-wise accuracy; 2) applying noise augmentation in the training process does not necessarily resolve the shrinking issue due to the inconsistent learning objectives."
            },
            "score": 5
        },
        {
            "id": "6d465be006615460d41060f9f5068d51fc1f46b1",
            "paperId": "6d465be006615460d41060f9f5068d51fc1f46b1",
            "title": "Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models",
            "abstract": "The integration of large language models (LLMs) with external content has enabled more up-to-date and wide-ranging applications of LLMs, such as Microsoft Copilot. However, this integration has also exposed LLMs to the risk of indirect prompt injection attacks, where an attacker can embed malicious instructions within external content, compromising LLM output and causing responses to deviate from user expectations. To investigate this important but underexplored issue, we introduce the first benchmark for indirect prompt injection attacks, named BIPIA, to evaluate the risk of such attacks. Based on the evaluation, our work makes a key analysis of the underlying reason for the success of the attack, namely the inability of LLMs to distinguish between instructions and external content and the absence of LLMs' awareness to not execute instructions within external content. Building upon this analysis, we develop two black-box methods based on prompt learning and a white-box defense method based on fine-tuning with adversarial training accordingly. Experimental results demonstrate that black-box defenses are highly effective in mitigating these attacks, while the white-box defense reduces the attack success rate to near-zero levels. Overall, our work systematically investigates indirect prompt injection attacks by introducing a benchmark, analyzing the underlying reason for the success of the attack, and developing an initial set of defenses.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work systematically investigates indirect prompt injection attacks by introducing a benchmark, analyzing the underlying reason for the success of the attack, and developing an initial set of defenses."
            },
            "score": 5
        },
        {
            "id": "db4517840e25fdd4cefe93a1c843b021ce1b25d5",
            "paperId": "db4517840e25fdd4cefe93a1c843b021ce1b25d5",
            "title": "Rethinking Textual Adversarial Defense for Pre-Trained Language Models",
            "abstract": "Although pre-trained language models (PrLMs) have achieved significant success, recent studies demonstrate that PrLMs are vulnerable to adversarial attacks. By generating adversarial examples with slight perturbations on different levels (sentence / word / character), adversarial attacks can fool PrLMs to generate incorrect predictions, which questions the robustness of PrLMs. However, we find that most existing textual adversarial examples are unnatural, which can be easily distinguished by both human and machine. Based on a general anomaly detector, we propose a novel metric (Degree of Anomaly) as a constraint to enable current adversarial attack approaches to generate more natural and imperceptible adversarial examples. Under this new constraint, the success rate of existing attacks drastically decreases, which reveals that the robustness of PrLMs is not as fragile as they claimed. In addition, we find that four types of randomization can invalidate a large portion of textual adversarial examples. Based on anomaly detector and randomization, we design a universal defense framework, which is among the first to perform textual adversarial defense without knowing the specific attack. Empirical results show that our universal defense framework achieves comparable or even higher after-attack accuracy with other specific defenses, while preserving higher original accuracy at the same time. Our work discloses the essence of textual adversarial attacks, and indicates that (i) further works of adversarial attacks should focus more on how to overcome the detection and resist the randomization, otherwise their adversarial examples would be easily detected and invalidated; and (ii) compared with the unnatural and perceptible adversarial examples, it is those undetectable adversarial examples that pose real risks for PrLMs and require more attention for future robustness-enhancing strategies.",
            "year": 2022,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A universal defense framework is designed, which is among the first to perform textual adversarial defense without knowing the specific attack and achieves comparable or even higher after-attack accuracy with other specific defenses, while preserving higher original accuracy at the same time."
            },
            "score": 5
        },
        {
            "id": "c4ff1be5c254b60b96b7455eefcc4ec9583f82ed",
            "paperId": "c4ff1be5c254b60b96b7455eefcc4ec9583f82ed",
            "title": "A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily",
            "abstract": "Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to provide useful and safe responses. However, adversarial prompts known as 'jailbreaks' can circumvent safeguards, leading LLMs to generate potentially harmful content. Exploring jailbreak prompts can help to better reveal the weaknesses of LLMs and further steer us to secure them. Unfortunately, existing jailbreak methods either suffer from intricate manual design or require optimization on other white-box models, which compromises either generalization or efficiency. In this paper, we generalize jailbreak prompt attacks into two aspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we propose ReNeLLM, an automatic framework that leverages LLMs themselves to generate effective jailbreak prompts. Extensive experiments demonstrate that ReNeLLM significantly improves the attack success rate while greatly reducing the time cost compared to existing baselines. Our study also reveals the inadequacy of current defense methods in safeguarding LLMs. Finally, we analyze the failure of LLMs defense from the perspective of prompt execution priority, and propose corresponding defense strategies. We hope that our research can catalyze both the academic community and LLMs developers towards the provision of safer and more regulated LLMs. The code is available at https://github.com/NJUNLP/ReNeLLM.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes ReNeLLM, an automatic framework that leverages LLMs themselves to generate effective jailbreak prompts and significantly improves the attack success rate while greatly reducing the time cost compared to existing baselines."
            },
            "score": 5
        },
        {
            "id": "3ee20a72e6008a125135e3f17c5bbdb8cbe9bd8d",
            "paperId": "3ee20a72e6008a125135e3f17c5bbdb8cbe9bd8d",
            "title": "Impact of Adversarial Training on Robustness and Generalizability of Language Models",
            "abstract": "Adversarial training is widely acknowledged as the most effective defense against adversarial attacks. However, it is also well established that achieving both robustness and generalization in adversarially trained models involves a trade-off. The goal of this work is to provide an in depth comparison of different approaches for adversarial training in language models. Specifically, we study the effect of pre-training data augmentation as well as training time input perturbations vs. embedding space perturbations on the robustness and generalization of transformer-based language models. Our findings suggest that better robustness can be achieved by pre-training data augmentation or by training with input space perturbation. However, training with embedding space perturbation significantly improves generalization. A linguistic correlation analysis of neurons of the learned models reveals that the improved generalization is due to 'more specialized' neurons. To the best of our knowledge, this is the first work to carry out a deep qualitative analysis of different methods of generating adversarial examples in adversarial training of language models.",
            "year": 2022,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This is the first work to carry out a deep qualitative analysis of different methods of generating adversarial examples in adversarial training of language models and suggests that better robustness can be achieved by pre-training data augmentation or by training with input space perturbation."
            },
            "score": 5
        },
        {
            "id": "ada01194e95945f8f54224f5e791476a4923362e",
            "paperId": "ada01194e95945f8f54224f5e791476a4923362e",
            "title": "P NEG : Prompt-based Negative Response Generation for Robust Response Selection Model",
            "abstract": "Dialogue response selection models typically 001 predict an appropriate response relying on the 002 context-response content similarity. However, 003 the selection model with over-reliance only on 004 superficial features is vulnerable to adversar-005 ial responses that are semantically similar but 006 irrelevant to dialogue context. Recent studies 007 have shown that leveraging these adversarial 008 responses as negative training samples is useful 009 for improving the robustness of the selection 010 model. Nevertheless, existing methods often 011 require further fine-tuning for data creation or 012 have limited scalability. To overcome these 013 limitations, this paper proposes a simple but ef-014 fective method for generating adversarial nega-015 tive responses leveraging a large-scale language 016 model. Our method can generate realistic nega-017 tive responses only with a few human-written 018 examples and a prompt designed to optimize 019 generation quality. Experimental results on the 020 dialogue selection task show that our method 021 outperforms existing methods for creating neg-022 ative responses. Synthetic quality analyses and 023 ablation studies prove that our method is scal-024 able and can generate high-quality negative re-025 sponses. These results suggest that our method 026 can be an effective alternative to human anno-027 tators in generating adversarial responses. 028",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A simple but ef-014 fective method for generating adversarial nega-015 tive responses leveraging a large-scale language 016 model and results suggest that the method 026 can be an effective alternative to human anno-027 tators in generating adversarial responses."
            },
            "score": 4
        },
        {
            "id": "8ecdbfe011b7189fa0ee49ffc4e42a93d728a371",
            "paperId": "8ecdbfe011b7189fa0ee49ffc4e42a93d728a371",
            "title": "On Evaluating Adversarial Robustness of Large Vision-Language Models",
            "abstract": "Large vision-language models (VLMs) such as GPT-4 have achieved unprecedented performance in response generation, especially with visual inputs, enabling more creative and adaptable interaction than large language models such as ChatGPT. Nonetheless, multimodal generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable modality (e.g., vision). To this end, we propose evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses. In particular, we first craft targeted adversarial examples against pretrained models such as CLIP and BLIP, and then transfer these adversarial examples to other VLMs such as MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt. In addition, we observe that black-box queries on these VLMs can further improve the effectiveness of targeted evasion, resulting in a surprisingly high success rate for generating targeted responses. Our findings provide a quantitative understanding regarding the adversarial vulnerability of large VLMs and call for a more thorough examination of their potential security flaws before deployment in practice. Code is at https://github.com/yunqing-me/AttackVLM.",
            "year": 2023,
            "citationCount": 47,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses is proposed."
            },
            "score": 4
        },
        {
            "id": "efabdd27929796b712cb1b3a3051ea5358dc1200",
            "paperId": "efabdd27929796b712cb1b3a3051ea5358dc1200",
            "title": "A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning",
            "abstract": "Vision-language models can encode societal biases and stereotypes, but there are challenges to measuring and mitigating these multimodal harms due to lacking measurement robustness and feature degradation. To address these challenges, we investigate bias measures and apply ranking metrics for image-text representations. We then investigate debiasing methods and show that prepending learned embeddings to text queries that are jointly trained with adversarial debiasing and a contrastive loss, reduces various bias measures with minimal degradation to the image-text representation.",
            "year": 2022,
            "citationCount": 53,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Debiasing methods are investigated and it is shown that prepending learned embeddings to text queries that are jointly trained with adversarial debiasing and a contrastive loss, reduces various bias measures with minimal degradation to the image-text representation."
            },
            "score": 4
        },
        {
            "id": "97a1617a789cd8432a8ade86a79b25f90fe2c3b1",
            "paperId": "97a1617a789cd8432a8ade86a79b25f90fe2c3b1",
            "title": "A general optimization framework for smoothing language models on graph structures",
            "abstract": "Recent work on language models for information retrieval has shown that smoothing language models is crucial for achieving good retrieval performance. Many different effective smoothing methods have been proposed, which mostly implement various heuristics to exploit corpus structures. In this paper, we propose a general and unified optimization framework for smoothing language models on graph structures. This framework not only provides a unified formulation of the existing smoothing heuristics, but also serves as a road map for systematically exploring smoothing methods for language models. We follow this road map and derive several different instantiations of the framework. Some of the instantiations lead to novel smoothing methods. Empirical results show that all such instantiations are effective with some outperforming the state of the art smoothing methods.",
            "year": 2008,
            "citationCount": 77,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a general and unified optimization framework for smoothing language models on graph structures that provides a unified formulation of the existing smoothing heuristics, and serves as a road map for systematically exploring smoothing methods for language models."
            },
            "score": 4
        },
        {
            "id": "ee5e077b6492f408897a63d5abbee5fa0349fb8e",
            "paperId": "ee5e077b6492f408897a63d5abbee5fa0349fb8e",
            "title": "Enhancing Fault Detection for Large Language Models via Mutation-Based Confidence Smoothing",
            "abstract": "Large language models (LLMs) achieved great success in multiple application domains and attracted huge attention from different research communities recently. Unfortunately, even for the best LLM, there still exist many faults that LLM cannot correctly predict. Such faults will harm the usability of LLMs. How to quickly reveal them in LLMs is important, but challenging. The reasons are twofold, 1) the heavy labeling effort for preparing the test data, and 2) accessing closed-source LLMs such as GPT4 is money-required. To handle this problem, in the traditional deep learning testing field, test selection methods have been proposed for efficiently testing deep learning models by prioritizing faults. However, the usefulness of these methods on LLMs is unclear and under exploration. In this paper, we first study the effectiveness of existing fault detection methods for LLMs. Experimental results on four different tasks~(including both code tasks and natural language processing tasks) and four LLMs (e.g., LLaMA and GPT4) demonstrated that existing fault detection methods cannot perform well on LLMs (e.g., seven out of eight methods perform worse than random selection on LLaMA). To enhance existing fault detection methods, we propose MuCS, a prompt Mutation-based prediction Confidence Smoothing method for LLMs. Concretely, we mutate the prompts and compute the average prediction confidence of all mutants as the input of fault detection methods. The results show that our proposed solution significantly enhances existing methods with the improvement of test relative coverage by up to 97.64%.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "MuCS, a prompt Mutation-based prediction Confidence Smoothing method for LLMs is proposed, which mutate the prompts and compute the average prediction confidence of all mutants as the input of fault detection methods."
            },
            "score": 4
        },
        {
            "id": "05ef3566499e24888a8c500944336616f8f418a4",
            "paperId": "05ef3566499e24888a8c500944336616f8f418a4",
            "title": "Calibrated Fine-Tuning for Pre-trained Language Models via Manifold Smoothing",
            "abstract": "Fine-tuned pre-trained language models can suffer from severe miscalibration for both in-distribution and out-of-distribution (OOD) data due to over-parameterization. To mitigate this issue, we propose a regularized fine-tuning method. Our method introduces two types of regularization for better calibration: (1) On-manifold regularization, which generates pseudo on-manifold samples through interpolation within the data manifold. Augmented training with these pseudo samples imposes a smoothness regularization to improve in-distribution calibration. (2) Off-manifold regularization, which encourages the model to output uniform distributions for pseudo off-manifold samples to address the over-confidence issue for OOD data. Our experiments demonstrate that the proposed method outperforms existing calibration methods for text classification in terms of expectation calibration error, misclassification detection, and OOD detection on six datasets. Our code can be found at this https URL.",
            "year": 2020,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The proposed regularized fine-tuning method outperforms existing calibration methods for text classification in terms of expectation calibration error, misclassification detection, and OOD detection on six datasets."
            },
            "score": 4
        },
        {
            "id": "5886df0f27f6beb38eac729cc38b9081e90ea0fa",
            "paperId": "5886df0f27f6beb38eac729cc38b9081e90ea0fa",
            "title": "Certifiably-Robust Federated Adversarial Learning via Randomized Smoothing",
            "abstract": "Federated learning is an emerging data-private distributed learning framework, which, however, is vulnerable to adversarial attacks. Although several heuristic defenses are proposed to enhance the robustness of federated learning, they do not provide certifiable robustness guarantees. In this paper, we incorporate randomized smoothing techniques into federated adversarial training to enable data-private distributed learning with certifiable robustness to test-time adversarial perturbations. Through comprehensive experiments, we show that such an advanced federated adversarial learning framework can deliver models as robust as those trained by the centralized training. Further, this enables training provably-robust classifiers to2 bounded adversarial perturbations in a distributed setup. We also show that the one-point gradient estimation-based training approach is $2 - 3 \\times$ faster than the popular stochastic estimator-based approach without any noticeable certified robustness differences.",
            "year": 2021,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper incorporates randomized smoothing techniques into federated adversarial training to enable data-private distributed learning with certifiable robustness to test-time adversarial perturbations, and shows that such an advanced federate adversarial learning framework can deliver models as robust as those trained by the centralized training."
            },
            "score": 4
        },
        {
            "id": "07cfaf543b2bd991406be1d72a52d784cc9c62fb",
            "paperId": "07cfaf543b2bd991406be1d72a52d784cc9c62fb",
            "title": "Prompt Makes mask Language Models Better Adversarial Attackers",
            "abstract": "Generating high-quality synonymous perturbations is a core challenge for textual adversarial tasks. However, candidates generated from the masked language model often contain many words that are antonyms or irrelevant to the original words, which limit the perturbation space and affect the attack\u2019s effectiveness. We present ProAttacker1 which uses Prompt to make the mask language models better adversarial Attackers. ProAttacker inverts the prompt paradigm by leveraging the prompt with the class label to guide the language model to generate more semantically-consistent perturbations. We present a systematic evaluation to analyze the attack performance on 6 NLP datasets, covering text classification and inference. Our experiments demonstrate that ProAttacker outperforms state-of-the-art attack strategies in both success rate and perturb rate.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ProAttacker1, which uses Prompt to make the mask language models better adversarial Attackers, inverts the prompt paradigm by leveraging the prompt with the class label to guide the language model to generate more semantically-consistent perturbations."
            },
            "score": 4
        },
        {
            "id": "e2302eda403de7000669813cf23bc0c0c08f3e93",
            "paperId": "e2302eda403de7000669813cf23bc0c0c08f3e93",
            "title": "COVER: A Heuristic Greedy Adversarial Attack on Prompt-based Learning in Language Models",
            "abstract": "Prompt-based learning has been proved to be an effective way in pre-trained language models (PLMs), especially in low-resource scenarios like few-shot settings. However, the trustworthiness of PLMs is of paramount significance and potential vulnerabilities have been shown in prompt-based templates that could mislead the predictions of language models, causing serious security concerns. In this paper, we will shed light on some vulnerabilities of PLMs, by proposing a prompt-based adversarial attack on manual templates in black box scenarios. First of all, we design character-level and word-level heuristic approaches to break manual templates separately. Then we present a greedy algorithm for the attack based on the above heuristic destructive approaches. Finally, we evaluate our approach with the classification tasks on three variants of BERT series models and eight datasets. And comprehensive experimental results justify the effectiveness of our approach in terms of attack success rate and attack speed.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a prompt-based adversarial attack on manual templates in black box scenarios by designing character-level and word-level heuristic approaches to break manual templates separately and presents a greedy algorithm for the attack based on the above heuristic destructive approaches."
            },
            "score": 4
        },
        {
            "id": "92b9d8b8c81c4c53ea62000c0924500b2dd11bce",
            "paperId": "92b9d8b8c81c4c53ea62000c0924500b2dd11bce",
            "title": "Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models",
            "abstract": "We introduce new jailbreak attacks on vision language models (VLMs), which use aligned LLMs and are resilient to text-only jailbreak attacks. Specifically, we develop cross-modality attacks on alignment where we pair adversarial images going through the vision encoder with textual prompts to break the alignment of the language model. Our attacks employ a novel compositional strategy that combines an image, adversarially targeted towards toxic embeddings, with generic prompts to accomplish the jailbreak. Thus, the LLM draws the context to answer the generic prompt from the adversarial image. The generation of benign-appearing adversarial images leverages a novel embedding-space-based methodology, operating with no access to the LLM model. Instead, the attacks require access only to the vision encoder and utilize one of our four embedding space targeting strategies. By not requiring access to the LLM, the attacks lower the entry barrier for attackers, particularly when vision encoders such as CLIP are embedded in closed-source LLMs. The attacks achieve a high success rate across different VLMs, highlighting the risk of cross-modality alignment vulnerabilities, and the need for new alignment approaches for multi-modal models.",
            "year": 2023,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Cross-modality attacks on alignment where adversarial images going through the vision encoder with textual prompts to break the alignment of the language model are developed."
            },
            "score": 4
        },
        {
            "id": "635bba2d18dba4ecd844414c4c813a4d7de26a15",
            "paperId": "635bba2d18dba4ecd844414c4c813a4d7de26a15",
            "title": "Certified Adversarial Robustness via Anisotropic Randomized Smoothing",
            "abstract": "Randomized smoothing has achieved great suc-cess for certi\ufb01ed robustness against adversarial perturbations. Given any arbitrary classi\ufb01er, randomized smoothing can guarantee the classi\ufb01er\u2019s prediction over the perturbed input with provable robustness bound by injecting noise into the classi\ufb01er. However, all of the existing methods rely on \ufb01xed i.i.d. probability distribution to generate noise for all dimensions of the data (e.g., all the pixels in an image), which ignores the heterogeneity of inputs and data dimensions. Thus, existing randomized smoothing methods cannot provide optimal protection for all the inputs. To address this limitation, we propose a novel anisotropic randomized smoothing method which ensures provable robustness guarantee based on pixel-wise noise distributions. Also, we design a novel CNN-based noise generator to ef\ufb01ciently \ufb01ne-tune the pixel-wise noise distributions for all the pixels in each input. Experimental results demonstrate that our method signi\ufb01cantly outperforms the state-of-the-art randomized smoothing methods.",
            "year": 2022,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel anisotropic randomized smoothing method which ensures provable robustness guarantee based on pixel-wise noise distributions and designs a novel CNN-based noise generator to tune the pixel- wise noise distributions for all the pixels in each input."
            },
            "score": 4
        },
        {
            "id": "5718861a2b5b8a40879f96a550dd132267b49886",
            "paperId": "5718861a2b5b8a40879f96a550dd132267b49886",
            "title": "DRF: Improving Certified Robustness via Distributional Robustness Framework",
            "abstract": "Randomized smoothing (RS) has provided state-of-the-art (SOTA) certified robustness against adversarial perturbations for large neural networks. Among studies in this field, methods based on adversarial training (AT) achieve remarkably robust performance by applying adversarial examples to construct the smoothed classifier. These AT-based RS methods typically seek a pointwise adversary that generates the worst-case adversarial examples by perturbing each input independently. However, there are unexplored benefits to considering such adversarial robustness across the entire data distribution. To this end, we provide a novel framework called DRF, which connects AT-based RS methods with distributional robustness (DR), and show that these methods are special cases of their counterparts in our framework. Due to the advantages conferred by DR, our framework can control the trade-off between the clean accuracy and certified robustness of smoothed classifiers to a significant extent. Our experiments demonstrate that DRF can substantially improve the certified robustness of AT-based RS.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel framework called DRF is provided, which connects AT-based RS methods with distributional robustness (DR), and it is demonstrated that DRF can substantially improve the certified robustness of AT-based RS."
            },
            "score": 4
        },
        {
            "id": "7ea9ff6dbf22ed2327eb44e04412dddb443e41c5",
            "paperId": "7ea9ff6dbf22ed2327eb44e04412dddb443e41c5",
            "title": "Black-Box Certification with Randomized Smoothing: A Functional Optimization Based Framework",
            "abstract": "Randomized classifiers have been shown to provide a promising approach for achieving certified robustness against adversarial attacks in deep learning. However, most existing methods only leverage Gaussian smoothing noise and only work for $\\ell_2$ perturbation. We propose a general framework of adversarial certification with non-Gaussian noise and for more general types of attacks, from a unified functional optimization perspective. Our new framework allows us to identify a key trade-off between accuracy and robustness via designing smoothing distributions, helping to design new families of non-Gaussian smoothing distributions that work more efficiently for different $\\ell_p$ settings, including $\\ell_1$, $\\ell_2$ and $\\ell_\\infty$ attacks. Our proposed methods achieve better certification results than previous works and provide a new perspective on randomized smoothing certification.",
            "year": 2020,
            "citationCount": 52,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a general framework of adversarial certification with non-Gaussian noise and for more general types of attacks, from a unified functional optimization perspective."
            },
            "score": 4
        },
        {
            "id": "cb76f7fc35ff289fdf1cf50d9cfe1493342a0dec",
            "paperId": "cb76f7fc35ff289fdf1cf50d9cfe1493342a0dec",
            "title": "AutoEval-Video: An Automatic Benchmark for Assessing Large Vision Language Models in Open-Ended Video Question Answering",
            "abstract": "We propose a novel and challenging benchmark, AutoEval-Video, to comprehensively evaluate large vision-language models in open-ended video question answering. The comprehensiveness of AutoEval-Video is demonstrated in two aspects: 1) AutoEval-Video constructs open-ended video-questions across 9 skill dimensions, addressing capabilities of perception, comprehension, and generation. 2) AutoEval-Video contains newly collected videos that cover over 40 distinct themes. To efficiently evaluate responses to the open-ended questions, we employ an LLM-based evaluation approach, but instead of merely providing a reference answer, we annotate unique evaluation rules for every single instance (video-question pair). To maximize the robustness of these rules, we develop a novel adversarial annotation mechanism. By using instance-specific rules as prompt, GPT-4, as an automatic evaluator, can achieve a stable evaluation accuracy of around 97.0\\%, comparable to the 94.9\\% - 97.5\\% accuracy of a human evaluator. Furthermore, we assess the performance of eight large vision-language models on AutoEval-Video. Among them, GPT-4V(ision) significantly outperforms other models, achieving an accuracy of 32.2\\%. However, there is still substantial room for improvement compared to human accuracy of 72.8\\%. By conducting an extensive case study, we uncover several drawbacks of GPT-4V, such as limited temporal and dynamic comprehension, and overly general responses. Code is available at \\href{https://github.com/Xiuyuan-Chen/AutoEval-Video}{\\color{magenta}https://github.com/Xiuyuan-Chen/AutoEval-Video}.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel and challenging benchmark, AutoEval-Video, to comprehensively evaluate large vision-language models in open-ended video question answering, and develops a novel adversarial annotation mechanism to maximize the robustness of these rules."
            },
            "score": 3
        },
        {
            "id": "8eb9a8d756e93530eb35e9f0e26a2a0190c1dd7c",
            "paperId": "8eb9a8d756e93530eb35e9f0e26a2a0190c1dd7c",
            "title": "The Biases of Pre-Trained Language Models: An Empirical Study on Prompt-Based Sentiment Analysis and Emotion Detection",
            "abstract": "Thanks to the breakthrough of large-scale pre-trained language model (PLM) technology, prompt-based classification tasks, e.g., sentiment analysis and emotion detection, have raised increasing attention. Such tasks are formalized as masked language prediction tasks which are in line with the pre-training objects of most language models. Thus, one can use a PLM to infer the masked words in a downstream task, then obtaining label predictions with manually defined label-word mapping templates. Prompt-based affective computing takes the advantages of both neural network modeling and explainable symbolic representations. However, there still remain many unclear issues related to the mechanisms of PLMs and prompt-based classification. We conduct a systematic empirical study on prompt-based sentiment analysis and emotion detection to study the biases of PLMs towards affective computing. We find that PLMs are biased in sentiment analysis and emotion detection tasks with respect to the number of label classes, emotional label-word selections, prompt templates and positions, and the word forms of emotion lexicons.",
            "year": 2023,
            "citationCount": 98,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that PLMs are biased in sentiment analysis and emotion detection tasks with respect to the number of label classes, emotional label-word selections, prompt templates and positions, and the word forms of emotion lexicons."
            },
            "score": 3
        },
        {
            "id": "b4ed9fb1d6cb510e84d8e8c1768d32517eb75a74",
            "paperId": "b4ed9fb1d6cb510e84d8e8c1768d32517eb75a74",
            "title": "Smoothing Entailment Graphs with Language Models",
            "abstract": "The diversity and Zipfian frequency distribution of natural language predicates in corpora leads to sparsity in Entailment Graphs (EGs) built by Open Relation Extraction (ORE). EGs are computationally efficient and explainable models of natural language inference, but as symbolic models, they fail if a novel premise or hypothesis vertex is missing at test-time. We present theory and methodology for overcoming such sparsity in symbolic models. First, we introduce a theory of optimal smoothing of EGs by constructing transitive chains. We then demonstrate an efficient, open-domain, and unsupervised smoothing method using an off-the-shelf Language Model to find approximations of missing premise predicates. This improves recall by 25.1 and 16.3 percentage points on two difficult directional entailment datasets, while raising average precision and maintaining model explainability. Further, in a QA task we show that EG smoothing is most useful for answering questions with lesser supporting text, where missing premise predicates are more costly. Finally, controlled experiments with WordNet confirm our theory and show that hypothesis smoothing is difficult, but possible in principle.",
            "year": 2022,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a theory of optimal smoothing of EGs by constructing transitive chains and demonstrates an efficient, open-domain, and unsupervised smoothing method using an off-the-shelf Language Model to find approximations of missing premise predicates."
            },
            "score": 3
        },
        {
            "id": "2d5069a99bfa0b47c095bbb5cefd6dba974f72a7",
            "paperId": "2d5069a99bfa0b47c095bbb5cefd6dba974f72a7",
            "title": "Data Noising as Smoothing in Neural Network Language Models",
            "abstract": "Data noising is an effective technique for regularizing neural network models. While noising is widely adopted in application domains such as vision and speech, commonly used noising primitives have not been developed for discrete sequence-level settings such as language modeling. In this paper, we derive a connection between input noising in neural network language models and smoothing in $n$-gram models. Using this connection, we draw upon ideas from smoothing to develop effective noising schemes. We demonstrate performance gains when applying the proposed schemes to language modeling and machine translation. Finally, we provide empirical analysis validating the relationship between noising and smoothing.",
            "year": 2017,
            "citationCount": 220,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper derives a connection between input noising in neural network language models and smoothing in $n$-gram models and draws upon ideas from smoothing to develop effective noising schemes."
            },
            "score": 3
        },
        {
            "id": "db3340742a56a4bbb9d7ee8dba1958fb132a3c0a",
            "paperId": "db3340742a56a4bbb9d7ee8dba1958fb132a3c0a",
            "title": "Token-level and sequence-level loss smoothing for RNN language models",
            "abstract": "Despite the effectiveness of recurrent neural network language models, their maximum likelihood estimation suffers from two limitations. It treats all sentences that do not match the ground truth as equally poor, ignoring the structure of the output space. Second, it suffers from \u2019exposure bias\u2019: during training tokens are predicted given ground-truth sequences, while at test time prediction is conditioned on generated output sequences. To overcome these limitations we build upon the recent reward augmented maximum likelihood approach that encourages the model to predict sentences that are close to the ground truth according to a given performance metric. We extend this approach to token-level loss smoothing, and propose improvements to the sequence-level smoothing approach. Our experiments on two different tasks, image captioning and machine translation, show that token-level and sequence-level loss smoothing are complementary, and significantly improve results.",
            "year": 2018,
            "citationCount": 19,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work builds upon the recent reward augmented maximum likelihood approach that encourages the model to predict sentences that are close to the ground truth according to a given performance metric, and proposes improvements to the sequence-level smoothing approach."
            },
            "score": 3
        },
        {
            "id": "a2c8d1c5470435176185bf891c76711a9b44808a",
            "paperId": "a2c8d1c5470435176185bf891c76711a9b44808a",
            "title": "PromptAid: Prompt Exploration, Perturbation, Testing and Iteration using Visual Analytics for Large Language Models",
            "abstract": "Large Language Models (LLMs) have gained widespread popularity due to their ability to perform ad-hoc Natural Language Processing (NLP) tasks with a simple natural language prompt. Part of the appeal for LLMs is their approachability to the general public, including individuals with no prior technical experience in NLP techniques. However, natural language prompts can vary significantly in terms of their linguistic structure, context, and other semantics. Modifying one or more of these aspects can result in significant differences in task performance. Non-expert users may find it challenging to identify the changes needed to improve a prompt, especially when they lack domain-specific knowledge and lack appropriate feedback. To address this challenge, we present PromptAid, a visual analytics system designed to interactively create, refine, and test prompts through exploration, perturbation, testing, and iteration. PromptAid uses multiple, coordinated visualizations which allow users to improve prompts by using the three strategies: keyword perturbations, paraphrasing perturbations, and obtaining the best set of in-context few-shot examples. PromptAid was designed through an iterative prototyping process involving NLP experts and was evaluated through quantitative and qualitative assessments for LLMs. Our findings indicate that PromptAid helps users to iterate over prompt template alterations with less cognitive overhead, generate diverse prompts with help of recommendations, and analyze the performance of the generated prompts while surpassing existing state-of-the-art prompting interfaces in performance.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The findings indicate that PromptAid helps users to iterate over prompt template alterations with less cognitive overhead, generate diverse prompts with help of recommendations, and analyze the performance of the generated prompts while surpassing existing state-of-the-art prompting interfaces in performance."
            },
            "score": 3
        },
        {
            "id": "b9df0d4631f9fab1432c152765e243ae4cd667f4",
            "paperId": "b9df0d4631f9fab1432c152765e243ae4cd667f4",
            "title": "Effective Prompt Extraction from Language Models",
            "abstract": "The text generated by large language models is commonly controlled by prompting, where a prompt prepended to a user's query guides the model's output. The prompts used by companies to guide their models are often treated as secrets, to be hidden from the user making the query. They have even been treated as commodities to be bought and sold. However, anecdotal reports have shown adversarial users employing prompt extraction attacks to recover these prompts. In this paper, we present a framework for systematically measuring the effectiveness of these attacks. In experiments with 3 different sources of prompts and 11 underlying large language models, we find that simple text-based attacks can in fact reveal prompts with high probability. Our framework determines with high precision whether an extracted prompt is the actual secret prompt, rather than a model hallucination. Prompt extraction experiments on real systems such as Bing Chat and ChatGPT suggest that system prompts can be revealed by an adversary despite existing defenses in place.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents a framework for systematically measuring the effectiveness of prompt extraction attacks and determines with high precision whether an extracted prompt is the actual secret prompt, rather than a model hallucination."
            },
            "score": 3
        },
        {
            "id": "947a4b2e31dcaeffa8d86bc8d6888665ec33c5f6",
            "paperId": "947a4b2e31dcaeffa8d86bc8d6888665ec33c5f6",
            "title": "Certified Robustness for Top-k Predictions against Adversarial Perturbations via Randomized Smoothing",
            "abstract": "It is well-known that classifiers are vulnerable to adversarial perturbations. To defend against adversarial perturbations, various certified robustness results have been derived. However, existing certified robustnesses are limited to top-1 predictions. In many real-world applications, top-$k$ predictions are more relevant. In this work, we aim to derive certified robustness for top-$k$ predictions. In particular, our certified robustness is based on randomized smoothing, which turns any classifier to a new classifier via adding noise to an input example. We adopt randomized smoothing because it is scalable to large-scale neural networks and applicable to any classifier. We derive a tight robustness in $\\ell_2$ norm for top-$k$ predictions when using randomized smoothing with Gaussian noise. We find that generalizing the certified robustness from top-1 to top-$k$ predictions faces significant technical challenges. We also empirically evaluate our method on CIFAR10 and ImageNet. For example, our method can obtain an ImageNet classifier with a certified top-5 accuracy of 62.8\\% when the $\\ell_2$-norms of the adversarial perturbations are less than 0.5 (=127/255). Our code is publicly available at: \\url{https://github.com/jjy1994/Certify_Topk}.",
            "year": 2019,
            "citationCount": 80,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work derives certified robustness for top-k predictions based on randomized smoothing, which turns any classifier to a new classifier via adding noise to an input example, and derives a tight robustness in $\\ell_2$ norm for top-$k$ predictions when using randomized smoothed with Gaussian noise."
            },
            "score": 3
        },
        {
            "id": "6e3f64e6a0659df80b68314adda75a87f29347c1",
            "paperId": "6e3f64e6a0659df80b68314adda75a87f29347c1",
            "title": "Adversarial robustness via robust low rank representations",
            "abstract": "Adversarial robustness measures the susceptibility of a classifier to imperceptible perturbations made to the inputs at test time. In this work we highlight the benefits of natural low rank representations that often exist for real data such as images, for training neural networks with certified robustness guarantees. \nOur first contribution is for certified robustness to perturbations measured in $\\ell_2$ norm. We exploit low rank data representations to provide improved guarantees over state-of-the-art randomized smoothing-based approaches on standard benchmark datasets such as CIFAR-10 and CIFAR-100. \nOur second contribution is for the more challenging setting of certified robustness to perturbations measured in $\\ell_\\infty$ norm. We demonstrate empirically that natural low rank representations have inherent robustness properties, that can be leveraged to provide significantly better guarantees for certified robustness to $\\ell_\\infty$ perturbations in those representations. Our certificate of $\\ell_\\infty$ robustness relies on a natural quantity involving the $\\infty \\to 2$ matrix operator norm associated with the representation, to translate robustness guarantees from $\\ell_2$ to $\\ell_\\infty$ perturbations. \nA key technical ingredient for our certification guarantees is a fast algorithm with provable guarantees based on the multiplicative weights update method to provide upper bounds on the above matrix norm. Our algorithmic guarantees improve upon the state of the art for this problem, and may be of independent interest.",
            "year": 2020,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work demonstrates empirically that natural low rank representations have inherent robustness properties, that can be leveraged to provide significantly better guarantees for certified robustness to $\\ell_\\infty$ perturbations in those representations."
            },
            "score": 3
        },
        {
            "id": "f2ea3cb46dff4daf2127271471fefe800d0212d0",
            "paperId": "f2ea3cb46dff4daf2127271471fefe800d0212d0",
            "title": "Provable Robustness against a Union of L_0 Adversarial Attacks",
            "abstract": "Sparse or L0 adversarial attacks arbitrarily perturb an unknown subset of the features. L0 robustness analysis is particularly well-suited for heterogeneous (tabular) data where features have different types or scales. State-of-the-art L0 certified defenses are based on randomized smoothing and apply to evasion attacks only. This paper proposes feature partition aggregation (FPA) -- a certified defense against the union of L0 evasion, backdoor, and poisoning attacks. FPA generates its stronger robustness guarantees via an ensemble whose submodels are trained on disjoint feature sets. Compared to state-of-the-art L0 defenses, FPA is up to 3,000x faster and provides larger median robustness guarantees (e.g., median certificates of 13 pixels over 10 for CIFAR10, 12 pixels over 10 for MNIST, 4 features over 1 for Weather, and 3 features over 1 for Ames), meaning FPA provides the additional dimensions of robustness essentially for free.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes feature partition aggregation (FPA) -- a certified defense against the union of L0 evasion, backdoor, and poisoning attacks, and generates its stronger robustness guarantees via an ensemble whose submodels are trained on disjoint feature sets."
            },
            "score": 3
        },
        {
            "id": "29c7f009df21d0112c48dec254ff80cc45fac3af",
            "paperId": "29c7f009df21d0112c48dec254ff80cc45fac3af",
            "title": "Are Emergent Abilities of Large Language Models a Mirage?",
            "abstract": "Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, emergent abilities appear due to the researcher's choice of metric rather than due to fundamental changes in model behavior with scale. Specifically, nonlinear or discontinuous metrics produce apparent emergent abilities, whereas linear or continuous metrics produce smooth, continuous predictable changes in model performance. We present our alternative explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities; (2) make, test and confirm two predictions about metric choices in a meta-analysis of emergent abilities on BIG-Bench; and (3) show to choose metrics to produce never-before-seen seemingly emergent abilities in multiple vision tasks across diverse deep networks. Via all three analyses, we provide evidence that alleged emergent abilities evaporate with different metrics or with better statistics, and may not be a fundamental property of scaling AI models.",
            "year": 2023,
            "citationCount": 162,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Evidence is provided that alleged emergent abilities evaporate with different metrics or with better statistics, and may not be a fundamental property of scaling AI models."
            },
            "score": 2
        },
        {
            "id": "828dbdab5791d8539a7f90063d168b9258083326",
            "paperId": "828dbdab5791d8539a7f90063d168b9258083326",
            "title": "Zero-Shot Information Extraction for Clinical Meta-Analysis using Large Language Models",
            "abstract": "Meta-analysis of randomized clinical trials (RCTs) plays a crucial role in evidence-based medicine but can be labor-intensive and error-prone. This study explores the use of large language models to enhance the efficiency of aggregating results from randomized clinical trials (RCTs) at scale. We perform a detailed comparison of the performance of these models in zero-shot prompt-based information extraction from a diverse set of RCTs to traditional manual annotation methods. We analyze the results for two different meta-analyses aimed at drug repurposing in cancer therapy pharmacovigilience in chronic myeloid leukemia. Our findings reveal that the best model for the two demonstrated tasks, ChatGPT can generally extract correct information and identify when the desired information is missing from an article. We additionally conduct a systematic error analysis, documenting the prevalence of diverse error types encountered during the process of prompt-based information extraction.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The use of large language models to enhance the efficiency of aggregating results from randomized clinical trials (RCTs) at scale is explored and the best model, ChatGPT, is revealed to be the best for the two demonstrated tasks."
            },
            "score": 2
        },
        {
            "id": "e262e1aa4d1efb909705a2dfcf9df53d76cb4e12",
            "paperId": "e262e1aa4d1efb909705a2dfcf9df53d76cb4e12",
            "title": "Towards Robustness of Text-to-SQL Models Against Natural and Realistic Adversarial Table Perturbation",
            "abstract": "The robustness of Text-to-SQL parsers against adversarial perturbations plays a crucial role in delivering highly reliable applications. Previous studies along this line primarily focused on perturbations in the natural language question side, neglecting the variability of tables. Motivated by this, we propose the Adversarial Table Perturbation (ATP) as a new attacking paradigm to measure robustness of Text-to-SQL models. Following this proposition, we curate ADVETA, the first robustness evaluation benchmark featuring natural and realistic ATPs. All tested state-of-the-art models experience dramatic performance drops on ADVETA, revealing significant room of improvement. To defense against ATP, we build a systematic adversarial training example generation framework tailored for better contextualization of tabular data. Experiments show that our approach brings models best robustness improvement against ATP, while also substantially boost model robustness against NL-side perturbations. We will release ADVETA and code to facilitate future research.",
            "year": 2022,
            "citationCount": 19,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experiments show that the proposed Adversarial Table Perturbation (ATP) as a new attacking paradigm to measure robustness of Text-to-SQL models brings models best robustness improvement against ATP, while also substantially boost model robustness against NL-side perturbations."
            },
            "score": 2
        },
        {
            "id": "569f369ef0b2db7d6f6dae4f1b757064f1c0145f",
            "paperId": "569f369ef0b2db7d6f6dae4f1b757064f1c0145f",
            "title": "Dimension-independent Certified Neural Network Watermarks via Mollifier Smoothing",
            "abstract": "Certi\ufb01ed Watermarks is the \ufb01rst to provide a watermark certi\ufb01cate against l 2 -norm watermark removal attacks, by leveraging the randomized smoothing techniques for certi\ufb01ed robustness to adversarial attacks. However, the randomized smoothing techniques suffer from hardness of certi\ufb01ed robustness in high-dimensional space against l p -norm attacks for large p ( p > 2 ). The certi\ufb01ed watermark method based on the randomized smoothing is no exception, i.e., fails to provide meaningful certi\ufb01cates in high-dimensional space against the l p -norm watermark removal attacks ( p > 2 ). By leveraging molli\ufb01er theory, this paper proposes a molli\ufb01er smoothing method with dimension-independent certi\ufb01ed radius of our proposed smooth classi\ufb01er, for conducting the certi\ufb01ed watermark problem against the l p - norm watermark removal attacks ( 1 \u2264 p \u2264 \u221e ) for high parameter dimension d . Based on partial differential equation (PDE) theory, an approximation of molli\ufb01er smoothing is developed to alleviate the inef\ufb01ciency of sampling and prediction in the randomized smoothing as well as numerical integration in the molli\ufb01er smoothing, while maintaining the certi\ufb01ed watermark against the l p - norm watermark removal attacks ( 1 \u2264 p \u2264 \u221e ).",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a molli\ufb01er smoothing method with dimension-independent certi \ufb01ed radius of this paper, for conducting the certi\ufdc1ed watermark problem against the l p - norm watermark removal attacks ( 1 \u2264 p \u2264 \u221e ) for high parameter dimension d ."
            },
            "score": 2
        }
    ],
    "novelty": "yes"
}