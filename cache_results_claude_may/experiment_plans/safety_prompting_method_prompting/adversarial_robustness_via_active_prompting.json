{
    "topic_description": "novel prompting methods to improve large language models' robustness against adversarial attacks and improve their security and privacy",
    "idea_name": "Adversarial Robustness via Active Prompting",
    "raw_idea": {
        "Problem": "Current adversarial defenses for language models often rely on passive strategies, such as data augmentation or model ensembling, which may not effectively target the most critical vulnerabilities.",
        "Existing Methods": "Existing methods for improving language models' adversarial robustness, such as adversarial training and randomized smoothing, can be computationally expensive and may not adapt well to evolving attack patterns.",
        "Motivation": "Active learning has been shown to improve the sample efficiency and adaptivity of machine learning models by strategically selecting the most informative examples for training. We hypothesize that actively selecting the most critical prompts for robustness evaluation and enhancement can also improve the efficiency and effectiveness of adversarial defenses for language models.",
        "Proposed Method": "We propose an active prompting framework called ARAP (Adversarial Robustness via Active Prompting) to iteratively improve the adversarial robustness of language models. In each iteration, ARAP first prompts the language model to generate a diverse set of potential adversarial prompts that could expose its vulnerabilities. Then, ARAP evaluates the generated prompts using an ensemble of adversarial attack and defense methods, and selects the most critical prompts that reveal new vulnerabilities or challenge existing defenses. Finally, ARAP prompts the language model to analyze the selected prompts and generate robust responses, which are used to fine-tune the model for enhanced robustness. By actively selecting and learning from the most informative prompts, ARAP enables the language model to efficiently adapt to new attack patterns and improve its worst-case robustness.",
        "Experiment Plan": "We will evaluate ARAP on a range of adversarial attack benchmarks and compare it with state-of-the-art passive defense methods. We will measure the sample efficiency and adaptivity of ARAP by analyzing the number of iterations and prompts needed to achieve a certain level of robustness, and the model's performance on new attack patterns not seen during training. We will also study the effect of different prompt generation, selection, and fine-tuning strategies on the robustness and generalization of the model."
    },
    "full_experiment_plan": {
        "Title": "ARAP: Adversarial Robustness via Active Prompting",
        "Problem Statement": "Current adversarial defenses for language models often rely on passive strategies, such as data augmentation or model ensembling, which may not effectively target the most critical vulnerabilities.",
        "Motivation": "Existing methods for improving language models' adversarial robustness, such as adversarial training and randomized smoothing, can be computationally expensive and may not adapt well to evolving attack patterns. Active learning has been shown to improve the sample efficiency and adaptivity of machine learning models by strategically selecting the most informative examples for training. We hypothesize that actively selecting the most critical prompts for robustness evaluation and enhancement can also improve the efficiency and effectiveness of adversarial defenses for language models.",
        "Proposed Method": "We propose an active prompting framework called ARAP (Adversarial Robustness via Active Prompting) to iteratively improve the adversarial robustness of language models. In each iteration, ARAP first prompts the language model to generate a diverse set of potential adversarial prompts that could expose its vulnerabilities. Then, ARAP evaluates the generated prompts using an ensemble of adversarial attack and defense methods, and selects the most critical prompts that reveal new vulnerabilities or challenge existing defenses. Finally, ARAP prompts the language model to analyze the selected prompts and generate robust responses, which are used to fine-tune the model for enhanced robustness. By actively selecting and learning from the most informative prompts, ARAP enables the language model to efficiently adapt to new attack patterns and improve its worst-case robustness.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We will use a range of adversarial attack benchmarks, such as ANLI, Adversarial GLUE, and Dynabench. These datasets cover different adversarial attack types, such as word substitution, paraphrasing, and logical reasoning. We will use accuracy and robustness (e.g., accuracy on adversarial examples) as the evaluation metrics.",
            "Step 2: Implement Baselines": "We will implement several state-of-the-art passive defense methods as baselines: (1) Adversarial training: fine-tune the language model on a mix of clean and adversarial examples; (2) Randomized smoothing: make random perturbations to the input during inference to smooth out adversarial noise; (3) Ensemble adversarial training: train an ensemble of models with different random initializations and adversarial examples.",
            "Step 3: Implement ARAP": "We will implement the three key components of ARAP: (1) Adversarial prompt generation: use few-shot prompting to guide the language model to generate potential adversarial prompts. The prompt can be like 'Generate a prompt that might cause the model to produce an incorrect or inconsistent response.'; (2) Prompt evaluation and selection: evaluate the generated prompts using an ensemble of attack and defense methods (e.g., TextFooler attack, BERT-Defense), and select the most critical prompts based on the drop in accuracy or robustness score; (3) Robust prompt learning: prompt the language model to analyze the selected prompts and generate robust responses. The prompt can be like 'Analyze why the model might fail on this prompt and generate a response that addresses the potential vulnerabilities.'",
            "Step 4: Fine-tune with ARAP": "We will fine-tune the language model using the selected prompts and robust responses generated by ARAP. We will experiment with different fine-tuning strategies, such as mixing the ARAP prompts with the original training data, or using the ARAP prompts for few-shot learning.",
            "Step 5: Evaluate and Analyze": "We will evaluate the fine-tuned models on the adversarial attack benchmarks and compare their performance with the baselines. We will analyze the effectiveness of ARAP in terms of the number of iterations and prompts needed to achieve a certain level of robustness, and the model's adaptivity to new attack patterns. We will also study the quality and diversity of the generated adversarial prompts and robust responses to gain insights into the strengths and limitations of ARAP."
        },
        "Test Case Examples": {
            "Baseline Prompt Input (Adversarial Training)": "Premise: The man on the bench is wearing a black jacket and blue jeans. He has a red beard and is reading a book. Hypothesis: The man is not wearing a black jacket.",
            "Baseline Prompt Expected Output (Adversarial Training)": "Contradiction",
            "Proposed Prompt Input (ARAP; Step 1: Adversarial Prompt Generation)": "Generate a prompt that might cause the model to produce an incorrect or inconsistent response, based on the following example: Premise: The man on the bench is wearing a black jacket and blue jeans. He has a red beard and is reading a book. Hypothesis: The man is not wearing a black jacket. Label: Contradiction",
            "Proposed Prompt Expected Output (ARAP; Step 1: Adversarial Prompt Generation)": "Premise: The tall man on the wooden bench is wearing a dark black jacket and blue denim jeans. He has a long red beard and is reading a hardcover book. Hypothesis: The man on the bench is not wearing jeans.",
            "Proposed Prompt Input (ARAP; Step 2: Prompt Evaluation and Selection)": "Evaluate the following generated prompt using an ensemble of adversarial attack and defense methods, and select it if it reveals new vulnerabilities or challenges existing defenses: Premise: The tall man on the wooden bench is wearing a dark black jacket and blue denim jeans. He has a long red beard and is reading a hardcover book. Hypothesis: The man on the bench is not wearing jeans.",
            "Proposed Prompt Expected Output (ARAP; Step 2: Prompt Evaluation and Selection)": "The generated prompt is selected as a critical prompt, as it reveals potential vulnerabilities in the model's ability to handle paraphrased and more descriptive inputs.",
            "Proposed Prompt Input (ARAP; Step 3: Robust Prompt Learning)": "Analyze why the model might fail on the following prompt and generate a response that addresses the potential vulnerabilities: Premise: The tall man on the wooden bench is wearing a dark black jacket and blue denim jeans. He has a long red beard and is reading a hardcover book. Hypothesis: The man on the bench is not wearing jeans.",
            "Proposed Prompt Expected Output (ARAP; Step 3: Robust Prompt Learning)": "The model might fail on this prompt because it contains more descriptive and paraphrased information compared to the original example. The model needs to carefully track all the details mentioned in the premise, such as 'tall man', 'wooden bench', 'dark black jacket', 'blue denim jeans', 'long red beard', and 'hardcover book', and then accurately compare them with the hypothesis. The hypothesis is a contradiction because the premise clearly states that the man is wearing blue denim jeans.",
            "Explanation": "The proposed ARAP method actively generates adversarial prompts that can expose the model's vulnerabilities, selects the most critical prompts for robustness evaluation, and learns to generate robust responses. In contrast, adversarial training only passively learns from a fixed set of adversarial examples and may not generalize well to new attack patterns."
        },
        "Fallback Plan": "If the proposed ARAP method does not significantly improve the model's adversarial robustness compared to the baselines, we can conduct additional analyses to understand the limitations and inform future improvements: (1) Analyze the quality and diversity of the generated adversarial prompts to see if they are sufficiently challenging and cover different attack types. We can experiment with different prompt generation strategies, such as using more diverse examples or more targeted instructions; (2) Analyze the selected critical prompts to see if they reveal meaningful vulnerabilities or are just noisy outliers. We can experiment with different prompt evaluation and selection criteria, such as using more robust attack and defense methods or considering the model's confidence scores; (3) Analyze the generated robust responses to see if they provide useful insights for improving the model's robustness. We can experiment with different prompt learning strategies, such as using more informative instructions or incorporating human feedback; (4) If the above analyses do not yield actionable insights, we can pivot the project to focus on understanding the limitations of active prompting for adversarial robustness. For example, we can study the trade-off between robustness and accuracy, the transferability of the learned robustness across different models and datasets, and the computational efficiency of active prompting compared to passive defenses. These analyses can provide valuable insights for future research on adversarial robustness for language models."
    },
    "novelty_queries": [
        "KeywordQuery(\"adversarial robustness language models active learning\")",
        "KeywordQuery(\"language models adversarial attacks prompting\")",
        "KeywordQuery(\"active prompting adversarial defenses nlp\")",
        "KeywordQuery(\"Adversarial Robustness via Active Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "77d6d7482d1a32ad147c39993758b6c63816f5c0",
            "paperId": "77d6d7482d1a32ad147c39993758b6c63816f5c0",
            "title": "PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
            "abstract": "The increasing reliance on Large Language Models (LLMs) across academia and industry necessitates a comprehensive understanding of their robustness to prompts. In response to this vital need, we introduce PromptBench, a robustness benchmark designed to measure LLMs' resilience to adversarial prompts. This study uses a plethora of adversarial textual attacks targeting prompts across multiple levels: character, word, sentence, and semantic. The adversarial prompts, crafted to mimic plausible user errors like typos or synonyms, aim to evaluate how slight deviations can affect LLM outcomes while maintaining semantic integrity. These prompts are then employed in diverse tasks, such as sentiment analysis, natural language inference, reading comprehension, machine translation, and math problem-solving. Our study generates 4788 adversarial prompts, meticulously evaluated over 8 tasks and 13 datasets. Our findings demonstrate that contemporary LLMs are not robust to adversarial prompts. Furthermore, we present comprehensive analysis to understand the mystery behind prompt robustness and its transferability. We then offer insightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users. Code is available at: https://github.com/microsoft/promptbench.",
            "year": 2023,
            "citationCount": 111,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study generates 4788 adversarial prompts and presents comprehensive analysis to understand the mystery behind prompt robustness and its transferability, and offers insightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is improving the adversarial robustness of language models using active prompting, while the paper focuses on evaluating the robustness of language models against adversarial prompts.\n\nProposal: Improving adversarial robustness of language models using active prompting.\nPaper: Evaluating the robustness of language models against adversarial prompts.\n\nAlthough both works deal with adversarial prompts and robustness, the proposal aims to improve robustness using active prompting, while the paper focuses on evaluating robustness without proposing a specific improvement method.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "1ab91d6ac7afc1a0121487a9089fa70edc1634d4",
            "paperId": "1ab91d6ac7afc1a0121487a9089fa70edc1634d4",
            "title": "Certifying LLM Safety against Adversarial Prompting",
            "abstract": "Large language models (LLMs) are vulnerable to adversarial attacks that add malicious tokens to an input prompt to bypass the safety guardrails of an LLM and cause it to produce harmful content. In this work, we introduce erase-and-check, the first framework for defending against adversarial prompts with certifiable safety guarantees. Given a prompt, our procedure erases tokens individually and inspects the resulting subsequences using a safety filter. Our safety certificate guarantees that harmful prompts are not mislabeled as safe due to an adversarial attack up to a certain size. We implement the safety filter in two ways, using Llama 2 and DistilBERT, and compare the performance of erase-and-check for the two cases. We defend against three attack modes: i) adversarial suffix, where an adversarial sequence is appended at the end of a harmful prompt; ii) adversarial insertion, where the adversarial sequence is inserted anywhere in the middle of the prompt; and iii) adversarial infusion, where adversarial tokens are inserted at arbitrary positions in the prompt, not necessarily as a contiguous block. Our experimental results demonstrate that this procedure can obtain strong certified safety guarantees on harmful prompts while maintaining good empirical performance on safe prompts. Additionally, we propose three efficient empirical defenses: i) RandEC, a randomized subsampling version of erase-and-check; ii) GreedyEC, which greedily erases tokens that maximize the softmax score of the harmful class; and iii) GradEC, which uses gradient information to optimize tokens to erase. We demonstrate their effectiveness against adversarial prompts generated by the Greedy Coordinate Gradient (GCG) attack algorithm. The code for our experiments is available at https://github.com/aounon/certified-llm-safety.",
            "year": 2023,
            "citationCount": 48,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "E erase-and-check, the first framework for defending against adversarial prompts with certifiable safety guarantees, is introduced and it is demonstrated that this procedure can obtain strong certified safety guarantees on harmful prompts while maintaining good empirical performance on safe prompts."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is improving the adversarial robustness of language models using active prompting, while the paper focuses on certifying the safety of language models against adversarial prompts.\n\nThe approach in the proposal is to actively generate, evaluate, and learn from adversarial prompts to improve robustness, while the paper proposes an erase-and-check framework to certify safety guarantees against adversarial prompts.\n\nAlthough both works deal with adversarial prompts, the proposal aims to improve robustness through active learning, while the paper focuses on certifying safety through erasure and checking.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "cf56a7b28fb27279b1c94fb920b5722cf50c8852",
            "paperId": "cf56a7b28fb27279b1c94fb920b5722cf50c8852",
            "title": "AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs",
            "abstract": "While recently Large Language Models (LLMs) have achieved remarkable successes, they are vulnerable to certain jailbreaking attacks that lead to generation of inappropriate or harmful content. Manual red-teaming requires finding adversarial prompts that cause such jailbreaking, e.g. by appending a suffix to a given instruction, which is inefficient and time-consuming. On the other hand, automatic adversarial prompt generation often leads to semantically meaningless attacks that can easily be detected by perplexity-based filters, may require gradient information from the TargetLLM, or do not scale well due to time-consuming discrete optimization processes over the token space. In this paper, we present a novel method that uses another LLM, called the AdvPrompter, to generate human-readable adversarial prompts in seconds, $\\sim800\\times$ faster than existing optimization-based approaches. We train the AdvPrompter using a novel algorithm that does not require access to the gradients of the TargetLLM. This process alternates between two steps: (1) generating high-quality target adversarial suffixes by optimizing the AdvPrompter predictions, and (2) low-rank fine-tuning of the AdvPrompter with the generated adversarial suffixes. The trained AdvPrompter generates suffixes that veil the input instruction without changing its meaning, such that the TargetLLM is lured to give a harmful response. Experimental results on popular open source TargetLLMs show state-of-the-art results on the AdvBench dataset, that also transfer to closed-source black-box LLM APIs. Further, we demonstrate that by fine-tuning on a synthetic dataset generated by AdvPrompter, LLMs can be made more robust against jailbreaking attacks while maintaining performance, i.e. high MMLU scores.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents a novel method that uses another LLM, called the AdvPrompter, to generate human-readable adversarial prompts in seconds, faster than existing optimization-based approaches, and demonstrates that by fine-tuning on a synthetic dataset generated by AdvPrompter, LLMs can be made more robust against jailbreaking attacks while maintaining performance, i.e. high MMLU scores."
            },
            "score": 8,
            "novelty_score": "The research problem in the project proposal is improving the adversarial robustness of language models using active prompting. The proposed approach is to iteratively generate adversarial prompts, evaluate and select the most critical ones, and learn robust responses from them to fine-tune the model.\n\nThe research problem in the paper is generating adversarial prompts efficiently to test the robustness of language models against jailbreaking attacks. The proposed approach is to train another language model, AdvPrompter, to generate adversarial prompts without requiring access to the gradients of the target model.\n\nWhile both works aim to improve the adversarial robustness of language models, the project proposal focuses on an active learning approach to iteratively improve the model's robustness, whereas the paper focuses on efficiently generating adversarial prompts to test the model's robustness. The methods proposed in the two works are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "47030369e97cc44d4b2e3cf1be85da0fd134904a",
            "paperId": "47030369e97cc44d4b2e3cf1be85da0fd134904a",
            "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
            "abstract": "Because\"out-of-the-box\"large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called\"jailbreaks\"against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods. Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.",
            "year": 2023,
            "citationCount": 386,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information."
            },
            "score": 8,
            "novelty_score": "The research problem in the project proposal is improving the adversarial robustness of language models using active prompting methods. The proposed approach is to iteratively generate adversarial prompts, evaluate and select the most critical ones, and learn robust responses from them to fine-tune the model.\n\nThe research problem in the paper is generating transferable adversarial attacks on aligned language models to induce objectionable content. The proposed approach is to automatically find adversarial prompt suffixes that maximize the probability of generating undesirable responses, using greedy and gradient-based search techniques.\n\nThe project proposal focuses on improving robustness against adversarial attacks, while the paper focuses on generating effective adversarial attacks. The proposal uses active prompting to iteratively improve the model, while the paper uses automatic search techniques to find transferable adversarial suffixes.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "e8b3b37c0d301ea41c75765f6ceb7fcbb2e088a4",
            "paperId": "e8b3b37c0d301ea41c75765f6ceb7fcbb2e088a4",
            "title": "AutoDAN: Automatic and Interpretable Adversarial Attacks on Large Language Models",
            "abstract": "Safety alignment of Large Language Models (LLMs) can be compromised with manual jailbreak attacks and (automatic) adversarial attacks. Recent work suggests that patching LLMs against these attacks is possible: manual jailbreak attacks are human-readable but often limited and public, making them easy to block; adversarial attacks generate gibberish prompts that can be detected using perplexity-based filters. In this paper, we show that these solutions may be too optimistic. We propose an interpretable adversarial attack, AutoDAN , that combines the strengths of both types of attacks. It automatically generates attack prompts that bypass perplexity-based filters while maintaining a high attack success rate like manual jailbreak attacks. These prompts are interpretable and diverse, exhibiting strategies commonly used in manual jailbreak attacks, and transfer better than their non-readable counterparts when using limited training data or a single proxy model. We also customize AutoDAN \u2019s objective to leak system prompts, another jailbreak application not addressed in the adversarial attack literature. Our work provides a new way to red-team LLMs and to understand the mechanism of jailbreak attacks.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An interpretable adversarial attack, AutoDAN, is proposed, that combines the strengths of both types of attacks and provides a new way to red-team LLMs and to understand the mechanism of jailbreak attacks."
            },
            "score": 8,
            "novelty_score": "The research problem in the project proposal is improving the adversarial robustness of language models using active prompting. The proposed approach is to iteratively generate adversarial prompts, evaluate and select the most critical ones, and learn robust responses from them to fine-tune the model.\n\nThe research problem in the paper is automatically generating interpretable adversarial attacks on large language models that can bypass perplexity-based filters. The proposed approach is AutoDAN, which generates readable and diverse attack prompts that exhibit strategies used in manual jailbreak attacks.\n\nThe project proposal focuses on improving robustness against adversarial attacks, while the paper focuses on generating more effective adversarial attacks. They have different goals and approaches.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "1227c2fcb8437441b7d72a29a4bc9eef1f5275d2",
            "paperId": "1227c2fcb8437441b7d72a29a4bc9eef1f5275d2",
            "title": "AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models",
            "abstract": "Safety alignment of Large Language Models (LLMs) can be compromised with manual jailbreak attacks and (automatic) adversarial attacks. Recent studies suggest that defending against these attacks is possible: adversarial attacks generate unlimited but unreadable gibberish prompts, detectable by perplexity-based filters; manual jailbreak attacks craft readable prompts, but their limited number due to the necessity of human creativity allows for easy blocking. In this paper, we show that these solutions may be too optimistic. We introduce AutoDAN, an interpretable, gradient-based adversarial attack that merges the strengths of both attack types. Guided by the dual goals of jailbreak and readability, AutoDAN optimizes and generates tokens one by one from left to right, resulting in readable prompts that bypass perplexity filters while maintaining high attack success rates. Notably, these prompts, generated from scratch using gradients, are interpretable and diverse, with emerging strategies commonly seen in manual jailbreak attacks. They also generalize to unforeseen harmful behaviors and transfer to black-box LLMs better than their unreadable counterparts when using limited training data or a single proxy model. Furthermore, we show the versatility of AutoDAN by automatically leaking system prompts using a customized objective. Our work offers a new way to red-team LLMs and understand jailbreak mechanisms via interpretability.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work offers a new way to red-team LLMs and understand jailbreak mechanisms via interpretability, by introducing AutoDAN, an interpretable, gradient-based adversarial attack that merges the strengths of both attack types."
            },
            "score": 8,
            "novelty_score": "The research problem in the project proposal is improving the adversarial robustness of language models using active prompting. The proposed approach is to iteratively generate adversarial prompts, select the most critical ones, and learn robust responses to them.\n\nThe research problem in the paper is generating interpretable and readable adversarial prompts that can bypass perplexity filters and maintain high attack success rates. The proposed approach is a gradient-based adversarial attack called AutoDAN that optimizes and generates tokens one by one from left to right.\n\nThe project proposal focuses on improving adversarial robustness, while the paper focuses on generating adversarial attacks. The project uses active prompting, while the paper uses gradient-based optimization.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "de2fd685f45ee916b9142bcb983d306b7da643a4",
            "paperId": "de2fd685f45ee916b9142bcb983d306b7da643a4",
            "title": "A Prompting-based Approach for Adversarial Example Generation and Robustness Enhancement",
            "abstract": "Recent years have seen the wide application of NLP models in crucial areas such as finance, medical treatment, and news media, raising concerns of the model robustness and vulnerabilities. In this paper, we propose a novel prompt-based adversarial attack to compromise NLP models and robustness enhancement technique. We first construct malicious prompts for each instance and generate adversarial examples via mask-and-filling under the effect of a malicious purpose. Our attack technique targets the inherent vulnerabilities of NLP models, allowing us to generate samples even without interacting with the victim NLP model, as long as it is based on pre-trained language models (PLMs). Furthermore, we design a prompt-based adversarial training method to improve the robustness of PLMs. As our training method does not actually generate adversarial samples, it can be applied to large-scale training sets efficiently. The experimental results show that our attack method can achieve a high attack success rate with more diverse, fluent and natural adversarial examples. In addition, our robustness enhancement method can significantly improve the robustness of models to resist adversarial attacks. Our work indicates that prompting paradigm has great potential in probing some fundamental flaws of PLMs and fine-tuning them for downstream tasks.",
            "year": 2022,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel prompt-based adversarial attack to compromise NLP models and robustness enhancement technique that can significantly improve the robustness of models to resist adversarial attacks and indicates that prompting paradigm has great potential in probing some fundamental flaws of PLMs and fine-tuning them for downstream tasks."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is improving the adversarial robustness of language models using active prompting. The approach is to iteratively generate adversarial prompts, select the most critical ones, and learn robust responses from them to fine-tune the model.\n\nThe research problem in the paper is also improving the adversarial robustness of language models, and the approach is also based on prompting. However, the paper focuses on generating adversarial examples via mask-and-filling under malicious prompts, and using prompt-based adversarial training without actually generating adversarial samples.\n\nWhile both works aim to improve adversarial robustness using prompting, the proposal focuses on active prompting to iteratively generate and select critical prompts, while the paper focuses on generating adversarial examples and prompt-based adversarial training. The methods have some differences.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "2c72ab10e7a5f2fd32e6f85b20c77bf64e6e220d",
            "paperId": "2c72ab10e7a5f2fd32e6f85b20c77bf64e6e220d",
            "title": "A prompt-based approach to adversarial example generation and robustness enhancement",
            "abstract": null,
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel robust training approach based on prompt paradigm which incorporates prompt texts as the alternatives to adversarial examples and enhances robustness under a lightweight minimax-style optimization framework is proposed."
            },
            "score": 8,
            "novelty_score": "The research problem in the project proposal is improving the adversarial robustness of language models using active prompting. The proposed approach is to iteratively generate adversarial prompts, select the most critical ones, and learn robust responses to fine-tune the model.\n\nThe research problem in the paper is also enhancing robustness, and the approach is also based on the prompt paradigm. However, the paper uses prompt texts as alternatives to adversarial examples under a minimax-style optimization framework, which is different from the active prompting approach in the proposal.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "8436897e713c2242d6291df9a6a33c1544d4dd39",
            "paperId": "8436897e713c2242d6291df9a6a33c1544d4dd39",
            "title": "Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models",
            "abstract": "Large-scale pre-trained language models have achieved tremendous success across a wide range of natural language understanding (NLU) tasks, even surpassing human performance. However, recent studies reveal that the robustness of these models can be challenged by carefully crafted textual adversarial examples. While several individual datasets have been proposed to evaluate model robustness, a principled and comprehensive benchmark is still missing. In this paper, we present Adversarial GLUE (AdvGLUE), a new multi-task benchmark to quantitatively and thoroughly explore and evaluate the vulnerabilities of modern large-scale language models under various types of adversarial attacks. In particular, we systematically apply 14 textual adversarial attack methods to GLUE tasks to construct AdvGLUE, which is further validated by humans for reliable annotations. Our findings are summarized as follows. (i) Most existing adversarial attack algorithms are prone to generating invalid or ambiguous adversarial examples, with around 90% of them either changing the original semantic meanings or misleading human annotators as well. Therefore, we perform a careful filtering process to curate a high-quality benchmark. (ii) All the language models and robust training methods we tested perform poorly on AdvGLUE, with scores lagging far behind the benign accuracy. We hope our work will motivate the development of new adversarial attacks that are more stealthy and semantic-preserving, as well as new robust language models against sophisticated adversarial attacks. AdvGLUE is available at https://adversarialglue.github.io.",
            "year": 2021,
            "citationCount": 126,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work systematically applies 14 textual adversarial attack methods to GLUE tasks to construct AdvGLUE, a new multi-task benchmark to quantitatively and thoroughly explore and evaluate the vulnerabilities of modern large-scale language models under various types of adversarial attacks."
            },
            "score": 7,
            "novelty_score": "The research problem in the project proposal is improving the adversarial robustness of language models using active prompting methods. The approach is to iteratively generate adversarial prompts, evaluate and select the most critical ones, and learn robust responses from them to fine-tune the model.\n\nThe research problem in the paper is to create a comprehensive benchmark for evaluating the robustness of language models under various adversarial attacks. The approach is to systematically apply different adversarial attack methods to GLUE tasks and create a validated benchmark.\n\nThe project proposal focuses on a novel method for improving robustness, while the paper focuses on creating a benchmark for evaluating robustness. They have different goals and approaches.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "620a1a585a5e433a47103c112de17553a81fcbe6",
            "paperId": "620a1a585a5e433a47103c112de17553a81fcbe6",
            "title": "Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks",
            "abstract": "Although remarkable progress has been achieved in preventing large language model (LLM) hallucinations using instruction tuning and retrieval augmentation, it remains challenging to measure the reliability of LLMs using human-crafted evaluation data which is not available for many tasks and domains and could suffer from data leakage. Inspired by adversarial machine learning, this paper aims to develop a method of automatically generating evaluation data by appropriately modifying existing data on which LLMs behave faithfully. Specifically, this paper presents AutoDebug, an LLM-based framework to use prompting chaining to generate transferable adversarial attacks in the form of question-answering examples. We seek to understand the extent to which these examples trigger the hallucination behaviors of LLMs. We implement AutoDebug using ChatGPT and evaluate the resulting two variants of a popular open-domain question-answering dataset, Natural Questions (NQ), on a collection of open-source and proprietary LLMs under various prompting settings. Our generated evaluation data is human-readable and, as we show, humans can answer these modified questions well. Nevertheless, we observe pronounced accuracy drops across multiple LLMs including GPT-4. Our experimental results show that LLMs are likely to hallucinate in two categories of question-answering scenarios where (1) there are conflicts between knowledge given in the prompt and their parametric knowledge, or (2) the knowledge expressed in the prompt is complex. Finally, we find that the adversarial examples generated by our method are transferable across all considered LLMs. The examples generated by a small model can be used to debug a much larger model, making our approach cost-effective.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents AutoDebug, an LLM-based framework to use prompting chaining to generate transferable adversarial attacks in the form of question-answering examples, and finds that the adversarial examples generated by the method are transferable across all considered LLMs."
            },
            "score": 7,
            "novelty_score": "The research problem in the project proposal is improving the adversarial robustness of language models, and the proposed approach is an active prompting framework called ARAP that iteratively generates, evaluates, and learns from critical adversarial prompts.\n\nThe research problem in the paper is automatically generating evaluation data to measure the reliability of language models in terms of hallucination, and the proposed approach is an LLM-based framework called AutoDebug that uses prompting chaining to generate transferable adversarial attacks in the form of question-answering examples.\n\nThe project proposal focuses on improving the model's robustness against adversarial attacks, while the paper focuses on generating adversarial examples to evaluate the model's hallucination behavior. Although both involve adversarial examples, their goals and approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "dceabf5eec3d1ef36c938fae8defbf6775d487f1",
            "paperId": "dceabf5eec3d1ef36c938fae8defbf6775d487f1",
            "title": "Eliciting Language Model Behaviors using Reverse Language Models",
            "abstract": "Despite advances in fine-tuning methods, language models (LMs) continue to output toxic and harmful responses on worst-case inputs, including adversarial attacks and jailbreaks. We train an LM on tokens in reverse order\u2014a reverse LM \u2014as a tool for identifying such worst-case inputs. By prompting a reverse LM with a problematic string, we can sample prefixes that are likely to precede the problematic suffix. We test our reverse LM by using it to guide beam search for prefixes that have high probability of generating toxic statements when input to a forwards LM. Our 160m parameter reverse LM outperforms the existing state-of-the-art adversarial attack method, GCG, when measuring the probability of toxic continuations from the Pythia-160m LM. We also find that the prefixes generated by our reverse LM for the Pythia model are more likely to transfer to other models, eliciting toxic responses also from Llama 2 when compared to GCG-generated attacks.",
            "year": null,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The 160m parameter reverse LM outperforms the existing state-of-the-art adversarial attack method, GCG, when measuring the probability of toxic continuations from the Pythia-160m LM, and the prefixes generated by the reverse LM for the Pythia model are more likely to transfer to other models."
            },
            "score": 7
        },
        {
            "id": "b6499bcc10d4a70c3ca8b84995270cfd0d29de4c",
            "paperId": "b6499bcc10d4a70c3ca8b84995270cfd0d29de4c",
            "title": "Model-tuning Via Prompts Makes NLP Models Adversarially Robust",
            "abstract": "In recent years, NLP practitioners have converged on the following practice: (i) import an off-the-shelf pretrained (masked) language model; (ii) append a multilayer perceptron atop the CLS token's hidden representation (with randomly initialized weights); and (iii) fine-tune the entire model on a downstream task (MLP-FT). This procedure has produced massive gains on standard NLP benchmarks, but these models remain brittle, even to mild adversarial perturbations. In this work, we demonstrate surprising gains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an alternative method of adapting to downstream tasks. Rather than appending an MLP head to make output prediction, MVP appends a prompt template to the input, and makes prediction via text infilling/completion. Across 5 NLP datasets, 4 adversarial attacks, and 3 different models, MVP improves performance against adversarial substitutions by an average of 8% over standard methods and even outperforms adversarial training-based state-of-art defenses by 3.5%. By combining MVP with adversarial training, we achieve further improvements in adversarial robustness while maintaining performance on unperturbed examples. Finally, we conduct ablations to investigate the mechanism underlying these gains. Notably, we find that the main causes of vulnerability of MLP-FT can be attributed to the misalignment between pre-training and fine-tuning tasks, and the randomly initialized MLP parameters.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work demonstrates surprising gains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an alternative method of adapting to downstream tasks that improves performance against adversarial substitutions and outperforms adversarial training-based state-of-art defenses by 3.5%."
            },
            "score": 7
        },
        {
            "id": "d42625e5d8c0ba481fc58be28849c3231250aa0b",
            "paperId": "d42625e5d8c0ba481fc58be28849c3231250aa0b",
            "title": "JAB: Joint Adversarial Prompting and Belief Augmentation",
            "abstract": "With the recent surge of language models in different applications, attention to safety and robustness of these models has gained significant importance. Here we introduce a joint framework in which we simultaneously probe and improve the robustness of a black-box target model via adversarial prompting and belief augmentation using iterative feedback loops. This framework utilizes an automated red teaming approach to probe the target model, along with a belief augmenter to generate instructions for the target model to improve its robustness to those adversarial probes. Importantly, the adversarial model and the belief generator leverage the feedback from past interactions to improve the effectiveness of the adversarial prompts and beliefs, respectively. In our experiments, we demonstrate that such a framework can reduce toxic content generation both in dynamic cases where an adversary directly interacts with a target model and static cases where we use a static benchmark dataset to evaluate our model.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A joint framework in which an automated red teaming approach is used to probe and improve the robustness of a black-box target model via adversarial prompting and belief augmentation using iterative feedback loops is introduced."
            },
            "score": 7
        },
        {
            "id": "8ecdbfe011b7189fa0ee49ffc4e42a93d728a371",
            "paperId": "8ecdbfe011b7189fa0ee49ffc4e42a93d728a371",
            "title": "On Evaluating Adversarial Robustness of Large Vision-Language Models",
            "abstract": "Large vision-language models (VLMs) such as GPT-4 have achieved unprecedented performance in response generation, especially with visual inputs, enabling more creative and adaptable interaction than large language models such as ChatGPT. Nonetheless, multimodal generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable modality (e.g., vision). To this end, we propose evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses. In particular, we first craft targeted adversarial examples against pretrained models such as CLIP and BLIP, and then transfer these adversarial examples to other VLMs such as MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt. In addition, we observe that black-box queries on these VLMs can further improve the effectiveness of targeted evasion, resulting in a surprisingly high success rate for generating targeted responses. Our findings provide a quantitative understanding regarding the adversarial vulnerability of large VLMs and call for a more thorough examination of their potential security flaws before deployment in practice. Code is at https://github.com/yunqing-me/AttackVLM.",
            "year": 2023,
            "citationCount": 47,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses is proposed."
            },
            "score": 6
        },
        {
            "id": "3e30a7ac4886b28eb50151f58e14a1d698cccd0e",
            "paperId": "3e30a7ac4886b28eb50151f58e14a1d698cccd0e",
            "title": "Baseline Defenses for Adversarial Attacks Against Aligned Language Models",
            "abstract": "As Large Language Models quickly become ubiquitous, it becomes critical to understand their security vulnerabilities. Recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. Drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: What threat models are practically useful in this domain? How do baseline defense techniques perform in this new domain? How does LLM security differ from computer vision? We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, discussing the various settings in which each is feasible and effective. Particularly, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. We discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. We find that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for LLMs. Future research will be needed to uncover whether more powerful optimizers can be developed, or whether the strength of filtering and preprocessing defenses is greater in the LLMs domain than it has been in computer vision.",
            "year": 2023,
            "citationCount": 97,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for LLMs."
            },
            "score": 6
        },
        {
            "id": "a4f533f2b7d77b667e1f05b210924ec7c90cc5d1",
            "paperId": "a4f533f2b7d77b667e1f05b210924ec7c90cc5d1",
            "title": "How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness?",
            "abstract": "The fine-tuning of pre-trained language models has a great success in many NLP fields. Yet, it is strikingly vulnerable to adversarial examples, e.g., word substitution attacks using only synonyms can easily fool a BERT-based sentiment analysis model. In this paper, we demonstrate that adversarial training, the prevalent defense technique, does not directly fit a conventional fine-tuning scenario, because it suffers severely from catastrophic forgetting: failing to retain the generic and robust linguistic features that have already been captured by the pre-trained model. In this light, we propose Robust Informative Fine-Tuning (RIFT), a novel adversarial fine-tuning method from an information-theoretical perspective. In particular, RIFT encourages an objective model to retain the features learned from the pre-trained model throughout the entire fine-tuning process, whereas a conventional one only uses the pre-trained weights for initialization. Experimental results show that RIFT consistently outperforms the state-of-the-arts on two popular NLP tasks: sentiment analysis and natural language inference, under different attacks across various pre-trained language models.",
            "year": 2021,
            "citationCount": 40,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Robust Informative Fine-Tuning (RIFT), a novel adversarial fine-tuning method from an information-theoretical perspective, which encourages an objective model to retain the features learned from the pre-trained model throughout the entire fine- Tuning process, whereas a conventional one only uses thePre-trained weights for initialization."
            },
            "score": 6
        },
        {
            "id": "16596dd03fa40ba278f9533ea9986982dcc81fb6",
            "paperId": "16596dd03fa40ba278f9533ea9986982dcc81fb6",
            "title": "Understanding Zero-Shot Adversarial Robustness for Large-Scale Models",
            "abstract": "Pretrained large-scale vision-language models like CLIP have exhibited strong generalization over unseen tasks. Yet imperceptible adversarial perturbations can significantly reduce CLIP's performance on new tasks. In this work, we identify and explore the problem of \\emph{adapting large-scale models for zero-shot adversarial robustness}. We first identify two key factors during model adaption -- training losses and adaptation methods -- that affect the model's zero-shot adversarial robustness. We then propose a text-guided contrastive adversarial training loss, which aligns the text embeddings and the adversarial visual features with contrastive learning on a small set of training data. We apply this training loss to two adaption methods, model finetuning and visual prompt tuning. We find that visual prompt tuning is more effective in the absence of texts, while finetuning wins in the existence of text guidance. Overall, our approach significantly improves the zero-shot adversarial robustness over CLIP, seeing an average improvement of over 31 points over ImageNet and 15 zero-shot datasets. We hope this work can shed light on understanding the zero-shot adversarial robustness of large-scale models.",
            "year": 2022,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work identifies two key factors during model adaption -- training losses and adaptation methods -- that affect the model's zero-shot adversarial robustness, and proposes a text-guided contrastive adversarial training loss, which aligns the text embeddings and the adversarial visual features with contrastive learning on a small set of training data."
            },
            "score": 6
        },
        {
            "id": "7aa38b85fa8cba64d6a4010543f6695dbf5f1386",
            "paperId": "7aa38b85fa8cba64d6a4010543f6695dbf5f1386",
            "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
            "abstract": "Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at this https URL and this https URL.",
            "year": 2017,
            "citationCount": 9511,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work studies the adversarial robustness of neural networks through the lens of robust optimization, and suggests the notion of security against a first-order adversary as a natural and broad security guarantee."
            },
            "score": 6
        },
        {
            "id": "8fdd34153d1035d09dd4a6efa9cb0c91d23d0045",
            "paperId": "8fdd34153d1035d09dd4a6efa9cb0c91d23d0045",
            "title": "More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models",
            "abstract": "We are currently witnessing dramatic advances in the capabilities of Large Language Models (LLMs). They are already being adopted in practice and integrated into many systems, including integrated development environments (IDEs) and search engines. The functionalities of current LLMs can be modulated via natural language prompts, while their exact internal functionality remains implicit and unassessable. This property, which makes them adaptable to even unseen tasks, might also make them susceptible to targeted adversarial prompting . Recently, several ways to misalign LLMs using Prompt Injection (PI) attacks have been introduced. In such attacks, an adversary can prompt the LLM to produce malicious content or override the original instructions and the employed \ufb01ltering schemes. Recent work showed that these attacks are hard to mitigate, as state-of-the-art LLMs are instruction-following . So far, these attacks assumed that the adversary is directly prompting the LLM. In this work, we show that augmenting LLMs with retrieval and API calling capabilities (so-called Application-Integrated LLMs ) induces a whole new set of attack vectors. These LLMs might process poisoned content retrieved from the Web that contains malicious prompts pre-injected and selected by adversaries. We demonstrate that an attacker can indirectly perform such PI attacks. Based on this key insight, we systematically analyze the resulting threat landscape of Application-Integrated LLMs and discuss a variety of new attack vectors. To demonstrate the practical viability of our attacks, we implemented speci\ufb01c demonstrations",
            "year": 2023,
            "citationCount": 73,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work shows that augmenting LLMs with retrieval and API calling capabilities (so-called Application-Integrated LLMs) induces a whole new set of attack vectors and systematically analyzes the resulting threat landscape of Application-Integrated LLMs."
            },
            "score": 6
        },
        {
            "id": "40ee4949c1050a465d418deb6dd7ea6304a3bc29",
            "paperId": "40ee4949c1050a465d418deb6dd7ea6304a3bc29",
            "title": "Adversarial Attacks and Defenses in Large Language Models: Old and New Threats",
            "abstract": "Over the past decade, there has been extensive research aimed at enhancing the robustness of neural networks, yet this problem remains vastly unsolved. Here, one major impediment has been the overestimation of the robustness of new defense approaches due to faulty defense evaluations. Flawed robustness evaluations necessitate rectifications in subsequent works, dangerously slowing down the research and providing a false sense of security. In this context, we will face substantial challenges associated with an impending adversarial arms race in natural language processing, specifically with closed-source Large Language Models (LLMs), such as ChatGPT, Google Bard, or Anthropic's Claude. We provide a first set of prerequisites to improve the robustness assessment of new approaches and reduce the amount of faulty evaluations. Additionally, we identify embedding space attacks on LLMs as another viable threat model for the purposes of generating malicious content in open-sourced models. Finally, we demonstrate on a recently proposed defense that, without LLM-specific best practices in place, it is easy to overestimate the robustness of a new approach.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work provides a first set of prerequisites to improve the robustness assessment of new approaches and reduce the amount of faulty evaluations, and identifies embedding space attacks on LLMs as another viable threat model for the purposes of generating malicious content in open-sourced models."
            },
            "score": 6
        },
        {
            "id": "b5a624da64475d735f0e298dc6f2f6669b5bb697",
            "paperId": "b5a624da64475d735f0e298dc6f2f6669b5bb697",
            "title": "Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield",
            "abstract": "Large Language Models' safety remains a critical concern due to their vulnerability to adversarial attacks, which can prompt these systems to produce harmful responses. In the heart of these systems lies a safety classifier, a computational model trained to discern and mitigate potentially harmful, offensive, or unethical outputs. However, contemporary safety classifiers, despite their potential, often fail when exposed to inputs infused with adversarial noise. In response, our study introduces the Adversarial Prompt Shield (APS), a lightweight model that excels in detection accuracy and demonstrates resilience against adversarial prompts. Additionally, we propose novel strategies for autonomously generating adversarial training datasets, named Bot Adversarial Noisy Dialogue (BAND) datasets. These datasets are designed to fortify the safety classifier's robustness, and we investigate the consequences of incorporating adversarial examples into the training process. Through evaluations involving Large Language Models, we demonstrate that our classifier has the potential to decrease the attack success rate resulting from adversarial attacks by up to 60%. This advancement paves the way for the next generation of more reliable and resilient conversational agents.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study introduces the Adversarial Prompt Shield (APS), a lightweight model that excels in detection accuracy and demonstrates resilience against adversarial prompts, and proposes novel strategies for autonomously generating adversarial training datasets, designed to fortify the safety classifier's robustness."
            },
            "score": 6
        },
        {
            "id": "9a0f53a0ff25d1a5fb016aaa22185c4d6b5a2ac8",
            "paperId": "9a0f53a0ff25d1a5fb016aaa22185c4d6b5a2ac8",
            "title": "LinkPrompt: Natural and Universal Adversarial Attacks on Prompt-based Language Models",
            "abstract": "Prompt-based learning is a new language model training paradigm that adapts the Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes the performance benchmarks across various natural language processing (NLP) tasks. Instead of using a fixed prompt template to fine-tune the model, some research demonstrates the effectiveness of searching for the prompt via optimization. Such prompt optimization process of prompt-based learning on PLMs also gives insight into generating adversarial prompts to mislead the model, raising concerns about the adversarial vulnerability of this paradigm. Recent studies have shown that universal adversarial triggers (UATs) can be generated to alter not only the predictions of the target PLMs but also the prediction of corresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based learning paradigm. However, UATs found in previous works are often unreadable tokens or characters and can be easily distinguished from natural texts with adaptive defenses. In this work, we consider the naturalness of the UATs and develop $\\textit{LinkPrompt}$, an adversarial attack algorithm to generate UATs by a gradient-based beam search algorithm that not only effectively attacks the target PLMs and PFMs but also maintains the naturalness among the trigger tokens. Extensive results demonstrate the effectiveness of $\\textit{LinkPrompt}$, as well as the transferability of UATs generated by $\\textit{LinkPrompt}$ to open-sourced Large Language Model (LLM) Llama2 and API-accessed LLM GPT-3.5-turbo. The resource is available at $\\href{https://github.com/SavannahXu79/LinkPrompt}{https://github.com/SavannahXu79/LinkPrompt}$.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An adversarial attack algorithm to generate UATs by a gradient-based beam search algorithm that not only effectively attacks the target PLMs and PFMs but also maintains the naturalness among the trigger tokens is developed."
            },
            "score": 6
        },
        {
            "id": "32316f1d08911b397f52f8644f46f5c129619383",
            "paperId": "32316f1d08911b397f52f8644f46f5c129619383",
            "title": "Defending Against Indirect Prompt Injection Attacks With Spotlighting",
            "abstract": "Large Language Models (LLMs), while powerful, are built and trained to process a single text input. In common applications, multiple inputs can be processed by concatenating them together into a single stream of text. However, the LLM is unable to distinguish which sections of prompt belong to various input sources. Indirect prompt injection attacks take advantage of this vulnerability by embedding adversarial instructions into untrusted data being processed alongside user commands. Often, the LLM will mistake the adversarial instructions as user commands to be followed, creating a security vulnerability in the larger system. We introduce spotlighting, a family of prompt engineering techniques that can be used to improve LLMs' ability to distinguish among multiple sources of input. The key insight is to utilize transformations of an input to provide a reliable and continuous signal of its provenance. We evaluate spotlighting as a defense against indirect prompt injection attacks, and find that it is a robust defense that has minimal detrimental impact to underlying NLP tasks. Using GPT-family models, we find that spotlighting reduces the attack success rate from greater than {50}\\% to below {2}\\% in our experiments with minimal impact on task efficacy.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces spotlighting, a family of prompt engineering techniques that can be used to improve LLMs' ability to distinguish among multiple sources of input, and finds that it is a robust defense that has minimal detrimental impact to underlying NLP tasks."
            },
            "score": 6
        },
        {
            "id": "5d437f98b7f3532ba693f13744427e87d61ee952",
            "paperId": "5d437f98b7f3532ba693f13744427e87d61ee952",
            "title": "In and Out-of-Domain Text Adversarial Robustness via Label Smoothing",
            "abstract": "Recently it has been shown that state-of-the-art NLP models are vulnerable to adversarial attacks, where the predictions of a model can be drastically altered by slight modifications to the input (such as synonym substitutions). While several defense techniques have been proposed, and adapted, to the discrete nature of text adversarial attacks, the benefits of general-purpose regularization methods such as label smoothing for language models, have not been studied. In this paper, we study the adversarial robustness provided by label smoothing strategies in foundational models for diverse NLP tasks in both in-domain and out-of-domain settings. Our experiments show that label smoothing significantly improves adversarial robustness in pre-trained models like BERT, against various popular attacks. We also analyze the relationship between prediction confidence and robustness, showing that label smoothing reduces over-confident errors on adversarial examples.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experiments show that label smoothing significantly improves adversarial robustness in pre-trained models like BERT, against various popular attacks, and the relationship between prediction confidence and robustness is analyzed, showing thatlabel smoothing reduces over-confident errors on adversarial examples."
            },
            "score": 6
        },
        {
            "id": "bbd6d6874a8ca1c155bcfb540e8d55199944cdc5",
            "paperId": "bbd6d6874a8ca1c155bcfb540e8d55199944cdc5",
            "title": "RoAST: Robustifying Language Models via Adversarial Perturbation with Selective Training",
            "abstract": "Fine-tuning pre-trained language models (LMs) has become the de facto standard in many NLP tasks. Nevertheless, fine-tuned LMs are still prone to robustness issues, such as adversarial robustness and model calibration. Several perspectives of robustness for LMs have been studied independently, but lacking a unified consideration in multiple perspectives. In this paper, we propose Robustifying LMs via Adversarial perturbation with Selective Training (RoAST), a simple yet effective fine-tuning technique to enhance the multi-perspective robustness of LMs in a unified way. RoAST effectively incorporates two important sources for the model robustness, robustness on the perturbed inputs and generalizable knowledge in pre-trained LMs. To be specific, RoAST introduces adversarial perturbation during fine-tuning while the model parameters are selectively updated upon their relative importance to minimize unnecessary deviation. Under a unified evaluation of fine-tuned LMs by incorporating four representative perspectives of model robustness, we demonstrate the effectiveness of RoAST compared to state-of-the-art fine-tuning methods on six different types of LMs, which indicates its usefulness in practice.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Under a unified evaluation of fine-tuned LMs by incorporating four representative perspectives of model robustness, the effectiveness of RoAST is demonstrated compared to state-of-the-art fine- tuning methods on six different types of LMs, which indicates its usefulness in practice."
            },
            "score": 6
        },
        {
            "id": "160b67f793ce4d8e0492b3b7918aaefd44350f92",
            "paperId": "160b67f793ce4d8e0492b3b7918aaefd44350f92",
            "title": "Generating Prompt-Based Adversarial Text Examples via Variable Neighborhood Search",
            "abstract": "Natural Language Processing (NLP) models are immensely vulnerable to adversarial text examples. Various word-level attacks have been proposed to modify input texts by carefully-picked substitute words via static or dynamic opti-mization algorithms. However, existing word-level attack methods usually ignore text fluency and semantic consistency for seeking a high attack success ratio, often resulting in unnatural adversarial text examples. In this paper, we propose to generate Prompt-based adversarial texts via Variable Neighborhood Search (P-VNS), which achieves a high attack success ratio while simulta-neously keeping text fluency and semantic similarity. Specifically, the well-designed prompt texts are constructed for input texts and the substitute words are obtained by mask-and-filling procedure under the effect of prompt texts, so the text fluency and semantic similarity can be enhanced. Additionally, the word modification priority is adaptively determined by employing the variable neighborhood search algorithm, yielding an improvement in the attack success ratio. Extensive experiments demonstrate that the P- VNS accomplishes the highest attack success ratio meanwhile preserving text fluency and semantic similarity. Besides, the pro-posed P- VNS also manifests effectiveness in adversarial training and transfer attack.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes to generate Prompt-based adversarial texts via Variable Neighborhood Search (P-VNS), which achieves a high attack success ratio while simulta-neously keeping text fluency and semantic similarity."
            },
            "score": 6
        },
        {
            "id": "1abfc211793c683972ded8d3268475e3ee7a88b0",
            "paperId": "1abfc211793c683972ded8d3268475e3ee7a88b0",
            "title": "Adversarial Demonstration Attacks on Large Language Models",
            "abstract": "With the emergence of more powerful large language models (LLMs), such as ChatGPT and GPT-4, in-context learning (ICL) has gained significant prominence in leveraging these models for specific tasks by utilizing data-label pairs as precondition prompts. While incorporating demonstrations can greatly enhance the performance of LLMs across various tasks, it may introduce a new security concern: attackers can manipulate only the demonstrations without changing the input to perform an attack. In this paper, we investigate the security concern of ICL from an adversarial perspective, focusing on the impact of demonstrations. We propose a novel attack method named advICL, which aims to manipulate only the demonstration without changing the input to mislead the models. Our results demonstrate that as the number of demonstrations increases, the robustness of in-context learning would decrease. Additionally, we also identify the intrinsic property of the demonstrations is that they can be used (prepended) with different inputs. As a result, it introduces a more practical threat model in which an attacker can attack the test input example even without knowing and manipulating it. To achieve it, we propose the transferable version of advICL, named Transferable-advICL. Our experiment shows that the adversarial demonstration generated by Transferable-advICL can successfully attack the unseen test input examples. We hope that our study reveals the critical security risks associated with ICL and underscores the need for extensive research on the robustness of ICL, particularly given its increasing significance in the advancement of LLMs.",
            "year": 2023,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper investigates the security concern of ICL from an adversarial perspective, focusing on the impact of demonstrations, and proposes a novel attack method named advICL, which aims to manipulate only the demonstration without changing the input to mislead the models."
            },
            "score": 5
        },
        {
            "id": "91c37a88c2b320725057260677ae79f3cdaa492b",
            "paperId": "91c37a88c2b320725057260677ae79f3cdaa492b",
            "title": "Active Learning Principles for In-Context Learning with Large Language Models",
            "abstract": "The remarkable advancements in large language models (LLMs) have significantly enhanced the performance in few-shot learning settings. By using only a small number of labeled examples, referred to as demonstrations, LLMs can effectively grasp the task at hand through in-context learning. However, the process of selecting appropriate demonstrations has received limited attention in prior work. This paper addresses the issue of identifying the most informative demonstrations for few-shot learning by approaching it as a pool-based Active Learning (AL) problem over a single iteration. Our objective is to investigate how AL algorithms can serve as effective demonstration selection methods for in-context learning. We compare various standard AL algorithms based on uncertainty, diversity, and similarity, and consistently observe that the latter outperforms all other methods, including random sampling. Notably, uncertainty sampling, despite its success in conventional supervised learning scenarios, performs poorly in this context. Our extensive experimentation involving a diverse range of GPT and OPT models across $24$ classification and multi-choice tasks, coupled with thorough analysis, unambiguously demonstrates that in-context example selection through AL prioritizes high-quality examples that exhibit low uncertainty and bear similarity to the test examples.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper addresses the issue of identifying the most informative demonstrations for few-shot learning by approaching it as a pool-based Active Learning (AL) problem over a single iteration, and unambiguously demonstrates that in-context example selection through AL prioritizes high-quality examples that exhibit low uncertainty and bear similarity to the test examples."
            },
            "score": 5
        },
        {
            "id": "386806bbd9d84e644ded24d4a9c7ea805c273891",
            "paperId": "386806bbd9d84e644ded24d4a9c7ea805c273891",
            "title": "FreeAL: Towards Human-Free Active Learning in the Era of Large Language Models",
            "abstract": "Collecting high-quality labeled data for model training is notoriously time-consuming and labor-intensive for various NLP tasks. While copious solutions, such as active learning for small language models (SLMs) and prevalent in-context learning in the era of large language models (LLMs), have been proposed and alleviate the labeling burden to some extent, their performances are still subject to human intervention. It is still underexplored how to reduce the annotation cost in the LLMs era. To bridge this, we revolutionize traditional active learning and propose an innovative collaborative learning framework FreeAL to interactively distill and filter the task-specific knowledge from LLMs. During collaborative training, an LLM serves as an active annotator inculcating its coarse-grained knowledge, while a downstream SLM is incurred as a student to filter out high-quality in-context samples to feedback LLM for the subsequent label refinery. Extensive experiments on eight benchmark datasets demonstrate that FreeAL largely enhances the zero-shot performances for both SLM and LLM without any human supervision. The code is available at https://github.com/Justherozen/FreeAL .",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work revolutionizes traditional active learning and proposes an innovative collaborative learning framework FreeAL to interactively distill and filter the task-specific knowledge from LLMs to significantly enhance the zero-shot performances for both SLM and LLM without any human supervision."
            },
            "score": 5
        },
        {
            "id": "5690e35b8beab92a80055fe2530c29c24e495379",
            "paperId": "5690e35b8beab92a80055fe2530c29c24e495379",
            "title": "On the Adversarial Robustness of Multi-Modal Foundation Models",
            "abstract": "Multi-modal foundation models combining vision and language models such as Flamingo or GPT-4 have recently gained enormous interest. Alignment of foundation models is used to prevent models from providing toxic or harmful output. While malicious users have successfully tried to jailbreak foundation models, an equally important question is if honest users could be harmed by malicious third-party content. In this paper we show that imperceivable attacks on images $\\left({{\\varepsilon _\\infty } = 1/255}\\right)$ in order to change the caption output of a multi-modal foundation model can be used by malicious content providers to harm honest users e.g. by guiding them to malicious websites or broadcast fake information. This indicates that countermeasures to adversarial attacks should be used by any deployed multi-modal foundation model. Note: This paper contains fake information to illustrate the outcome of our attacks. It does not reflect the opinion of the authors.",
            "year": 2023,
            "citationCount": 25,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that imperceivable attacks on images in order to change the caption output of a multi-modal foundation model can be used by malicious content providers to harm honest users e.g. by guiding them to malicious websites or broadcast fake information, indicating that countermeasures to adversarial attacks should be used by any deployed multi-modal foundation model."
            },
            "score": 5
        },
        {
            "id": "3c0c14882318fd7ad3fd51cdc5fc515a7f78effd",
            "paperId": "3c0c14882318fd7ad3fd51cdc5fc515a7f78effd",
            "title": "LLMaAA: Making Large Language Models as Active Annotators",
            "abstract": "Prevalent supervised learning methods in natural language processing (NLP) are notoriously data-hungry, which demand large amounts of high-quality annotated data. In practice, acquiring such data is a costly endeavor. Recently, the superior few-shot performance of large language models (LLMs) has propelled the development of dataset generation, where the training data are solely synthesized from LLMs. However, such an approach usually suffers from low-quality issues, and requires orders of magnitude more labeled data to achieve satisfactory performance. To fully exploit the potential of LLMs and make use of massive unlabeled data, we propose LLMaAA, which takes LLMs as annotators and puts them into an active learning loop to determine what to annotate efficiently. To learn robustly with pseudo labels, we optimize both the annotation and training processes: (1) we draw k-NN examples from a small demonstration pool as in-context examples, and (2) we adopt the example reweighting technique to assign training samples with learnable weights. Compared with previous approaches, LLMaAA features both efficiency and reliability. We conduct experiments and analysis on two classic NLP tasks, named entity recognition and relation extraction. With LLMaAA, task-specific models trained from LLM-generated labels can outperform the teacher within only hundreds of annotated examples, which is much more cost-effective than other baselines.",
            "year": 2023,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LLMaAA is proposed, which takes LLMs as annotators and puts them into an active learning loop to determine what to annotate efficiently, which can outperform the teacher within only hundreds of annotated examples, which is much more cost-effective than other baselines."
            },
            "score": 5
        },
        {
            "id": "dbac86036cb5ed4dd6bbdda4a8613b163e20ec90",
            "paperId": "dbac86036cb5ed4dd6bbdda4a8613b163e20ec90",
            "title": "Fundamental Limitations of Alignment in Large Language Models",
            "abstract": "An important aspect in developing language models that interact with humans is aligning their behavior to be useful and unharmful for their human users. This is usually achieved by tuning the model in a way that enhances desired behaviors and inhibits undesired ones, a process referred to as alignment. In this paper, we propose a theoretical approach called Behavior Expectation Bounds (BEB) which allows us to formally investigate several inherent characteristics and limitations of alignment in large language models. Importantly, we prove that within the limits of this framework, for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt. This implies that any alignment process that attenuates an undesired behavior but does not remove it altogether, is not safe against adversarial prompting attacks. Furthermore, our framework hints at the mechanism by which leading alignment approaches such as reinforcement learning from human feedback make the LLM prone to being prompted into the undesired behaviors. This theoretical result is being experimentally demonstrated in large scale by the so called contemporary\"chatGPT jailbreaks\", where adversarial users trick the LLM into breaking its alignment guardrails by triggering it into acting as a malicious persona. Our results expose fundamental limitations in alignment of LLMs and bring to the forefront the need to devise reliable mechanisms for ensuring AI safety.",
            "year": 2023,
            "citationCount": 82,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is proved that within the limits of this framework, for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt."
            },
            "score": 5
        },
        {
            "id": "b2fd6a1b86af52bd6aaa5988a1efc408df6477f3",
            "paperId": "b2fd6a1b86af52bd6aaa5988a1efc408df6477f3",
            "title": "ZDDR: A Zero-Shot Defender for Adversarial Samples Detection and Restoration",
            "abstract": "Natural language processing (NLP) models find extensive applications but face vulnerabilities against adversarial inputs. Traditional defenses lean heavily on supervised detection techniques, which makes them vulnerable to issues arising from training data quality, inherent biases, noise, or adversarial inputs. This study observed common compromises in sentence fluency during aggression. On this basis, the Zero Sample Defender (ZDDR) is introduced for adversarial sample detection and recovery without relying on prior knowledge. ZDDR combines the log probability calculated by the model and the syntactic normative score of a large language model (LLM) to detect adversarial examples. Furthermore, using strategic prompts, ZDDR guides LLM in rephrasing adversarial content, maintaining clarity, structure, and meaning, thereby restoring the sentence from the attack. Benchmarking reveals a 9% improvement in area under receiver operating characteristic curve (AUROC) for adversarial detection over existing techniques. Post-restoration, model classification efficacy surges by 45% compared to the offensive inputs, setting new performance standards against other restoration techniques.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Zero Sample Defender (ZDDR) is introduced for adversarial sample detection and recovery without relying on prior knowledge, and reveals a 9% improvement in area under receiver operating characteristic curve (AUROC) for adversarial detection over existing techniques."
            },
            "score": 5
        },
        {
            "id": "9ed4387cf5f25aa53d13f29b5b5c107f70a881cc",
            "paperId": "9ed4387cf5f25aa53d13f29b5b5c107f70a881cc",
            "title": "Robustness Evaluation of Cloud-Deployed Large Language Models against Chinese Adversarial Text Attacks",
            "abstract": "In the evolving digital realm, Large Language Models (LLMs) like ChatGPT, which recently achieved state-of-the-art results across diverse NLP tasks, are extensively used. Deployed on the cloud, ChatGPT allows interaction via its API, providing rich and high-quality solutions. However, its vulnerability to adversarial attacks, potentially compromising the quality and reliability of cloud services and leading to information leakage, raises security concerns. Investigating the robustness of ChatGPT against adversarial attacks enables a preliminary understanding of its weaknesses and facilitates the subsequent integration of targeted defensive mechanisms into the cloud framework. Most current research on the robustness of LLMs against adversarial attacks focuses on BERT, with few studies on ChatGPT under similar conditions. This paper explores the robustness of ChatGPT against Chinese adversarial text attacks in text classification tasks and proposes a ChatGPT-based adversarial text fluency evaluation method that eliminates the need for human involvement. Experiments conducted on the real-world dataset, THUCNews, examined the robustness of Chinese BERT and ChatGPT against adversarial attacks generated via various Chinese adversarial text generation methods. A multidimensional assessment revealed that both models are susceptible to attacks, leading to decreased text classification accuracy. The attack success rate on ChatGPT reached nearly 45%.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper explores the robustness of ChatGPT against Chinese adversarial text attacks in text classification tasks and proposes a ChatGPT-based adversarial text fluency evaluation method that eliminates the need for human involvement."
            },
            "score": 5
        },
        {
            "id": "23756011261f20d411dc4d9bcd3a56b69884a72a",
            "paperId": "23756011261f20d411dc4d9bcd3a56b69884a72a",
            "title": "Flooding-X: Improving BERT\u2019s Resistance to Adversarial Attacks via Loss-Restricted Fine-Tuning",
            "abstract": "Adversarial robustness has attracted much attention recently, and the mainstream solution is adversarial training. However, the tradition of generating adversarial perturbations for each input embedding (in the settings of NLP) scales up the training computational complexity by the number of gradient steps it takes to obtain the adversarial samples. To address this problem, we leverage Flooding method which primarily aims at better generalization and we find promising in defending adversarial attacks. We further propose an effective criterion to bring hyper-parameter-dependent flooding into effect with a narrowed-down search space by measuring how the gradient steps taken within one epoch affect the loss of each batch. Our approach requires zero adversarial sample for training, and its time consumption is equivalent to fine-tuning, which can be 2-15 times faster than standard adversarial training. We experimentally show that our method improves BERT\u2019s resistance to textual adversarial attacks by a large margin, and achieves state-of-the-art robust accuracy on various text classification and GLUE tasks.",
            "year": 2022,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work leverages Flooding method which primarily aims at better generalization and finds promising in defending adversarial attacks, and proposes an effective criterion to bring hyper-parameter-dependent flooding into effect with a narrowed-down search space by measuring how the gradient steps taken within one epoch affect the loss of each batch."
            },
            "score": 5
        },
        {
            "id": "bf91d2cab320e4c21fa81863705a3ef90278bc42",
            "paperId": "bf91d2cab320e4c21fa81863705a3ef90278bc42",
            "title": "SCAT: Robust Self-supervised Contrastive Learning via Adversarial Training for Text Classification",
            "abstract": "Despite their promising performance across various natural language processing (NLP) tasks, current NLP systems are vulnerable to textual adversarial attacks. To defend against these attacks, most existing methods apply adversarial training by incorporating adversarial examples. However, these methods have to rely on ground-truth labels to generate adversarial examples, rendering it impractical for large-scale model pre-training which is commonly used nowadays for NLP and many other tasks. In this paper, we propose a novel learning framework called SCAT (Self-supervised Contrastive Learning via Adversarial Training), which can learn robust representations without requiring labeled data. Specifically, SCAT modifies random augmentations of the data in a fully labelfree manner to generate adversarial examples. Adversarial training is achieved by minimizing the contrastive loss between the augmentations and their adversarial counterparts. We evaluate SCAT on two text classification datasets using two state-of-the-art attack schemes proposed recently. Our results show that SCAT can not only train robust language models from scratch, but it can also significantly improve the robustness of existing pre-trained language models. Moreover, to demonstrate its flexibility, we show that SCAT can also be combined with supervised adversarial training to further enhance model robustness.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel learning framework called SCAT (Self-supervised Contrastive Learning via Adversarial Training), which can learn robust representations without requiring labeled data and can also significantly improve the robustness of existing pre-trained language models."
            },
            "score": 5
        },
        {
            "id": "efabdd27929796b712cb1b3a3051ea5358dc1200",
            "paperId": "efabdd27929796b712cb1b3a3051ea5358dc1200",
            "title": "A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning",
            "abstract": "Vision-language models can encode societal biases and stereotypes, but there are challenges to measuring and mitigating these multimodal harms due to lacking measurement robustness and feature degradation. To address these challenges, we investigate bias measures and apply ranking metrics for image-text representations. We then investigate debiasing methods and show that prepending learned embeddings to text queries that are jointly trained with adversarial debiasing and a contrastive loss, reduces various bias measures with minimal degradation to the image-text representation.",
            "year": 2022,
            "citationCount": 53,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Debiasing methods are investigated and it is shown that prepending learned embeddings to text queries that are jointly trained with adversarial debiasing and a contrastive loss, reduces various bias measures with minimal degradation to the image-text representation."
            },
            "score": 4
        },
        {
            "id": "29b64e41bbea997019b64696d2374c3c96dbdf5c",
            "paperId": "29b64e41bbea997019b64696d2374c3c96dbdf5c",
            "title": "Delving into the Adversarial Robustness of Federated Learning",
            "abstract": "In Federated Learning (FL), models are as fragile as centrally trained models against adversarial examples. However, the adversarial robustness of federated learning remains largely unexplored. This paper casts light on the challenge of adversarial robustness of federated learning. To facilitate a better understanding of the adversarial vulnerability of the existing FL methods, we conduct comprehensive robustness evaluations on various attacks and adversarial training methods. Moreover, we reveal the negative impacts induced by directly adopting adversarial training in FL, which seriously hurts the test accuracy, especially in non-IID settings. In this work, we propose a novel algorithm called Decision Boundary based Federated Adversarial Training (DBFAT), which consists of two components (local re-weighting and global regularization) to improve both accuracy and robustness of FL systems. Extensive experiments on multiple datasets demonstrate that DBFAT consistently outperforms other baselines under both IID and non-IID settings.",
            "year": 2023,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel algorithm called Decision Boundary based Federated Adversarial Training (DBFAT) is proposed, which consists of two components (local re-weighting and global regularization) to improve both accuracy and robustness of FL systems."
            },
            "score": 4
        },
        {
            "id": "8bcb5534227214b83255f5b9dedbc0d46a44794a",
            "paperId": "8bcb5534227214b83255f5b9dedbc0d46a44794a",
            "title": "Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?",
            "abstract": "While additional training data improves the robustness of deep neural networks against adversarial examples, it presents the challenge of curating a large number of specific real-world samples. We circumvent this challenge by using additional data from proxy distributions learned by advanced generative models. We first seek to formally understand the transfer of robustness from classifiers trained on proxy distributions to the real data distribution. We prove that the difference between the robustness of a classifier on the two distributions is upper bounded by the conditional Wasserstein distance between them. Next we use proxy distributions to significantly improve the performance of adversarial training on five different datasets. For example, we improve robust accuracy by up to 7.5% and 6.7% in $\\ell_{\\infty}$ and $\\ell_2$ threat model over baselines that are not using proxy distributions on the CIFAR-10 dataset. We also improve certified robust accuracy by 7.6% on the CIFAR-10 dataset. We further demonstrate that different generative models bring a disparate improvement in the performance in robust training. We propose a robust discrimination approach to characterize the impact of individual generative models and further provide a deeper understanding of why current state-of-the-art in diffusion-based generative models are a better choice for proxy distribution than generative adversarial networks.",
            "year": 2021,
            "citationCount": 90,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A robust discrimination approach to characterize the impact of individual generative models is proposed and a deeper understanding of why current state-of-the-art in diffusion-based generative model are a better choice for proxy distribution than generative adversarial networks is provided."
            },
            "score": 4
        },
        {
            "id": "63e2740dc581b4186b4e277a9955e8048c414521",
            "paperId": "63e2740dc581b4186b4e277a9955e8048c414521",
            "title": "Large Language Models for Code: Security Hardening and Adversarial Testing",
            "abstract": "Large language models (large LMs) are increasingly trained on massive codebases and used to generate code. However, LMs lack awareness of security and are found to frequently produce unsafe code. This work studies the security of LMs along two important axes: (i) security hardening, which aims to enhance LMs' reliability in generating secure code, and (ii) adversarial testing, which seeks to evaluate LMs' security at an adversarial standpoint. We address both of these by formulating a new security task called controlled code generation. The task is parametric and takes as input a binary property to guide the LM to generate secure or unsafe code, while preserving the LM's capability of generating functionally correct code. We propose a novel learning-based approach called SVEN to solve this task. SVEN leverages property-specific continuous vectors to guide program generation towards the given property, without modifying the LM's weights. Our training procedure optimizes these continuous vectors by enforcing specialized loss terms on different regions of code, using a high-quality dataset carefully curated by us. Our extensive evaluation shows that SVEN is highly effective in achieving strong security control. For instance, a state-of-the-art CodeGen LM with 2.7B parameters generates secure code for 59.1% of the time. When we employ SVEN to perform security hardening (or adversarial testing) on this LM, the ratio is significantly boosted to 92.3% (or degraded to 36.8%). Importantly, SVEN closely matches the original LMs in functional correctness.",
            "year": 2023,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel learning-based approach called SVEN, which leverages property-specific continuous vectors to guide program generation towards the given property, without modifying the LM's weights, and closely matches the original LMs in functional correctness."
            },
            "score": 4
        },
        {
            "id": "b9df0d4631f9fab1432c152765e243ae4cd667f4",
            "paperId": "b9df0d4631f9fab1432c152765e243ae4cd667f4",
            "title": "Effective Prompt Extraction from Language Models",
            "abstract": "The text generated by large language models is commonly controlled by prompting, where a prompt prepended to a user's query guides the model's output. The prompts used by companies to guide their models are often treated as secrets, to be hidden from the user making the query. They have even been treated as commodities to be bought and sold. However, anecdotal reports have shown adversarial users employing prompt extraction attacks to recover these prompts. In this paper, we present a framework for systematically measuring the effectiveness of these attacks. In experiments with 3 different sources of prompts and 11 underlying large language models, we find that simple text-based attacks can in fact reveal prompts with high probability. Our framework determines with high precision whether an extracted prompt is the actual secret prompt, rather than a model hallucination. Prompt extraction experiments on real systems such as Bing Chat and ChatGPT suggest that system prompts can be revealed by an adversary despite existing defenses in place.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents a framework for systematically measuring the effectiveness of prompt extraction attacks and determines with high precision whether an extracted prompt is the actual secret prompt, rather than a model hallucination."
            },
            "score": 4
        },
        {
            "id": "92b9d8b8c81c4c53ea62000c0924500b2dd11bce",
            "paperId": "92b9d8b8c81c4c53ea62000c0924500b2dd11bce",
            "title": "Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models",
            "abstract": "We introduce new jailbreak attacks on vision language models (VLMs), which use aligned LLMs and are resilient to text-only jailbreak attacks. Specifically, we develop cross-modality attacks on alignment where we pair adversarial images going through the vision encoder with textual prompts to break the alignment of the language model. Our attacks employ a novel compositional strategy that combines an image, adversarially targeted towards toxic embeddings, with generic prompts to accomplish the jailbreak. Thus, the LLM draws the context to answer the generic prompt from the adversarial image. The generation of benign-appearing adversarial images leverages a novel embedding-space-based methodology, operating with no access to the LLM model. Instead, the attacks require access only to the vision encoder and utilize one of our four embedding space targeting strategies. By not requiring access to the LLM, the attacks lower the entry barrier for attackers, particularly when vision encoders such as CLIP are embedded in closed-source LLMs. The attacks achieve a high success rate across different VLMs, highlighting the risk of cross-modality alignment vulnerabilities, and the need for new alignment approaches for multi-modal models.",
            "year": 2023,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Cross-modality attacks on alignment where adversarial images going through the vision encoder with textual prompts to break the alignment of the language model are developed."
            },
            "score": 4
        },
        {
            "id": "8951bbc8c1eb3fd43c9e47025268cc79b868f514",
            "paperId": "8951bbc8c1eb3fd43c9e47025268cc79b868f514",
            "title": "Why do universal adversarial attacks work on large language models?: Geometry might be the answer",
            "abstract": "Transformer based large language models with emergent capabilities are becoming increasingly ubiquitous in society. However, the task of understanding and interpreting their internal workings, in the context of adversarial attacks, remains largely unsolved. Gradient-based universal adversarial attacks have been shown to be highly effective on large language models and potentially dangerous due to their input-agnostic nature. This work presents a novel geometric perspective explaining universal adversarial attacks on large language models. By attacking the 117M parameter GPT-2 model, we find evidence indicating that universal adversarial triggers could be embedding vectors which merely approximate the semantic information in their adversarial training region. This hypothesis is supported by white-box model analysis comprising dimensionality reduction and similarity measurement of hidden representations. We believe this new geometric perspective on the underlying mechanism driving universal attacks could help us gain deeper insight into the internal workings and failure modes of LLMs, thus enabling their mitigation.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "By attacking the 117M parameter GPT-2 model, this work finds evidence indicating that universal adversarial triggers could be embedding vectors which merely approximate the semantic information in their adversarial training region."
            },
            "score": 4
        },
        {
            "id": "4d9fc5972ab0f17f3c8aa27b4d9372f029d4dded",
            "paperId": "4d9fc5972ab0f17f3c8aa27b4d9372f029d4dded",
            "title": "Adversarial Attacks on Large Language Model-Based System and Mitigating Strategies: A Case Study on ChatGPT",
            "abstract": "Machine learning algorithms are at the forefront of the development of advanced information systems. The rapid progress in machine learning technology has enabled cutting-edge large language models (LLMs), represented by GPT-3 and ChatGPT, to perform a wide range of NLP tasks with a stunning performance. However, research on adversarial machine learning highlights the need for these intelligent systems to be more robust. Adversarial machine learning aims to evaluate attack and defense mechanisms to prevent the malicious exploitation of these systems. In the case of ChatGPT, adversarial induction prompt can cause the model to generate toxic texts that could pose serious security risks or propagate false information. To address this challenge, we first analyze the effectiveness of inducing attacks on ChatGPT. Then, two effective mitigating mechanisms are proposed. The first is a training-free prefix prompt mechanism to detect and prevent the generation of toxic texts. The second is a RoBERTa-based mechanism that identifies manipulative or misleading input text via external detection models. The availability of this method is demonstrated through experiments.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A training-free prefix prompt mechanism to detect and prevent the generation of toxic texts and a RoBERTa-based mechanism that identifies manipulative or misleading input text via external detection models are proposed."
            },
            "score": 4
        },
        {
            "id": "e6a690d40d1811140ef12b4977e43e9fd2908e18",
            "paperId": "e6a690d40d1811140ef12b4977e43e9fd2908e18",
            "title": "Improving Adversarial Robustness of Masked Autoencoders via Test-time Frequency-domain Prompting",
            "abstract": "In this paper, we investigate the adversarial robustness of vision transformers that are equipped with BERT pretraining (e.g., BEiT, MAE). A surprising observation is that MAE has significantly worse adversarial robustness than other BERT pretraining methods. This observation drives us to rethink the basic differences between these BERT pretraining methods and how these differences affect the robustness against adversarial perturbations. Our empirical analysis reveals that the adversarial robustness of BERT pretraining is highly related to the reconstruction target, i.e., predicting the raw pixels of masked image patches will degrade more adversarial robustness of the model than predicting the semantic context, since it guides the model to concentrate more on medium-/high-frequency components of images. Based on our analysis, we provide a simple yet effective way to boost the adversarial robustness of MAE. The basic idea is using the dataset-extracted domain knowledge to occupy the medium-/high-frequency of images, thus narrowing the optimization space of adversarial perturbations. Specifically, we group the distribution of pretraining data and optimize a set of cluster-specific visual prompts on frequency domain. These prompts are incorporated with input images through prototype-based prompt selection during test period. Extensive evaluation shows that our method clearly boost MAE\u2019s adversarial robustness while maintaining its clean performance on ImageNet-1k classification. Our code is available at: https://github.com/shikiw/RobustMAE.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper provides a simple yet effective way to boost MAE\u2019s adversarial robustness by using the dataset-extracted domain knowledge to occupy the medium-/high-frequency of images, thus narrowing the optimization space of adversarial perturbations."
            },
            "score": 4
        },
        {
            "id": "99ea0f4ebdb405fbbddc48ad788382209d609321",
            "paperId": "99ea0f4ebdb405fbbddc48ad788382209d609321",
            "title": "KTGAT: Improving the Robustness of Knowledge-enhanced Text Generation via Adversarial Training",
            "abstract": "The shortage of information in text generation has been a prominent area of research in Natural Language Processing (NLP). Current research endeavors aim to combine pre-trained models with rich open-world knowledge from external sources to increase the priori information and thereby enhance the informativeness of text generation. While recent studies suggest that integrating open-world and task-specific knowledge can improve text generation by addressing specific knowledge gaps in downstream tasks, the inherent semantic ambiguity in natural language remains a significant challenge that may impede knowledge acquisition and text generation. To overcome this challenge and improve the model's semantic comprehension and overall robustness, we propose a novel framework, the Knowledge Augmentation Text Generation model via Adversarial Training (KTGAT). Our method adds perturbations to the embedding layer, which are equivalent to constructing unstable samples. This approach improves the model's robustness to adversarial samples and the generalization performance of the original samples. Our experiments demonstrate that the proposed KTGAT framework outperforms the baseline model, thus proving its effectiveness in improving text generation. The generated text cases illustrate that our method enhances the model's semantic comprehension and enables it to search for knowledge items more effectively and accurately.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel framework, the Knowledge Augmentation Text Generation model via Adversarial Training (KTGAT), is proposed that improves the model's robustness to adversarial samples and the generalization performance of the original samples, and outperforms the baseline model, thus proving its effectiveness in improving text generation."
            },
            "score": 4
        },
        {
            "id": "064e5385de8fc15339025449c4e9a0f02dd6b42c",
            "paperId": "064e5385de8fc15339025449c4e9a0f02dd6b42c",
            "title": "Detection of Word Adversarial Examples in NLP: Benchmark and Baseline via Robust Density Estimation",
            "abstract": "Word-level adversarial attacks have shown suc-001 cess in NLP models, drastically decreasing the 002 performance of transformer-based models in 003 recent years. As a countermeasure, adversarial 004 defense has been explored, but relatively few 005 efforts have been made to detect adversarial ex-006 amples. However, detecting adversarial exam-007 ples in NLP may be crucial for automated task 008 (e",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Word-level adversarial attacks have shown suc-001 cess in NLP models, drastically decreasing the performance of transformer-based models in recent years, and detecting adversarial exam-007 ples in NLP may be crucial for automated task 008."
            },
            "score": 4
        },
        {
            "id": "ce86274c3661341fc3200533ccab20fccdb32a6a",
            "paperId": "ce86274c3661341fc3200533ccab20fccdb32a6a",
            "title": "Exploring the Vulnerability of Natural Language Processing Models via Universal Adversarial Texts",
            "abstract": "Universal adversarial texts (UATs) refer to short pieces of text units that can largely affect the predictions of NLP models. Recent studies on universal adversarial attacks assume the accessibility of datasets for the task, which is not realistic. We propose two types of Data-Free Adjusted Gradient (DFAG) attacks to show that it is possible to generate effective UATs with only one arbitrary example which could be manually crafted. Based on the proposed DFAG attacks, this paper explores the vulnerability of commonly used NLP models in terms of two factors: network architectures and pre-trained embeddings. Our empirical studies on three text classification datasets reveal that: 1) CNN based models are more extremely vulnerable to UATs while self-attention models show the most robustness, 2) the vulnerability of CNN and LSTM models and robustness of self-attention models could be attributed to whether they rely on training data artifacts for their predictions, and 3) the pre-trained embeddings could expose vulnerability to both universal adversarial attack and the UAT transfer attack.",
            "year": 2021,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes two types of Data-Free Adjusted Gradient attacks to show that it is possible to generate effective UATs with only one arbitrary example which could be manually crafted and explores the vulnerability of commonly used NLP models in terms of two factors: network architectures and pre-trained embeddings."
            },
            "score": 4
        },
        {
            "id": "f4a425499128d53d97a86f2cd625990653101356",
            "paperId": "f4a425499128d53d97a86f2cd625990653101356",
            "title": "Improved Generalization of Arabic Text Classifiers",
            "abstract": "While transfer learning for text has been very active in the English language, progress in Arabic has been slow, including the use of Domain Adaptation (DA). Domain Adaptation is used to generalize the performance of any classifier by trying to balance the classifier\u2019s accuracy for a particular task among different text domains. In this paper, we propose and evaluate two variants of a domain adaptation technique: the first is a base model called Domain Adversarial Neural Network (DANN), while the second is a variation that incorporates representational learning. Similar to previous approaches, we propose the use of proxy A-distance as a metric to assess the success of generalization. We make use of ArSentDLEV, a multi-topic dataset collected from the Levantine countries, to test the performance of the models. We show the superiority of the proposed method in accuracy and robustness when dealing with the Arabic language.",
            "year": 2019,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes and evaluates two variants of a domain adaptation technique and proposes the use of proxy A-distance as a metric to assess the success of generalization, and shows the superiority of the proposed method in accuracy and robustness when dealing with the Arabic language."
            },
            "score": 3
        },
        {
            "id": "3fe1be9e5b8203d4f2500e565b7bdd228f65d3c7",
            "paperId": "3fe1be9e5b8203d4f2500e565b7bdd228f65d3c7",
            "title": "Push-Pull: Characterizing the Adversarial Robustness for Audio-Visual Active Speaker Detection",
            "abstract": "Audio-visual active speaker detection (AVASD) is well-developed, and now is an indispensable front-end for several multi-modal applications. However, to the best of our knowledge, the adversarial robustness of AVASD models hasn't been investigated, not to mention the effective defense against such attacks. In this paper, we are the first to reveal the vulnerability of AVASD models under audio-only, visual-only, and audio-visual adversarial attacks through extensive experiments. What's more, we also propose a novel audio-visual interaction loss (AVIL) for making attackers difficult to find feasible adversarial examples under an allocated attack budget. The loss aims at pushing the inter-class embeddings to be dispersed, namely non-speech and speech clusters, sufficiently disentangled, and pulling the intra-class embeddings as close as possible to keep them compact. Experimental results show the AVIL outperforms the adversarial training by 33.14 mAP (%) under multi-modal attacks.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper is the first to reveal the vulnerability of AVASD models under audio- only, visual-only, and audio-visual adversarial attacks through extensive experiments, and proposes a novel audio- visual interaction loss (AVIL) for making attackers difficult to find feasible adversarial examples under an allocated attack budget."
            },
            "score": 3
        },
        {
            "id": "8f57052eeeb3cc04159655583cb9ed2396ade233",
            "paperId": "8f57052eeeb3cc04159655583cb9ed2396ade233",
            "title": "Competence-Based Analysis of Language Models",
            "abstract": "Despite the recent success of large pretrained language models (LMs) on a variety of prompting tasks, these models can be alarmingly brittle to small changes in inputs or application contexts. To better understand such behavior and motivate the design of more robust LMs, we propose a general experimental framework, CALM (Competence-based Analysis of Language Models), where targeted causal interventions are utilized to damage an LM's internal representation of various linguistic properties in order to evaluate its use of each representation in performing a given task. We implement these interventions as gradient-based adversarial attacks, which (in contrast to prior causal probing methodologies) are able to target arbitrarily-encoded representations of relational properties, and carry out a case study of this approach to analyze how BERT-like LMs use representations of several relational properties in performing associated relation prompting tasks. We find that, while the representations LMs leverage in performing each task are highly entangled, they may be meaningfully interpreted in terms of the tasks where they are most utilized; and more broadly, that CALM enables an expanded scope of inquiry in LM analysis that may be useful in predicting and explaining weaknesses of existing LMs.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that, while the representations LMs leverage in performing each task are highly entangled, they may be meaningfully interpreted in terms of the tasks where they are most utilized; and more broadly, that CALM enables an expanded scope of inquiry in LM analysis that may be useful in predicting and explaining weaknesses of existing LMs."
            },
            "score": 3
        },
        {
            "id": "d18287d5ef8653aa1276a11957f2b3934c7c93e1",
            "paperId": "d18287d5ef8653aa1276a11957f2b3934c7c93e1",
            "title": "CodeAttack: Code-based Adversarial Attacks for Pre-Trained Programming Language Models",
            "abstract": "Pre-trained programming language (PL) models (such as CodeT5, CodeBERT, GraphCodeBERT, etc.,) have the potential to automate software engineering tasks involving code understanding and code generation. However, these models operate in the natural channel of code, i.e., primarily concerned with the human understanding of code. They are not robust to changes in the input and thus, are potentially susceptible to adversarial attacks in the natural channel. We propose, Code Attack, a simple yet effective black-box attack model that uses code structure to generate effective, efficient, and imperceptible adversarial code samples and demonstrates the vulnerabilities of the state-of-the-art PL models to code-specific adversarial attacks. We evaluate the transferability of CodeAttack on several code-code (translation and repair) and code-NL (summarization) tasks across different programming languages. Code Attack outperforms state-of-the-art adversarial NLP attack models to achieve the best overall drop in performance while being more efficient, imperceptible, consistent, and fluent. The code can be found at https://github.com/reddy-lab-code-research/CodeAttack.",
            "year": 2022,
            "citationCount": 26,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes, Code Attack, a simple yet effective black-box attack model that uses code structure to generate effective, efficient, and imperceptible adversarial code samples and demonstrates the vulnerabilities of the state-of-the-art PL models to code-specific adversarial attacks."
            },
            "score": 3
        },
        {
            "id": "9b54941de1e21826ecc28b32730ac3f69991ede4",
            "paperId": "9b54941de1e21826ecc28b32730ac3f69991ede4",
            "title": "Robustness Gym: Unifying the NLP Evaluation Landscape",
            "abstract": "Despite impressive performance on standard benchmarks, natural language processing (NLP) models are often brittle when deployed in real-world systems. In this work, we identify challenges with evaluating NLP systems and propose a solution in the form of Robustness Gym (RG), a simple and extensible evaluation toolkit that unifies 4 standard evaluation paradigms: subpopulations, transformations, evaluation sets, and adversarial attacks. By providing a common platform for evaluation, RG enables practitioners to compare results from disparate evaluation paradigms with a single click, and to easily develop and share novel evaluation methods using a built-in set of abstractions. RG is under active development and we welcome feedback & contributions from the community.",
            "year": 2021,
            "citationCount": 119,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Robustness Gym (RG), a simple and extensible evaluation toolkit that unifies 4 standard evaluation paradigms: subpopulations, transformations, evaluation sets, and adversarial attacks, is proposed."
            },
            "score": 3
        },
        {
            "id": "6d68b5c1eaf03aba857476a9825acf3e48edd840",
            "paperId": "6d68b5c1eaf03aba857476a9825acf3e48edd840",
            "title": "Hijacking Large Language Models via Adversarial In-Context Learning",
            "abstract": "In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs for specific tasks by utilizing labeled examples as demonstrations in the precondition prompts. Despite its promising performance, ICL suffers from instability with the choice and arrangement of examples. Additionally, crafted adversarial attacks pose a notable threat to the robustness of ICL. However, existing attacks are either easy to detect, rely on external models, or lack specificity towards ICL. To address these issues, this work introduces a novel transferable attack for ICL, aiming to hijack LLMs to generate the targeted response. The proposed LLM hijacking attack leverages a gradient-based prompt search method to learn and append imperceptible adversarial suffixes to the in-context demonstrations. Extensive experimental results on various tasks and datasets demonstrate the effectiveness of our LLM hijacking attack, resulting in a distracted attention towards adversarial tokens, consequently leading to the targeted unwanted outputs.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a novel transferable attack for ICL, aiming to hijack LLMs to generate the targeted response, and leverages a gradient-based prompt search method to learn and append imperceptible adversarial suffixes to the in-context demonstrations."
            },
            "score": 3
        },
        {
            "id": "91159f6d3d52e6cfed1e4d1c6e50d1b17086a910",
            "paperId": "91159f6d3d52e6cfed1e4d1c6e50d1b17086a910",
            "title": "On the Robustness of Large Multimodal Models Against Image Adversarial Attacks",
            "abstract": "Recent advances in instruction tuning have led to the development of State-of-the-Art Large Multimodal Models (LMMs). Given the novelty of these models, the impact of visual adversarial attacks on LMMs has not been thoroughly examined. We conduct a comprehensive study of the robustness of various LMMs against different adversarial attacks, evaluated across tasks including image classification, image captioning, and Visual Question Answer (VQA). We find that in general LMMs are not robust to visual adversarial inputs. However, our findings suggest that context provided to the model via prompts, such as questions in a QA pair helps to mitigate the effects of visual adversarial inputs. Notably, the LMMs evaluated demonstrated remarkable resilience to such attacks on the ScienceQA task with only an 8.10% drop in performance compared to their visual counterparts which dropped 99.73%. We also propose a new approach to real-world image classification which we term query decomposition. By incorporating existence queries into our input prompt we observe diminished attack effectiveness and improvements in image classification accuracy. This research highlights a previously under-explored facet of LMM robustness and sets the stage for future work aimed at strengthening the resilience of multimodal systems in adversarial environments.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A comprehensive study of the robustness of various LMMs against different adversarial attacks, evaluated across tasks including image classification, image captioning, and Visual Question Answer, finds that in general LMMs are not robust to visual adversarial inputs and proposes a new approach to real-world image classification which is term query decomposition."
            },
            "score": 3
        },
        {
            "id": "d4788b3996a5ec681ec72373111035f6d84da4f6",
            "paperId": "d4788b3996a5ec681ec72373111035f6d84da4f6",
            "title": "Improving White-box Robustness of Pre-processing Defenses via Joint Adversarial Training",
            "abstract": "Deep neural networks (DNNs) are vulnerable to adversarial noise. A range of adversarial defense techniques have been proposed to mitigate the interference of adversarial noise, among which the input pre-processing methods are scalable and show great potential to safeguard DNNs. However, pre-processing methods may suffer from the robustness degradation effect, in which the defense reduces rather than improving the adversarial robustness of a target model in a white-box setting. A potential cause of this negative effect is that adversarial training examples are static and independent to the pre-processing model. To solve this problem, we investigate the influence of full adversarial examples which are crafted against the full model, and find they indeed have a positive impact on the robustness of defenses. Furthermore, we find that simply changing the adversarial training examples in pre-processing methods does not completely alleviate the robustness degradation effect. This is due to the adversarial risk of the pre-processed model being neglected, which is another cause of the robustness degradation effect. Motivated by above analyses, we propose a method called Joint Adversarial Training based Pre-processing (JATP) defense. Specifically, we formulate a feature similarity based adversarial risk for the pre-processing model by using full adversarial examples found in a feature space. Unlike standard adversarial training, we only update the pre-processing model, which prompts us to introduce a pixel-wise loss to improve its cross-model transferability. We then conduct a joint adversarial training on the pre-processing model to minimize this overall risk. Empirical results show that our method could effectively mitigate the robustness degradation effect across different target models in comparison to previous state-of-the-art approaches.",
            "year": 2021,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A method called Joint Adversarial Training based Pre-processing (JATP) defense is proposed that could effectively mitigate the robustness degradation effect across different target models in comparison to previous state-of-the-art approaches."
            },
            "score": 3
        },
        {
            "id": "1cd8373490efc2d74c2796f4b2aa27c7d4415ec9",
            "paperId": "1cd8373490efc2d74c2796f4b2aa27c7d4415ec9",
            "title": "VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models",
            "abstract": "Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a vision-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop robot trajectories with robustness to dynamic perturbations. We further demonstrate how the proposed framework can benefit from online experiences by efficiently learning a dynamics model for scenes that involve contact-rich interactions. We present a large-scale study of the proposed method in both simulated and real-robot environments, showcasing the ability to perform a large variety of everyday manipulation tasks specified in free-form natural language. Videos and code at https://voxposer.github.io",
            "year": 2023,
            "citationCount": 156,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A large-scale study of the proposed framework to synthesize closed-loop robot trajectories with robustness to dynamic perturbations is presented, showcasing the ability to perform a large variety of everyday manipulation tasks specified in free-form natural language."
            },
            "score": 2
        },
        {
            "id": "91099bbb96133c70db091041900ecff502a5e3a8",
            "paperId": "91099bbb96133c70db091041900ecff502a5e3a8",
            "title": "Harnessing the Power of Adversarial Prompting and Large Language Models for Robust Hypothesis Generation in Astronomy",
            "abstract": "This study investigates the application of Large Language Models (LLMs), specifically GPT-4, within Astronomy. We employ in-context prompting, supplying the model with up to 1000 papers from the NASA Astrophysics Data System, to explore the extent to which performance can be improved by immersing the model in domain-specific literature. Our findings point towards a substantial boost in hypothesis generation when using in-context prompting, a benefit that is further accentuated by adversarial prompting. We illustrate how adversarial prompting empowers GPT-4 to extract essential details from a vast knowledge base to produce meaningful hypotheses, signaling an innovative step towards employing LLMs for scientific research in Astronomy.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is illustrated how adversarial prompting empowers GPT-4 to extract essential details from a vast knowledge base to produce meaningful hypotheses, signaling an innovative step towards employing LLMs for scientific research in Astronomy."
            },
            "score": 2
        },
        {
            "id": "5bed42ded64cd7d285088385b687479e9344e862",
            "paperId": "5bed42ded64cd7d285088385b687479e9344e862",
            "title": "Wei Yang's Research Statement",
            "abstract": "I enjoy doing research in Computer Security and Software Engineering and specifically in mobile security and adversarial machine learning. A primary goal of my research is to build adversarial-resilient intelligent security systems. I have been developing such security systems for the mobile device ecosystem that serves billions of users, millions of apps, and hundreds of thousands of app developers. For an ecosystem of this magnitude, manual inspection or rule-based security systems are costly and error-prone. There is a strong need for intelligent security systems that can learn from experiences, solve problems, and use knowledge to adapt to new situations. However, achieving intelligence in security systems is challenging. In the cat-and-mouse game between security analysts and adversaries, the intelligence of adversaries also increases. In this never-ending game, the adversaries continuously evolve their attacks to be specifically adversarial to newly proposed intelligent security techniques. To address this challenge, I have been pursuing two lines of research: (1) enhancing intelligence of existing security systems to automate the security-decision making by techniques such as program analysis [11, 8, 10, 6, U6] , natural language processing (NLP) [9, 7, U7, 1] , and machine learning [8, 4, 3, 2] ; (2) guarding against emerging attacks specifically adversarial to these newly-proposed intelligent security techniques by developing corresponding defenses [13, U1, U2] and testing methodologies [12, 5] . Throughout these research efforts, my general research methodology is to extract insightful data for security systems (through program analysis and NLP techniques), to enable intelligent decision making in security systems (through machine learning techniques to learn from the extracted data), and to strengthen robustness of the security systems by generating adversarial-testing inputs to check these intelligent security techniques and building defense to prevent the adversarial attacks. With this methodology, my research has derived solutions that have high impact on real-world systems. For instance, my work on analysis and testing of mobile applications (apps) [11, 10] in collaboration with Tencent Ltd. has been deployed and adopted in daily testing of a mobile app named WeChat, a popular messenger app with over 900 million monthly active users. A number of tools grown out of my research have been adopted by companies such as Fujitsu [P1, P2, 13, 6] , Samsung [12, 5] , and IBM.",
            "year": 2018,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work aims to build adversarial-resilient intelligent security systems for the mobile device ecosystem that serves billions of users, millions of apps, and hundreds of thousands of app developers by enhancing intelligence of existing security systems and guarding against emerging attacks specifically adversarial to newly proposed intelligent security techniques."
            },
            "score": 2
        }
    ],
    "novelty": "yes"
}