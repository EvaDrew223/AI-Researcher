{
    "topic_description": "novel prompting methods to improve large language models' robustness against adversarial attacks and improve their security and privacy",
    "idea_name": "Parallel Prompting",
    "raw_idea": {
        "Problem": "Adversaries can craft prompts that are specially engineered to induce inconsistent or contradictory behaviors from a language model. For example, an adversary might discover a set of semantically equivalent prompts that lead to very different outputs. These inconsistencies can be exploited to degrade the model's performance or elicit harmful behaviors.",
        "Existing Methods": "Existing consistency defense methods for language models often focus on the training process, e.g., using consistency regularization or contrastive learning. However, these methods can be computationally expensive and may not fully protect against adversaries who craft inconsistencies at the prompting stage.",
        "Motivation": "We propose a novel prompting strategy called Parallel Prompting that can defend against inconsistency attacks at inference time. The key idea is to prompt the model with multiple paraphrased versions of the same input, and then cross-check the outputs for consistency. Any inconsistencies found can then be resolved through a consensus mechanism.",
        "Proposed Method": "Our Parallel Prompting system works as follows: 1) Prompt Paraphrasing: Given an input prompt, we generate a set of k paraphrased versions. These paraphrases should capture the same semantic meaning as the original, but with variations in wording and syntax. This can be done using another language model trained for paraphrasing. 2) Parallel Generation: We feed each of the k paraphrased prompts independently to the language model, generating k parallel outputs. 3) Consistency Checking: We use a consistency scoring function to measure the semantic similarity of the k outputs. If the consistency score falls below a certain threshold, we flag a potential inconsistency attack. 4) Consensus Resolution: In case of detected inconsistencies, we can either alert the user, or try to resolve a consensus output, e.g., by taking a majority vote or averaging the embeddings of the outputs.",
        "Experiment Plan": "We can evaluate Parallel Prompting on adversarial datasets that are designed to induce inconsistencies, like the ANLI dataset. Metrics include the system's ability to detect and resolve inconsistencies, as well as the quality of the final consensus outputs."
    },
    "full_experiment_plan": {
        "Title": "Parallel Prompting: Defending Against Inconsistency Attacks in Large Language Models",
        "Problem Statement": "Adversaries can craft prompts that are specially engineered to induce inconsistent or contradictory behaviors from a language model. For example, an adversary might discover a set of semantically equivalent prompts that lead to very different outputs. These inconsistencies can be exploited to degrade the model's performance or elicit harmful behaviors.",
        "Motivation": "Existing consistency defense methods for language models often focus on the training process, e.g., using consistency regularization or contrastive learning. However, these methods can be computationally expensive and may not fully protect against adversaries who craft inconsistencies at the prompting stage. Inspired by ensemble methods in machine learning that can improve robustness and consistency, we propose a novel prompting strategy called Parallel Prompting that can defend against inconsistency attacks at inference time. The key idea is to prompt the model with multiple paraphrased versions of the same input, and then cross-check the outputs for consistency. Any inconsistencies found can then be resolved through a consensus mechanism.",
        "Proposed Method": "Our Parallel Prompting system works as follows:\n1. Prompt Paraphrasing: Given an input prompt, we generate a set of k paraphrased versions. These paraphrases should capture the same semantic meaning as the original, but with variations in wording and syntax. This can be done using another language model trained for paraphrasing.\n2. Parallel Generation: We feed each of the k paraphrased prompts independently to the language model, generating k parallel outputs.\n3. Consistency Checking: We use a consistency scoring function to measure the semantic similarity of the k outputs. If the consistency score falls below a certain threshold, we flag a potential inconsistency attack.\n4. Consensus Resolution: In case of detected inconsistencies, we can either alert the user, or try to resolve a consensus output, e.g., by taking a majority vote or averaging the embeddings of the outputs.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We can evaluate Parallel Prompting on adversarial datasets that are designed to induce inconsistencies, like the Adversarial NLI (ANLI) dataset. We can also test on regular NLP benchmarks like GLUE and SuperGLUE to ensure the method does not degrade performance on clean data. Metrics include the system's ability to detect and resolve inconsistencies (e.g., precision, recall, F1 score), as well as the quality of the final consensus outputs (e.g., accuracy, BLEU, ROUGE).",
            "Step 2: Implement Prompt Paraphraser": "We need a module to generate semantically equivalent paraphrases of the input prompt. We can use a pre-trained paraphrasing model like Pegasus or T5. Fine-tune it on paraphrase datasets like Quora Question Pairs or PAWS. Experiment with different values of k (number of paraphrases).",
            "Step 3: Implement Parallel Generator": "For each of the k paraphrased prompts, query the target language model to generate k outputs. We can use APIs like OpenAI or Anthropic to access the model. Log the generated outputs for consistency checking.",
            "Step 4: Implement Consistency Checker": "We need a module to score the semantic similarity of the k generated outputs. Options include using an NLI model to check if the outputs entail each other, or using a semantic textual similarity model like SentenceBERT to embed the outputs and calculate cosine similarities. Experiment with different consistency thresholds.",
            "Step 5: Implement Consensus Resolver": "If an inconsistency is detected, we need to resolve it. Implement several strategies: (a) Majority vote: take the majority output as the consensus; (b) Embedding averaging: average the embeddings of the outputs to get a centroid and then decode it; (c) Prompt the model to resolve the inconsistency by showing it the conflicting outputs and asking it to judge.",
            "Step 6: Evaluate on Adversarial Datasets": "Run the full Parallel Prompting pipeline on adversarial test sets like ANLI. Report the system's precision and recall in detecting inconsistencies. For detected inconsistencies, report the quality of the resolved consensus output.",
            "Step 7: Evaluate on Regular Datasets": "Run the full pipeline on standard NLP benchmarks to measure any drop in performance. The consensus output should be the final output. Compare the scores with and without Parallel Prompting."
        },
        "Test Case Examples": {
            "Test Case 1": {
                "Input": "Premise: I just bought a new car. Hypothesis: I don't have a car.",
                "Baseline Output": "Entailment",
                "Paraphrased Prompts": [
                    "Premise: I recently purchased a new vehicle. Hypothesis: I don't own an automobile.",
                    "Premise: I have just acquired a new motor vehicle. Hypothesis: I am without a car."
                ],
                "Parallel Outputs": [
                    "Contradiction",
                    "Contradiction"
                ],
                "Consistency Score": "0.9",
                "Consensus Output": "Contradiction",
                "Explanation": "The baseline NLI model was fooled by the lexical variation between 'bought' and 'have'. Parallel Prompting generated consistent outputs of 'Contradiction' for the paraphrased prompts, allowing it to resolve the inconsistency."
            },
            "Test Case 2": {
                "Input": "Translate the following sentence to French: I love eating apples.",
                "Baseline Output": "J'aime manger des pommes.",
                "Paraphrased Prompts": [
                    "Translate this sentence into French: I enjoy consuming apples.",
                    "Please translate to French: I am fond of eating apples."
                ],
                "Parallel Outputs": [
                    "J'aime manger des pommes.",
                    "J'adore manger des pommes."
                ],
                "Consistency Score": "0.8",
                "Consensus Output": "J'aime manger des pommes.",
                "Explanation": "The baseline and Parallel Prompting outputs are consistent and correct. Parallel Prompting does not degrade performance on benign inputs."
            }
        },
        "Fallback Plan": "If the proposed Parallel Prompting method does not effectively defend against inconsistency attacks, we can conduct additional analyses to understand why:\n1. Check the quality of the generated paraphrases. Are they semantically equivalent to the original prompt? Experiment with different paraphrasing models and datasets.\n2. Analyze the consistency checker. Is it able to reliably detect semantic inconsistencies? Try different consistency scoring functions and thresholds.\n3. Examine the consensus resolution strategies. Do majority voting or embedding averaging produce good consensus outputs? Experiment with prompting the model to directly resolve inconsistencies.\n4. Investigate the types of inconsistency attacks that Parallel Prompting fails on. Are there certain linguistic phenomena or adversarial patterns that are particularly challenging? This can inform the design of new defense methods.\nIf Parallel Prompting still underperforms after these analyses, we can pivot the project to be an empirical study on the challenges of defending against inconsistency attacks in language models. The analyses above can yield insights into the strengths and limitations of prompting-based defense methods, and suggest directions for future work, such as combining prompting with other defense paradigms like adversarial training."
    },
    "novelty_queries": [
        "KeywordQuery(\"language models inconsistency attacks\")",
        "KeywordQuery(\"language models prompting robustness\")",
        "KeywordQuery(\"language models paraphrasing consensus\")",
        "KeywordQuery(\"language models adversarial defense\")",
        "KeywordQuery(\"Parallel Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "47030369e97cc44d4b2e3cf1be85da0fd134904a",
            "paperId": "47030369e97cc44d4b2e3cf1be85da0fd134904a",
            "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
            "abstract": "Because\"out-of-the-box\"large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called\"jailbreaks\"against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods. Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.",
            "year": 2023,
            "citationCount": 385,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to defend against inconsistency attacks in large language models using a novel prompting strategy called Parallel Prompting, which generates paraphrased versions of the input prompt and checks the consistency of the outputs.\n\nThe paper proposes a method to automatically generate adversarial prompts that cause aligned language models to produce objectionable content, and shows that these prompts are transferable across different models.\n\nThe project focuses on defending against inconsistency attacks, while the paper focuses on generating adversarial prompts to induce objectionable content. The approaches and goals are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "8cf9b49698fdb1b754df2556576412a7b44929f6",
            "paperId": "8cf9b49698fdb1b754df2556576412a7b44929f6",
            "title": "SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks",
            "abstract": "Despite efforts to align large language models (LLMs) with human values, widely-used LLMs such as GPT, Llama, Claude, and PaLM are susceptible to jailbreaking attacks, wherein an adversary fools a targeted LLM into generating objectionable content. To address this vulnerability, we propose SmoothLLM, the first algorithm designed to mitigate jailbreaking attacks on LLMs. Based on our finding that adversarially-generated prompts are brittle to character-level changes, our defense first randomly perturbs multiple copies of a given input prompt, and then aggregates the corresponding predictions to detect adversarial inputs. SmoothLLM reduces the attack success rate on numerous popular LLMs to below one percentage point, avoids unnecessary conservatism, and admits provable guarantees on attack mitigation. Moreover, our defense uses exponentially fewer queries than existing attacks and is compatible with any LLM. Our code is publicly available at the following link: https://github.com/arobey1/smooth-llm.",
            "year": 2023,
            "citationCount": 59,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes SmoothLLM, the first algorithm designed to mitigate jailbreaking attacks on LLMs, which first randomly perturbs multiple copies of a given input prompt, and then aggregates the corresponding predictions to detect adversarial inputs."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is defending against inconsistency attacks in large language models, and the proposed approach is parallel prompting, which generates paraphrased prompts and checks the consistency of the outputs. The research problem in the paper is defending against jailbreaking attacks in large language models, and the proposed approach is SmoothLLM, which perturbs the input prompts and aggregates the predictions to detect adversarial inputs.\n\nWhile both works aim to defend large language models against adversarial attacks, the specific types of attacks (inconsistency vs. jailbreaking) and the defense methods (parallel prompting vs. input perturbation) are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "e92e8ff1becb9a9e4a7dd09878eaacb2a62ffb6b",
            "paperId": "e92e8ff1becb9a9e4a7dd09878eaacb2a62ffb6b",
            "title": "Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization",
            "abstract": "Large Language Models (LLMs) continue to advance in their capabilities, yet this progress is accompanied by a growing array of safety risks. While significant attention has been dedicated to exploiting weaknesses in LLMs through jailbreaking attacks, there remains a paucity of exploration into defending against these attacks. We point out a pivotal factor contributing to the success of jailbreaks: the inherent conflict between the goals of being helpful and ensuring safety. To counter jailbreaking attacks, we propose to integrate goal prioritization at both training and inference stages. Implementing goal prioritization during inference substantially diminishes the Attack Success Rate (ASR) of jailbreaking attacks, reducing it from 66.4% to 2.0% for ChatGPT and from 68.2% to 19.4% for Vicuna-33B, without compromising general performance. Furthermore, integrating the concept of goal prioritization into the training phase reduces the ASR from 71.0% to 6.6% for LLama2-13B. Remarkably, even in scenarios where no jailbreaking samples are included during training, our approach slashes the ASR by half, decreasing it from 71.0% to 34.0%. Additionally, our findings reveal that while stronger LLMs face greater safety risks, they also possess a greater capacity to be steered towards defending against such attacks. We hope our work could contribute to the comprehension of jailbreaking attacks and defenses, and shed light on the relationship between LLMs' capability and safety. Our code will be available at \\url{https://github.com/thu-coai/JailbreakDefense_GoalPriority}.",
            "year": 2023,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes to integrate goal prioritization at both training and inference stages to counter jailbreaking attacks, and reveals that while stronger LLMs face greater safety risks, they also possess a greater capacity to be steered towards defending against such attacks."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is defending against inconsistency attacks in large language models, and the proposed approach is parallel prompting, which generates multiple paraphrased prompts and checks the consistency of the outputs. The research problem in the paper is defending against jailbreaking attacks in large language models, and the proposed approach is goal prioritization during training and inference.\n\nThe proposal focuses on inconsistency attacks that try to induce contradictory behaviors, while the paper focuses on jailbreaking attacks that try to bypass safety constraints. The proposal uses prompting techniques to detect and resolve inconsistencies, while the paper uses goal prioritization to steer the model towards safe behaviors.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "e8b3b37c0d301ea41c75765f6ceb7fcbb2e088a4",
            "paperId": "e8b3b37c0d301ea41c75765f6ceb7fcbb2e088a4",
            "title": "AutoDAN: Automatic and Interpretable Adversarial Attacks on Large Language Models",
            "abstract": "Safety alignment of Large Language Models (LLMs) can be compromised with manual jailbreak attacks and (automatic) adversarial attacks. Recent work suggests that patching LLMs against these attacks is possible: manual jailbreak attacks are human-readable but often limited and public, making them easy to block; adversarial attacks generate gibberish prompts that can be detected using perplexity-based filters. In this paper, we show that these solutions may be too optimistic. We propose an interpretable adversarial attack, AutoDAN , that combines the strengths of both types of attacks. It automatically generates attack prompts that bypass perplexity-based filters while maintaining a high attack success rate like manual jailbreak attacks. These prompts are interpretable and diverse, exhibiting strategies commonly used in manual jailbreak attacks, and transfer better than their non-readable counterparts when using limited training data or a single proxy model. We also customize AutoDAN \u2019s objective to leak system prompts, another jailbreak application not addressed in the adversarial attack literature. Our work provides a new way to red-team LLMs and to understand the mechanism of jailbreak attacks.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An interpretable adversarial attack, AutoDAN, is proposed, that combines the strengths of both types of attacks and provides a new way to red-team LLMs and to understand the mechanism of jailbreak attacks."
            },
            "score": 6,
            "novelty_score": "The project proposal is about defending against inconsistency attacks in large language models using parallel prompting, while the paper is about automatically generating interpretable adversarial prompts that can bypass perplexity-based filters.\n\nThe project focuses on improving the robustness and consistency of language models, whereas the paper aims to expose vulnerabilities and understand the mechanism of jailbreak attacks.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "ccaff61e0c1e629d91d78f82a64b3cbc8f3f7023",
            "paperId": "ccaff61e0c1e629d91d78f82a64b3cbc8f3f7023",
            "title": "Robust Distortion-free Watermarks for Language Models",
            "abstract": "We propose a methodology for planting watermarks in text from an autoregressive language model that are robust to perturbations without changing the distribution over text up to a certain maximum generation budget. We generate watermarked text by mapping a sequence of random numbers -- which we compute using a randomized watermark key -- to a sample from the language model. To detect watermarked text, any party who knows the key can align the text to the random number sequence. We instantiate our watermark methodology with two sampling schemes: inverse transform sampling and exponential minimum sampling. We apply these watermarks to three language models -- OPT-1.3B, LLaMA-7B and Alpaca-7B -- to experimentally validate their statistical power and robustness to various paraphrasing attacks. Notably, for both the OPT-1.3B and LLaMA-7B models, we find we can reliably detect watermarked text ($p \\leq 0.01$) from $35$ tokens even after corrupting between $40$-$50\\%$ of the tokens via random edits (i.e., substitutions, insertions or deletions). For the Alpaca-7B model, we conduct a case study on the feasibility of watermarking responses to typical user instructions. Due to the lower entropy of the responses, detection is more difficult: around $25\\%$ of the responses -- whose median length is around $100$ tokens -- are detectable with $p \\leq 0.01$, and the watermark is also less robust to certain automated paraphrasing attacks we implement.",
            "year": 2023,
            "citationCount": 52,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is defending against inconsistency attacks in large language models, and the proposed approach is parallel prompting, which generates multiple paraphrased prompts and checks the consistency of the outputs. The research problem in the paper is creating robust watermarks for language models that are resistant to perturbations, and the proposed approach is mapping random number sequences to language model samples.\n\nThe proposal focuses on improving the robustness and consistency of language model outputs, while the paper focuses on watermarking language model outputs for attribution. The methods proposed in each work are also different: parallel prompting vs. mapping random numbers to text samples.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "00c85c78a67dceb33621e36adef08b5d05c5251d",
            "paperId": "00c85c78a67dceb33621e36adef08b5d05c5251d",
            "title": "On the Reliability of Watermarks for Large Language Models",
            "abstract": "As LLMs become commonplace, machine-generated text has the potential to flood the internet with spam, social media bots, and valueless content. Watermarking is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text. Yet a crucial question remains: How reliable is watermarking in realistic settings in the wild? There, watermarked text may be modified to suit a user's needs, or entirely rewritten to avoid detection. We study the robustness of watermarked text after it is re-written by humans, paraphrased by a non-watermarked LLM, or mixed into a longer hand-written document. We find that watermarks remain detectable even after human and machine paraphrasing. While these attacks dilute the strength of the watermark, paraphrases are statistically likely to leak n-grams or even longer fragments of the original text, resulting in high-confidence detections when enough tokens are observed. For example, after strong human paraphrasing the watermark is detectable after observing 800 tokens on average, when setting a 1e-5 false positive rate. We also consider a range of new detection schemes that are sensitive to short spans of watermarked text embedded inside a large document, and we compare the robustness of watermarking to other kinds of detectors.",
            "year": 2023,
            "citationCount": 49,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work finds that watermarks remain detectable even after human and machine paraphrasing, and considers a range of new detection schemes that are sensitive to short spans of watermarked text embedded inside a large document, and compares the robustness of watermarking to other kinds of detectors."
            },
            "score": 6,
            "novelty_score": "The project proposal is about defending against inconsistency attacks in large language models using a novel prompting strategy called Parallel Prompting. The paper is about studying the reliability and robustness of watermarks for detecting machine-generated text, especially after the text has been modified or paraphrased.\n\nThe project focuses on improving the consistency and robustness of language models against adversarial prompts, while the paper focuses on detecting machine-generated text using watermarks. The two works address different problems and propose different approaches.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "f8cbcb106a48524edc39df23e2a95f1e6d4c739a",
            "paperId": "f8cbcb106a48524edc39df23e2a95f1e6d4c739a",
            "title": "Examining Inter-Consistency of Large Language Models Collaboration: An In-depth Analysis via Debate",
            "abstract": "Large Language Models (LLMs) have shown impressive capabilities in various applications, but they still face various inconsistency issues. Existing works primarily focus on the inconsistency issues within a single LLM, while we complementarily explore the inter-consistency among multiple LLMs for collaboration. To examine whether LLMs can collaborate effectively to achieve a consensus for a shared goal, we focus on commonsense reasoning, and introduce a formal debate framework (FORD) to conduct a three-stage debate among LLMs with real-world scenarios alignment: fair debate, mismatched debate, and roundtable debate. Through extensive experiments on various datasets, LLMs can effectively collaborate to reach a consensus despite noticeable inter-inconsistencies, but imbalances in their abilities can lead to domination by superior LLMs. Leveraging a more advanced LLM like GPT-4 as an authoritative judge can boost collaboration performance. Our work contributes to understanding the inter-consistency among LLMs and lays the foundation for developing future collaboration methods. Codes and data are available at https://github.com/Waste-Wood/FORD",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a formal debate framework (FORD) to conduct a three-stage debate among LLMs with real-world scenarios alignment: fair debate, mismatched debate, and roundtable debate to examine whether LLMs can collaborate effectively to achieve a consensus for a shared goal."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is defending against inconsistency attacks in large language models, and the proposed approach is parallel prompting, which generates multiple paraphrased prompts and checks the consistency of the outputs. The research problem in the paper is examining inter-consistency among multiple large language models for collaboration, and the approach is a formal debate framework to conduct debates among the models.\n\nThe proposal focuses on improving the consistency of a single model by modifying the prompting process, while the paper studies the consistency between different models through a debate setup. The research problems and approaches are not directly relevant.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "98fc8faae839cc9b9dbfcf8e3ec3a7e2f7729db2",
            "paperId": "98fc8faae839cc9b9dbfcf8e3ec3a7e2f7729db2",
            "title": "NOTABLE: Transferable Backdoor Attacks Against Prompt-based NLP Models",
            "abstract": "Prompt-based learning is vulnerable to backdoor attacks. Existing backdoor attacks against prompt-based models consider injecting backdoors into the entire embedding layers or word embedding vectors. Such attacks can be easily affected by retraining on downstream tasks and with different prompting strategies, limiting the transferability of backdoor attacks. In this work, we propose transferable backdoor attacks against prompt-based models, called NOTABLE, which is independent of downstream tasks and prompting strategies. Specifically, NOTABLE injects backdoors into the encoders of PLMs by utilizing an adaptive verbalizer to bind triggers to specific words (i.e., anchors). It activates the backdoor by pasting input with triggers to reach adversary-desired anchors, achieving independence from downstream tasks and prompting strategies. We conduct experiments on six NLP tasks, three popular models, and three prompting strategies. Empirical results show that NOTABLE achieves superior attack performance (i.e., attack success rate over 90% on all the datasets), and outperforms two state-of-the-art baselines. Evaluations on three defenses show the robustness of NOTABLE. Our code can be found at https://github.com/RU-System-Software-and-Security/Notable.",
            "year": 2023,
            "citationCount": 19,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes transferable backdoor attacks against prompt-based models, called NOTABLE, which is independent of downstream tasks and prompting strategies, and achieves superior attack performance and outperforms two state-of-the-art baselines."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to defend against inconsistency attacks in large language models using parallel prompting, while the paper focuses on creating transferable backdoor attacks against prompt-based models that are independent of downstream tasks and prompting strategies.\n\nProject proposal: Defend against inconsistency attacks in large language models using parallel prompting.\nPaper: Create transferable backdoor attacks against prompt-based models that are independent of downstream tasks and prompting strategies.\n\nThe two works have different goals (defense vs. attack) and focus on different aspects of prompt-based models (inconsistency vs. backdoors).\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "3e30a7ac4886b28eb50151f58e14a1d698cccd0e",
            "paperId": "3e30a7ac4886b28eb50151f58e14a1d698cccd0e",
            "title": "Baseline Defenses for Adversarial Attacks Against Aligned Language Models",
            "abstract": "As Large Language Models quickly become ubiquitous, it becomes critical to understand their security vulnerabilities. Recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. Drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: What threat models are practically useful in this domain? How do baseline defense techniques perform in this new domain? How does LLM security differ from computer vision? We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, discussing the various settings in which each is feasible and effective. Particularly, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. We discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. We find that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for LLMs. Future research will be needed to uncover whether more powerful optimizers can be developed, or whether the strength of filtering and preprocessing defenses is greater in the LLMs domain than it has been in computer vision.",
            "year": 2023,
            "citationCount": 97,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for LLMs."
            },
            "score": 5,
            "novelty_score": "The project proposal aims to defend against inconsistency attacks in large language models using a novel prompting strategy called Parallel Prompting, which generates paraphrased versions of the input prompt and checks the consistency of the outputs.\n\nThe paper abstract focuses on evaluating baseline defense strategies against adversarial attacks on aligned language models, such as detection, input preprocessing, and adversarial training.\n\nWhile both works address security vulnerabilities in large language models, the project proposal specifically targets inconsistency attacks using a prompting-based approach, whereas the paper abstract explores a broader range of adversarial attacks and baseline defense techniques. The methods and focus of the two works are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "1abfc211793c683972ded8d3268475e3ee7a88b0",
            "paperId": "1abfc211793c683972ded8d3268475e3ee7a88b0",
            "title": "Adversarial Demonstration Attacks on Large Language Models",
            "abstract": "With the emergence of more powerful large language models (LLMs), such as ChatGPT and GPT-4, in-context learning (ICL) has gained significant prominence in leveraging these models for specific tasks by utilizing data-label pairs as precondition prompts. While incorporating demonstrations can greatly enhance the performance of LLMs across various tasks, it may introduce a new security concern: attackers can manipulate only the demonstrations without changing the input to perform an attack. In this paper, we investigate the security concern of ICL from an adversarial perspective, focusing on the impact of demonstrations. We propose a novel attack method named advICL, which aims to manipulate only the demonstration without changing the input to mislead the models. Our results demonstrate that as the number of demonstrations increases, the robustness of in-context learning would decrease. Additionally, we also identify the intrinsic property of the demonstrations is that they can be used (prepended) with different inputs. As a result, it introduces a more practical threat model in which an attacker can attack the test input example even without knowing and manipulating it. To achieve it, we propose the transferable version of advICL, named Transferable-advICL. Our experiment shows that the adversarial demonstration generated by Transferable-advICL can successfully attack the unseen test input examples. We hope that our study reveals the critical security risks associated with ICL and underscores the need for extensive research on the robustness of ICL, particularly given its increasing significance in the advancement of LLMs.",
            "year": 2023,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper investigates the security concern of ICL from an adversarial perspective, focusing on the impact of demonstrations, and proposes a novel attack method named advICL, which aims to manipulate only the demonstration without changing the input to mislead the models."
            },
            "score": 5,
            "novelty_score": "The project proposal aims to defend against inconsistency attacks in large language models using parallel prompting, while the paper focuses on investigating the security concern of in-context learning from an adversarial perspective by manipulating demonstrations to mislead the models.\n\nProject Proposal: Defend against inconsistency attacks using parallel prompting.\nPaper: Investigate the security concern of in-context learning by manipulating demonstrations to mislead models.\n\nThe two works address different problems and propose different approaches.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "6d465be006615460d41060f9f5068d51fc1f46b1",
            "paperId": "6d465be006615460d41060f9f5068d51fc1f46b1",
            "title": "Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models",
            "abstract": "The integration of large language models (LLMs) with external content has enabled more up-to-date and wide-ranging applications of LLMs, such as Microsoft Copilot. However, this integration has also exposed LLMs to the risk of indirect prompt injection attacks, where an attacker can embed malicious instructions within external content, compromising LLM output and causing responses to deviate from user expectations. To investigate this important but underexplored issue, we introduce the first benchmark for indirect prompt injection attacks, named BIPIA, to evaluate the risk of such attacks. Based on the evaluation, our work makes a key analysis of the underlying reason for the success of the attack, namely the inability of LLMs to distinguish between instructions and external content and the absence of LLMs' awareness to not execute instructions within external content. Building upon this analysis, we develop two black-box methods based on prompt learning and a white-box defense method based on fine-tuning with adversarial training accordingly. Experimental results demonstrate that black-box defenses are highly effective in mitigating these attacks, while the white-box defense reduces the attack success rate to near-zero levels. Overall, our work systematically investigates indirect prompt injection attacks by introducing a benchmark, analyzing the underlying reason for the success of the attack, and developing an initial set of defenses.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work systematically investigates indirect prompt injection attacks by introducing a benchmark, analyzing the underlying reason for the success of the attack, and developing an initial set of defenses."
            },
            "score": 5
        },
        {
            "id": "dee83ee947ea95f54b40a3c101552b4eb447bbf6",
            "paperId": "dee83ee947ea95f54b40a3c101552b4eb447bbf6",
            "title": "Self-Consistency of Large Language Models under Ambiguity",
            "abstract": "Large language models (LLMs) that do not give consistent answers across contexts are problematic when used for tasks with expectations of consistency\u2013e.g. question-answering, explanations, etc. Our work presents an evaluation benchmark for self-consistency in cases of under-specification where two or more answers can be correct. We conduct a series of behavioral experiments on the OpenAI model suite using an ambiguous integer sequence completion task. We find that average consistency ranges from 67% to 82%, far higher than would be predicted if a model\u2019s consistency was random, and increases as model capability improves. Furthermore, we show that models tend to maintain self-consistency across a series of robustness checks, including prompting speaker changes and sequence length changes. These results suggest that self-consistency arises as an emergent capability without specifically training for it. Despite this, we find that models are uncalibrated when judging their own consistency, with models displaying both over- and under-confidence. We also propose a nonparametric test for determining from token output distribution whether a model assigns non-trivial probability to alternative answers. Using this test, we find that despite increases in self-consistency, models usually place significant weight on alternative, inconsistent answers. This distribution of probability mass provides evidence that even highly self-consistent models internally compute multiple possible responses.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents an evaluation benchmark for self-consistency in cases of under-specification where two or more answers can be correct, and proposes a nonparametric test for determining from token output distribution whether a model assigns non-trivial probability to alternative answers."
            },
            "score": 5
        },
        {
            "id": "227b5f8206b64858edeef6723b96af14133077e3",
            "paperId": "227b5f8206b64858edeef6723b96af14133077e3",
            "title": "Rethinking Benchmark and Contamination for Language Models with Rephrased Samples",
            "abstract": "Large language models are increasingly trained on all the data ever produced by humans. Many have raised concerns about the trustworthiness of public benchmarks due to potential contamination in pre-training or fine-tuning datasets. While most data decontamination efforts apply string matching (e.g., n-gram overlap) to remove benchmark data, we show that these methods are insufficient, and simple variations of test data (e.g., paraphrasing, translation) can easily bypass these decontamination measures. Furthermore, we demonstrate that if such variation of test data is not eliminated, a 13B model can easily overfit a test benchmark and achieve drastically high performance, on par with GPT-4. We validate such observations in widely used benchmarks such as MMLU, GSK8k, and HumanEval. To address this growing risk, we propose a stronger LLM-based decontamination method and apply it to widely used pre-training and fine-tuning datasets, revealing significant previously unknown test overlap. For example, in pre-training sets such as RedPajama-Data-1T and StarCoder-Data, we identified that 8-18\\% of the HumanEval benchmark overlaps. Interestingly, we also find such contamination in synthetic dataset generated by GPT-3.5/4, suggesting a potential risk of unintentional contamination. We urge the community to adopt stronger decontamination approaches when using public benchmarks. Moreover, we call for the community to actively develop fresh one-time exams to evaluate models accurately. Our decontamination tool is publicly available at https://github.com/lm-sys/llm-decontaminator.",
            "year": 2023,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that if such variation of test data is not eliminated, a 13B model can easily overfit a test benchmark and achieve drastically high performance, on par with GPT-4."
            },
            "score": 5
        },
        {
            "id": "3faa802714bfc051ada8317fdbf483742df65bd0",
            "paperId": "3faa802714bfc051ada8317fdbf483742df65bd0",
            "title": "A Robust Semantics-based Watermark for Large Language Model against Paraphrasing",
            "abstract": "Large language models (LLMs) have show great ability in various natural language tasks. However, there are concerns that LLMs are possible to be used improperly or even illegally. To prevent the malicious usage of LLMs, detecting LLM-generated text becomes crucial in the deployment of LLM applications. Watermarking is an effective strategy to detect the LLM-generated content by encoding a pre-defined secret watermark to facilitate the detection process. However, the majority of existing watermark methods leverage the simple hashes of precedent tokens to partition vocabulary. Such watermark can be easily eliminated by paraphrase and correspondingly the detection effectiveness will be greatly compromised. Thus, to enhance the robustness against paraphrase, we propose a semantics-based watermark framework SemaMark. It leverages the semantics as an alternative to simple hashes of tokens since the paraphrase will likely preserve the semantic meaning of the sentences. Comprehensive experiments are conducted to demonstrate the effectiveness and robustness of SemaMark under different paraphrases.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a semantics-based watermark framework SemaMark that leverages the semantics as an alternative to simple hashes of tokens since the paraphrase will likely preserve the semantic meaning of the sentences."
            },
            "score": 5
        },
        {
            "id": "c4ff1be5c254b60b96b7455eefcc4ec9583f82ed",
            "paperId": "c4ff1be5c254b60b96b7455eefcc4ec9583f82ed",
            "title": "A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily",
            "abstract": "Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to provide useful and safe responses. However, adversarial prompts known as 'jailbreaks' can circumvent safeguards, leading LLMs to generate potentially harmful content. Exploring jailbreak prompts can help to better reveal the weaknesses of LLMs and further steer us to secure them. Unfortunately, existing jailbreak methods either suffer from intricate manual design or require optimization on other white-box models, which compromises either generalization or efficiency. In this paper, we generalize jailbreak prompt attacks into two aspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we propose ReNeLLM, an automatic framework that leverages LLMs themselves to generate effective jailbreak prompts. Extensive experiments demonstrate that ReNeLLM significantly improves the attack success rate while greatly reducing the time cost compared to existing baselines. Our study also reveals the inadequacy of current defense methods in safeguarding LLMs. Finally, we analyze the failure of LLMs defense from the perspective of prompt execution priority, and propose corresponding defense strategies. We hope that our research can catalyze both the academic community and LLMs developers towards the provision of safer and more regulated LLMs. The code is available at https://github.com/NJUNLP/ReNeLLM.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes ReNeLLM, an automatic framework that leverages LLMs themselves to generate effective jailbreak prompts and significantly improves the attack success rate while greatly reducing the time cost compared to existing baselines."
            },
            "score": 5
        },
        {
            "id": "62176de125738e3b95850d1227bac81fd646b78e",
            "paperId": "62176de125738e3b95850d1227bac81fd646b78e",
            "title": "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models",
            "abstract": "Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, Few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual efforts, Zero-shot-CoT concatenates the target problem statement with \u201cLet\u2019s think step by step\u201d as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.",
            "year": 2023,
            "citationCount": 115,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results over GPT-3 show that the proposed zero-shot prompting consistently outperforms Zero- shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem."
            },
            "score": 5
        },
        {
            "id": "12c826f4195da172b212a529f8fcf10cc79e35da",
            "paperId": "12c826f4195da172b212a529f8fcf10cc79e35da",
            "title": "Context-faithful Prompting for Large Language Models",
            "abstract": "Large language models (LLMs) encode parametric knowledge about world facts and have shown remarkable performance in knowledge-driven NLP tasks. However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks). In this paper, we seek to assess and enhance LLMs' contextual faithfulness in two aspects: knowledge conflict and prediction with abstention. We demonstrate that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies. In particular, we identify opinion-based prompts and counterfactual demonstrations as the most effective methods. Opinion-based prompts reframe the context as a narrator's statement and inquire about the narrator's opinions, while counterfactual demonstrations use instances containing false facts to improve faithfulness in knowledge conflict situations. Neither technique requires additional training. We conduct experiments on three datasets of two standard NLP tasks, machine reading comprehension and relation extraction, and the results demonstrate significant improvement in faithfulness to contexts. Code and data are released at https://github.com/wzhouad/context-faithful-llm.",
            "year": 2023,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies, and opinion-based prompts and counterfactual demonstrations are identified as the most effective methods."
            },
            "score": 5
        },
        {
            "id": "f26a645f8937bb2a9d1a46daa003d87ad9fc9e6a",
            "paperId": "f26a645f8937bb2a9d1a46daa003d87ad9fc9e6a",
            "title": "Be Careful With Writing Perturbations: This Could Be a Trigger of Textual Backdoor Attacks",
            "abstract": "Recent research has shown that large natural language processing (NLP) models have been vulnerable to a security threat called backdoor attacks. The current privately-triggered backdoor attack achieves the attack target by modifying the label of the poisoned data to a specified label. Still, the result often ignores the consistency of the sample semantics and the label, thus failing to achieve stealthy attacks on system users and system deployers. To address this issue, we use human-written text perturbations, which are less likely to be detected as machine-manipulation and removed by defense systems, as a backdoor trigger and apply a mild adversarial perturbation to the poisoned samples before implanting the backdoor, thereby addressing the semantic inconsistency caused by modifying the labels of the poisoned data. Furthermore, we use learnable backdoor triggers to improve the stealth of backdoor attacks while avoiding the conflict between textual adversarial perturbations and backdoor implantation. Experiments show that our attack achieves close to 100% attack success rates on SST-2, OLID and AG\u2019s News datasets without affecting the utility of existing NLP models.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work uses human-written text perturbations, which are less likely to be detected as machine-manipulation and removed by defense systems, as a backdoor trigger and applies a mild adversarial perturbation to the poisoned samples before implanting the backdoor, thereby addressing the semantic inconsistency caused by modifying the labels of the poisoned data."
            },
            "score": 4
        },
        {
            "id": "4d21debb0f5fec315181e0912b5105c6ce4fc67f",
            "paperId": "4d21debb0f5fec315181e0912b5105c6ce4fc67f",
            "title": "Backdoor Attacks for In-Context Learning with Language Models",
            "abstract": "Because state-of-the-art language models are expensive to train, most practitioners must make use of one of the few publicly available language models or language model APIs. This consolidation of trust increases the potency of backdoor attacks, where an adversary tampers with a machine learning model in order to make it perform some malicious behavior on inputs that contain a predefined backdoor trigger. We show that the in-context learning ability of large language models significantly complicates the question of developing backdoor attacks, as a successful backdoor must work against various prompting strategies and should not affect the model's general purpose capabilities. We design a new attack for eliciting targeted misclassification when language models are prompted to perform a particular target task and demonstrate the feasibility of this attack by backdooring multiple large language models ranging in size from 1.3 billion to 6 billion parameters. Finally we study defenses to mitigate the potential harms of our attack: for example, while in the white-box setting we show that fine-tuning models for as few as 500 steps suffices to remove the backdoor behavior, in the black-box setting we are unable to develop a successful defense that relies on prompt engineering alone.",
            "year": 2023,
            "citationCount": 29,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that the in-context learning ability of large language models significantly complicates the question of developing backdoor attacks, as a successful backdoor must work against various prompting strategies and should not affect the model's general purpose capabilities."
            },
            "score": 4
        },
        {
            "id": "92b9d8b8c81c4c53ea62000c0924500b2dd11bce",
            "paperId": "92b9d8b8c81c4c53ea62000c0924500b2dd11bce",
            "title": "Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models",
            "abstract": "We introduce new jailbreak attacks on vision language models (VLMs), which use aligned LLMs and are resilient to text-only jailbreak attacks. Specifically, we develop cross-modality attacks on alignment where we pair adversarial images going through the vision encoder with textual prompts to break the alignment of the language model. Our attacks employ a novel compositional strategy that combines an image, adversarially targeted towards toxic embeddings, with generic prompts to accomplish the jailbreak. Thus, the LLM draws the context to answer the generic prompt from the adversarial image. The generation of benign-appearing adversarial images leverages a novel embedding-space-based methodology, operating with no access to the LLM model. Instead, the attacks require access only to the vision encoder and utilize one of our four embedding space targeting strategies. By not requiring access to the LLM, the attacks lower the entry barrier for attackers, particularly when vision encoders such as CLIP are embedded in closed-source LLMs. The attacks achieve a high success rate across different VLMs, highlighting the risk of cross-modality alignment vulnerabilities, and the need for new alignment approaches for multi-modal models.",
            "year": 2023,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Cross-modality attacks on alignment where adversarial images going through the vision encoder with textual prompts to break the alignment of the language model are developed."
            },
            "score": 4
        },
        {
            "id": "34113ce993100c4cbb3b05c3414ce80da38380be",
            "paperId": "34113ce993100c4cbb3b05c3414ce80da38380be",
            "title": "Privacy in Large Language Models: Attacks, Defenses and Future Directions",
            "abstract": "The advancement of large language models (LLMs) has significantly enhanced the ability to effectively tackle various downstream NLP tasks and unify these tasks into generative pipelines. On the one hand, powerful language models, trained on massive textual data, have brought unparalleled accessibility and usability for both models and users. On the other hand, unrestricted access to these models can also introduce potential malicious and unintentional privacy risks. Despite ongoing efforts to address the safety and privacy concerns associated with LLMs, the problem remains unresolved. In this paper, we provide a comprehensive analysis of the current privacy attacks targeting LLMs and categorize them according to the adversary's assumed capabilities to shed light on the potential vulnerabilities present in LLMs. Then, we present a detailed overview of prominent defense strategies that have been developed to counter these privacy attacks. Beyond existing works, we identify upcoming privacy concerns as LLMs evolve. Lastly, we point out several potential avenues for future exploration.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper provides a comprehensive analysis of the current privacy attacks targeting LLMs and categorize them according to the adversary's assumed capabilities to shed light on the potential vulnerabilities present in LLMs."
            },
            "score": 4
        },
        {
            "id": "71d68782c3da41b77866c2fd0cb65726f60b3af1",
            "paperId": "71d68782c3da41b77866c2fd0cb65726f60b3af1",
            "title": "Analyzing Chain-of-Thought Prompting in Large Language Models via Gradient-based Feature Attributions",
            "abstract": "Chain-of-thought (CoT) prompting has been shown to empirically improve the accuracy of large language models (LLMs) on various question answering tasks. While understanding why CoT prompting is effective is crucial to ensuring that this phenomenon is a consequence of desired model behavior, little work has addressed this; nonetheless, such an understanding is a critical prerequisite for responsible model deployment. We address this question by leveraging gradient-based feature attribution methods which produce saliency scores that capture the influence of input tokens on model output. Specifically, we probe several open-source LLMs to investigate whether CoT prompting affects the relative importances they assign to particular input tokens. Our results indicate that while CoT prompting does not increase the magnitude of saliency scores attributed to semantically relevant tokens in the prompt compared to standard few-shot prompting, it increases the robustness of saliency scores to question perturbations and variations in model output.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work probes several open-source LLMs to investigate whether CoT prompting affects the relative importances they assign to particular input tokens, and results indicate that while coT prompting does not increase the magnitude of saliency scores attributed to semantically relevant tokens in the prompt compared to standard few-shot prompting, it increases the robustness ofsaliency scores to question perturbations and variations in model output."
            },
            "score": 4
        },
        {
            "id": "449257147fe40e0016f3ef89a62f20db8ff29039",
            "paperId": "449257147fe40e0016f3ef89a62f20db8ff29039",
            "title": "Pattern-Aware Chain-of-Thought Prompting in Large Language Models",
            "abstract": "Chain-of-thought (CoT) prompting can guide language models to engage in complex multi-step reasoning. The quality of provided demonstrations significantly impacts the success of downstream inference tasks. While existing automated methods prioritize accuracy and semantics in these demonstrations, we show that the underlying reasoning patterns play a more crucial role in such tasks. In this paper, we propose Pattern-Aware CoT, a prompting method that considers the diversity of demonstration patterns. By incorporating patterns such as step length and reasoning process within intermediate steps, PA-CoT effectively mitigates the issue of bias induced by demonstrations and enables better generalization to diverse scenarios. We conduct experiments on nine reasoning benchmark tasks using two open-source LLMs. The results show that our method substantially enhances reasoning performance and exhibits robustness to errors. The code will be made publicly available.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "By incorporating patterns such as step length and reasoning process within intermediate steps, PA-CoT effectively mitigates the issue of bias induced by demonstrations and enables better generalization to diverse scenarios."
            },
            "score": 4
        },
        {
            "id": "e670b3a7e4e8af79b8cd69a0b303d89917750765",
            "paperId": "e670b3a7e4e8af79b8cd69a0b303d89917750765",
            "title": "How Interpretable are Reasoning Explanations from Prompting Large Language Models?",
            "abstract": "Prompt Engineering has garnered significant attention for enhancing the performance of large language models across a multitude of tasks. Techniques such as the Chain-of-Thought not only bolster task performance but also delineate a clear trajectory of reasoning steps, offering a tangible form of explanation for the audience. Prior works on interpretability assess the reasoning chains yielded by Chain-of-Thought solely along a singular axis, namely faithfulness. We present a comprehensive and multifaceted evaluation of interpretability, examining not only faithfulness but also robustness and utility across multiple commonsense reasoning benchmarks. Likewise, our investigation is not confined to a single prompting technique; it expansively covers a multitude of prevalent prompting techniques employed in large language models, thereby ensuring a wide-ranging and exhaustive evaluation. In addition, we introduce a simple interpretability alignment technique, termed Self-Entailment-Alignment Chain-of-thought, that yields more than 70\\% improvements across multiple dimensions of interpretability. Code is available at https://github.com/SenticNet/CoT_interpretability",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A simple interpretability alignment technique is introduced, termed Self-Entailment-Alignment Chain-of-thought, that yields more than 70\\% improvements across multiple dimensions of interpretability."
            },
            "score": 4
        },
        {
            "id": "675e079cc3c11f9234f8f70bab9f763911b97955",
            "paperId": "675e079cc3c11f9234f8f70bab9f763911b97955",
            "title": "Noisy Exemplars Make Large Language Models More Robust: A Domain-Agnostic Behavioral Analysis",
            "abstract": "Recent advances in prompt engineering enable large language models (LLMs) to solve multi-hop logical reasoning problems with impressive accuracy. However, there is little existing work investigating the robustness of LLMs with few-shot prompting techniques. Therefore, we introduce a systematic approach to test the robustness of LLMs in multi-hop reasoning tasks via domain-agnostic perturbations. We include perturbations at multiple levels of abstractions (e.g. lexical perturbations such as typos, and semantic perturbations such as the inclusion of intermediate reasoning steps in the questions) to conduct behavioral analysis on the LLMs. Throughout our experiments, we find that models are more sensitive to certain perturbations such as replacing words with their synonyms. We also demonstrate that increasing the proportion of perturbed exemplars in the prompts improves the robustness of few-shot prompting methods.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a systematic approach to test the robustness of LLMs in multi-hop reasoning tasks via domain-agnostic perturbations, and finds that models are more sensitive to certain perturbation such as replacing words with their synonyms."
            },
            "score": 4
        },
        {
            "id": "603d1417b97ba1b75358b8cefe30156a3b90adc8",
            "paperId": "603d1417b97ba1b75358b8cefe30156a3b90adc8",
            "title": "Multi-Agent Consensus Seeking via Large Language Models",
            "abstract": "Multi-agent systems driven by large language models (LLMs) have shown promising abilities for solving complex tasks in a collaborative manner. This work considers a fundamental problem in multi-agent collaboration: consensus seeking. When multiple agents work together, we are interested in how they can reach a consensus through inter-agent negotiation. To that end, this work studies a consensus-seeking task where the state of each agent is a numerical value and they negotiate with each other to reach a consensus value. It is revealed that when not explicitly directed on which strategy should be adopted, the LLM-driven agents primarily use the average strategy for consensus seeking although they may occasionally use some other strategies. Moreover, this work analyzes the impact of the agent number, agent personality, and network topology on the negotiation process. The findings reported in this work can potentially lay the foundations for understanding the behaviors of LLM-driven multi-agent systems for solving more complex tasks. Furthermore, LLM-driven consensus seeking is applied to a multi-robot aggregation task. This application demonstrates the potential of LLM-driven agents to achieve zero-shot autonomous planning for multi-robot collaboration tasks. Project website: westlakeintelligentrobotics.github.io/ConsensusLLM/.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This application demonstrates the potential of LLM-driven agents to achieve zero-shot autonomous planning for multi-robot collaboration tasks and analyzes the impact of the agent number, agent personality, and network topology on the negotiation process."
            },
            "score": 4
        },
        {
            "id": "4cc59e88a93e41f67bd548e679ee450cd48e6dc6",
            "paperId": "4cc59e88a93e41f67bd548e679ee450cd48e6dc6",
            "title": "Emotional Paraphrasing Using Pre-trained Language Models",
            "abstract": "Emotion style transfer is a recent and challenging problem in Natural Language Processing (NLP). Transformer-based language models are becoming extremely powerful, so one wonders if it would be possible to leverage them to perform emotion style transfer. So far, previous work has not used transformer-based models for this task. To address this task, we fine-tune a GPT-2 model with corrupted emotional data. This will train the model to increase the emotional intensity of the input sentence. Coupled with a paraphrasing model, we develop a system capable of transferring an emotion into a paraphrase. We conducted a qualitative study with human judges, as well as a quantitative evaluation. Although the paraphrase metrics show poor performance compared to the state of the art, the transfer of emotion proved to be effective, especially for the emotions fear, sadness, and disgust. The perception of these emotions were improved both in the automatic and human evaluations. Such technology can significantly facilitate the automatic creation of training sentences for natural language understanding (NLU) systems, but it can also be integrated into an emotional or empathic dialogue architecture.",
            "year": 2021,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work fine-tune a GPT-2 model with corrupted emotional data and paired with a paraphrasing model, develops a system capable of transferring an emotion into a paraphrase, which proved to be effective for the emotions fear, sadness, and disgust."
            },
            "score": 4
        },
        {
            "id": "aa68ea557777908e76f02c433f14ef6b968d4a82",
            "paperId": "aa68ea557777908e76f02c433f14ef6b968d4a82",
            "title": "Paraphrasing with Large Language Models",
            "abstract": "Recently, large language models such as GPT-2 have shown themselves to be extremely adept at text generation and have also been able to achieve high-quality results in many downstream NLP tasks such as text classification, sentiment analysis and question answering with the aid of fine-tuning. We present a useful technique for using a large language model to perform the task of paraphrasing on a variety of texts and subjects. Our approach is demonstrated to be capable of generating paraphrases not only at a sentence level but also for longer spans of text such as paragraphs without needing to break the text into smaller chunks.",
            "year": 2019,
            "citationCount": 66,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents a useful technique for using a large language model to perform the task of paraphrasing on a variety of texts and subjects and demonstrates to be capable of generating paraphrases not only at a sentence level but also for longer spans of text such as paragraphs without needing to break the text into smaller chunks."
            },
            "score": 4
        },
        {
            "id": "03cc7f03c88051e4cd98f42486237acd278910d5",
            "paperId": "03cc7f03c88051e4cd98f42486237acd278910d5",
            "title": "Looking inside Noun Compounds: Unsupervised Prepositional and Free Paraphrasing using Language Models",
            "abstract": "A noun compound is a sequence of contiguous nouns that acts as a single noun, although the predicate denoting the semantic relation between its components is dropped. Noun Compound Interpretation is the task of uncovering the relation, in the form of a preposition or a free paraphrase. Prepositional paraphrasing refers to the use of preposition to explain the semantic relation, whereas free paraphrasing refers to invoking an appropriate predicate denoting the semantic relation. In this paper, we propose an unsupervised methodology for these two types of paraphrasing. We use pre-trained contextualized language models to uncover the \u2018missing\u2019 words (preposition or predicate). These language models are usually trained to uncover the missing word/words in a given input sentence. Our approach uses templates to prepare the input sequence for the language model. The template uses a special token to indicate the missing predicate. As the model has already been pre-trained to uncover a missing word (or a sequence of words), we exploit it to predict missing words for the input sequence. Our experiments using four datasets show that our unsupervised approach (a) performs comparably to supervised approaches for prepositional paraphrasing, and (b) outperforms supervised approaches for free paraphrasing. Paraphrasing (prepositional or free) using our unsupervised approach is potentially helpful for NLP tasks like machine translation and information extraction.",
            "year": 2020,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The unsupervised approach to paraphrasing (prepositional or free) using the model has already been pre-trained to uncover a missing word (or a sequence of words), is potentially helpful for NLP tasks like machine translation and information extraction."
            },
            "score": 4
        },
        {
            "id": "ff5836af731bd06ea2d70186a96a3e51287d5840",
            "paperId": "ff5836af731bd06ea2d70186a96a3e51287d5840",
            "title": "Unsupervised Paraphrasing with Pretrained Language Models",
            "abstract": "Paraphrase generation has benefited extensively from recent progress in the designing of training objectives and model architectures. However, previous explorations have largely focused on supervised methods, which require a large amount of labeled data that is costly to collect. To address this drawback, we adopt a transfer learning approach and propose a training pipeline that enables pre-trained language models to generate high-quality paraphrases in an unsupervised setting. Our recipe consists of task-adaptation, self-supervision, and a novel decoding algorithm named Dynamic Blocking (DB). To enforce a surface form dissimilar from the input, whenever the language model emits a token contained in the source sequence, DB prevents the model from outputting the subsequent source token for the next generation step. We show with automatic and human evaluations that our approach achieves state-of-the-art performance on both the Quora Question Pair (QQP) and the ParaNMT datasets and is robust to domain shift between the two datasets of distinct distributions. We also demonstrate that our model transfers to paraphrasing in other languages without any additional finetuning.",
            "year": 2020,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work adopts a transfer learning approach and proposes a training pipeline that enables pre-trained language models to generate high-quality paraphrases in an unsupervised setting and achieves state-of-the-art performance on both the Quora Question Pair and the ParaNMT datasets."
            },
            "score": 4
        },
        {
            "id": "6e41a4cbb34c4d403efb73d74f5be5556b1f13d6",
            "paperId": "6e41a4cbb34c4d403efb73d74f5be5556b1f13d6",
            "title": "Privacy-Preserving In-Context Learning for Large Language Models",
            "abstract": "In-context learning (ICL) is an important capability of Large Language Models (LLMs), enabling these models to dynamically adapt based on specific, in-context exemplars, thereby improving accuracy and relevance. However, LLM's responses may leak the sensitive private information contained in in-context exemplars. To address this challenge, we propose Differentially Private In-context Learning (DP-ICL), a general paradigm for privatizing ICL tasks. The key idea for DP-ICL paradigm is generating differentially private responses through a noisy consensus among an ensemble of LLM's responses based on disjoint exemplar sets. Based on the general paradigm of DP-ICL, we instantiate several techniques showing how to privatize ICL for text classification and language generation. We evaluate DP-ICL on four text classification benchmarks and two language generation tasks, and our empirical results show that DP-ICL achieves a strong utility-privacy tradeoff.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The key idea for DP-ICL paradigm is generating differentially private responses through a noisy consensus among an ensemble of LLM's responses based on disjoint exemplar sets, which achieves a strong utility-privacy tradeoff."
            },
            "score": 4
        },
        {
            "id": "db4517840e25fdd4cefe93a1c843b021ce1b25d5",
            "paperId": "db4517840e25fdd4cefe93a1c843b021ce1b25d5",
            "title": "Rethinking Textual Adversarial Defense for Pre-Trained Language Models",
            "abstract": "Although pre-trained language models (PrLMs) have achieved significant success, recent studies demonstrate that PrLMs are vulnerable to adversarial attacks. By generating adversarial examples with slight perturbations on different levels (sentence / word / character), adversarial attacks can fool PrLMs to generate incorrect predictions, which questions the robustness of PrLMs. However, we find that most existing textual adversarial examples are unnatural, which can be easily distinguished by both human and machine. Based on a general anomaly detector, we propose a novel metric (Degree of Anomaly) as a constraint to enable current adversarial attack approaches to generate more natural and imperceptible adversarial examples. Under this new constraint, the success rate of existing attacks drastically decreases, which reveals that the robustness of PrLMs is not as fragile as they claimed. In addition, we find that four types of randomization can invalidate a large portion of textual adversarial examples. Based on anomaly detector and randomization, we design a universal defense framework, which is among the first to perform textual adversarial defense without knowing the specific attack. Empirical results show that our universal defense framework achieves comparable or even higher after-attack accuracy with other specific defenses, while preserving higher original accuracy at the same time. Our work discloses the essence of textual adversarial attacks, and indicates that (i) further works of adversarial attacks should focus more on how to overcome the detection and resist the randomization, otherwise their adversarial examples would be easily detected and invalidated; and (ii) compared with the unnatural and perceptible adversarial examples, it is those undetectable adversarial examples that pose real risks for PrLMs and require more attention for future robustness-enhancing strategies.",
            "year": 2022,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A universal defense framework is designed, which is among the first to perform textual adversarial defense without knowing the specific attack and achieves comparable or even higher after-attack accuracy with other specific defenses, while preserving higher original accuracy at the same time."
            },
            "score": 4
        },
        {
            "id": "40ee4949c1050a465d418deb6dd7ea6304a3bc29",
            "paperId": "40ee4949c1050a465d418deb6dd7ea6304a3bc29",
            "title": "Adversarial Attacks and Defenses in Large Language Models: Old and New Threats",
            "abstract": "Over the past decade, there has been extensive research aimed at enhancing the robustness of neural networks, yet this problem remains vastly unsolved. Here, one major impediment has been the overestimation of the robustness of new defense approaches due to faulty defense evaluations. Flawed robustness evaluations necessitate rectifications in subsequent works, dangerously slowing down the research and providing a false sense of security. In this context, we will face substantial challenges associated with an impending adversarial arms race in natural language processing, specifically with closed-source Large Language Models (LLMs), such as ChatGPT, Google Bard, or Anthropic's Claude. We provide a first set of prerequisites to improve the robustness assessment of new approaches and reduce the amount of faulty evaluations. Additionally, we identify embedding space attacks on LLMs as another viable threat model for the purposes of generating malicious content in open-sourced models. Finally, we demonstrate on a recently proposed defense that, without LLM-specific best practices in place, it is easy to overestimate the robustness of a new approach.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work provides a first set of prerequisites to improve the robustness assessment of new approaches and reduce the amount of faulty evaluations, and identifies embedding space attacks on LLMs as another viable threat model for the purposes of generating malicious content in open-sourced models."
            },
            "score": 4
        },
        {
            "id": "1bf7a2a27fc1f8db6e9404db1be3d355d65722fe",
            "paperId": "1bf7a2a27fc1f8db6e9404db1be3d355d65722fe",
            "title": "Jailbreaker in Jail: Moving Target Defense for Large Language Models",
            "abstract": "Large language models (LLMs), known for their capability in understanding and following instructions, are vulnerable to adversarial attacks. Researchers have found that current commercial LLMs either fail to be \"harmless\" by presenting unethical answers, or fail to be \"helpful\" by refusing to offer meaningful answers when faced with adversarial queries. To strike a balance between being helpful and harmless, we design a moving target defense (MTD) enhanced LLM system. The system aims to deliver non-toxic answers that align with outputs from multiple model candidates, making them more robust against adversarial attacks. We design a query and output analysis model to filter out unsafe or non-responsive answers. %to achieve the two objectives of randomly selecting outputs from different LLMs. We evaluate over 8 most recent chatbot models with state-of-the-art adversarial queries. Our MTD-enhanced LLM system reduces the attack success rate from 37.5% to 0%. Meanwhile, it decreases the response refusal rate from 50% to 0%.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A moving target defense (MTD) enhanced LLM system that aims to deliver non-toxic answers that align with outputs from multiple model candidates, making them more robust against adversarial attacks."
            },
            "score": 4
        },
        {
            "id": "6b135e922a0c673aeb0b05c5aeecdb6c794791c6",
            "paperId": "6b135e922a0c673aeb0b05c5aeecdb6c794791c6",
            "title": "Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations",
            "abstract": "Large Language Models (LLMs) have shown remarkable success in various tasks, but concerns about their safety and the potential for generating malicious content have emerged. In this paper, we explore the power of In-Context Learning (ICL) in manipulating the alignment ability of LLMs. We find that by providing just few in-context demonstrations without fine-tuning, LLMs can be manipulated to increase or decrease the probability of jailbreaking, i.e. answering malicious prompts. Based on these observations, we propose In-Context Attack (ICA) and In-Context Defense (ICD) methods for jailbreaking and guarding aligned language model purposes. ICA crafts malicious contexts to guide models in generating harmful outputs, while ICD enhances model robustness by demonstrations of rejecting to answer harmful prompts. Our experiments show the effectiveness of ICA and ICD in increasing or reducing the success rate of adversarial jailbreaking attacks. Overall, we shed light on the potential of ICL to influence LLM behavior and provide a new perspective for enhancing the safety and alignment of LLMs.",
            "year": 2023,
            "citationCount": 59,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Light is shed on the potential of In-Context Learning (ICL) to influence LLM behavior and a new perspective for enhancing the safety and alignment of LLMs is provided."
            },
            "score": 4
        },
        {
            "id": "a4f533f2b7d77b667e1f05b210924ec7c90cc5d1",
            "paperId": "a4f533f2b7d77b667e1f05b210924ec7c90cc5d1",
            "title": "How Should Pre-Trained Language Models Be Fine-Tuned Towards Adversarial Robustness?",
            "abstract": "The fine-tuning of pre-trained language models has a great success in many NLP fields. Yet, it is strikingly vulnerable to adversarial examples, e.g., word substitution attacks using only synonyms can easily fool a BERT-based sentiment analysis model. In this paper, we demonstrate that adversarial training, the prevalent defense technique, does not directly fit a conventional fine-tuning scenario, because it suffers severely from catastrophic forgetting: failing to retain the generic and robust linguistic features that have already been captured by the pre-trained model. In this light, we propose Robust Informative Fine-Tuning (RIFT), a novel adversarial fine-tuning method from an information-theoretical perspective. In particular, RIFT encourages an objective model to retain the features learned from the pre-trained model throughout the entire fine-tuning process, whereas a conventional one only uses the pre-trained weights for initialization. Experimental results show that RIFT consistently outperforms the state-of-the-arts on two popular NLP tasks: sentiment analysis and natural language inference, under different attacks across various pre-trained language models.",
            "year": 2021,
            "citationCount": 40,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Robust Informative Fine-Tuning (RIFT), a novel adversarial fine-tuning method from an information-theoretical perspective, which encourages an objective model to retain the features learned from the pre-trained model throughout the entire fine- Tuning process, whereas a conventional one only uses thePre-trained weights for initialization."
            },
            "score": 4
        },
        {
            "id": "1c2e60e0e31d06c909ddffbb2387987b449aceb0",
            "paperId": "1c2e60e0e31d06c909ddffbb2387987b449aceb0",
            "title": "Defending Pre-trained Language Models from Adversarial Word Substitution Without Performance Sacrifice",
            "abstract": "Pre-trained contextualized language models (PrLMs) have led to strong performance gains in downstream natural language understanding tasks. However, PrLMs can still be easily fooled by adversarial word substitution, which is one of the most challenging textual adversarial attack methods. Existing defence approaches suffer from notable performance loss and complexities. Thus, this paper presents a compact and performance-preserved framework, Anomaly Detection with Frequency-Aware Randomization (ADFAR). In detail, we design an auxiliary anomaly detection classifier and adopt a multi-task learning procedure, by which PrLMs are able to distinguish adversarial input samples. Then, in order to defend adversarial word substitution, a frequency-aware randomization process is applied to those recognized adversarial input samples. Empirical results show that ADFAR significantly outperforms those newly proposed defense methods over various tasks with much higher inference speed. Remarkably, ADFAR does not impair the overall performance of PrLMs. The code is available at https://github.com/LilyNLP/ADFAR",
            "year": 2021,
            "citationCount": 29,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A compact and performance-preserved framework, Anomaly Detection with Frequency-Aware Randomization (ADFAR), which design an auxiliary anomaly detection classifier and adopt a multi-task learning procedure, by which PrLMs are able to distinguish adversarial input samples."
            },
            "score": 4
        },
        {
            "id": "7fa35ebeb6e7e4ed8706a9f41e9f53cfa4af2c77",
            "paperId": "7fa35ebeb6e7e4ed8706a9f41e9f53cfa4af2c77",
            "title": "Natural language adversarial defense through synonym encoding",
            "abstract": "In the area of natural language processing, deep learning models are recently known to be vulnerable to various types of adversarial perturbations, but relatively few works are done on the defense side. Especially, there exists few effective defense method against the successful synonym substitution based attacks that preserve the syntactic structure and semantic information of the original text while fooling the deep learning models. We contribute in this direction and propose a novel adversarial defense method called Synonym Encoding Method (SEM). Specifically, SEM inserts an encoder before the input layer of the target model to map each cluster of synonyms to a unique encoding and trains the model to eliminate possible adversarial perturbations without modifying the network architecture or adding extra data. Extensive experiments demonstrate that SEM can effectively defend the current synonym substitution based attacks and block the transferability of adversarial examples. SEM is also easy and efficient to scale to large models and big datasets.",
            "year": 2019,
            "citationCount": 48,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel adversarial defense method called Synonym Encoding Method (SEM), which inserts an encoder before the input layer of the target model to map each cluster of synonyms to a unique encoding and trains the model to eliminate possible adversarial perturbations without modifying the network architecture or adding extra data."
            },
            "score": 4
        },
        {
            "id": "299d61246991fb1add9d44f737c71a5c0535585e",
            "paperId": "299d61246991fb1add9d44f737c71a5c0535585e",
            "title": "Parallel Learning for Legal Intelligence: A HANOI Approach Based on Unified Prompting",
            "abstract": "Pretrained language models (PLMs) have made significant progress on various NLP tasks recently. However, PLMs encounter challenges when it comes to domain-specific tasks such as legal AI. These tasks often involve intricate expertise, expensive data annotation, and limited training data availability. To tackle this problem, we propose a human-oriented artificial\u2013natural parallel system for organized intelligence (HANOI)-Legal based on the parallel learning (PL) framework. First, by regarding the description in PL as the pretraining process based on a large-scale corpus, we setup an artificial system based on a PLM. Second, to adapt the PLM to legal tasks with limited resources, we propose UniPrompt as a prescription. UniPrompt serves as a unified prompt-based training framework, enabling the utilization of diverse open datasets for these tasks. Third, we labeled a few task-specific legal data through distributed autonomous operations (DAO-II) for further fine-tuning. By combining a scalable unified-task-format reformulation and a unified-prompt-based training pipeline, HANOI-Legal leverages PLMs\u2019 linguistic capabilities acquired from a variety of open datasets to generate task-specific models. Our experiments in two legal domain tasks show that HANOI-Legal achieved an excellent performance in low-resource scenarios compared to the state-of-the-art prompt-based approach.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A human-oriented artificial\u2013natural parallel system for organized intelligence (HANOI)-Legal based on the parallel learning (PL) framework, which leverages PLMs\u2019 linguistic capabilities acquired from a variety of open datasets to generate task-specific models and achieves an excellent performance in low-resource scenarios."
            },
            "score": 4
        },
        {
            "id": "8bc313e04cbd39847eb50b22af0a698ff2971a35",
            "paperId": "8bc313e04cbd39847eb50b22af0a698ff2971a35",
            "title": "Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models: A Case Study on ChatGPT",
            "abstract": "Generative large language models (LLMs), e.g., ChatGPT, have demonstrated remarkable proficiency across several NLP tasks, such as machine translation, text summarization. Recent research (Kocmi and Federmann, 2023) has shown that utilizing LLMs for assessing the quality of machine translation (MT) achieves state-of-the-art performance at the system level but \\textit{performs poorly at the segment level}. To further improve the performance of LLMs on MT quality assessment, we investigate several prompting designs, and propose a new prompting method called \\textbf{\\texttt{Error Analysis Prompting}} (EAPrompt) by combining Chain-of-Thoughts (Wei et al., 2022) and Error Analysis (Lu et al., 2023). This technique emulates the commonly accepted human evaluation framework - Multidimensional Quality Metrics (MQM, Freitag et al. (2021)) and \\textit{produces explainable and reliable MT evaluations at both the system and segment level}. Experimental Results from the WMT22 metrics shared task validate the effectiveness of EAPrompt on various LLMs, with different structures. Further analysis confirms that EAPrompt effectively distinguishes major errors from minor ones, while also sharing a similar distribution of the number of errors with MQM. These findings highlight the potential of EAPrompt as a human-like evaluator prompting technique for MT evaluation.",
            "year": 2023,
            "citationCount": 58,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Findings highlight the potential of EAPrompt as a human-like evaluator prompting technique for MT evaluation, and investigate several prompting designs, and propose a new prompting method called EAPrompt by combining Chain-of-Thoughts and Error Analysis."
            },
            "score": 4
        },
        {
            "id": "dfab0f3ee6f47e36cccee145794cd117773e6f73",
            "paperId": "dfab0f3ee6f47e36cccee145794cd117773e6f73",
            "title": "Towards LLM-based Fact Verification on News Claims with a Hierarchical Step-by-Step Prompting Method",
            "abstract": "While large pre-trained language models (LLMs) have shown their impressive capabilities in various NLP tasks, they are still under-explored in the misinformation domain. In this paper, we examine LLMs with in-context learning (ICL) for news claim verification, and find that only with 4-shot demonstration examples, the performance of several prompting methods can be comparable with previous supervised models. To further boost performance, we introduce a Hierarchical Step-by-Step (HiSS) prompting method which directs LLMs to separate a claim into several subclaims and then verify each of them via multiple questions-answering steps progressively. Experiment results on two public misinformation datasets show that HiSS prompting outperforms state-of-the-art fully-supervised approach and strong few-shot ICL-enabled baselines.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Hierarchical Step-by-Step (HiSS) prompting method is introduced which directs LLMs to separate a claim into several subclaims and then verify each of them via multiple questions-answering steps progressively."
            },
            "score": 4
        },
        {
            "id": "9ffefdf1fcd780cb71450b0a7a29247c66aa87be",
            "paperId": "9ffefdf1fcd780cb71450b0a7a29247c66aa87be",
            "title": "The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning",
            "abstract": "Does prompting a large language model (LLM) like GPT-3 with explanations improve in-context learning? We study this question on two NLP tasks that involve reasoning over text, namely question answering and natural language inference. We test the performance of four LLMs on three textual reasoning datasets using prompts that include explanations in multiple different styles. For these tasks, we find that including explanations in the prompts for OPT, GPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to moderate accuracy improvements over standard few-show learning. However, text-davinci-002 is able to benefit more substantially. We further show that explanations generated by the LLMs may not entail the models' predictions nor be factually grounded in the input, even on simple tasks with extractive explanations. However, these flawed explanations can still be useful as a way to verify LLMs' predictions post-hoc. Through analysis in our three settings, we show that explanations judged by humans to be good--logically consistent with the input and the prediction--more likely cooccur with accurate predictions. Following these observations, we train calibrators using automatically extracted scores that assess the reliability of explanations, allowing us to improve performance post-hoc across all of our datasets.",
            "year": 2022,
            "citationCount": 95,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work studies two NLP tasks that involve reasoning over text, namely question answering and natural language inference, and shows that explanations judged by humans to be good--logically consistent with the input and the prediction--more likely cooccur with accurate predictions."
            },
            "score": 4
        },
        {
            "id": "d5a6fc6aa139066e3b66ba63002e7d84c109aebc",
            "paperId": "d5a6fc6aa139066e3b66ba63002e7d84c109aebc",
            "title": "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing",
            "abstract": "Large language models (LLMs) have shown remarkable capabilities in Natural Language Processing (NLP), especially in domains where labeled data is scarce or expensive, such as clinical domain. However, to unlock the clinical knowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform specific clinical NLP tasks without any task-specific training data. This is known as in-context learning, which is an art and science that requires understanding the strengths and weaknesses of different LLMs and prompt engineering approaches. In this paper, we present a comprehensive and systematic experimental study on prompt engineering for five clinical NLP tasks: Clinical Sense Disambiguation, Biomedical Evidence Extraction, Coreference Resolution, Medication Status Extraction, and Medication Attribute Extraction. We assessed the prompts proposed in recent literature, including simple prefix, simple cloze, chain of thought, and anticipatory prompts, and introduced two new types of prompts, namely heuristic prompting and ensemble prompting. We evaluated the performance of these prompts on three state-of-the-art LLMs: GPT-3.5, BARD, and LLAMA2. We also contrasted zero-shot prompting with few-shot prompting, and provide novel insights and guidelines for prompt engineering for LLMs in clinical NLP. To the best of our knowledge, this is one of the first works on the empirical evaluation of different prompt engineering approaches for clinical NLP in this era of generative AI, and we hope that it will inspire and inform future research in this area.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents a comprehensive and systematic experimental study on prompt engineering for five clinical NLP tasks: Clinical Sense Disambiguation, Biomedical Evidence Extraction, Coreference Resolution, Medication Status Ext extraction, and Medication Attribute Extraction."
            },
            "score": 4
        },
        {
            "id": "0abcd69d87723b93bfc2074167418a4cf81a892e",
            "paperId": "0abcd69d87723b93bfc2074167418a4cf81a892e",
            "title": "Logically Consistent Adversarial Attacks for Soft Theorem Provers",
            "abstract": "Recent efforts within the AI community have yielded impressive results towards \u201csoft theorem proving\u201d over natural language sentences using language models. We propose a novel, generative adversarial framework for probing and improving these models\u2019 reasoning capabilities. Adversarial attacks in this domain suffer from the logical inconsistency problem, whereby perturbations to the input may alter the label. Our Logically consistent AdVersarial Attacker, LAVA, addresses this by combining a structured generative process with a symbolic solver, guaranteeing logical consistency. Our framework successfully generates adversarial attacks and identifies global weaknesses common across multiple target models. Our analyses reveal naive heuristics and vulnerabilities in these models\u2019 reasoning capabilities, exposing an incomplete grasp of logical deduction under logic programs. Finally, in addition to effective probing of these models, we show that training on the generated samples improves the target model\u2019s performance.",
            "year": 2022,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel, generative adversarial framework for probing and improving these models\u2019 reasoning capabilities, and shows that training on the generated samples improves the target model\u2019s performance."
            },
            "score": 3
        },
        {
            "id": "2f33488d1e4d405f84b1f15021c0870aec8bf680",
            "paperId": "2f33488d1e4d405f84b1f15021c0870aec8bf680",
            "title": "Exploring the pitfalls of large language models: Inconsistency and inaccuracy in answering pathology board examination\u2010style questions",
            "abstract": "In the rapidly advancing field of artificial intelligence, large language models (LLMs) such as ChatGPT and Google Bard are making significant progress, with applications extending across various fields, including medicine. This study explores their potential utility and pitfalls by assessing the performance of these LLMs in answering 150 multiple-choice questions sourced from the PathologyOutlines.com Question Bank, a well-established resource for pathology examination preparation. The assessment, encompassing 15 subspecialties in pathology, evaluated the accuracy and consistency of responses by these LLMs. Overall, ChatGPT outperformed Google Bard, scoring 122 out of 150, while Google Bard achieved a score of 70. In addition to accuracy, we explored the consistency of these LLMs by applying a test-retest approach over a two-week interval. ChatGPT showed a consistency rate of 85%, while Google Bard exhibited a lower consistency rate of 61%. In-depth analysis of incorrect responses identified potential factual inaccuracies and interpretive errors, underscoring the need for ongoing model refinement and human oversight. In conclusion, while LLMs have potential to enhance medical education and assist clinical decision-making, their current limitations underscore the need for continued development and the critical role of human expertise in the application of such models.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "While LLMs have potential to enhance medical education and assist clinical decision-making, their current limitations underscore the need for continued development and the critical role of human expertise in the application of such models."
            },
            "score": 3
        },
        {
            "id": "cb754310302086dfbbcd098263200e2a03f65874",
            "paperId": "cb754310302086dfbbcd098263200e2a03f65874",
            "title": "Membership Inference Attacks against Language Models via Neighbourhood Comparison",
            "abstract": "Membership Inference attacks (MIAs) aim to predict whether a data sample was present in the training data of a machine learning model or not, and are widely used for assessing the privacy risks of language models. Most existing attacks rely on the observation that models tend to assign higher probabilities to their training samples than non-training points. However, simple thresholding of the model score in isolation tends to lead to high false-positive rates as it does not account for the intrinsic complexity of a sample. Recent work has demonstrated that reference-based attacks which compare model scores to those obtained from a reference model trained on similar data can substantially improve the performance of MIAs. However, in order to train reference models, attacks of this kind make the strong and arguably unrealistic assumption that an adversary has access to samples closely resembling the original training data. Therefore, we investigate their performance in more realistic scenarios and find that they are highly fragile in relation to the data distribution used to train reference models. To investigate whether this fragility provides a layer of safety, we propose and evaluate neighbourhood attacks, which compare model scores for a given sample to scores of synthetically generated neighbour texts and therefore eliminate the need for access to the training data distribution. We show that, in addition to being competitive with reference-based attacks that have perfect knowledge about the training data distribution, our attack clearly outperforms existing reference-free attacks as well as reference-based attacks with imperfect knowledge, which demonstrates the need for a reevaluation of the threat model of adversarial attacks.",
            "year": 2023,
            "citationCount": 43,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Neighbourhood attacks are proposed and evaluated, which compare model scores for a given sample to scores of synthetically generated neighbour texts and therefore eliminate the need for access to the training data distribution and clearly outperforms existing reference-free attacks as well as reference-based attacks with imperfect knowledge."
            },
            "score": 3
        },
        {
            "id": "5d896fb2f0da16060f22ed43e582464605237f28",
            "paperId": "5d896fb2f0da16060f22ed43e582464605237f28",
            "title": "Training-free Lexical Backdoor Attacks on Language Models",
            "abstract": "Large-scale language models have achieved tremendous success across various natural language processing (NLP) applications. Nevertheless, language models are vulnerable to backdoor attacks, which inject stealthy triggers into models for steering them to undesirable behaviors. Most existing backdoor attacks, such as data poisoning, require further (re)training or fine-tuning language models to learn the intended backdoor patterns. The additional training process however diminishes the stealthiness of the attacks, as training a language model usually requires long optimization time, a massive amount of data, and considerable modifications to the model parameters. In this work, we propose Training-Free Lexical Backdoor Attack (TFLexAttack) as the first training-free backdoor attack on language models. Our attack is achieved by injecting lexical triggers into the tokenizer of a language model via manipulating its embedding dictionary using carefully designed rules. These rules are explainable to human developers which inspires attacks from a wider range of hackers. The sparse manipulation of the dictionary also habilitates the stealthiness of our attack. We conduct extensive experiments on three dominant NLP tasks based on nine language models to demonstrate the effectiveness and universality of our attack. The code of this work is available at https://github.com/Jinxhy/TFLexAttack.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes Training-Free Lexical Backdoor Attack (TFLexAttack) as the first training-free backdoor attack on language models by injecting lexical triggers into the tokenizer of a language model via manipulating its embedding dictionary using carefully designed rules."
            },
            "score": 3
        },
        {
            "id": "d55ed10e6a77e8f0a2359eb92221915f56481843",
            "paperId": "d55ed10e6a77e8f0a2359eb92221915f56481843",
            "title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
            "abstract": "Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 15 LLMs of four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators, such as System Star is better than System Square. We then evaluate the quality of ranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the Egocentric bias where a model prefers to rank its own outputs highly in evaluation. We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (average of 40% of comparisons across all models) within each of their evaluations that question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the average Rank-Biased Overlap (RBO) score to be 49.6%, indicating that machine preferences are misaligned with humans. According to our findings, LLMs may still be unable to be utilized for automatic annotation aligned with human preferences. Our project page is at: https://minnesotanlp.github.io/cobbler.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work assembles 15 LLMs of four different size ranges and finds that LLMs are biased text quality evaluators, exhibiting strong indications on the authors' bias benchmark within each of their evaluations that question their robustness as evaluator."
            },
            "score": 3
        },
        {
            "id": "f50c4075682d8e394d2b0ad5973d6a2020062dec",
            "paperId": "f50c4075682d8e394d2b0ad5973d6a2020062dec",
            "title": "RoCoIns: Enhancing Robustness of Large Language Models through Code-Style Instructions",
            "abstract": "Large Language Models (LLMs) have showcased remarkable capabilities in following human instructions. However, recent studies have raised concerns about the robustness of LLMs when prompted with instructions combining textual adversarial samples. In this paper, drawing inspiration from recent works that LLMs are sensitive to the design of the instructions, we utilize instructions in code style, which are more structural and less ambiguous, to replace typically natural language instructions. Through this conversion, we provide LLMs with more precise instructions and strengthen the robustness of LLMs. Moreover, under few-shot scenarios, we propose a novel method to compose in-context demonstrations using both clean and adversarial samples (\\textit{adversarial context method}) to further boost the robustness of the LLMs. Experiments on eight robustness datasets show that our method consistently outperforms prompting LLMs with natural language instructions. For example, with gpt-3.5-turbo, our method achieves an improvement of 5.68\\% in test set accuracy and a reduction of 5.66 points in Attack Success Rate (ASR).",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper utilizes instructions in code style, which are more structural and less ambiguous, to replace typically natural language instructions to provide LLMs with more precise instructions and strengthen the robustness of LLMs."
            },
            "score": 3
        },
        {
            "id": "c782bd29db2fd79d5511ccdb78ba907accb348d1",
            "paperId": "c782bd29db2fd79d5511ccdb78ba907accb348d1",
            "title": "Understanding the Effectiveness of Large Language Models in Detecting Security Vulnerabilities",
            "abstract": "Security vulnerabilities in modern software are prevalent and harmful. While automated vulnerability detection tools have made promising progress, their scalability and applicability remain challenging. Recently, Large Language Models (LLMs), such as GPT-4 and CodeLlama, have demonstrated remarkable performance on code-related tasks. However, it is unknown whether such LLMs can do complex reasoning over code. In this work, we explore whether pre-trained LLMs can detect security vulnerabilities and address the limitations of existing tools. We evaluate the effectiveness of pre-trained LLMs on a set of five diverse security benchmarks spanning two languages, Java and C/C++, and including code samples from synthetic and real-world projects. We evaluate the effectiveness of LLMs in terms of their performance, explainability, and robustness. By designing a series of effective prompting strategies, we obtain the best results on the synthetic datasets with GPT-4: F1 scores of 0.79 on OWASP, 0.86 on Juliet Java, and 0.89 on Juliet C/C++. Expectedly, the performance of LLMs drops on the more challenging real-world datasets: CVEFixes Java and CVEFixes C/C++, with GPT-4 reporting F1 scores of 0.48 and 0.62, respectively. We show that LLMs can often perform better than existing static analysis and deep learning-based vulnerability detection tools, especially for certain classes of vulnerabilities. Moreover, LLMs also often provide reliable explanations, identifying the vulnerable data flows in code. We find that fine-tuning smaller LLMs can outperform the larger LLMs on synthetic datasets but provide limited gains on real-world datasets. When subjected to adversarial attacks on code, LLMs show mild degradation, with average accuracy reduction of up to 12.67%. Finally, we share our insights and recommendations for future work on leveraging LLMs for vulnerability detection.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that fine-tuning smaller LLMs can outperform the larger LLMs on synthetic datasets but provide limited gains on real-world datasets, as well as designing a series of effective prompting strategies that obtain the best results on the synthetic datasets with GPT-4."
            },
            "score": 3
        },
        {
            "id": "b73a4e21c894c7d1f8eabbc7d5b4e9b4e86f2e08",
            "paperId": "b73a4e21c894c7d1f8eabbc7d5b4e9b4e86f2e08",
            "title": "Towards Robust Prompts on Vision-Language Models",
            "abstract": "With the advent of vision-language models (VLMs) that can perform in-context and prompt-based learning, how can we design prompting approaches that robustly generalize to distribution shift and can be used on novel classes outside the support set of the prompts? In this work, we first define two types of robustness to distribution shift on VLMs, namely, robustness on base classes (the classes included in the support set of prompts) and robustness on novel classes. Then, we study the robustness of existing in-context learning and prompt learning approaches, where we find that prompt learning performs robustly on test images from base classes, while it does not generalize well on images from novel classes. We propose robust prompt learning by integrating multiple-scale image features into the prompt, which improves both types of robustness. Comprehensive experiments are conducted to study the defined robustness on six benchmarks and show the effectiveness of our proposal.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes robust prompt learning by integrating multiple-scale image features into the prompt, which improves both types of robustness of existing in-context and prompt learning approaches."
            },
            "score": 3
        },
        {
            "id": "a21de70160c91dcf9b1e7a93fbb32f4b2687860a",
            "paperId": "a21de70160c91dcf9b1e7a93fbb32f4b2687860a",
            "title": "Small Language Models Improve Giants by Rewriting Their Outputs",
            "abstract": "Despite the impressive performance of large language models (LLMs), theyoften lag behind specialized models in various tasks. LLMs only use a fractionof the existing training data for in-context learning, while task-specificmodels harness the full dataset for fine-tuning. In this work, we tackle theproblem of leveraging training data to improve the performance of LLMs withoutfine-tuning. Our approach directly targets LLM predictions without requiringaccess to their weights. We create a pool of candidates from the LLM throughfew-shot prompting and we employ a compact model, the LM-corrector (LMCor),specifically trained to merge these candidates to produce an enhanced output.Our experiments on four natural language generation tasks demonstrate that evena small LMCor model (250M) substantially improves the few-shot performance ofLLMs (62B), matching and even outperforming standard fine-tuning. Furthermore,we illustrate the robustness of LMCor against different prompts, therebyminimizing the need for extensive prompt engineering. Finally, we show thatLMCor can be seamlessly integrated with different LLMs at inference, serving asa plug-and-play module to improve their performance.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work addresses the problem of leveraging training data to improve the performance of large language models without fine-tuning, and creates a compact model, the LM-corrector (LMCor), specifically trained to merge candidates to produce an enhanced output."
            },
            "score": 3
        },
        {
            "id": "7e8453fa32708b2690178c3a7a6618aba20928af",
            "paperId": "7e8453fa32708b2690178c3a7a6618aba20928af",
            "title": "Training Socially Aligned Language Models in Simulated Human Society",
            "abstract": "Social alignment in AI systems aims to ensure that these models behave according to established societal values. However, unlike humans, who derive consensus on value judgments through social interaction, current language models (",
            "year": 2023,
            "citationCount": 74,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Social alignment in AI systems aims to ensure that these models behave according to established societal values, but unlike humans, who derive consensus on value judgments through social interaction, current language models lack consensus on value judgments through social interaction."
            },
            "score": 3
        },
        {
            "id": "f29f8b8aa2b7e608199b65d3cf751969d4024132",
            "paperId": "f29f8b8aa2b7e608199b65d3cf751969d4024132",
            "title": "Physics of Language Models: Part 3.1, Knowledge Storage and Extraction",
            "abstract": "Large language models (LLMs) can store a vast amount of world knowledge, often extractable via question-answering (e.g.,\"What is Abraham Lincoln's birthday?\"). However, do they answer such questions based on exposure to similar questions during training (i.e., cheating), or by genuinely learning to extract knowledge from sources like Wikipedia? In this paper, we investigate this issue using a controlled biography dataset. We find a strong correlation between the model's ability to extract knowledge and various diversity measures of the training data. $\\textbf{Essentially}$, for knowledge to be reliably extracted, it must be sufficiently augmented (e.g., through paraphrasing, sentence shuffling) $\\textit{during pretraining}$. Without such augmentation, knowledge may be memorized but not extractable, leading to 0% accuracy, regardless of subsequent instruction fine-tuning. To understand why this occurs, we employ (nearly) linear probing to demonstrate a strong connection between the observed correlation and how the model internally encodes knowledge -- whether it is linearly encoded in the hidden embeddings of entity names or distributed across other token embeddings in the training text. This paper provides $\\textbf{several key recommendations for LLM pretraining in the industry}$: (1) rewrite the pretraining data -- using small, auxiliary models -- to provide knowledge augmentation, and (2) incorporate more instruction-finetuning data into the pretraining stage before it becomes too late.",
            "year": 2023,
            "citationCount": 21,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Several key recommendations for LLM pretraining in the industry are provided: rewrite the pretraining data -- using small, auxiliary models -- to provide knowledge augmentation, and incorporate more instruction-finetuning data into the pretraining stage before it becomes too late."
            },
            "score": 3
        },
        {
            "id": "e468ed6b824e60f45ba9a20b034e4090c6630751",
            "paperId": "e468ed6b824e60f45ba9a20b034e4090c6630751",
            "title": "Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources",
            "abstract": "We present chain-of-knowledge (CoK), a novel framework that augments large language models (LLMs) by dynamically incorporating grounding information from heterogeneous sources. It results in more factual rationales and reduced hallucination in generation. Specifically, CoK consists of three stages: reasoning preparation, dynamic knowledge adapting, and answer consolidation. Given a knowledge-intensive question, CoK first prepares several preliminary rationales and answers while identifying the relevant knowledge domains. If there is no majority consensus among the answers from samples, CoK corrects the rationales step by step by adapting knowledge from the identified domains. These corrected rationales can plausibly serve as a better foundation for the final answer consolidation. Unlike prior studies that primarily use unstructured data, CoK also leverages structured knowledge sources such as Wikidata and tables that provide more reliable factual information. To access both unstructured and structured knowledge sources in the dynamic knowledge adapting stage, we propose an adaptive query generator that allows the generation of queries for various types of query languages, including SPARQL, SQL, and natural sentences. Moreover, to minimize error propagation between rationales, CoK corrects the rationales progressively using preceding corrected rationales to generate and correct subsequent rationales. Extensive experiments show that CoK consistently improves the performance of LLMs on knowledge-intensive tasks across different domains.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Chain-of-knowledge (CoK), a novel framework that augments large language models (LLMs) by dynamically incorporating grounding information from heterogeneous sources, results in more factual rationales and reduced hallucination in generation."
            },
            "score": 3
        },
        {
            "id": "44d16a076c00ecada3d425203377e4ec951c4ed0",
            "paperId": "44d16a076c00ecada3d425203377e4ec951c4ed0",
            "title": "MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning",
            "abstract": "Large language models (LLMs), despite their remarkable progress across various general domains, encounter significant barriers in medicine and healthcare. This field faces unique challenges such as domain-specific terminologies and reasoning over specialized knowledge. To address these issues, we propose a novel Multi-disciplinary Collaboration (MC) framework for the medical domain that leverages LLM-based agents in a role-playing setting that participate in a collaborative multi-round discussion, thereby enhancing LLM proficiency and reasoning capabilities. This training-free framework encompasses five critical steps: gathering domain experts, proposing individual analyses, summarising these analyses into a report, iterating over discussions until a consensus is reached, and ultimately making a decision. Our work focuses on the zero-shot setting, which is applicable in real-world scenarios. Experimental results on nine datasets (MedQA, MedMCQA, PubMedQA, and six subtasks from MMLU) establish that our proposed MC framework excels at mining and harnessing the medical expertise within LLMs, as well as extending its reasoning abilities. Our code can be found at \\url{https://github.com/gersteinlab/MedAgents}.",
            "year": 2023,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel Multi-disciplinary Collaboration (MC) framework for the medical domain that leverages LLM-based agents in a role-playing setting that participate in a collaborative multi-round discussion, thereby enhancing LLM proficiency and reasoning capabilities."
            },
            "score": 3
        },
        {
            "id": "c16c05ca0a3d24519405849fd24604fc1ce47751",
            "paperId": "c16c05ca0a3d24519405849fd24604fc1ce47751",
            "title": "Towards Best Practices of Activation Patching in Language Models: Metrics and Methods",
            "abstract": "Mechanistic interpretability seeks to understand the internal mechanisms of machine learning models, where localization -- identifying the important model components -- is a key step. Activation patching, also known as causal tracing or interchange intervention, is a standard technique for this task (Vig et al., 2020), but the literature contains many variants with little consensus on the choice of hyperparameters or methodology. In this work, we systematically examine the impact of methodological details in activation patching, including evaluation metrics and corruption methods. In several settings of localization and circuit discovery in language models, we find that varying these hyperparameters could lead to disparate interpretability results. Backed by empirical observations, we give conceptual arguments for why certain metrics or methods may be preferred. Finally, we provide recommendations for the best practices of activation patching going forwards.",
            "year": 2023,
            "citationCount": 20,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "In several settings of localization and circuit discovery in language models, it is found that varying these hyperparameters could lead to disparate interpretability results."
            },
            "score": 3
        },
        {
            "id": "ca0dcc09e69732b7fc582d8a610488f86ad07e36",
            "paperId": "ca0dcc09e69732b7fc582d8a610488f86ad07e36",
            "title": "In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning",
            "abstract": "The performance of Large Language Models (LLMs) on downstream tasks often improves significantly when including examples of the input\u2013label relationship in the context. However, there is currently no consensus about how this in-context learning (ICL) ability of LLMs works: for example, while Xie et al. (2022) liken ICL to a general-purpose learning algorithm, Min et al. (2022b) argue ICL does not even learn label relationships from in-context examples. In this paper, we study (1) how labels of in-context examples affect predictions, (2) how label relationships learned during pre-training interact with input\u2013label examples provided in-context, and (3) how ICL aggregates label information across in-context examples. Our findings suggests LLMs usually incorporate information from in-context labels, but that pre-training and in-context label relationships are treated differently, and that the model does not consider all in-context information equally. Our results give insights into understanding and aligning LLM behavior.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "How labels of in-context examples affect predictions, how label relationships learned during pre-training interact with input\u2013label examples provided in- context, and how ICL aggregates label information across in- Context examples are studied."
            },
            "score": 3
        },
        {
            "id": "13fd4277388cc2a9da75e8b772e5efcf6ebe2d32",
            "paperId": "13fd4277388cc2a9da75e8b772e5efcf6ebe2d32",
            "title": "Training Socially Aligned Language Models on Simulated Social Interactions",
            "abstract": "Social alignment in AI systems aims to ensure that these models behave according to established societal values. However, unlike humans, who derive consensus on value judgments through social interaction, current language models (LMs) are trained to rigidly replicate their training corpus in isolation, leading to subpar generalization in unfamiliar scenarios and vulnerability to adversarial attacks. This work presents a novel training paradigm that permits LMs to learn from simulated social interactions. In comparison to existing methodologies, our approach is considerably more scalable and efficient, demonstrating superior performance in alignment benchmarks and human evaluations. This paradigm shift in the training of LMs brings us a step closer to developing AI systems that can robustly and accurately reflect societal norms and values.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents a novel training paradigm that permits LMs to learn from simulated social interactions, and is considerably more scalable and efficient, demonstrating superior performance in alignment benchmarks and human evaluations."
            },
            "score": 3
        },
        {
            "id": "8b1e3d09b12e0f324c48114eea71564f51c62dba",
            "paperId": "8b1e3d09b12e0f324c48114eea71564f51c62dba",
            "title": "FeatureMix: A General Adversarial Defense Method for Pretrained Language Models",
            "abstract": "Pretrained language models (PLMs) that are trained over large-scale data and then finetuned on downstream tasks have achieved great success. However, they are vulnerable to adversarial attacks. Adversarial training with both clean and adversarial data is a widely-used technique to improve model robustness. In this paper, we propose FeatureMix, a straightforward yet effective adversarial defense strategy for PLMs by finetuning on both discrete adversarial examples and online virtual examples. During finetuning, we augment clean data with discrete attacks first and generate virtual examples in each finetuning epoch by randomly mixing local latent features in the hidden layers of augmented data pairs. The virtual examples serve as additional training signals, regularizing the PLMs to favor mixing of latent features between discrete augmented examples and thus enhance adversarial robustness. The experimental evaluation results show that FeatureMix outperforms prevailing baseline methods in terms of robustness against adversarial attacks, without significantly reducing generalization performance.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "FeatureMix is proposed, a straightforward yet effective adversarial defense strategy for PLMs by finetuning on both discrete adversarial examples and online virtual examples that outperforms prevailing baseline methods in terms of robustness against adversarial attacks, without significantly reducing generalization performance."
            },
            "score": 3
        },
        {
            "id": "3ee20a72e6008a125135e3f17c5bbdb8cbe9bd8d",
            "paperId": "3ee20a72e6008a125135e3f17c5bbdb8cbe9bd8d",
            "title": "Impact of Adversarial Training on Robustness and Generalizability of Language Models",
            "abstract": "Adversarial training is widely acknowledged as the most effective defense against adversarial attacks. However, it is also well established that achieving both robustness and generalization in adversarially trained models involves a trade-off. The goal of this work is to provide an in depth comparison of different approaches for adversarial training in language models. Specifically, we study the effect of pre-training data augmentation as well as training time input perturbations vs. embedding space perturbations on the robustness and generalization of transformer-based language models. Our findings suggest that better robustness can be achieved by pre-training data augmentation or by training with input space perturbation. However, training with embedding space perturbation significantly improves generalization. A linguistic correlation analysis of neurons of the learned models reveals that the improved generalization is due to 'more specialized' neurons. To the best of our knowledge, this is the first work to carry out a deep qualitative analysis of different methods of generating adversarial examples in adversarial training of language models.",
            "year": 2022,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This is the first work to carry out a deep qualitative analysis of different methods of generating adversarial examples in adversarial training of language models and suggests that better robustness can be achieved by pre-training data augmentation or by training with input space perturbation."
            },
            "score": 3
        },
        {
            "id": "e45ea633280f799dff22c09ca3dfe9c7fc88c0a2",
            "paperId": "e45ea633280f799dff22c09ca3dfe9c7fc88c0a2",
            "title": "Quantifying the Performance of Adversarial Training on Language Models with Distribution Shifts",
            "abstract": "Adversarial training has recently emerged as an important defense mechanism to robustify machine learning models in the presence adversarial examples. Although adversarial training can boost the robustness of machine learning algorithms by a margin, research has not been conducted to determine if adversarial training is effective in the long-term. As deployments of machine learning algorithms are characterized by dynamics, change of the underlying model is inevitable. The dynamics are a result of model's evolution over time by introducing new training data and drifting the model by changing its parameters. In this paper, we examine the limitations of adversarial training due to the temporal changes of machine learning models. Using a natural language task, we conduct various experiments using a variety of datasets to measure the impact of concept drift on the efficacy of adversarial training. In particular, our analysis shows that certain adversarially-trained models are even more prone to the drift than others. In particular, WordCNN and LSTM-based models are shown more susceptible to the temporal changes than others such as BERT. We validate our findings using multiple real-world datasets on different network architectures. Our work calls for further research into the temporal aspects of adversarial training.",
            "year": 2022,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper examines the limitations of adversarial training due to the temporal changes of machine learning models using a natural language task and shows that certain adversarially-trained models are even more prone to the drift than others."
            },
            "score": 3
        },
        {
            "id": "877bd47e214b858933a8e4cb53c5d74a1b095fc4",
            "paperId": "877bd47e214b858933a8e4cb53c5d74a1b095fc4",
            "title": "Textual Manifold-based Defense Against Natural Language Adversarial Examples",
            "abstract": "Despite the recent success of large pretrained language models in NLP, they are susceptible to adversarial examples. Concurrently, several studies on adversarial images have observed an intriguing property: the adversarial images tend to leave the low-dimensional natural data manifold. In this study, we find a similar phenomenon occurs in the contextualized embedding space of natural sentences induced by pretrained language models in which textual adversarial examples tend to have their embeddings diverge off the manifold of natural sentence embeddings. Based on this finding, we propose Textual Manifold-based Defense (TMD), a defense mechanism that learns the embedding space manifold of the underlying language model and projects novel inputs back to the approximated structure before classification. Through extensive experiments, we find that our method consistently and significantly outperforms previous defenses under various attack settings while remaining unaffected to the clean accuracy. To the best of our knowledge, this is the first kind of manifold-based defense adapted to the NLP domain.",
            "year": 2022,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes Textual Manifold-based Defense (TMD), a defense mechanism that learns the embedding space manifold of the underlying language model and projects novel inputs back to the approximated structure before classification, which is the first kind of manifold-based defense adapted to the NLP domain."
            },
            "score": 3
        },
        {
            "id": "3a391dfd536625e068f3888c817cc6cbe7fcea9c",
            "paperId": "3a391dfd536625e068f3888c817cc6cbe7fcea9c",
            "title": "One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models",
            "abstract": "Large pre-trained Vision-Language Models (VLMs) like CLIP, despite having remarkable generalization ability, are highly vulnerable to adversarial examples. This work studies the adversarial robustness of VLMs from the novel perspective of the text prompt instead of the extensively studied model weights (frozen in this work). We first show that the effectiveness of both adversarial attack and defense are sensitive to the used text prompt. Inspired by this, we propose a method to improve resilience to adversarial attacks by learning a robust text prompt for VLMs. The proposed method, named Adversarial Prompt Tuning (APT), is effective while being both computationally and data efficient. Extensive experiments are conducted across 15 datasets and 4 data sparsity schemes (from 1-shot to full training data settings) to show APT's superiority over hand-engineered prompts and other state-of-the-art adaption methods. APT demonstrated excellent abilities in terms of the in-distribution performance and the generalization under input distribution shift and across datasets. Surprisingly, by simply adding one learned word to the prompts, APT can significantly boost the accuracy and robustness (epsilon=4/255) over the hand-engineered prompts by +13% and +8.5% on average respectively. The improvement further increases, in our most effective setting, to +26.4% for accuracy and +16.7% for robustness. Code is available at https://github.com/TreeLLi/APT.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work studies the adversarial robustness of VLMs from the novel perspective of the text prompt instead of the extensively studied model weights, and proposes a method to improve resilience to adversarial attacks by learning a robust text prompt for VLMs."
            },
            "score": 3
        },
        {
            "id": "e10c2ed8de5568e6a41fb60c2adc4bf9c225da25",
            "paperId": "e10c2ed8de5568e6a41fb60c2adc4bf9c225da25",
            "title": "Is EVALITA Done? On the Impact of Prompting on the Italian NLP Evaluation Campaign",
            "abstract": "Prompt-based learning is a recent paradigm in NLP that leverages large pre-trained language models to perform a variety of tasks. With this technique, it is possible to build classifiers that do not need training data (zero-shot). In this paper, we assess the status of prompt-based learning applied to several text classification tasks in the Italian language. The results indicate that the performance gap towards current supervised methods is still relevant. However, the difference in performance between pre-trained models and the characteristic of the prompt-based classifier of operating in a zero-shot fashion open a discussion regarding the next generation of evaluation campaigns for NLP.",
            "year": 2022,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results indicate that the performance gap towards current supervised methods is still relevant, however, the difference in performance between pre-trained models and the characteristic of the prompt-based classifier of operating in a zero-shot fashion open a discussion regarding the next generation of evaluation campaigns for NLP."
            },
            "score": 3
        },
        {
            "id": "c879413103f8950bdd414c7f60a39bd7748c9be8",
            "paperId": "c879413103f8950bdd414c7f60a39bd7748c9be8",
            "title": "Prompting Large Language Model for Machine Translation: A Case Study",
            "abstract": "Research on prompting has shown excellent performance with little or even no supervised training across many tasks. However, prompting for machine translation is still under-explored in the literature. We fill this gap by offering a systematic study on prompting strategies for translation, examining various factors for prompt template and demonstration example selection. We further explore the use of monolingual data and the feasibility of cross-lingual, cross-domain, and sentence-to-document transfer learning in prompting. Extensive experiments with GLM-130B (Zeng et al., 2022) as the testbed show that 1) the number and the quality of prompt examples matter, where using suboptimal examples degenerates translation; 2) several features of prompt examples, such as semantic similarity, show significant Spearman correlation with their prompting performance; yet, none of the correlations are strong enough; 3) using pseudo parallel prompt examples constructed from monolingual data via zero-shot prompting could improve translation; and 4) improved performance is achievable by transferring knowledge from prompt examples selected in other settings. We finally provide an analysis on the model outputs and discuss several problems that prompting still suffers from.",
            "year": 2023,
            "citationCount": 105,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A systematic study on prompting strategies for translation, examining various factors for prompt template and demonstration example selection, and exploring the use of monolingual data and the feasibility of cross-lingual, cross-domain, and sentence-to-document transfer learning in prompting."
            },
            "score": 3
        },
        {
            "id": "80c698688bb4488beaceaab5c64f701a946cb7ae",
            "paperId": "80c698688bb4488beaceaab5c64f701a946cb7ae",
            "title": "All in One: Multi-Task Prompting for Graph Neural Networks",
            "abstract": "Recently, \"pre-training and fine-tuning'' has been adopted as a standard workflow for many graph tasks since it can take general graph knowledge to relieve the lack of graph annotations from each application. However, graph tasks with node level, edge level, and graph level are far diversified, making the pre-training pretext often incompatible with these multiple tasks. This gap may even cause a \"negative transfer'' to the specific application, leading to poor results. Inspired by the prompt learning in natural language processing (NLP), which has presented significant effectiveness in leveraging prior knowledge for various NLP tasks, we study the prompting topic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models. Specifically, we first unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern. In this way, the prompting idea from NLP can be seamlessly introduced to the graph area. Then, to further narrow the gap between various graph tasks and state-of-the-art pre-training strategies, we further study the task space of various graph applications and reformulate downstream problems to the graph-level task. Afterward, we introduce meta-learning to efficiently learn a better initialization for the multi-task prompt of graphs so that our prompting framework can be more reliable and general for different tasks. We conduct extensive experiments, results from which demonstrate the superiority of our method.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel multi-task prompting method for graph models that unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern, and introduces meta-learning to efficiently learn a better initialization for the multi- task prompt of graphs so that the prompting framework can be more reliable and general for different tasks."
            },
            "score": 3
        },
        {
            "id": "9141480721653789597b6e537ee0eeab401f3e60",
            "paperId": "9141480721653789597b6e537ee0eeab401f3e60",
            "title": "PromptNER: Prompting For Named Entity Recognition",
            "abstract": "In a surprising turn, Large Language Models (LLMs) together with a growing arsenal of prompt-based heuristics now offer powerful off-the-shelf approaches providing few-shot solutions to myriad classic NLP problems. However, despite promising early results, these LLM-based few-shot methods remain far from the state of the art in Named Entity Recognition (NER), where prevailing methods include learning representations via end-to-end structural understanding and fine-tuning on standard labeled corpora. In this paper, we introduce PromptNER, a new state-of-the-art algorithm for few-Shot and cross-domain NER. To adapt to any new NER task PromptNER requires a set of entity definitions in addition to the standard few-shot examples. Given a sentence, PromptNER prompts an LLM to produce a list of potential entities along with corresponding explanations justifying their compatibility with the provided entity type definitions. Remarkably, PromptNER achieves state-of-the-art performance on few-shot NER, achieving a 4% (absolute) improvement in F1 score on the ConLL dataset, a 9% (absolute) improvement on the GENIA dataset, and a 4% (absolute) improvement on the FewNERD dataset. PromptNER also moves the state of the art on Cross Domain NER, outperforming prior methods (including those not limited to the few-shot setting), setting a new mark on 3/5 CrossNER target domains, with an average F1 gain of 3%, despite using less than 2% of the available data.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces PromptNER, a new state-of-the-art algorithm for few-Shot and cross-domain NER, and prompts an LLM to produce a list of potential entities along with corresponding explanations justifying their compatibility with the provided entity type definitions."
            },
            "score": 3
        },
        {
            "id": "197ba7bbfdbb052b0770088815c110774220f397",
            "paperId": "197ba7bbfdbb052b0770088815c110774220f397",
            "title": "Prompting PaLM for Translation: Assessing Strategies and Performance",
            "abstract": "Large language models (LLMs) that have been trained on multilingual but not parallel text exhibit a remarkable ability to translate between languages. We probe this ability in an in-depth study of the pathways language model (PaLM), which has demonstrated the strongest machine translation (MT) performance among similarly-trained LLMs to date. We investigate various strategies for choosing translation examples for few-shot prompting, concluding that example quality is the most important factor. Using optimized prompts, we revisit previous assessments of PaLM\u2019s MT capabilities with more recent test sets, modern MT metrics, and human evaluation, and find that its performance, while impressive, still lags that of state-of-the-art supervised systems. We conclude by providing an analysis of PaLM\u2019s MT output which reveals some interesting properties and prospects for future work.",
            "year": 2022,
            "citationCount": 93,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An in-depth study of the pathways language model (PaLM), which has demonstrated the strongest machine translation performance among similarly-trained LLMs to date, investigates various strategies for choosing translation examples for few-shot prompting, concluding that example quality is the most important factor."
            },
            "score": 3
        },
        {
            "id": "c7ae496ae37b5c1fca5c0f7e277bc9356172efd1",
            "paperId": "c7ae496ae37b5c1fca5c0f7e277bc9356172efd1",
            "title": "PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs",
            "abstract": "Large language models (LLMs) have shown great abilities of solving various natural language tasks in different domains. Due to the training objective of LLMs and their pre-training data, LLMs are not very well equipped for tasks involving structured data generation. We propose a framework, Prompting with Iterative Verification (PiVe), to improve graph-based generative capability of LLMs. We show how a small language model could be trained to act as a verifier module for the output of an LLM(i.e., ChatGPT, GPT-4), and to iteratively improve its performance via fine-grained corrective instructions. We also show how the verifier module could apply iterative corrections offline for a more cost-effective solution to the text-to-graph generation task. Experiments on three graph-based datasets show consistent improvement gained via PiVe. Additionally, we create GenWiki-HIQ and highlight that the verifier module can be used as a data augmentation tool to help improve the quality of automatically generated parallel text-graph datasets.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A framework, Prompting with Iterative Verification (PiVe), is proposed, to improve graph-based generative capability of LLMs and shows how a small language model could be trained to act as a verifier module for the output of an LLM, and to iteratively improve its performance via fine-grained corrective instructions."
            },
            "score": 3
        },
        {
            "id": "22c638d7b7b14de8f16425f85605961804f1c42c",
            "paperId": "22c638d7b7b14de8f16425f85605961804f1c42c",
            "title": "Adversarial Attacks on Protein Language Models",
            "abstract": "Deep Learning models for protein structure prediction, such as AlphaFold2, leverage Transformer architectures and their attention mechanism to capture structural and functional properties of amino acid sequences. Despite the high accuracy of predictions, biologically insignificant perturbations of the input sequences, or even single point mutations, can lead to substantially different 3d structures. On the other hand, protein language models are often insensitive to biologically relevant mutations that induce misfolding or dysfunction (e.g. missense mutations). Precisely, predictions of the 3d coordinates do not reveal the structure-disruptive effect of these mutations. Therefore, there is an evident inconsistency between the biological importance of mutations and the resulting change in structural prediction. Inspired by this problem, we introduce the concept of adversarial perturbation of protein sequences in continuous embedding spaces of protein language models. Our method relies on attention scores to detect the most vulnerable amino acid positions in the input sequences. Adversarial mutations are biologically diverse from their references and are able to significantly alter the resulting 3d structures.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces the concept of adversarial perturbation of protein sequences in continuous embedding spaces of protein language models and relies on attention scores to detect the most vulnerable amino acid positions in the input sequences."
            },
            "score": 2
        },
        {
            "id": "35342082e9d8cb68570c544d97b0a34a06cbc42c",
            "paperId": "35342082e9d8cb68570c544d97b0a34a06cbc42c",
            "title": "Optimizing Uncertainty Quantification of Vision Transformers in Optimizing Uncertainty Quantification of Vision Transformers in Deep Learning on Novel AI Architectures Deep Learning on Novel AI Architectures",
            "abstract": "Deep Learning (DL) methods have shown substantial efficacy in computer vision (CV) and natural language processing (NLP). Despite their proficiency, the inconsistency in input data distributions can compromise prediction reliability. This study mitigates this issue by introducing uncertainty evaluations in DL models, thereby enhancing dependability through a distribution of predictions. Our focus lies on the Vision Transformer (ViT), a DL model that har-monizes both local and global behavior. We conduct extensive experiments on the ImageNet-1K dataset, a vast resource with over a million images across 1,000 categories. ViTs, while competitive, are vulnerable to adversarial attacks, making uncertainty estimation crucial for robust predictions. Our research advances the field by integrating uncertainty evaluations into ViTs, comparing two significant uncertainty estimation methodologies, and expediting uncertainty computations on high-performance computing (HPC) architectures, such as the Cerebras CS-2, SambaNova DataScale, and the Polaris supercomputer, utilizing the MPI4PY package for efficient distributed training.",
            "year": null,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This research advances the field by integrating uncertainty evaluations into ViTs, comparing two significant uncertainty estimation methodologies, and expediting uncertainty computations on high-performance computing (HPC) architectures, such as the Cerebras CS-2, SambaNova DataScale, and the Polaris supercomputer."
            },
            "score": 2
        },
        {
            "id": "e4e744cc96da7987a072571fc3817f040d456566",
            "paperId": "e4e744cc96da7987a072571fc3817f040d456566",
            "title": "Large Language Models Know Your Contextual Search Intent: A Prompting Framework for Conversational Search",
            "abstract": "Precisely understanding users' contextual search intent has been an important challenge for conversational search. As conversational search sessions are much more diverse and long-tailed, existing methods trained on limited data still show unsatisfactory effectiveness and robustness to handle real conversational search scenarios. Recently, large language models (LLMs) have demonstrated amazing capabilities for text generation and conversation understanding. In this work, we present a simple yet effective prompting framework, called LLM4CS, to leverage LLMs as a text-based search intent interpreter to help conversational search. Under this framework, we explore three prompting methods to generate multiple query rewrites and hypothetical responses, and propose to aggregate them into an integrated representation that can robustly represent the user's real contextual search intent. Extensive automatic evaluations and human evaluations on three widely used conversational search benchmarks, including CAsT-19, CAsT-20, and CAsT-21, demonstrate the remarkable performance of our simple LLM4CS framework compared with existing methods and even using human rewrites. Our findings provide important evidence to better understand and leverage LLMs for conversational search.",
            "year": 2023,
            "citationCount": 26,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents a simple yet effective prompting framework, called LLM4CS, to leverage LLMs as a text-based search intent interpreter to help conversational search."
            },
            "score": 2
        },
        {
            "id": "ca261cb681b082e90ca6c7a9d325b4265ed1dc28",
            "paperId": "ca261cb681b082e90ca6c7a9d325b4265ed1dc28",
            "title": "MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models",
            "abstract": "Large language models (LLMs) have achieved remarkable performance in natural language understanding and generation tasks. However, they often suffer from limitations such as difficulty in incorporating new knowledge, generating hallucinations, and explaining their reasoning process. To address these challenges, we propose a novel prompting pipeline, named \\method, that leverages knowledge graphs (KGs) to enhance LLMs' inference and transparency. Our method enables LLMs to comprehend KG inputs and infer with a combination of implicit and external knowledge. Moreover, our method elicits the mind map of LLMs, which reveals their reasoning pathways based on the ontology of knowledge. We evaluate our method on diverse question \\&answering tasks, especially in medical domains, and show significant improvements over baselines. We also introduce a new hallucination evaluation benchmark and analyze the effects of different components of our method. Our results demonstrate the effectiveness and robustness of our method in merging knowledge from LLMs and KGs for combined inference. To reproduce our results and extend the framework further, we make our codebase available at https://github.com/wyl-willing/MindMap.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel prompting pipeline is proposed that leverages knowledge graphs (KGs) to enhance LLMs' inference and transparency and elicits the mind map of LLMs, which reveals their reasoning pathways based on the ontology of knowledge."
            },
            "score": 2
        },
        {
            "id": "d53945d4afb4528590d79e20de52883d29037e86",
            "paperId": "d53945d4afb4528590d79e20de52883d29037e86",
            "title": "FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo Embeddings",
            "abstract": "Logo embedding plays a crucial role in various e-commerce applications by facilitating image retrieval or recognition, such as intellectual property protection and product search. However, current methods treat logo embedding as a purely visual problem, which may limit their performance in real-world scenarios. A notable issue is that the textual knowledge embedded in logo images has not been adequately explored. Therefore, we propose a novel approach that leverages textual knowledge as an auxiliary to improve the robustness of logo embedding. The emerging Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in both visual and textual understanding and could become valuable visual assistants in understanding logo images. Inspired by this observation, our proposed method, FashionLOGO, aims to utilize MLLMs to enhance fashion logo embedding. We explore how MLLMs can improve logo embedding by prompting them to generate explicit textual knowledge through three types of prompts, including image OCR, brief captions, and detailed descriptions prompts, in a zero-shot setting. We adopt a cross-attention transformer to enable image embedding queries to learn supplementary knowledge from textual embeddings automatically. To reduce computational costs, we only use the image embedding model in the inference stage, similar to traditional inference pipelines. Our extensive experiments on three real-world datasets demonstrate that FashionLOGO learns generalized and robust logo embeddings, achieving state-of-the-art performance in all benchmark datasets. Furthermore, we conduct comprehensive ablation studies to demonstrate the performance improvements resulting from the introduction of MLLMs.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores how MLLMs can improve logo embedding by prompting them to generate explicit textual knowledge through three types of prompts, including image OCR, brief captions, and detailed descriptions prompts, in a zero-shot setting and adopt a cross-attention transformer to enable image embedding queries to learn supplementary knowledge from textual embeddings automatically."
            },
            "score": 2
        },
        {
            "id": "df788461949434a8fd7bac2952e98454e46c0825",
            "paperId": "df788461949434a8fd7bac2952e98454e46c0825",
            "title": "Multi-modal Attribute Prompting for Vision-Language Models",
            "abstract": "Large pre-trained Vision-Language Models (VLMs), like CLIP, exhibit strong generalization ability to downstream tasks but struggle in few-shot scenarios. Existing prompting techniques primarily focus on global text and image representations, yet overlooking multi-modal attribute characteristics. This limitation hinders the model's ability to perceive fine-grained visual details and restricts its generalization ability to a broader range of unseen classes. To address this issue, we propose a Multi-modal Attribute Prompting method (MAP) by jointly exploring textual attribute prompting, visual attribute prompting, and attribute-level alignment. The proposed MAP enjoys several merits. First, we introduce learnable visual attribute prompts enhanced by textual attribute semantics to adaptively capture visual attributes for images from unknown categories, boosting fine-grained visual perception capabilities for CLIP. Second, the proposed attribute-level alignment complements the global alignment to enhance the robustness of cross-modal alignment for open-vocabulary objects. To our knowledge, this is the first work to establish cross-modal attribute-level alignment for CLIP-based few-shot adaptation. Extensive experimental results on 11 datasets demonstrate that our method performs favorably against state-of-the-art approaches.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a Multi-modal Attribute Prompting method (MAP) by jointly exploring textual attribute prompting, visual attribute prompting, and attribute-level alignment to establish cross-modal attribute-level alignment for CLIP-based few-shot adaptation."
            },
            "score": 2
        },
        {
            "id": "86c7cb73e3a42a130c8d43ae3e26cb9c8df381a1",
            "paperId": "86c7cb73e3a42a130c8d43ae3e26cb9c8df381a1",
            "title": "Theory of Mind in Large Language Models: Examining Performance of 11 State-of-the-Art models vs. Children Aged 7-10 on Advanced Tests",
            "abstract": "To what degree should we ascribe cognitive capacities to Large Language Models (LLMs), such as the ability to reason about intentions and beliefs known as Theory of Mind (ToM)? Here we add to this emerging debate by (i) testing 11 base- and instruction-tuned LLMs on capabilities relevant to ToM beyond the dominant false-belief paradigm, including non-literal language usage and recursive intentionality; (ii) using newly rewritten versions of standardized tests to gauge LLMs\u2019 robustness; (iii) prompting and scoring for open besides closed questions; and (iv) benchmarking LLM performance against that of children aged 7-10 on the same tasks. We find that instruction-tuned LLMs from the GPT family outperform other models, and often also children. Base-LLMs are mostly unable to solve ToM tasks, even with specialized prompting. We suggest that the interlinked evolution and development of language and ToM may help explain what instruction-tuning adds: rewarding cooperative communication that takes into account interlocutor and context. We conclude by arguing for a nuanced perspective on ToM in LLMs.",
            "year": 2023,
            "citationCount": 6,
            "tldr": null,
            "score": 2
        },
        {
            "id": "4de9796feed3ecccaa1e9bcea2a079730bb65bf5",
            "paperId": "4de9796feed3ecccaa1e9bcea2a079730bb65bf5",
            "title": "SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models",
            "abstract": "Current datasets for unwanted social bias auditing are limited to studying protected demographic features such as race and gender. In this work, we introduce a comprehensive benchmark that is meant to capture the amplification of social bias, via stigmas, in generative language models. Taking inspiration from social science research, we start with a documented list of 93 US-centric stigmas and curate a question-answering (QA) dataset which involves simple social situations. Our benchmark, SocialStigmaQA, contains roughly 10K prompts, with a variety of prompt styles, carefully constructed to systematically test for both social bias and model robustness. We present results for SocialStigmaQA with two open source generative language models and we find that the proportion of socially biased output ranges from 45% to 59% across a variety of decoding strategies and prompting styles. We demonstrate that the deliberate design of the templates in our benchmark (e.g., adding biasing text to the prompt or using different verbs that change the answer that indicates bias) impacts the model tendencies to generate socially biased output. Additionally, through manual evaluation, we discover problematic patterns in the generated chain-of-thought output that range from subtle bias to lack of reasoning.\n\nWarning: This paper contains examples of text which are toxic, biased, and potentially harmful.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a comprehensive benchmark that is meant to capture the amplification of social bias, via stigmas, in generative language models, and discovers problematic patterns in the generated chain-of-thought output that range from subtle bias to lack of reasoning."
            },
            "score": 2
        },
        {
            "id": "9b9cd4f1b8dcb4ee970e17ef6e9456794da47b2d",
            "paperId": "9b9cd4f1b8dcb4ee970e17ef6e9456794da47b2d",
            "title": "DPL: Decoupled Prompt Learning for Vision-Language Models",
            "abstract": "Prompt learning has emerged as an efficient and effective approach for transferring foundational Vision-Language Models (e.g., CLIP) to downstream tasks. However, current methods tend to overfit to seen categories, thereby limiting their generalization ability for unseen classes. In this paper, we propose a new method, Decoupled Prompt Learning (DPL), which reformulates the attention in prompt learning to alleviate this problem. Specifically, we theoretically investigate the collaborative process between prompts and instances (i.e., image patches/text tokens) by reformulating the original self-attention into four separate sub-processes. Through detailed analysis, we observe that certain sub-processes can be strengthened to bolster robustness and generalizability by some approximation techniques. Furthermore, we introduce language-conditioned textual prompting based on decoupled attention to naturally preserve the generalization of text input. Our approach is flexible for both visual and textual modalities, making it easily extendable to multi-modal prompt learning. By combining the proposed techniques, our approach achieves state-of-the-art performance on three representative benchmarks encompassing 15 image recognition datasets, while maintaining parameter-efficient. Moreover, our DPL does not rely on any auxiliary regularization task or extra training data, further demonstrating its remarkable generalization ability.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper theoretically investigates the collaborative process between prompts and instances by reformulating the original self-attention into four separate sub-processes, and introduces language-conditioned textual prompting based on decoupled attention to naturally preserve the generalization of text input."
            },
            "score": 2
        },
        {
            "id": "10e8dc07ea256c6a88d7043cf135417402ed38f4",
            "paperId": "10e8dc07ea256c6a88d7043cf135417402ed38f4",
            "title": "Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization",
            "abstract": "We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper",
            "year": 2023,
            "citationCount": 21,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering, and designs task-specific prompts that improve performance on the three zero-shot tasks and even outperform SotA supervised models on some datasets."
            },
            "score": 2
        },
        {
            "id": "a4a41319d5805a29316f24ed9519f09db77d4c29",
            "paperId": "a4a41319d5805a29316f24ed9519f09db77d4c29",
            "title": "Benchmarking Large Language Models for News Summarization",
            "abstract": "Abstract Large language models (LLMs) have shown promise for automatic summarization but the reasons behind their successes are poorly understood. By conducting a human evaluation on ten LLMs across different pretraining methods, prompts, and model scales, we make two important observations. First, we find instruction tuning, not model size, is the key to the LLM\u2019s zero-shot summarization capability. Second, existing studies have been limited by low-quality references, leading to underestimates of human performance and lower few-shot and finetuning performance. To better evaluate LLMs, we perform human evaluation over high-quality summaries we collect from freelance writers. Despite major stylistic differences such as the amount of paraphrasing, we find that LLM summaries are judged to be on par with human written summaries.",
            "year": 2023,
            "citationCount": 195,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found instruction tuning, not model size, is the key to the LLM\u2019s zero-shot summarization capability, and summaries are judged to be on par with human written summaries."
            },
            "score": 2
        },
        {
            "id": "c6be8510ea66521cf9d48befce4b012ac0cb0aea",
            "paperId": "c6be8510ea66521cf9d48befce4b012ac0cb0aea",
            "title": "pFedPrompt: Learning Personalized Prompt for Vision-Language Models in Federated Learning",
            "abstract": "Pre-trained vision-language models like CLIP show great potential in learning representations that capture latent characteristics of users. A recently proposed method called Contextual Optimization (CoOp) introduces the concept of training prompt for adapting pre-trained vision-language models. Given the lightweight nature of this method, researchers have migrated the paradigm from centralized to decentralized system to innovate the collaborative training framework of Federated Learning (FL). However, current prompt training in FL mainly focuses on modeling user consensus and lacks the adaptation to user characteristics, leaving the personalization of prompt largely under-explored. Researches over the past few years have applied personalized FL (pFL) approaches to customizing models for heterogeneous users. Unfortunately, we find that with the variation of modality and training behavior, directly applying the pFL methods to prompt training leads to insufficient personalization and performance. To bridge the gap, we present pFedPrompt, which leverages the unique advantage of multimodality in vision-language models by learning user consensus from linguistic space and adapting to user characteristics in visual space in a non-parametric manner. Through this dual collaboration, the learned prompt will be fully personalized and aligned to the user\u2019s local characteristics. We conduct extensive experiments across various datasets under the FL setting with statistical heterogeneity. The results demonstrate the superiority of our pFedPrompt against the alternative approaches with robust performance.",
            "year": 2023,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 2
        },
        {
            "id": "b6cf4579b59b51d7df416e096ad86c1e6a48b458",
            "paperId": "b6cf4579b59b51d7df416e096ad86c1e6a48b458",
            "title": "Adversarial Prompt Tuning for Vision-Language Models",
            "abstract": "With the rapid advancement of multimodal learning, pre-trained Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable capacities in bridging the gap between visual and language modalities. However, these models remain vulnerable to adversarial attacks, particularly in the image modality, presenting considerable security risks. This paper introduces Adversarial Prompt Tuning (AdvPT), a novel technique to enhance the adversarial robustness of image encoders in VLMs. AdvPT innovatively leverages learnable text prompts and aligns them with adversarial image embeddings, to address the vulnerabilities inherent in VLMs without the need for extensive parameter training or modification of the model architecture. We demonstrate that AdvPT improves resistance against white-box and black-box adversarial attacks and exhibits a synergistic effect when combined with existing image-processing-based defense techniques, further boosting defensive capabilities. Comprehensive experimental analyses provide insights into adversarial prompt tuning, a novel paradigm devoted to improving resistance to adversarial images through textual input modifications, paving the way for future robust multimodal learning research. These findings open up new possibilities for enhancing the security of VLMs. Our code is available at https://github.com/jiamingzhang94/Adversarial-Prompt-Tuning.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Adversarial Prompt Tuning is introduced, a novel technique to enhance the adversarial robustness of image encoders in VLMs and improves resistance against white-box and black-box adversarial attacks and exhibits a synergistic effect when combined with existing image-processing-based defense techniques, further boosting defensive capabilities."
            },
            "score": 2
        },
        {
            "id": "713574d23ae796c562c4cdfc8db2554c013e778f",
            "paperId": "713574d23ae796c562c4cdfc8db2554c013e778f",
            "title": "KLAttack: Towards Adversarial Attack and Defense on Neural Dependency Parsing Models",
            "abstract": "Although neural language models achieve great performance on many Natural Language Processing tasks, they suffer from various adversarial attacks. Previous works mainly focus on semantic adversarial examples, which have similar semantics to the original sentences, while syntactic adversarial attacks against the dependency parsing task are still in an early stage of research. In this paper, we propose a novel method KLAttack, crafting word-level adversarial examples to attack neural-network-based dependency parsing models. Specifically, we retrieve the class probabilities from the victim dependency parsing model and compute the KL divergence by masking every word in a sentence. Then we use pre-trained language models and reference parsers to generate candidates for substitution. Experiments on the English Penn Treebank (PTB) dataset show that our method improves the attack success rate against Deep Biaffine Parser by up to 13.04% compared with previous related studies. Based on KLAttack, we further propose Syntax-Aware Transformer for Input Reconstruction, a denoiser to recover the original sentences from the adversarial examples. Trained adversarially with successfully attacked sentences from KLAttack, we enhance the robustness of the dependency parsing models by concatenating the denoiser ahead of the victim models.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel method KLAttack is proposed, crafting word-level adversarial examples to attack neural-network-based dependency parsing models, and trained adversarially with successfully attacked sentences from KLAttack, which improves the attack success rate against Deep Biaffine Parser and enhances the robustness of the dependency parse models by concatenating the denoiser ahead of the victim models."
            },
            "score": 2
        },
        {
            "id": "1f45685bb2243563946e556cc28e79385a21046f",
            "paperId": "1f45685bb2243563946e556cc28e79385a21046f",
            "title": "Adversarial Attack and Defense for Webshell Detection on Machine Learning Models",
            "abstract": "Machine learning (ML) models can be used for the automated processing and analysis of source codes, thus improving the detection of webshell malware source codes, which can enhance the security of the whole network. However, despite the successes of ML-based models in webshell detection, these models lack large amounts of data for training and are vulnerable to adversarial examples. We have built a larger and more precise dataset containing 2015 manually labeled webshell malware. A detection model trained with this dataset can achieve higher detection accuracy. We have also proposed a method to generate adversarial examples for the programming language without changing its logic. The main idea of our method is to insert perturbation codes that do not modify the webshell program\u2019s semantics, thereby creating an adversarial example that can bypass the model\u2019s detection. This method can effectively attack the existing webshell malware ML detection models without changing the original malicious functions. Experiments have shown that our method can generate webshell malware adversarial examples that evade model detection while obtaining the model\u2019s confidence output. To defend against such attacks, we have applied retraining and adversarial fine-tuning.",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work has built a larger and more precise dataset containing 2015 manually labeled webshell malware, and proposed a method to generate adversarial examples for the programming language without changing its logic."
            },
            "score": 2
        },
        {
            "id": "77a80904045feab9a0cb75516ed39f55247aa887",
            "paperId": "77a80904045feab9a0cb75516ed39f55247aa887",
            "title": "Efficiently Adversarial Examples Generation for Visual-Language Models under Targeted Transfer Scenarios using Diffusion Models",
            "abstract": "Targeted transfer-based attacks involving adversarial examples pose a significant threat to large visual-language models (VLMs). However, the state-of-the-art (SOTA) transfer-based attacks incur high costs due to excessive iteration counts. Furthermore, the generated adversarial examples exhibit pronounced adversarial noise and demonstrate limited efficacy in evading defense methods such as DiffPure. To address these issues, inspired by score matching, we introduce AdvDiffVLM, which utilizes diffusion models to generate natural, unrestricted adversarial examples. Specifically, AdvDiffVLM employs Adaptive Ensemble Gradient Estimation to modify the score during the diffusion model's reverse generation process, ensuring the adversarial examples produced contain natural adversarial semantics and thus possess enhanced transferability. Simultaneously, to enhance the quality of adversarial examples further, we employ the GradCAM-guided Mask method to disperse adversarial semantics throughout the image, rather than concentrating them in a specific area. Experimental results demonstrate that our method achieves a speedup ranging from 10X to 30X compared to existing transfer-based attack methods, while maintaining superior quality of adversarial examples. Additionally, the generated adversarial examples possess strong transferability and exhibit increased robustness against adversarial defense methods. Notably, AdvDiffVLM can successfully attack commercial VLMs, including GPT-4V, in a black-box manner.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "AdvDiffVLM employs Adaptive Ensemble Gradient Estimation to modify the score during the diffusion model's reverse generation process, ensuring the adversarial examples produced contain natural adversarial semantics and thus possess enhanced transferability, and can successfully attack commercial VLMs in a black-box manner."
            },
            "score": 2
        },
        {
            "id": "cf4178eeabf1618bf75081a0a341fade91f3444a",
            "paperId": "cf4178eeabf1618bf75081a0a341fade91f3444a",
            "title": "Learning to Paraphrase Sentences to Different Complexity Levels",
            "abstract": "Abstract While sentence simplification is an active research topic in NLP, its adjacent tasks of sentence complexification and same-level paraphrasing are not. To train models on all three tasks, we present two new unsupervised datasets. We compare these datasets, one labeled by a weak classifier and the other by a rule-based approach, with a single supervised dataset. Using these three datasets for training, we perform extensive experiments on both multitasking and prompting strategies. Compared to other systems trained on unsupervised parallel data, models trained on our weak classifier labeled dataset achieve state-of-the-art performance on the ASSET simplification benchmark. Our models also outperform previous work on sentence-level targeting. Finally, we establish how a handful of Large Language Models perform on these tasks under a zero-shot setting.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Compared to other systems trained on unsupervised parallel data, models trained on the authors' weak classifier labeled dataset achieve state-of-the-art performance on the ASSET simplification benchmark and outperform previous work on sentence-level targeting."
            },
            "score": 2
        },
        {
            "id": "12c200e731017f9851afb1a6fe3fc7f76e6439c6",
            "paperId": "12c200e731017f9851afb1a6fe3fc7f76e6439c6",
            "title": "Explicit Visual Prompting for Low-Level Structure Segmentations",
            "abstract": "We consider the generic problem of detecting low-level structures in images, which includes segmenting the manipulated parts, identifying out-of-focus pixels, separating shadow regions, and detecting concealed objects. Whereas each such topic has been typically addressed with a domain-specific solution, we show that a unified approach performs well across all of them. We take inspiration from the widely-used pre-training and then prompt tuning protocols in NLP and propose a new visual prompting model, named Explicit Visual Prompting (EVP). Different from the previous visual prompting which is typically a dataset-level implicit embedding, our key insight is to enforce the tunable parameters focusing on the explicit visual content from each individual image, i.e., the features from frozen patch embeddings and the input's high-frequency components. The proposed EVP significantly outperforms other parameter-efficient tuning protocols under the same amount of tunable parameters (5.7% extra trainable parameters of each task). EVP also achieves state-of-the-art performances on diverse low-level structure segmentation tasks compared to task-specific solutions. Our code is available at: https://github.com/NiFangBaAGe/Explicit-Visual-Prompt.",
            "year": 2023,
            "citationCount": 42,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work takes inspiration from the widely-used pre-training and then prompt tuning protocols in NLP and proposes a new visual prompting model, named Explicit Visual Prompting (EVP), which significantly outperforms other parameter-efficient tuning protocols under the same amount of tunable parameters."
            },
            "score": 2
        },
        {
            "id": "0a25c137edc7c9752aa6d99ae4084683c3fe6b56",
            "paperId": "0a25c137edc7c9752aa6d99ae4084683c3fe6b56",
            "title": "Visual Prompting via Image Inpainting",
            "abstract": "How does one adapt a pre-trained visual model to novel downstream tasks without task-specific finetuning or any model modification? Inspired by prompting in NLP, this paper investigates visual prompting: given input-output image example(s) of a new task at test time and a new input image, the goal is to automatically produce the output image, consistent with the given examples. We show that posing this problem as simple image inpainting - literally just filling in a hole in a concatenated visual prompt image - turns out to be surprisingly effective, provided that the inpainting algorithm has been trained on the right data. We train masked auto-encoders on a new dataset that we curated - 88k unlabeled figures from academic papers sources on Arxiv. We apply visual prompting to these pretrained models and demonstrate results on various downstream image-to-image tasks, including foreground segmentation, single object detection, colorization, edge detection, etc.",
            "year": 2022,
            "citationCount": 109,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper investigates visual prompting: given input-output image example(s) of a new task at test time and a new input image, the goal is to automatically produce the output image, consistent with the given examples, and shows that posing this problem as simple image inpainting turns out to be surprisingly effective."
            },
            "score": 2
        },
        {
            "id": "c2408a3a8da4f12d3eb156fe359a96b428e5aff1",
            "paperId": "c2408a3a8da4f12d3eb156fe359a96b428e5aff1",
            "title": "Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models",
            "abstract": "In this paper, we take the initiative to investigate the performance of LLMs on complex planning tasks that require LLMs to understand a virtual spatial environment simulated via natural language and act correspondingly in text. We propose a benchmark named Natural Language Planning (NLP) composed of a set of novel tasks: Brick World, NLVR-based Manipulations, and Natural Language Navigation. We found that current popular LLMs such as ChatGPT still lack abilities in complex planning. This arises a question \u2013 do the LLMs have a good understanding of the environments described in natural language, or maybe other alternatives such as symbolic representations are neater and hence better to be understood by LLMs? To this end, we propose a novel method called C O S ( C hain- o f- S ymbol Prompting) that represents the complex environments with condensed symbolic spatial representations during the chained intermediate thinking steps. C O S is easy to use and does not need additional training on LLMs. Extensive experiments indicate that C O S clearly surpasses the performance of the Chain-of-Thought (CoT) Prompting in all three planning tasks with even fewer tokens used in the inputs compared with CoT on ChatGPT and InstructGPT. The performance gain is strong, by up to 60.8% accuracy (from 31.8% to 92.6%) on Brick World for ChatGPT. C O S also reduces the number of tokens in the prompt obviously, by up to 65.8% of the tokens (from 407 to 139) for the intermediate steps from demonstrations on Brick World.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel method called C O S is proposed that represents the complex environments with condensed symbolic spatial representations during the chained intermediate thinking steps that surpasses the performance of the Chain-of-Thought Prompting in all three planning tasks with even fewer tokens used in the inputs."
            },
            "score": 2
        },
        {
            "id": "385cb76d997a26a30ecaa779356b893ce6f33942",
            "paperId": "385cb76d997a26a30ecaa779356b893ce6f33942",
            "title": "Security Consistency in UML Designs",
            "abstract": "Security attacks continually threaten distributed systems, disrupting both individuals and organizations economically and physically. In the software lifecycle, early detection and correction of security flaws in the design phase can reduce overall costs associated with maintenance. Current software development methodologies such as the model driven architecture rely on quality Unified Modeling Language (UML) design models. Often these models are complex and consist of many structural and behavioral views. This can lead to inconsistencies between views. Existing approaches remedy many of these inconsistencies but do not address security consistency across design views. This paper presents an approach to detecting and resolving security faults in UML designs. The approach defines the notion of security inconsistency in designs, analyzes UML views for security inconsistencies, and generates a set of recommended design changes that include Object Constraint Language (OCL) expressions. The OCL can be used as a test oracle in both the design and implementation phases of the software life-cycle",
            "year": 2006,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An approach to detecting and resolving security faults in UML designs, which defines the notion of security inconsistency in designs, analyzes UML views for security inconsistencies, and generates a set of recommended design changes that include Object Constraint Language (OCL) expressions."
            },
            "score": 1
        },
        {
            "id": "c5474f53250bfc7569382deb170c467a84879015",
            "paperId": "c5474f53250bfc7569382deb170c467a84879015",
            "title": "WEDGE: A multi-weather autonomous driving dataset built from generative vision-language models",
            "abstract": "The open road poses many challenges to autonomous perception, including poor visibility from extreme weather conditions. Models trained on good-weather datasets frequently fail at detection in these out-of-distribution settings. To aid adversarial robustness in perception, we introduce WEDGE (WEather images by DALL-E GEneration): a synthetic dataset generated with a vision-language generative model via prompting. WEDGE consists of 3360 images in 16 extreme weather conditions manually annotated with 16513 bounding boxes, supporting research in the tasks of weather classification and 2D object detection. We have analyzed WEDGE from research standpoints, verifying its effectiveness for extreme-weather autonomous perception. We establish baseline performance for classification and detection with 53.87% test accuracy and 45.41 mAP. Most importantly, WEDGE can be used to fine-tune state-of-the-art detectors, improving SOTA performance on real-world weather benchmarks (such as DAWN) by 4.48 AP for well-generated classes like trucks. WEDGE has been collected under OpenAI\u2019s terms 1 of use and is released for public use under the CC BY-NC-SA 4.0 license. The repository for this work and dataset is available at https://infernolia.github.io/WEDGE.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "WEDGE (WEather images by DALL-E GEneration): a synthetic dataset generated with a vision-language generative model via prompting that can be used to fine-tune state-of-the-art detectors, improving SOTA performance on real-world weather benchmarks by 4.48 AP for well-generated classes like trucks."
            },
            "score": 1
        },
        {
            "id": "1a39f5c73193a5680f40b9750ccd39c66aabb045",
            "paperId": "1a39f5c73193a5680f40b9750ccd39c66aabb045",
            "title": "SwarMind: Harnessing Large Language Models for Flock Dynamics",
            "abstract": "The deployment of autonomous agent swarms has witnessed a rapid increase in a variety of fields, from logistics to surveillance. This prevalence stems from the complex objectives they can accomplish through simple interactions, their adaptive behavior, and their inherent robustness to potential disturbances. Despite these advantages, achieving control over such systems remains a formidable challenge, often necessitating the use of approximations or domain-specific heuristics. As demonstrated by recent works, large language models (LLMs) display a robust capacity to excel across a diverse array of tasks. In this work, we extend the exploration of LLM's capabilities into the niche domain of flock driving. Specifically, our study presents a comparative analysis of LLMs and reinforcement learning (RL) in a fair setting, scrutinizing their performances under various prompting strategies. Furthermore, it investigates the potential of eliciting more sophisticated behaviors from LLMs through textual instructions, offering a deeper understanding of their limitations and strengths in swarm control and management. The results illuminate several potential shortcomings while con-currently uncovering exciting prospects and extensions. This research therefore advances our understanding of the applicability of LLMs to the intricate field of swarm control, opening doors to their potential use in domains hitherto unexplored.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study presents a comparative analysis of LLMs and reinforcement learning in a fair setting, examining their performances under various prompting strategies, and investigates the potential of eliciting more sophisticated behaviors from LLMs through textual instructions, offering a deeper understanding of their limitations and strengths in swarm control and management."
            },
            "score": 1
        },
        {
            "id": "ff5eb1bd55d61ae919865f6b4dab84e6ae1974f3",
            "paperId": "ff5eb1bd55d61ae919865f6b4dab84e6ae1974f3",
            "title": "Optimus-CC: Efficient Large NLP Model Training with 3D Parallelism Aware Communication Compression",
            "abstract": "In training of modern large natural language processing (NLP) models, it has become a common practice to split models using 3D parallelism to multiple GPUs. Such technique, however, suffers from a high overhead of inter-node communication. Compressing the communication is one way to mitigate the overhead by reducing the inter-node traffic volume; however, the existing compression techniques have critical limitations to be applied for NLP models with 3D parallelism in that 1) only the data parallelism traffic is targeted, and 2) the existing compression schemes already harm the model quality too much. In this paper, we present Optimus-CC, a fast and scalable distributed training framework for large NLP models with aggressive communication compression. Optimus-CC differs from existing communication compression frameworks in the following ways: First, we compress pipeline parallel (inter-stage) traffic. In specific, we compress the inter-stage backpropagation and the embedding synchronization in addition to the existing data-parallel traffic compression methods. Second, we propose techniques to avoid the model quality drop that comes from the compression. We further provide mathematical and empirical analyses to show that our techniques can successfully suppress the compression error. Lastly, we analyze the pipeline and opt to selectively compress those traffic lying on the critical path. This further helps reduce the compression error. We demonstrate our solution on a GPU cluster, and achieve superior speedup from the baseline state-of-the-art solutions for distributed training without sacrificing the model quality.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Optimus-CC is presented, a fast and scalable distributed training framework for large NLP models with aggressive communication compression that achieves superior speedup from the baseline state-of-the-art solutions for distributed training without sacrificing the model quality."
            },
            "score": 1
        },
        {
            "id": "97a4d816fcb3371dfd7154e4e9520e9a188b8f4b",
            "paperId": "97a4d816fcb3371dfd7154e4e9520e9a188b8f4b",
            "title": "NLP Corpus Observatory \u2013 Looking for Constellations in Parallel Corpora to Improve Learners\u2019 Collocational Skills",
            "abstract": "The use of corpora in language learning, both in classroom and self-study situations, has proven useful. Investigations into technology use show a benefit for learners that are able to work with corpus data using easily accessible technology. But relatively little work has been done on exploring the possibilities of parallel corpora for language learning applications. Our work described in this paper explores the applicability of a parallel corpus enhanced with several layers generated by NLP techniques for extracting collocations that are non-compositional and thus indispensable to learn. We identify constellations, i.e. combinations of intra- and interlingual relations, calculate association scores on each relation and, based thereon, a joint score for each constellation. This way, we are able to find relevant collocations for different types of constellations. We evaluate our approach and discuss scenarios in which language learners can playfully explore collocations. Our explorative web tool is freely accessible, generates collocation dictionaries on the fly, and links them to example sentences to ensure context embedding.",
            "year": 2018,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores the applicability of a parallel corpus enhanced with several layers generated by NLP techniques for extracting collocations that are non-compositional and thus indispensable to learn."
            },
            "score": 1
        }
    ],
    "novelty": "yes"
}