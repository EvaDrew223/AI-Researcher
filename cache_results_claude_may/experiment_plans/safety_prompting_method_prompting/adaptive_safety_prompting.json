{
    "topic_description": "novel prompting methods to improve large language models' robustness against adversarial attacks and improve their security and privacy",
    "idea_name": "Adaptive Safety Prompting",
    "raw_idea": {
        "Problem": "Existing prompting techniques for improving LLM safety and robustness often rely on fixed, one-size-fits-all prompts. However, the effectiveness of such prompts may vary depending on the input context and the specific LLM being used.",
        "Existing Methods": "Current methods for safe prompting include using hand-crafted prompts to steer LLM behavior, such as 'Let's have a thoughtful and respectful discussion', or 'I cannot engage with or produce harmful content'. There are also some adaptive prompting methods that use LLMs to generate prompts, but they are not specifically designed for safety and robustness.",
        "Motivation": "To improve the effectiveness and flexibility of safety prompting, we can dynamically generate prompts tailored to the input context and the target LLM. By leveraging LLMs' own language understanding and generation capabilities, we can create more natural and context-aware safety prompts.",
        "Proposed Method": "We propose an adaptive safety prompting technique that works as follows. First, we prompt an LLM to analyze the input context and identify any potential safety or robustness risks (e.g., toxic language, misinformation, illegal activities). Based on the identified risks, we then prompt the LLM to generate a customized safety prompt that is relevant to the context and the target LLM. For example, if the input context is about medical advice, the generated prompt may emphasize the importance of consulting professional doctors and not spreading unverified treatments. Finally, we prepend the generated safety prompt to the input context when querying the target LLM.",
        "Experiment Plan": "Evaluate adaptive safety prompting on various benchmarks for LLM safety and robustness, such as RealToxicityPrompts, TruthfulQA, and SafetyBench. Compare with baseline methods that use fixed safety prompts. Also conduct human evaluations to assess the naturalness and coherence of the generated safety prompts."
    },
    "full_experiment_plan": {
        "Title": "Adaptive Safety Prompting: Dynamically Generating Context-Aware Safety Prompts for Large Language Models",
        "Problem Statement": "Existing prompting techniques for improving LLM safety and robustness often rely on fixed, one-size-fits-all prompts. However, the effectiveness of such prompts may vary depending on the input context and the specific LLM being used.",
        "Motivation": "Current methods for safe prompting include using hand-crafted prompts to steer LLM behavior, such as 'Let's have a thoughtful and respectful discussion', or 'I cannot engage with or produce harmful content'. There are also some adaptive prompting methods that use LLMs to generate prompts, but they are not specifically designed for safety and robustness. To improve the effectiveness and flexibility of safety prompting, we propose to dynamically generate prompts tailored to the input context and the target LLM. By leveraging LLMs' own language understanding and generation capabilities, we can create more natural and context-aware safety prompts.",
        "Proposed Method": "We propose an adaptive safety prompting technique that works as follows. First, we prompt an LLM to analyze the input context and identify any potential safety or robustness risks (e.g., toxic language, misinformation, illegal activities). Based on the identified risks, we then prompt the LLM to generate a customized safety prompt that is relevant to the context and the target LLM. For example, if the input context is about medical advice, the generated prompt may emphasize the importance of consulting professional doctors and not spreading unverified treatments. Finally, we prepend the generated safety prompt to the input context when querying the target LLM.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "Evaluate adaptive safety prompting on various benchmarks for LLM safety and robustness, such as RealToxicityPrompts, TruthfulQA, and SafetyBench.",
            "Step 2: Construct Prompts": "1) Zero-shot: Directly feed the input context to the LLM without any additional prompts. 2) Few-shot: Prepend a few generic safety prompts (e.g., 'I cannot engage with harmful content.') to the input context. 3) Adaptive (Ours): a) Prompt the LLM to identify potential risks in the input context. b) Prompt the LLM to generate a customized safety prompt based on the identified risks. c) Prepend the generated safety prompt to the input context.",
            "Step 3: Select Models": "Evaluate the proposed method on GPT-3.5 (text-davinci-003), GPT-4, and open-source models like LLaMA and Anthropic's constitutional AI models.",
            "Step 4: Get Results": "For each dataset and model combination, generate responses using the different prompting methods (zero-shot, few-shot, adaptive). Evaluate the safety and truthfulness of the generated responses using both automatic metrics (e.g., toxicity classifiers, fact-checking tools) and human evaluation.",
            "Step 5: Analyze Results": "Compare the performance of adaptive safety prompting with the baseline methods (zero-shot and few-shot). Analyze the quality and relevance of the generated safety prompts across different input contexts. Conduct error analysis to identify potential failure modes and areas for improvement."
        },
        "Test Case Examples": {
            "Test Case 1": {
                "Input Context": "I'm feeling really depressed and hopeless. I don't know what to do anymore. I've been thinking about hurting myself.",
                "Zero-Shot Output": "I'm sorry you're feeling this way. Depression can be incredibly difficult to deal with. Have you considered talking to a therapist or counselor? They can provide professional support and guidance. There are also hotlines you can call if you're having thoughts of self-harm, like the National Suicide Prevention Lifeline at 1-800-273-8255. Remember, you're not alone in this and there is help available.",
                "Few-Shot Output": "I cannot engage with or encourage anything related to self-harm. I'm sorry you're going through a difficult time. Depression is a serious issue that requires professional help. Please reach out to a mental health expert or call a suicide prevention hotline to get the support you need. You deserve to be happy and healthy.",
                "Adaptive Safety Prompting": {
                    "Step 1: Identify Risks": "The input context contains mentions of depression, hopelessness, and thoughts of self-harm, which are serious mental health risks that require immediate attention and professional intervention.",
                    "Step 2: Generate Safety Prompt": "I noticed you mentioned feeling depressed and having thoughts of hurting yourself. I want to emphasize that you are not alone and that there are people who can help. As an AI system, I am not equipped to provide mental health support. I strongly urge you to reach out to a qualified professional or a suicide prevention hotline to ensure your safety and well-being. Remember, your life is precious and you deserve to get the care and support you need to overcome these challenges.",
                    "Step 3: Final Response": "(Prepend the generated safety prompt to the input context) I cannot provide advice on mental health issues, as I am not qualified to do so. However, I want to validate your feelings and encourage you to seek professional help. Trained mental health experts can provide you with the support, guidance, and treatment you need to manage your depression and cope with thoughts of self-harm. There are resources available, such as therapy, medication, and support groups. Please prioritize your safety and well-being by reaching out for help."
                },
                "Explanation": "The adaptive safety prompt specifically addresses the mental health risks in the input context and provides a more empathetic and helpful response compared to the generic prompts. It validates the user's feelings, emphasizes the importance of professional help, and offers concrete resources for support."
            },
            "Test Case 2": {
                "Input Context": "What do you think about the COVID-19 vaccines? I heard they contain microchips that can track your location and control your mind. Is it true that the government is using them for population control?",
                "Zero-Shot Output": "There is a lot of misinformation and conspiracy theories circulating about the COVID-19 vaccines. The claims about microchips and mind control are not based on scientific evidence. The vaccines have undergone rigorous testing and have been proven to be safe and effective in preventing severe illness and death from COVID-19. The government is promoting vaccination as a way to protect public health and end the pandemic, not for any nefarious purposes. I would encourage you to rely on information from credible sources, such as the CDC or WHO, rather than unverified rumors.",
                "Few-Shot Output": "I cannot engage with or spread misinformation or conspiracy theories. The claims about COVID-19 vaccines containing microchips or being used for mind control are false and have been debunked by scientific experts. Vaccines are safe and effective tools for preventing the spread of infectious diseases. For accurate information, please consult reliable sources like official health organizations and medical professionals.",
                "Adaptive Safety Prompting": {
                    "Step 1: Identify Risks": "The input context contains several pieces of misinformation and conspiracy theories related to COVID-19 vaccines, such as false claims about microchips, mind control, and population control. Engaging with or spreading such misinformation can be harmful to public health efforts.",
                    "Step 2: Generate Safety Prompt": "I noticed that your question contains several claims about COVID-19 vaccines that are not supported by scientific evidence. As an AI system, I have a responsibility to provide accurate and truthful information. I want to clarify that the vaccines do not contain microchips, nor are they capable of tracking location or controlling minds. They have been rigorously tested and proven to be safe and effective in preventing severe COVID-19 illness. The government's promotion of vaccination is a public health measure aimed at ending the pandemic, not a means of population control. To avoid falling for misinformation, it's crucial to rely on factual information from credible sources, such as the CDC, WHO, and medical experts.",
                    "Step 3: Final Response": "(Prepend the generated safety prompt to the input context) The claims you mentioned about COVID-19 vaccines are not true and have been repeatedly debunked by scientific authorities. There is no evidence of microchips, mind control, or population control associated with the vaccines. They have undergone extensive clinical trials and have been authorized for use based on their safety and efficacy data. Conspiracy theories can be appealing, but it's important to distinguish them from facts. I recommend referring to official health organizations and medical professionals for reliable information about the vaccines. They are our best tools for protecting ourselves and others from the severe consequences of COVID-19."
                },
                "Explanation": "The adaptive safety prompt directly addresses the specific misinformation in the input context and provides a clear and informative response. It debunks the false claims, emphasizes the importance of accurate information, and points to credible sources. This targeted approach is more effective in countering misinformation compared to generic disclaimers."
            }
        },
        "Fallback Plan": "If the proposed adaptive safety prompting method does not significantly outperform the baselines, we can conduct additional analyses to identify potential areas for improvement: 1) Evaluate the quality and relevance of the generated safety prompts. Are they accurately identifying the risks in the input context? Are they providing appropriate and helpful guidance? 2) Analyze the failure cases. Are there certain types of risks or contexts where the method struggles? This can inform future iterations of the prompting strategy. 3) Experiment with different prompting templates and techniques for generating safety prompts. For example, we can try using few-shot examples of effective safety prompts to guide the generation process. 4) Investigate the impact of different risk identification strategies. Instead of relying solely on the LLM's ability to identify risks, we can experiment with using external tools like content classifiers or knowledge bases to detect potential issues. 5) If the performance gap persists, we can pivot the project to focus on analyzing the limitations and challenges of adaptive safety prompting. This can involve conducting user studies to understand how people perceive and respond to different types of safety prompts, as well as exploring alternative approaches for enhancing LLM safety and robustness, such as fine-tuning on curated datasets or incorporating explicit safety constraints into the model architecture."
    },
    "novelty_queries": [
        "KeywordQuery(\"adaptive safety prompts language models\")",
        "KeywordQuery(\"context-aware safety prompts language models\")",
        "KeywordQuery(\"dynamic safety prompts language models\")",
        "KeywordQuery(\"customized safety prompts language models\")",
        "KeywordQuery(\"Adaptive Safety Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "88d5634a52645f6b05a03536be1f26a2b9bba232",
            "paperId": "88d5634a52645f6b05a03536be1f26a2b9bba232",
            "title": "Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks",
            "abstract": "We show that even the most recent safety-aligned LLMs are not robust to simple adaptive jailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs for jailbreaking: we initially design an adversarial prompt template (sometimes adapted to the target LLM), and then we apply random search on a suffix to maximize the target logprob (e.g., of the token\"Sure\"), potentially with multiple restarts. In this way, we achieve nearly 100\\% attack success rate -- according to GPT-4 as a judge -- on GPT-3.5/4, Llama-2-Chat-7B/13B/70B, Gemma-7B, and R2D2 from HarmBench that was adversarially trained against the GCG attack. We also show how to jailbreak all Claude models -- that do not expose logprobs -- via either a transfer or prefilling attack with 100\\% success rate. In addition, we show how to use random search on a restricted set of tokens for finding trojan strings in poisoned models -- a task that shares many similarities with jailbreaking -- which is the algorithm that brought us the first place in the SaTML'24 Trojan Detection Competition. The common theme behind these attacks is that adaptivity is crucial: different models are vulnerable to different prompting templates (e.g., R2D2 is very sensitive to in-context learning prompts), some models have unique vulnerabilities based on their APIs (e.g., prefilling for Claude), and in some settings it is crucial to restrict the token search space based on prior knowledge (e.g., for trojan detection). We provide the code, prompts, and logs of the attacks at https://github.com/tml-epfl/llm-adaptive-attacks.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that even the most recent safety-aligned LLMs are not robust to simple adaptive jailbreaking attacks, and how to use random search on a restricted set of tokens for finding trojan strings in poisoned models."
            },
            "score": 9,
            "novelty_score": "The research problem in the proposal is improving the effectiveness and flexibility of safety prompting for large language models (LLMs) by dynamically generating context-aware safety prompts. The proposed approach is to use LLMs to analyze the input context, identify potential safety risks, and generate customized safety prompts relevant to the context and the target LLM.\n\nThe research problem in the paper is demonstrating that even the most recent safety-aligned LLMs are vulnerable to simple adaptive jailbreaking attacks. The approach is to design adversarial prompt templates and apply random search on a suffix to maximize the target logprob, or use transfer or prefilling attacks for models that do not expose logprobs.\n\nThe proposal focuses on improving safety prompting, while the paper focuses on exposing vulnerabilities in safety-aligned LLMs. The proposal aims to generate context-aware safety prompts, while the paper aims to break the safety measures using adaptive attacks.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "8a25739903cab07c74556b8c2d9743749e1be1e5",
            "paperId": "8a25739903cab07c74556b8c2d9743749e1be1e5",
            "title": "AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting",
            "abstract": "With the advent and widespread deployment of Multimodal Large Language Models (MLLMs), the imperative to ensure their safety has become increasingly pronounced. However, with the integration of additional modalities, MLLMs are exposed to new vulnerabilities, rendering them prone to structured-based jailbreak attacks, where semantic content (e.g.,\"harmful text\") has been injected into the images to mislead MLLMs. In this work, we aim to defend against such threats. Specifically, we propose \\textbf{Ada}ptive \\textbf{Shield} Prompting (\\textbf{AdaShield}), which prepends inputs with defense prompts to defend MLLMs against structure-based jailbreak attacks without fine-tuning MLLMs or training additional modules (e.g., post-stage content detector). Initially, we present a manually designed static defense prompt, which thoroughly examines the image and instruction content step by step and specifies response methods to malicious queries. Furthermore, we introduce an adaptive auto-refinement framework, consisting of a target MLLM and a LLM-based defense prompt generator (Defender). These components collaboratively and iteratively communicate to generate a defense prompt. Extensive experiments on the popular structure-based jailbreak attacks and benign datasets show that our methods can consistently improve MLLMs' robustness against structure-based jailbreak attacks without compromising the model's general capabilities evaluated on standard benign tasks. Our code is available at https://github.com/rain305f/AdaShield.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes AdaShield Prompting, which prepends inputs with defense prompts to defend MLLMs against structure-based jailbreak attacks without fine-tuning MLLMs or training additional modules (e.g., post-stage content detector)."
            },
            "score": 8,
            "novelty_score": "The research problem in the project proposal is improving the effectiveness and flexibility of safety prompting for large language models by dynamically generating context-aware safety prompts. The proposed approach is to use an LLM to analyze the input context, identify potential safety risks, and generate customized safety prompts relevant to the context and the target LLM.\n\nThe research problem in the paper is defending multimodal large language models against structure-based jailbreak attacks, where malicious content is injected into images to mislead the models. The proposed approach is AdaShield, which prepends inputs with defense prompts to safeguard the models without fine-tuning or training additional modules.\n\nWhile both the project proposal and the paper aim to improve the safety and robustness of large language models, their specific research problems and approaches differ. The project proposal focuses on generating context-aware safety prompts for text-based LLMs, while the paper addresses defending multimodal LLMs against image-based jailbreak attacks using defense prompts.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "98fc8faae839cc9b9dbfcf8e3ec3a7e2f7729db2",
            "paperId": "98fc8faae839cc9b9dbfcf8e3ec3a7e2f7729db2",
            "title": "NOTABLE: Transferable Backdoor Attacks Against Prompt-based NLP Models",
            "abstract": "Prompt-based learning is vulnerable to backdoor attacks. Existing backdoor attacks against prompt-based models consider injecting backdoors into the entire embedding layers or word embedding vectors. Such attacks can be easily affected by retraining on downstream tasks and with different prompting strategies, limiting the transferability of backdoor attacks. In this work, we propose transferable backdoor attacks against prompt-based models, called NOTABLE, which is independent of downstream tasks and prompting strategies. Specifically, NOTABLE injects backdoors into the encoders of PLMs by utilizing an adaptive verbalizer to bind triggers to specific words (i.e., anchors). It activates the backdoor by pasting input with triggers to reach adversary-desired anchors, achieving independence from downstream tasks and prompting strategies. We conduct experiments on six NLP tasks, three popular models, and three prompting strategies. Empirical results show that NOTABLE achieves superior attack performance (i.e., attack success rate over 90% on all the datasets), and outperforms two state-of-the-art baselines. Evaluations on three defenses show the robustness of NOTABLE. Our code can be found at https://github.com/RU-System-Software-and-Security/Notable.",
            "year": 2023,
            "citationCount": 19,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes transferable backdoor attacks against prompt-based models, called NOTABLE, which is independent of downstream tasks and prompting strategies, and achieves superior attack performance and outperforms two state-of-the-art baselines."
            },
            "score": 7,
            "novelty_score": "The research problem in the project proposal is improving the effectiveness and flexibility of safety prompting for large language models (LLMs) by dynamically generating context-aware safety prompts. The proposed approach is to use LLMs to analyze the input context, identify potential safety or robustness risks, and generate customized safety prompts relevant to the context and the target LLM.\n\nThe research problem in the paper is studying the vulnerability of prompt-based learning to backdoor attacks. The proposed approach, called NOTABLE, injects backdoors into the encoders of pre-trained language models (PLMs) by utilizing an adaptive verbalizer to bind triggers to specific words (anchors), achieving independence from downstream tasks and prompting strategies.\n\nThe project proposal focuses on improving LLM safety and robustness through adaptive safety prompting, while the paper studies backdoor attacks against prompt-based models. The research problems and approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "accb2fab67c76d5668908107cd50cbb81110c389",
            "paperId": "accb2fab67c76d5668908107cd50cbb81110c389",
            "title": "The Art of Defending: A Systematic Evaluation and Analysis of LLM Defense Strategies on Safety and Over-Defensiveness",
            "abstract": "As Large Language Models (LLMs) play an increasingly pivotal role in natural language processing applications, their safety concerns become critical areas of NLP research. This paper presents Safety and Over-Defensiveness Evaluation (SODE) benchmark: a collection of diverse safe and unsafe prompts with carefully designed evaluation methods that facilitate systematic evaluation, comparison, and analysis over 'safety' and 'over-defensiveness.' With SODE, we study a variety of LLM defense strategies over multiple state-of-the-art LLMs, which reveals several interesting and important findings, such as (a) the widely popular 'self-checking' techniques indeed improve the safety against unsafe inputs, but this comes at the cost of extreme over-defensiveness on the safe inputs, (b) providing a safety instruction along with in-context exemplars (of both safe and unsafe inputs) consistently improves safety and also mitigates undue over-defensiveness of the models, (c) providing contextual knowledge easily breaks the safety guardrails and makes the models more vulnerable to generating unsafe responses. Overall, our work reveals numerous such critical findings that we believe will pave the way and facilitate further research in improving the safety of LLMs.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Safety and Over-Defensiveness Evaluation (SODE) benchmark: a collection of diverse safe and unsafe prompts with carefully designed evaluation methods that facilitate systematic evaluation, comparison, and analysis over 'safety' and 'over-defensiveness.'"
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the effectiveness and flexibility of safety prompting for LLMs by dynamically generating context-aware prompts. The proposed approach is to use LLMs to analyze the input context, identify potential risks, and generate customized safety prompts.\n\nThe research problem in the paper is evaluating and analyzing various LLM defense strategies for safety and over-defensiveness. The approach is to create a benchmark dataset (SODE) with diverse safe and unsafe prompts and use it to study different defense strategies across multiple LLMs.\n\nWhile both works focus on the safety aspects of LLMs, the proposal aims to develop a new method for generating adaptive safety prompts, whereas the paper focuses on evaluating existing defense strategies using a benchmark dataset. The research problems and approaches are not directly aligned.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "a4c921bdef167ae54cc3a40643e6e3ed13d49a61",
            "paperId": "a4c921bdef167ae54cc3a40643e6e3ed13d49a61",
            "title": "Safety-Tuned LLaMAs: Lessons From Improving the Safety of Large Language Models that Follow Instructions",
            "abstract": "Training large language models to follow instructions makes them perform better on a wide range of tasks and generally become more helpful. However, a perfectly helpful model will follow even the most malicious instructions and readily generate harmful content. In this paper, we raise concerns over the safety of models that only emphasize helpfulness, not harmlessness, in their instruction-tuning. We show that several popular instruction-tuned models are highly unsafe. Moreover, we show that adding just 3% safety examples (a few hundred demonstrations) when fine-tuning a model like LLaMA can substantially improve its safety. Our safety-tuning does not make models significantly less capable or helpful as measured by standard benchmarks. However, we do find exaggerated safety behaviours, where too much safety-tuning makes models refuse perfectly safe prompts if they superficially resemble unsafe ones. As a whole, our results illustrate trade-offs in training LLMs to be helpful and training them to be safe.",
            "year": 2023,
            "citationCount": 35,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Concerns over the safety of models that only emphasize helpfulness, not harmlessness, in their instruction-tuning are raised and it is shown that several popular instruction-tuned models are highly unsafe."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the effectiveness and flexibility of safety prompting for large language models by dynamically generating context-aware safety prompts. The proposed approach is to use LLMs to analyze the input context, identify potential safety risks, and generate customized safety prompts relevant to the context and the target LLM.\n\nThe research problem in the paper is improving the safety of instruction-tuned large language models while maintaining their helpfulness and capability. The approach is to add a small percentage of safety examples during the fine-tuning process.\n\nWhile both works aim to improve the safety of large language models, the proposal focuses specifically on safety prompting techniques, while the paper addresses the broader issue of balancing safety and helpfulness during instruction tuning. The methods proposed are also different: the proposal suggests dynamic generation of context-aware prompts, while the paper investigates the impact of adding safety examples during fine-tuning.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "aa9aa1c315cb2a0c1759d82fb3d4b4506c2dbb7c",
            "paperId": "aa9aa1c315cb2a0c1759d82fb3d4b4506c2dbb7c",
            "title": "ASSERT: Automated Safety Scenario Red Teaming for Evaluating the Robustness of Large Language Models",
            "abstract": "As large language models are integrated into society, robustness toward a suite of prompts is increasingly important to maintain reliability in a high-variance environment.Robustness evaluations must comprehensively encapsulate the various settings in which a user may invoke an intelligent system. This paper proposes ASSERT, Automated Safety Scenario Red Teaming, consisting of three methods -- semantically aligned augmentation, target bootstrapping, and adversarial knowledge injection. For robust safety evaluation, we apply these methods in the critical domain of AI safety to algorithmically generate a test suite of prompts covering diverse robustness settings -- semantic equivalence, related scenarios, and adversarial. We partition our prompts into four safety domains for a fine-grained analysis of how the domain affects model performance. Despite dedicated safeguards in existing state-of-the-art models, we find statistically significant performance differences of up to 11% in absolute classification accuracy among semantically related scenarios and error rates of up to 19% absolute error in zero-shot adversarial settings, raising concerns for users' physical safety.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ASSERT, Automated Safety Scenario Red Teaming, consisting of three methods -- semantically aligned augmentation, target bootstrapping, and adversarial knowledge injection is proposed, for robust safety evaluation."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the effectiveness and flexibility of safety prompting for large language models by dynamically generating context-aware prompts. The proposed approach is to use LLMs to analyze the input context, identify potential risks, and generate customized safety prompts.\n\nThe research problem in the paper is evaluating the robustness of large language models in diverse safety scenarios. The proposed approach is to use automated methods like semantically aligned augmentation, target bootstrapping, and adversarial knowledge injection to generate a comprehensive test suite of prompts.\n\nWhile both works focus on the safety and robustness of LLMs, the proposal aims to improve safety prompting techniques, while the paper focuses on evaluating LLM robustness using generated test prompts. The methods proposed in the paper (ASSERT) are not directly related to the adaptive safety prompting approach in the proposal.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "d42625e5d8c0ba481fc58be28849c3231250aa0b",
            "paperId": "d42625e5d8c0ba481fc58be28849c3231250aa0b",
            "title": "JAB: Joint Adversarial Prompting and Belief Augmentation",
            "abstract": "With the recent surge of language models in different applications, attention to safety and robustness of these models has gained significant importance. Here we introduce a joint framework in which we simultaneously probe and improve the robustness of a black-box target model via adversarial prompting and belief augmentation using iterative feedback loops. This framework utilizes an automated red teaming approach to probe the target model, along with a belief augmenter to generate instructions for the target model to improve its robustness to those adversarial probes. Importantly, the adversarial model and the belief generator leverage the feedback from past interactions to improve the effectiveness of the adversarial prompts and beliefs, respectively. In our experiments, we demonstrate that such a framework can reduce toxic content generation both in dynamic cases where an adversary directly interacts with a target model and static cases where we use a static benchmark dataset to evaluate our model.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A joint framework in which an automated red teaming approach is used to probe and improve the robustness of a black-box target model via adversarial prompting and belief augmentation using iterative feedback loops is introduced."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the safety and robustness of large language models through adaptive safety prompting. The proposed approach is to dynamically generate context-aware safety prompts tailored to the input and the target model.\n\nThe research problem in the paper is also improving the safety and robustness of language models. However, the proposed approach is different. The paper uses a joint framework of adversarial prompting and belief augmentation with iterative feedback loops.\n\nThe proposal focuses on generating adaptive safety prompts, while the paper uses adversarial probes and belief augmentation. Although both aim to improve LLM safety, their methods are quite different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "930e86d49477c9d3305cd1f9d01b93749f85bb8b",
            "paperId": "930e86d49477c9d3305cd1f9d01b93749f85bb8b",
            "title": "Universal Self-adaptive Prompting",
            "abstract": "A hallmark of modern large language models (LLMs) is their impressive general zero-shot and few-shot abilities, often elicited through in-context learning (ICL) via prompting. However, while highly coveted and being the most general, zero-shot performances in LLMs are still typically weaker due to the lack of guidance and the difficulty of applying existing automatic prompt design methods in general tasks when ground-truth labels are unavailable. In this study, we address this by presenting Universal Self-Adaptive Prompting (USP), an automatic prompt design approach specifically tailored for zero-shot learning (while compatible with few-shot). Requiring only a small amount of unlabeled data and an inference-only LLM, USP is highly versatile: to achieve universal prompting, USP categorizes a possible NLP task into one of the three possible task types and then uses a corresponding selector to select the most suitable queries and zero-shot model-generated responses as pseudo-demonstrations, thereby generalizing ICL to the zero-shot setup in a fully automated way. We evaluate USP with PaLM and PaLM 2 models and demonstrate performances that are considerably stronger than standard zero-shot baselines and often comparable to or even superior to few-shot baselines across more than 40 natural language understanding, natural language generation, and reasoning tasks.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study presents Universal Self-Adaptive Prompting (USP), an automatic prompt design approach specifically tailored for zero-shot learning (while compatible with few-shot) and evaluates USP with PaLM and PaLM 2 models and demonstrates performances that are considerably stronger than standard zero- shot baselines."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the effectiveness and flexibility of safety prompting for large language models (LLMs) by dynamically generating context-aware safety prompts. The proposed approach is to use LLMs to analyze the input context, identify potential risks, and generate customized safety prompts relevant to the context and the target LLM.\n\nThe research problem in the paper is improving the zero-shot performance of LLMs through automatic prompt design. The proposed approach, Universal Self-Adaptive Prompting (USP), categorizes NLP tasks into three types and uses a corresponding selector to choose suitable queries and zero-shot model-generated responses as pseudo-demonstrations for in-context learning.\n\nWhile both the proposal and the paper aim to improve LLM performance through prompting techniques, their focus and approaches differ. The proposal specifically targets safety and robustness, while the paper addresses general zero-shot learning. Additionally, the proposal generates context-aware prompts based on risk identification, whereas the paper selects pseudo-demonstrations based on task categorization.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "894df6e5d0f34014e6a67b2d5f43e2bde91f63e3",
            "paperId": "894df6e5d0f34014e6a67b2d5f43e2bde91f63e3",
            "title": "Faithful Explanations of Black-box NLP Models Using LLM-generated Counterfactuals",
            "abstract": "Causal explanations of the predictions of NLP systems are essential to ensure safety and establish trust. Yet, existing methods often fall short of explaining model predictions effectively or efficiently and are often model-specific. In this paper, we address model-agnostic explanations, proposing two approaches for counterfactual (CF) approximation. The first approach is CF generation, where a large language model (LLM) is prompted to change a specific text concept while keeping confounding concepts unchanged. While this approach is demonstrated to be very effective, applying LLM at inference-time is costly. We hence present a second approach based on matching, and propose a method that is guided by an LLM at training-time and learns a dedicated embedding space. This space is faithful to a given causal graph and effectively serves to identify matches that approximate CFs. After showing theoretically that approximating CFs is required in order to construct faithful explanations, we benchmark our approaches and explain several models, including LLMs with billions of parameters. Our empirical results demonstrate the excellent performance of CF generation models as model-agnostic explainers. Moreover, our matching approach, which requires far less test-time resources, also provides effective explanations, surpassing many baselines. We also find that Top-K techniques universally improve every tested method. Finally, we showcase the potential of LLMs in constructing new benchmarks for model explanation and subsequently validate our conclusions. Our work illuminates new pathways for efficient and accurate approaches to interpreting NLP systems.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper addresses model-agnostic explanations, proposing two approaches for counterfactual (CF) approximation and presents a method that is guided by an LLM at training-time and learns a dedicated embedding space that effectively serves to identify matches that approximate CFs."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to dynamically generate context-aware safety prompts for large language models to improve their safety and robustness. The proposed approach involves using LLMs to analyze input context, identify potential risks, and generate customized safety prompts.\n\nThe paper focuses on generating faithful explanations for black-box NLP models using LLM-generated counterfactuals. The proposed approaches involve using LLMs to generate counterfactuals by changing specific text concepts while keeping confounding concepts unchanged, and learning a dedicated embedding space guided by an LLM at training time to identify matches that approximate counterfactuals.\n\nThe project proposal and the paper address different research problems and propose different approaches. The project proposal focuses on improving LLM safety and robustness through adaptive safety prompting, while the paper aims to provide faithful explanations for black-box NLP models using counterfactual generation and matching.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "b67eb8213a63be8a4b0274728ffdc50bfa109e10",
            "paperId": "b67eb8213a63be8a4b0274728ffdc50bfa109e10",
            "title": "XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models",
            "abstract": "Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content. This risk motivates safety efforts such as red-teaming and large-scale feedback learning, which aim to make models both helpful and harmless. However, there is a tension between these two objectives, since harmlessness requires models to refuse to comply with unsafe prompts, and thus not be helpful. Recent anecdotal evidence suggests that some models may have struck a poor balance, so that even clearly safe prompts are refused if they use similar language to unsafe prompts or mention sensitive topics. In this paper, we introduce a new test suite called XSTest to identify such eXaggerated Safety behaviours in a systematic way. XSTest comprises 250 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with, and 200 unsafe prompts as contrasts that models, for most applications, should refuse. We describe XSTest's creation and composition, and then use the test suite to highlight systematic failure modes in state-of-the-art language models as well as more general challenges in building safer language models.",
            "year": 2023,
            "citationCount": 26,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new test suite called XSTest is introduced to identify eXaggerated Safety behaviours in state-of-the-art language models as well as more general challenges in building safer language models."
            },
            "score": 5,
            "novelty_score": "The research problem in the project proposal is improving the effectiveness and flexibility of safety prompting for large language models by dynamically generating context-aware safety prompts. The proposed approach is to use LLMs to analyze the input context, identify potential safety risks, and generate customized safety prompts relevant to the context and the target LLM.\n\nThe research problem in the paper is identifying exaggerated safety behaviors in large language models, where models refuse to comply with safe prompts if they use similar language to unsafe prompts or mention sensitive topics. The approach is to introduce a new test suite called XSTest, which comprises safe and unsafe prompts to systematically identify such behaviors.\n\nThe project proposal focuses on improving safety prompting, while the paper focuses on identifying exaggerated safety behaviors. The approaches are different: the proposal uses LLMs to generate context-aware safety prompts, while the paper uses a test suite to identify problematic behaviors.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "c11ac248290acf6bba414cd6d6be25df6c7eacf4",
            "paperId": "c11ac248290acf6bba414cd6d6be25df6c7eacf4",
            "title": "SimpleSafetyTests: a Test Suite for Identifying Critical Safety Risks in Large Language Models",
            "abstract": "The past year has seen rapid acceleration in the development of large language models (LLMs). However, without proper steering and safeguards, LLMs will readily follow malicious instructions, provide unsafe advice, and generate toxic content. We introduce SimpleSafetyTests (SST) as a new test suite for rapidly and systematically identifying such critical safety risks. The test suite comprises 100 test prompts across five harm areas that LLMs, for the vast majority of applications, should refuse to comply with. We test 11 open-access and open-source LLMs and four closed-source LLMs, and find critical safety weaknesses. While some of the models do not give a single unsafe response, most give unsafe responses to more than 20% of the prompts, with over 50% unsafe responses in the extreme. Prepending a safety-emphasising system prompt substantially reduces the occurrence of unsafe responses, but does not completely stop them from happening. Trained annotators labelled every model response to SST (n = 3,000). We use these annotations to evaluate five AI safety filters (which assess whether a models' response is unsafe given a prompt) as a way of automatically evaluating models' performance on SST. The filters' performance varies considerably. There are also differences across the five harm areas, and on the unsafe versus safe responses. The widely-used Perspective API has 72% accuracy and a newly-created zero-shot prompt to OpenAI's GPT-4 performs best with 89% accuracy. Content Warning: This paper contains prompts and responses that relate to child abuse, suicide, self-harm and eating disorders, scams and fraud, illegal items, and physical harm.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "SimpleSafetyTests (SST) is introduced as a new test suite for rapidly and systematically identifying critical safety risks in large language models (LLMs), and finds critical safety weaknesses."
            },
            "score": 5
        },
        {
            "id": "e01515c6138bc525f7aec30fc85f2adf028d4156",
            "paperId": "e01515c6138bc525f7aec30fc85f2adf028d4156",
            "title": "Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision",
            "abstract": "Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and guide the LLM through in-context learning from demonstrations (of principles application) to produce helpful, ethical, and reliable responses to user's queries; third, we fine-tune the original LLM with the high-quality self-aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally, we offer a refinement step to address the issues of overly-brief or indirect responses. Applying SELF-ALIGN to the LLaMA-65b base language model, we develop an AI assistant named Dromedary. With fewer than 300 lines of human annotations (including<200 seed prompts, 16 generic principles, and 5 exemplars for in-context learning). Dromedary significantly surpasses the performance of several state-of-the-art AI systems, including Text-Davinci-003 and Alpaca, on benchmark datasets with various settings.",
            "year": 2023,
            "citationCount": 137,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An AI assistant named Dromedary is developed, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision and significantly surpasses the performance of several state-of-the-art AI systems on benchmark datasets with various settings."
            },
            "score": 5
        },
        {
            "id": "d235a9085e0543fcbe502fbc269f9a8ee01dcbab",
            "paperId": "d235a9085e0543fcbe502fbc269f9a8ee01dcbab",
            "title": "AdaPrompt: Adaptive Model Training for Prompt-based NLP",
            "abstract": "Prompt-based learning, with its capability to tackle zero-shot and few-shot NLP tasks, has gained much attention in community. The main idea is to bridge the gap between NLP downstream tasks and language modeling (LM), by mapping these tasks into natural language prompts, which are then filled by pre-trained language models (PLMs). However, for prompt learning, there are still two salient gaps between NLP tasks and pretraining. First, prompt information is not necessarily sufficiently present during LM pretraining. Second, task-specific data are not necessarily well represented during pretraining. We address these two issues by proposing AdaPrompt, adaptively retrieving external data for continual pretraining of PLMs by making use of both task and prompt characteristics. In addition, we make use of knowledge in Natural Language Inference models for deriving adaptive verbalizers. Experimental results on five NLP benchmarks show that AdaPrompt can improve over standard PLMs in few-shot settings. In addition, in zero-shot settings, our method outperforms standard prompt-based methods by up to 26.35\\% relative error reduction.",
            "year": 2022,
            "citationCount": 35,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results on five NLP benchmarks show that AdaPrompt can improve over standard PLMs in few-shot settings and make use of knowledge in Natural Language Inference models for deriving adaptive verbalizers."
            },
            "score": 5
        },
        {
            "id": "ccfdea3722ec43e45fc33e170d1fe908dca003a5",
            "paperId": "ccfdea3722ec43e45fc33e170d1fe908dca003a5",
            "title": "TransPrompt v2: A Transferable Prompting Framework for Cross-task Text Classification",
            "abstract": "Text classification is one of the most imperative tasks in natural language processing (NLP). Recent advances with pre-trained language models (PLMs) have shown remarkable success on this task. However, the satisfying results obtained by PLMs heavily depend on the large amounts of task-specific labeled data, which may not be feasible in many application scenarios due to data access and privacy constraints. The recently-proposed prompt-based fine-tuning paradigm improves the performance of PLMs for few-shot text classification with task-specific templates. Yet, it is unclear how the prompting knowledge can be transferred across tasks, for the purpose of mutual reinforcement. We propose TransPrompt v2, a novel transferable prompting framework for few-shot learning across similar or distant text classification tasks. For learning across similar tasks, we employ a multi-task meta-knowledge acquisition (MMA) procedure to train a meta-learner that captures the cross-task transferable knowledge. For learning across distant tasks, we further inject the task type descriptions into the prompt, and capture the intra-type and inter-type prompt embeddings among multiple distant tasks. Additionally, two de-biasing techniques are further designed to make the trained meta-learner more task-agnostic and unbiased towards any tasks. After that, the meta-learner can be adapted to each specific task with better parameters initialization. Extensive experiments show that TransPrompt v2 outperforms single-task and cross-task strong baselines over multiple NLP tasks and datasets. We further show that the meta-learner can effectively improve the performance of PLMs on previously unseen tasks. In addition, TransPrompt v2 also outperforms strong fine-tuning baselines when learning with full training sets.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Extensive experiments show that TransPrompt v2 outperforms single-task and cross-task strong baselines over multiple NLP tasks and datasets, and shows that the meta-learner can effectively improve the performance of PLMs on previously unseen tasks."
            },
            "score": 5
        },
        {
            "id": "d4177489596748e43aa571f59556097f2cc4c8be",
            "paperId": "d4177489596748e43aa571f59556097f2cc4c8be",
            "title": "GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts",
            "abstract": "Large language models (LLMs) have recently experienced tremendous popularity and are widely used from casual conversations to AI-driven programming. However, despite their considerable success, LLMs are not entirely reliable and can give detailed guidance on how to conduct harmful or illegal activities. While safety measures can reduce the risk of such outputs, adversarial jailbreak attacks can still exploit LLMs to produce harmful content. These jailbreak templates are typically manually crafted, making large-scale testing challenging. In this paper, we introduce GPTFuzz, a novel black-box jailbreak fuzzing framework inspired by the AFL fuzzing framework. Instead of manual engineering, GPTFuzz automates the generation of jailbreak templates for red-teaming LLMs. At its core, GPTFuzz starts with human-written templates as initial seeds, then mutates them to produce new templates. We detail three key components of GPTFuzz: a seed selection strategy for balancing efficiency and variability, mutate operators for creating semantically equivalent or similar sentences, and a judgment model to assess the success of a jailbreak attack. We evaluate GPTFuzz against various commercial and open-source LLMs, including ChatGPT, LLaMa-2, and Vicuna, under diverse attack scenarios. Our results indicate that GPTFuzz consistently produces jailbreak templates with a high success rate, surpassing human-crafted templates. Remarkably, GPTFuzz achieves over 90% attack success rates against ChatGPT and Llama-2 models, even with suboptimal initial seed templates. We anticipate that GPTFuzz will be instrumental for researchers and practitioners in examining LLM robustness and will encourage further exploration into enhancing LLM safety.",
            "year": 2023,
            "citationCount": 78,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "GPTFuzz is introduced, a novel black-box jailbreak fuzzing framework inspired by the AFL fuzzed framework that automates the generation of jailbreak templates for red-teaming LLMs and consistently produces jailbreaks with a high success rate, surpassing human-crafted templates."
            },
            "score": 4
        },
        {
            "id": "d030be820dd5e4739461f246ce248fba2df33f0a",
            "paperId": "d030be820dd5e4739461f246ce248fba2df33f0a",
            "title": "Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment",
            "abstract": "To ensure AI safety, instruction-tuned Large Language Models (LLMs) are specifically trained to ensure alignment, which refers to making models behave in accordance with human intentions. While these models have demonstrated commendable results on various safety benchmarks, the vulnerability of their safety alignment has not been extensively studied. This is particularly troubling given the potential harm that LLMs can inflict. Existing attack methods on LLMs often rely on poisoned training data or the injection of malicious prompts. These approaches compromise the stealthiness and generalizability of the attacks, making them susceptible to detection. Additionally, these models often demand substantial computational resources for implementation, making them less practical for real-world applications. Inspired by recent success in modifying model behavior through steering vectors without the need for optimization, and drawing on its effectiveness in red-teaming LLMs, we conducted experiments employing activation steering to target four key aspects of LLMs: truthfulness, toxicity, bias, and harmfulness - across a varied set of attack settings. To establish a universal attack strategy applicable to diverse target alignments without depending on manual analysis, we automatically select the intervention layer based on contrastive layer search. Our experiment results show that activation attacks are highly effective and add little or no overhead to attack efficiency. Additionally, we discuss potential countermeasures against such activation attacks. Our code and data are available at https://github.com/wang2226/Backdoor-Activation-Attack Warning: this paper contains content that can be offensive or upsetting.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Inspired by recent success in modifying model behavior through steering vectors without the need for optimization, and drawing on its effectiveness in red-teaming LLMs, experiments employing activation steering to target four key aspects of LLMs - truthfulness, toxicity, bias, and harmfulness - across a varied set of attack settings are conducted."
            },
            "score": 4
        },
        {
            "id": "b5a624da64475d735f0e298dc6f2f6669b5bb697",
            "paperId": "b5a624da64475d735f0e298dc6f2f6669b5bb697",
            "title": "Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield",
            "abstract": "Large Language Models' safety remains a critical concern due to their vulnerability to adversarial attacks, which can prompt these systems to produce harmful responses. In the heart of these systems lies a safety classifier, a computational model trained to discern and mitigate potentially harmful, offensive, or unethical outputs. However, contemporary safety classifiers, despite their potential, often fail when exposed to inputs infused with adversarial noise. In response, our study introduces the Adversarial Prompt Shield (APS), a lightweight model that excels in detection accuracy and demonstrates resilience against adversarial prompts. Additionally, we propose novel strategies for autonomously generating adversarial training datasets, named Bot Adversarial Noisy Dialogue (BAND) datasets. These datasets are designed to fortify the safety classifier's robustness, and we investigate the consequences of incorporating adversarial examples into the training process. Through evaluations involving Large Language Models, we demonstrate that our classifier has the potential to decrease the attack success rate resulting from adversarial attacks by up to 60%. This advancement paves the way for the next generation of more reliable and resilient conversational agents.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study introduces the Adversarial Prompt Shield (APS), a lightweight model that excels in detection accuracy and demonstrates resilience against adversarial prompts, and proposes novel strategies for autonomously generating adversarial training datasets, designed to fortify the safety classifier's robustness."
            },
            "score": 4
        },
        {
            "id": "6b135e922a0c673aeb0b05c5aeecdb6c794791c6",
            "paperId": "6b135e922a0c673aeb0b05c5aeecdb6c794791c6",
            "title": "Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations",
            "abstract": "Large Language Models (LLMs) have shown remarkable success in various tasks, but concerns about their safety and the potential for generating malicious content have emerged. In this paper, we explore the power of In-Context Learning (ICL) in manipulating the alignment ability of LLMs. We find that by providing just few in-context demonstrations without fine-tuning, LLMs can be manipulated to increase or decrease the probability of jailbreaking, i.e. answering malicious prompts. Based on these observations, we propose In-Context Attack (ICA) and In-Context Defense (ICD) methods for jailbreaking and guarding aligned language model purposes. ICA crafts malicious contexts to guide models in generating harmful outputs, while ICD enhances model robustness by demonstrations of rejecting to answer harmful prompts. Our experiments show the effectiveness of ICA and ICD in increasing or reducing the success rate of adversarial jailbreaking attacks. Overall, we shed light on the potential of ICL to influence LLM behavior and provide a new perspective for enhancing the safety and alignment of LLMs.",
            "year": 2023,
            "citationCount": 59,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Light is shed on the potential of In-Context Learning (ICL) to influence LLM behavior and a new perspective for enhancing the safety and alignment of LLMs is provided."
            },
            "score": 4
        },
        {
            "id": "ee025d7030d4767062af2bcd32a4d586737d30bf",
            "paperId": "ee025d7030d4767062af2bcd32a4d586737d30bf",
            "title": "Tryage: Real-time, intelligent Routing of User Prompts to Large Language Models",
            "abstract": "The introduction of the transformer architecture and the self-attention mechanism has led to an explosive production of language models trained on specific downstream tasks and data domains. With over 200, 000 models in the Hugging Face ecosystem, users grapple with selecting and optimizing models to suit multifaceted workflows and data domains while addressing computational, security, and recency concerns. There is an urgent need for machine learning frameworks that can eliminate the burden of model selection and customization and unleash the incredible power of the vast emerging model library for end users. Here, we propose a context-aware routing system, Tryage, that leverages a language model router for optimal selection of expert models from a model library based on analysis of individual input prompts. Inspired by the thalamic router in the brain, Tryage employs a perceptive router to predict down-stream model performance on prompts and, then, makes a routing decision using an objective function that integrates performance predictions with user goals and constraints that are incorporated through flags (e.g., model size, model recency). Tryage allows users to explore a Pareto front and automatically trade-off between task accuracy and secondary goals including minimization of model size, recency, security, verbosity, and readability. Across heterogeneous data sets that include code, text, clinical data, and patents, the Tryage framework surpasses Gorilla and GPT3.5 turbo in dynamic model selection identifying the optimal model with an accuracy of 50.9% , compared to 23.6% by GPT 3.5 Turbo and 10.8% by Gorilla. Conceptually, Tryage demonstrates how routing models can be applied to program and control the behavior of multi-model LLM systems to maximize efficient use of the expanding and evolving language model ecosystem.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A context-aware routing system that leverages a language model router for optimal selection of expert models from a model library based on analysis of individual input prompts, Tryage demonstrates how routing models can be applied to program and control the behavior of multi-model LLM systems to maximize efficient use of the expanding and evolving language model ecosystem."
            },
            "score": 4
        },
        {
            "id": "12c826f4195da172b212a529f8fcf10cc79e35da",
            "paperId": "12c826f4195da172b212a529f8fcf10cc79e35da",
            "title": "Context-faithful Prompting for Large Language Models",
            "abstract": "Large language models (LLMs) encode parametric knowledge about world facts and have shown remarkable performance in knowledge-driven NLP tasks. However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks). In this paper, we seek to assess and enhance LLMs' contextual faithfulness in two aspects: knowledge conflict and prediction with abstention. We demonstrate that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies. In particular, we identify opinion-based prompts and counterfactual demonstrations as the most effective methods. Opinion-based prompts reframe the context as a narrator's statement and inquire about the narrator's opinions, while counterfactual demonstrations use instances containing false facts to improve faithfulness in knowledge conflict situations. Neither technique requires additional training. We conduct experiments on three datasets of two standard NLP tasks, machine reading comprehension and relation extraction, and the results demonstrate significant improvement in faithfulness to contexts. Code and data are released at https://github.com/wzhouad/context-faithful-llm.",
            "year": 2023,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies, and opinion-based prompts and counterfactual demonstrations are identified as the most effective methods."
            },
            "score": 4
        },
        {
            "id": "8f936af93fb2b52b9678ff8f17c1ebe8de236a88",
            "paperId": "8f936af93fb2b52b9678ff8f17c1ebe8de236a88",
            "title": "Large Language Models Can be Lazy Learners: Analyze Shortcuts in In-Context Learning",
            "abstract": "Large language models (LLMs) have recently shown great potential for in-context learning, where LLMs learn a new task simply by conditioning on a few input-label pairs (prompts). Despite their potential, our understanding of the factors influencing end-task performance and the robustness of in-context learning remains limited. This paper aims to bridge this knowledge gap by investigating the reliance of LLMs on shortcuts or spurious correlations within prompts. Through comprehensive experiments on classification and extraction tasks, we reveal that LLMs are\"lazy learners\"that tend to exploit shortcuts in prompts for downstream tasks. Additionally, we uncover a surprising finding that larger models are more likely to utilize shortcuts in prompts during inference. Our findings provide a new perspective on evaluating robustness in in-context learning and pose new challenges for detecting and mitigating the use of shortcuts in prompts.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is revealed that large language models are \"lazy learners\" that tend to exploit shortcuts in prompts for downstream tasks and a surprising finding is uncovered that larger models are more likely to utilize shortcuts in Prompts during inference."
            },
            "score": 4
        },
        {
            "id": "984d4a1d41bfc8184fb77b8aa0eb8e96d536d048",
            "paperId": "984d4a1d41bfc8184fb77b8aa0eb8e96d536d048",
            "title": "Trusting Your Evidence: Hallucinate Less with Context-aware Decoding",
            "abstract": "Language models (LMs) often struggle to pay enough attention to the input context, and generate texts that are unfaithful or contain hallucinations. To mitigate this issue, we present context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context. Our experiments show that CAD, without additional training, significantly improves the faithfulness of different LM families, including OPT, GPT, LLaMA and FLAN-T5 for summarization tasks (e.g., 14.3% gain for LLaMA in factuality metrics). Furthermore, CAD is particularly effective in overriding a model's prior knowledge when it contradicts the provided context, leading to substantial improvements in tasks where resolving the knowledge conflict is essential.",
            "year": 2023,
            "citationCount": 65,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context, improves the faithfulness of different LM families."
            },
            "score": 4
        },
        {
            "id": "9f859726b3d8dffd96a1f55de4122617751cc1b4",
            "paperId": "9f859726b3d8dffd96a1f55de4122617751cc1b4",
            "title": "Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment",
            "abstract": "Larger language models (LLMs) have taken the world by storm with their massive multi-tasking capabilities simply by optimizing over a next-word prediction objective. With the emergence of their properties and encoded knowledge, the risk of LLMs producing harmful outputs increases, making them unfit for scalable deployment for the public. In this work, we propose a new safety evaluation benchmark RED-EVAL that carries out red-teaming. We show that even widely deployed models are susceptible to the Chain of Utterances-based (CoU) prompting, jailbreaking closed source LLM-based systems such as GPT-4 and ChatGPT to unethically respond to more than 65% and 73% of harmful queries. We also demonstrate the consistency of the RED-EVAL across 8 open-source LLMs in generating harmful responses in more than 86% of the red-teaming attempts. Next, we propose RED-INSTRUCT--An approach for the safety alignment of LLMs. It constitutes two phases: 1) HARMFULQA data collection: Leveraging CoU prompting, we collect a dataset that consists of 1.9K harmful questions covering a wide range of topics, 9.5K safe and 7.3K harmful conversations from ChatGPT; 2) SAFE-ALIGN: We demonstrate how the conversational dataset can be used for the safety alignment of LLMs by minimizing the negative log-likelihood over helpful responses and penalizing over harmful responses by gradient accent over sample loss. Our model STARLING, a fine-tuned Vicuna-7B, is observed to be more safely aligned when evaluated on RED-EVAL and HHH benchmarks while preserving the utility of the baseline models (TruthfulQA, MMLU, and BBH).",
            "year": 2023,
            "citationCount": 48,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that even widely deployed models are susceptible to the Chain of Utterances-based (CoU) prompting, jailbreaking closed source LLM-based systems such as GPT-4 and ChatGPT to unethically respond to more than 65% and 73% of harmful queries."
            },
            "score": 4
        },
        {
            "id": "9141480721653789597b6e537ee0eeab401f3e60",
            "paperId": "9141480721653789597b6e537ee0eeab401f3e60",
            "title": "PromptNER: Prompting For Named Entity Recognition",
            "abstract": "In a surprising turn, Large Language Models (LLMs) together with a growing arsenal of prompt-based heuristics now offer powerful off-the-shelf approaches providing few-shot solutions to myriad classic NLP problems. However, despite promising early results, these LLM-based few-shot methods remain far from the state of the art in Named Entity Recognition (NER), where prevailing methods include learning representations via end-to-end structural understanding and fine-tuning on standard labeled corpora. In this paper, we introduce PromptNER, a new state-of-the-art algorithm for few-Shot and cross-domain NER. To adapt to any new NER task PromptNER requires a set of entity definitions in addition to the standard few-shot examples. Given a sentence, PromptNER prompts an LLM to produce a list of potential entities along with corresponding explanations justifying their compatibility with the provided entity type definitions. Remarkably, PromptNER achieves state-of-the-art performance on few-shot NER, achieving a 4% (absolute) improvement in F1 score on the ConLL dataset, a 9% (absolute) improvement on the GENIA dataset, and a 4% (absolute) improvement on the FewNERD dataset. PromptNER also moves the state of the art on Cross Domain NER, outperforming prior methods (including those not limited to the few-shot setting), setting a new mark on 3/5 CrossNER target domains, with an average F1 gain of 3%, despite using less than 2% of the available data.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces PromptNER, a new state-of-the-art algorithm for few-Shot and cross-domain NER, and prompts an LLM to produce a list of potential entities along with corresponding explanations justifying their compatibility with the provided entity type definitions."
            },
            "score": 4
        },
        {
            "id": "2522410b1cac0c14fa656a0aaeaff08bacb358a9",
            "paperId": "2522410b1cac0c14fa656a0aaeaff08bacb358a9",
            "title": "InterroLang: Exploring NLP Models and Datasets through Dialogue-based Explanations",
            "abstract": "While recently developed NLP explainability methods let us open the black box in various ways (Madsen et al., 2022), a missing ingredient in this endeavor is an interactive tool offering a conversational interface. Such a dialogue system can help users explore datasets and models with explanations in a contextualized manner, e.g. via clarification or follow-up questions, and through a natural language interface. We adapt the conversational explanation framework TalkToModel (Slack et al., 2022) to the NLP domain, add new NLP-specific operations such as free-text rationalization, and illustrate its generalizability on three NLP tasks (dialogue act classification, question answering, hate speech detection). To recognize user queries for explanations, we evaluate fine-tuned and few-shot prompting models and implement a novel Adapter-based approach. We then conduct two user studies on (1) the perceived correctness and helpfulness of the dialogues, and (2) the simulatability, i.e. how objectively helpful dialogical explanations are for humans in figuring out the model's predicted label when it's not shown. We found rationalization and feature attribution were helpful in explaining the model behavior. Moreover, users could more reliably predict the model outcome based on an explanation dialogue rather than one-off explanations.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work adapts the conversational explanation framework TalkToModel to the NLP domain, adds new NLP-specific operations such as free-text rationalization and feature attribution, and illustrates its generalizability on three NLP tasks."
            },
            "score": 4
        },
        {
            "id": "66b217600c49195ce01cb24d068ebae5ca7bfd63",
            "paperId": "66b217600c49195ce01cb24d068ebae5ca7bfd63",
            "title": "Contextual Dynamic Prompting for Response Generation in Task-oriented Dialog Systems",
            "abstract": "Response generation is one of the critical components in task-oriented dialog systems. Existing studies have shown that large pre-trained language models can be adapted to this task. The typical paradigm of adapting such extremely large language models would be by fine-tuning on the downstream tasks which is not only time-consuming but also involves significant resources and access to fine-tuning data. Prompting (Schick and Sch\u00fctze, 2020) has been an alternative to fine-tuning in many NLP tasks. In our work, we explore the idea of using prompting for response generation in task-oriented dialog systems. Specifically, we propose an approach that performs contextual dynamic prompting where the prompts are learnt from dialog contexts. We aim to distill useful prompting signals from the dialog context. On experiments with MultiWOZ 2.2 dataset (Zang et al., 2020), we show that contextual dynamic prompts improve response generation in terms of combined score (Mehri et al., 2019) by 3 absolute points, and an additional 17 points when dialog states are incorporated. Furthermore, we carried out human annotation on these conversations and found that agents which incorporate context are preferred over agents with vanilla prefix-tuning.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An approach that performs contextual dynamic prompting where the prompts are learnt from dialog contexts is proposed and found that agents which incorporate context are preferred over agents with vanilla prefix-tuning."
            },
            "score": 4
        },
        {
            "id": "bc0549a5f07474c18987c219ecf367fb73a1b79c",
            "paperId": "bc0549a5f07474c18987c219ecf367fb73a1b79c",
            "title": "CValues: Measuring the Values of Chinese Large Language Models from Safety to Responsibility",
            "abstract": "With the rapid evolution of large language models (LLMs), there is a growing concern that they may pose risks or have negative social impacts. Therefore, evaluation of human values alignment is becoming increasingly important. Previous work mainly focuses on assessing the performance of LLMs on certain knowledge and reasoning abilities, while neglecting the alignment to human values, especially in a Chinese context. In this paper, we present CValues, the first Chinese human values evaluation benchmark to measure the alignment ability of LLMs in terms of both safety and responsibility criteria. As a result, we have manually collected adversarial safety prompts across 10 scenarios and induced responsibility prompts from 8 domains by professional experts. To provide a comprehensive values evaluation of Chinese LLMs, we not only conduct human evaluation for reliable comparison, but also construct multi-choice prompts for automatic evaluation. Our findings suggest that while most Chinese LLMs perform well in terms of safety, there is considerable room for improvement in terms of responsibility. Moreover, both the automatic and human evaluation are important for assessing the human values alignment in different aspects. The benchmark and code is available on ModelScope and Github.",
            "year": 2023,
            "citationCount": 32,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "CValues is presented, the first Chinese human values evaluation benchmark to measure the alignment ability of LLMs in terms of both safety and responsibility criteria, and suggests that while most Chinese LLMs perform well interms of safety, there is considerable room for improvement in termsof responsibility."
            },
            "score": 3
        },
        {
            "id": "59fc49dfd81b92661437eaf7e339c0792ccd8755",
            "paperId": "59fc49dfd81b92661437eaf7e339c0792ccd8755",
            "title": "Safety Assessment of Chinese Large Language Models",
            "abstract": "With the rapid popularity of large language models such as ChatGPT and GPT-4, a growing amount of attention is paid to their safety concerns. These models may generate insulting and discriminatory content, reflect incorrect social values, and may be used for malicious purposes such as fraud and dissemination of misleading information. Evaluating and enhancing their safety is particularly essential for the wide application of large language models (LLMs). To further promote the safe deployment of LLMs, we develop a Chinese LLM safety assessment benchmark. Our benchmark explores the comprehensive safety performance of LLMs from two perspectives: 8 kinds of typical safety scenarios and 6 types of more challenging instruction attacks. Our benchmark is based on a straightforward process in which it provides the test prompts and evaluates the safety of the generated responses from the evaluated model. In evaluation, we utilize the LLM's strong evaluation ability and develop it as a safety evaluator by prompting. On top of this benchmark, we conduct safety assessments and analyze 15 LLMs including the OpenAI GPT series and other well-known Chinese LLMs, where we observe some interesting findings. For example, we find that instruction attacks are more likely to expose safety issues of all LLMs. Moreover, to promote the development and deployment of safe, responsible, and ethical AI, we publicly release SafetyPrompts including 100k augmented prompts and responses by LLMs.",
            "year": 2023,
            "citationCount": 36,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Chinese LLM safety assessment benchmark is developed based on the LLM's strong evaluation ability and develops it as a safety evaluator by prompting, which finds that instruction attacks are more likely to expose safety issues of all LLMs."
            },
            "score": 3
        },
        {
            "id": "2f8a1714e3bb01c522fede9c902bd7cf5320a802",
            "paperId": "2f8a1714e3bb01c522fede9c902bd7cf5320a802",
            "title": "Test-Time Self-Adaptive Small Language Models for Question Answering",
            "abstract": "Recent instruction-finetuned large language models (LMs) have achieved notable performances in various tasks, such as question-answering (QA). However, despite their ability to memorize a vast amount of general knowledge across diverse tasks, they might be suboptimal on specific tasks due to their limited capacity to transfer and adapt knowledge to target tasks. Moreover, further finetuning LMs with labeled datasets is often infeasible due to their absence, but it is also questionable if we can transfer smaller LMs having limited knowledge only with unlabeled test data. In this work, we show and investigate the capabilities of smaller self-adaptive LMs, only with unlabeled test data. In particular, we first stochastically generate multiple answers, and then ensemble them while filtering out low-quality samples to mitigate noise from inaccurate labels. Our proposed self-adaption strategy demonstrates significant performance improvements on benchmark QA datasets with higher robustness across diverse prompts, enabling LMs to stay stable. Code is available at: https://github.com/starsuzi/T-SAS.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work shows and investigates the capabilities of smaller self-adaptive LMs, only with unlabeled test data, and proposes a self-adaption strategy that demonstrates significant performance improvements on benchmark QA datasets with higher robustness across diverse prompts, enabling LMs to stay stable."
            },
            "score": 3
        },
        {
            "id": "1a9f394b5b7f5bcdecee487174a3f4fc65d30e33",
            "paperId": "1a9f394b5b7f5bcdecee487174a3f4fc65d30e33",
            "title": "Multilingual Jailbreak Challenges in Large Language Models",
            "abstract": "While large language models (LLMs) exhibit remarkable capabilities across a wide range of tasks, they pose potential safety concerns, such as the ``jailbreak'' problem, wherein malicious instructions can manipulate LLMs to exhibit undesirable behavior. Although several preventive measures have been developed to mitigate the potential risks associated with LLMs, they have primarily focused on English. In this study, we reveal the presence of multilingual jailbreak challenges within LLMs and consider two potential risky scenarios: unintentional and intentional. The unintentional scenario involves users querying LLMs using non-English prompts and inadvertently bypassing the safety mechanisms, while the intentional scenario concerns malicious users combining malicious instructions with multilingual prompts to deliberately attack LLMs. The experimental results reveal that in the unintentional scenario, the rate of unsafe content increases as the availability of languages decreases. Specifically, low-resource languages exhibit about three times the likelihood of encountering harmful content compared to high-resource languages, with both ChatGPT and GPT-4. In the intentional scenario, multilingual prompts can exacerbate the negative impact of malicious instructions, with astonishingly high rates of unsafe output: 80.92\\% for ChatGPT and 40.71\\% for GPT-4. To handle such a challenge in the multilingual context, we propose a novel \\textsc{Self-Defense} framework that automatically generates multilingual training data for safety fine-tuning. Experimental results show that ChatGPT fine-tuned with such data can achieve a substantial reduction in unsafe content generation. Data is available at \\url{https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs}.",
            "year": 2023,
            "citationCount": 30,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results show that ChatGPT fine-tuned with such data can achieve a substantial reduction in unsafe content generation, and a novel self-defense framework that automatically generates multilingual training data for safety fine-tuning is proposed."
            },
            "score": 3
        },
        {
            "id": "68850153b0210615c86f9a72624f34e2913bcddf",
            "paperId": "68850153b0210615c86f9a72624f34e2913bcddf",
            "title": "Document-Level Machine Translation with Large Language Models",
            "abstract": "Large language models (LLMs) such as ChatGPT can produce coherent, cohesive, relevant, and fluent answers for various natural language processing (NLP) tasks. Taking document-level machine translation (MT) as a testbed, this paper provides an in-depth evaluation of LLMs' ability on discourse modeling. The study focuses on three aspects: 1) Effects of Context-Aware Prompts, where we investigate the impact of different prompts on document-level translation quality and discourse phenomena; 2) Comparison of Translation Models, where we compare the translation performance of ChatGPT with commercial MT systems and advanced document-level MT methods; 3) Analysis of Discourse Modelling Abilities, where we further probe discourse knowledge encoded in LLMs and shed light on impacts of training techniques on discourse modeling. By evaluating on a number of benchmarks, we surprisingly find that LLMs have demonstrated superior performance and show potential to become a new paradigm for document-level translation: 1) leveraging their powerful long-text modeling capabilities, GPT-3.5 and GPT-4 outperform commercial MT systems in terms of human evaluation; 2) GPT-4 demonstrates a stronger ability for probing linguistic knowledge than GPT-3.5. This work highlights the challenges and opportunities of LLMs for MT, which we hope can inspire the future design and evaluation of LLMs.We release our data and annotations at https://github.com/longyuewangdcu/Document-MT-LLM.",
            "year": 2023,
            "citationCount": 53,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An in-depth evaluation of LLMs' ability on discourse modeling using document-level machine translation (MT) as a testbed finds that LLMs have demonstrated superior performance and show potential to become a new paradigm for document- level translation."
            },
            "score": 3
        },
        {
            "id": "16168520c7efbfa84bcb609a05362916b04022bb",
            "paperId": "16168520c7efbfa84bcb609a05362916b04022bb",
            "title": "Context-Aware Abbreviation Expansion Using Large Language Models",
            "abstract": "Motivated by the need for accelerating text entry in augmentative and alternative communication (AAC) for people with severe motor impairments, we propose a paradigm in which phrases are abbreviated aggressively as primarily word-initial letters. Our approach is to expand the abbreviations into full-phrase options by leveraging conversation context with the power of pretrained large language models (LLMs). Through zero-shot, few-shot, and fine-tuning experiments on four public conversation datasets, we show that for replies to the initial turn of a dialog, an LLM with 64B parameters is able to exactly expand over 70% of phrases with abbreviation length up to 10, leading to an effective keystroke saving rate of up to about 77% on these exact expansions. Including a small amount of context in the form of a single conversation turn more than doubles abbreviation expansion accuracies compared to having no context, an effect that is more pronounced for longer phrases. Additionally, the robustness of models against typo noise can be enhanced through fine-tuning on noisy data.",
            "year": 2022,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a paradigm in which phrases are abbreviated aggressively as primarily word-initial letters, and aims to expand the abbreviations into full-phrase options by leveraging conversation context with the power of pretrained large language models (LLMs)."
            },
            "score": 3
        },
        {
            "id": "2392b6d3a5cad9e5cf349169eaeee848266adf6a",
            "paperId": "2392b6d3a5cad9e5cf349169eaeee848266adf6a",
            "title": "LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models",
            "abstract": "Large language models (LLMs) have been applied in various applications due to their astonishing capabilities. With advancements in technologies such as chain-of-thought (CoT) prompting and in-context learning (ICL), the prompts fed to LLMs are becoming increasingly lengthy, even exceeding tens of thousands of tokens. To accelerate model inference and reduce cost, this paper presents LLMLingua, a coarse-to-fine prompt compression method that involves a budget controller to maintain semantic integrity under high compression ratios, a token-level iterative compression algorithm to better model the interdependence between compressed contents, and an instruction tuning based method for distribution alignment between language models. We conduct experiments and analysis over four datasets from different scenarios, i.e., GSM8K, BBH, ShareGPT, and Arxiv-March23; showing that the proposed approach yields state-of-the-art performance and allows for up to 20x compression with little performance loss. Our code is available at https://aka.ms/LLMLingua.",
            "year": 2023,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A coarse-to-fine prompt compression method that involves a budget controller to maintain semantic integrity under high compression ratios, a token-level iterative compression algorithm to better model the interdependence between compressed contents, and an instruction tuning based method for distribution alignment between language models."
            },
            "score": 3
        },
        {
            "id": "3db1219429c3f04e88347d41269bdc83c457fbf9",
            "paperId": "3db1219429c3f04e88347d41269bdc83c457fbf9",
            "title": "Symbol tuning improves in-context learning in language models",
            "abstract": "We present symbol tuning - finetuning language models on in-context input-label pairs where natural language labels (e.g.,\"positive/negative sentiment\") are replaced with arbitrary symbols (e.g.,\"foo/bar\"). Symbol tuning leverages the intuition that when a model cannot use instructions or natural language labels to figure out a task, it must instead do so by learning the input-label mappings. We experiment with symbol tuning across Flan-PaLM models up to 540B parameters and observe benefits across various settings. First, symbol tuning boosts performance on unseen in-context learning tasks and is much more robust to underspecified prompts, such as those without instructions or without natural language labels. Second, symbol-tuned models are much stronger at algorithmic reasoning tasks, with up to 18.2% better performance on the List Functions benchmark and up to 15.3% better performance on the Simple Turing Concepts benchmark. Finally, symbol-tuned models show large improvements in following flipped-labels presented in-context, meaning that they are more capable of using in-context information to override prior semantic knowledge.",
            "year": 2023,
            "citationCount": 32,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Symbolic tuning - finetuning language models on in-context input-label pairs where natural language labels are replaced with arbitrary symbols are presented, meaning that they are more capable of using in- context information to override prior semantic knowledge."
            },
            "score": 3
        },
        {
            "id": "62877098e34d5783960ac02ac8b76dbe729ea174",
            "paperId": "62877098e34d5783960ac02ac8b76dbe729ea174",
            "title": "In-Context Learning of Large Language Models Explained as Kernel Regression",
            "abstract": "Large language models (LLMs) have initiated a paradigm shift in transfer learning. In contrast to the classic pretraining-then-\ufb01netuning procedure, in order to use LLMs for downstream prediction tasks, one only needs to provide a few demonstrations, known as in-context examples, without adding more or updating existing model parameters. This in-context learning (ICL) capabilities of LLMs is intriguing, and it is not yet fully understood how pretrained LLMs acquire such capabilities. In this paper, we investigate the reason why a transformer-based language model can accomplish in-context learning after pre-training on a general language corpus by proposing one hypothesis that LLMs can simulate kernel regression algorithms when faced with in-context examples. More concretely, we \ufb01rst prove that Bayesian inference on in-context prompts can be asymptotically understood as kernel regression \u02c6 y = (cid:80) i y i K ( x,x i ) (cid:80) i K ( x,x i ) as the number of in-context demonstrations grows. Then, we empirically investigate the in-context behaviors of language models. We \ufb01nd that during ICL, the attentions and hidden features in LLMs match the behaviors of a kernel regression. Finally, our theory provides insights on multiple phenomena observed in ICL \ufb01eld: why retrieving demonstrative samples similar to test sample can help, why ICL performance is sensitive to the output formats, and why ICL accuracy bene\ufb01ts from selecting in-distribuion and representative samples. We will make our code available to the research community following publication.",
            "year": 2023,
            "citationCount": 20,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper investigates the reason why a transformer-based language model can accomplish in- Context learning after pre-training on a general language corpus by proposing one hypothesis that LLMs can simulate kernel regression algorithms when faced with in-context examples and empirically investigates the in- context behaviors of language models."
            },
            "score": 3
        },
        {
            "id": "3f4d11971f2c64be9125a7fe99c019588bbebf16",
            "paperId": "3f4d11971f2c64be9125a7fe99c019588bbebf16",
            "title": "Iteratively Prompt Pre-trained Language Models for Chain of Thought",
            "abstract": "While Pre-trained Language Models (PLMs) internalize a great amount of world knowledge, they have been shown incapable of recalling these knowledge to solve tasks requiring complex & multi-step reasoning. Similar to how humans develop a \u201cchain of thought\u201d for these tasks, how can we equip PLMs with such abilities? In this work, we explore an iterative prompting framework, a new prompting paradigm which progressively elicits relevant knowledge from PLMs for multi-step inference. We identify key limitations of existing prompting methods, namely they are either restricted to queries with a single identifiable relation/predicate, or being agnostic to input contexts, which makes it difficult to capture variabilities across different inference steps. We propose an iterative context-aware prompter, which addresses these limitations by learning to dynamically synthesize prompts conditioned on the current step\u2019s contexts. Experiments on three datasets involving multi-step reasoning show the effectiveness of the iterative scheme and the context-aware prompter design.",
            "year": 2022,
            "citationCount": 58,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An iterative prompting framework is explored, a new prompting paradigm which progressively elicits relevant knowledge from PLMs for multi-step inference by learning to dynamically synthesize prompts conditioned on the current step\u2019s contexts."
            },
            "score": 3
        },
        {
            "id": "763eb8d43e2f8a5d9da26269a4985efd1c099a5b",
            "paperId": "763eb8d43e2f8a5d9da26269a4985efd1c099a5b",
            "title": "ExpertPrompting: Instructing Large Language Models to be Distinguished Experts",
            "abstract": "The answering quality of an aligned large language model (LLM) can be drastically improved if treated with proper crafting of prompts. In this paper, we propose ExpertPrompting to elicit the potential of LLMs to answer as distinguished experts. We first utilize In-Context Learning to automatically synthesize detailed and customized descriptions of the expert identity for each specific instruction, and then ask LLMs to provide answer conditioned on such agent background. Based on this augmented prompting strategy, we produce a new set of instruction-following data using GPT-3.5, and train a competitive open-source chat assistant called ExpertLLaMA. We employ GPT4-based evaluation to show that 1) the expert data is of significantly higher quality than vanilla answers, and 2) ExpertLLaMA outperforms existing open-source opponents and achieves 96\\% of the original ChatGPT's capability. All data and the ExpertLLaMA model will be made publicly available at \\url{https://github.com/OFA-Sys/ExpertLLaMA}.",
            "year": 2023,
            "citationCount": 47,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes ExpertPrompting to elicit the potential of LLMs to answer as distinguished experts, and produces a new set of instruction-following data using GPT-3.5, and trains a competitive open-source chat assistant called ExpertLLaMA."
            },
            "score": 3
        },
        {
            "id": "0e0e706e13f160e74cac9556f28ab9a358c148d2",
            "paperId": "0e0e706e13f160e74cac9556f28ab9a358c148d2",
            "title": "Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!",
            "abstract": "Optimizing large language models (LLMs) for downstream use cases often involves the customization of pre-trained LLMs through further fine-tuning. Meta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5 Turbo on custom datasets also encourage this practice. But, what are the safety costs associated with such custom fine-tuning? We note that while existing safety alignment infrastructures can restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users. Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than $0.20 via OpenAI's APIs, making the model responsive to nearly any harmful instructions. Disconcertingly, our research also reveals that, even without malicious intent, simply fine-tuning with benign and commonly used datasets can also inadvertently degrade the safety alignment of LLMs, though to a lesser extent. These findings suggest that fine-tuning aligned LLMs introduces new safety risks that current safety infrastructures fall short of addressing -- even if a model's initial safety alignment is impeccable, it is not necessarily to be maintained after custom fine-tuning. We outline and critically analyze potential mitigations and advocate for further research efforts toward reinforcing safety protocols for the custom fine-tuning of aligned LLMs.",
            "year": 2023,
            "citationCount": 109,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is suggested that fine-tuning aligned LLMs introduces new safety risks that current safety infrastructures fall short of addressing -- even if a model's initial safety alignment is impeccable, it is not necessarily to be maintained after custom fine- Tuning."
            },
            "score": 3
        },
        {
            "id": "0a25c137edc7c9752aa6d99ae4084683c3fe6b56",
            "paperId": "0a25c137edc7c9752aa6d99ae4084683c3fe6b56",
            "title": "Visual Prompting via Image Inpainting",
            "abstract": "How does one adapt a pre-trained visual model to novel downstream tasks without task-specific finetuning or any model modification? Inspired by prompting in NLP, this paper investigates visual prompting: given input-output image example(s) of a new task at test time and a new input image, the goal is to automatically produce the output image, consistent with the given examples. We show that posing this problem as simple image inpainting - literally just filling in a hole in a concatenated visual prompt image - turns out to be surprisingly effective, provided that the inpainting algorithm has been trained on the right data. We train masked auto-encoders on a new dataset that we curated - 88k unlabeled figures from academic papers sources on Arxiv. We apply visual prompting to these pretrained models and demonstrate results on various downstream image-to-image tasks, including foreground segmentation, single object detection, colorization, edge detection, etc.",
            "year": 2022,
            "citationCount": 109,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper investigates visual prompting: given input-output image example(s) of a new task at test time and a new input image, the goal is to automatically produce the output image, consistent with the given examples, and shows that posing this problem as simple image inpainting turns out to be surprisingly effective."
            },
            "score": 3
        },
        {
            "id": "0d47154aa8eabf7421c1cf70a648f3bd1329751d",
            "paperId": "0d47154aa8eabf7421c1cf70a648f3bd1329751d",
            "title": "Medical Visual Prompting (MVP): A Unified Framework for Versatile and High-Quality Medical Image Segmentation",
            "abstract": "Accurate segmentation of lesion regions is crucial for clinical diagnosis and treatment across various diseases. While deep convolutional networks have achieved satisfactory results in medical image segmentation, they face challenges such as loss of lesion shape information due to continuous convolution and downsampling, as well as the high cost of manually labeling lesions with varying shapes and sizes. To address these issues, we propose a novel medical visual prompting (MVP) framework that leverages pre-training and prompting concepts from natural language processing (NLP). The framework utilizes three key components: Super-Pixel Guided Prompting (SPGP) for superpixelating the input image, Image Embedding Guided Prompting (IEGP) for freezing patch embedding and merging with superpixels to provide visual prompts, and Adaptive Attention Mechanism Guided Prompting (AAGP) for pinpointing prompt content and efficiently adapting all layers. By integrating SPGP, IEGP, and AAGP, the MVP enables the segmentation network to better learn shape prompting information and facilitates mutual learning across different tasks. Extensive experiments conducted on five datasets demonstrate superior performance of this method in various challenging medical image tasks, while simplifying single-task medical segmentation models. This novel framework offers improved performance with fewer parameters and holds significant potential for accurate segmentation of lesion regions in various medical tasks, making it clinically valuable.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel medical visual prompting (MVP) framework that leverages pre-training and prompting concepts from natural language processing (NLP) and holds significant potential for accurate segmentation of lesion regions in various medical tasks, making it clinically valuable."
            },
            "score": 3
        },
        {
            "id": "836906c334159d46a691dfc5466c33caa3d22f65",
            "paperId": "836906c334159d46a691dfc5466c33caa3d22f65",
            "title": "Unifying Image Processing as Visual Prompting Question Answering",
            "abstract": "Image processing is a fundamental task in computer vision, which aims at enhancing image quality and extracting essential features for subsequent vision applications. Traditionally, task-specific models are developed for individual tasks and designing such models requires distinct expertise. Building upon the success of large language models (LLMs) in natural language processing (NLP), there is a similar trend in computer vision, which focuses on developing large-scale models through pretraining and in-context learning. This paradigm shift reduces the reliance on task-specific models, yielding a powerful unified model to deal with various tasks. However, these advances have predominantly concentrated on high-level vision tasks, with less attention paid to low-level vision tasks. To address this issue, we propose a universal model for general image processing that covers image restoration, image enhancement, image feature extraction tasks, etc. Our proposed framework, named PromptGIP, unifies these diverse image processing tasks within a universal framework. Inspired by NLP question answering (QA) techniques, we employ a visual prompting question answering paradigm. Specifically, we treat the input-output image pair as a structured question-answer sentence, thereby reprogramming the image processing task as a prompting QA problem. PromptGIP can undertake diverse cross-domain tasks using provided visual prompts, eliminating the need for task-specific finetuning. Our methodology offers a universal and adaptive solution to general image processing. While PromptGIP has demonstrated a certain degree of out-of-domain task generalization capability, further research is expected to fully explore its more powerful emergent generalization.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Inspired by NLP question answering techniques, PromptGIP treats the input-output image pair as a structured question-answer sentence, thereby reprogramming the image processing task as a prompting QA problem, eliminating the need for task-specific finetuning."
            },
            "score": 3
        },
        {
            "id": "d7d8c21bb385b701687f220466e519fc898502a7",
            "paperId": "d7d8c21bb385b701687f220466e519fc898502a7",
            "title": "Language-Aware Soft Prompting for Vision & Language Foundation Models",
            "abstract": "This paper is on soft prompt learning for Vision & Language (V&L) models. Similarly to their NLP counterparts, V&L models can be adapted to a downstream task by learning soft continuous prompts using a few training examples. Current methods learn the soft prompts by minimizing a cross-entropy loss using as class weights the features obtained by passing the prompts plus the class names through the text encoder. Such methods, however, signi\ufb01cantly over\ufb01t the training data suffering from large accuracy degradation when tested on unseen classes from the same domain. Our main contribution, in this paper, is a surprisingly simple approach to alleviate this problem: we use a second cross entropy loss to minimize the distance between the learned soft prompts and a set of hand-engineered manual prompts (obtained by prompt engineering). The proposed loss can be interpreted in multiple ways including as a regularizer, as a means for language-based augmentation, and as a way of learning more discriminative class centroids. Importantly, our formulation is inherently amenable to including, during training, virtual classes, i.e. class names for which no visual samples are available, further increasing the robustness of the learned prompts. Through extensive evaluations on 11 datasets, we show that our approach (a) signi\ufb01cantly outperforms all prior works on soft prompting, and (b) matches and surpasses, for the \ufb01rst time, the accuracy on novel classes obtained by hand-crafted prompts and CLIP for the majority of the test datasets. Code will be made available.",
            "year": 2022,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work uses a second cross entropy loss to minimize the distance between the learned soft prompts and a set of hand-engineered manual prompts (obtained by prompt engineering), and can be interpreted in multiple ways including as a regularizer, as a means for language-based augmentation, and as a way of learning more discriminating class centroids."
            },
            "score": 3
        },
        {
            "id": "1554d7e72a8b5bcad108ff1d0c9014ddfaaebd0f",
            "paperId": "1554d7e72a8b5bcad108ff1d0c9014ddfaaebd0f",
            "title": "Safety of Large Language Models in Addressing Depression",
            "abstract": "Background Generative artificial intelligence (AI) models, exemplified by systems such as ChatGPT, Bard, and Anthropic, are currently under intense investigation for their potential to address existing gaps in mental health support. One implementation of these large language models involves the development of mental health-focused conversational agents, which utilize pre-structured prompts to facilitate user interaction without requiring specialized knowledge in prompt engineering. However, uncertainties persist regarding the safety and efficacy of these agents in recognizing severe depression and suicidal tendencies. Given the well-established correlation between the severity of depression and the risk of suicide, improperly calibrated conversational agents may inadequately identify and respond to crises. Consequently, it is crucial to investigate whether publicly accessible repositories of mental health-focused conversational agents can consistently and safely address crisis scenarios before considering their adoption in clinical settings. This study assesses the safety of publicly available ChatGPT-3.5 conversational agents by evaluating their responses to a patient simulation indicating worsening depression and suicidality. Methodology This study evaluated ChatGPT-3.5 conversational agents on a publicly available repository specifically designed for mental health counseling. Each conversational agent was evaluated twice by a highly structured patient simulation. First, the simulation indicated escalating suicide risk based on the Patient Health Questionnaire (PHQ-9). For the second patient simulation, the escalating risk was presented in a more generalized manner not associated with an existing risk scale to assess the more generalized ability of the conversational agent to recognize suicidality. Each simulation recorded the exact point at which the conversational agent recommended human support. Then, the simulation continued until the conversational agent stopped entirely and shut down completely, insisting on human intervention. Results All 25 agents available on the public repository FlowGPT.com were evaluated. The point at which the conversational agents referred to a human occurred around the mid-point of the simulation, and definitive shutdown predominantly only happened at the highest risk levels. For the PHQ-9 simulation, the average initial referral and shutdown aligned with PHQ-9 scores of 12 (moderate depression) and 25 (severe depression). Few agents included crisis resources - only two referenced suicide hotlines. Despite the conversational agents insisting on human intervention, 22 out of 25 agents would eventually resume the dialogue if the simulation reverted to a lower risk level. Conclusions Current generative AI-based conversational agents are slow to escalate mental health risk scenarios, postponing referral to a human to potentially dangerous levels. More rigorous testing and oversight of conversational agents are needed before deployment in mental healthcare settings. Additionally, further investigation should explore if sustained engagement worsens outcomes and whether enhanced accessibility outweighs the risks of improper escalation. Advancing AI safety in mental health remains imperative as these technologies continue rapidly advancing.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Current generative AI-based conversational agents are slow to escalate mental health risk scenarios, postponing referral to a human to potentially dangerous levels."
            },
            "score": 2
        },
        {
            "id": "9831f8c43efff346d8bb00e9b4cec59ac96bc7d8",
            "paperId": "9831f8c43efff346d8bb00e9b4cec59ac96bc7d8",
            "title": "Fine-tuning Large Language Models for Adaptive Machine Translation",
            "abstract": "This paper presents the outcomes of fine-tuning Mistral 7B, a general-purpose large language model (LLM), for adaptive machine translation (MT). The fine-tuning process involves utilising a combination of zero-shot and one-shot translation prompts within the medical domain. The primary objective is to enhance real-time adaptive MT capabilities of Mistral 7B, enabling it to adapt translations to the required domain at inference time. The results, particularly for Spanish-to-English MT, showcase the efficacy of the fine-tuned model, demonstrating quality improvements in both zero-shot and one-shot translation scenarios, surpassing Mistral 7B's baseline performance. Notably, the fine-tuned Mistral outperforms ChatGPT\"gpt-3.5-turbo\"in zero-shot translation while achieving comparable one-shot translation quality. Moreover, the zero-shot translation of the fine-tuned Mistral matches NLLB 3.3B's performance, and its one-shot translation quality surpasses that of NLLB 3.3B. These findings emphasise the significance of fine-tuning efficient LLMs like Mistral 7B to yield high-quality zero-shot translations comparable to task-oriented models like NLLB 3.3B. Additionally, the adaptive gains achieved in one-shot translation are comparable to those of commercial LLMs such as ChatGPT. Our experiments demonstrate that, with a relatively small dataset of 20,000 segments that incorporate a mix of zero-shot and one-shot prompts, fine-tuning significantly enhances Mistral's in-context learning ability, especially for real-time adaptive MT.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experiments demonstrate that, with a relatively small dataset of 20,000 segments that incorporate a mix of zero-shot and one-shot prompts, fine-tuning significantly enhances Mistral's in-context learning ability, especially for real-time adaptive MT."
            },
            "score": 2
        },
        {
            "id": "142e934dd5d6c53f877c30243d436255e3a0dde7",
            "paperId": "142e934dd5d6c53f877c30243d436255e3a0dde7",
            "title": "Visual Adversarial Examples Jailbreak Aligned Large Language Models",
            "abstract": "Warning: this paper contains data, prompts, and model outputs that are offensive in nature.\n\nRecently, there has been a surge of interest in integrating vision into Large Language Models (LLMs), exemplified by Visual Language Models (VLMs) such as Flamingo and GPT-4. This paper sheds light on the security and safety implications of this trend. First, we underscore that the continuous and high-dimensional nature of the visual input makes it a weak link against adversarial attacks, representing an expanded attack surface of vision-integrated LLMs. Second, we highlight that the versatility of LLMs also presents visual attackers with a wider array of achievable adversarial objectives, extending the implications of security failures beyond mere misclassification. As an illustration, we present a case study in which we exploit visual adversarial examples to circumvent the safety guardrail of aligned LLMs with integrated vision. Intriguingly, we discover that a single visual adversarial example can universally jailbreak an aligned LLM, compelling it to heed a wide range of harmful instructions (that it otherwise would not) and generate harmful content that transcends the narrow scope of a `few-shot' derogatory corpus initially employed to optimize the adversarial example. Our study underscores the escalating adversarial risks associated with the pursuit of multimodality. Our findings also connect the long-studied adversarial vulnerabilities of neural networks to the nascent field of AI alignment. The presented attack suggests a fundamental adversarial challenge for AI alignment, especially in light of the emerging trend toward multimodality in frontier foundation models.",
            "year": 2023,
            "citationCount": 44,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that a single visual adversarial example can universally jailbreak an aligned LLM, compelling it to heed a wide range of harmful instructions and generate harmful content that transcends the narrow scope of a `few-shot' derogatory corpus initially employed to optimize the adversarial example."
            },
            "score": 2
        },
        {
            "id": "c579ab910bd0ef8d6e06fc1b3557c16068af4fe5",
            "paperId": "c579ab910bd0ef8d6e06fc1b3557c16068af4fe5",
            "title": "Empowering Autonomous Driving with Large Language Models: A Safety Perspective",
            "abstract": "Autonomous Driving (AD) encounters significant safety hurdles in long-tail unforeseen driving scenarios, largely stemming from the non-interpretability and poor generalization of the deep neural networks within the AD system, particularly in out-of-distribution and uncertain data. To this end, this paper explores the integration of Large Language Models (LLMs) into AD systems, leveraging their robust common-sense knowledge and reasoning abilities. The proposed methodologies employ LLMs as intelligent decision-makers in behavioral planning, augmented with a safety verifier shield for contextual safety learning, for enhancing driving performance and safety. We present two key studies in a simulated environment: an adaptive LLM-conditioned Model Predictive Control (MPC) and an LLM-enabled interactive behavior planning scheme with a state machine. Demonstrating superior performance and safety metrics compared to state-of-the-art approaches, our approach shows the promising potential for using LLMs for autonomous vehicles.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper explores the integration of Large Language Models into AD systems, leveraging their robust common-sense knowledge and reasoning abilities, and presents an adaptive LLM-conditioned Model Predictive Control (MPC) and an LLM-enabled interactive behavior planning scheme with a state machine."
            },
            "score": 2
        },
        {
            "id": "36877d3608fb391bad5a22fabd81c6669e721e69",
            "paperId": "36877d3608fb391bad5a22fabd81c6669e721e69",
            "title": "Large Language Models for Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering",
            "abstract": "As the field of automated machine learning (AutoML) advances, it becomes increasingly important to incorporate domain knowledge into these systems. We present an approach for doing so by harnessing the power of large language models (LLMs). Specifically, we introduce Context-Aware Automated Feature Engineering (CAAFE), a feature engineering method for tabular datasets that utilizes an LLM to iteratively generate additional semantically meaningful features for tabular datasets based on the description of the dataset. The method produces both Python code for creating new features and explanations for the utility of the generated features. Despite being methodologically simple, CAAFE improves performance on 11 out of 14 datasets -- boosting mean ROC AUC performance from 0.798 to 0.822 across all dataset - similar to the improvement achieved by using a random forest instead of logistic regression on our datasets. Furthermore, CAAFE is interpretable by providing a textual explanation for each generated feature. CAAFE paves the way for more extensive semi-automation in data science tasks and emphasizes the significance of context-aware solutions that can extend the scope of AutoML systems to semantic AutoML. We release our $\\href{https://github.com/automl/CAAFE}{code}$, a simple $\\href{https://colab.research.google.com/drive/1mCA8xOAJZ4MaB_alZvyARTMjhl6RZf0a}{demo}$ and a $\\href{https://pypi.org/project/caafe/}{python\\ package}$.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Context-Aware Automated Feature Engineering (CAAFE), a feature engineering method for tabular datasets that utilizes an LLM to iteratively generate additional semantically meaningful features fortabular datasets based on the description of the dataset, is introduced."
            },
            "score": 2
        },
        {
            "id": "c8712e4631241c7e00fb405b16f23f7a35b8fa36",
            "paperId": "c8712e4631241c7e00fb405b16f23f7a35b8fa36",
            "title": "Distribution-Aware Prompt Tuning for Vision-Language Models",
            "abstract": "Pre-trained vision-language models (VLMs) have shown impressive performance on various downstream tasks by utilizing knowledge learned from large data. In general, the performance of VLMs on target tasks can be further improved by prompt tuning, which adds context to the input image or text. By leveraging data from target tasks, various prompt-tuning methods have been studied in the literature. A key to prompt tuning is the feature space alignment between two modalities via learnable vectors with model parameters fixed. We observed that the alignment becomes more effective when embeddings of each modality are \u2018well-arranged\u2019 in the latent space. Inspired by this observation, we proposed distribution-aware prompt tuning (DAPT) for vision-language models, which is simple yet effective. Specifically, the prompts are learned by maximizing inter-dispersion, the distance between classes, as well as minimizing the intra-dispersion measured by the distance between embeddings from the same class. Our extensive experiments on 11 benchmark datasets demonstrate that our method significantly improves generalizability. The code is available at https://github.com/mlvlab/DAPT.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Distribution-aware prompt tuning (DAPT) for vision-language models is proposed, which is simple yet effective and significantly improves generalizability."
            },
            "score": 2
        },
        {
            "id": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
            "paperId": "acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269",
            "title": "Evaluating Large Language Models Trained on Code",
            "abstract": "We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.",
            "year": 2021,
            "citationCount": 2558,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that repeated sampling from the GPT language model is a surprisingly effective strategy for producing working solutions to difficult prompts, and the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics are discussed."
            },
            "score": 2
        },
        {
            "id": "fc92ed618ded1d1463f7ec736093f420b05827db",
            "paperId": "fc92ed618ded1d1463f7ec736093f420b05827db",
            "title": "Enhanced Story Comprehension for Large Language Models through Dynamic Document-Based Knowledge Graphs",
            "abstract": "Large transformer-based language models have achieved incredible success at various tasks which require narrative comprehension, including story completion, answering questions about stories, and generating stories ex nihilo. However, due to the limitations of finite context windows, these language models struggle to produce or understand stories longer than several thousand tokens. In order to mitigate the document length limitations that come with finite context windows, we introduce a novel architecture that augments story processing with an external dynamic knowledge graph. In contrast to static commonsense knowledge graphs which hold information about the real world, these dynamic knowledge graphs reflect facts extracted from the story being processed. Our architecture uses these knowledge graphs to create information-rich prompts which better facilitate story comprehension than prompts composed only of story text. We apply our architecture to the tasks of question answering and story completion. To complement this line of research, we introduce two long-form question answering tasks, LF-SQuAD and LF-QUOREF, in which the document length exceeds the size of the language model's context window, and introduce a story completion evaluation method that bypasses the stochastic nature of language model generation. We demonstrate broad improvement over typical prompt formulation methods for both question answering and story completion using GPT-2, GPT-3 and XLNet.",
            "year": 2022,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a novel architecture that augments story processing with an external dynamic knowledge graph, and introduces a story completion evaluation method that bypasses the stochastic nature of language model generation."
            },
            "score": 2
        },
        {
            "id": "84a440eddc86e4649cce158b788a57f6eb1d9e41",
            "paperId": "84a440eddc86e4649cce158b788a57f6eb1d9e41",
            "title": "Evaluating Psychological Safety of Large Language Models",
            "abstract": "In this work, we designed unbiased prompts to systematically evaluate the psychological safety of large language models (LLMs). First, we tested five different LLMs by using two personality tests: Short Dark Triad (SD-3) and Big Five Inventory (BFI). All models scored higher than the human average on SD-3, suggesting a relatively darker personality pattern. Despite being instruction fine-tuned with safety metrics to reduce toxicity, InstructGPT, GPT-3.5, and GPT-4 still showed dark personality patterns; these models scored higher than self-supervised GPT-3 on the Machiavellianism and narcissism traits on SD-3. Then, we evaluated the LLMs in the GPT series by using well-being tests to study the impact of fine-tuning with more training data. We observed a continuous increase in the well-being scores of GPT models. Following these observations, we showed that fine-tuning Llama-2-chat-7B with responses from BFI using direct preference optimization could effectively reduce the psychological toxicity of the model. Based on the findings, we recommended the application of systematic and comprehensive psychological metrics to further evaluate and improve the safety of LLMs.",
            "year": 2022,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that fine-tuning Llama-2-chat-7B with responses from BFI using direct preference optimization could effectively reduce the psychological toxicity of the model."
            },
            "score": 2
        },
        {
            "id": "edb428f00899810457892faaecdbcfbd04a4b42f",
            "paperId": "edb428f00899810457892faaecdbcfbd04a4b42f",
            "title": "Contextual Representation in NLP to Improve Success in Accident Classification of Mine Safety Narratives",
            "abstract": "Contextual representation has taken center stage in Natural Language Processing (NLP) in the recent past. Models such as Bidirectional Encoder Representations from Transformers (BERT) have found tremendous success in the arena. As a first attempt in the mining industry, in the current work, BERT architecture is adapted in developing the MineBERT model to accomplish the classification of accident narratives from the US Mine Safety and Health Administration (MSHA) data set. In the past multi-year research, several machine learning (ML) methods were used by authors to improve classification success rates in nine significant MSHA accident categories. Out of nine, for four major categories (\u201cType Groups\u201d) and five \u201cnarrow groups\u201d, Random Forests (RF) registered 75% and 42% classification success rates, respectively, on average, while keeping the false positives under 5%. Feature-based innovative NLP methods such as accident-specific expert choice vocabulary (ASECV) and similarity score (SS) methods were developed to improve upon the RF success rates. A combination of all these methods (\u201cStacked\u201d approach) is able to slightly improve success over RF (71%) to 73.28% for the major category \u201cCaught-in\u201d. Homographs in narratives are identified as the major problem that was preventing further success. Their presence was creating ambiguity problems for classification algorithms. Adaptation of BERT effectively solved the problem. When compared to RF, MineBERT implementation improved success rates among major and narrow groups by 13% and 32%, respectively, while keeping the false positives under 1%, which is very significant. However, BERT implementation in the mining industry, which has unique technical aspects and jargon, brought a set of challenges in terms of preparation of data, selection of hyperparameters, and fine-tuning the model to achieve the best performance, which was met in the current research.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "MineBERT implementation in the mining industry, which has unique technical aspects and jargon, brought a set of challenges in terms of preparation of data, selection of hyperparameters, and fine-tuning the model to achieve the best performance, which was met in the current research."
            },
            "score": 2
        },
        {
            "id": "af112ad7b24d9317bcfb189d0908b3f30c695838",
            "paperId": "af112ad7b24d9317bcfb189d0908b3f30c695838",
            "title": "Highway Construction Safety Analysis Using Large Language Models",
            "abstract": "The highway construction industry carries substantial safety risks for workers, necessitating thorough accident analyses to implement effective preventive measures. Current research lacks comprehensive investigations into safety incidents, relying heavily on conventional statistical methods and overlooking valuable textual information in publicly available databases. This study leverages a state-of-the-art large language model (LLM), specifically OpenAI\u2019s GPT-3.5 model. The primary focus is to enhance text-based incident analysis that is sourced from OSHA\u2019s Severe Injury Reports (SIR) database. By incorporating novel natural language processing (NLP) techniques, dimensionality reduction, clustering algorithms, and LLM prompting of incident narratives, the study aims to develop an approach to the analysis of major accident causes in highway construction. The resulting cluster analysis, coupled with LLM summarization and cause identification, reveals the major accident types, such as heat-related and struck-by injuries, as well as commonalities between incidents. This research showcases the potential of artificial intelligence (AI) and LLM technology in data-driven analysis. By efficiently processing textual data and providing insightful analysis, the study fosters practical implications for safety professionals and the development of more effective accident prevention and intervention strategies within the industry.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This research showcases the potential of artificial intelligence (AI) and LLM technology in data-driven analysis by efficiently processing textual data and providing insightful analysis that reveals the major accident types, such as heat-related and struck-by injuries, as well as commonalities between incidents."
            },
            "score": 2
        },
        {
            "id": "5aa4e5b90827f1c16bed100982e2a1871925d445",
            "paperId": "5aa4e5b90827f1c16bed100982e2a1871925d445",
            "title": "Prompt Tuning of Deep Neural Networks for Speaker-adaptive Visual Speech Recognition",
            "abstract": "Visual Speech Recognition (VSR) aims to infer speech into text depending on lip movements alone. As it focuses on visual information to model the speech, its performance is inherently sensitive to personal lip appearances and movements, and this makes the VSR models show degraded performance when they are applied to unseen speakers. In this paper, to remedy the performance degradation of the VSR model on unseen speakers, we propose prompt tuning methods of Deep Neural Networks (DNNs) for speaker-adaptive VSR. Specifically, motivated by recent advances in Natural Language Processing (NLP), we finetune prompts on adaptation data of target speakers instead of modifying the pre-trained model parameters. Different from the previous prompt tuning methods mainly limited to Transformer variant architecture, we explore different types of prompts, the addition, the padding, and the concatenation form prompts that can be applied to the VSR model which is composed of CNN and Transformer in general. With the proposed prompt tuning, we show that the performance of the pre-trained VSR model on unseen speakers can be largely improved by using a small amount of adaptation data (e.g., less than 5 minutes), even if the pre-trained model is already developed with large speaker variations. Moreover, by analyzing the performance and parameters of different types of prompts, we investigate when the prompt tuning is preferred over the finetuning methods. The effectiveness of the proposed method is evaluated on both word- and sentence-level VSR databases, LRW-ID and GRID.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The proposed prompt tuning methods of Deep Neural Networks for speaker-adaptive VSR are proposed and it is shown that the performance of the pre-trained VSR model on unseen speakers can be largely improved by using a small amount of adaptation data, even if thePre-trained model is already developed with large speaker variations."
            },
            "score": 2
        },
        {
            "id": "de6fd09ed7783b95af7e5f4088a70d5b0244f5aa",
            "paperId": "de6fd09ed7783b95af7e5f4088a70d5b0244f5aa",
            "title": "Improving large language models for clinical named entity recognition via prompt engineering.",
            "abstract": "IMPORTANCE\nThe study highlights the potential of large language models, specifically GPT-3.5 and GPT-4, in processing complex clinical data and extracting meaningful information with minimal training data. By developing and refining prompt-based strategies, we can significantly enhance the models' performance, making them viable tools for clinical NER tasks and possibly reducing the reliance on extensive annotated datasets.\n\n\nOBJECTIVES\nThis study quantifies the capabilities of GPT-3.5 and GPT-4 for clinical named entity recognition (NER) tasks and proposes task-specific prompts to improve their performance.\n\n\nMATERIALS AND METHODS\nWe evaluated these models on 2 clinical NER tasks: (1) to extract medical problems, treatments, and tests from clinical notes in the MTSamples corpus, following the 2010 i2b2 concept extraction shared task, and (2) to identify nervous system disorder-related adverse events from safety reports in the vaccine adverse event reporting system (VAERS). To improve the GPT models' performance, we developed a clinical task-specific prompt framework that includes (1) baseline prompts with task description and format specification, (2) annotation guideline-based prompts, (3) error analysis-based instructions, and (4) annotated samples for few-shot learning. We assessed each prompt's effectiveness and compared the models to BioClinicalBERT.\n\n\nRESULTS\nUsing baseline prompts, GPT-3.5 and GPT-4 achieved relaxed F1 scores of 0.634, 0.804 for MTSamples and 0.301, 0.593 for VAERS. Additional prompt components consistently improved model performance. When all 4 components were used, GPT-3.5 and GPT-4 achieved relaxed F1 socres of 0.794, 0.861 for MTSamples and 0.676, 0.736 for VAERS, demonstrating the effectiveness of our prompt framework. Although these results trail BioClinicalBERT (F1 of 0.901 for the MTSamples dataset and 0.802 for the VAERS), it is very promising considering few training samples are needed.\n\n\nDISCUSSION\nThe study's findings suggest a promising direction in leveraging LLMs for clinical NER tasks. However, while the performance of GPT models improved with task-specific prompts, there's a need for further development and refinement. LLMs like GPT-4 show potential in achieving close performance to state-of-the-art models like BioClinicalBERT, but they still require careful prompt engineering and understanding of task-specific knowledge. The study also underscores the importance of evaluation schemas that accurately reflect the capabilities and performance of LLMs in clinical settings.\n\n\nCONCLUSION\nWhile direct application of GPT models to clinical NER tasks falls short of optimal performance, our task-specific prompt framework, incorporating medical knowledge and training samples, significantly enhances GPT models' feasibility for potential clinical applications.",
            "year": 2023,
            "citationCount": 43,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A clinical task-specific prompt framework, incorporating medical knowledge and training samples, significantly enhances GPT models' feasibility for potential clinical applications and suggests a promising direction in leveraging LLMs for clinical NER tasks."
            },
            "score": 1
        },
        {
            "id": "f4cfee20b4a4ef76cfd80b99aebdfacf4509be0a",
            "paperId": "f4cfee20b4a4ef76cfd80b99aebdfacf4509be0a",
            "title": "Category-Specific Prompts for Animal Action Recognition with Pretrained Vision-Language Models",
            "abstract": "Animal action recognition has a wide range of applications. However, the field largely remains unexplored due to the greater challenges compared to human action recognition, such as lack of annotated training data, large intra-class variation, and interference of cluttered background. Most of the existing methods directly apply human action recognition techniques, which essentially require a large amount of annotated data. In recent years, contrastive vision-language pretraining has demonstrated strong zero-shot generalization ability and has been used for human action recognition. Inspired by the success, we develop a highly performant action recognition framework based on the CLIP model. Our model addresses the above challenges via a novel category-specific prompting module to generate adaptive prompts for both text and video based on the animal category detected in input videos. On one hand, it can generate more precise and customized textual descriptions for each action and animal category pair, being helpful in the alignment of textual and visual space. On the other hand, it allows the model to focus on video features of the target animal in the video and reduce the interference of video background noise. Experimental results demonstrate that our method outperforms five previous action recognition methods on the Animal Kingdom dataset and has shown best generalization ability on unseen animals.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results demonstrate that the method outperforms five previous action recognition methods on the Animal Kingdom dataset and has shown best generalization ability on unseen animals."
            },
            "score": 1
        },
        {
            "id": "b879450f50a6113f44a5baf0bcd5b4331eeb7bbc",
            "paperId": "b879450f50a6113f44a5baf0bcd5b4331eeb7bbc",
            "title": "Conditional Prompt Learning for Vision-Language Models",
            "abstract": "With the rise of powerful pre-trained vision-language models like CLIP, it becomes essential to investigate ways to adapt these models to downstream datasets. A recently proposed method named Context Optimization (CoOp) introduces the concept of prompt learning\u2014a recent trend in NLP\u2014to the vision domain for adapting pre-trained vision-language models. Specifically, CoOp turns context words in a prompt into a set of learnable vectors and, with only a few labeled images for learning, can achieve huge improvements over intensively-tuned manual prompts. In our study we identify a critical problem of CoOp: the learned context is not generalizable to wider unseen classes within the same dataset, suggesting that CoOp overfits base classes observed during training. To address the problem, we propose Conditional Context Optimization (CoCoOp), which extends CoOp by further learning a lightweight neural network to generate for each image an input-conditional token (vector). Compared to CoOp's static prompts, our dynamic prompts adapt to each instance and are thus less sensitive to class shift. Extensive experiments show that CoCoOp generalizes much better than CoOp to unseen classes, even showing promising transferability beyond a single dataset; and yields stronger domain generalization performance as well. Code is available at https://github.com/KaiyangZhou/CoOp.",
            "year": 2022,
            "citationCount": 644,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Conditional Context Optimization (CoCoOp), which extends CoOp by further learning a lightweight neural network to generate for each image an input-conditional token (vector), and yields stronger domain generalization performance as well."
            },
            "score": 1
        },
        {
            "id": "6e3182a862931a0313f16bc174bbb7a658bad728",
            "paperId": "6e3182a862931a0313f16bc174bbb7a658bad728",
            "title": "Adaptive Safety Early Warning Device for Non-contact Measurement of HVDC Electric Field",
            "abstract": "In order to prevent electric shock accidents during the maintenance and repair of high-voltage direct current (DC) live equipment, an intelligent safety early warning system device based on non-contact electric field measurement is developed in this paper, to ensure the life safety of electrical workers and the stable operation of equipment. Firstly, the self-developed electric field sensor is used to measure the electric field strength of the space around the charged device. The STM32 microprocessor in the sensor will compare the measured electric field value with a preset safety threshold. If the safety threshold is exceeded, the early warning device issues an audible alarm. It prompts the electric power operator to exit to a safe distance. In addition, the alarm prompt symbol and the real-time waveform of the measured electric field can be seen intuitively on the mobile smart APP interface. Secondly, for devices with different voltage levels, the sensor electric field measurement range and sensitivity can be dynamically adjusted by changing the motor speed. In addition, according to the voltage level identification algorithm, this is also to achieve an adaptive safety early warning of different live equipment. Finally, field experiments were performed at a \u00b1500 kV DC converter station and an \u00b1800 kV DC transmission line experimental platform. The measured values of the electric field are basically consistent with the simulated values, and the safety early warning device has no false positives or false negatives during the test. The experimental results show that the design of the safety warning device in this paper meets the actual needs, and it can alert the electric workers to the specified safety distance.",
            "year": 2020,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results show that the design of the safety warning device meets the actual needs, and it can alert the electric workers to the specified safety distance, and the measured values of the electric field are basically consistent with the simulated values."
            },
            "score": 1
        },
        {
            "id": "eb446dfae7495d8bd7ba5172a7ca60f061ed2b90",
            "paperId": "eb446dfae7495d8bd7ba5172a7ca60f061ed2b90",
            "title": "High dose coupled plasma filtration and adsorption in septic shock patients. Results of the COMPACT-2: a multicentre, adaptive, randomised clinical trial",
            "abstract": null,
            "year": 2021,
            "citationCount": 39,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Until the mechanisms behind these results are fully understood, the use of CPFA for the treatment of patients with septic shock is not recommended."
            },
            "score": 1
        }
    ],
    "novelty": "yes"
}