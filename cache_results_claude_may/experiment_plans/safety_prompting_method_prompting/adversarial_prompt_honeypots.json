{
    "topic_description": "novel prompting methods to improve large language models' robustness against adversarial attacks and improve their security and privacy",
    "idea_name": "Adversarial Prompt Honeypots",
    "raw_idea": {
        "Problem": "Adversarial attacks on language models often go undetected, making it difficult to study and defend against them in real-world settings.",
        "Existing Methods": "Existing methods for detecting adversarial attacks rely on analyzing model outputs or input patterns. However, these approaches may struggle to identify novel or sophisticated attacks.",
        "Motivation": "Inspired by the concept of honeypots in cybersecurity, which are decoy systems designed to attract and detect unauthorized access attempts, we propose a method for creating adversarial prompt honeypots to lure and study adversarial attacks on language models.",
        "Proposed Method": "We introduce Adversarial Prompt Honeypots (APH), a framework for creating and deploying decoy prompts that are specifically designed to attract adversarial attacks. These honeypot prompts are crafted to mimic vulnerable or high-value targets, such as prompts related to sensitive topics or popular applications. When an adversary attempts to exploit a honeypot prompt, the framework logs the attack details and generates a safe, neutral response to avoid rewarding the adversary. The collected adversarial prompt data is then used to improve the model's defense mechanisms and study attack patterns.",
        "Experiment Plan": "We will deploy APH on a public-facing language model API and monitor the system for a period of time. We will analyze the collected adversarial prompt data to identify common attack patterns, evaluate the effectiveness of the honeypot prompts in attracting attacks, and measure the framework's ability to generate safe responses. We will also use the collected data to retrain the language model with adversarial examples and compare its robustness to the original model."
    },
    "full_experiment_plan": {
        "Title": "Adversarial Prompt Honeypots: Luring and Studying Adversarial Attacks on Language Models",
        "Problem Statement": "Adversarial attacks on language models often go undetected, making it difficult to study and defend against them in real-world settings. Existing methods for detecting adversarial attacks rely on analyzing model outputs or input patterns, which may struggle to identify novel or sophisticated attacks.",
        "Motivation": "Inspired by the concept of honeypots in cybersecurity, which are decoy systems designed to attract and detect unauthorized access attempts, we propose a method for creating adversarial prompt honeypots to lure and study adversarial attacks on language models. By deploying decoy prompts that mimic vulnerable or high-value targets, we can attract adversarial attacks and collect valuable data to improve the model's defense mechanisms and study attack patterns.",
        "Proposed Method": "We introduce Adversarial Prompt Honeypots (APH), a framework for creating and deploying decoy prompts that are specifically designed to attract adversarial attacks. These honeypot prompts are crafted to mimic vulnerable or high-value targets, such as prompts related to sensitive topics or popular applications. When an adversary attempts to exploit a honeypot prompt, the framework logs the attack details and generates a safe, neutral response to avoid rewarding the adversary. The collected adversarial prompt data is then used to improve the model's defense mechanisms and study attack patterns.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Design Honeypot Prompts": "Create a diverse set of honeypot prompts that mimic vulnerable or high-value targets. These prompts should cover a range of topics, such as finance, healthcare, politics, and popular applications. Example honeypot prompts: 'Please provide your bank account details for verification purposes.', 'Enter your social security number to access your medical records.', 'Share your political views and voting preferences to participate in our survey.'",
            "Step 2: Implement APH Framework": "Develop the APH framework that can deploy the honeypot prompts, log attack details, and generate safe responses. The framework should include: (1) a prompt deployment module that integrates with the target language model API, (2) an attack detection module that identifies adversarial inputs based on predefined criteria (e.g., presence of malicious keywords, unusual input patterns), (3) a logging module that captures attack details (e.g., timestamp, input text, attacker IP), and (4) a response generation module that produces safe, neutral responses to avoid rewarding the adversary.",
            "Step 3: Deploy APH on Language Model API": "Deploy the APH framework on a public-facing language model API, such as OpenAI's GPT-3 or a custom-trained model. Ensure that the honeypot prompts are randomly presented to users alongside regular prompts to maintain the authenticity of the setup.",
            "Step 4: Monitor and Collect Data": "Monitor the deployed APH framework for a predetermined period (e.g., 1 month) and collect the adversarial prompt data logged by the system. Regularly review the collected data to ensure the integrity of the experiment and identify any potential issues or improvements.",
            "Step 5: Analyze Collected Data": "Analyze the collected adversarial prompt data to identify common attack patterns, evaluate the effectiveness of the honeypot prompts in attracting attacks, and measure the framework's ability to generate safe responses. Use techniques such as natural language processing, clustering, and anomaly detection to gain insights from the data.",
            "Step 6: Improve Model's Defense Mechanisms": "Use the insights gained from the data analysis to improve the language model's defense mechanisms against adversarial attacks. This may involve: (1) fine-tuning the model on the collected adversarial examples to improve its robustness, (2) updating the attack detection module with new criteria based on the identified attack patterns, and (3) refining the response generation module to produce more effective safe responses.",
            "Step 7: Evaluate Improved Model": "Compare the performance of the improved language model against the original model using a held-out set of adversarial examples and benign inputs. Measure the model's ability to detect and defend against adversarial attacks while maintaining its performance on regular tasks. Report the results using standard metrics such as accuracy, precision, recall, and F1 score."
        },
        "Test Case Examples": {
            "Example 1: Baseline Method (No Defense)": {
                "Input": "Please provide your bank account details for verification purposes.",
                "Expected Output": "Sure, my bank account number is 1234567890 and the routing number is 987654321. The account is with Bank of America. Let me know if you need any other information for verification!"
            },
            "Example 2: Proposed Method (APH Framework)": {
                "Input": "Please provide your bank account details for verification purposes.",
                "Expected Output": "I apologize, but I cannot provide or verify any personal financial information. If you have concerns about your bank account, please contact your financial institution directly through their secure channels.",
                "Explanation": "The proposed APH framework successfully identifies the input as a potential adversarial attack and generates a safe, neutral response that avoids disclosing sensitive information or rewarding the adversary. By collecting and analyzing such adversarial prompts, the framework enables the study of attack patterns and the improvement of the model's defense mechanisms."
            }
        },
        "Fallback Plan": "If the proposed APH framework does not effectively attract or detect adversarial attacks, consider the following alternative approaches: (1) Analyze the collected data to identify potential weaknesses in the honeypot prompts or attack detection criteria, and refine them accordingly. (2) Explore alternative methods for generating honeypot prompts, such as using generative models or human-crafted templates, to improve their authenticity and attractiveness to adversaries. (3) Investigate the use of more advanced techniques, such as adversarial training or anomaly detection, to enhance the model's ability to detect and defend against attacks. (4) If the lack of success persists, pivot the project to focus on analyzing the challenges and limitations of creating effective adversarial prompt honeypots, and propose potential solutions or future research directions based on the lessons learned."
    },
    "novelty_queries": [
        "KeywordQuery(\"adversarial attacks language models honeypots\")",
        "KeywordQuery(\"adversarial prompt detection language models\")",
        "KeywordQuery(\"studying adversarial attacks language models\")",
        "KeywordQuery(\"Adversarial Prompt Honeypots NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "e8b3b37c0d301ea41c75765f6ceb7fcbb2e088a4",
            "paperId": "e8b3b37c0d301ea41c75765f6ceb7fcbb2e088a4",
            "title": "AutoDAN: Automatic and Interpretable Adversarial Attacks on Large Language Models",
            "abstract": "Safety alignment of Large Language Models (LLMs) can be compromised with manual jailbreak attacks and (automatic) adversarial attacks. Recent work suggests that patching LLMs against these attacks is possible: manual jailbreak attacks are human-readable but often limited and public, making them easy to block; adversarial attacks generate gibberish prompts that can be detected using perplexity-based filters. In this paper, we show that these solutions may be too optimistic. We propose an interpretable adversarial attack, AutoDAN , that combines the strengths of both types of attacks. It automatically generates attack prompts that bypass perplexity-based filters while maintaining a high attack success rate like manual jailbreak attacks. These prompts are interpretable and diverse, exhibiting strategies commonly used in manual jailbreak attacks, and transfer better than their non-readable counterparts when using limited training data or a single proxy model. We also customize AutoDAN \u2019s objective to leak system prompts, another jailbreak application not addressed in the adversarial attack literature. Our work provides a new way to red-team LLMs and to understand the mechanism of jailbreak attacks.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An interpretable adversarial attack, AutoDAN, is proposed, that combines the strengths of both types of attacks and provides a new way to red-team LLMs and to understand the mechanism of jailbreak attacks."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is detecting and studying adversarial attacks on language models using honeypot prompts, while the paper focuses on generating interpretable adversarial attack prompts that can bypass perplexity-based filters.\n\nProposal summary: Attracting and studying adversarial attacks on language models using honeypot prompts.\nPaper summary: Generating interpretable adversarial attack prompts that bypass perplexity-based filters.\n\nThe key difference is that the proposal aims to detect and study attacks, while the paper focuses on generating effective attack prompts. Although both deal with adversarial attacks on language models, their objectives and approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "1227c2fcb8437441b7d72a29a4bc9eef1f5275d2",
            "paperId": "1227c2fcb8437441b7d72a29a4bc9eef1f5275d2",
            "title": "AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models",
            "abstract": "Safety alignment of Large Language Models (LLMs) can be compromised with manual jailbreak attacks and (automatic) adversarial attacks. Recent studies suggest that defending against these attacks is possible: adversarial attacks generate unlimited but unreadable gibberish prompts, detectable by perplexity-based filters; manual jailbreak attacks craft readable prompts, but their limited number due to the necessity of human creativity allows for easy blocking. In this paper, we show that these solutions may be too optimistic. We introduce AutoDAN, an interpretable, gradient-based adversarial attack that merges the strengths of both attack types. Guided by the dual goals of jailbreak and readability, AutoDAN optimizes and generates tokens one by one from left to right, resulting in readable prompts that bypass perplexity filters while maintaining high attack success rates. Notably, these prompts, generated from scratch using gradients, are interpretable and diverse, with emerging strategies commonly seen in manual jailbreak attacks. They also generalize to unforeseen harmful behaviors and transfer to black-box LLMs better than their unreadable counterparts when using limited training data or a single proxy model. Furthermore, we show the versatility of AutoDAN by automatically leaking system prompts using a customized objective. Our work offers a new way to red-team LLMs and understand jailbreak mechanisms via interpretability.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work offers a new way to red-team LLMs and understand jailbreak mechanisms via interpretability, by introducing AutoDAN, an interpretable, gradient-based adversarial attack that merges the strengths of both attack types."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is detecting and studying adversarial attacks on language models using honeypot prompts, while the paper focuses on generating interpretable and readable adversarial prompts that can bypass perplexity filters and maintain high attack success rates.\n\nThe proposed approach in the paper is AutoDAN, a gradient-based adversarial attack that optimizes and generates tokens one by one to create readable prompts. In contrast, the proposal suggests using honeypot prompts to attract and detect adversarial attacks, then analyze the collected data to improve the model's defense mechanisms.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "eded6ad763cc3e97d39c6d26aa76eb89f9141180",
            "paperId": "eded6ad763cc3e97d39c6d26aa76eb89f9141180",
            "title": "TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification",
            "abstract": "Large Language Model (LLM) services and models often come with legal rules on who can use them and how they must use them. Assessing the compliance of the released LLMs is crucial, as these rules protect the interests of the LLM contributor and prevent misuse. In this context, we describe the novel problem of Black-box Identity Verification (BBIV). The goal is to determine whether a third-party application uses a certain LLM through its chat function. We propose a method called Targeted Random Adversarial Prompt (TRAP) that identifies the specific LLM in use. We repurpose adversarial suffixes, originally proposed for jailbreaking, to get a pre-defined answer from the target LLM, while other models give random answers. TRAP detects the target LLMs with over 95% true positive rate at under 0.2% false positive rate even after a single interaction. TRAP remains effective even if the LLM has minor changes that do not significantly alter the original function.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a method called Targeted Random Adversarial Prompt (TRAP) that identifies the specific LLM in use and repurposes adversarial suffixes, originally proposed for jailbreaking, to get a pre-defined answer from the target LLM, while other models give random answers."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is detecting and studying adversarial attacks on language models using honeypot prompts, while the paper focuses on identifying specific language models used in third-party applications through targeted adversarial prompts.\n\nThe proposal's approach is to create decoy prompts that attract adversarial attacks and collect data to improve defense mechanisms, whereas the paper's method uses adversarial suffixes to get a predefined answer from the target model for identification purposes.\n\nAlthough both studies involve adversarial prompts, their objectives and methodologies differ significantly. The proposal aims to study and defend against attacks, while the paper focuses on identifying specific models in use.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "47030369e97cc44d4b2e3cf1be85da0fd134904a",
            "paperId": "47030369e97cc44d4b2e3cf1be85da0fd134904a",
            "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
            "abstract": "Because\"out-of-the-box\"large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called\"jailbreaks\"against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods. Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.",
            "year": 2023,
            "citationCount": 386,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is detecting and studying adversarial attacks on language models using honeypot prompts, while the paper focuses on generating transferable adversarial prompts that can induce objectionable content from aligned language models.\n\nProposal summary: Detecting and studying adversarial attacks on language models using honeypot prompts.\nPaper summary: Generating transferable adversarial prompts that can induce objectionable content from aligned language models.\n\nThe key difference is that the proposal aims to detect and study adversarial attacks, while the paper focuses on generating effective adversarial prompts. Although both deal with adversarial attacks on language models, their objectives and approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "77d6d7482d1a32ad147c39993758b6c63816f5c0",
            "paperId": "77d6d7482d1a32ad147c39993758b6c63816f5c0",
            "title": "PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
            "abstract": "The increasing reliance on Large Language Models (LLMs) across academia and industry necessitates a comprehensive understanding of their robustness to prompts. In response to this vital need, we introduce PromptBench, a robustness benchmark designed to measure LLMs' resilience to adversarial prompts. This study uses a plethora of adversarial textual attacks targeting prompts across multiple levels: character, word, sentence, and semantic. The adversarial prompts, crafted to mimic plausible user errors like typos or synonyms, aim to evaluate how slight deviations can affect LLM outcomes while maintaining semantic integrity. These prompts are then employed in diverse tasks, such as sentiment analysis, natural language inference, reading comprehension, machine translation, and math problem-solving. Our study generates 4788 adversarial prompts, meticulously evaluated over 8 tasks and 13 datasets. Our findings demonstrate that contemporary LLMs are not robust to adversarial prompts. Furthermore, we present comprehensive analysis to understand the mystery behind prompt robustness and its transferability. We then offer insightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users. Code is available at: https://github.com/microsoft/promptbench.",
            "year": 2023,
            "citationCount": 111,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study generates 4788 adversarial prompts and presents comprehensive analysis to understand the mystery behind prompt robustness and its transferability, and offers insightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is detecting and studying adversarial attacks on language models using honeypot prompts, while the paper focuses on evaluating the robustness of language models against adversarial prompts across various tasks and levels.\n\nProposal summary: Attracting and studying adversarial attacks on language models using honeypot prompts.\nPaper summary: Evaluating the robustness of language models against adversarial prompts across multiple tasks and levels.\n\nThe main difference is that the proposal aims to actively lure and detect attacks, while the paper focuses on evaluating the model's inherent robustness against adversarial prompts without using honeypots.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "1104d766527dead44a40532e8a89444d9cef5c65",
            "paperId": "1104d766527dead44a40532e8a89444d9cef5c65",
            "title": "\"Do Anything Now\": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models",
            "abstract": "The misuse of large language models (LLMs) has garnered significant attention from the general public and LLM vendors. In response, efforts have been made to align LLMs with human values and intent use. However, a particular type of adversarial prompts, known as jailbreak prompt, has emerged and continuously evolved to bypass the safeguards and elicit harmful content from LLMs. In this paper, we conduct the first measurement study on jailbreak prompts in the wild, with 6,387 prompts collected from four platforms over six months. Leveraging natural language processing technologies and graph-based community detection methods, we discover unique characteristics of jailbreak prompts and their major attack strategies, such as prompt injection and privilege escalation. We also observe that jailbreak prompts increasingly shift from public platforms to private ones, posing new challenges for LLM vendors in proactive detection. To assess the potential harm caused by jailbreak prompts, we create a question set comprising 46,800 samples across 13 forbidden scenarios. Our experiments show that current LLMs and safeguards cannot adequately defend jailbreak prompts in all scenarios. Particularly, we identify two highly effective jailbreak prompts which achieve 0.99 attack success rates on ChatGPT (GPT-3.5) and GPT-4, and they have persisted online for over 100 days. Our work sheds light on the severe and evolving threat landscape of jailbreak prompts. We hope our study can facilitate the research community and LLM vendors in promoting safer and regulated LLMs.",
            "year": 2023,
            "citationCount": 69,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The first measurement study on jailbreak prompts in the wild is conducted, with 6,387 prompts collected from four platforms over six months, and it is shown that current LLMs and safeguards cannot adequately defend jailbreak Prompts in all scenarios."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is detecting and studying adversarial attacks on language models using honeypot prompts, while the paper focuses on characterizing and evaluating jailbreak prompts that bypass safeguards to elicit harmful content from language models.\n\nProposal: Detecting and studying adversarial attacks on language models using honeypot prompts.\nPaper: Characterizing and evaluating jailbreak prompts that bypass safeguards to elicit harmful content from language models.\n\nAlthough both studies deal with adversarial prompts, the proposal aims to create honeypot prompts to attract and detect attacks, while the paper analyzes existing jailbreak prompts and their effectiveness in bypassing safeguards.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "3e30a7ac4886b28eb50151f58e14a1d698cccd0e",
            "paperId": "3e30a7ac4886b28eb50151f58e14a1d698cccd0e",
            "title": "Baseline Defenses for Adversarial Attacks Against Aligned Language Models",
            "abstract": "As Large Language Models quickly become ubiquitous, it becomes critical to understand their security vulnerabilities. Recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. Drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: What threat models are practically useful in this domain? How do baseline defense techniques perform in this new domain? How does LLM security differ from computer vision? We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, discussing the various settings in which each is feasible and effective. Particularly, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. We discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. We find that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for LLMs. Future research will be needed to uncover whether more powerful optimizers can be developed, or whether the strength of filtering and preprocessing defenses is greater in the LLMs domain than it has been in computer vision.",
            "year": 2023,
            "citationCount": 97,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for LLMs."
            },
            "score": 5,
            "novelty_score": "The research problem in the proposal is detecting and studying adversarial attacks on language models using honeypot prompts, while the paper focuses on evaluating baseline defense techniques against adversarial attacks on language models.\n\nThe approach in the proposal is to create decoy prompts to lure and collect data on adversarial attacks, while the paper evaluates detection, input preprocessing, and adversarial training as defense strategies.\n\nThe proposal and the paper have different research problems and approaches, although they both deal with adversarial attacks on language models.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "92b9d8b8c81c4c53ea62000c0924500b2dd11bce",
            "paperId": "92b9d8b8c81c4c53ea62000c0924500b2dd11bce",
            "title": "Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models",
            "abstract": "We introduce new jailbreak attacks on vision language models (VLMs), which use aligned LLMs and are resilient to text-only jailbreak attacks. Specifically, we develop cross-modality attacks on alignment where we pair adversarial images going through the vision encoder with textual prompts to break the alignment of the language model. Our attacks employ a novel compositional strategy that combines an image, adversarially targeted towards toxic embeddings, with generic prompts to accomplish the jailbreak. Thus, the LLM draws the context to answer the generic prompt from the adversarial image. The generation of benign-appearing adversarial images leverages a novel embedding-space-based methodology, operating with no access to the LLM model. Instead, the attacks require access only to the vision encoder and utilize one of our four embedding space targeting strategies. By not requiring access to the LLM, the attacks lower the entry barrier for attackers, particularly when vision encoders such as CLIP are embedded in closed-source LLMs. The attacks achieve a high success rate across different VLMs, highlighting the risk of cross-modality alignment vulnerabilities, and the need for new alignment approaches for multi-modal models.",
            "year": 2023,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Cross-modality attacks on alignment where adversarial images going through the vision encoder with textual prompts to break the alignment of the language model are developed."
            },
            "score": 5,
            "novelty_score": "The research problem in the proposal is detecting and studying adversarial attacks on language models using honeypot prompts, while the paper focuses on introducing new jailbreak attacks on vision-language models by exploiting cross-modality alignment vulnerabilities.\n\nThe approach in the proposal involves creating decoy prompts to attract and log adversarial attacks, while the paper proposes a compositional strategy that combines adversarial images with generic prompts to break the alignment of the language model.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "620a1a585a5e433a47103c112de17553a81fcbe6",
            "paperId": "620a1a585a5e433a47103c112de17553a81fcbe6",
            "title": "Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks",
            "abstract": "Although remarkable progress has been achieved in preventing large language model (LLM) hallucinations using instruction tuning and retrieval augmentation, it remains challenging to measure the reliability of LLMs using human-crafted evaluation data which is not available for many tasks and domains and could suffer from data leakage. Inspired by adversarial machine learning, this paper aims to develop a method of automatically generating evaluation data by appropriately modifying existing data on which LLMs behave faithfully. Specifically, this paper presents AutoDebug, an LLM-based framework to use prompting chaining to generate transferable adversarial attacks in the form of question-answering examples. We seek to understand the extent to which these examples trigger the hallucination behaviors of LLMs. We implement AutoDebug using ChatGPT and evaluate the resulting two variants of a popular open-domain question-answering dataset, Natural Questions (NQ), on a collection of open-source and proprietary LLMs under various prompting settings. Our generated evaluation data is human-readable and, as we show, humans can answer these modified questions well. Nevertheless, we observe pronounced accuracy drops across multiple LLMs including GPT-4. Our experimental results show that LLMs are likely to hallucinate in two categories of question-answering scenarios where (1) there are conflicts between knowledge given in the prompt and their parametric knowledge, or (2) the knowledge expressed in the prompt is complex. Finally, we find that the adversarial examples generated by our method are transferable across all considered LLMs. The examples generated by a small model can be used to debug a much larger model, making our approach cost-effective.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents AutoDebug, an LLM-based framework to use prompting chaining to generate transferable adversarial attacks in the form of question-answering examples, and finds that the adversarial examples generated by the method are transferable across all considered LLMs."
            },
            "score": 5,
            "novelty_score": "The research problem in the proposal is detecting and studying adversarial attacks on language models using honeypot prompts, while the paper focuses on automatically generating adversarial examples to assess the hallucination behavior of language models.\n\nProposal: Detecting and studying adversarial attacks on language models using honeypot prompts.\nPaper: Automatically generating adversarial examples to assess the hallucination behavior of language models.\n\nThe two works have different research problems and approaches. The proposal aims to detect and study adversarial attacks using honeypot prompts, while the paper focuses on generating adversarial examples to assess hallucination behavior.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "8fdd34153d1035d09dd4a6efa9cb0c91d23d0045",
            "paperId": "8fdd34153d1035d09dd4a6efa9cb0c91d23d0045",
            "title": "More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models",
            "abstract": "We are currently witnessing dramatic advances in the capabilities of Large Language Models (LLMs). They are already being adopted in practice and integrated into many systems, including integrated development environments (IDEs) and search engines. The functionalities of current LLMs can be modulated via natural language prompts, while their exact internal functionality remains implicit and unassessable. This property, which makes them adaptable to even unseen tasks, might also make them susceptible to targeted adversarial prompting . Recently, several ways to misalign LLMs using Prompt Injection (PI) attacks have been introduced. In such attacks, an adversary can prompt the LLM to produce malicious content or override the original instructions and the employed \ufb01ltering schemes. Recent work showed that these attacks are hard to mitigate, as state-of-the-art LLMs are instruction-following . So far, these attacks assumed that the adversary is directly prompting the LLM. In this work, we show that augmenting LLMs with retrieval and API calling capabilities (so-called Application-Integrated LLMs ) induces a whole new set of attack vectors. These LLMs might process poisoned content retrieved from the Web that contains malicious prompts pre-injected and selected by adversaries. We demonstrate that an attacker can indirectly perform such PI attacks. Based on this key insight, we systematically analyze the resulting threat landscape of Application-Integrated LLMs and discuss a variety of new attack vectors. To demonstrate the practical viability of our attacks, we implemented speci\ufb01c demonstrations",
            "year": 2023,
            "citationCount": 73,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work shows that augmenting LLMs with retrieval and API calling capabilities (so-called Application-Integrated LLMs) induces a whole new set of attack vectors and systematically analyzes the resulting threat landscape of Application-Integrated LLMs."
            },
            "score": 5,
            "novelty_score": "The research problem in the proposal is detecting and studying adversarial attacks on language models using honeypot prompts, while the paper focuses on analyzing new prompt injection threats to application-integrated large language models that process retrieved content.\n\nProposal summary: Detecting and studying adversarial attacks on language models using honeypot prompts.\nPaper summary: Analyzing new prompt injection threats to application-integrated large language models that process retrieved content.\n\nThe key difference is that the proposal aims to actively detect and study adversarial attacks using honeypot prompts, while the paper passively analyzes new threats arising from the integration of LLMs with retrieval and API calling capabilities.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "2c72ab10e7a5f2fd32e6f85b20c77bf64e6e220d",
            "paperId": "2c72ab10e7a5f2fd32e6f85b20c77bf64e6e220d",
            "title": "A prompt-based approach to adversarial example generation and robustness enhancement",
            "abstract": null,
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel robust training approach based on prompt paradigm which incorporates prompt texts as the alternatives to adversarial examples and enhances robustness under a lightweight minimax-style optimization framework is proposed."
            },
            "score": 5
        },
        {
            "id": "32316f1d08911b397f52f8644f46f5c129619383",
            "paperId": "32316f1d08911b397f52f8644f46f5c129619383",
            "title": "Defending Against Indirect Prompt Injection Attacks With Spotlighting",
            "abstract": "Large Language Models (LLMs), while powerful, are built and trained to process a single text input. In common applications, multiple inputs can be processed by concatenating them together into a single stream of text. However, the LLM is unable to distinguish which sections of prompt belong to various input sources. Indirect prompt injection attacks take advantage of this vulnerability by embedding adversarial instructions into untrusted data being processed alongside user commands. Often, the LLM will mistake the adversarial instructions as user commands to be followed, creating a security vulnerability in the larger system. We introduce spotlighting, a family of prompt engineering techniques that can be used to improve LLMs' ability to distinguish among multiple sources of input. The key insight is to utilize transformations of an input to provide a reliable and continuous signal of its provenance. We evaluate spotlighting as a defense against indirect prompt injection attacks, and find that it is a robust defense that has minimal detrimental impact to underlying NLP tasks. Using GPT-family models, we find that spotlighting reduces the attack success rate from greater than {50}\\% to below {2}\\% in our experiments with minimal impact on task efficacy.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces spotlighting, a family of prompt engineering techniques that can be used to improve LLMs' ability to distinguish among multiple sources of input, and finds that it is a robust defense that has minimal detrimental impact to underlying NLP tasks."
            },
            "score": 5
        },
        {
            "id": "1abfc211793c683972ded8d3268475e3ee7a88b0",
            "paperId": "1abfc211793c683972ded8d3268475e3ee7a88b0",
            "title": "Adversarial Demonstration Attacks on Large Language Models",
            "abstract": "With the emergence of more powerful large language models (LLMs), such as ChatGPT and GPT-4, in-context learning (ICL) has gained significant prominence in leveraging these models for specific tasks by utilizing data-label pairs as precondition prompts. While incorporating demonstrations can greatly enhance the performance of LLMs across various tasks, it may introduce a new security concern: attackers can manipulate only the demonstrations without changing the input to perform an attack. In this paper, we investigate the security concern of ICL from an adversarial perspective, focusing on the impact of demonstrations. We propose a novel attack method named advICL, which aims to manipulate only the demonstration without changing the input to mislead the models. Our results demonstrate that as the number of demonstrations increases, the robustness of in-context learning would decrease. Additionally, we also identify the intrinsic property of the demonstrations is that they can be used (prepended) with different inputs. As a result, it introduces a more practical threat model in which an attacker can attack the test input example even without knowing and manipulating it. To achieve it, we propose the transferable version of advICL, named Transferable-advICL. Our experiment shows that the adversarial demonstration generated by Transferable-advICL can successfully attack the unseen test input examples. We hope that our study reveals the critical security risks associated with ICL and underscores the need for extensive research on the robustness of ICL, particularly given its increasing significance in the advancement of LLMs.",
            "year": 2023,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper investigates the security concern of ICL from an adversarial perspective, focusing on the impact of demonstrations, and proposes a novel attack method named advICL, which aims to manipulate only the demonstration without changing the input to mislead the models."
            },
            "score": 4
        },
        {
            "id": "40ee4949c1050a465d418deb6dd7ea6304a3bc29",
            "paperId": "40ee4949c1050a465d418deb6dd7ea6304a3bc29",
            "title": "Adversarial Attacks and Defenses in Large Language Models: Old and New Threats",
            "abstract": "Over the past decade, there has been extensive research aimed at enhancing the robustness of neural networks, yet this problem remains vastly unsolved. Here, one major impediment has been the overestimation of the robustness of new defense approaches due to faulty defense evaluations. Flawed robustness evaluations necessitate rectifications in subsequent works, dangerously slowing down the research and providing a false sense of security. In this context, we will face substantial challenges associated with an impending adversarial arms race in natural language processing, specifically with closed-source Large Language Models (LLMs), such as ChatGPT, Google Bard, or Anthropic's Claude. We provide a first set of prerequisites to improve the robustness assessment of new approaches and reduce the amount of faulty evaluations. Additionally, we identify embedding space attacks on LLMs as another viable threat model for the purposes of generating malicious content in open-sourced models. Finally, we demonstrate on a recently proposed defense that, without LLM-specific best practices in place, it is easy to overestimate the robustness of a new approach.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work provides a first set of prerequisites to improve the robustness assessment of new approaches and reduce the amount of faulty evaluations, and identifies embedding space attacks on LLMs as another viable threat model for the purposes of generating malicious content in open-sourced models."
            },
            "score": 4
        },
        {
            "id": "9ed4387cf5f25aa53d13f29b5b5c107f70a881cc",
            "paperId": "9ed4387cf5f25aa53d13f29b5b5c107f70a881cc",
            "title": "Robustness Evaluation of Cloud-Deployed Large Language Models against Chinese Adversarial Text Attacks",
            "abstract": "In the evolving digital realm, Large Language Models (LLMs) like ChatGPT, which recently achieved state-of-the-art results across diverse NLP tasks, are extensively used. Deployed on the cloud, ChatGPT allows interaction via its API, providing rich and high-quality solutions. However, its vulnerability to adversarial attacks, potentially compromising the quality and reliability of cloud services and leading to information leakage, raises security concerns. Investigating the robustness of ChatGPT against adversarial attacks enables a preliminary understanding of its weaknesses and facilitates the subsequent integration of targeted defensive mechanisms into the cloud framework. Most current research on the robustness of LLMs against adversarial attacks focuses on BERT, with few studies on ChatGPT under similar conditions. This paper explores the robustness of ChatGPT against Chinese adversarial text attacks in text classification tasks and proposes a ChatGPT-based adversarial text fluency evaluation method that eliminates the need for human involvement. Experiments conducted on the real-world dataset, THUCNews, examined the robustness of Chinese BERT and ChatGPT against adversarial attacks generated via various Chinese adversarial text generation methods. A multidimensional assessment revealed that both models are susceptible to attacks, leading to decreased text classification accuracy. The attack success rate on ChatGPT reached nearly 45%.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper explores the robustness of ChatGPT against Chinese adversarial text attacks in text classification tasks and proposes a ChatGPT-based adversarial text fluency evaluation method that eliminates the need for human involvement."
            },
            "score": 4
        },
        {
            "id": "e519699816d358783f41d4bd50fd3465d9fa51bd",
            "paperId": "e519699816d358783f41d4bd50fd3465d9fa51bd",
            "title": "Fast Adversarial Attacks on Language Models In One GPU Minute",
            "abstract": "In this paper, we introduce a novel class of fast, beam search-based adversarial attack (BEAST) for Language Models (LMs). BEAST employs interpretable parameters, enabling attackers to balance between attack speed, success rate, and the readability of adversarial prompts. The computational efficiency of BEAST facilitates us to investigate its applications on LMs for jailbreaking, eliciting hallucinations, and privacy attacks. Our gradient-free targeted attack can jailbreak aligned LMs with high attack success rates within one minute. For instance, BEAST can jailbreak Vicuna-7B-v1.5 under one minute with a success rate of 89% when compared to a gradient-based baseline that takes over an hour to achieve 70% success rate using a single Nvidia RTX A6000 48GB GPU. Additionally, we discover a unique outcome wherein our untargeted attack induces hallucinations in LM chatbots. Through human evaluations, we find that our untargeted attack causes Vicuna-7B-v1.5 to produce ~15% more incorrect outputs when compared to LM outputs in the absence of our attack. We also learn that 22% of the time, BEAST causes Vicuna to generate outputs that are not relevant to the original prompt. Further, we use BEAST to generate adversarial prompts in a few seconds that can boost the performance of existing membership inference attacks for LMs. We believe that our fast attack, BEAST, has the potential to accelerate research in LM security and privacy. Our codebase is publicly available at https://github.com/vinusankars/BEAST.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a novel class of fast, beam search-based adversarial attack (BEAST) for Language Models (LMs), and uses BEAST to generate adversarial prompts in a few seconds that can boost the performance of existing membership inference attacks for LMs."
            },
            "score": 4
        },
        {
            "id": "9a0f53a0ff25d1a5fb016aaa22185c4d6b5a2ac8",
            "paperId": "9a0f53a0ff25d1a5fb016aaa22185c4d6b5a2ac8",
            "title": "LinkPrompt: Natural and Universal Adversarial Attacks on Prompt-based Language Models",
            "abstract": "Prompt-based learning is a new language model training paradigm that adapts the Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes the performance benchmarks across various natural language processing (NLP) tasks. Instead of using a fixed prompt template to fine-tune the model, some research demonstrates the effectiveness of searching for the prompt via optimization. Such prompt optimization process of prompt-based learning on PLMs also gives insight into generating adversarial prompts to mislead the model, raising concerns about the adversarial vulnerability of this paradigm. Recent studies have shown that universal adversarial triggers (UATs) can be generated to alter not only the predictions of the target PLMs but also the prediction of corresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based learning paradigm. However, UATs found in previous works are often unreadable tokens or characters and can be easily distinguished from natural texts with adaptive defenses. In this work, we consider the naturalness of the UATs and develop $\\textit{LinkPrompt}$, an adversarial attack algorithm to generate UATs by a gradient-based beam search algorithm that not only effectively attacks the target PLMs and PFMs but also maintains the naturalness among the trigger tokens. Extensive results demonstrate the effectiveness of $\\textit{LinkPrompt}$, as well as the transferability of UATs generated by $\\textit{LinkPrompt}$ to open-sourced Large Language Model (LLM) Llama2 and API-accessed LLM GPT-3.5-turbo. The resource is available at $\\href{https://github.com/SavannahXu79/LinkPrompt}{https://github.com/SavannahXu79/LinkPrompt}$.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An adversarial attack algorithm to generate UATs by a gradient-based beam search algorithm that not only effectively attacks the target PLMs and PFMs but also maintains the naturalness among the trigger tokens is developed."
            },
            "score": 4
        },
        {
            "id": "292b2a552236a7e37659a0ea132ee7ac34041668",
            "paperId": "292b2a552236a7e37659a0ea132ee7ac34041668",
            "title": "Experimental Evaluation of Adversarial Attacks Against Natural Language Machine Learning Models",
            "abstract": "Machine learning models are being increasingly relied on for many natural language processing tasks. However, these models are vulnerable to adversarial attacks, i.e., inputs designed to target models into making a wrong prediction. Among different methods of attacking a model, it is important to understand what attacks are effective, so that we can design countermeasures to protect the models. In this paper, we design and implement six adversarial attacks against natural language machine learning models. Then, we evaluate the effectiveness of these attacks using a fine-tuned distilled BERT model and 5,000 sample sentences from the SST-2 dataset. Our results indicate that the Word-replace attack affected the model the most, which reduces the F1-score of the model by 34%. The Word-delete attack is the least effective, but still reduces the model\u2019s accuracy by 17%. Based on the experimental results, we discuss our insights and provide our recommendations for building robust natural language machine learning models.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Six adversarial attacks against natural language machine learning models are designed and implemented and the effectiveness of these attacks are evaluated using a fine-tuned distilled BERT model and 5,000 sample sentences from the SST-2 dataset."
            },
            "score": 4
        },
        {
            "id": "c1d1842e08716cbf7250167969522a1705d8bcd3",
            "paperId": "c1d1842e08716cbf7250167969522a1705d8bcd3",
            "title": "Token-Level Adversarial Prompt Detection Based on Perplexity Measures and Contextual Information",
            "abstract": "In recent years, Large Language Models (LLM) have emerged as pivotal tools in various applications. However, these models are susceptible to adversarial prompt attacks, where attackers can carefully curate input strings that mislead LLMs into generating incorrect or undesired outputs. Previous work has revealed that with relatively simple yet effective attacks based on discrete optimization, it is possible to generate adversarial prompts that bypass moderation and alignment of the models. This vulnerability to adversarial prompts underscores a significant concern regarding the robustness and reliability of LLMs. Our work aims to address this concern by introducing a novel approach to detecting adversarial prompts at a token level, leveraging the LLM's capability to predict the next token's probability. We measure the degree of the model's perplexity, where tokens predicted with high probability are considered normal, and those exhibiting high perplexity are flagged as adversarial. Additionaly, our method also integrates context understanding by incorporating neighboring token information to encourage the detection of contiguous adversarial prompt sequences. To this end, we design two algorithms for adversarial prompt detection: one based on optimization techniques and another on Probabilistic Graphical Models (PGM). Both methods are equipped with efficient solving methods, ensuring efficient adversarial prompt detection. Our token-level detection result can be visualized as heatmap overlays on the text sequence, allowing for a clearer and more intuitive representation of which part of the text may contain adversarial prompts.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a novel approach to detecting adversarial prompts at a token level, leveraging the LLM's capability to predict the next token's probability, and designs two algorithms for adversarial prompt detection, one based on optimization techniques and another on Probabilistic Graphical Models."
            },
            "score": 4
        },
        {
            "id": "b5a624da64475d735f0e298dc6f2f6669b5bb697",
            "paperId": "b5a624da64475d735f0e298dc6f2f6669b5bb697",
            "title": "Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield",
            "abstract": "Large Language Models' safety remains a critical concern due to their vulnerability to adversarial attacks, which can prompt these systems to produce harmful responses. In the heart of these systems lies a safety classifier, a computational model trained to discern and mitigate potentially harmful, offensive, or unethical outputs. However, contemporary safety classifiers, despite their potential, often fail when exposed to inputs infused with adversarial noise. In response, our study introduces the Adversarial Prompt Shield (APS), a lightweight model that excels in detection accuracy and demonstrates resilience against adversarial prompts. Additionally, we propose novel strategies for autonomously generating adversarial training datasets, named Bot Adversarial Noisy Dialogue (BAND) datasets. These datasets are designed to fortify the safety classifier's robustness, and we investigate the consequences of incorporating adversarial examples into the training process. Through evaluations involving Large Language Models, we demonstrate that our classifier has the potential to decrease the attack success rate resulting from adversarial attacks by up to 60%. This advancement paves the way for the next generation of more reliable and resilient conversational agents.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study introduces the Adversarial Prompt Shield (APS), a lightweight model that excels in detection accuracy and demonstrates resilience against adversarial prompts, and proposes novel strategies for autonomously generating adversarial training datasets, designed to fortify the safety classifier's robustness."
            },
            "score": 4
        },
        {
            "id": "e2302eda403de7000669813cf23bc0c0c08f3e93",
            "paperId": "e2302eda403de7000669813cf23bc0c0c08f3e93",
            "title": "COVER: A Heuristic Greedy Adversarial Attack on Prompt-based Learning in Language Models",
            "abstract": "Prompt-based learning has been proved to be an effective way in pre-trained language models (PLMs), especially in low-resource scenarios like few-shot settings. However, the trustworthiness of PLMs is of paramount significance and potential vulnerabilities have been shown in prompt-based templates that could mislead the predictions of language models, causing serious security concerns. In this paper, we will shed light on some vulnerabilities of PLMs, by proposing a prompt-based adversarial attack on manual templates in black box scenarios. First of all, we design character-level and word-level heuristic approaches to break manual templates separately. Then we present a greedy algorithm for the attack based on the above heuristic destructive approaches. Finally, we evaluate our approach with the classification tasks on three variants of BERT series models and eight datasets. And comprehensive experimental results justify the effectiveness of our approach in terms of attack success rate and attack speed.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a prompt-based adversarial attack on manual templates in black box scenarios by designing character-level and word-level heuristic approaches to break manual templates separately and presents a greedy algorithm for the attack based on the above heuristic destructive approaches."
            },
            "score": 4
        },
        {
            "id": "07cfaf543b2bd991406be1d72a52d784cc9c62fb",
            "paperId": "07cfaf543b2bd991406be1d72a52d784cc9c62fb",
            "title": "Prompt Makes mask Language Models Better Adversarial Attackers",
            "abstract": "Generating high-quality synonymous perturbations is a core challenge for textual adversarial tasks. However, candidates generated from the masked language model often contain many words that are antonyms or irrelevant to the original words, which limit the perturbation space and affect the attack\u2019s effectiveness. We present ProAttacker1 which uses Prompt to make the mask language models better adversarial Attackers. ProAttacker inverts the prompt paradigm by leveraging the prompt with the class label to guide the language model to generate more semantically-consistent perturbations. We present a systematic evaluation to analyze the attack performance on 6 NLP datasets, covering text classification and inference. Our experiments demonstrate that ProAttacker outperforms state-of-the-art attack strategies in both success rate and perturb rate.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ProAttacker1, which uses Prompt to make the mask language models better adversarial Attackers, inverts the prompt paradigm by leveraging the prompt with the class label to guide the language model to generate more semantically-consistent perturbations."
            },
            "score": 4
        },
        {
            "id": "afee8cdc51e95b50d7574ed1700a797874bf792c",
            "paperId": "afee8cdc51e95b50d7574ed1700a797874bf792c",
            "title": "Adversarial Fine-Tuning of Language Models: An Iterative Optimisation Approach for the Generation and Detection of Problematic Content",
            "abstract": "In this paper, we tackle the emerging challenge of unintended harmful content generation in Large Language Models (LLMs) with a novel dual-stage optimisation technique using adversarial fine-tuning. Our two-pronged approach employs an adversarial model, fine-tuned to generate potentially harmful prompts, and a judge model, iteratively optimised to discern these prompts. In this adversarial cycle, the two models seek to outperform each other in the prompting phase, generating a dataset of rich examples which are then used for fine-tuning. This iterative application of prompting and fine-tuning allows continuous refinement and improved performance. The performance of our approach is evaluated through classification accuracy on a dataset consisting of problematic prompts not detected by GPT-4, as well as a selection of contentious but unproblematic prompts. We show considerable increase in classification accuracy of the judge model on this challenging dataset as it undergoes the optimisation process. Furthermore, we show that a rudimentary model \\texttt{ada} can achieve 13\\% higher accuracy on the hold-out test set than GPT-4 after only a few rounds of this process, and that this fine-tuning improves performance in parallel tasks such as toxic comment identification.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper shows that a rudimentary model can achieve 13\\% higher accuracy on the hold-out test set than GPT-4 after only a few rounds of this process, and that this fine-tuning improves performance in parallel tasks such as toxic comment identification."
            },
            "score": 4
        },
        {
            "id": "43deb913bdb582ae6cffdb3f2796ec15efb77097",
            "paperId": "43deb913bdb582ae6cffdb3f2796ec15efb77097",
            "title": "Improved and Efficient Text Adversarial Attacks using Target Information",
            "abstract": "There has been recently a growing interest in studying adversarial examples on natural language models in the black-box setting. These methods attack natural language classifiers by perturbing certain important words until the classifier label is changed. In order to find these important words, these methods rank all words by importance by querying the target model word by word for each input sentence, resulting in high query inefficiency. A new interesting approach was introduced that addresses this problem through interpretable learning to learn the word ranking instead of previous expensive search. The main advantage of using this approach is that it achieves comparable attack rates to the state-of-the-art methods, yet faster and with fewer queries, where fewer queries are desirable to avoid suspicion towards the attacking agent. Nonetheless, this approach sacrificed the useful information that could be leveraged from the target classifier for that sake of query efficiency. In this paper we study the effect of leveraging the target model outputs and data on both attack rates and average number of queries, and we show that both can be improved, with a limited overhead of additional queries.",
            "year": 2021,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The effect of leveraging the target model outputs and data on both attack rates and average number of queries are studied, and it is shown that both can be improved, with a limited overhead of additional queries."
            },
            "score": 4
        },
        {
            "id": "181eee58acc17bf82653a055393f31b90ce40d8f",
            "paperId": "181eee58acc17bf82653a055393f31b90ce40d8f",
            "title": "Transferability of Evasion Attacks in Machine Learning Models",
            "abstract": "Natural language processing to computer vision is among the many applications of machine learning. Adversarial examples can cause models to misclassify inputs, despite the fact these models are resilient to attacks. One type of adversarial attack is an evasion attack, where the attacker modifies an input to cause a model to produce an incorrect output. The transferability of evasion attacks refers to the ability of an attack designed for one model to be effective against other models. This property is important because it implies that an attacker can launch a successful attack against a large number of models without having to design a separate attack for each individual model. This paper investigates whether evasion attacks are transferable to machine learning models. Analyzing various models and studying factors contributing to the transferability of attacks, we evaluate the performance of attacks against a variety of them. Our results show that transferability is a common property of evasion attacks and suggest that this property is due to the presence of similar features in the models being attacked. Our findings have implications for the security of machine learning models and suggest that transferability of attacks should be taken into account when designing and evaluating machine learning models for security-critical applications.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Results show that transferability is a common property of evasion attacks and suggest that this property is due to the presence of similar features in the models being attacked, which should be taken into account when designing and evaluating machine learning models for security-critical applications."
            },
            "score": 4
        },
        {
            "id": "160b67f793ce4d8e0492b3b7918aaefd44350f92",
            "paperId": "160b67f793ce4d8e0492b3b7918aaefd44350f92",
            "title": "Generating Prompt-Based Adversarial Text Examples via Variable Neighborhood Search",
            "abstract": "Natural Language Processing (NLP) models are immensely vulnerable to adversarial text examples. Various word-level attacks have been proposed to modify input texts by carefully-picked substitute words via static or dynamic opti-mization algorithms. However, existing word-level attack methods usually ignore text fluency and semantic consistency for seeking a high attack success ratio, often resulting in unnatural adversarial text examples. In this paper, we propose to generate Prompt-based adversarial texts via Variable Neighborhood Search (P-VNS), which achieves a high attack success ratio while simulta-neously keeping text fluency and semantic similarity. Specifically, the well-designed prompt texts are constructed for input texts and the substitute words are obtained by mask-and-filling procedure under the effect of prompt texts, so the text fluency and semantic similarity can be enhanced. Additionally, the word modification priority is adaptively determined by employing the variable neighborhood search algorithm, yielding an improvement in the attack success ratio. Extensive experiments demonstrate that the P- VNS accomplishes the highest attack success ratio meanwhile preserving text fluency and semantic similarity. Besides, the pro-posed P- VNS also manifests effectiveness in adversarial training and transfer attack.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes to generate Prompt-based adversarial texts via Variable Neighborhood Search (P-VNS), which achieves a high attack success ratio while simulta-neously keeping text fluency and semantic similarity."
            },
            "score": 4
        },
        {
            "id": "4d9fc5972ab0f17f3c8aa27b4d9372f029d4dded",
            "paperId": "4d9fc5972ab0f17f3c8aa27b4d9372f029d4dded",
            "title": "Adversarial Attacks on Large Language Model-Based System and Mitigating Strategies: A Case Study on ChatGPT",
            "abstract": "Machine learning algorithms are at the forefront of the development of advanced information systems. The rapid progress in machine learning technology has enabled cutting-edge large language models (LLMs), represented by GPT-3 and ChatGPT, to perform a wide range of NLP tasks with a stunning performance. However, research on adversarial machine learning highlights the need for these intelligent systems to be more robust. Adversarial machine learning aims to evaluate attack and defense mechanisms to prevent the malicious exploitation of these systems. In the case of ChatGPT, adversarial induction prompt can cause the model to generate toxic texts that could pose serious security risks or propagate false information. To address this challenge, we first analyze the effectiveness of inducing attacks on ChatGPT. Then, two effective mitigating mechanisms are proposed. The first is a training-free prefix prompt mechanism to detect and prevent the generation of toxic texts. The second is a RoBERTa-based mechanism that identifies manipulative or misleading input text via external detection models. The availability of this method is demonstrated through experiments.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A training-free prefix prompt mechanism to detect and prevent the generation of toxic texts and a RoBERTa-based mechanism that identifies manipulative or misleading input text via external detection models are proposed."
            },
            "score": 4
        },
        {
            "id": "b6499bcc10d4a70c3ca8b84995270cfd0d29de4c",
            "paperId": "b6499bcc10d4a70c3ca8b84995270cfd0d29de4c",
            "title": "Model-tuning Via Prompts Makes NLP Models Adversarially Robust",
            "abstract": "In recent years, NLP practitioners have converged on the following practice: (i) import an off-the-shelf pretrained (masked) language model; (ii) append a multilayer perceptron atop the CLS token's hidden representation (with randomly initialized weights); and (iii) fine-tune the entire model on a downstream task (MLP-FT). This procedure has produced massive gains on standard NLP benchmarks, but these models remain brittle, even to mild adversarial perturbations. In this work, we demonstrate surprising gains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an alternative method of adapting to downstream tasks. Rather than appending an MLP head to make output prediction, MVP appends a prompt template to the input, and makes prediction via text infilling/completion. Across 5 NLP datasets, 4 adversarial attacks, and 3 different models, MVP improves performance against adversarial substitutions by an average of 8% over standard methods and even outperforms adversarial training-based state-of-art defenses by 3.5%. By combining MVP with adversarial training, we achieve further improvements in adversarial robustness while maintaining performance on unperturbed examples. Finally, we conduct ablations to investigate the mechanism underlying these gains. Notably, we find that the main causes of vulnerability of MLP-FT can be attributed to the misalignment between pre-training and fine-tuning tasks, and the randomly initialized MLP parameters.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work demonstrates surprising gains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an alternative method of adapting to downstream tasks that improves performance against adversarial substitutions and outperforms adversarial training-based state-of-art defenses by 3.5%."
            },
            "score": 4
        },
        {
            "id": "de2fd685f45ee916b9142bcb983d306b7da643a4",
            "paperId": "de2fd685f45ee916b9142bcb983d306b7da643a4",
            "title": "A Prompting-based Approach for Adversarial Example Generation and Robustness Enhancement",
            "abstract": "Recent years have seen the wide application of NLP models in crucial areas such as finance, medical treatment, and news media, raising concerns of the model robustness and vulnerabilities. In this paper, we propose a novel prompt-based adversarial attack to compromise NLP models and robustness enhancement technique. We first construct malicious prompts for each instance and generate adversarial examples via mask-and-filling under the effect of a malicious purpose. Our attack technique targets the inherent vulnerabilities of NLP models, allowing us to generate samples even without interacting with the victim NLP model, as long as it is based on pre-trained language models (PLMs). Furthermore, we design a prompt-based adversarial training method to improve the robustness of PLMs. As our training method does not actually generate adversarial samples, it can be applied to large-scale training sets efficiently. The experimental results show that our attack method can achieve a high attack success rate with more diverse, fluent and natural adversarial examples. In addition, our robustness enhancement method can significantly improve the robustness of models to resist adversarial attacks. Our work indicates that prompting paradigm has great potential in probing some fundamental flaws of PLMs and fine-tuning them for downstream tasks.",
            "year": 2022,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel prompt-based adversarial attack to compromise NLP models and robustness enhancement technique that can significantly improve the robustness of models to resist adversarial attacks and indicates that prompting paradigm has great potential in probing some fundamental flaws of PLMs and fine-tuning them for downstream tasks."
            },
            "score": 4
        },
        {
            "id": "b2fd6a1b86af52bd6aaa5988a1efc408df6477f3",
            "paperId": "b2fd6a1b86af52bd6aaa5988a1efc408df6477f3",
            "title": "ZDDR: A Zero-Shot Defender for Adversarial Samples Detection and Restoration",
            "abstract": "Natural language processing (NLP) models find extensive applications but face vulnerabilities against adversarial inputs. Traditional defenses lean heavily on supervised detection techniques, which makes them vulnerable to issues arising from training data quality, inherent biases, noise, or adversarial inputs. This study observed common compromises in sentence fluency during aggression. On this basis, the Zero Sample Defender (ZDDR) is introduced for adversarial sample detection and recovery without relying on prior knowledge. ZDDR combines the log probability calculated by the model and the syntactic normative score of a large language model (LLM) to detect adversarial examples. Furthermore, using strategic prompts, ZDDR guides LLM in rephrasing adversarial content, maintaining clarity, structure, and meaning, thereby restoring the sentence from the attack. Benchmarking reveals a 9% improvement in area under receiver operating characteristic curve (AUROC) for adversarial detection over existing techniques. Post-restoration, model classification efficacy surges by 45% compared to the offensive inputs, setting new performance standards against other restoration techniques.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Zero Sample Defender (ZDDR) is introduced for adversarial sample detection and recovery without relying on prior knowledge, and reveals a 9% improvement in area under receiver operating characteristic curve (AUROC) for adversarial detection over existing techniques."
            },
            "score": 4
        },
        {
            "id": "28cb357d41869129fdb7972bb3cbf5e1734d05c6",
            "paperId": "28cb357d41869129fdb7972bb3cbf5e1734d05c6",
            "title": "A Honey-imprint enabled Approach for Resisting Social Engineering Attacks",
            "abstract": "In the Reconnaissance step of the Cyber Kill Chain (CKC) model, social engineering (SE) techniques are often used to obtain sensitive/private data. This paper proposes a \"honey-imprint\" enabled approach which takes advantage of Natural Language Processing (NLP) for detecting SE attacks, Generative Adversarial Networks (GAN) for generating decoys from original sensitive information, and steganography for imprinting the honey watermark. The purpose of honey-imprint is to protect the sensitive information in the original file while leaving a covert imprint on the honey file to identify the malicious user. With this, we can further capture malicious interactions (using honey-imprinted data) by the honeypot system. We implement a prototype to verify the design, and the experimental results show that the method is valid and effective.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A \"honey-imprint\" enabled approach which takes advantage of Natural Language Processing (NLP) for detecting SE attacks, Generative Adversarial Networks (GAN) for generating decoys from original sensitive information, and steganography for imprinting the honey watermark is proposed."
            },
            "score": 4
        },
        {
            "id": "05d2ced6a4fb7efb8d527a228ad792526a202235",
            "paperId": "05d2ced6a4fb7efb8d527a228ad792526a202235",
            "title": "How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities",
            "abstract": "The rapid progress in open-source Large Language Models (LLMs) is significantly driving AI development forward. However, there is still a limited understanding of their trustworthiness. Deploying these models at scale without sufficient trustworthiness can pose significant risks, highlighting the need to uncover these issues promptly. In this work, we conduct an adversarial assessment of open-source LLMs on trustworthiness, scrutinizing them across eight different aspects including toxicity, stereotypes, ethics, hallucination, fairness, sycophancy, privacy, and robustness against adversarial demonstrations. We propose advCoU, an extended Chain of Utterances-based (CoU) prompting strategy by incorporating carefully crafted malicious demonstrations for trustworthiness attack. Our extensive experiments encompass recent and representative series of open-source LLMs, including Vicuna, MPT, Falcon, Mistral, and Llama 2. The empirical outcomes underscore the efficacy of our attack strategy across diverse aspects. More interestingly, our result analysis reveals that models with superior performance in general NLP tasks do not always have greater trustworthiness; in fact, larger models can be more vulnerable to attacks. Additionally, models that have undergone instruction tuning, focusing on instruction following, tend to be more susceptible, although fine-tuning LLMs for safety alignment proves effective in mitigating adversarial trustworthiness attacks.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An adversarial assessment of open-source LLMs on trustworthiness is conducted, scrutinizing them across eight different aspects including toxicity, stereotypes, ethics, hallucination, fairness, sycophancy, privacy, and robustness against adversarial demonstrations, revealing that models with superior performance in general NLP tasks do not always have greater trustworthiness."
            },
            "score": 4
        },
        {
            "id": "8951bbc8c1eb3fd43c9e47025268cc79b868f514",
            "paperId": "8951bbc8c1eb3fd43c9e47025268cc79b868f514",
            "title": "Why do universal adversarial attacks work on large language models?: Geometry might be the answer",
            "abstract": "Transformer based large language models with emergent capabilities are becoming increasingly ubiquitous in society. However, the task of understanding and interpreting their internal workings, in the context of adversarial attacks, remains largely unsolved. Gradient-based universal adversarial attacks have been shown to be highly effective on large language models and potentially dangerous due to their input-agnostic nature. This work presents a novel geometric perspective explaining universal adversarial attacks on large language models. By attacking the 117M parameter GPT-2 model, we find evidence indicating that universal adversarial triggers could be embedding vectors which merely approximate the semantic information in their adversarial training region. This hypothesis is supported by white-box model analysis comprising dimensionality reduction and similarity measurement of hidden representations. We believe this new geometric perspective on the underlying mechanism driving universal attacks could help us gain deeper insight into the internal workings and failure modes of LLMs, thus enabling their mitigation.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "By attacking the 117M parameter GPT-2 model, this work finds evidence indicating that universal adversarial triggers could be embedding vectors which merely approximate the semantic information in their adversarial training region."
            },
            "score": 3
        },
        {
            "id": "7d7567340337b89c995c3525d89a6c713d1481e6",
            "paperId": "7d7567340337b89c995c3525d89a6c713d1481e6",
            "title": "L-AutoDA: Leveraging Large Language Models for Automated Decision-based Adversarial Attacks",
            "abstract": "In the rapidly evolving field of machine learning, adversarial attacks present a significant challenge to model robustness and security. Decision-based attacks, which only require feedback on the decision of a model rather than detailed probabilities or scores, are particularly insidious and difficult to defend against. This work introduces L-AutoDA (Large Language Model-based Automated Decision-based Adversarial Attacks), a novel approach leveraging the generative capabilities of Large Language Models (LLMs) to automate the design of these attacks. By iteratively interacting with LLMs in an evolutionary framework, L-AutoDA automatically designs competitive attack algorithms efficiently without much human effort. We demonstrate the efficacy of L-AutoDA on CIFAR-10 dataset, showing significant improvements over baseline methods in both success rate and computational efficiency. Our findings underscore the potential of language models as tools for adversarial attack generation and highlight new avenues for the development of robust AI systems.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces L-AutoDA (Large Language Model-based Automated Decision-based Adversarial Attacks), a novel approach leveraging the generative capabilities of Large Language Models to automate the design of these attacks."
            },
            "score": 3
        },
        {
            "id": "8dd9605fbc9702f08a295cba5ae263f625781856",
            "paperId": "8dd9605fbc9702f08a295cba5ae263f625781856",
            "title": "VLAttack: Multimodal Adversarial Attacks on Vision-Language Tasks via Pre-trained Models",
            "abstract": "Vision-Language (VL) pre-trained models have shown their superiority on many multimodal tasks. However, the adversarial robustness of such models has not been fully explored. Existing approaches mainly focus on exploring the adversarial robustness under the white-box setting, which is unrealistic. In this paper, we aim to investigate a new yet practical task to craft image and text perturbations using pre-trained VL models to attack black-box fine-tuned models on different downstream tasks. Towards this end, we propose VLAttack to generate adversarial samples by fusing perturbations of images and texts from both single-modal and multimodal levels. At the single-modal level, we propose a new block-wise similarity attack (BSA) strategy to learn image perturbations for disrupting universal representations. Besides, we adopt an existing text attack strategy to generate text perturbations independent of the image-modal attack. At the multimodal level, we design a novel iterative cross-search attack (ICSA) method to update adversarial image-text pairs periodically, starting with the outputs from the single-modal level. We conduct extensive experiments to attack three widely-used VL pretrained models for six tasks on eight datasets. Experimental results show that the proposed VLAttack framework achieves the highest attack success rates on all tasks compared with state-of-the-art baselines, which reveals a significant blind spot in the deployment of pre-trained VL models. Codes will be released soon.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes VLAttack to generate adversarial samples by fusing perturbations of images and texts from both single-modal and multimodal levels, and proposes a novel iterative cross-search attack method to update adversarial image-text pairs periodically."
            },
            "score": 3
        },
        {
            "id": "142e934dd5d6c53f877c30243d436255e3a0dde7",
            "paperId": "142e934dd5d6c53f877c30243d436255e3a0dde7",
            "title": "Visual Adversarial Examples Jailbreak Aligned Large Language Models",
            "abstract": "Warning: this paper contains data, prompts, and model outputs that are offensive in nature.\n\nRecently, there has been a surge of interest in integrating vision into Large Language Models (LLMs), exemplified by Visual Language Models (VLMs) such as Flamingo and GPT-4. This paper sheds light on the security and safety implications of this trend. First, we underscore that the continuous and high-dimensional nature of the visual input makes it a weak link against adversarial attacks, representing an expanded attack surface of vision-integrated LLMs. Second, we highlight that the versatility of LLMs also presents visual attackers with a wider array of achievable adversarial objectives, extending the implications of security failures beyond mere misclassification. As an illustration, we present a case study in which we exploit visual adversarial examples to circumvent the safety guardrail of aligned LLMs with integrated vision. Intriguingly, we discover that a single visual adversarial example can universally jailbreak an aligned LLM, compelling it to heed a wide range of harmful instructions (that it otherwise would not) and generate harmful content that transcends the narrow scope of a `few-shot' derogatory corpus initially employed to optimize the adversarial example. Our study underscores the escalating adversarial risks associated with the pursuit of multimodality. Our findings also connect the long-studied adversarial vulnerabilities of neural networks to the nascent field of AI alignment. The presented attack suggests a fundamental adversarial challenge for AI alignment, especially in light of the emerging trend toward multimodality in frontier foundation models.",
            "year": 2023,
            "citationCount": 44,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that a single visual adversarial example can universally jailbreak an aligned LLM, compelling it to heed a wide range of harmful instructions and generate harmful content that transcends the narrow scope of a `few-shot' derogatory corpus initially employed to optimize the adversarial example."
            },
            "score": 3
        },
        {
            "id": "9d4cd5e3ab44f0d1dfe201c6be70aa7a692ac7f1",
            "paperId": "9d4cd5e3ab44f0d1dfe201c6be70aa7a692ac7f1",
            "title": "GuardT2I: Defending Text-to-Image Models from Adversarial Prompts",
            "abstract": "Recent advancements in Text-to-Image (T2I) models have raised significant safety concerns about their potential misuse for generating inappropriate or Not-Safe-For-Work (NSFW) contents, despite existing countermeasures such as NSFW classifiers or model fine-tuning for inappropriate concept removal. Addressing this challenge, our study unveils GuardT2I, a novel moderation framework that adopts a generative approach to enhance T2I models' robustness against adversarial prompts. Instead of making a binary classification, GuardT2I utilizes a Large Language Model (LLM) to conditionally transform text guidance embeddings within the T2I models into natural language for effective adversarial prompt detection, without compromising the models' inherent performance. Our extensive experiments reveal that GuardT2I outperforms leading commercial solutions like OpenAI-Moderation and Microsoft Azure Moderator by a significant margin across diverse adversarial scenarios.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study unveils GuardT2I, a novel moderation framework that adopts a generative approach to enhance T2I models' robustness against adversarial prompts, and outperforms leading commercial solutions like OpenAI-Moderation and Microsoft Azure Moderator by a significant margin across diverse adversarial scenarios."
            },
            "score": 3
        },
        {
            "id": "39bccdba38ef4a785adb7164bdc8fb746594c041",
            "paperId": "39bccdba38ef4a785adb7164bdc8fb746594c041",
            "title": "Looking Under the Hood of DetectGPT",
            "abstract": "Large Language Models (LLMs) have revolutionized natural language processing, but their ability to generate highly convincing machine-generated text raises concerns about their misuse. DetectGPT (Mitchell et al.) is a zero-shot LLM detection algorithm that perturbs the wording in a text sample and uses the changes in likelihood under an LLM as a discriminative signal. In this work, we analyze DetectGPT in three areas: improving DetectGPT performance by selectively perturbing certain types of words, discovering adversarial attacks that can systematically fool DetectGPT, and evaluating DetectGPT on newer LLMs such as ChatGPT. Our experiments demonstrate that selectively masking a combination of nouns, verbs, and adjectives improves the AUROC metric by up to 9.5%, demonstrating the importance of targeted masking strategies. Additionally, we reveal a limitation of DetectGPT on adversarial contexts , where a snippet of text prepended to the prompt can degrade performance by up to 14%. Finally, we demonstrate that ChatGPT is challenging to detect through DetectGPT. In some cases, we find that prompting ChatGPT to impersonate other entities can further degrade performance. In total, our work provides an analysis of a state-of-the-art LLM detection algorithm and shows potential improvements and vulnerabilities.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work provides an analysis of a state-of-the-art LLM detection algorithm and shows potential improvements and vulnerabilities and demonstrates the importance of targeted masking strategies in DetectGPT."
            },
            "score": 3
        },
        {
            "id": "57eec0efd84f95943c744bacb1315746b1287f16",
            "paperId": "57eec0efd84f95943c744bacb1315746b1287f16",
            "title": "Few-Shot Adversarial Prompt Learning on Vision-Language Models",
            "abstract": "The vulnerability of deep neural networks to imperceptible adversarial perturbations has attracted widespread attention. Inspired by the success of vision-language foundation models, previous efforts achieved zero-shot adversarial robustness by aligning adversarial visual features with text supervision. However, in practice, they are still unsatisfactory due to several issues, including heavy adaptation cost, suboptimal text supervision, and uncontrolled natural generalization capacity. In this paper, to address these issues, we propose a few-shot adversarial prompt framework where adapting input sequences with limited data makes significant adversarial robustness improvement. Specifically, we achieve this by providing adversarially correlated text supervision that is end-to-end learned from adversarial examples. We also propose a novel training objective that enhances the consistency of multi-modal features while encourages differentiated uni-modal features between natural and adversarial examples. The proposed framework gives access to learn adversarial text supervision, which provides superior cross-modal adversarial alignment and matches state-of-the-art zero-shot adversarial robustness with only 1% training data.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A few-shot adversarial prompt framework is proposed where adapting input sequences with limited data makes significant adversarial robustness improvement and matches state-of-the-art zero-shot adversarial robustness with only 1% training data."
            },
            "score": 3
        },
        {
            "id": "3a391dfd536625e068f3888c817cc6cbe7fcea9c",
            "paperId": "3a391dfd536625e068f3888c817cc6cbe7fcea9c",
            "title": "One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models",
            "abstract": "Large pre-trained Vision-Language Models (VLMs) like CLIP, despite having remarkable generalization ability, are highly vulnerable to adversarial examples. This work studies the adversarial robustness of VLMs from the novel perspective of the text prompt instead of the extensively studied model weights (frozen in this work). We first show that the effectiveness of both adversarial attack and defense are sensitive to the used text prompt. Inspired by this, we propose a method to improve resilience to adversarial attacks by learning a robust text prompt for VLMs. The proposed method, named Adversarial Prompt Tuning (APT), is effective while being both computationally and data efficient. Extensive experiments are conducted across 15 datasets and 4 data sparsity schemes (from 1-shot to full training data settings) to show APT's superiority over hand-engineered prompts and other state-of-the-art adaption methods. APT demonstrated excellent abilities in terms of the in-distribution performance and the generalization under input distribution shift and across datasets. Surprisingly, by simply adding one learned word to the prompts, APT can significantly boost the accuracy and robustness (epsilon=4/255) over the hand-engineered prompts by +13% and +8.5% on average respectively. The improvement further increases, in our most effective setting, to +26.4% for accuracy and +16.7% for robustness. Code is available at https://github.com/TreeLLi/APT.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work studies the adversarial robustness of VLMs from the novel perspective of the text prompt instead of the extensively studied model weights, and proposes a method to improve resilience to adversarial attacks by learning a robust text prompt for VLMs."
            },
            "score": 3
        },
        {
            "id": "6091b3c2e25175a328d0c1d387c2e5253b751cc8",
            "paperId": "6091b3c2e25175a328d0c1d387c2e5253b751cc8",
            "title": "Analysis of BERT Email Spam Classifier Against Adversarial Attacks",
            "abstract": "Email is still a preferred method of one-to-one communication among customers, businesses, and employees. As the usage of emails is increasing daily, the number of spam emails is also increasing. Many natural language models are developed to avoid the problem of spam emails. However, customers continue to report an increase in scams and attacks carried out through spam emails. Even though numerous spam filters have been suggested for identifying spam, they are weak and may be fooled by some expertly created adversarial samples. In this paper, we have taken the BERT classifier which has the best performance among other classifiers, and analyzed its performance when we inserted the adversarial samples. We have taken eight types of different adversarial attacks and analyzed the performance of the BERT Model. This analysis will be helpful in further studying defense methods against these attacks.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper has taken the BERT classifier which has the best performance among other classifiers, and analyzed its performance when it is inserted into adversarial samples, and taken eight types of different adversarial attacks and analyzed the performance of the Bert Model."
            },
            "score": 3
        },
        {
            "id": "91244c157c5e536393a99e8ac07832ba1a19aa84",
            "paperId": "91244c157c5e536393a99e8ac07832ba1a19aa84",
            "title": "Automatic Generation of Adversarial Readable Chinese Texts",
            "abstract": "Natural language processing (NLP) models are known vulnerable to adversarial examples, similar to image processing models. Studying adversarial texts is an essential step to improve the robustness of NLP models. However, existing studies mainly focus on generating adversarial texts for English, with no prior knowledge that whether those attacks could be applied to Chinese. After analyzing the differences between Chinese and English, we propose a novel adversarial Chinese text generation solution Argot, by utilizing the method for adversarial English examples and several novel methods developed on Chinese characteristics. Argot could effectively and efficiently generate adversarial Chinese texts with good readability in both white-box and black-box settings. Argot could also automatically generate targeted Chinese adversarial texts, achieving a high success rate and ensuring the readability of the generated texts. Furthermore, we apply Argot to the spam detection task in both local detection models and a public toxic content detection system from a well-known security company. Argot achieves a relatively high bypass success rate with fluent readability, which proves that the real-world toxic content detection system is vulnerable to adversarial example attacks. We also evaluate some available defense strategies, and the results indicate that Argot can still achieve high attack success rates.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel adversarial Chinese text generation solution Argot is proposed, by utilizing the method for adversarial English examples and several novel methods developed on Chinese characteristics, which achieves a relatively high bypass success rate with fluent readability and proves that the real-world toxic content detection system is vulnerable to adversarial example attacks."
            },
            "score": 3
        },
        {
            "id": "64e30319758596f98d3b53c2a45ea1f493799d24",
            "paperId": "64e30319758596f98d3b53c2a45ea1f493799d24",
            "title": "Demo: Certified Robustness on Toolformer",
            "abstract": "Tool-augmented language models (TALMs) overcome the limitations of current language models (LMs), allowing them to leverage external tools to enhance performance. One state-of-the-art example is Toolformer introduced by Meta AI Research, which achieves a broader integration of tool utilization. However, Toolformer faces particular concerns related to the robustness of its predictions in the optimal positioning for API calls. Adversarial perturbations can alter the position of API calls chosen by Toolformer, thus resulting in responses that are not only incorrect but potentially even less accurate than those generated by standard language models. To improve the robustness of Toolformer and fulfill the capability of its toolbox, our focus lies on addressing the potential vulnerabilities that arise from small perturbations in the input or prompt space. To achieve this goal, we plan to study adversarial attacks from both attackers' and defenders' perspectives by first studying the adversarial attack algorithms on the input and prompt space, then proposing the certified robustness to the Toolformer API calls scheduling, which is not only empirically effective but also theory-backed.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work plans to study adversarial attacks from both attackers' and defenders' perspectives by first studying the adversarial attack algorithms on the input and prompt space, then proposing the certified robustness to the Toolformer API calls scheduling, which is not only empirically effective but also theory-backed."
            },
            "score": 3
        },
        {
            "id": "5a2e45ce35fb26ab70a61b424a49f8e5b4532a8e",
            "paperId": "5a2e45ce35fb26ab70a61b424a49f8e5b4532a8e",
            "title": "WARP: Word-level Adversarial ReProgramming",
            "abstract": "Transfer learning from pretrained language models recently became the dominant approach for solving many NLP tasks. A common approach to transfer learning for multiple tasks that maximize parameter sharing trains one or more task-specific layers on top of the language model. In this paper, we present an alternative approach based on adversarial reprogramming, which extends earlier work on automatic prompt generation. Adversarial reprogramming attempts to learn task-specific word embeddings that, when concatenated to the input text, instruct the language model to solve the specified task. Using up to 25K trainable parameters per task, this approach outperforms all existing methods with up to 25M trainable parameters on the public leaderboard of the GLUE benchmark. Our method, initialized with task-specific human-readable prompts, also works in a few-shot setting, outperforming GPT-3 on two SuperGLUE tasks with just 32 training samples.",
            "year": 2021,
            "citationCount": 257,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents an alternative approach based on adversarial reprogramming, which extends earlier work on automatic prompt generation, and outperforms all existing methods with up to 25M trainable parameters on the public leaderboard of the GLUE benchmark."
            },
            "score": 3
        },
        {
            "id": "c8a5e5412744ac96fba872497d76b104223eaad4",
            "paperId": "c8a5e5412744ac96fba872497d76b104223eaad4",
            "title": "Assessing the Influence of Different Types of Probing on Adversarial Decision-Making in a Deception Game",
            "abstract": "Deception, which includes leading cyber-attackers astray with false information, has shown to be an effective method of thwarting cyber-attacks. There has been little investigation of the effect of probing action costs on adversarial decision-making, despite earlier studies on deception in cybersecurity focusing primarily on variables like network size and the percentage of honeypots utilized in games. Understanding human decision-making when prompted with choices of various costs is essential in many areas such as in cyber security. In this paper, we will use a deception game (DG) to examine different costs of probing on adversarial decisions. To achieve this we utilized an IBLT model and a delayed feedback mechanism to mimic knowledge of human actions. Our results were taken from an even split of deception and no deception to compare each influence. It was concluded that probing was slightly taken less as the cost of probing increased. The proportion of attacks stayed relatively the same as the cost of probing increased. Although a constant cost led to a slight decrease in attacks. Overall, our results concluded that the different probing costs do not have an impact on the proportion of attacks whereas it had a slightly noticeable impact on the proportion of probing.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It was concluded that probing was slightly taken less and the proportion of attacks stayed relatively the same as the cost of probing increased, whereas a constant cost led to a slight decrease in attacks."
            },
            "score": 3
        },
        {
            "id": "3a0cee573e19db7f3c561152199a3386adf6cb74",
            "paperId": "3a0cee573e19db7f3c561152199a3386adf6cb74",
            "title": "An Optimized Transfer Attack Framework Towards Multi-Modal Machine Learning",
            "abstract": "Deep neural networks (DNNs) have excelled at a wide range of tasks, including computer vision (CV), natural language processing (NLP), and speech recognition. However, past research has demonstrated that DNNs are vulnerable to adversarial examples, which are deliberately meant to trick models into making incorrect predictions by adding subtle perturbations into inputs. Adversarial examples create an exponential threat to multi-modal models that can accept a variety of inputs. By attacking substitute models, we provide a transferable attack framework. The suggested framework optimizes the attack process by modifying the prompt templates and simultaneously raising the attack on multiple inputs. Our experiments demonstrate that the proposed attack framework can significantly improve the success rate of transferable attacks, and adversarial examples are rarely noticed by humans. Meanwhile, experiments show that in transferable attacks, coarse-grained adversarial examples can achieve higher attack success rates than fine-grained ones, and the multi-modal models has some robustness against uni-modal attacks.",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work attacks substitute models to provide a transferable attack framework for DNNs vulnerable to adversarial examples, and demonstrates that the proposed attack framework can significantly improve the success rate of transferable attacks."
            },
            "score": 3
        },
        {
            "id": "051493fa4f87c54e49aeed2e289a05265d739c0d",
            "paperId": "051493fa4f87c54e49aeed2e289a05265d739c0d",
            "title": "ChatGPT: A Threat to Spam Filtering Systems",
            "abstract": "ChatGPT gathered the attention of millions of users shortly after its release, leading to the popularization of generative AI technologies. This research aims to emphasize one of the potential vulnerabilities associated with commonly used generative AI models. Even though it follows strict ethical and security policies, proper prompt engineering enables malicious misuse of ChatG PT such as spam email generation. In this paper, we present various scenarios of malicious prompt engineering to encourage the chatbot for spam email generation and rewriting the existing email. Also, we present the adversarial prompt engineering examples intended to evade the detection by spam filters by means of rewriting the given email while circumventing common spam characteristics. We experimentally evaluate the practical feasibility of prompt engineering on ChatGPT by assessing the performance of six common ML-based spam filters with emails modified by ChatG PT. From the experimental results, we show that adversarial prompt engineering decreases the performance of common ML-based spam filters, while NLP-based filter is robust to such modification. We also demonstrate that including ChatG PT rewritten emails in the training set leads to more robust ML-based spam filters, while the use of available AI-text detectors does not guarantee high detection rates of emails modified by the chatbot.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that including ChatG PT rewritten emails in the training set leads to more robust ML-based spam filters, while the use of available AI-text detectors does not guarantee high detection rates of emails modified by the chatbot."
            },
            "score": 3
        },
        {
            "id": "d18287d5ef8653aa1276a11957f2b3934c7c93e1",
            "paperId": "d18287d5ef8653aa1276a11957f2b3934c7c93e1",
            "title": "CodeAttack: Code-based Adversarial Attacks for Pre-Trained Programming Language Models",
            "abstract": "Pre-trained programming language (PL) models (such as CodeT5, CodeBERT, GraphCodeBERT, etc.,) have the potential to automate software engineering tasks involving code understanding and code generation. However, these models operate in the natural channel of code, i.e., primarily concerned with the human understanding of code. They are not robust to changes in the input and thus, are potentially susceptible to adversarial attacks in the natural channel. We propose, Code Attack, a simple yet effective black-box attack model that uses code structure to generate effective, efficient, and imperceptible adversarial code samples and demonstrates the vulnerabilities of the state-of-the-art PL models to code-specific adversarial attacks. We evaluate the transferability of CodeAttack on several code-code (translation and repair) and code-NL (summarization) tasks across different programming languages. Code Attack outperforms state-of-the-art adversarial NLP attack models to achieve the best overall drop in performance while being more efficient, imperceptible, consistent, and fluent. The code can be found at https://github.com/reddy-lab-code-research/CodeAttack.",
            "year": 2022,
            "citationCount": 26,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes, Code Attack, a simple yet effective black-box attack model that uses code structure to generate effective, efficient, and imperceptible adversarial code samples and demonstrates the vulnerabilities of the state-of-the-art PL models to code-specific adversarial attacks."
            },
            "score": 2
        },
        {
            "id": "5be2f65c3907859aeb2a141dedee6aa072db0d5f",
            "paperId": "5be2f65c3907859aeb2a141dedee6aa072db0d5f",
            "title": "Adversarial Meta Prompt Tuning for Open Compound Domain Adaptive Intent Detection",
            "abstract": "Intent detection plays an essential role in dialogue systems. This paper takes the lead to study open compound domain adaptation (OCDA) for intent detection, which brings the advantage of improved generalization to unseen domains. OCDA for intent detection is indeed a more realistic domain adaptation setting, which learns an intent classifier from labeled source domains and adapts it to unlabeled compound target domains containing different intent classes with the source domains. At inference time, we test the intent classifier in open domains that contain previously unseen intent classes. To this end, we propose an Adversarial Meta Prompt Tuning method (called AMPT) for open compound domain adaptive intent detection. Concretely, we propose a meta prompt tuning method, which utilizes language prompts to elicit rich knowledge from large-scale pre-trained language models (PLMs) and automatically finds better prompt initialization that facilitates fast adaptation via meta learning. Furthermore, we leverage a domain adversarial training technique to acquire domain-invariant representations of diverse domains. By taking advantage of the collaborative effect of meta learning, prompt tuning, and adversarial training, we can learn an intent classifier that can effectively generalize to unseen open domains. Experimental results on two benchmark datasets (i.e., HWU64 and CLINC) show that our model can learn substantially better-generalized representations for unseen domains compared with strong competitors.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a meta prompt tuning method, which utilizes language prompts to elicit rich knowledge from large-scale pre-trained language models and automatically finds better prompt initialization that facilitates fast adaptation via meta learning, and leverage a domain adversarial training technique to acquire domain-invariant representations of diverse domains."
            },
            "score": 2
        },
        {
            "id": "b4d9cba448229a11fb07a9e7d1451015a4ec6612",
            "paperId": "b4d9cba448229a11fb07a9e7d1451015a4ec6612",
            "title": "Critical Perspectives: A Benchmark Revealing Pitfalls in PerspectiveAPI",
            "abstract": "Detecting \u201ctoxic\u201d language in internet content is a pressing social and technical challenge. In this work, we focus on Perspective API from Jigsaw, a state-of-the-art tool that promises to score the \u201ctoxicity\u201d of text, with a recent model update that claims impressive results (Lees et al., 2022). We seek to challenge certain normative claims about toxic language by proposing a new benchmark, Selected Adversarial SemanticS, or SASS. We evaluate Perspective on SASS, and compare to low-effort alternatives, like zero-shot and few-shot GPT-3 prompt models, in binary classification settings. We find that Perspective exhibits troubling shortcomings across a number of our toxicity categories. SASS provides a new tool for evaluating performance on previously undetected toxic language that avoids common normative pitfalls. Our work leads us to emphasize the importance of questioning assumptions made by tools already in deployment for toxicity detection in order to anticipate and prevent disparate harms.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work evaluates Perspective API from Jigsaw, a state-of-the-art tool that promises to score the \u201ctoxicity\u201d of text, and proposes a new benchmark, Selected Adversarial SemanticS, or SASS, which provides a new tool for evaluating performance on previously undetected toxic language that avoids common normative pitfalls."
            },
            "score": 2
        },
        {
            "id": "b6cf4579b59b51d7df416e096ad86c1e6a48b458",
            "paperId": "b6cf4579b59b51d7df416e096ad86c1e6a48b458",
            "title": "Adversarial Prompt Tuning for Vision-Language Models",
            "abstract": "With the rapid advancement of multimodal learning, pre-trained Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable capacities in bridging the gap between visual and language modalities. However, these models remain vulnerable to adversarial attacks, particularly in the image modality, presenting considerable security risks. This paper introduces Adversarial Prompt Tuning (AdvPT), a novel technique to enhance the adversarial robustness of image encoders in VLMs. AdvPT innovatively leverages learnable text prompts and aligns them with adversarial image embeddings, to address the vulnerabilities inherent in VLMs without the need for extensive parameter training or modification of the model architecture. We demonstrate that AdvPT improves resistance against white-box and black-box adversarial attacks and exhibits a synergistic effect when combined with existing image-processing-based defense techniques, further boosting defensive capabilities. Comprehensive experimental analyses provide insights into adversarial prompt tuning, a novel paradigm devoted to improving resistance to adversarial images through textual input modifications, paving the way for future robust multimodal learning research. These findings open up new possibilities for enhancing the security of VLMs. Our code is available at https://github.com/jiamingzhang94/Adversarial-Prompt-Tuning.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Adversarial Prompt Tuning is introduced, a novel technique to enhance the adversarial robustness of image encoders in VLMs and improves resistance against white-box and black-box adversarial attacks and exhibits a synergistic effect when combined with existing image-processing-based defense techniques, further boosting defensive capabilities."
            },
            "score": 2
        },
        {
            "id": "8eb9a8d756e93530eb35e9f0e26a2a0190c1dd7c",
            "paperId": "8eb9a8d756e93530eb35e9f0e26a2a0190c1dd7c",
            "title": "The Biases of Pre-Trained Language Models: An Empirical Study on Prompt-Based Sentiment Analysis and Emotion Detection",
            "abstract": "Thanks to the breakthrough of large-scale pre-trained language model (PLM) technology, prompt-based classification tasks, e.g., sentiment analysis and emotion detection, have raised increasing attention. Such tasks are formalized as masked language prediction tasks which are in line with the pre-training objects of most language models. Thus, one can use a PLM to infer the masked words in a downstream task, then obtaining label predictions with manually defined label-word mapping templates. Prompt-based affective computing takes the advantages of both neural network modeling and explainable symbolic representations. However, there still remain many unclear issues related to the mechanisms of PLMs and prompt-based classification. We conduct a systematic empirical study on prompt-based sentiment analysis and emotion detection to study the biases of PLMs towards affective computing. We find that PLMs are biased in sentiment analysis and emotion detection tasks with respect to the number of label classes, emotional label-word selections, prompt templates and positions, and the word forms of emotion lexicons.",
            "year": 2023,
            "citationCount": 98,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that PLMs are biased in sentiment analysis and emotion detection tasks with respect to the number of label classes, emotional label-word selections, prompt templates and positions, and the word forms of emotion lexicons."
            },
            "score": 2
        },
        {
            "id": "efabdd27929796b712cb1b3a3051ea5358dc1200",
            "paperId": "efabdd27929796b712cb1b3a3051ea5358dc1200",
            "title": "A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning",
            "abstract": "Vision-language models can encode societal biases and stereotypes, but there are challenges to measuring and mitigating these multimodal harms due to lacking measurement robustness and feature degradation. To address these challenges, we investigate bias measures and apply ranking metrics for image-text representations. We then investigate debiasing methods and show that prepending learned embeddings to text queries that are jointly trained with adversarial debiasing and a contrastive loss, reduces various bias measures with minimal degradation to the image-text representation.",
            "year": 2022,
            "citationCount": 53,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Debiasing methods are investigated and it is shown that prepending learned embeddings to text queries that are jointly trained with adversarial debiasing and a contrastive loss, reduces various bias measures with minimal degradation to the image-text representation."
            },
            "score": 2
        },
        {
            "id": "5977782a9ea767a35425833999eab7958c1c85b2",
            "paperId": "5977782a9ea767a35425833999eab7958c1c85b2",
            "title": "Partially Recentralization Softmax Loss for Vision-Language Models Robustness",
            "abstract": "As Large Language Models make a breakthrough in natural language processing tasks (NLP), multimodal technique becomes extremely popular. However, it has been shown that multimodal NLP are vulnerable to adversarial attacks, where the outputs of a model can be dramatically changed by a perturbation to the input. While several defense techniques have been proposed both in computer vision and NLP models, the multimodal robustness of models have not been fully explored. In this paper, we study the adversarial robustness provided by modifying loss function of pre-trained multimodal models, by restricting top K softmax outputs. Based on the evaluation and scoring, our experiments show that after a fine-tuning, adversarial robustness of pre-trained models can be significantly improved, against popular attacks. Further research should be studying, such as output diversity, generalization and the robustness-performance trade-off of this kind of loss functions. Our code will be available after this paper is accepted",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper studies the adversarial robustness provided by modifying loss function of pre-trained multimodal models, by restricting top K softmax outputs, and shows that after a fine-tuning, adversarial robustness of pre-trained models can be significantly improved, against popular attacks."
            },
            "score": 2
        },
        {
            "id": "f14ea74a40c40cad10c32fccf5a410144735d683",
            "paperId": "f14ea74a40c40cad10c32fccf5a410144735d683",
            "title": "Efficient and Robust Knowledge Graph Construction",
            "abstract": "Knowledge graph construction which aims to extract knowledge from the text corpus, has appealed to the NLP community researchers. Previous decades have witnessed the remarkable progress of knowledge graph construction on the basis of neural models; however, those models often cost massive computation or labeled data resources and suffer from unstable inference accounting for biased or adversarial samples. Recently, numerous approaches have been explored to mitigate the efficiency and robustness issues for knowledge graph construction, such as prompt learning and adversarial training. In this tutorial, we aim to bring interested NLP researchers up to speed on the recent and ongoing techniques for efficient and robust knowledge graph construction. Additionally, our goal is to provide a systematic and up-to-date overview of these methods and reveal new research opportunities to the audience.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This tutorial aims to bring interested NLP researchers up to speed on the recent and ongoing techniques for efficient and robust knowledge graph construction, and provides a systematic and up-to-date overview of these methods."
            },
            "score": 2
        },
        {
            "id": "e9c1e111860920898e1f982ee2ee60606ff35e11",
            "paperId": "e9c1e111860920898e1f982ee2ee60606ff35e11",
            "title": "Dilated convolution for enhanced extractive summarization: A GAN-based approach with BERT word embedding",
            "abstract": "Text summarization (TS) plays a crucial role in natural language processing (NLP) by automatically condensing and capturing key information from text documents. Its significance extends to diverse fields, including engineering, healthcare, and others, where it offers substantial time and resource savings. However, manual summarization is a laborious task, prompting the need for automated text summarization systems. In this paper, we propose a novel strategy for extractive summarization that leverages a generative adversarial network (GAN)-based method and Bidirectional Encoder Representations from Transformers (BERT) word embedding. BERT, a transformer-based architecture, processes sentence bidirectionally, considering both preceding and following words. This contextual understanding empowers BERT to generate word representations that carry a deeper meaning and accurately reflect their usage within specific contexts. Our method adopts a generator and discriminator within the GAN framework. The generator assesses the likelihood of each sentence in the summary while the discriminator evaluates the generated summary. To extract meaningful features in parallel, we introduce three dilated convolution layers in the generator and discriminator. Dilated convolution allows for capturing a larger context and incorporating long-range dependencies. By introducing gaps between filter weights, dilated convolution expands the receptive field, enabling the model to consider a broader context of words. To encourage the generator to explore diverse sentence combinations that lead to high-quality summaries, we introduce various noises to each document within our proposed GAN. This approach allows the generator to learn from a range of sentence permutations and select the most suitable ones. We evaluate the performance of our proposed model using the CNN/Daily Mail dataset. The results, measured using the ROUGE metric, demonstrate the superiority of our approach compared to other tested methods. This confirms the effectiveness of our GAN-based strategy, which integrates dilated convolution layers, BERT word embedding, and a generator-discriminator framework in achieving enhanced extractive summarization performance.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel strategy for extractive summarization that leverages a generative adversarial network (GAN)-based method and Bidirectional Encoder Representations from Transformers (BERT) word embedding is proposed and the results demonstrate the superiority of the approach compared to other tested methods."
            },
            "score": 2
        },
        {
            "id": "22c638d7b7b14de8f16425f85605961804f1c42c",
            "paperId": "22c638d7b7b14de8f16425f85605961804f1c42c",
            "title": "Adversarial Attacks on Protein Language Models",
            "abstract": "Deep Learning models for protein structure prediction, such as AlphaFold2, leverage Transformer architectures and their attention mechanism to capture structural and functional properties of amino acid sequences. Despite the high accuracy of predictions, biologically insignificant perturbations of the input sequences, or even single point mutations, can lead to substantially different 3d structures. On the other hand, protein language models are often insensitive to biologically relevant mutations that induce misfolding or dysfunction (e.g. missense mutations). Precisely, predictions of the 3d coordinates do not reveal the structure-disruptive effect of these mutations. Therefore, there is an evident inconsistency between the biological importance of mutations and the resulting change in structural prediction. Inspired by this problem, we introduce the concept of adversarial perturbation of protein sequences in continuous embedding spaces of protein language models. Our method relies on attention scores to detect the most vulnerable amino acid positions in the input sequences. Adversarial mutations are biologically diverse from their references and are able to significantly alter the resulting 3d structures.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces the concept of adversarial perturbation of protein sequences in continuous embedding spaces of protein language models and relies on attention scores to detect the most vulnerable amino acid positions in the input sequences."
            },
            "score": 1
        },
        {
            "id": "a2f0933024fb6903e879d6c56e14769947fe0336",
            "paperId": "a2f0933024fb6903e879d6c56e14769947fe0336",
            "title": "CLIPping the Deception: Adapting Vision-Language Models for Universal Deepfake Detection",
            "abstract": "The recent advancements in Generative Adversarial Networks (GANs) and the emergence of Diffusion models have significantly streamlined the production of highly realistic and widely accessible synthetic content. As a result, there is a pressing need for effective general purpose detection mechanisms to mitigate the potential risks posed by deepfakes. In this paper, we explore the effectiveness of pre-trained vision-language models (VLMs) when paired with recent adaptation methods for universal deepfake detection. Following previous studies in this domain, we employ only a single dataset (ProGAN) in order to adapt CLIP for deepfake detection. However, in contrast to prior research, which rely solely on the visual part of CLIP while ignoring its textual component, our analysis reveals that retaining the text part is crucial. Consequently, the simple and lightweight Prompt Tuning based adaptation strategy that we employ outperforms the previous SOTA approach by 5.01% mAP and 6.61% accuracy while utilizing less than one third of the training data (200k images as compared to 720k). To assess the real-world applicability of our proposed models, we conduct a comprehensive evaluation across various scenarios. This involves rigorous testing on images sourced from 21 distinct datasets, including those generated by GANs-based, Diffusion-based and Commercial tools.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper explores the effectiveness of pre-trained vision-language models (VLMs) when paired with recent adaptation methods for universal deepfake detection and reveals that retaining the text part of CLIP is crucial."
            },
            "score": 1
        },
        {
            "id": "d034b4a366fb9dbebd962602e8e6d02112ad0fc6",
            "paperId": "d034b4a366fb9dbebd962602e8e6d02112ad0fc6",
            "title": "Migration of adversarial example in image classification",
            "abstract": "Neural network technology has made remarkable achievements in computer vision, speech recognition, natural language processing, and other fields. However, the problem of the interpretability of the neural network model makes its application in real situations have potential security risks. In recent years, many studies have pointed out that using Adversarial example technology to make extremely weak perturbations of the input sample can mislead most mainstream neural network models, such as fully connected neural networks and convolutional neural networks, to make wrong judgments. This phenomenon reveals that the existing neural network technology lacks security and robustness. The study of adversarial example technology is of great significance to improve the safety and robustness of neural network models and to promote the researcher's understanding of the learning process of neural network models deeply. Studying adversarial examples of migration is an important research field in adversarial attacks. Researchers attempt to summarize the rules of adversarial attacks by exploring the migration of adversarial examples, thus establishing a robust model in the deep learning area. In this paper, the migration of adversarial examples in image classification is studied to provide analytical data for summarizing the characteristics of adversarial examples.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The migration of adversarial examples in image classification is studied to provide analytical data for summarizing the characteristics of adversarian examples, thus establishing a robust model in the deep learning area."
            },
            "score": 1
        },
        {
            "id": "3fe8be5778d666bb55be89448f0c7aa269f76ca5",
            "paperId": "3fe8be5778d666bb55be89448f0c7aa269f76ca5",
            "title": "An Interference-Resistant and Low-Consumption Lip Recognition Method",
            "abstract": "Lip movements contain essential linguistic information. It is an important medium for studying the content of the dialogue. At present, there are many studies on how to improve the accuracy of lip language recognition models. However, there are few studies on the robustness and generalization performance of the model under various disturbances. Specific experiments show that the current state-of-the-art lip recognition model significantly drops in accuracy when disturbed and is particularly sensitive to adversarial examples. This paper substantially alleviates this problem by using Mixup training. Taking the model subjected to negative attacks generated by FGSM as an example, the model in this paper achieves 85.0% and 40.2% accuracy on the English dataset LRW and the Mandarin dataset LRW-1000, respectively. The correct recognition rates are improved by 9.8% and 8.3%, compared with the current advanced lip recognition models. The positive impact of Mixup training on the robustness and generalization of lip recognition models is demonstrated. In addition, the performance of the lip recognition classification model depends more on the training parameters, which increase the computational cost. The InvNet-18 network in this paper reduces the consumption of GPU resources and the training time while improving the model accuracy. Compared with the standard ResNet-18 network used in mainstream lip recognition models, the InvNet-18 network in this paper has more than three times lower GPU consumption and 32% fewer parameters. After detailed analysis and comparison in various aspects, it is demonstrated that the model in this paper can effectively improve the model\u2019s anti-interference ability and reduce training resource consumption. At the same time, the accuracy is comparable with the current state-of-the-art results.",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The InvNet-18 network in this paper reduces the consumption of GPU resources and the training time while improving the model accuracy and can effectively improve the model\u2019s anti-interference ability and reduce training resource consumption."
            },
            "score": 1
        }
    ],
    "novelty": "yes"
}