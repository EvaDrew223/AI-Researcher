{
    "topic_description": "novel prompting methods to improve large language models' robustness against adversarial attacks and improve their security and privacy",
    "idea_name": "Rationality Verification Prompting",
    "raw_idea": {
        "Problem": "LLMs can sometimes generate irrational or logically inconsistent responses when prompted with complex reasoning tasks. Verifying the rationality of LLM responses is important for improving their robustness and reliability.",
        "Existing Methods": "Current methods for verifying LLM rationality include using external knowledge bases or rule-based systems to check for logical consistency, or using human evaluations to assess the reasonableness of responses. However, these methods can be limited by the coverage of the knowledge bases or the scalability of human evaluations.",
        "Motivation": "We can leverage LLMs' own reasoning capabilities to verify the rationality of their responses. By prompting LLMs to analyze and critique their own responses, we can identify potential logical flaws or inconsistencies in a more flexible and contextual way.",
        "Proposed Method": "We propose a rationality verification prompting technique that works as follows. First, we prompt the LLM to generate a response to the input prompt as usual. We then prompt the LLM to critically analyze its own response, using instructions like 'Identify any logical flaws or inconsistencies in the previous response', or 'Argue against the claims made in the previous response'. If the LLM identifies any issues with its original response, we prompt it to generate a revised response that addresses those issues. We repeat this process of response generation and self-verification until the LLM cannot find any more issues with its response.",
        "Experiment Plan": "Evaluate rationality verification prompting on various benchmarks for logical reasoning and consistency, such as LogiQA, CLUTRR, and StrategyQA. Compare with baseline methods that use external knowledge bases or human evaluations. Also measure the number of verification rounds needed to achieve stable and rational responses for different types of prompts."
    },
    "full_experiment_plan": {
        "Title": "Rationality Verification Prompting: Improving Language Model Reasoning through Self-Analysis and Revision",
        "Problem Statement": "Large Language Models (LLMs) can sometimes generate irrational or logically inconsistent responses when prompted with complex reasoning tasks. Verifying the rationality of LLM responses is important for improving their robustness and reliability.",
        "Motivation": "Current methods for verifying LLM rationality, such as using external knowledge bases, rule-based systems, or human evaluations, can be limited by the coverage of the knowledge bases or the scalability of human evaluations. We propose leveraging LLMs' own reasoning capabilities to verify the rationality of their responses. By prompting LLMs to analyze and critique their own responses, we can identify potential logical flaws or inconsistencies in a more flexible and contextual way.",
        "Proposed Method": "We propose a rationality verification prompting technique that works as follows. First, we prompt the LLM to generate a response to the input prompt as usual. We then prompt the LLM to critically analyze its own response, using instructions like 'Identify any logical flaws or inconsistencies in the previous response', or 'Argue against the claims made in the previous response'. If the LLM identifies any issues with its original response, we prompt it to generate a revised response that addresses those issues. We repeat this process of response generation and self-verification until the LLM cannot find any more issues with its response.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "Evaluate rationality verification prompting on various benchmarks for logical reasoning and consistency, such as LogiQA, CLUTRR, and StrategyQA.",
            "Step 2: Construct Prompts": "1. Baseline prompt: Simply ask the question or task directly without any additional instructions.\n2. Rationality verification prompt:\n  a) First, prompt the model to generate an initial response to the question or task.\n  b) Then, prompt the model to critically analyze its own response. Use instructions like:\n    - 'Identify any logical flaws, inconsistencies, or unsupported claims in the previous response.'\n    - 'Play devil's advocate and argue against the claims made in the previous response.'\n    - 'Explain why the reasoning in the previous response may be incorrect or incomplete.'\n  c) If issues are identified, prompt the model to generate a revised response that addresses them. Use instructions like:\n    - 'Rewrite the previous response to address the identified logical flaws and inconsistencies.'\n    - 'Generate a revised response that incorporates the counterarguments and additional considerations mentioned.'\n  d) Repeat steps b and c until no more issues can be identified in the response.",
            "Step 3: Select Models": "Evaluate the proposed method on state-of-the-art LLMs such as GPT-3.5 (text-davinci-002), GPT-4, PaLM, and Anthropic's Claude.",
            "Step 4: Collect Results": "1. For each dataset and model, generate responses using both the baseline and rationality verification prompts.\n2. Collect the initial response, the self-critiques, and the revised responses at each iteration of the rationality verification process.\n3. Record the number of verification rounds needed to achieve a stable response for each example.",
            "Step 5: Analyze Results": "1. Evaluate the quality of the initial and final revised responses using both automated metrics (e.g., BLEU, ROUGE, BERTScore) and human judgments of logical consistency, factual accuracy, and overall coherence.\n2. Compare the performance of rationality verification prompting to the baseline prompts across different models and datasets.\n3. Analyze the types of logical flaws and inconsistencies identified by the models during the self-verification process.\n4. Investigate the relationship between the number of verification rounds and the quality of the final response."
        },
        "Test Case Examples": {
            "Baseline Prompt Input": "Question: Anna is taller than Bob. Bob is taller than Charlie. Who is the tallest?\nAnswer:",
            "Baseline Prompt Expected Output": "Based on the given information, we can conclude that Anna is the tallest among the three people mentioned.",
            "Proposed Prompt Input (Rationality Verification Round 1)": "Question: Anna is taller than Bob. Bob is taller than Charlie. Who is the tallest?\nInitial response: Based on the given information, we can conclude that Anna is the tallest among the three people mentioned.\nIdentify any logical flaws, inconsistencies, or unsupported claims in the previous response.",
            "Proposed Prompt Expected Output (Rationality Verification Round 1)": "The initial response is logically sound and consistent with the given information. The transitive property of the 'taller than' relation is correctly applied to conclude that Anna is the tallest. There are no apparent logical flaws or unsupported claims in the response.",
            "Proposed Prompt Input (Rationality Verification Round 2)": "Question: Anna is taller than Bob. Bob is taller than Charlie. Who is the tallest?\nInitial response: Based on the given information, we can conclude that Anna is the tallest among the three people mentioned.\nSelf-critique: The initial response is logically sound and consistent with the given information. The transitive property of the 'taller than' relation is correctly applied to conclude that Anna is the tallest. There are no apparent logical flaws or unsupported claims in the response.\nGenerate a revised response that incorporates the self-critique.",
            "Proposed Prompt Expected Output (Rationality Verification Round 2)": "The initial response accurately concludes that Anna is the tallest among the three people mentioned. This conclusion is logically derived from the given statements using the transitive property of the 'taller than' relation. Since Anna is taller than Bob, and Bob is taller than Charlie, it necessarily follows that Anna is also taller than Charlie. Therefore, Anna is the tallest of the three.",
            "Explanation": "In this example, the rationality verification process does not identify any logical flaws or inconsistencies in the initial response. The self-critique confirms that the reasoning is sound and the conclusion is well-supported. The revised response elaborates on the logical steps used to arrive at the conclusion, providing a more detailed and explicit explanation of the transitive property at work. This showcases how the rationality verification prompting can lead to clearer and more well-reasoned responses, even when the initial response is already logically valid."
        },
        "Fallback Plan": "If the rationality verification prompting does not consistently lead to improved response quality or reasoning abilities, we can consider the following fallback plans:\n1. Analyze the self-critiques generated by the models to identify common failure modes or limitations in their ability to recognize logical flaws and inconsistencies. This can inform the development of more targeted prompting strategies or fine-tuning approaches.\n2. Experiment with different prompting strategies for the self-verification step, such as using more specific or structured critique templates, or providing examples of common logical fallacies to guide the model's analysis.\n3. Investigate the impact of different model sizes, architectures, or pre-training data on the effectiveness of rationality verification prompting. This can help identify the key factors that influence a model's ability to engage in self-verification and improve its reasoning.\n4. Conduct a more in-depth error analysis to understand the types of reasoning tasks or domains where rationality verification prompting is most effective, and where it falls short. This can guide the development of more specialized verification strategies for different types of reasoning problems.\n5. If the rationality verification prompting does not yield significant improvements, the project can pivot to a more analytical focus, investigating the limitations of current LLMs in self-verification and logical reasoning. This can involve comparing the performance of different models, analyzing the types of errors they make, and proposing potential solutions or areas for future research."
    },
    "novelty_queries": [
        "KeywordQuery(\"language models rationality verification\")",
        "KeywordQuery(\"language models self-critique logical reasoning\")",
        "KeywordQuery(\"language models iterative refinement prompting\")",
        "KeywordQuery(\"Rationality Verification Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "7715ba5e75f5256e1061c7473afe61bb0dbb9065",
            "paperId": "7715ba5e75f5256e1061c7473afe61bb0dbb9065",
            "title": "Large Language Models are Better Reasoners with Self-Verification",
            "abstract": "Recently, with the chain of thought (CoT) prompting, large language models (LLMs), e.g., GPT-3, have shown strong reasoning ability in several natural language processing tasks such as arithmetic, commonsense, and logical reasoning. However, LLMs with CoT require multi-step prompting and multi-token prediction, which is highly sensitive to individual mistakes and vulnerable to error accumulation. The above issues make the LLMs need the ability to verify the answers. In fact, after inferring conclusions in some thinking decision tasks, people often check them by re-verifying steps to avoid some mistakes. In this paper, we propose and prove that LLMs also have similar self-verification abilities. We take the conclusion obtained by CoT as one of the conditions for solving the original problem. By performing a backward verification of the answers that LLM deduced for itself, we can obtain interpretable answer validation scores to select the candidate answer with the highest score. Experimental results demonstrate that the proposed method can improve the reasoning performance on various arithmetic, commonsense, and logical reasoning datasets. Our code is publicly available at: https://github.com/WENGSYX/Self-Verification.",
            "year": 2022,
            "citationCount": 54,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes and proves that LLMs also have similar self-verification abilities, and takes the conclusion obtained by CoT as one of the conditions for solving the original problem."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is improving the rationality and logical consistency of large language model (LLM) responses through self-verification and revision. The approach involves prompting the LLM to analyze its own responses, identify flaws or inconsistencies, and generate revised responses.\n\nThe research problem in the paper is also improving the reasoning ability of LLMs. The approach involves using the conclusion obtained by chain of thought (CoT) prompting as a condition for solving the original problem and performing backward verification to select the best answer.\n\nBoth the proposal and the paper aim to enhance the reasoning capabilities of LLMs through self-verification. However, the proposal focuses on an iterative process of response generation and self-critique, while the paper uses the CoT conclusion as a starting point for backward verification.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "9a9b1e2968302eb882870537d4af6e2c722dfd1a",
            "paperId": "9a9b1e2968302eb882870537d4af6e2c722dfd1a",
            "title": "Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement",
            "abstract": "To enhance the multi-step reasoning capabilities of large language models, researchers have extensively explored prompting methods, notably the Chain-of-Thought (CoT) method which explicitly elicits human-like rationales. However, they have inadvertently overlooked the potential of enhancing model reasoning performance by formulating higher-quality problems. In this work, we start from the problem side and propose Self-Polish (SP), a novel method that facilitates the model's reasoning by guiding it to progressively refine the given problems to be more comprehensible and solvable. We also explore several automatic prompting varients and propose the Self-Polish prompt bank for the community. SP is orthogonal to all other prompting methods of answer/reasoning side like CoT, allowing for seamless integration with state-of-the-art techniques for further improvement. Thorough experiments show that the proposed method attains notable and consistent effectiveness on five reasoning benchmarks across different models. Furthermore, our method also showcases impressive performance on robustness evaluation. Codes and prompts are available at https://github.com/WooooDyy/Self-Polish.",
            "year": 2023,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Self-Polish (SP) is proposed, a novel method that facilitates the model's reasoning by guiding it to progressively refine the given problems to be more comprehensible and solvable and seamless integration with state-of-the-art techniques for further improvement."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is improving the rationality and logical consistency of large language model (LLM) responses through self-verification and revision. The proposed approach is to prompt LLMs to analyze and critique their own responses, identify logical flaws or inconsistencies, and generate revised responses that address those issues.\n\nThe research problem in the paper is enhancing the multi-step reasoning capabilities of LLMs. The proposed approach is Self-Polish (SP), which guides the model to progressively refine the given problems to be more comprehensible and solvable.\n\nWhile both the proposal and the paper aim to improve the reasoning capabilities of LLMs, their focus and approaches differ. The proposal focuses on rationality verification through self-analysis and revision of the model's responses, while the paper focuses on problem refinement to make the input more comprehensible and solvable for the model.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "3aaf6a2cbad5850ad81ab5c163599cb3d523436f",
            "paperId": "3aaf6a2cbad5850ad81ab5c163599cb3d523436f",
            "title": "Self-Refine: Iterative Refinement with Self-Feedback",
            "abstract": "Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.",
            "year": 2023,
            "citationCount": 505,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Self-Refine is introduced, an approach for improving initial outputs from LLMs through iterative feedback and refinement that demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using this simple, standalone approach."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is improving the rationality and logical consistency of LLM responses through self-verification and iterative refinement. The approach involves prompting the LLM to analyze its own responses, identify flaws or inconsistencies, and generate revised responses.\n\nThe research problem in the paper is also improving the quality of LLM outputs through iterative refinement and self-feedback. The approach, called Self-Refine, uses the same LLM to generate an initial output, provide feedback on its own output, and iteratively refine it.\n\nBoth the proposal and the paper aim to improve LLM outputs through iterative refinement and self-feedback, without requiring additional training data or reinforcement learning. They share the same high-level goal and approach of using the LLM's own capabilities for self-improvement.\n\nYes",
            "novelty_judgment": "yes"
        },
        {
            "id": "e826ac71dad8c4ce36d82fb7add43e3d306bb7e1",
            "paperId": "e826ac71dad8c4ce36d82fb7add43e3d306bb7e1",
            "title": "Making Language Models Better Reasoners with Step-Aware Verifier",
            "abstract": "Few-shot learning is a challenging task that requires language models to generalize from limited examples. Large language models like GPT-3 and PaLM have made impressive progress in this area, but they still face difficulties in reasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve their reasoning skills, previous work has proposed to guide the language model with prompts that elicit a series of reasoning steps before giving the final answer, achieving a significant improvement on GSM8K from 17.9% to 58.1% in problem-solving rate. In this paper, we present DiVeRSe (Diverse Verifier on Reasoning Step), a novel approach that further enhances the reasoning capability of language models. DiVeRSe has three main components: first, it generates diverse prompts to explore different reasoning paths for the same question; second, it uses a verifier to filter out incorrect answers based on a weighted voting scheme; and third, it verifies each reasoning step individually instead of the whole chain. We evaluate DiVeRSe on the latest language model code-davinci-002 and show that it achieves new state-of-the-art results on six of eight reasoning benchmarks (e.g., GSM8K 74.4% to 83.2%).",
            "year": 2022,
            "citationCount": 87,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents DiVeRSe (Diverse Verifier on Reasoning Step), a novel approach that further enhances the reasoning capability of language models and achieves new state-of-the-art results on six of eight reasoning benchmarks."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the rationality and logical consistency of language model responses through self-verification and revision. The approach involves prompting the model to analyze its own responses, identify flaws, and generate revised responses.\n\nThe research problem in the paper is improving the reasoning skills of language models on benchmarks like GSM8K. The approach involves using diverse prompts to explore different reasoning paths, a verifier to filter out incorrect answers, and step-by-step verification.\n\nWhile both works aim to improve the reasoning capabilities of language models, the proposal focuses on self-verification and revision to improve logical consistency, while the paper focuses on using diverse prompts and a verifier to improve performance on specific reasoning benchmarks.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "6233b5863f9a0e8bacce47ce21bc3e81c09497bd",
            "paperId": "6233b5863f9a0e8bacce47ce21bc3e81c09497bd",
            "title": "A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning",
            "abstract": "Logical reasoning has been an ongoing pursuit in the field of AI. Despite significant advancements made by large language models (LLMs), they still struggle with complex logical reasoning problems. To enhance reasoning performance, one promising direction is scalable oversight, which requires LLMs to identify their own errors and then improve by themselves. Various self-verification methods have been proposed in pursuit of this goal. Nevertheless, whether existing models understand their own errors well is still under investigation. In this paper, we take a closer look at the self-verification abilities of LLMs in the context of logical reasoning, focusing on their ability to identify logical fallacies accurately. We introduce a dataset, FALLACIES, containing 232 types of reasoning fallacies categorized in a hierarchical taxonomy. By conducting exhaustive experiments on FALLACIES, we obtain comprehensive and detailed analyses of a series of models on their verification abilities. Our main findings suggest that existing LLMs could struggle to identify fallacious reasoning steps accurately and may fall short of guaranteeing the validity of self-verification methods. Drawing from these observations, we offer suggestions for future research and practical applications of self-verification methods.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A closer look at the self-verification abilities of LLMs in the context of logical reasoning, focusing on their ability to identify logical fallacies accurately, suggests that existing LLMs could struggle to identify fallacious reasoning steps accurately and may fall short of guaranteeing the validity of self- Verification methods."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the rationality and logical consistency of LLM responses through self-verification and iterative revision. The approach involves prompting LLMs to analyze their own responses, identify flaws or inconsistencies, and generate revised responses.\n\nThe research problem in the paper is investigating the self-verification abilities of LLMs in logical reasoning, focusing on their ability to accurately identify logical fallacies. The approach involves evaluating LLMs on a dataset of reasoning fallacies and analyzing their performance in identifying these fallacies.\n\nWhile both works involve LLMs and logical reasoning, the proposal focuses on improving LLM responses through self-verification, while the paper focuses on evaluating LLMs' ability to identify logical fallacies. The proposal aims to develop a method for enhancing LLM reasoning, while the paper aims to analyze the limitations of LLMs in self-verification.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "44d74b0d77b4056ddd4c6611a76711c8bab2e0a7",
            "paperId": "44d74b0d77b4056ddd4c6611a76711c8bab2e0a7",
            "title": "Dehallucinating Large Language Models Using Formal Methods Guided Iterative Prompting",
            "abstract": "Large language models (LLMs) such as ChatGPT have been trained to generate human-like responses to natural language prompts. LLMs use a vast corpus of text data for training, and can generate coherent and contextually relevant responses to a wide range of questions and statements. Despite this remarkable progress, LLMs are prone to hallucinations making their application to safety-critical applications such as autonomous systems difficult. The hallucinations in LLMs refer to instances where the model generates responses that are not factually accurate or contextually appropriate. These hallucinations can occur due to a variety of factors, such as the model\u2019s lack of real-world knowledge, the influence of biased or inaccurate training data, or the model\u2019s tendency to generate responses based on statistical patterns rather than a true understanding of the input. While these hallucinations are a nuisance in tasks such as text summarization and question-answering, they can be catastrophic when LLMs are used in autonomy-relevant applications such as planning. In this paper, we focus on the application of LLMs in autonomous systems and sketch a novel self-monitoring and iterative prompting architecture that uses formal methods to detect these errors in the LLM response automatically. We exploit the dialog capability of LLMs to iteratively steer them to responses that are consistent with our correctness specification. We report preliminary experiments that show the promise of the proposed approach on tasks such as automated planning.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper sketches a novel self-monitoring and iterative prompting architecture that uses formal methods to detect errors in the LLM response automatically and exploits the dialog capability of LLMs to iteratively steer them to responses that are consistent with the correctness specification."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the rationality and logical consistency of large language model (LLM) responses through self-verification and iterative refinement. The proposed approach is to prompt the LLM to analyze its own responses, identify logical flaws or inconsistencies, and generate revised responses that address those issues.\n\nThe research problem in the paper is reducing hallucinations (factual inaccuracies or contextually inappropriate responses) in LLMs when applied to autonomous systems. The proposed approach is to use formal methods to automatically detect errors in LLM responses and iteratively steer the model towards responses that are consistent with a correctness specification.\n\nWhile both works aim to improve the reliability and accuracy of LLM responses, the specific research problems and proposed approaches are different. The proposal focuses on logical consistency and self-verification, while the paper focuses on factual accuracy and using formal methods for error detection.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "5fb3521f87d03899731b98718702927afd227f3a",
            "paperId": "5fb3521f87d03899731b98718702927afd227f3a",
            "title": "Enhancing Ethical Explanations of Large Language Models through Iterative Symbolic Refinement",
            "abstract": "An increasing amount of research in Natural Language Inference (NLI) focuses on the application and evaluation of Large Language Models (LLMs) and their reasoning capabilities. Despite their success, however, LLMs are still prone to factual errors and inconsistencies in their explanations, offering limited control and interpretability for inference in complex domains. In this paper, we focus on ethical NLI, investigating how hybrid neuro-symbolic techniques can enhance the logical validity and alignment of ethical explanations produced by LLMs. Specifically, we present an abductive-deductive framework named Logic-Explainer, which integrates LLMs with an external backward-chaining solver to refine step-wise natural language explanations and jointly verify their correctness, reduce incompleteness and minimise redundancy. An extensive empirical analysis demonstrates that Logic-Explainer can improve explanations generated via in-context learning methods and Chain-of-Thought (CoT) on challenging ethical NLI tasks, while, at the same time, producing formal proofs describing and supporting models\u2019 reasoning. As ethical NLI requires commonsense reasoning to identify underlying moral violations, our results suggest the effectiveness of neuro-symbolic methods for multi-step NLI more broadly, opening new opportunities to enhance the logical consistency, reliability, and alignment of LLMs.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An abductive-deductive framework named Logic-Explainer is presented, which integrates LLMs with an external backward-chaining solver to refine step-wise natural language explanations and jointly verify their correctness, reduce incompleteness and minimise redundancy."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the rationality and logical consistency of LLM responses through self-verification and iterative refinement. The approach involves prompting LLMs to analyze and critique their own responses to identify and correct logical flaws or inconsistencies.\n\nThe research problem in the paper is enhancing the logical validity and alignment of ethical explanations produced by LLMs. The approach involves integrating LLMs with an external backward-chaining solver to refine explanations and verify their correctness.\n\nWhile both works aim to improve the logical consistency and reliability of LLM outputs, the proposal focuses on self-verification through iterative prompting, while the paper uses a hybrid neuro-symbolic approach with an external solver. The application domains also differ, with the proposal targeting general reasoning tasks and the paper focusing specifically on ethical reasoning.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "409c3c9c9c8729c46113845c304329a24489ebcb",
            "paperId": "409c3c9c9c8729c46113845c304329a24489ebcb",
            "title": "Language Models with Rationality",
            "abstract": "While large language models (LLMs) are proficient at question-answering (QA), it is not always clear how (or even if) an answer follows from their latent\"beliefs\". This lack of interpretability is a growing impediment to widespread use of LLMs. To address this, our goals are to make model beliefs and their inferential relationships explicit, and to resolve inconsistencies that may exist, so that answers are supported by interpretable chains of reasoning drawn from a consistent network of beliefs. Our approach, which we call REFLEX, is to add a rational, self-reflecting layer on top of the LLM. First, given a question, we construct a belief graph using a backward-chaining process to materialize relevant model beliefs (including beliefs about answer candidates) and their inferential relationships. Second, we identify and minimize contradictions in that graph using a formal constraint reasoner. We find that REFLEX significantly improves consistency (by 8%-11% absolute) without harming overall answer accuracy, resulting in answers supported by faithful chains of reasoning drawn from a more consistent belief system. This suggests a new style of system architecture in which an LLM extended with a rational layer can provide an interpretable window into system beliefs, add a systematic reasoning capability, and repair latent inconsistencies present in the LLM.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new style of system architecture in which an LLM extended with a rational layer can provide an interpretable window into system beliefs, add a systematic reasoning capability, and repair latent inconsistencies present in the LLM is suggested."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the rationality and logical consistency of large language model (LLM) responses through self-verification and revision. The approach involves prompting the LLM to analyze its own responses, identify logical flaws or inconsistencies, and generate revised responses.\n\nThe research problem in the paper is also improving the rationality and consistency of LLM responses. However, the approach is different. The paper proposes adding a rational, self-reflecting layer on top of the LLM to construct a belief graph, identify contradictions, and minimize them using a constraint reasoner.\n\nWhile both the proposal and the paper aim to improve LLM rationality, their methods differ significantly. The proposal focuses on prompting for self-verification and revision, while the paper introduces a separate rational layer for belief graph construction and contradiction resolution.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
            "paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
            "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "abstract": "Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).",
            "year": 2022,
            "citationCount": 1396,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting that first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the rationality and logical consistency of language model responses through self-verification and revision. The approach involves prompting the model to analyze its own responses, identify flaws, and generate revised responses.\n\nThe research problem in the paper is improving the reasoning performance of language models on complex tasks. The approach involves using a self-consistency decoding strategy that samples multiple reasoning paths and selects the most consistent answer.\n\nWhile both works aim to improve the reasoning abilities of language models, the proposal focuses on self-verification and iterative revision of responses, while the paper proposes a new decoding strategy for chain-of-thought prompting. The specific research problems and approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "6d4bacb69923e1e94fb4de468b939ce6db32fb51",
            "paperId": "6d4bacb69923e1e94fb4de468b939ce6db32fb51",
            "title": "Large Language Models Cannot Self-Correct Reasoning Yet",
            "abstract": "Large Language Models (LLMs) have emerged as a groundbreaking technology with their unparalleled text generation capabilities across various applications. Nevertheless, concerns persist regarding the accuracy and appropriateness of their generated content. A contemporary methodology, self-correction, has been proposed as a remedy to these issues. Building upon this premise, this paper critically examines the role and efficacy of self-correction within LLMs, shedding light on its true potential and limitations. Central to our investigation is the notion of intrinsic self-correction, whereby an LLM attempts to correct its initial responses based solely on its inherent capabilities, without the crutch of external feedback. In the context of reasoning, our research indicates that LLMs struggle to self-correct their responses without external feedback, and at times, their performance even degrades after self-correction. Drawing from these insights, we offer suggestions for future research and practical applications in this field.",
            "year": 2023,
            "citationCount": 122,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is indicated that LLMs struggle to self-correct their responses without external feedback, and at times, their performance even degrades after self-correction."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the rationality and logical consistency of LLM responses through self-verification and revision. The approach involves prompting LLMs to analyze and critique their own responses iteratively until a satisfactory response is generated.\n\nThe research problem in the paper is examining the efficacy of self-correction in LLMs for improving reasoning capabilities. The approach involves testing the intrinsic self-correction abilities of LLMs without external feedback.\n\nWhile both the proposal and the paper focus on improving LLM reasoning, the proposal aims to develop a specific prompting technique for self-verification, while the paper investigates the general ability of LLMs to self-correct reasoning without external intervention. The approaches differ in their focus on prompting versus intrinsic capabilities.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "e879f54b2b5760bbb6d010977ddcedfb62452b38",
            "paperId": "e879f54b2b5760bbb6d010977ddcedfb62452b38",
            "title": "Can Large Language Models Really Improve by Self-critiquing Their Own Plans?",
            "abstract": "There have been widespread claims about Large Language Models (LLMs) being able to successfully verify or self-critique their candidate solutions in reasoning problems in an iterative mode. Intrigued by those claims, in this paper we set out to investigate the verification/self-critiquing abilities of large language models in the context of planning. We evaluate a planning system that employs LLMs for both plan generation and verification. We assess the verifier LLM's performance against ground-truth verification, the impact of self-critiquing on plan generation, and the influence of varying feedback levels on system performance. Using GPT-4, a state-of-the-art LLM, for both generation and verification, our findings reveal that self-critiquing appears to diminish plan generation performance, especially when compared to systems with external, sound verifiers and the LLM verifiers in that system produce a notable number of false positives, compromising the system's reliability. Additionally, the nature of feedback, whether binary or detailed, showed minimal impact on plan generation. Collectively, our results cast doubt on the effectiveness of LLMs in a self-critiquing, iterative framework for planning tasks.",
            "year": 2023,
            "citationCount": 32,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Investigation of the verification/self-critique abilities of large language models in the context of planning reveals that self-critiquing appears to diminish plan generation performance, especially when compared to systems with external, sound verifiers and the LLM verifiers in that system produce a notable number of false positives, compromising the system's reliability."
            },
            "score": 6
        },
        {
            "id": "0885471c0215b3c0d31c82518066913f7f738128",
            "paperId": "0885471c0215b3c0d31c82518066913f7f738128",
            "title": "Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement",
            "abstract": "The ability to derive underlying principles from a handful of observations and then generalize to novel situations -- known as inductive reasoning -- is central to human intelligence. Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks. In this work, we conduct a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting. Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules. By examining the intermediate rules, we observe that LMs are phenomenal hypothesis proposers (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the proposed set of rules, this hybrid approach achieves strong results across inductive reasoning benchmarks that require inducing causal relations, language-like instructions, and symbolic concepts. However, they also behave as puzzling inductive reasoners, showing notable performance gaps between rule induction (i.e., identifying plausible rules) and rule application (i.e., applying proposed rules to instances), suggesting that LMs are proposing hypotheses without being able to actually apply the rules. Through empirical and human analyses, we further reveal several discrepancies between the inductive reasoning processes of LMs and humans, shedding light on both the potentials and limitations of using LMs in inductive reasoning tasks.",
            "year": 2023,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work conducts a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting."
            },
            "score": 6
        },
        {
            "id": "ac37accd7aedf1c25c3d54c7982579b297b3ff2b",
            "paperId": "ac37accd7aedf1c25c3d54c7982579b297b3ff2b",
            "title": "Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models",
            "abstract": "Large language models (LLMs) can achieve highly effective performance on various reasoning tasks by incorporating step-by-step chain-of-thought (CoT) prompting as demonstrations. However, the reasoning chains of demonstrations generated by LLMs are prone to errors, which can subsequently lead to incorrect reasoning during inference. Furthermore, inappropriate exemplars (overly simplistic or complex), can affect overall performance among varying levels of difficulty. We introduce Iter-CoT (Iterative bootstrapping in Chain-of-Thoughts Prompting), an iterative bootstrapping approach for selecting exemplars and generating reasoning chains. By utilizing iterative bootstrapping, our approach enables LLMs to autonomously rectify errors, resulting in more precise and comprehensive reasoning chains. Simultaneously, our approach selects challenging yet answerable questions accompanied by reasoning chains as exemplars with a moderate level of difficulty, which enhances the LLMs' generalizability across varying levels of difficulty. Experimental results indicate that Iter-CoT exhibits superiority, achieving competitive performance across three distinct reasoning tasks on ten datasets.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "By utilizing iterative bootstrapping, this approach enables LLMs to autonomously rectify errors, resulting in more precise and comprehensive reasoning chains, which enhances the LLMs' generalizability across varying levels of difficulty."
            },
            "score": 6
        },
        {
            "id": "2aba5bba16dac5cd62683bab9de5d6faaaed0de1",
            "paperId": "2aba5bba16dac5cd62683bab9de5d6faaaed0de1",
            "title": "Shepherd Pre-trained Language Models to Develop a Train of Thought: An Iterative Prompting Approach",
            "abstract": "While Pre-trained Language Models (PLMs) 001 internalize a great amount of world knowledge, 002 they have been shown incapable of recalling 003 these knowledge to solve tasks requiring com-004 plex & multi-step inference procedures. Simi-005 lar to how humans develop a \u201ctrain of thought\u201d 006 for these tasks, how can we equip PLMs with 007 such abilities? In this work, we explore an iter-008 ative prompting framework, a new prompting 009 paradigm which progressively elicits relevant 010 knowledge from PLMs for multi-step inference 011 tasks. We identify key limitations of existing 012 prompting methods, namely they are either re-013 stricted to queries with a single identifiable re-014 lation/predicate, or being agnostic to input con-015 texts, which makes it difficult to capture vari-016 abilities across different inference steps. We 017 propose an iterative context-aware prompter, 018 which addresses these limitations by learning 019 to dynamically synthesize prompts conditioned 020 on the current step\u2019s contexts. Experiments on 021 three datasets involving multi-step inference 022 show the effectiveness of the iterative scheme 023 and the context-aware prompter design. 1 024",
            "year": 2022,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores an iter-008 ative prompting framework, a new prompting 009 paradigm which progressively elicits relevant 010 knowledge from PLMs for multi-step inference 011 tasks, and proposes an iterative context-aware prompter design which addresses key limitations of existing prompting methods."
            },
            "score": 6
        },
        {
            "id": "7011bf9aa7e68fabaa1df498da6d2dd8a950f037",
            "paperId": "7011bf9aa7e68fabaa1df498da6d2dd8a950f037",
            "title": "Pushing the Limits of ChatGPT on NLP Tasks",
            "abstract": "Despite the success of ChatGPT, its performances on most NLP tasks are still well below the supervised baselines. In this work, we looked into the causes, and discovered that its subpar performance was caused by the following factors: (1) token limit in the prompt does not allow for the full utilization of the supervised datasets; (2) mismatch between the generation nature of ChatGPT and NLP tasks; (3) intrinsic pitfalls of LLMs models, e.g., hallucination, overly focus on certain keywords, etc. In this work, we propose a collection of general modules to address these issues, in an attempt to push the limits of ChatGPT on NLP tasks. Our proposed modules include (1) a one-input-multiple-prompts strategy that employs multiple prompts for one input to accommodate more demonstrations; (2) using fine-tuned models for better demonstration retrieval; (3) transforming tasks to formats that are more tailored to the generation nature; (4) employing reasoning strategies that are tailored to addressing the task-specific complexity; (5) the self-verification strategy to address the hallucination issue of LLMs; (6) the paraphrase strategy to improve the robustness of model predictions. We conduct experiments on 21 datasets of 10 representative NLP tasks, including question answering, commonsense reasoning, natural language inference, sentiment analysis, named entity recognition, entity-relation extraction, event extraction, dependency parsing, semantic role labeling, and part-of-speech tagging. Using the proposed assemble of techniques, we are able to significantly boost the performance of ChatGPT on the selected NLP tasks, achieving performances comparable to or better than supervised baselines, or even existing SOTA performances.",
            "year": 2023,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Using the proposed assemble of techniques, this work is able to significantly boost the performance of ChatGPT on the selected NLP tasks, achieving performances comparable to or better than supervised baselines, or even existing SOTA performances."
            },
            "score": 6
        },
        {
            "id": "629c441076da3f8185b1cf85e8036064b714e249",
            "paperId": "629c441076da3f8185b1cf85e8036064b714e249",
            "title": "Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework",
            "abstract": "As large language models (LLMs) have become the norm in NLP, demonstrating good performance in generation and reasoning tasks, one of its most fatal disadvantages is the lack of factual correctness. Generating unfactual texts not only leads to lower performances but also degrades the trust and validity of their applications. Chain-of-Thought (CoT) prompting improves trust and model performance on complex reasoning tasks by generating interpretable reasoning chains, but still suffers from factuality concerns in knowledge-intensive tasks. In this paper, we propose the Verify-and-Edit framework for CoT prompting, which seeks to increase prediction factuality by post-editing reasoning chains according to external knowledge. Building on top of GPT-3, our framework lead to accuracy improvements in multiple open-domain question-answering tasks.",
            "year": 2023,
            "citationCount": 69,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Verify-and-Edit framework for CoT prompting is proposed, which seeks to increase prediction factuality by post-editing reasoning chains according to external knowledge and lead to accuracy improvements in multiple open-domain question-answering tasks."
            },
            "score": 6
        },
        {
            "id": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
            "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
            "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
            "year": 2022,
            "citationCount": 3517,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks."
            },
            "score": 5
        },
        {
            "id": "8236c7cb26222b98af6dbcb15f9166f957b6ea67",
            "paperId": "8236c7cb26222b98af6dbcb15f9166f957b6ea67",
            "title": "Knowledge-Augmented Language Model Verification",
            "abstract": "Recent Language Models (LMs) have shown impressive capabilities in generating texts with the knowledge internalized in parameters. Yet, LMs often generate the factually incorrect responses to the given queries, since their knowledge may be inaccurate, incomplete, and outdated. To address this problem, previous works propose to augment LMs with the knowledge retrieved from an external knowledge source. However, such approaches often show suboptimal text generation performance due to two reasons: 1) the model may fail to retrieve the knowledge relevant to the given query, or 2) the model may not faithfully reflect the retrieved knowledge in the generated text. To overcome these, we propose to verify the output and the knowledge of the knowledge-augmented LMs with a separate verifier, which is a small LM that is trained to detect those two types of errors through instruction-finetuning. Then, when the verifier recognizes an error, we can rectify it by either retrieving new knowledge or generating new text. Further, we use an ensemble of the outputs from different instructions with a single verifier to enhance the reliability of the verification processes. We validate the effectiveness of the proposed verification steps on multiple question answering benchmarks, whose results show that the proposed verifier effectively identifies retrieval and generation errors, allowing LMs to provide more factually correct outputs. Our code is available at https://github.com/JinheonBaek/KALMV.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes to verify the output and the knowledge of the knowledge-augmented LMs with a separate verifier, which is a small LM that is trained to detect those two types of errors through instruction-finetuning."
            },
            "score": 5
        },
        {
            "id": "806b5882c983bd156a8c10bcd34fe285d8a0593b",
            "paperId": "806b5882c983bd156a8c10bcd34fe285d8a0593b",
            "title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
            "abstract": "Recently, large language models (LLMs), including notable models such as GPT-4 and burgeoning community models, have showcased significant general language understanding abilities. However, there has been a scarcity of attempts to assess the logical reasoning capacities of these LLMs, an essential facet of natural language understanding. To encourage further investigation in this area, we introduce GLoRE, a meticulously assembled General Logical Reasoning Evaluation benchmark comprised of 12 datasets that span three different types of tasks. Our experimental results show that compared to the performance of human and supervised fine-tuning, the logical reasoning capabilities of open LLM models necessitate additional improvement; ChatGPT and GPT-4 show a strong capability of logical reasoning, with GPT-4 surpassing ChatGPT by a large margin. We propose a self-consistency probing method to enhance the accuracy of ChatGPT and a fine-tuned method to boost the performance of an open LLM. We release the datasets and evaluation programs to facilitate future research.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "GLoRE is introduced, a meticulously assembled General Logical Reasoning Evaluation benchmark comprised of 12 datasets that span three different types of tasks and proposes a self-consistency probing method to enhance the accuracy of ChatGPT and a fine-tuned method to boost the performance of an open LLM."
            },
            "score": 5
        },
        {
            "id": "9cd398e75e89b9d8104837da44ad17e110a4e4f9",
            "paperId": "9cd398e75e89b9d8104837da44ad17e110a4e4f9",
            "title": "Explicit Planning Helps Language Models in Logical Reasoning",
            "abstract": "Language models have been shown to perform remarkably well on a wide range of natural language processing tasks. In this paper, we propose LEAP, a novel system that uses language models to perform multi-step logical reasoning and incorporates explicit planning into the inference procedure. Explicit planning enables the system to make more informed reasoning decisions at each step by looking ahead into their future effects. Moreover, we propose a training strategy that safeguards the planning process from being led astray by spurious features. Our full system significantly outperforms other competing methods on multiple standard datasets. When using small T5 models as its core selection and deduction components, our system performs competitively compared to GPT-3 despite having only about 1B parameters (i.e., 175 times smaller than GPT-3). When using GPT-3.5, it significantly outperforms chain-of-thought prompting on the challenging PrOntoQA dataset. We have conducted extensive empirical studies to demonstrate that explicit planning plays a crucial role in the system's performance.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes LEAP, a novel system that uses language models to perform multi-step logical reasoning and incorporates explicit planning into the inference procedure and proposes a training strategy that safeguards the planning process from being led astray by spurious features."
            },
            "score": 5
        },
        {
            "id": "f53a4f34757d1f237446b4d887d5323f2a17ed02",
            "paperId": "f53a4f34757d1f237446b4d887d5323f2a17ed02",
            "title": "PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine",
            "abstract": "As an effective tool for eliciting the power of Large Language Models (LLMs), prompting has recently demonstrated unprecedented abilities across a variety of complex tasks. To further improve the performance, prompt ensemble has attracted substantial interest for tackling the hallucination and instability of LLMs. However, existing methods usually adopt a two-stage paradigm, which requires a pre-prepared set of prompts with substantial manual effort, and is unable to perform directed optimization for different weak learners. In this paper, we propose a simple, universal, and automatic method named PREFER (Prompt Ensemble learning via Feedback-Reflect-Refine) to address the stated limitations. Specifically, given the fact that weak learners are supposed to focus on hard examples during boosting, PREFER builds a feedback mechanism for reflecting on the inadequacies of existing weak learners. Based on this, the LLM is required to automatically synthesize new prompts for iterative refinement. Moreover, to enhance stability of the prompt effect evaluation, we propose a novel prompt bagging method involving forward and backward thinking, which is superior to majority voting and is beneficial for both feedback and weight calculation in boosting. Extensive experiments demonstrate that our PREFER achieves state-of-the-art performance in multiple types of tasks by a significant margin. We have made our code publicly available.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a simple, universal, and automatic method named PREFER (Prompt Ensemble learning via Feedback-Reflect-Refine) to address the stated limitations of prompt ensemble and builds a feedback mechanism for reflecting on the inadequacies of existing weak learners."
            },
            "score": 5
        },
        {
            "id": "7ed0faa6720cd176d57badbc0455af31a03f080c",
            "paperId": "7ed0faa6720cd176d57badbc0455af31a03f080c",
            "title": "Towards Expert-Level Medical Question Answering with Large Language Models",
            "abstract": "Recent artificial intelligence (AI) systems have reached milestones in\"grand challenges\"ranging from Go to protein-folding. The capability to retrieve medical knowledge, reason over it, and answer medical questions comparably to physicians has long been viewed as one such grand challenge. Large language models (LLMs) have catalyzed significant progress in medical question answering; Med-PaLM was the first model to exceed a\"passing\"score in US Medical Licensing Examination (USMLE) style questions with a score of 67.2% on the MedQA dataset. However, this and other prior work suggested significant room for improvement, especially when models' answers were compared to clinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by leveraging a combination of base LLM improvements (PaLM 2), medical domain finetuning, and prompting strategies including a novel ensemble refinement approach. Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 19% and setting a new state-of-the-art. We also observed performance approaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU clinical topics datasets. We performed detailed human evaluations on long-form questions along multiple axes relevant to clinical applications. In pairwise comparative ranking of 1066 consumer medical questions, physicians preferred Med-PaLM 2 answers to those produced by physicians on eight of nine axes pertaining to clinical utility (p<0.001). We also observed significant improvements compared to Med-PaLM on every evaluation axis (p<0.001) on newly introduced datasets of 240 long-form\"adversarial\"questions to probe LLM limitations. While further studies are necessary to validate the efficacy of these models in real-world settings, these results highlight rapid progress towards physician-level performance in medical question answering.",
            "year": 2023,
            "citationCount": 233,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Results highlight rapid progress towards physician-level performance in medical question answering by leveraging a combination of base LLM improvements (PaLM 2), medical domain finetuning, and prompting strategies including a novel ensemble refinement approach."
            },
            "score": 5
        },
        {
            "id": "995b2f650f55de6077b87db6dadb01cecd86dbd7",
            "paperId": "995b2f650f55de6077b87db6dadb01cecd86dbd7",
            "title": "Advanced prompting as a catalyst: Empowering large language models in the management of gastrointestinal cancers",
            "abstract": "Large Language Models' (LLMs) performance in healthcare can be significantly impacted by prompt engineering. However, the area of study remains relatively uncharted in gastrointestinal oncology until now. Our research delves into this unexplored territory, investigating the efficacy of varied prompting strategies, including simple prompts, templated prompts, in-context learning (ICL), and multi-round iterative questioning, for optimizing the performance of LLMs within a medical setting. We develop a comprehensive evaluation system to assess the performance of LLMs across multiple dimensions. This robust evaluation system ensures a thorough assessment of the LLMs' capabilities in the field of medicine. Our findings suggest a positive relationship between the comprehensiveness of the prompts and the LLMs' performance. Notably, the multi-round strategy, which is characterized by iterative question-and-answer rounds, consistently yields the best results. ICL, a strategy that capitalizes on interrelated contextual learning, also displays significant promise, surpassing the outcomes achieved with simpler prompts. The research underscores the potential of advanced prompt engineering and iterative learning approaches for boosting the applicability of LLMs in healthcare. We recommend that additional research be conducted to refine these strategies and investigate their potential integration, to truly harness the full potential of LLMs in medical applications.\n",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The research investigates the efficacy of varied prompting strategies, including simple prompts, templated prompts, in-context learning (ICL), and multi-round iterative questioning, for optimizing the performance of LLMs within a medical setting, and develops a comprehensive evaluation system."
            },
            "score": 5
        },
        {
            "id": "9ffefdf1fcd780cb71450b0a7a29247c66aa87be",
            "paperId": "9ffefdf1fcd780cb71450b0a7a29247c66aa87be",
            "title": "The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning",
            "abstract": "Does prompting a large language model (LLM) like GPT-3 with explanations improve in-context learning? We study this question on two NLP tasks that involve reasoning over text, namely question answering and natural language inference. We test the performance of four LLMs on three textual reasoning datasets using prompts that include explanations in multiple different styles. For these tasks, we find that including explanations in the prompts for OPT, GPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to moderate accuracy improvements over standard few-show learning. However, text-davinci-002 is able to benefit more substantially. We further show that explanations generated by the LLMs may not entail the models' predictions nor be factually grounded in the input, even on simple tasks with extractive explanations. However, these flawed explanations can still be useful as a way to verify LLMs' predictions post-hoc. Through analysis in our three settings, we show that explanations judged by humans to be good--logically consistent with the input and the prediction--more likely cooccur with accurate predictions. Following these observations, we train calibrators using automatically extracted scores that assess the reliability of explanations, allowing us to improve performance post-hoc across all of our datasets.",
            "year": 2022,
            "citationCount": 95,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work studies two NLP tasks that involve reasoning over text, namely question answering and natural language inference, and shows that explanations judged by humans to be good--logically consistent with the input and the prediction--more likely cooccur with accurate predictions."
            },
            "score": 5
        },
        {
            "id": "e811e771f5950d86eafe50655c0d1e5b571e19b6",
            "paperId": "e811e771f5950d86eafe50655c0d1e5b571e19b6",
            "title": "The Unreliability of Explanations in Few-Shot In-Context Learning",
            "abstract": "How can prompting a large language model like GPT-3 with explanations improve in-context learning? We focus speci\ufb01cally on two NLP tasks that involve reasoning over text, namely question answering and natural language inference. Including explanations in the prompt and having the model generate them does not consistently improve performance in the settings we study, contrary to recent results on symbolic reasoning tasks (Nye et al., 2021; Wei et al., 2022). Despite careful prompting, explanations generated by GPT-3 may not even be factually grounded in the input, even on simple tasks with straightforward extractive explanations. However, these \ufb02awed explanations can still be useful as a way to verify GPT-3\u2019s predictions post-hoc. Through analysis in three settings, we show that explanations judged as good by humans\u2014those that are logically consistent with the input and the prediction\u2014usually indicate more accurate predictions. Following these observations, we present a framework for calibrating model predictions based on the reliability of the explanations. Our framework trains calibrators using automatically extracted scores that approximately assess the reliability of explanations, which helps improve performance across three different datasets",
            "year": 2022,
            "citationCount": 29,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A framework for calibrating model predictions based on the reliability of explanations is presented, showing that explanations judged as good by humans\u2014those that are logically consistent with the input and the prediction\u2014usually indicate more accurate predictions."
            },
            "score": 5
        },
        {
            "id": "99832586d55f540f603637e458a292406a0ed75d",
            "paperId": "99832586d55f540f603637e458a292406a0ed75d",
            "title": "LANGUAGE MODELS",
            "abstract": "While large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. Furthermore, on two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.",
            "year": 2023,
            "citationCount": 600,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces."
            },
            "score": 4
        },
        {
            "id": "8666f9f379389a5dff31e72fb0f992a37763ba41",
            "paperId": "8666f9f379389a5dff31e72fb0f992a37763ba41",
            "title": "Teaching language models to support answers with verified quotes",
            "abstract": "Recent large language models often answer factual questions correctly. But users can't trust any given claim a model makes without fact-checking, because language models can hallucinate convincing nonsense. In this work we use reinforcement learning from human preferences (RLHP) to train\"open-book\"QA models that generate answers whilst also citing specific evidence for their claims, which aids in the appraisal of correctness. Supporting evidence is drawn from multiple documents found via a search engine, or from a single user-provided document. Our 280 billion parameter model, GopherCite, is able to produce answers with high quality supporting evidence and abstain from answering when unsure. We measure the performance of GopherCite by conducting human evaluation of answers to questions in a subset of the NaturalQuestions and ELI5 datasets. The model's response is found to be high-quality 80\\% of the time on this Natural Questions subset, and 67\\% of the time on the ELI5 subset. Abstaining from the third of questions for which it is most unsure improves performance to 90\\% and 80\\% respectively, approaching human baselines. However, analysis on the adversarial TruthfulQA dataset shows why citation is only one part of an overall strategy for safety and trustworthiness: not all claims supported by evidence are true.",
            "year": 2022,
            "citationCount": 157,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work uses reinforcement learning from human preferences (RLHP) to train\"open-book\"QA models that generate answers whilst also citing specific evidence for their claims, which aids in the appraisal of correctness."
            },
            "score": 4
        },
        {
            "id": "9e9e4df2996bac794c4f04cb887df3e553bae4fd",
            "paperId": "9e9e4df2996bac794c4f04cb887df3e553bae4fd",
            "title": "Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning",
            "abstract": "Large Language Models (LLMs) have shown human-like reasoning abilities but still struggle with complex logical problems. This paper introduces a novel framework, Logic-LM, which integrates LLMs with symbolic solvers to improve logical problem-solving. Our method first utilizes LLMs to translate a natural language problem into a symbolic formulation. Afterward, a deterministic symbolic solver performs inference on the formulated problem. We also introduce a self-refinement module, which utilizes the symbolic solver's error messages to revise symbolic formalizations. We demonstrate Logic-LM's effectiveness on five logical reasoning datasets: ProofWriter, PrOntoQA, FOLIO, LogicalDeduction, and AR-LSAT. On average, Logic-LM achieves a significant performance boost of 39.2% over using LLM alone with standard prompting and 18.4% over LLM with chain-of-thought prompting. Our findings suggest that Logic-LM, by combining LLMs with symbolic logic, offers a promising avenue for faithful logical reasoning. Code and data are publicly available at https://github.com/teacherpeterpan/Logic-LLM.",
            "year": 2023,
            "citationCount": 59,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a novel framework, Logic-LM, which integrates LLMs with symbolic solvers to improve logical problem-solving and suggests that it offers a promising avenue for faithful logical reasoning."
            },
            "score": 4
        },
        {
            "id": "66d98dc2aad17c03532dbae21d05f098257cc2e2",
            "paperId": "66d98dc2aad17c03532dbae21d05f098257cc2e2",
            "title": "LINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models with First-Order Logic Provers",
            "abstract": "Logical reasoning, i.e., deductively inferring the truth value of a conclusion from a set of premises, is an important task for artificial intelligence with wide potential impacts on science, mathematics, and society. While many prompting-based strategies have been proposed to enable Large Language Models (LLMs) to do such reasoning more effectively, they still appear unsatisfactory, often failing in subtle and unpredictable ways. In this work, we investigate the validity of instead reformulating such tasks as modular neurosymbolic programming, which we call LINC: Logical Inference via Neurosymbolic Computation. In LINC, the LLM acts as a semantic parser, translating premises and conclusions from natural language to expressions in first-order logic. These expressions are then offloaded to an external theorem prover, which symbolically performs deductive inference. Leveraging this approach, we observe significant performance gains on FOLIO and a balanced subset of ProofWriter for three different models in nearly all experimental conditions we evaluate. On ProofWriter, augmenting the comparatively small open-source StarCoder+ (15.5B parameters) with LINC even outperforms GPT-3.5 and GPT-4 with Chain-of-Thought (CoT) prompting by an absolute 38% and 10%, respectively. When used with GPT-4, LINC scores 26% higher than CoT on ProofWriter while performing comparatively on FOLIO. Further analysis reveals that although both methods on average succeed roughly equally often on this dataset, they exhibit distinct and complementary failure modes. We thus provide promising evidence for how logical reasoning over natural language can be tackled through jointly leveraging LLMs alongside symbolic provers. All corresponding code is publicly available at https://github.com/benlipkin/linc",
            "year": 2023,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Analysis reveals that although both methods on average succeed roughly equally often on this dataset, they exhibit distinct and complementary failure modes, which provides promising evidence for how logical reasoning over natural language can be tackled through jointly leveraging LLMs alongside symbolic provers."
            },
            "score": 4
        },
        {
            "id": "44772fe1c3fa422a3da7e25092db2544893d6bfb",
            "paperId": "44772fe1c3fa422a3da7e25092db2544893d6bfb",
            "title": "Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming",
            "abstract": "Pre-trained large language models (LMs) struggle to perform logical reasoning reliably despite advances in scale and compositionality. In this work, we tackle this challenge through the lens of symbolic programming. We propose DSR-LM, a Differentiable Symbolic Reasoning framework where pre-trained LMs govern the perception of factual knowledge, and a symbolic module performs deductive reasoning. In contrast to works that rely on hand-crafted logic rules, our differentiable symbolic reasoning framework efficiently learns weighted rules and applies semantic loss to further improve LMs. DSR-LM is scalable, interpretable, and allows easy integration of prior knowledge, thereby supporting extensive symbolic programming to robustly derive a logical conclusion. The results of our experiments suggest that DSR-LM improves the logical reasoning abilities of pre-trained language models, resulting in a significant increase in accuracy of over 20% on deductive reasoning benchmarks. Furthermore, DSR-LM outperforms a variety of competitive baselines when faced with systematic changes in sequence length.",
            "year": 2023,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "DSR-LM is proposed, a Differentiable Symbolic Reasoning framework where pre-trained LMs govern the perception of factual knowledge, and a symbolic module performs deductive reasoning, and efficiently learns weighted rules and applies semantic loss to further improve LMs."
            },
            "score": 4
        },
        {
            "id": "d48b29889241551e1ee6622fa78c3fa4159255dd",
            "paperId": "d48b29889241551e1ee6622fa78c3fa4159255dd",
            "title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning",
            "abstract": "Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.",
            "year": 2022,
            "citationCount": 212,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Selection-Inference (SI) framework is proposed that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer."
            },
            "score": 4
        },
        {
            "id": "e43e95f706762b64d4ae17a5eb72731d2bd9047a",
            "paperId": "e43e95f706762b64d4ae17a5eb72731d2bd9047a",
            "title": "Learning To Teach Large Language Models Logical Reasoning",
            "abstract": "Large language models (LLMs) have gained enormous attention from both academia and industry, due to their exceptional ability in language generation and extremely powerful generalization. However, current LLMs still output unreliable content in practical reasoning tasks due to their inherent issues (e.g., hallucination). To better disentangle this problem, in this paper, we conduct an in-depth investigation to systematically explore the capability of LLMs in logical reasoning. More in detail, we first investigate the deficiency of LLMs in logical reasoning on different tasks, including event relation extraction and deductive reasoning. Our study demonstrates that LLMs are not good reasoners in solving tasks with rigorous reasoning and will produce counterfactual answers, which require us to iteratively refine. Therefore, we comprehensively explore different strategies to endow LLMs with logical reasoning ability, and thus enable them to generate more logically consistent answers across different scenarios. Based on our approach, we also contribute a synthesized dataset (LLM-LR) involving multi-hop reasoning for evaluation and pre-training. Extensive quantitative and qualitative analyses on different tasks also validate the effectiveness and necessity of teaching LLMs with logic and provide insights for solving practical tasks with LLMs in future work.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study comprehensively explore different strategies to endow LLMs with logical reasoning ability, and thus enable them to generate more logically consistent answers across different scenarios."
            },
            "score": 4
        },
        {
            "id": "6e5daccbab84481909578ae070507f4887b9808e",
            "paperId": "6e5daccbab84481909578ae070507f4887b9808e",
            "title": "Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning",
            "abstract": "Large language models (LLMs), such as LLaMA, Alpaca, Vicuna, GPT-3.5 and GPT-4, have advanced the performance of AI systems on various natural language processing tasks to human-like levels. However, their generalisation and robustness when performing logical reasoning has not been sufficiently assessed. To comprehensively evaluate this ability, we develop three new logical reasoning datasets named\"ReClor-plus\",\"LogiQA-plus\"and\"LogiQAv2-plus\"that extend standard logical reasoning datasets to evaluate the robustness of the LLM's reasoning. For each, we create three subsets: the first with randomly shuffled options, the second with the correct choices replaced by\"none of the other options is correct\", and the third with a combination of shuffling and substitution. Experiments on these datasets show that these simple augmentations greatly hinder the models' performance. Despite their high performance on the original publicly available datasets, we find that all models perform poorly on these newly constructed datasets. We also demonstrate that introducing task variations into the training set can markedly improve the model's performance on both the original and our developed datasets. Finally, we show that applying logic-driven data augmentation for fine-tuning and prompting can enhance generalisation in both discriminative and generative models, offering a path to improving their robustness for tasks involving logical reasoning. Source code and data are made publicly available at https://github.com/Strong-AI-Lab/Logical-and-abstract-reasoning.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that applying logic-driven data augmentation for fine-tuning and prompting can enhance generalisation in both discriminative and generative models, offering a path to improving their robustness for tasks involving logical reasoning."
            },
            "score": 4
        },
        {
            "id": "afee8cdc51e95b50d7574ed1700a797874bf792c",
            "paperId": "afee8cdc51e95b50d7574ed1700a797874bf792c",
            "title": "Adversarial Fine-Tuning of Language Models: An Iterative Optimisation Approach for the Generation and Detection of Problematic Content",
            "abstract": "In this paper, we tackle the emerging challenge of unintended harmful content generation in Large Language Models (LLMs) with a novel dual-stage optimisation technique using adversarial fine-tuning. Our two-pronged approach employs an adversarial model, fine-tuned to generate potentially harmful prompts, and a judge model, iteratively optimised to discern these prompts. In this adversarial cycle, the two models seek to outperform each other in the prompting phase, generating a dataset of rich examples which are then used for fine-tuning. This iterative application of prompting and fine-tuning allows continuous refinement and improved performance. The performance of our approach is evaluated through classification accuracy on a dataset consisting of problematic prompts not detected by GPT-4, as well as a selection of contentious but unproblematic prompts. We show considerable increase in classification accuracy of the judge model on this challenging dataset as it undergoes the optimisation process. Furthermore, we show that a rudimentary model \\texttt{ada} can achieve 13\\% higher accuracy on the hold-out test set than GPT-4 after only a few rounds of this process, and that this fine-tuning improves performance in parallel tasks such as toxic comment identification.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper shows that a rudimentary model can achieve 13\\% higher accuracy on the hold-out test set than GPT-4 after only a few rounds of this process, and that this fine-tuning improves performance in parallel tasks such as toxic comment identification."
            },
            "score": 4
        },
        {
            "id": "e69b41e53ce897a576ab80a6e9d59a253cbd6c62",
            "paperId": "e69b41e53ce897a576ab80a6e9d59a253cbd6c62",
            "title": "Iterative Prompt Refinement for Radiation Oncology Symptom Extraction Using Teacher-Student Large Language Models",
            "abstract": "This study introduces a novel teacher-student architecture utilizing Large Language Models (LLMs) to improve prostate cancer radiotherapy symptom extraction from clinical notes. Mixtral, the student model, initially extracts symptoms, followed by GPT-4, the teacher model, which refines prompts based on Mixtral's performance. This iterative process involved 294 single symptom clinical notes across 12 symptoms, with up to 16 rounds of refinement per epoch. Results showed significant improvements in extracting symptoms from both single and multi-symptom notes. For 59 single symptom notes, accuracy increased from 0.51 to 0.71, precision from 0.52 to 0.82, recall from 0.52 to 0.72, and F1 score from 0.49 to 0.73. In 375 multi-symptom notes, accuracy rose from 0.24 to 0.43, precision from 0.6 to 0.76, recall from 0.24 to 0.43, and F1 score from 0.20 to 0.44. These results demonstrate the effectiveness of advanced prompt engineering in LLMs for radiation oncology use.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel teacher-student architecture utilizing Large Language Models (LLMs) to improve prostate cancer radiotherapy symptom extraction from clinical notes is introduced, demonstrating the effectiveness of advanced prompt engineering in LLMs for radiation oncology use."
            },
            "score": 4
        },
        {
            "id": "f197bf0fc2f228483f6af3285000d54d8d97f9eb",
            "paperId": "f197bf0fc2f228483f6af3285000d54d8d97f9eb",
            "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models",
            "abstract": "We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",
            "year": 2023,
            "citationCount": 336,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 4
        },
        {
            "id": "56e7bda25b83228f91962d3465fd587cfe8908e1",
            "paperId": "56e7bda25b83228f91962d3465fd587cfe8908e1",
            "title": "How Far Can We Extract Diverse Perspectives from Large Language Models? Criteria-Based Diversity Prompting!",
            "abstract": "Collecting diverse human opinions is costly and challenging. This leads to a recent trend in collaborative efforts between humans and Large Language Models (LLMs) for generating diverse data, offering potential scalable and efficient solutions. However, the extent of LLMs' capability to generate diverse perspectives on subjective topics remains an unexplored question. In this study, we investigate LLMs' capacity for generating diverse perspectives and rationales on subjective topics, such as social norms and argumentative texts. We formulate a new problem of maximum diversity extraction from LLMs. Motivated by how humans develop their opinions through their values, we propose a criteria-based prompting technique to ground diverse opinions. To see how far we can extract diverse perspectives from LLMs, or called diversity coverage, we employ a step-by-step recall prompting for generating more outputs from the model in an iterative manner. As we apply our methods to various tasks, indeed we find that LLMs can generate diverse opinions according to the degree of task subjectivity",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study investigates LLMs' capacity for generating diverse perspectives and rationales on subjective topics, such as social norms and argumentative texts, and proposes a criteria-based prompting technique to ground diverse opinions."
            },
            "score": 4
        },
        {
            "id": "dfab0f3ee6f47e36cccee145794cd117773e6f73",
            "paperId": "dfab0f3ee6f47e36cccee145794cd117773e6f73",
            "title": "Towards LLM-based Fact Verification on News Claims with a Hierarchical Step-by-Step Prompting Method",
            "abstract": "While large pre-trained language models (LLMs) have shown their impressive capabilities in various NLP tasks, they are still under-explored in the misinformation domain. In this paper, we examine LLMs with in-context learning (ICL) for news claim verification, and find that only with 4-shot demonstration examples, the performance of several prompting methods can be comparable with previous supervised models. To further boost performance, we introduce a Hierarchical Step-by-Step (HiSS) prompting method which directs LLMs to separate a claim into several subclaims and then verify each of them via multiple questions-answering steps progressively. Experiment results on two public misinformation datasets show that HiSS prompting outperforms state-of-the-art fully-supervised approach and strong few-shot ICL-enabled baselines.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Hierarchical Step-by-Step (HiSS) prompting method is introduced which directs LLMs to separate a claim into several subclaims and then verify each of them via multiple questions-answering steps progressively."
            },
            "score": 4
        },
        {
            "id": "185a429c53a6ee00f49d9d043bafc790e310f625",
            "paperId": "185a429c53a6ee00f49d9d043bafc790e310f625",
            "title": "An Empirical Study of Using ChatGPT for Fact Verification Task",
            "abstract": "ChatGPT has recently emerged as a powerful tool for performing diverse NLP tasks. However, ChatGPT has been criticized for generating nonfactual responses, raising concerns about its usability for sensitive tasks like fact verification. This study investigates three key research questions: (1) Can ChatGPT be used for fact verification tasks? (2) What are different prompts performance using ChatGPT for fact verification tasks? (3) For the best-performing prompt, what common mistakes does ChatGPT make? Specifically, this study focuses on conducting a comprehensive and systematic analysis by designing and comparing the performance of three different prompts for fact verification tasks on the benchmark FEVER dataset using ChatGPT.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study focuses on conducting a comprehensive and systematic analysis by designing and comparing the performance of three different prompts for fact verification tasks on the benchmark FEVER dataset using ChatGPT."
            },
            "score": 4
        },
        {
            "id": "cf65db0934696e1f8896da52811b3d7f79836abd",
            "paperId": "cf65db0934696e1f8896da52811b3d7f79836abd",
            "title": "From Instructions to Constraints: Language Model Alignment with Automatic Constraint Verification",
            "abstract": "User alignment is crucial for adapting general-purpose language models (LMs) to downstream tasks, but human annotations are often not available for all types of instructions, especially those with customized constraints. We observe that user instructions typically contain constraints. While assessing response quality in terms of the whole instruction is often costly, efficiently evaluating the satisfaction rate of constraints is feasible. We investigate common constraints in NLP tasks, categorize them into three classes based on the types of their arguments, and propose a unified framework, ACT (Aligning to ConsTraints), to automatically produce supervision signals for user alignment with constraints. Specifically, ACT uses constraint verifiers, which are typically easy to implement in practice, to compute constraint satisfaction rate (CSR) of each response. It samples multiple responses for each prompt and collect preference labels based on their CSR automatically. Subsequently, ACT adapts the LM to the target task through a ranking-based learning process. Experiments on fine-grained entity typing, abstractive summarization, and temporal question answering show that ACT is able to enhance LMs' capability to adhere to different classes of constraints, thereby improving task performance. Further experiments show that the constraint-following capabilities are transferable.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A unified framework, ACT (Aligning to ConsTraints), is proposed, to automatically produce supervision signals for user alignment with constraints, and is able to enhance LMs' capability to adhere to different classes of constraints, thereby improving task performance."
            },
            "score": 4
        },
        {
            "id": "a5cd606537a666e4807f1c9d342cfeb2b71c4a34",
            "paperId": "a5cd606537a666e4807f1c9d342cfeb2b71c4a34",
            "title": "Are Large Language Models Table-based Fact-Checkers?",
            "abstract": "Table-based Fact Verification (TFV) aims to extract the entailment relation between statements and structured tables. Existing TFV methods based on small-scaled models suffer from insufficient labeled data and weak zero-shot ability. Recently, the appearance of Large Language Models (LLMs) has gained lots of attraction in research fields. They have shown powerful zero-shot and in-context learning abilities on several NLP tasks, but their potential on TFV is still unknown. In this work, we implement a preliminary study about whether LLMs are table-based fact-checkers. In detail, we design diverse prompts to explore how the in-context learning can help LLMs in TFV, i.e., zero-shot and few-shot TFV capability. Besides, we carefully design and construct TFV instructions to study the performance gain brought by the instruction tuning of LLMs. Experimental results demonstrate that LLMs can achieve acceptable results on zero-shot and few-shot TFV with prompt engineering, while instruction-tuning can stimulate the TFV capability significantly. We also make some valuable findings about the format of zero-shot prompts and the number of in-context examples. Finally, we analyze some possible directions to promote the accuracy of TFV via LLMs, which is beneficial to further research of table reasoning.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work implements a preliminary study about whether LLMs are table-based fact-checkers, and designs diverse prompts to explore how the in-context learning can help LLMs in TFV, i.e., zero-shot and few-shot TFV capability."
            },
            "score": 4
        },
        {
            "id": "45cf036cfd96581f34a4f5287c24b9948c79d44a",
            "paperId": "45cf036cfd96581f34a4f5287c24b9948c79d44a",
            "title": "Who Wrote it and Why? Prompting Large-Language Models for Authorship Verification",
            "abstract": "Authorship verification (AV) is a fundamental task in natural language processing (NLP) and computational linguistics, with applications in forensic analysis, plagiarism detection, and identification of deceptive content. Existing AV techniques, including traditional stylometric and deep learning approaches, face limitations in terms of data requirements and lack of explainability. To address these limitations, this paper proposes PromptAV, a novel technique that leverages Large-Language Models (LLMs) for AV by providing step-by-step stylometric explanation prompts. PromptAV outperforms state-of-the-art baselines, operates effectively with limited training data, and enhances interpretability through intuitive explanations, showcasing its potential as an effective and interpretable solution for the AV task.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 3
        },
        {
            "id": "d75387fcf6a839f2aa8af5778aa6931eea5ec969",
            "paperId": "d75387fcf6a839f2aa8af5778aa6931eea5ec969",
            "title": "Explainable Claim Verification via Knowledge-Grounded Reasoning with Large Language Models",
            "abstract": "Claim verification plays a crucial role in combating misinformation. While existing works on claim verification have shown promising results, a crucial piece of the puzzle that remains unsolved is to understand how to verify claims without relying on human-annotated data, which is expensive to create at a large scale. Additionally, it is important for models to provide comprehensive explanations that can justify their decisions and assist human fact-checkers. This paper presents First-Order-Logic-Guided Knowledge-Grounded (FOLK) Reasoning that can verify complex claims and generate explanations without the need for annotated evidence using Large Language Models (LLMs). FOLK leverages the in-context learning ability of LLMs to translate the claim into a First-Order-Logic (FOL) clause consisting of predicates, each corresponding to a sub-claim that needs to be verified. Then, FOLK performs FOL-Guided reasoning over a set of knowledge-grounded question-and-answer pairs to make veracity predictions and generate explanations to justify its decision-making process. This process makes our model highly explanatory, providing clear explanations of its reasoning process in human-readable form. Our experiment results indicate that FOLK outperforms strong baselines on three datasets encompassing various claim verification challenges. Our code and data are available.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents First-Order-Logic-Guided Knowledge-Grounded Reasoning that can verify complex claims and generate explanations without the need for annotated evidence using Large Language Models (LLMs)."
            },
            "score": 3
        },
        {
            "id": "a1675f47125aa409525c5f759b5e6bcc1c8831aa",
            "paperId": "a1675f47125aa409525c5f759b5e6bcc1c8831aa",
            "title": "Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy",
            "abstract": "Large language models are powerful text processors and reasoners, but are still subject to limitations including outdated knowledge and hallucinations, which necessitates connecting them to the world. Retrieval-augmented large language models have raised extensive attention for grounding model generation on external knowledge. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work has proposed to improve relevance modeling by having large language models actively involved in retrieval, i.e., to improve retrieval with generation. In this paper, we show that strong performance can be achieved by a method we call Iter-RetGen, which synergizes retrieval and generation in an iterative manner. A model output shows what might be needed to finish a task, and thus provides an informative context for retrieving more relevant knowledge which in turn helps generate a better output in the next iteration. Compared with recent work which interleaves retrieval with generation when producing an output, Iter-RetGen processes all retrieved knowledge as a whole and largely preserves the flexibility in generation without structural constraints. We evaluate Iter-RetGen on multi-hop question answering, fact verification, and commonsense reasoning, and show that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval and generation. We can further improve performance via generation-augmented retrieval adaptation.",
            "year": 2023,
            "citationCount": 42,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper shows that strong performance can be achieved by a method, called Iter-RetGen, which synergizes retrieval and generation in an iterative manner, and shows that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval andgeneration."
            },
            "score": 3
        },
        {
            "id": "acf90b4d165690fe27c62c4af1a28d540c784000",
            "paperId": "acf90b4d165690fe27c62c4af1a28d540c784000",
            "title": "Automatic Evaluation of Attribution by Large Language Models",
            "abstract": "A recent focus of large language model (LLM) development, as exemplified by generative search engines, is to incorporate external references to generate and support its claims. However, evaluating the attribution, i.e., verifying whether the generated statement is fully supported by the cited reference, remains an open problem. Although human evaluation is common practice, it is costly and time-consuming. In this paper, we investigate the automatic evaluation of attribution given by LLMs. We begin by defining different types of attribution errors, and then explore two approaches for automatic evaluation: prompting LLMs and fine-tuning smaller LMs. The fine-tuning data is repurposed from related tasks such as question answering, fact-checking, natural language inference, and summarization. We manually curate a set of test examples covering 12 domains from a generative search engine, New Bing. Our results on this curated test set and simulated examples from existing benchmarks highlight both promising signals and challenges. We hope our problem formulation, testbeds, and findings will help lay the foundation for future studies on this important problem.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper investigates the automatic evaluation of attribution given by large language model (LLMs), defining different types of attribution errors, and exploring two approaches for automatic evaluation: prompting LLMs and fine-tuning smaller LMs."
            },
            "score": 3
        },
        {
            "id": "ba4aa83248a1d08b521392eb971e47d10b7c74e1",
            "paperId": "ba4aa83248a1d08b521392eb971e47d10b7c74e1",
            "title": "Boosting Logical Reasoning in Large Language Models through a New Framework: The Graph of Thought",
            "abstract": "Recent advancements in large-scale models, such as GPT-4, have showcased remarkable capabilities in addressing standard queries. However, when facing complex problems that require multi-step logical reasoning, their accuracy dramatically decreases. Current research has explored the realm of \\textit{prompting engineering} to bolster the inferential capacities of these models. Our paper unveils a pioneering prompting technique, dubbed \\textit{Graph of Thoughts (GoT)}. Through testing on a trio of escalating challenges: the 24-point game, resolution of high-degree polynomial equations, and derivation of formulas for recursive sequences, our method outperformed GPT-4, achieving accuracy improvements of $89.7\\%$, $86\\%$, and $56\\%$ for each respective task. Moreover, when juxtaposed with the state-of-the-art (SOTA) prompting method, \\textit{Tree of Thought (ToT)}, our approach registered an average accuracy boost of $23\\%$, $24\\%$, and $15\\%$.",
            "year": 2023,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper unveils a pioneering prompting technique, dubbed \\textit{Graph of Thoughts (GoT), which outperformed GPT-4 and when juxtaposed with the state-of-the-art (SOTA) prompting method, the Tree of Thought (ToT), registered an average accuracy boost."
            },
            "score": 3
        },
        {
            "id": "75577678786101592187261719988c26478a2060",
            "paperId": "75577678786101592187261719988c26478a2060",
            "title": "Adaptive Pre-training of Language Models for Better Logical Reasoning",
            "abstract": "Logical reasoning of text is an important ability that requires understanding the 1 logical information present in the text and reasoning through them to infer new 2 conclusions. Prior works on improving the logical reasoning ability of language 3 models require complex processing of training data (e.g., aligning symbolic knowl-4 edge to text), yielding task-specific data augmentation solutions that restrict the 5 learning of general logical reasoning skills. In this work, we propose AERIE, 6 an adaptively pre-trained language model that has improved logical reasoning 7 abilities. We select a subset of Wikipedia, based on a set of logical inference key-8 words, for continued pretraining of a language model. We use two self-supervised 9 loss functions: a modified masked language modeling loss where only specific 10 parts-of-speech words, that would likely require more reasoning than basic lan-11",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes AERIE, an adaptively pre-trained language model that has improved logical reasoning abilities and selects a subset of Wikipedia, based on a set of logical inference key-8 words, for continued pretraining of a language model."
            },
            "score": 3
        },
        {
            "id": "67590dc371a89bef960b7bd547110f43cbe7196e",
            "paperId": "67590dc371a89bef960b7bd547110f43cbe7196e",
            "title": "APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning",
            "abstract": "Logical reasoning over text is an important ability that requires understanding the semantics of the text and reasoning through them to arrive at correct inferences. Prior works on pretraining language models to improve the logical reasoning ability require complex processing of training data (e.g., aligning symbolic knowledge to text), yielding task-specific data augmentation that is not easy to adapt to any general text corpus. In this work, we propose APOLLO, a simple adaptive pretraining approach to improve the logical reasoning skills of language models. We select a subset of Wikipedia for adaptive pretraining using a set of logical inference keywords as filter words. Further, we propose two self-supervised loss functions for training. First, we modify the masked language modeling loss only to mask specific parts-of-speech words that likely require higher-order reasoning to predict them. Second, we propose a sentence-level classification loss that teaches the model to distinguish between entailment and contradiction types of sentences. The proposed pretraining paradigm is both simple and independent of task formats. We demonstrate the effectiveness of APOLLO by comparing it with prior baselines on two logical reasoning datasets. APOLLO performs comparably on ReClor and outperforms baselines on LogiQA.",
            "year": 2022,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes APOLLO, a simple adaptive Pretraining approach to improve the logical reasoning skills of language models using a subset of Wikipedia for adaptive pretraining using a set of logical inference keywords as filter words."
            },
            "score": 3
        },
        {
            "id": "5f9f6b462759b56c242459b7e976b8858b141eeb",
            "paperId": "5f9f6b462759b56c242459b7e976b8858b141eeb",
            "title": "Complex Logical Reasoning over Knowledge Graphs using Large Language Models",
            "abstract": "Reasoning over knowledge graphs (KGs) is a challenging task that requires a deep understanding of the complex relationships between entities and the underlying logic of their relations. Current approaches rely on learning geometries to embed entities in vector space for logical query operations, but they suffer from subpar performance on complex queries and dataset-specific representations. In this paper, we propose a novel decoupled approach, Language-guided Abstract Reasoning over Knowledge graphs (LARK), that formulates complex KG reasoning as a combination of contextual KG search and logical query reasoning, to leverage the strengths of graph extraction algorithms and large language models (LLM), respectively. Our experiments demonstrate that the proposed approach outperforms state-of-the-art KG reasoning methods on standard benchmark datasets across several logical query constructs, with significant performance gain for queries of higher complexity. Furthermore, we show that the performance of our approach improves proportionally to the increase in size of the underlying LLM, enabling the integration of the latest advancements in LLMs for logical reasoning over KGs. Our work presents a new direction for addressing the challenges of complex KG reasoning and paves the way for future research in this area.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel decoupled approach, Language-guided Abstract Reasoning over Knowledge graphs (LARK), that formulates complex KG reasoning as a combination of contextual KG search and logical query reasoning, to leverage the strengths of graph extraction algorithms and large language models (LLM), respectively."
            },
            "score": 3
        },
        {
            "id": "cac951523d6b7d1bbd014bd8b5e87afd9ecdd318",
            "paperId": "cac951523d6b7d1bbd014bd8b5e87afd9ecdd318",
            "title": "Enhancing Logical Reasoning in Large Language Models to Facilitate Legal Applications",
            "abstract": "Language serves as a vehicle for conveying thought, enabling communication among individuals. The ability to distinguish between diverse concepts, identify fairness and injustice, and comprehend a range of legal notions fundamentally relies on logical reasoning. Large Language Models (LLMs) attempt to emulate human language understanding and generation, but their competency in logical reasoning remains limited. This paper seeks to address the philosophical question: How can we effectively teach logical reasoning to LLMs while maintaining a deep understanding of the intricate relationship between language and logic? By focusing on bolstering LLMs' capabilities in logical reasoning, we aim to expand their applicability in law and other logic-intensive disciplines. To this end, we propose a Reinforcement Learning from Logical Feedback (RLLF) approach, which serves as a potential framework for refining LLMs' reasoning capacities. Through RLLF and a revised evaluation methodology, we explore new avenues for research in this domain and contribute to the development of LLMs capable of handling complex legal reasoning tasks while acknowledging the fundamental connection between language and logic.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Reinforcement Learning from Logical Feedback (RLLF) approach is proposed, which serves as a potential framework for refining LLMs' reasoning capacities and contributes to the development of LLMs capable of handling complex legal reasoning tasks while acknowledging the fundamental connection between language and logic."
            },
            "score": 3
        },
        {
            "id": "bb2ef694e8b5a99e1f7ceb014968b4d1dc2e122a",
            "paperId": "bb2ef694e8b5a99e1f7ceb014968b4d1dc2e122a",
            "title": "EIPE-text: Evaluation-Guided Iterative Plan Extraction for Long-Form Narrative Text Generation",
            "abstract": "Plan-and-Write is a common hierarchical approach in long-form narrative text generation, which first creates a plan to guide the narrative writing. Following this approach, several studies rely on simply prompting large language models for planning, which often yields suboptimal results. In this paper, we propose a new framework called Evaluation-guided Iterative Plan Extraction for long-form narrative text generation (EIPE-text), which extracts plans from the corpus of narratives and utilizes the extracted plans to construct a better planner. EIPE-text has three stages: plan extraction, learning, and inference. In the plan extraction stage, it iteratively extracts and improves plans from the narrative corpus and constructs a plan corpus. We propose a question answer (QA) based evaluation mechanism to automatically evaluate the plans and generate detailed plan refinement instructions to guide the iterative improvement. In the learning stage, we build a better planner by fine-tuning with the plan corpus or in-context learning with examples in the plan corpus. Finally, we leverage a hierarchical approach to generate long-form narratives. We evaluate the effectiveness of EIPE-text in the domains of novels and storytelling. Both GPT-4-based evaluations and human evaluations demonstrate that our method can generate more coherent and relevant long-form narratives. Our code will be released in the future.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new framework called Evaluation-guided Iterative Plan Extraction for long-form narrative text generation (EIPE-text), which extracts plans from the corpus of narratives and utilizes the extracted plans to construct a better planner."
            },
            "score": 3
        },
        {
            "id": "bf4810017b54e50354cccffd8966121c7166cb17",
            "paperId": "bf4810017b54e50354cccffd8966121c7166cb17",
            "title": "Iterative Translation Refinement with Large Language Models",
            "abstract": "Large language models have shown surprising performances in understanding instructions and performing natural language tasks. In this paper, we propose iterative translation refinement to leverage the power of large language models for more natural translation and post-editing. We show that by simply involving a large language model in an iterative process, the output quality improves beyond mere translation. Extensive test scenarios with GPT-3.5 reveal that although iterations reduce string-based metric scores, neural metrics indicate comparable if not improved translation quality. Further, human evaluations demonstrate that our method effectively reduces translationese compared to initial GPT translations and even human references, especially for into-English directions. Ablation studies underscore the importance of anchoring the refinement process to the source input and a reasonable initial translation.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that by simply involving a large language model in an iterative process, the output quality improves beyond mere translation, and that although iterations reduce string-based metric scores, neural metrics indicate comparable if not improved translation quality."
            },
            "score": 3
        },
        {
            "id": "4637f79ddfaf923ce569996ffa5b6cda1996faa1",
            "paperId": "4637f79ddfaf923ce569996ffa5b6cda1996faa1",
            "title": "Jailbreaking Black Box Large Language Models in Twenty Queries",
            "abstract": "There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR -- which is inspired by social engineering attacks -- uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and PaLM-2.",
            "year": 2023,
            "citationCount": 119,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "PAIR is an algorithm that generates semantic jailbreaks with only black-box access to an LLM with competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and PaLM."
            },
            "score": 3
        },
        {
            "id": "b73ea8adfef3cdc63167e3a7f1f2be3f4ef4219d",
            "paperId": "b73ea8adfef3cdc63167e3a7f1f2be3f4ef4219d",
            "title": "Knowledge Refinement via Interaction Between Search Engines and Large Language Models",
            "abstract": "Information retrieval (IR) plays a crucial role in locating relevant resources from vast amounts of data, and its applications have evolved from traditional knowledge bases to modern search engines (SEs). The emergence of large language models (LLMs) has further revolutionized the IR field by enabling users to interact with search systems in natural language. In this paper, we explore the advantages and disadvantages of LLMs and SEs, highlighting their respective strengths in understanding user-issued queries and retrieving up-to-date information. To leverage the benefits of both paradigms while circumventing their limitations, we propose InteR, a novel framework that facilitates knowledge refinement through interaction between SEs and LLMs. InteR allows SEs to expand knowledge in queries using LLM-generated knowledge collections and enables LLMs to enhance prompt formulation using SE-retrieved documents. This iterative refinement process augments the inputs of SEs and LLMs, leading to more accurate retrieval. Experiments on large-scale retrieval benchmarks involving web search and low-resource retrieval tasks demonstrate that InteR achieves overall superior zero-shot retrieval performance compared to state-of-the-art methods, even those using relevance judgment. Source code is available at https://github.com/Cyril-JZ/InteR.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "InteR is proposed, a novel framework that facilitates knowledge refinement through interaction between SEs and LLMs, which allows SEs to expand knowledge in queries using LLM-generated knowledge collections and enables LLMs to enhance prompt formulation using SE-retrieved documents."
            },
            "score": 3
        },
        {
            "id": "7c9f69848d28e0a7cbb00942ee83dab9773c23e4",
            "paperId": "7c9f69848d28e0a7cbb00942ee83dab9773c23e4",
            "title": "GPT-NER: Named Entity Recognition via Large Language Models",
            "abstract": "Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model. In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text\"Columbus is a city\"is transformed to generate the text sequence\"@@Columbus## is a city\", where special tokens @@## marks the entity to extract. To efficiently address the\"hallucination\"issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a self-verification strategy by prompting LLMs to ask itself whether the extracted entities belong to a labeled entity tag. We conduct experiments on five widely adopted NER datasets, and GPT-NER achieves comparable performances to fully supervised baselines, which is the first time as far as we are concerned. More importantly, we find that GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, GPT-NER performs significantly better than supervised models. This demonstrates the capabilities of GPT-NER in real-world NER applications where the number of labeled examples is limited.",
            "year": 2023,
            "citationCount": 58,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, and performs significantly better than supervised models, which demonstrates the capabilities of GPT-ner in real-world NER applications where the number of labeled examples is limited."
            },
            "score": 3
        },
        {
            "id": "dd7a2613ee15cd10938e7ad40f3aa052e98fda3c",
            "paperId": "dd7a2613ee15cd10938e7ad40f3aa052e98fda3c",
            "title": "GROVE: A Retrieval-augmented Complex Story Generation Framework with A Forest of Evidence",
            "abstract": "Conditional story generation is significant in human-machine interaction, particularly in producing stories with complex plots. While Large language models (LLMs) perform well on multiple NLP tasks, including story generation, it is challenging to generate stories with both complex and creative plots. Existing methods often rely on detailed prompts to guide LLMs to meet target conditions, which inadvertently restrict the creative potential of the generated stories. We argue that leveraging information from exemplary human-written stories facilitates generating more diverse plotlines. Delving deeper into story details helps build complex and credible plots. In this paper, we propose a retrieval-au\\textbf{G}mented sto\\textbf{R}y generation framework with a f\\textbf{O}rest of e\\textbf{V}id\\textbf{E}nce (GROVE) to enhance stories' complexity. We build a retrieval repository for target conditions to produce few-shot examples to prompt LLMs. Additionally, we design an ``asking-why'' prompting scheme that extracts a forest of evidence, providing compensation for the ambiguities that may occur in the generated story. This iterative process uncovers underlying story backgrounds. Finally, we select the most fitting chains of evidence from the evidence forest and integrate them into the generated story, thereby enhancing the narrative's complexity and credibility. Experimental results and numerous examples verify the effectiveness of our method.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is argued that leveraging information from exemplary human-written stories facilitates generating more diverse plotlines, and a retrieval-au-Gmented sto-R-y generation framework with arest of GROVE to enhance stories' complexity and credibility is proposed."
            },
            "score": 3
        },
        {
            "id": "a8a71f9b10b281e796fdc2ee7aaec40067739574",
            "paperId": "a8a71f9b10b281e796fdc2ee7aaec40067739574",
            "title": "STEPS: A Benchmark for Order Reasoning in Sequential Tasks",
            "abstract": "Various human activities can be abstracted into a sequence of actions in natural text, i.e. cooking, repairing, manufacturing, etc. Such action sequences heavily depend on the executing order, while disorder in action sequences leads to failure of further task execution by robots or AI agents. Therefore, to verify the order reasoning capability of current neural models in sequential tasks, we propose a challenging benchmark , named STEPS. STEPS involves two subtask settings, focusing on determining the rationality of given next step in recipes and selecting the reasonable step from the multi-choice question, respectively. We describe the data construction and task formulations, and benchmark most of significant Large Language Models (LLMs). The experimental results demonstrate 1) The commonsense reasoning of action orders in sequential tasks are challenging to resolve via zero-shot prompting or few-shot in-context learning for LLMs; 2) Prompting method still significantly lags behind tuning-based method on STEPS.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results demonstrate that the commonsense reasoning of action orders in sequential tasks are challenging to resolve via zero-shot prompting or few-shot in-context learning for LLMs, and Prompting method still significantly lags behind tuning-based method on STEPS."
            },
            "score": 3
        },
        {
            "id": "af444ae0ba12c6506a02118cf4036a9262c4aa3c",
            "paperId": "af444ae0ba12c6506a02118cf4036a9262c4aa3c",
            "title": "Explanation Regeneration via Information Bottleneck",
            "abstract": "Explaining the black-box predictions of NLP models naturally and accurately is an important open problem in natural language generation. These free-text explanations are expected to contain sufficient and carefully-selected evidence to form supportive arguments for predictions. Due to the superior generative capacity of large pretrained language models, recent work built on prompt engineering enables explanation generation without specific training. However, explanation generated through single-pass prompting often lacks sufficiency and conciseness. To address this problem, we develop an information bottleneck method EIB to produce refined explanations that are sufficient and concise. Our approach regenerates the free-text explanation by polishing the single-pass output from the pretrained language model but retaining the information that supports the contents being explained. Experiments on two out-of-domain tasks verify the effectiveness of EIB through automatic evaluation and thoroughly-conducted human evaluation.",
            "year": 2022,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work develops an information bottleneck method EIB that regenerates the free-text explanation by polishing the single-pass output from the pretrained language model but retaining the information that supports the contents being explained."
            },
            "score": 3
        },
        {
            "id": "36cd3d3a9e8a64f322476153513ece1fd617acfc",
            "paperId": "36cd3d3a9e8a64f322476153513ece1fd617acfc",
            "title": "nl2spec: Interactively Translating Unstructured Natural Language to Temporal Logics with Large Language Models",
            "abstract": "A rigorous formalization of desired system requirements is indispensable when performing any verification task. This often limits the application of verification techniques, as writing formal specifications is an error-prone and time-consuming manual task. To facilitate this, we present nl2spec, a framework for applying Large Language Models (LLMs) to derive formal specifications (in temporal logics) from unstructured natural language. In particular, we introduce a new methodology to detect and resolve the inherent ambiguity of system requirements in natural language: we utilize LLMs to map subformulas of the formalization back to the corresponding natural language fragments of the input. Users iteratively add, delete, and edit these sub-translations to amend erroneous formalizations, which is easier than manually redrafting the entire formalization. The framework is agnostic to specific application domains and can be extended to similar specification languages and new neural models. We perform a user study to obtain a challenging dataset, which we use to run experiments on the quality of translations. We provide an open-source implementation, including a web-based frontend.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 2
        },
        {
            "id": "974f0e1a85c1ece2555718342ff2abb6bcb6a825",
            "paperId": "974f0e1a85c1ece2555718342ff2abb6bcb6a825",
            "title": "KG-GPT: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models",
            "abstract": "While large language models (LLMs) have made considerable advancements in understanding and generating unstructured text, their application in structured data remains underexplored. Particularly, using LLMs for complex reasoning tasks on knowledge graphs (KGs) remains largely untouched. To address this, we propose KG-GPT, a multi-purpose framework leveraging LLMs for tasks employing KGs. KG-GPT comprises three steps: Sentence Segmentation, Graph Retrieval, and Inference, each aimed at partitioning sentences, retrieving relevant graph components, and deriving logical conclusions, respectively. We evaluate KG-GPT using KG-based fact verification and KGQA benchmarks, with the model showing competitive and robust performance, even outperforming several fully-supervised models. Our work, therefore, marks a significant step in unifying structured and unstructured data processing within the realm of LLMs.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "KG-GPT is proposed, a multi-purpose framework leveraging LLMs for complex reasoning tasks on knowledge graphs, with the model showing competitive and robust performance, even outperforming several fully-supervised models."
            },
            "score": 2
        },
        {
            "id": "9c217c50addaee50e91e3481be8778bf62d71df0",
            "paperId": "9c217c50addaee50e91e3481be8778bf62d71df0",
            "title": "HoneyBee: Progressive Instruction Finetuning of Large Language Models for Materials Science",
            "abstract": "We propose an instruction-based process for trustworthy data curation in materials science (MatSci-Instruct), which we then apply to finetune a LLaMa-based language model targeted for materials science (HoneyBee). MatSci-Instruct helps alleviate the scarcity of relevant, high-quality materials science textual data available in the open literature, and HoneyBee is the first billion-parameter language model specialized to materials science. In MatSci-Instruct we improve the trustworthiness of generated data by prompting multiple commercially available large language models for generation with an Instructor module (e.g. Chat-GPT) and verification from an independent Verifier module (e.g. Claude). Using MatSci-Instruct, we construct a dataset of multiple tasks and measure the quality of our dataset along multiple dimensions, including accuracy against known facts, relevance to materials science, as well as completeness and reasonableness of the data. Moreover, we iteratively generate more targeted instructions and instruction-data in a finetuning-evaluation-feedback loop leading to progressively better performance for our finetuned HoneyBee models. Our evaluation on the MatSci-NLP benchmark shows HoneyBee's outperformance of existing language models on materials science tasks and iterative improvement in successive stages of instruction-data refinement. We study the quality of HoneyBee's language modeling through automatic evaluation and analyze case studies to further understand the model's capabilities and limitations. Our code and relevant datasets are publicly available at \\url{https://github.com/BangLab-UdeM-Mila/NLP4MatSci-HoneyBee}.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An instruction-based process for trustworthy data curation in materials science (MatSci-Instruct), which is then applied to finetune a LLaMa-based language model targeted for materialsScience (HoneyBee), which shows HoneyBee's outperformance of existing language models on materials science tasks and iterative improvement in successive stages of instruction-data refinement."
            },
            "score": 2
        },
        {
            "id": "e0384ba36555232c587d4a80d527895a095a9001",
            "paperId": "e0384ba36555232c587d4a80d527895a095a9001",
            "title": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
            "abstract": "Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the Hallucination Evaluation benchmark for Large Language Models (HaluEval), a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing hallucination. To generate these samples, we propose a ChatGPT-based two-step framework, i.e., sampling-then-filtering. Besides, we also hire some human labelers to annotate the hallucinations in ChatGPT responses. The empirical results suggest that ChatGPT is likely to generate hallucinated content in specific topics by fabricating unverifiable information (i.e., about $19.5\\%$ responses). Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts. However, our experiments also prove that providing external knowledge or adding reasoning steps can help LLMs recognize hallucinations. Our benchmark can be accessed at https://github.com/RUCAIBox/HaluEval.",
            "year": 2023,
            "citationCount": 57,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Hallucination Evaluation benchmark for Large Language Models (HaluEval), a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing hallucination, is introduced and it is proved that providing external knowledge or adding reasoning steps can help LLMs recognize hallucinations."
            },
            "score": 2
        },
        {
            "id": "99267305914a44ad6d626b7a6406fd0079b5508d",
            "paperId": "99267305914a44ad6d626b7a6406fd0079b5508d",
            "title": "Incorporating Medical Knowledge to Transformer-based Language Models for Medical Dialogue Generation",
            "abstract": "Medical dialogue systems have the potential to assist doctors in expanding access to medical care, improving the quality of patient experiences, and lowering medical expenses. The computational methods are still in their early stages and are not ready for widespread application despite their great potential. Existing transformer-based language models have shown promising results but lack domain-specific knowledge. However, to diagnose like doctors, an automatic medical diagnosis necessitates more stringent requirements for the rationality of the dialogue in the context of relevant knowledge. In this study, we propose a new method that addresses the challenges of medical dialogue generation by incorporating medical knowledge into transformer-based language models. We present a method that leverages an external medical knowledge graph and injects triples as domain knowledge into the utterances. Automatic and human evaluation on a publicly available dataset demonstrates that incorporating medical knowledge outperforms several state-of-the-art baseline methods.",
            "year": 2022,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study presents a new method that addresses the challenges of medical dialogue generation by incorporating medical knowledge into transformer-based language models that leverages an external medical knowledge graph and injects triples as domain knowledge into the utterances."
            },
            "score": 2
        },
        {
            "id": "bb2049d437dbeaead770f4c36c4e541bc44a654f",
            "paperId": "bb2049d437dbeaead770f4c36c4e541bc44a654f",
            "title": "Process Modeling With Large Language Models",
            "abstract": "In the realm of Business Process Management (BPM), process modeling plays a crucial role in translating complex process dynamics into comprehensible visual representations, facilitating the understanding, analysis, improvement, and automation of organizational processes. Traditional process modeling methods often require extensive expertise and can be time-consuming. This paper explores the integration of Large Language Models (LLMs) into process modeling to enhance the accessibility of process modeling, offering a more intuitive entry point for non-experts while augmenting the efficiency of experts. We propose a framework that leverages LLMs for the automated generation and iterative refinement of process models starting from textual descriptions. Our framework involves innovative prompting strategies for effective LLM utilization, along with a secure model generation protocol and an error-handling mechanism. Moreover, we instantiate a concrete system extending our framework. This system provides robust quality guarantees on the models generated and supports exporting them in standard modeling notations, such as the Business Process Modeling Notation (BPMN) and Petri nets. Preliminary results demonstrate the framework's ability to streamline process modeling tasks, underscoring the transformative potential of generative AI in the BPM field.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a framework that leverages LLMs for the automated generation and iterative refinement of process models starting from textual descriptions, and instantiates a concrete system extending this framework that provides robust quality guarantees on the models generated and supports exporting them in standard modeling notations."
            },
            "score": 2
        },
        {
            "id": "e90c2f1b74d2108cafbdfb5d287d771bf2d6e5bd",
            "paperId": "e90c2f1b74d2108cafbdfb5d287d771bf2d6e5bd",
            "title": "Toward Auto-Modeling of Formal Verification for NextG Protocols: A Multimodal Cross- and Self-Attention Large Language Model Approach",
            "abstract": "This paper introduces Auto-modeling of Formal Verification with Real-world Prompting for 5G and NextG protocols (AVRE), a novel system designed for the formal verification of Next Generation (NextG) communication protocols, addressing the increasing complexity and scalability challenges in network protocol design and verification. Utilizing Large Language Models (LLMs), AVRE transforms protocol descriptions into dependency graphs and formal models, efficiently resolving ambiguities and capturing design intent. The system integrates a transformer model with LLMs to autonomously establish quantifiable dependency relationships through cross- and self-attention mechanisms. Enhanced by iterative feedback from the HyFuzz experimental platform, AVRE significantly advances the accuracy and relevance of formal verification in complex communication protocols, offering a groundbreaking approach to validating sophisticated communication systems. We compare CAL\u2019s performance with state-of-the-art LLM-based models and traditional time sequence models, demonstrating its superiority in accuracy and robustness, achieving an accuracy of 95.94% and an AUC of 0.98. This NLP-based approach enables, for the first time, the creation of exploits directly from design documents, making remarkable progress in scalable system verification and validation.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This NLP-based approach enables, for the first time, the creation of exploits directly from design documents, making remarkable progress in scalable system verification and validation."
            },
            "score": 2
        },
        {
            "id": "e476b207fffb2b15f0adc39f8f8c1c8643a63311",
            "paperId": "e476b207fffb2b15f0adc39f8f8c1c8643a63311",
            "title": "PromptCARE: Prompt Copyright Protection by Watermark Injection and Verification",
            "abstract": "Large language models (LLMs) have witnessed a meteoric rise in popularity among the general public users over the past few months, facilitating diverse downstream tasks with human-level accuracy and proficiency. Prompts play an essential role in this success, which efficiently adapt pre-trained LLMs to task-specific applications by simply prepending a sequence of tokens to the query texts. However, designing and selecting an optimal prompt can be both expensive and demanding, leading to the emergence of Prompt-as-a-Service providers who profit by providing well-designed prompts for authorized use. With the growing popularity of prompts and their indispensable role in LLM-based services, there is an urgent need to protect the copyright of prompts against unauthorized use. In this paper, we propose PromptCARE, the first framework for prompt copyright protection through watermark injection and verification. Prompt watermarking presents unique challenges that render existing watermarking techniques developed for model and dataset copyright verification ineffective. PromptCARE overcomes these hurdles by proposing watermark injection and verification schemes tailor-made for prompts and NLP characteristics. Extensive experiments on six well-known benchmark datasets, using three prevalent pre-trained LLMs (BERT, RoBERTa, and Facebook OPT-1.3b), demonstrate the effectiveness, harmlessness, robustness, and stealthiness of PromptCARE.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes PromptCARE, the first framework for prompt copyright protection through watermark injection and verification schemes tailor-made for prompts and NLP characteristics, and demonstrates the effectiveness, harmlessness, robustness, and stealthiness of this proposed framework."
            },
            "score": 2
        },
        {
            "id": "db74890e86e5440ad0371e3b01551a8eb97f347a",
            "paperId": "db74890e86e5440ad0371e3b01551a8eb97f347a",
            "title": "Automatic Noise Generation and Reduction for Text Classification",
            "abstract": "Label noise is an important issue in machine learning, which might lead to negative influences on various tasks. Given that real benchmarks for evaluation of noise reduction methods are limited, plenty of studies construct pseudo noisy data to verify their proposed methods. However, very few works have realized the rationality of the noise generation strategies. If the generated pseudo datasets are biased, their final conclusions might also be problematic. In this work, we focus on text classification of natural language processing (NLP) to investigate various pseudo noise generation methods, which is the first work of this line for NLP. In particular, we compare the noise generated with crowdsourcing noise, a kind of real noise as gold-standard, to evaluate these noise generation methods. After then, we measure and compare the performance of representative noise reduction methods respectively based on the data of crowdsourcing and our top-ranked pseudo noisy generation strategies. We conduct experiments on five text classification datasets, offering detailed comparison results as well as discussions.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work focuses on text classification of natural language processing (NLP) to investigate various pseudo noisy generation methods, which is the first work of this line for NLP, and compares the noise generated with crowdsourcing noise, a kind of real noise as gold-standard, to evaluate these noise generation methods."
            },
            "score": 2
        },
        {
            "id": "49b4d8b6f3acf0bf094e8b80640bc585a6ce8918",
            "paperId": "49b4d8b6f3acf0bf094e8b80640bc585a6ce8918",
            "title": "Few-Shot Character Understanding in Movies as an Assessment to Meta-Learning of Theory-of-Mind",
            "abstract": "When reading a story, humans can quickly understand new fictional characters with a few observations, mainly by drawing analogies to fictional and real people they already know. This reflects the few-shot and meta-learning essence of humans' inference of characters' mental states, i.e., theory-of-mind (ToM), which is largely ignored in existing research. We fill this gap with a novel NLP dataset, ToM-in-AMC, the first assessment of machines' meta-learning of ToM in a realistic narrative understanding scenario. Our dataset consists of ~1,000 parsed movie scripts, each corresponding to a few-shot character understanding task that requires models to mimic humans' ability of fast digesting characters with a few starting scenes in a new movie. We propose a novel ToM prompting approach designed to explicitly assess the influence of multiple ToM dimensions. It surpasses existing baseline models, underscoring the significance of modeling multiple ToM dimensions for our task. Our extensive human study verifies that humans are capable of solving our problem by inferring characters' mental states based on their previously seen movies. In comparison, our systems based on either state-of-the-art large language models (GPT-4) or meta-learning algorithms lags>20% behind, highlighting a notable limitation in existing approaches' ToM capabilities.",
            "year": 2022,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel NLP dataset, ToM-in-AMC, the first assessment of machines' meta-learning of ToM in a realistic narrative understanding scenario, and a novel ToM prompting approach designed to explicitly assess the influence of multiple ToM dimensions."
            },
            "score": 2
        },
        {
            "id": "e4e625f8e8ae5ee82e75de5ad6e07af57cca7f53",
            "paperId": "e4e625f8e8ae5ee82e75de5ad6e07af57cca7f53",
            "title": "Harnessing the Power of Large Language Models for Natural Language to First-Order Logic Translation",
            "abstract": "Translating natural language sentences to first-order logic (NL-FOL translation) is a longstanding challenge in the NLP and formal logic literature. This paper introduces LogicLLaMA, a LLaMA-7B model fine-tuned for NL-FOL translation using LoRA on a single GPU. LogicLLaMA is capable of directly translating natural language into FOL rules, which outperforms GPT-3.5. LogicLLaMA is also equipped to correct FOL rules predicted by GPT-3.5, and can achieve similar performance as GPT-4 with a fraction of the cost. This correction ability was achieved by a novel supervised fine-tuning (SFT) + reinforcement learning with human feedback (RLHF) framework, which initially trains on synthetically perturbed NL-FOL pairs to encourage chain-of-thought reasoning and then fine-tunes with RLHF on GPT-3.5 outputs using a FOL verifier as the reward model. To train LogicLLaMA, we present MALLS (large language $\\textbf{M}$odel gener$\\textbf{A}$ted N$\\textbf{L}$-FO$\\textbf{L}$ pair$\\textbf{S}$), a dataset of 34K high-quality and diverse sentence-level NL-FOL pairs collected from GPT-4. The dataset was created by implementing a pipeline that prompts GPT-4 for pairs, and dynamically adjusts the prompts to ensure the collection of pairs with rich and diverse contexts at different levels of complexity, and verifies the validity of the generated FOL rules. Codes, weights, and data are available at $\\href{https://github.com/gblackout/LogicLLaMA}{{\\small \\text{https://github.com/gblackout/LogicLLaMA}}}$.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LogicLLaMA, a LLaMA-7B model fine-tuned for NL-FOL translation using LoRA on a single GPU, is introduced, capable of directly translating natural language into FOL rules, which outperforms GPT-3.5."
            },
            "score": 2
        },
        {
            "id": "35820d9ec212bf24118cde5becc9bc53eb47ffc3",
            "paperId": "35820d9ec212bf24118cde5becc9bc53eb47ffc3",
            "title": "Analysis and verification of rationality of dead markings in colored Petri net models",
            "abstract": "To determinate that whether the deadmarkings affect the safety of the system and the correctness of the model or not in the state space report which obtained by using CPN Tools to simulate a CPN model,this paper proposed an algorithm which based on the ASK-CTL and model checking theories by using the non-standard state space query to verify the rationality of the deadmarkings in a CPN model. This algorithm was based on the theory of model checking and non-standard state space query which applied ML language to edit the functional functions would be obtained for a further study of the deadmarking in order to ensure the safety of the system or the correctness of the CPN model. Finally,it took an elevator door system as an example to verify the effectiveness of the proposed algorithm.",
            "year": 2014,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An algorithm was proposed which based on the ASK-CTL and model checking theories by using the non-standard state space query to verify the rationality of the deadmarkings in a CPN model to ensure the safety of the system or the correctness of the CPN models."
            },
            "score": 1
        },
        {
            "id": "459c82205d2a27a8542bba7a4d478a8a23be2f5d",
            "paperId": "459c82205d2a27a8542bba7a4d478a8a23be2f5d",
            "title": "Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent",
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable zero-shot generalization across various language-related tasks, including search engines. However, existing work utilizes the generative ability of LLMs for Information Retrieval (IR) rather than direct passage ranking. The discrepancy between the pre-training objectives of LLMs and the ranking objective poses another challenge. In this paper, we first investigate generative LLMs such as ChatGPT and GPT-4 for relevance ranking in IR. Surprisingly, our experiments reveal that properly instructed LLMs can deliver competitive, even superior results to state-of-the-art supervised methods on popular IR benchmarks. Furthermore, to address concerns about data contamination of LLMs, we collect a new test set called NovelEval, based on the latest knowledge and aiming to verify the model's ability to rank unknown knowledge. Finally, to improve efficiency in real-world applications, we delve into the potential for distilling the ranking capabilities of ChatGPT into small specialized models using a permutation distillation scheme. Our evaluation results turn out that a distilled 440M model outperforms a 3B supervised model on the BEIR benchmark. The code to reproduce our results is available at www.github.com/sunnweiwei/RankGPT.",
            "year": 2023,
            "citationCount": 122,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Surprisingly, this paper reveals that properly instructed LLMs can deliver competitive, even superior results to state-of-the-art supervised methods on popular IR benchmarks, and investigates generative LLMs such as ChatGPT and GPT-4 for relevance ranking in IR."
            },
            "score": 1
        },
        {
            "id": "a3ba7fdf789bcef381acd0d277a086428153bb9f",
            "paperId": "a3ba7fdf789bcef381acd0d277a086428153bb9f",
            "title": "TemporalWiki: A Lifelong Benchmark for Training and Evaluating Ever-Evolving Language Models",
            "abstract": "Language Models (LMs) become outdated as the world changes; they often fail to perform tasks requiring recent factual information which was absent or different during training, a phenomenon called temporal misalignment. This is especially a challenging problem because the research community still lacks a coherent dataset for assessing the adaptability of LMs to frequently-updated knowledge corpus such as Wikipedia. To this end, we introduce TemporalWiki, a lifelong benchmark for ever-evolving LMs that utilizes the difference between consecutive snapshots of English Wikipedia and English Wikidata for training and evaluation, respectively. The benchmark hence allows researchers to periodically track an LM\u2019s ability to retain previous knowledge and acquire updated/new knowledge at each point in time. We also find that training an LM on the diff data through continual learning methods achieves similar or better perplexity than on the entire snapshot in our benchmark with 12 times less computational cost, which verifies that factual knowledge in LMs can be safely updated with minimal training data via continual learning.",
            "year": 2022,
            "citationCount": 51,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "TemporalWiki is introduced, a lifelong benchmark for ever-evolving LMs that utilizes the difference between consecutive snapshots of English Wikipedia and English Wikidata for training and evaluation, which verifies that factual knowledge in LMs can be safely updated with minimal training data via continual learning."
            },
            "score": 1
        },
        {
            "id": "93c61ab2c3b10fb7d574d2f167fbbbee50678c2b",
            "paperId": "93c61ab2c3b10fb7d574d2f167fbbbee50678c2b",
            "title": "\"Stars in the Water\": Hermann Broch's Die Unbekannte Gr\u00f6\u00dfe and the Language of Interwar Experience",
            "abstract": "Abstract:This article investigates the role of the \"Sterne im Wasser\" as the central leitmotif in Hermann Broch's Die Unbekannte Gr\u00f6\u00dfe (1933). As a plot device, the \"Sterne im Wasser\" motif fuels Broch's construction and parody of the main character Richard Hieck's mathematical world view. Richard's failures to assign meaning to the \"Sterne im Wasser\" by means of mathematical reasoning mirror Broch's critique of the narrowly discursive attempts at meaning-making, and especially logical positivism, that characterize the Viennese interwar period and the late modernist obsession with linguistic precision. Beyond the parodic dimension of Broch's novel, however, the \"Sterne im Wasser\" motif also models the production of meaning via metaphor and self-consciously performs literature's ability to create meaning where other discursive languages fail. What Broch regards as his least successful novel in fact delivers a powerful defence for the necessity of literature in the age of scientific precision.",
            "year": 2023,
            "citationCount": 0,
            "tldr": null,
            "score": 1
        },
        {
            "id": "a512537b5a9933ebb3bc7f7c4a9d42509d07b8ee",
            "paperId": "a512537b5a9933ebb3bc7f7c4a9d42509d07b8ee",
            "title": "A Simulation Based Approach for Rationality Verification of Test Sequence for CTCS-3 Train Control System",
            "abstract": "A CTCS-3 train control system is the key equipment for guaranteeing the safety, reliability and efficiency of high speed railway. Test sequence is the important document to guide the testing for train control system. Manual preparation of test sequence is characteristic of low efficiency and high demand for professional expertise, so it is difficult to guarantee the rationality of test sequence. This paper proposes a simulation based approach for rationality verification of test sequence. Firstly through analyzing the generation process of test sequence, the rationality criteria are obtained; secondly a rule base is built to store the rules used to verifying the rationality of test sequence; finally a train operation simulation environment is established for verifying the rationality of test sequence. Several key issues such as rationality criteria, rationality rules, rules base, and verification strategy are addressed. Introduction CTCS-3 train control system is the key equipment that ensures the safety, reliability and high efficiency of train operation. Before putting the system into service, it is necessary to carry out a series of testing including laboratory testing, pilot line testing, integrated testing and commissioning, as well as interoperability testing. To ensure the safety and orderliness of a field testing, a set of test sequences must be prepared by concatenating the test cases [1-2]. Some research related to the generation of test sequence has been carried out. Various methods have been proposed, such as the methods based on Chinese Postman algorithm[3-4], formal method[5], expert system[6], rule inference[7], state matching[8] and case-based reasoning[9]. A computer-aided tool[10] has been developed with the aim to providing an integrated graphical editing environment, with such function as automatic generation of test sequence files in customized WORD format for field testing and XML test scripts for laboratory simulation platform, and electronic management of test cases, test subsequences and test sequences. This paper focuses on the rationality of the test sequence on the basis of rationality requirement, constructs the rules base for rationality verification, and proposes the test sequence rationality verification method bases on train operation simulation. This paper is organized as follows. Firstly, the criteria for test sequence rationality is defined; secondly the simulation method for train operation is introduced. Thirdly, the rule base for the verification is introduced; fourthly, the verification process and verification strategy are elaborated; finally, some conclusions are drawn and some future works are envisaged. Test Sequence Rationality Terminology (1) Feature and Test Case [1-2] A feature is a group of requirements in System Requirements Specification for CTCS-3 Train Control System (v1.0) (SRS) [1], which can be tested on the available standard interfaces of the system. 4th International Conference on Machinery, Materials and Computing Technology (ICMMCT 2016) \u00a9 2016. The authors Published by Atlantis Press 654 A test case is for one test of a feature, which contains: 1basic information of the test case, including the number, the description of the test case, the applicable operating modes and application levels, the test target, the test method and test constraints; 2start/end conditions, including the state of the internal variables and the interfaces before/after the execution of the test case; 3step-by-step description of the test procedure. In each step, the input/output data on the interfaces are specified. (2) Test Sequence The field testing of a train control system is a comprehensive project, which involves not only the operation of the whole train control system, but also the co-operation of other railway sectors including rolling stock, power supply, permanent way, transportation organization and so on. It consumes not only a lot of manpower, material and financial resources, but also has potential safety hazards. In order to ensure the safety and orderliness of a field testing, it is necessary to prepare a set of test sequences according to the test conditions and requirements. A test sequence document shall at least contains: 1identity information of the test sequence, such as the number of the test sequence, the major functional points to be tested; 2 the test prerequisites, including the start position of the train, and the initial state of the on-board equipment; 3 the track layout graph, which is used to illustrate the overview track layout of the test line, the train path, the test conditions (such as track occupation, Temporary Speed Restriction (TSR)) and some prompt boxes for the focus points during a test trip;4the test procedures, which describe step by step the status of the train, the change of operating mode of the onboard equipment, the test conditions, the related cases and the expected test results. Criteria for Test Sequence Rationality The rationality of test sequence rationality shall be reflected in the following aspects: (1) Completeness The set of test sequences shall cover all the test cases to be tested. Additionally, the track conditions needed by some of the test cases shall be included in the test sequence. (2) Continuity A test sequence is formed by concatenating the test cases according to the start and end condition of each test case. The end mode of the former test case should be same as the start mode of the latter test case. (3) Correctness Considering that there are some special locations on the line, for example, RBC/RBC border, Neutral zone, it is necessary to ensure various constraints for the special locations shall be met. And the concatenation of test cases shall comply with the working process of the on-board equipment. (4) Feasibility In a field testing, due to the limitation of the actual conditions, various constraints related with special time and location, special equipment and special status shall be taken into consideration. In addition, test conditions shall be arranged at convenient places and the test scenarios shall be achievable. (5) Efficiency Under the premise of guaranteeing the completeness, continuity, correctness and feasibility, the consumption of test resources including test time, test manpower and material resources shall be kept as minimum as possible. Test Sequence Verification Software The test sequence verification software is shown as in Fig.1.",
            "year": 2016,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A simulation based approach for rationality verification of test sequence is proposed and several key issues such as rationality criteria, rationality rules, rules base, and verification strategy are addressed."
            },
            "score": 1
        }
    ],
    "novelty": "no"
}