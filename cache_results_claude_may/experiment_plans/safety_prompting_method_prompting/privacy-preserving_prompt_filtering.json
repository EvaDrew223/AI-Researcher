{
    "topic_description": "novel prompting methods to improve large language models' robustness against adversarial attacks and improve their security and privacy",
    "idea_name": "Privacy-Preserving Prompt Filtering",
    "raw_idea": {
        "Problem": "Large Language Models (LLMs) can be exploited to generate outputs that reveal sensitive information about the training data or the individuals represented in it. This raises significant privacy concerns, particularly in domains such as healthcare, finance, and personal communication.",
        "Existing Methods": "Existing approaches to preserving privacy in LLMs include differential privacy, federated learning, and post-processing techniques such as text sanitization. However, these methods often require significant computational overhead and can degrade the model's utility.",
        "Motivation": "We propose a novel prompting strategy that enables LLMs to filter out privacy-sensitive information from their outputs, without the need for expensive training techniques or post-processing. By leveraging the model's language understanding capabilities, we aim to enable LLMs to dynamically identify and remove sensitive content from their responses.",
        "Proposed Method": "We introduce Privacy-Preserving Prompt Filtering (PPPF), a flexible and efficient approach that works as follows: 1) Privacy Policy Prompt: We prepend a privacy policy prompt to the input prompt, specifying the types of sensitive information that should be excluded from the output (e.g., personal names, addresses, financial details). 2) Sensitive Content Detection: We use a second prompt to instruct the LLM to identify any sensitive information in its generated response that violates the privacy policy. 3) Content Filtering: If sensitive information is detected, we use a third prompt to instruct the LLM to remove or replace the sensitive content with neutral, non-identifying information. 4) Response Generation: The filtered response is then returned to the user. By using prompts to guide the LLM's content generation and filtering process, PPPF enables the model to dynamically preserve privacy without the need for expensive training techniques or post-processing. This approach leverages the LLM's inherent language understanding capabilities to identify and remove sensitive information in real-time.",
        "Experiment Plan": "We will evaluate PPPF on a range of language generation tasks that involve potentially sensitive information, such as medical record summarization and personal email generation. We will measure the model's ability to generate outputs that comply with the specified privacy policies, as well as the utility and fluency of the filtered responses. We will compare PPPF to baseline methods such as differential privacy and post-processing techniques. We hypothesize that PPPF will effectively preserve privacy while maintaining high output quality, all without the need for expensive training or post-processing steps."
    },
    "full_experiment_plan": {
        "Title": "Privacy-Preserving Prompt Filtering: Enabling Large Language Models to Protect Sensitive Information",
        "Problem Statement": "Large Language Models (LLMs) can generate outputs that reveal sensitive information about the training data or the individuals represented in it, raising significant privacy concerns in domains such as healthcare, finance, and personal communication.",
        "Motivation": "Existing approaches to preserving privacy in LLMs, such as differential privacy, federated learning, and post-processing techniques like text sanitization, often require significant computational overhead and can degrade the model's utility. We propose a novel prompting strategy that enables LLMs to filter out privacy-sensitive information from their outputs, without the need for expensive training techniques or post-processing. By leveraging the model's language understanding capabilities, we aim to enable LLMs to dynamically identify and remove sensitive content from their responses.",
        "Proposed Method": "We introduce Privacy-Preserving Prompt Filtering (PPPF), a flexible and efficient approach that works as follows:\n1. Privacy Policy Prompt: We prepend a privacy policy prompt to the input prompt, specifying the types of sensitive information that should be excluded from the output (e.g., personal names, addresses, financial details).\n2. Sensitive Content Detection: We use a second prompt to instruct the LLM to identify any sensitive information in its generated response that violates the privacy policy.\n3. Content Filtering: If sensitive information is detected, we use a third prompt to instruct the LLM to remove or replace the sensitive content with neutral, non-identifying information.\n4. Response Generation: The filtered response is then returned to the user.\nBy using prompts to guide the LLM's content generation and filtering process, PPPF enables the model to dynamically preserve privacy without the need for expensive training techniques or post-processing. This approach leverages the LLM's inherent language understanding capabilities to identify and remove sensitive information in real-time.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Dataset Selection": "We will evaluate PPPF on a range of language generation tasks that involve potentially sensitive information, such as medical record summarization (e.g., i2b2 NLP Dataset), personal email generation (e.g., Enron Email Dataset), and financial document summarization (e.g., Financial Phrase Bank). These datasets contain real-world examples of text with sensitive information that needs to be protected.",
            "Step 2: Baseline Methods": "We will compare PPPF to the following baseline methods:\n1. Unfiltered Generation: The LLM generates responses without any privacy-preserving measures.\n2. Differential Privacy: We will apply differential privacy techniques during the LLM's training process to limit the risk of revealing sensitive information.\n3. Text Sanitization: We will post-process the LLM's generated responses using text sanitization techniques, such as named entity recognition and replacement, to remove sensitive information.",
            "Step 3: Evaluation Metrics": "We will measure the performance of PPPF and the baseline methods using the following metrics:\n1. Privacy Preservation: We will manually annotate the generated responses to determine the percentage of outputs that successfully exclude sensitive information, as defined by the privacy policy prompt.\n2. Fluency and Coherence: We will use automated metrics, such as perplexity and BLEU score, to assess the fluency and coherence of the generated responses.\n3. Content Preservation: We will manually evaluate the generated responses to determine the extent to which the filtered outputs preserve the relevant, non-sensitive information from the original responses.",
            "Step 4: Prompt Engineering": "We will design a set of privacy policy prompts, sensitive content detection prompts, and content filtering prompts for each dataset. For example, in the medical domain, the privacy policy prompt may specify that personal names, dates of birth, and specific diagnoses should be excluded from the output. The sensitive content detection prompt will instruct the LLM to identify any instances of this information in its generated response, and the content filtering prompt will guide the LLM to replace the sensitive information with neutral, non-identifying alternatives.",
            "Step 5: Model Selection": "We will evaluate PPPF using state-of-the-art LLMs, such as GPT-3.5 (text-davinci-003) and GPT-4 from the OpenAI API. We will compare the performance of these models to determine the impact of model size and architecture on the effectiveness of PPPF.",
            "Step 6: Experiment Execution": "For each combination of dataset, baseline method, and LLM, we will generate responses to a sample of input prompts. We will apply PPPF to the LLM-generated responses and evaluate the results using the metrics described in Step 3. We will also conduct a qualitative analysis of the filtered outputs to identify any common patterns or challenges in the content filtering process.",
            "Step 7: Result Analysis": "We will compare the performance of PPPF to the baseline methods across all datasets and LLMs. We will determine the extent to which PPPF improves privacy preservation while maintaining the fluency, coherence, and content preservation of the generated responses. We will also analyze the impact of different prompt engineering strategies on the effectiveness of PPPF."
        },
        "Test Case Examples": {
            "Example 1": {
                "Input": "Generate a summary of the patient's medical record.",
                "Privacy Policy Prompt": "Exclude any personal names, dates of birth, and specific diagnoses from the summary.",
                "Unfiltered Output": "John Doe, born on 01/01/1980, was diagnosed with type 2 diabetes mellitus on 03/15/2020. He has been prescribed metformin to manage his blood sugar levels.",
                "PPPF Output": "The patient, a 43-year-old male, was recently diagnosed with a chronic metabolic disorder. He has been prescribed medication to manage his condition."
            },
            "Example 2": {
                "Input": "Generate a summary of the financial report.",
                "Privacy Policy Prompt": "Exclude any specific financial figures, account numbers, and personally identifiable information from the summary.",
                "Unfiltered Output": "Acme Inc., account number 1234567890, reported a net profit of $1,500,000 for the fiscal year 2022. The CEO, Jane Smith, attributed the success to the launch of a new product line.",
                "PPPF Output": "The company reported a substantial net profit for the most recent fiscal year. The CEO attributed the success to the launch of a new product line."
            },
            "Explanation": "In both examples, PPPF successfully identifies and removes sensitive information from the generated responses, as specified by the privacy policy prompts. The filtered outputs maintain the core message of the original responses while protecting the privacy of the individuals and organizations involved."
        },
        "Fallback Plan": "If PPPF does not achieve satisfactory performance in terms of privacy preservation, fluency, coherence, or content preservation, we will explore the following alternative approaches:\n1. Fine-tuning: Instead of relying solely on prompts, we will fine-tune the LLMs on datasets that have been annotated for sensitive information. This may help the models learn to identify and filter sensitive content more effectively.\n2. Hybrid Approaches: We will combine PPPF with other privacy-preserving techniques, such as differential privacy or text sanitization, to create a hybrid approach that leverages the strengths of each method.\n3. Iterative Refinement: We will investigate the use of iterative refinement techniques, where the LLM generates multiple candidate responses, and the privacy policy prompts are used to guide the selection of the most privacy-preserving output.\n4. Human Evaluation: We will conduct a more extensive human evaluation of the generated responses to identify the specific challenges and limitations of PPPF. This analysis will inform the development of more advanced prompt engineering strategies or alternative approaches to privacy preservation in LLMs."
    },
    "novelty_queries": [
        "KeywordQuery(\"privacy-preserving language models\")",
        "KeywordQuery(\"prompt filtering sensitive information language models\")",
        "KeywordQuery(\"language models protect privacy prompts\")",
        "KeywordQuery(\"Privacy-Preserving Prompt Filtering NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "85e51f70d0a48ab87b8df0eee3ef55c93e65b8ce",
            "paperId": "85e51f70d0a48ab87b8df0eee3ef55c93e65b8ce",
            "title": "Privacy-Preserving Recommender Systems with Synthetic Query Generation using Differentially Private Large Language Models",
            "abstract": "We propose a novel approach for developing privacy-preserving large-scale recommender systems using differentially private (DP) large language models (LLMs) which overcomes certain challenges and limitations in DP training these complex systems. Our method is particularly well suited for the emerging area of LLM-based recommender systems, but can be readily employed for any recommender systems that process representations of natural language inputs. Our approach involves using DP training methods to fine-tune a publicly pre-trained LLM on a query generation task. The resulting model can generate private synthetic queries representative of the original queries which can be freely shared for any downstream non-private recommendation training procedures without incurring any additional privacy cost. We evaluate our method on its ability to securely train effective deep retrieval models, and we observe significant improvements in their retrieval quality without compromising query-level privacy guarantees compared to methods where the retrieval models are directly DP trained.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 7,
            "novelty_score": "The project proposal aims to develop a prompting strategy for large language models to filter out privacy-sensitive information from their outputs without the need for expensive training techniques or post-processing. The paper proposes using differentially private training methods to fine-tune a pre-trained large language model on a query generation task, which can then generate private synthetic queries for downstream non-private recommendation training.\n\nThe project focuses on preserving privacy in large language model outputs, while the paper focuses on preserving privacy in recommender system queries. Although both deal with privacy preservation, the project targets the output of language models, while the paper targets the input queries for recommender systems.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "47979c7e95705c1a609ac81ec9a82d9a3af88431",
            "paperId": "47979c7e95705c1a609ac81ec9a82d9a3af88431",
            "title": "Hide and Seek (HaS): A Lightweight Framework for Prompt Privacy Protection",
            "abstract": "Numerous companies have started offering services based on large language models (LLM), such as ChatGPT, which inevitably raises privacy concerns as users' prompts are exposed to the model provider. Previous research on secure reasoning using multi-party computation (MPC) has proven to be impractical for LLM applications due to its time-consuming and communication-intensive nature. While lightweight anonymization techniques can protect private information in prompts through substitution or masking, they fail to recover sensitive data replaced in the LLM-generated results. In this paper, we expand the application scenarios of anonymization techniques by training a small local model to de-anonymize the LLM's returned results with minimal computational overhead. We introduce the HaS framework, where\"H(ide)\"and\"S(eek)\"represent its two core processes: hiding private entities for anonymization and seeking private entities for de-anonymization, respectively. To quantitatively assess HaS's privacy protection performance, we propose both black-box and white-box adversarial models. Furthermore, we conduct experiments to evaluate HaS's usability in translation and classification tasks. The experimental findings demonstrate that the HaS framework achieves an optimal balance between privacy protection and utility.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The HaS framework is introduced, where\"H(ide)\"and\"S(eek)\"represent its two core processes: hiding private entities for anonymization and seekingprivate entities for de-anonymization, respectively, and to quantitatively assess HaS's privacy protection performance, both black-box and white-box adversarial models are proposed."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to develop a prompting strategy called Privacy-Preserving Prompt Filtering (PPPF) that enables large language models to filter out privacy-sensitive information from their outputs without the need for expensive training techniques or post-processing. The proposed approach leverages the model's language understanding capabilities to dynamically identify and remove sensitive content from the responses.\n\nThe paper introduces the HaS framework, which uses anonymization techniques to protect private information in prompts and trains a small local model to de-anonymize the LLM-generated results. The framework aims to achieve a balance between privacy protection and utility while minimizing computational overhead.\n\nWhile both the project proposal and the paper address the issue of privacy protection in large language models, their approaches differ. The project proposal focuses on using prompts to guide the LLM's content generation and filtering process, while the paper proposes anonymizing the input prompts and de-anonymizing the LLM's output using a separate local model.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "2403c8e72a90d9c778970fc0812ecdcc58800c5d",
            "paperId": "2403c8e72a90d9c778970fc0812ecdcc58800c5d",
            "title": "Can Language Models be Instructed to Protect Personal Information?",
            "abstract": "Large multimodal language models have proven transformative in numerous applications. However, these models have been shown to memorize and leak pre-training data, raising serious user privacy and information security concerns. While data leaks should be prevented, it is also crucial to examine the trade-off between the privacy protection and model utility of proposed approaches. In this paper, we introduce PrivQA -- a multimodal benchmark to assess this privacy/utility trade-off when a model is instructed to protect specific categories of personal information in a simulated scenario. We also propose a technique to iteratively self-moderate responses, which significantly improves privacy. However, through a series of red-teaming experiments, we find that adversaries can also easily circumvent these protections with simple jailbreaking methods through textual and/or image inputs. We believe PrivQA has the potential to support the development of new models with improved privacy protections, as well as the adversarial robustness of these protections. We release the entire PrivQA dataset at https://llm-access-control.github.io/.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "PrivQA is introduced -- a multimodal benchmark to assess this privacy/utility trade-off when a model is instructed to protect specific categories of personal information in a simulated scenario and a technique to iteratively self-moderate responses, which significantly improves privacy."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is preserving privacy in large language model outputs without sacrificing utility, and the proposed approach is a novel prompting strategy called Privacy-Preserving Prompt Filtering (PPPF) that guides the model to identify and remove sensitive information from its responses.\n\nThe research problem in the paper is assessing the trade-off between privacy protection and model utility when instructing a model to protect personal information, and the proposed approach is a multimodal benchmark called PrivQA and an iterative self-moderation technique.\n\nWhile both works aim to protect privacy in language model outputs, the proposal focuses on a prompting strategy to filter sensitive information, whereas the paper introduces a benchmark and explores the robustness of privacy protections against adversarial attacks. The approaches and focus of the two works are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "6e41a4cbb34c4d403efb73d74f5be5556b1f13d6",
            "paperId": "6e41a4cbb34c4d403efb73d74f5be5556b1f13d6",
            "title": "Privacy-Preserving In-Context Learning for Large Language Models",
            "abstract": "In-context learning (ICL) is an important capability of Large Language Models (LLMs), enabling these models to dynamically adapt based on specific, in-context exemplars, thereby improving accuracy and relevance. However, LLM's responses may leak the sensitive private information contained in in-context exemplars. To address this challenge, we propose Differentially Private In-context Learning (DP-ICL), a general paradigm for privatizing ICL tasks. The key idea for DP-ICL paradigm is generating differentially private responses through a noisy consensus among an ensemble of LLM's responses based on disjoint exemplar sets. Based on the general paradigm of DP-ICL, we instantiate several techniques showing how to privatize ICL for text classification and language generation. We evaluate DP-ICL on four text classification benchmarks and two language generation tasks, and our empirical results show that DP-ICL achieves a strong utility-privacy tradeoff.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The key idea for DP-ICL paradigm is generating differentially private responses through a noisy consensus among an ensemble of LLM's responses based on disjoint exemplar sets, which achieves a strong utility-privacy tradeoff."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to develop a prompting strategy called Privacy-Preserving Prompt Filtering (PPPF) that enables LLMs to filter out privacy-sensitive information from their outputs without the need for expensive training techniques or post-processing. The paper proposes Differentially Private In-context Learning (DP-ICL), a paradigm for privatizing in-context learning tasks by generating differentially private responses through a noisy consensus among an ensemble of LLM's responses based on disjoint exemplar sets.\n\nWhile both the project proposal and the paper address the issue of preserving privacy in LLMs, their approaches differ. The project proposal focuses on a prompting strategy to filter sensitive information from the LLM's outputs, while the paper proposes a paradigm for privatizing in-context learning tasks using differential privacy and an ensemble of LLM responses.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "82d226701ab9c829345809e95d3ec6b0b0791727",
            "paperId": "82d226701ab9c829345809e95d3ec6b0b0791727",
            "title": "Synthetic Query Generation for Privacy-Preserving Deep Retrieval Systems using Differentially Private Language Models",
            "abstract": "We address the challenge of ensuring differential privacy (DP) guarantees in training deep retrieval systems. Training these systems often involves the use of contrastive-style losses, which are typically non-per-example decomposable, making them difficult to directly DP-train with since common techniques require per-example gradient. To address this issue, we propose an approach that prioritizes ensuring query privacy prior to training a deep retrieval system. Our method employs DP language models (LMs) to generate private synthetic queries representative of the original data. These synthetic queries can be used in downstream retrieval system training without compromising privacy. Our approach demonstrates a significant enhancement in retrieval quality compared to direct DP-training, all while maintaining query-level privacy guarantees. This work highlights the potential of harnessing LMs to overcome limitations in standard DP-training methods.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work employs DP language models (LMs) to generate private synthetic queries representative of the original data to demonstrate a significant enhancement in retrieval quality compared to direct DP-training, all while maintaining query-level privacy guarantees."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to develop a prompting strategy called Privacy-Preserving Prompt Filtering (PPPF) that enables large language models to filter out privacy-sensitive information from their outputs without the need for expensive training techniques or post-processing.\n\nThe paper proposes using differentially private language models to generate synthetic queries representative of the original data, which can be used to train deep retrieval systems without compromising privacy.\n\nWhile both the project proposal and the paper address privacy concerns in natural language processing tasks, their approaches and specific research problems differ. The project proposal focuses on using prompts to guide language models in filtering sensitive information during generation, while the paper proposes generating synthetic queries using differentially private language models for training deep retrieval systems.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "f648d5bff46b180c633c812c71aa1cfafd7576dc",
            "paperId": "f648d5bff46b180c633c812c71aa1cfafd7576dc",
            "title": "Privacy-Preserving Instructions for Aligning Large Language Models",
            "abstract": "Service providers of large language model (LLM) applications collect user instructions in the wild and use them in further aligning LLMs with users' intentions. These instructions, which potentially contain sensitive information, are annotated by human workers in the process. This poses a new privacy risk not addressed by the typical private optimization. To this end, we propose using synthetic instructions to replace real instructions in data annotation and model fine-tuning. Formal differential privacy is guaranteed by generating those synthetic instructions using privately fine-tuned generators. Crucial in achieving the desired utility is our novel filtering algorithm that matches the distribution of the synthetic instructions to that of the real ones. In both supervised fine-tuning and reinforcement learning from human feedback, our extensive experiments demonstrate the high utility of the final set of synthetic instructions by showing comparable results to real instructions. In supervised fine-tuning, models trained with private synthetic instructions outperform leading open-source models such as Vicuna.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes using synthetic instructions to replace real instructions in data annotation and model fine-tuning, and demonstrates the high utility of the final set of synthetic instructions by showing comparable results to real instructions."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to develop a prompting strategy called Privacy-Preserving Prompt Filtering (PPPF) that enables large language models to filter out privacy-sensitive information from their outputs without the need for expensive training techniques or post-processing.\n\nThe paper proposes using synthetic instructions, generated by privately fine-tuned generators, to replace real instructions containing sensitive information in data annotation and model fine-tuning for aligning large language models with users' intentions.\n\nWhile both the project proposal and the paper address the issue of preserving privacy in large language models, their approaches differ. The project proposal focuses on using prompts to guide the model's content generation and filtering process, while the paper proposes using synthetic instructions to replace real ones in data annotation and model fine-tuning.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "46eea7d651420e60f9b1393e3f5eda14cbff7a2a",
            "paperId": "46eea7d651420e60f9b1393e3f5eda14cbff7a2a",
            "title": "Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks",
            "abstract": "Pretrained language models sometimes possess knowledge that we do not wish them to, including memorized personal information and knowledge that could be used to harm people. They can also output toxic or harmful text. To mitigate these safety and informational issues, we propose an attack-and-defense framework for studying the task of deleting sensitive information directly from model weights. We study direct edits to model weights because (1) this approach should guarantee that particular deleted information is never extracted by future prompt attacks, and (2) it should protect against whitebox attacks, which is necessary for making claims about safety/privacy in a setting where publicly available model weights could be used to elicit sensitive information. Our threat model assumes that an attack succeeds if the answer to a sensitive question is located among a set of B generated candidates, based on scenarios where the information would be insecure if the answer is among B candidates. Experimentally, we show that even state-of-the-art model editing methods such as ROME struggle to truly delete factual information from models like GPT-J, as our whitebox and blackbox attacks can recover\"deleted\"information from an edited model 38% of the time. These attacks leverage two key observations: (1) that traces of deleted information can be found in intermediate model hidden states, and (2) that applying an editing method for one question may not delete information across rephrased versions of the question. Finally, we provide new defense methods that protect against some extraction attacks, but we do not find a single universally effective defense method. Our results suggest that truly deleting sensitive information is a tractable but difficult problem, since even relatively low attack success rates have potentially severe societal implications for real-world deployment of language models.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that even state-of-the-art model editing methods such as ROME struggle to truly delete factual information from models like GPT-J, as whitebox and blackbox attacks can recover \"deleted\" information from an edited model 38% of the time."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to develop a prompting strategy called Privacy-Preserving Prompt Filtering (PPPF) that enables large language models to filter out privacy-sensitive information from their outputs without the need for expensive training techniques or post-processing.\n\nThe paper abstract proposes an attack-and-defense framework for studying the task of deleting sensitive information directly from model weights to mitigate safety and informational issues in pretrained language models.\n\nWhile both the project proposal and the paper abstract address the problem of sensitive information in language models, their approaches differ. The project proposal focuses on a prompting strategy to filter sensitive information during the generation process, while the paper abstract explores directly editing model weights to delete sensitive information.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "af710ada8965f274e810053f716f966627a136d9",
            "paperId": "af710ada8965f274e810053f716f966627a136d9",
            "title": "Differentially Private Language Models for Secure Data Sharing",
            "abstract": "To protect the privacy of individuals whose data is being shared, it is of high importance to develop methods allowing researchers and companies to release textual data while providing formal privacy guarantees to its originators. In the field of NLP, substantial efforts have been directed at building mechanisms following the framework of local differential privacy, thereby anonymizing individual text samples before releasing them. In practice, these approaches are often dissatisfying in terms of the quality of their output language due to the strong noise required for local differential privacy. In this paper, we approach the problem at hand using global differential privacy, particularly by training a generative language model in a differentially private manner and consequently sampling data from it. Using natural language prompts and a new prompt-mismatch loss, we are able to create highly accurate and fluent textual datasets taking on specific desired attributes such as sentiment or topic and resembling statistical properties of the training data. We perform thorough experiments indicating that our synthetic datasets do not leak information from our original data and are of high language quality and highly suitable for training models for further analysis on real-world data. Notably, we also demonstrate that training classifiers on private synthetic data outperforms directly training classifiers with DP-SGD.",
            "year": 2022,
            "citationCount": 19,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper training a generative language model in a differentially private manner and consequently sampling data from it is used to create highly accurate and fluent textual datasets taking on specific desired attributes such as sentiment or topic and resembling statistical properties of the training data."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to develop a prompting strategy that enables large language models to filter out privacy-sensitive information from their outputs without the need for expensive training techniques or post-processing. The proposed approach, Privacy-Preserving Prompt Filtering (PPPF), uses prompts to guide the LLM's content generation and filtering process.\n\nThe paper focuses on training generative language models using global differential privacy to create synthetic textual datasets that protect the privacy of individuals while maintaining high language quality and resemblance to the original data's statistical properties.\n\nWhile both the project proposal and the paper address privacy concerns in natural language processing, their approaches differ significantly. The project proposal uses prompting strategies to filter sensitive information during the generation process, while the paper trains language models with differential privacy to create synthetic datasets that protect privacy.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "68226f0ae91283d240a9c294b956cda239a56281",
            "paperId": "68226f0ae91283d240a9c294b956cda239a56281",
            "title": "DP-TabICL: In-Context Learning with Differentially Private Tabular Data",
            "abstract": "In-context learning (ICL) enables large language models (LLMs) to adapt to new tasks by conditioning on demonstrations of question-answer pairs and it has been shown to have comparable performance to costly model retraining and fine-tuning. Recently, ICL has been extended to allow tabular data to be used as demonstration examples by serializing individual records into natural language formats. However, it has been shown that LLMs can leak information contained in prompts, and since tabular data often contain sensitive information, understanding how to protect the underlying tabular data used in ICL is a critical area of research. This work serves as an initial investigation into how to use differential privacy (DP) -- the long-established gold standard for data privacy and anonymization -- to protect tabular data used in ICL. Specifically, we investigate the application of DP mechanisms for private tabular ICL via data privatization prior to serialization and prompting. We formulate two private ICL frameworks with provable privacy guarantees in both the local (LDP-TabICL) and global (GDP-TabICL) DP scenarios via injecting noise into individual records or group statistics, respectively. We evaluate our DP-based frameworks on eight real-world tabular datasets and across multiple ICL and DP settings. Our evaluations show that DP-based ICL can protect the privacy of the underlying tabular data while achieving comparable performance to non-LLM baselines, especially under high privacy regimes.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is preserving privacy in large language models when generating outputs, while the approach is using prompts to guide the model to filter out sensitive information dynamically. \n\nThe research problem in the paper is protecting sensitive tabular data used for in-context learning in large language models, and the approach is applying differential privacy mechanisms to the data before serializing it into prompts.\n\nAlthough both works aim to preserve privacy in large language models, the proposal focuses on using prompts to filter generated outputs, while the paper focuses on privatizing the input data used for in-context learning. The methods employed are quite different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "b2c68b708a9f98996b18c8d21b53a815a2c46a8b",
            "paperId": "b2c68b708a9f98996b18c8d21b53a815a2c46a8b",
            "title": "ProPILE: Probing Privacy Leakage in Large Language Models",
            "abstract": "The rapid advancement and widespread use of large language models (LLMs) have raised significant concerns regarding the potential leakage of personally identifiable information (PII). These models are often trained on vast quantities of web-collected data, which may inadvertently include sensitive personal data. This paper presents ProPILE, a novel probing tool designed to empower data subjects, or the owners of the PII, with awareness of potential PII leakage in LLM-based services. ProPILE lets data subjects formulate prompts based on their own PII to evaluate the level of privacy intrusion in LLMs. We demonstrate its application on the OPT-1.3B model trained on the publicly available Pile dataset. We show how hypothetical data subjects may assess the likelihood of their PII being included in the Pile dataset being revealed. ProPILE can also be leveraged by LLM service providers to effectively evaluate their own levels of PII leakage with more powerful prompts specifically tuned for their in-house models. This tool represents a pioneering step towards empowering the data subjects for their awareness and control over their own data on the web.",
            "year": 2023,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ProPILE lets data subjects formulate prompts based on their own PII to evaluate the level of privacy intrusion in LLMs, and can be leveraged by LLM service providers to effectively evaluate their own levels of PII leakage with more powerful prompts specifically tuned for their in-house models."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is preserving privacy in large language model outputs by filtering out sensitive information, while the paper focuses on probing and evaluating potential leakage of personally identifiable information in large language models. The proposal aims to develop a prompting strategy to enable LLMs to filter out sensitive information, whereas the paper introduces a probing tool for data subjects to assess the level of privacy intrusion in LLMs.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "89512c767e0ca0fe64d12a436c64f15dffdad1e0",
            "paperId": "89512c767e0ca0fe64d12a436c64f15dffdad1e0",
            "title": "Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory",
            "abstract": "The interactive use of large language models (LLMs) in AI assistants (at work, home, etc.) introduces a new set of inference-time privacy risks: LLMs are fed different types of information from multiple sources in their inputs and are expected to reason about what to share in their outputs, for what purpose and with whom, within a given context. In this work, we draw attention to the highly critical yet overlooked notion of contextual privacy by proposing ConfAIde, a benchmark designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs. Our experiments show that even the most capable models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively. This leakage persists even when we employ privacy-inducing prompts or chain-of-thought reasoning. Our work underscores the immediate need to explore novel inference-time privacy-preserving approaches, based on reasoning and theory of mind.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes ConfAIde, a benchmark designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs, and underscores the immediate need to explore novel inference-time privacy-preserving approaches, based on reasoning and theory of mind."
            },
            "score": 6
        },
        {
            "id": "3592dce9bbd1afcfb9e8935a55af1346374fd6fd",
            "paperId": "3592dce9bbd1afcfb9e8935a55af1346374fd6fd",
            "title": "Reducing Privacy Risks in Online Self-Disclosures with Language Models",
            "abstract": "Self-disclosure, while being common and rewarding in social media interaction, also poses privacy risks. In this paper, we take the initiative to protect the user-side privacy associated with online self-disclosure through detection and abstraction. We develop a taxonomy of 19 self-disclosure categories and curate a large corpus consisting of 4.8K annotated disclosure spans. We then fine-tune a language model for detection, achieving over 65% partial span F$_1$. We further conduct an HCI user study, with 82% of participants viewing the model positively, highlighting its real-world applicability. Motivated by the user feedback, we introduce the task of self-disclosure abstraction, which is paraphrasing disclosures into less specific terms while preserving their utility, e.g.,\"Im 16F\"to\"I'm a teenage girl\". We explore various fine-tuning strategies, and our best model can generate diverse abstractions that moderately reduce privacy risks while maintaining high utility according to human evaluation. To help users in deciding which disclosures to abstract, we present a task of rating their importance for context understanding. Our fine-tuned model achieves 80% accuracy, on-par with GPT-3.5. Given safety and privacy considerations, we will only release our corpus to researchers who agree to ethical guidelines.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper develops a taxonomy of 19 self-disclosure categories and curates a large corpus of annotated disclosure spans, and introduces the task of self-disclosure abstraction, which is paraphrasing disclosures into less specific terms while preserving their utility."
            },
            "score": 6
        },
        {
            "id": "2f2a430ba6c93bcfaf4818316ff8a27b1e034b1a",
            "paperId": "2f2a430ba6c93bcfaf4818316ff8a27b1e034b1a",
            "title": "Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models",
            "abstract": "Large language models (LLMs) are excellent in-context learners. However, the sensitivity of data contained in prompts raises privacy concerns. Our work first shows that these concerns are valid: we instantiate a simple but highly effective membership inference attack against the data used to prompt LLMs. To address this vulnerability, one could forego prompting and resort to fine-tuning LLMs with known algorithms for private gradient descent. However, this comes at the expense of the practicality and efficiency offered by prompting. Therefore, we propose to privately learn to prompt. We first show that soft prompts can be obtained privately through gradient descent on downstream data. However, this is not the case for discrete prompts. Thus, we orchestrate a noisy vote among an ensemble of LLMs presented with different prompts, i.e., a flock of stochastic parrots. The vote privately transfers the flock's knowledge into a single public prompt. We show that LLMs prompted with our private algorithms closely match the non-private baselines. For example, using GPT3 as the base model, we achieve a downstream accuracy of 92.7% on the sst2 dataset with ($\\epsilon=0.147, \\delta=10^{-6}$)-differential privacy vs. 95.2% for the non-private baseline. Through our experiments, we also show that our prompt-based approach is easily deployed with existing commercial APIs.",
            "year": 2023,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work first shows that privacy concerns are valid: it instantiates a simple but highly effective membership inference attack against the data used to prompt LLMs, and proposes to privately learn to prompt."
            },
            "score": 6
        },
        {
            "id": "97333fea241659b8d804d04d326c3590c4a528f2",
            "paperId": "97333fea241659b8d804d04d326c3590c4a528f2",
            "title": "Just Fine-tune Twice: Selective Differential Privacy for Large Language Models",
            "abstract": "Protecting large language models from privacy leakage is becoming increasingly crucial with their wide adoption in real-world products. Yet applying *differential privacy* (DP), a canonical notion with provable privacy guarantees for machine learning models, to those models remains challenging due to the trade-off between model utility and privacy loss. Utilizing the fact that sensitive information in language data tends to be sparse, Shi et al. (2021) formalized a DP notion extension called *Selective Differential Privacy* (SDP) to protect only the sensitive tokens defined by a policy function. However, their algorithm only works for RNN-based models. In this paper, we develop a novel framework, *Just Fine-tune Twice* (JFT), that achieves SDP for state-of-the-art large transformer-based models. Our method is easy to implement: it first fine-tunes the model with *redacted* in-domain data, and then fine-tunes it again with the *original* in-domain data using a private training mechanism. Furthermore, we study the scenario of imperfect implementation of policy functions that misses sensitive tokens and develop systematic methods to handle it. Experiments show that our method achieves strong utility compared to previous baselines. We also analyze the SDP privacy guarantee empirically with the canary insertion attack.",
            "year": 2022,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel framework, *Just Fine-tune Twice* (JFT), that achieves SDP for state-of-the-art large transformer-based models and studies the scenario of imperfect implementation of policy functions that misses sensitive tokens and develops systematic methods to handle it."
            },
            "score": 6
        },
        {
            "id": "15a782b48fd26f2ce63ab1259e647c3656ce43c7",
            "paperId": "15a782b48fd26f2ce63ab1259e647c3656ce43c7",
            "title": "Split-and-Denoise: Protect large language model inference with local differential privacy",
            "abstract": "Large Language Models (LLMs) shows powerful capability in natural language understanding by capturing hidden semantics in vector space. This process enriches the value of the text embeddings for various downstream tasks, thereby fostering the Embedding-as-a-Service (EaaS) business model. However, the direct transmission of text to servers poses a largely unaddressed risk of privacy leakage. To mitigate this issue, we introduce Split-N-Denoise (SnD), an innovative framework that split the model to execute the token embedding layer on the client side at minimal computational cost. This allows the client to introduce noise prior to transmitting the embeddings to the server, and subsequently receive and denoise the perturbed output embeddings for downstream tasks. Our approach is designed for the inference stage of LLMs and requires no modifications to the model parameters. Extensive experiments demonstrate SnD's effectiveness in optimizing the privacy-utility tradeoff across various LLM architectures and diverse downstream tasks. The results reveal a significant performance improvement under the same privacy budget compared to the baseline, offering clients a privacy-preserving solution for local privacy protection.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Split-N-Denoise (SnD), an innovative framework that split the model to execute the token embedding layer on the client side at minimal computational cost, is introduced, offering clients a privacy-preserving solution for local privacy protection."
            },
            "score": 6
        },
        {
            "id": "e9ca67b67f4b43650baaef0d03013683eeb4528e",
            "paperId": "e9ca67b67f4b43650baaef0d03013683eeb4528e",
            "title": "Large Language Models Can Be Good Privacy Protection Learners",
            "abstract": "The proliferation of Large Language Models (LLMs) has driven considerable interest in fine-tuning them with domain-specific data to create specialized language models. Nevertheless, such domain-specific fine-tuning data often contains sensitive personally identifiable information (PII). Direct fine-tuning LLMs on this data without privacy protection poses a risk of leakage. To address this challenge, we introduce Privacy Protection Language Models (PPLM), a novel paradigm for fine-tuning LLMs that effectively injects domain-specific knowledge while safeguarding data privacy. Our work offers a theoretical analysis for model design and delves into various techniques such as corpus curation, penalty-based unlikelihood in training loss, and instruction-based tuning, etc. Extensive experiments across diverse datasets and scenarios demonstrate the effectiveness of our approaches. In particular, instruction tuning with both positive and negative examples, stands out as a promising method, effectively protecting private data while enhancing the model's knowledge. Our work underscores the potential for Large Language Models as robust privacy protection learners.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Privacy Protection Language Models (PPLM) is introduced, a novel paradigm for fine-tuning LLMs that effectively injects domain-specific knowledge while safeguarding data privacy and underscores the potential for Large Language Models as robust privacy protection learners."
            },
            "score": 6
        },
        {
            "id": "187f4521e6080f93fc2a26bf91b4e7d64f94e18a",
            "paperId": "187f4521e6080f93fc2a26bf91b4e7d64f94e18a",
            "title": "FedBPT: Efficient Federated Black-box Prompt Tuning for Large Language Models",
            "abstract": "Pre-trained language models (PLM) have revolutionized the NLP landscape, achieving stellar performances across diverse tasks. These models, while benefiting from vast training data, often require fine-tuning on specific data to cater to distinct downstream tasks. However, this data adaptation process has inherent security and privacy concerns, primarily when leveraging user-generated, device-residing data. Federated learning (FL) provides a solution, allowing collaborative model fine-tuning without centralized data collection. However, applying FL to finetune PLMs is hampered by challenges, including restricted model parameter access, high computational requirements, and communication overheads. This paper introduces Federated Black-box Prompt Tuning (FedBPT), a framework designed to address these challenges. FedBPT does not require the clients to access the model parameters. By focusing on training optimal prompts and utilizing gradient-free optimization methods, FedBPT reduces the number of exchanged variables, boosts communication efficiency, and minimizes computational and storage costs. Experiments highlight the framework's ability to drastically cut communication and memory costs while maintaining competitive performance. Ultimately, FedBPT presents a promising solution for efficient, privacy-preserving fine-tuning of PLM in the age of large language models.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Federated Black-box Prompt Tuning (FedBPT) is introduced, a framework designed to address challenges of fine-tuning of PLM in the age of large language models that reduces the number of exchanged variables, boosts communication efficiency, and minimizes computational and storage costs."
            },
            "score": 6
        },
        {
            "id": "ab90da70bf4671bb95d8e7ff97b2cce19768c579",
            "paperId": "ab90da70bf4671bb95d8e7ff97b2cce19768c579",
            "title": "Efficient Federated Prompt Tuning for Black-box Large Pre-trained Models",
            "abstract": "With the blowout development of pre-trained models (PTMs), the efficient tuning of these models for diverse downstream applications has emerged as a pivotal research concern. Although recent investigations into prompt tuning have provided promising avenues, three salient challenges persist: (1) memory constraint: the continuous growth in the size of open-source PTMs renders fine-tuning, even a fraction of their parameters, challenging for many practitioners. (2) model privacy: existing PTMs often function as public API services, with their parameters inaccessible for effective or tailored fine-tuning. (3) data privacy: the fine-tuning of PTMs necessitates high-quality datasets, which are typically localized and not shared to public. To optimally harness each local dataset while navigating memory constraints and preserving privacy, we propose Federated Black-Box Prompt Tuning (Fed-BBPT). This innovative approach eschews reliance on parameter architectures and private dataset access, instead capitalizing on a central server that aids local users in collaboratively training a prompt generator through regular aggregation. Local users leverage API-driven learning via a zero-order optimizer, obviating the need for PTM deployment. Relative to extensive fine-tuning, Fed-BBPT proficiently sidesteps memory challenges tied to PTM storage and fine-tuning on local machines, tapping into comprehensive, high-quality, yet private training datasets. A thorough evaluation across 40 datasets spanning CV and NLP tasks underscores the robustness of our proposed model.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Federated Black-Box Prompt Tuning proficiently sidesteps memory challenges tied to PTM storage and fine-tuning on local machines, tapping into comprehensive, high-quality, yet private training datasets."
            },
            "score": 6
        },
        {
            "id": "e25b9330e01b667e784003373d1b9cdbd3521c7e",
            "paperId": "e25b9330e01b667e784003373d1b9cdbd3521c7e",
            "title": "Privacy Preserving Large Language Models: ChatGPT Case Study Based Vision and Framework",
            "abstract": "The generative Artificial Intelligence (AI) tools based on Large Language Models (LLMs) use billions of parameters to extensively analyse large datasets and extract critical private information such as, context, specific details, identifying information etc. This have raised serious threats to user privacy and reluctance to use such tools. This article proposes the conceptual model called PrivChatGPT, a privacy-preserving model for LLMs that consists of two main components i.e., preserving user privacy during the data curation/pre-processing together with preserving private context and the private training process for large-scale data. To demonstrate its applicability, we show how a private mechanism could be integrated into the existing model for training LLMs to protect user privacy; specifically, we employed differential privacy and private training using Reinforcement Learning (RL). We measure the privacy loss and evaluate the measure of uncertainty or randomness once differential privacy is applied. It further recursively evaluates the level of privacy guarantees and the measure of uncertainty of public database and resources, during each update when new information is added for training purposes. To critically evaluate the use of differential privacy for private LLMs, we hypothetically compared other mechanisms e..g, Blockchain, private information retrieval, randomisation, for various performance measures such as the model performance and accuracy, computational complexity, privacy vs. utility etc. We conclude that differential privacy, randomisation, and obfuscation can impact utility and performance of trained models, conversely, the use of ToR, Blockchain, and PIR may introduce additional computational complexity and high training latency. We believe that the proposed model could be used as a benchmark for proposing privacy preserving LLMs for generative AI tools.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The conceptual model called PrivChatGPT, a privacy-preserving model for LLMs that consists of two main components i.e., preserving user privacy during the data curation/pre-processing together with preserving private context and the private training process for large-scale data is proposed."
            },
            "score": 5
        },
        {
            "id": "553d85e202fdbdd4101673b9205135b8eb94811d",
            "paperId": "553d85e202fdbdd4101673b9205135b8eb94811d",
            "title": "Recovering from Privacy-Preserving Masking with Large Language Models",
            "abstract": "Model adaptation is crucial to handle the discrepancy between proxy training data and actual users data received. To effectively perform adaptation, textual data of users is typically stored on servers or their local devices, where downstream natural language processing (NLP) models can be directly trained using such in-domain data. However, this might raise privacy and security concerns due to the extra risks of exposing user information to adversaries. Replacing identifying information in textual data with a generic marker has been recently explored. In this work, we leverage large language models (LLMs) to suggest substitutes of masked tokens and have their effectiveness evaluated on downstream language modeling tasks. Specifically, we propose multiple pre-trained and fine-tuned LLM-based approaches and perform empirical studies on various datasets for the comparison of these methods. Experimental results show that models trained on the obfuscation corpora are able to achieve comparable performance with the ones trained on the original data without privacy-preserving token masking.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results show that modelstrained on the obfuscation corpora are able to achieve comparable performance with the ones trained on the original data without privacy-preserving token masking, and have their effectiveness evaluated on downstream language modeling tasks."
            },
            "score": 5
        },
        {
            "id": "e9e746956d7f22bd844d2975a48d0ff8100e4bf2",
            "paperId": "e9e746956d7f22bd844d2975a48d0ff8100e4bf2",
            "title": "Privacy-Preserving Language Model Inference with Instance Obfuscation",
            "abstract": "Language Models as a Service (LMaaS) offers convenient access for developers and researchers to perform inference using pre-trained language models. Nonetheless, the input data and the inference results containing private information are exposed as plaintext during the service call, leading to privacy issues. Recent studies have started tackling the privacy issue by transforming input data into privacy-preserving representation from the user-end with the techniques such as noise addition and content perturbation, while the exploration of inference result protection, namely decision privacy, is still a blank page. In order to maintain the black-box manner of LMaaS, conducting data privacy protection, especially for the decision, is a challenging task because the process has to be seamless to the models and accompanied by limited communication and computation overhead. We thus propose Instance-Obfuscated Inference (IOI) method, which focuses on addressing the decision privacy issue of natural language understanding tasks in their complete life-cycle. Besides, we conduct comprehensive experiments to evaluate the performance as well as the privacy-protection strength of the proposed method on various benchmarking tasks.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Instance-Obfuscated Inference (IOI) method is proposed, which focuses on addressing the decision privacy issue of natural language understanding tasks in their complete life-cycle and conducts comprehensive experiments to evaluate the performance as well as the privacy-protection strength of the proposed method on various benchmarking tasks."
            },
            "score": 5
        },
        {
            "id": "0a5ca7649ab378cc8c734ddac5bf6c6c00f086c1",
            "paperId": "0a5ca7649ab378cc8c734ddac5bf6c6c00f086c1",
            "title": "CodexLeaks: Privacy Leaks from Code Generation Language Models in GitHub Copilot",
            "abstract": "Code generation language models are trained on billions of lines of source code to provide code generation and auto-completion features, like those offered by code assistant GitHub Copilot with more than a million users. These datasets may contain sensitive personal information\u2014personally identifiable, private, or secret\u2014that these models may regurgitate. This paper introduces and evaluates a semi-automated pipeline for extracting sensitive personal information from the Codex model used in GitHub Copilot. We employ carefully-designed templates to construct prompts that are more likely to result in privacy leaks. To overcome the non-public training data, we propose a semi-automated filtering method using a blind membership inference attack. We validate the effectiveness of our membership inference approach on different code generation models. We utilize hit rate through the GitHub Search API as a distinguishing heuristic followed by human-in-the-loop evaluation, uncovering that approximately 8% (43) of the prompts yield privacy leaks. Notably, we observe that the model tends to produce indirect leaks, compromising privacy as contextual integrity by generating information from individuals closely related to the queried subject in the training corpus.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A semi-automated pipeline for extracting sensitive personal information from the Codex model used in GitHub Copilot is introduced and it is observed that the model tends to produce indirect leaks, compromising privacy as contextual integrity by generating information from individuals closely related to the queried subject in the training corpus."
            },
            "score": 5
        },
        {
            "id": "c5120b546f1bd99df5bd2e2bf44db5c7c46d1545",
            "paperId": "c5120b546f1bd99df5bd2e2bf44db5c7c46d1545",
            "title": "Pretraining Language Models with Human Preferences",
            "abstract": "Language models (LMs) are pretrained to imitate internet text, including content that would violate human preferences if generated by an LM: falsehoods, offensive comments, personally identifiable information, low-quality or buggy code, and more. Here, we explore alternative objectives for pretraining LMs in a way that also guides them to generate text aligned with human preferences. We benchmark five objectives for pretraining with human feedback across three tasks and study how they affect the trade-off between alignment and capabilities of pretrained LMs. We find a Pareto-optimal and simple approach among those we explored: conditional training, or learning distribution over tokens conditional on their human preference scores given by a reward model. Conditional training reduces the rate of undesirable content by up to an order of magnitude, both when generating without a prompt and with an adversarially-chosen prompt. Moreover, conditional training maintains the downstream task performance of standard LM pretraining, both before and after task-specific finetuning. Pretraining with human feedback results in much better preference satisfaction than standard LM pretraining followed by finetuning with feedback, i.e., learning and then unlearning undesirable behavior. Our results suggest that we should move beyond imitation learning when pretraining LMs and incorporate human preferences from the start of training.",
            "year": 2023,
            "citationCount": 96,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results suggest that the authors should move beyond imitation learning when pretraining LMs and incorporate human preferences from the start of training, i.e., learning and then unlearning undesirable behavior."
            },
            "score": 5
        },
        {
            "id": "deb8f26509ae320fc975b32922416cb156c61bbd",
            "paperId": "deb8f26509ae320fc975b32922416cb156c61bbd",
            "title": "Emergent and Predictable Memorization in Large Language Models",
            "abstract": "Memorization, or the tendency of large language models (LLMs) to output entire sequences from their training data verbatim, is a key concern for safely deploying language models. In particular, it is vital to minimize a model's memorization of sensitive datapoints such as those containing personal identifiable information (PII). The prevalence of such undesirable memorization can pose issues for model trainers, and may even require discarding an otherwise functional model. We therefore seek to predict which sequences will be memorized before a large model's full train-time by extrapolating the memorization behavior of lower-compute trial runs. We measure memorization of the Pythia model suite and plot scaling laws for forecasting memorization, allowing us to provide equi-compute recommendations to maximize the reliability (recall) of such predictions. We additionally provide further novel discoveries on the distribution of memorization scores across models and data. We release all code and data necessary to reproduce the results in this paper at https://github.com/EleutherAI/pythia",
            "year": 2023,
            "citationCount": 53,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work seeks to predict which sequences will be memorized before a large model's full train-time by extrapolating the memorization behavior of lower-compute trial runs, and measures memorization of the Pythia model suite and plot scaling laws for forecasting memorization, allowing for equi-computes recommendations to maximize the reliability of such predictions."
            },
            "score": 5
        },
        {
            "id": "26a8832c313d3fa9820e3de2d738b04d7ed61060",
            "paperId": "26a8832c313d3fa9820e3de2d738b04d7ed61060",
            "title": "Privacy in the Time of Language Models",
            "abstract": "Pretrained large language models (LLMs) have consistently shown state-of-the-art performance across multiple natural language processing (NLP) tasks. These models are of much interest for a variety of industrial applications that use NLP as a core component. However, LLMs have also been shown to memorize portions of their training data, which can contain private information. Therefore, when building and deploying LLMs, it is of value to apply privacy-preserving techniques that protect sensitive data. In this talk, we discuss privacy measurement and preservation techniques for LLMs that can be applied in the context of industrial applications and present case studies of preliminary solutions. We discuss select strategies and metrics relevant for measuring memorization in LLMs that can, in turn, be used to measure privacy-risk in these models. We then discuss privacy-preservation techniques that can be applied at different points of the LLM training life-cycle; including our work on an algorithm for fine-tuning LLMs with improved privacy. In addition, we discuss our work on privacy-preserving solutions that can be applied to LLMs during inference and are feasible for use at run time.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This talk discusses privacy measurement and preservation techniques for LLMs that can be applied in the context of industrial applications and presents case studies of preliminary solutions, including work on an algorithm for fine-tuning LLMs with improved privacy."
            },
            "score": 5
        },
        {
            "id": "cf08fa8e17e4cfe44ef728dccc296467eb5b88b4",
            "paperId": "cf08fa8e17e4cfe44ef728dccc296467eb5b88b4",
            "title": "Assessing Privacy Risks in Language Models: A Case Study on Summarization Tasks",
            "abstract": "Large language models have revolutionized the field of NLP by achieving state-of-the-art performance on various tasks. However, there is a concern that these models may disclose information in the training data. In this study, we focus on the summarization task and investigate the membership inference (MI) attack: given a sample and black-box access to a model's API, it is possible to determine if the sample was part of the training data. We exploit text similarity and the model's resistance to document modifications as potential MI signals and evaluate their effectiveness on widely used datasets. Our results demonstrate that summarization models are at risk of exposing data membership, even in cases where the reference summary is not available. Furthermore, we discuss several safeguards for training summarization models to protect against MI attacks and discuss the inherent trade-off between privacy and utility.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study focuses on the summarization task and investigates the membership inference attack: given a sample and black-box access to a model's API, it is possible to determine if the sample was part of the training data."
            },
            "score": 5
        },
        {
            "id": "d5f4ecbb3fc2220eed7c62ea308e4f6cba2240b5",
            "paperId": "d5f4ecbb3fc2220eed7c62ea308e4f6cba2240b5",
            "title": "Beyond Memorization: Violating Privacy Via Inference with Large Language Models",
            "abstract": "Current privacy research on large language models (LLMs) primarily focuses on the issue of extracting memorized training data. At the same time, models' inference capabilities have increased drastically. This raises the key question of whether current LLMs could violate individuals' privacy by inferring personal attributes from text given at inference time. In this work, we present the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from text. We construct a dataset consisting of real Reddit profiles, and show that current LLMs can infer a wide range of personal attributes (e.g., location, income, sex), achieving up to $85\\%$ top-1 and $95.8\\%$ top-3 accuracy at a fraction of the cost ($100\\times$) and time ($240\\times$) required by humans. As people increasingly interact with LLM-powered chatbots across all aspects of life, we also explore the emerging threat of privacy-invasive chatbots trying to extract personal information through seemingly benign questions. Finally, we show that common mitigations, i.e., text anonymization and model alignment, are currently ineffective at protecting user privacy against LLM inference. Our findings highlight that current LLMs can infer personal data at a previously unattainable scale. In the absence of working defenses, we advocate for a broader discussion around LLM privacy implications beyond memorization, striving for a wider privacy protection.",
            "year": 2023,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents the first comprehensive study on the capabilities of pretrained LLMs to infer personal attributes from text, and highlights that current LLMs can infer personal data at a previously unattainable scale."
            },
            "score": 5
        },
        {
            "id": "c6fbe66f962908a6cf7cd771268d9a161000ce4d",
            "paperId": "c6fbe66f962908a6cf7cd771268d9a161000ce4d",
            "title": "Quantifying Association Capabilities of Large Language Models and Its Implications on Privacy Leakage",
            "abstract": "The advancement of large language models (LLMs) brings notable improvements across various applications, while simultaneously raising concerns about potential private data exposure. One notable capability of LLMs is their ability to form associations between different pieces of information, but this raises concerns when it comes to personally identifiable information (PII). This paper delves into the association capabilities of language models, aiming to uncover the factors that influence their proficiency in associating information. Our study reveals that as models scale up, their capacity to associate entities/information intensifies, particularly when target pairs demonstrate shorter co-occurrence distances or higher co-occurrence frequencies. However, there is a distinct performance gap when associating commonsense knowledge versus PII, with the latter showing lower accuracy. Despite the proportion of accurately predicted PII being relatively small, LLMs still demonstrate the capability to predict specific instances of email addresses and phone numbers when provided with appropriate prompts. These findings underscore the potential risk to PII confidentiality posed by the evolving capabilities of LLMs, especially as they continue to expand in scale and power.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper delves into the association capabilities of language models, aiming to uncover the factors that influence their proficiency in associating information, and reveals that as models scale up, their capacity to associate entities/information intensifies, particularly when target pairs demonstrate shorter co-occurrence distances or higher co-Occurrence frequencies."
            },
            "score": 5
        },
        {
            "id": "3954e59506fd6a1e199f69e254fc2d1c25e7450d",
            "paperId": "3954e59506fd6a1e199f69e254fc2d1c25e7450d",
            "title": "Reduce Communication Costs and Preserve Privacy: Prompt Tuning Method in Federated Learning",
            "abstract": "Federated learning (FL) has enabled global model training on decentralized data in a privacy-preserving way by aggregating model updates. However, for many natural language processing (NLP) tasks that utilize pre-trained language models (PLMs) with large numbers of parameters, there are considerable communication costs associated with FL. Recently, prompt tuning, which tunes some soft prompts without modifying PLMs, has achieved excellent performance as a new learning paradigm. Therefore we want to combine the two methods and explore the effect of prompt tuning under FL. In this paper, we propose \u201dFedPrompt\u201d as the \ufb01rst work study prompt tuning in a model split learning way using FL, and prove that split learning greatly reduces the communication cost, only 0.01% of the PLMs\u2019 parameters, with little decrease on accuracy both on IID and Non-IID data distribution. This improves the ef\ufb01ciency of FL method while also protecting the data privacy in prompt tuning. In addition, like PLMs, prompts are uploaded and downloaded between public platforms and personal users, so we try to \ufb01gure out whether there is still a backdoor threat using only soft prompt in FL scenarios. We further conduct backdoor attacks by data poisoning on FedPrompt. Our experiments show that normal backdoor attack can not achieve a high attack success rate, proving the robustness of FedPrompt. We hope this work can promote the application of prompt in FL and raise the awareness of the possible security threats.",
            "year": 2022,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is proved that split learning greatly reduces the communication cost, only 0.01% of the PLMs\u2019 parameters, with little decrease on accuracy both on IID and Non-IID data distribution, which improves theency of FL method while also protecting the data privacy in prompt tuning."
            },
            "score": 5
        },
        {
            "id": "16fee2acddbaa4b58f39d1cbba12961178186e7c",
            "paperId": "16fee2acddbaa4b58f39d1cbba12961178186e7c",
            "title": "Safeguarding Ethical AI: Detecting Potentially Sensitive Data Re-Identification and Generation of Misleading or Abusive Content from Quantized Large Language Models",
            "abstract": ": Research on privacy-preserving Machine Learning (ML) is essential to prevent the re-identification of health data ensuring the confidentiality and security of sensitive patient information. In this era of unprecedented usage of large language models (LLMs), LLMs carry inherent risks when applied to sensitive data, especially as LLMs are trained on trillions of words from the internet, without a global standard for data selection. The lack of standardization in training LLMs poses a significant risk in the field of health informatics, potentially resulting in the inadvertent release of sensitive information, despite the availability of context-aware redaction of sensitive information. The research goal of this paper is to determine whether sensitive information could be re-identified from electronic health records during Natural Language Processing (NLP) tasks such as text classification without using any dedicated re-identification techniques. We performed zero and 8-shot learning with the quantized LLM models FLAN, Llama2, Mistral, and Vicuna for classifying social context data extracted from MIMIC-III. In this text classification task, our focus was on detecting potential sensitive data re-identification and the generation of misleading or abusive content during the fine-tuning and prompting stages of the process, along with evaluating the performance of the classification.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This research goal is to determine whether sensitive information could be re-identified from electronic health records during Natural Language Processing tasks such as text classification without using any dedicated re-identification techniques."
            },
            "score": 5
        },
        {
            "id": "457b945ad7868b674222791d699f933be6fca58e",
            "paperId": "457b945ad7868b674222791d699f933be6fca58e",
            "title": "Privacy-Preserving Federated Learning through Clustered Sampling on Fine-Tuning Distributed non-iid Large Language Models",
            "abstract": "Recently, Large Language Models (LLMs) have been a phenomenal trend in the Artificial intelligence field. However, training and fine-tuning can be challenging because of privacy concerns and limited computing resources. Federated Learning (FL) has emerged as a novel machine learning framework offering privacy protection. The challenges in applying FL to real-world applications include dealing with heterogeneous data, poor client updates, and client selection. This paper introduces Privacy-preserving Federated Learning through Clustered Sampling on LLMs (FCLM), a framework that clusters models by their distribution similarity. It helps the model group similar models to improve text data heterogeneity handling and privacy concerns in distributed machine-learning environments. The FCLM framework is implemented and evaluated using popular Language models and text data. The framework shows a robust performance over the heterogeneous text data, which can further extend to the use of more complex LLMs.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Privacy-preserving Federated Learning through Clustered Sampling on LLMs (FCLM), a framework that clusters models by their distribution similarity to improve text data heterogeneity handling and privacy concerns in distributed machine-learning environments is introduced."
            },
            "score": 4
        },
        {
            "id": "4fef6fc97b194a8ba3dddc8ae6b5d6a632dbf778",
            "paperId": "4fef6fc97b194a8ba3dddc8ae6b5d6a632dbf778",
            "title": "Client Availability Aware Privacy-Preserving Federated Learning for Language Models",
            "abstract": "Language Models (LMs) have significantly advanced various natural language processing applications but pose challenges related to data privacy and the need for massive computational resources. Federated Learning (FL) decentralizes the training across multiple devices, enhancing data privacy. Yet, real-world FL faces complexities stemming from inconsistent client availability, especially in devices affected by factors like usage patterns and timezone correlations. Recent frameworks like CA-Fed have addressed client availability discrepancies, but they are less effective when applied to the nuanced domain of LMs. In this context, we introduce Fed-Cab, a novel Client Availability Aware Privacy-preserving Federated Learning framework tailored for LMs. Leveraging finite-state Markov chains, Fed-Cab simulates real-world client availability. Our framework introduces a novel dynamic weighting mechanism that understands and capitalizes on client availability, placing emphasis on clients that aid optimal convergence. By managing client contributions and introducing an innovative communication design, Fed-Cab efficiently captures client availability, determines weights, and dynamically selects contributors and updates models. Our results indicate that Fed-Cab not only maintains high accuracy but also excels in training speed and consistency over time.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces Fed-Cab, a novel Client Availability Aware Privacy-preserving Federated Learning framework tailored for LMs that introduces a novel dynamic weighting mechanism that understands and capitalizes on client availability, placing emphasis on clients that aid optimal convergence."
            },
            "score": 4
        },
        {
            "id": "977bfb905183a2ad0a5433efbb84086a87140c67",
            "paperId": "977bfb905183a2ad0a5433efbb84086a87140c67",
            "title": "SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for Large Language Models",
            "abstract": "With the growing use of large language models hosted on cloud platforms to offer inference services, privacy concerns are escalating, especially concerning sensitive data like investment plans and bank account details. Secure Multi-Party Computing (SMPC) emerges as a promising solution to protect the privacy of inference data and model parameters. However, the application of SMPC in Privacy-Preserving Inference (PPI) for large language models, particularly those based on the Transformer architecture, often leads to considerable slowdowns or declines in performance. This is largely due to the multitude of nonlinear operations in the Transformer architecture, which are not well-suited to SMPC and difficult to circumvent or optimize effectively. To address this concern, we introduce an advanced optimization framework called SecFormer, to achieve fast and accurate PPI for Transformer models. By implementing model design optimization, we successfully eliminate the high-cost exponential and maximum operations in PPI without sacrificing model performance. Additionally, we have developed a suite of efficient SMPC protocols that utilize segmented polynomials, Fourier series and Goldschmidt's method to handle other complex nonlinear functions within PPI, such as GeLU, LayerNorm, and Softmax. Our extensive experiments reveal that SecFormer outperforms MPCFormer in performance, showing improvements of $5.6\\%$ and $24.2\\%$ for BERT$_{\\text{BASE}}$ and BERT$_{\\text{LARGE}}$, respectively. In terms of efficiency, SecFormer is 3.56 and 3.58 times faster than Puma for BERT$_{\\text{BASE}}$ and BERT$_{\\text{LARGE}}$, demonstrating its effectiveness and speed.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces an advanced optimization framework called SecFormer, to achieve fast and accurate PPI for Transformer models, and successfully eliminates the high-cost exponential and maximum operations in PPI without sacrificing model performance."
            },
            "score": 4
        },
        {
            "id": "5661b4387a62b31bc02e4566330e71522b4397ab",
            "paperId": "5661b4387a62b31bc02e4566330e71522b4397ab",
            "title": "Privacy-preserving Fine-tuning of Large Language Models through Flatness",
            "abstract": "The privacy concerns associated with the use of Large Language Models (LLMs) have grown recently with the development of LLMs such as ChatGPT. Differential Privacy (DP) techniques are explored in existing work to mitigate their privacy risks at the cost of generalization degradation. Our paper reveals that the flatness of DP-trained models' loss landscape plays an essential role in the trade-off between their privacy and generalization. We further propose a holistic framework to enforce appropriate weight flatness, which substantially improves model generalization with competitive privacy preservation. It innovates from three coarse-to-grained levels, including perturbation-aware min-max optimization on model weights within a layer, flatness-guided sparse prefix-tuning on weights across layers, and weight knowledge distillation between DP \\&non-DP weights copies. Comprehensive experiments of both black-box and white-box scenarios are conducted to demonstrate the effectiveness of our proposal in enhancing generalization and maintaining DP characteristics. For instance, on text classification dataset QNLI, DP-Flat achieves similar performance with non-private full fine-tuning but with DP guarantee under privacy budget $\\epsilon=3$, and even better performance given higher privacy budgets. Codes are provided in the supplement.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper reveals that the flatness of DP-trained models' loss landscape plays an essential role in the trade-off between their privacy and generalization, and proposes a holistic framework to enforce appropriate weight flatness, which substantially improves model generalization with competitive privacy preservation."
            },
            "score": 4
        },
        {
            "id": "597cad6c7b9de94eecc153c7cdcaf824905fe915",
            "paperId": "597cad6c7b9de94eecc153c7cdcaf824905fe915",
            "title": "You Are What You Write: Preserving Privacy in the Era of Large Language Models",
            "abstract": "Large scale adoption of large language models has introduced a new era of convenient knowledge transfer for a slew of natural language processing tasks. However, these models also run the risk of undermining user trust by exposing unwanted information about the data subjects, which may be extracted by a malicious party, e.g. through adversarial attacks. We present an empirical investigation into the extent of the personal information encoded into pre-trained representations by a range of popular models, and we show a positive correlation between the complexity of a model, the amount of data used in pre-training, and data leakage. In this paper, we present the first wide coverage evaluation and comparison of some of the most popular privacy-preserving algorithms, on a large, multi-lingual dataset on sentiment analysis annotated with demographic information (location, age and gender). The results show since larger and more complex models are more prone to leaking private information, use of privacy-preserving methods is highly desirable. We also find that highly privacy-preserving technologies like differential privacy (DP) can have serious model utility effects, which can be ameliorated using hybrid or metric-DP techniques.",
            "year": 2022,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents the first wide coverage evaluation and comparison of some of the most popular privacy-preserving algorithms, on a large, multi-lingual dataset on sentiment analysis annotated with demographic information (location, age and gender)."
            },
            "score": 4
        },
        {
            "id": "17a6116e5bbd8b87082cbb2e795885567300c483",
            "paperId": "17a6116e5bbd8b87082cbb2e795885567300c483",
            "title": "Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting",
            "abstract": "As large language models (LLMs) are adopted as a fundamental component of language technologies, it is crucial to accurately characterize their performance. Because choices in prompt design can strongly influence model behavior, this design process is critical in effectively using any modern pre-trained generative language model. In this work, we focus on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting. We find that several widely used open-source LLMs are extremely sensitive to subtle changes in prompt formatting in few-shot settings, with performance differences of up to 76 accuracy points when evaluated using LLaMA-2-13B. Sensitivity remains even when increasing model size, the number of few-shot examples, or performing instruction tuning. Our analysis suggests that work evaluating LLMs with prompting-based methods would benefit from reporting a range of performance across plausible prompt formats, instead of the currently-standard practice of reporting performance on a single format. We also show that format performance only weakly correlates between models, which puts into question the methodological validity of comparing models with an arbitrarily chosen, fixed prompt format. To facilitate systematic analysis we propose FormatSpread, an algorithm that rapidly evaluates a sampled set of plausible prompt formats for a given task, and reports the interval of expected performance without accessing model weights. Furthermore, we present a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats.",
            "year": 2023,
            "citationCount": 63,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work focuses on LLM sensitivity to a quintessential class of meaning-preserving design choices: prompt formatting, and presents a suite of analyses that characterize the nature of this sensitivity, including exploring the influence of particular atomic perturbations and the internal representation of particular formats."
            },
            "score": 4
        },
        {
            "id": "de6fd09ed7783b95af7e5f4088a70d5b0244f5aa",
            "paperId": "de6fd09ed7783b95af7e5f4088a70d5b0244f5aa",
            "title": "Improving large language models for clinical named entity recognition via prompt engineering.",
            "abstract": "IMPORTANCE\nThe study highlights the potential of large language models, specifically GPT-3.5 and GPT-4, in processing complex clinical data and extracting meaningful information with minimal training data. By developing and refining prompt-based strategies, we can significantly enhance the models' performance, making them viable tools for clinical NER tasks and possibly reducing the reliance on extensive annotated datasets.\n\n\nOBJECTIVES\nThis study quantifies the capabilities of GPT-3.5 and GPT-4 for clinical named entity recognition (NER) tasks and proposes task-specific prompts to improve their performance.\n\n\nMATERIALS AND METHODS\nWe evaluated these models on 2 clinical NER tasks: (1) to extract medical problems, treatments, and tests from clinical notes in the MTSamples corpus, following the 2010 i2b2 concept extraction shared task, and (2) to identify nervous system disorder-related adverse events from safety reports in the vaccine adverse event reporting system (VAERS). To improve the GPT models' performance, we developed a clinical task-specific prompt framework that includes (1) baseline prompts with task description and format specification, (2) annotation guideline-based prompts, (3) error analysis-based instructions, and (4) annotated samples for few-shot learning. We assessed each prompt's effectiveness and compared the models to BioClinicalBERT.\n\n\nRESULTS\nUsing baseline prompts, GPT-3.5 and GPT-4 achieved relaxed F1 scores of 0.634, 0.804 for MTSamples and 0.301, 0.593 for VAERS. Additional prompt components consistently improved model performance. When all 4 components were used, GPT-3.5 and GPT-4 achieved relaxed F1 socres of 0.794, 0.861 for MTSamples and 0.676, 0.736 for VAERS, demonstrating the effectiveness of our prompt framework. Although these results trail BioClinicalBERT (F1 of 0.901 for the MTSamples dataset and 0.802 for the VAERS), it is very promising considering few training samples are needed.\n\n\nDISCUSSION\nThe study's findings suggest a promising direction in leveraging LLMs for clinical NER tasks. However, while the performance of GPT models improved with task-specific prompts, there's a need for further development and refinement. LLMs like GPT-4 show potential in achieving close performance to state-of-the-art models like BioClinicalBERT, but they still require careful prompt engineering and understanding of task-specific knowledge. The study also underscores the importance of evaluation schemas that accurately reflect the capabilities and performance of LLMs in clinical settings.\n\n\nCONCLUSION\nWhile direct application of GPT models to clinical NER tasks falls short of optimal performance, our task-specific prompt framework, incorporating medical knowledge and training samples, significantly enhances GPT models' feasibility for potential clinical applications.",
            "year": 2023,
            "citationCount": 43,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A clinical task-specific prompt framework, incorporating medical knowledge and training samples, significantly enhances GPT models' feasibility for potential clinical applications and suggests a promising direction in leveraging LLMs for clinical NER tasks."
            },
            "score": 4
        },
        {
            "id": "b67eb8213a63be8a4b0274728ffdc50bfa109e10",
            "paperId": "b67eb8213a63be8a4b0274728ffdc50bfa109e10",
            "title": "XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models",
            "abstract": "Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content. This risk motivates safety efforts such as red-teaming and large-scale feedback learning, which aim to make models both helpful and harmless. However, there is a tension between these two objectives, since harmlessness requires models to refuse to comply with unsafe prompts, and thus not be helpful. Recent anecdotal evidence suggests that some models may have struck a poor balance, so that even clearly safe prompts are refused if they use similar language to unsafe prompts or mention sensitive topics. In this paper, we introduce a new test suite called XSTest to identify such eXaggerated Safety behaviours in a systematic way. XSTest comprises 250 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with, and 200 unsafe prompts as contrasts that models, for most applications, should refuse. We describe XSTest's creation and composition, and then use the test suite to highlight systematic failure modes in state-of-the-art language models as well as more general challenges in building safer language models.",
            "year": 2023,
            "citationCount": 26,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new test suite called XSTest is introduced to identify eXaggerated Safety behaviours in state-of-the-art language models as well as more general challenges in building safer language models."
            },
            "score": 4
        },
        {
            "id": "e0384ba36555232c587d4a80d527895a095a9001",
            "paperId": "e0384ba36555232c587d4a80d527895a095a9001",
            "title": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
            "abstract": "Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the Hallucination Evaluation benchmark for Large Language Models (HaluEval), a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing hallucination. To generate these samples, we propose a ChatGPT-based two-step framework, i.e., sampling-then-filtering. Besides, we also hire some human labelers to annotate the hallucinations in ChatGPT responses. The empirical results suggest that ChatGPT is likely to generate hallucinated content in specific topics by fabricating unverifiable information (i.e., about $19.5\\%$ responses). Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts. However, our experiments also prove that providing external knowledge or adding reasoning steps can help LLMs recognize hallucinations. Our benchmark can be accessed at https://github.com/RUCAIBox/HaluEval.",
            "year": 2023,
            "citationCount": 57,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Hallucination Evaluation benchmark for Large Language Models (HaluEval), a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing hallucination, is introduced and it is proved that providing external knowledge or adding reasoning steps can help LLMs recognize hallucinations."
            },
            "score": 4
        },
        {
            "id": "1819c5c8c556db205ec9a49d5ba4a8c2afb28a4a",
            "paperId": "1819c5c8c556db205ec9a49d5ba4a8c2afb28a4a",
            "title": "Preventing Verbatim Memorization in Language Models Gives a False Sense of Privacy",
            "abstract": "Studying data memorization in neural language models helps us understand the risks (e.g., to privacy or copyright) associated with models regurgitating training data, and aids in the evaluation of potential countermeasures. Many prior works\u2014and some recently deployed defenses\u2014focus on \u201cverbatim memo-rization\u201d, de\ufb01ned as a model generation that exactly matches a substring from the training set. We argue that verbatim memorization def-initions are too restrictive and fail to capture more subtle forms of memorization. Speci\ufb01cally, we design and implement an ef\ufb01cient defense based on Bloom \ufb01lters that perfectly prevents all verbatim memorization. And yet, we demonstrate that this \u201cperfect\u201d \ufb01lter does not prevent the leakage of training data. Indeed, it is easily circumvented by plausible and mini-mally modi\ufb01ed \u201cstyle-transfer\u201d prompts\u2014and in some cases even the non-modi\ufb01ed original prompts\u2014to extract memorized information. For example, instructing the model to output ALL-CAPITAL texts bypasses memorization checks based on verbatim matching. We conclude by discussing potential alternative de\ufb01nitions and why de\ufb01ning memorization is a dif\ufb01cult yet crucial open question for neural language models.",
            "year": 2022,
            "citationCount": 58,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is argued that verbatim memorization def-initions are too restrictive and fail to capture more subtle forms of memorization, and potential alternative de\ufb01nitions are discussed and why memorization is a crucial open question for neural language models."
            },
            "score": 4
        },
        {
            "id": "75c68b6fb45e6f41f8e6419414e474da445ea36d",
            "paperId": "75c68b6fb45e6f41f8e6419414e474da445ea36d",
            "title": "Preventing Generation of Verbatim Memorization in Language Models Gives a False Sense of Privacy",
            "abstract": "Studying data memorization in neural language models helps us understand the risks (e.g., to privacy or copyright) associated with models regurgitating training data and aids in the development of countermeasures. Many prior works\u2014and some recently deployed defenses\u2014focus on \u201cverbatim memorization\u201d, defined as a model generation that exactly matches a substring from the training set. We argue that verbatim memorization definitions are too restrictive and fail to capture more subtle forms of memorization. Specifically, we design and implement an efficient defense that _perfectly_ prevents all verbatim memorization. And yet, we demonstrate that this \u201cperfect\u201d filter does not prevent the leakage of training data. Indeed, it is easily circumvented by plausible and minimally modified \u201cstyle-transfer\u201d prompts\u2014and in some cases even the non-modified original prompts\u2014to extract memorized information. We conclude by discussing potential alternative definitions and why defining memorization is a difficult yet crucial open question for neural language models.",
            "year": 2022,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that this \u201cperfect\u201d filter does not prevent the leakage of training data, and is easily circumvented by plausible and minimally modified \u201cstyle-transfer\u201d prompts\u2014and in some cases even the non-modified original prompts\u2014to extract memorized information."
            },
            "score": 4
        },
        {
            "id": "b04b1771ebbf35027d39835e08ffab7bb81a5dd8",
            "paperId": "b04b1771ebbf35027d39835e08ffab7bb81a5dd8",
            "title": "Catch Me If You Can: Deceiving Stance Detection and Geotagging Models to Protect Privacy of Individuals on Twitter",
            "abstract": "The recent advances in natural language processing have yielded many exciting developments in text analysis and language understanding models; however, these models can also be used to track people, bringing severe privacy concerns. In this work, we investigate what individuals can do to avoid being\ndetected by those models while using social media platforms. We ground our investigation in two exposure-risky\ntasks, stance detection and geotagging. We explore a variety of simple techniques for modifying text, such as inserting typos\nin salient words, paraphrasing, and adding dummy social media posts. Our experiments show that the performance of\nBERT-based models fine-tuned for stance detection decreases significantly due to typos, but it is not affected by paraphrasing.\nMoreover, we find that typos have minimal impact on state-of-the-art geotagging models due to their increased reliance\non social networks; however, we show that users can deceive those models by interacting with different users, reducing\ntheir performance by almost 50%.",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores a variety of simple techniques for modifying text, such as inserting typos in salient words, paraphrasing, and adding dummy social media posts, and finds that typos have minimal impact on state-of-the-art geotagging models due to their increased reliance on social networks."
            },
            "score": 4
        },
        {
            "id": "1ec17ca3b6ee13a7f24ea5400c803d8f9d78f87b",
            "paperId": "1ec17ca3b6ee13a7f24ea5400c803d8f9d78f87b",
            "title": "A privacy-preserving distributed filtering framework for NLP artifacts",
            "abstract": null,
            "year": 2019,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work demonstrates the feasibility of using homomorphic encryption to develop a secure and efficient multi-party protocol based on private set intersection and secure thresholding to identify uncommon and low-frequency terms, which can be used to guide sentence filtering."
            },
            "score": 4
        },
        {
            "id": "341546ab4ff1945b004d18300749419c3896c6c9",
            "paperId": "341546ab4ff1945b004d18300749419c3896c6c9",
            "title": "Federated Few-Shot Learning for Mobile NLP",
            "abstract": "Natural language processing (NLP) sees rich mobile applications. To support various language understanding tasks, a foundation NLP model is often fine-tuned in a federated, privacy-preserving setting (FL). This process currently relies on at least hundreds of thousands of labeled training samples from mobile clients; yet mobile users often lack willingness or knowledge to label their data. Such an inadequacy of data labels is known as a few-shot scenario; it becomes the key blocker for mobile NLP applications. For the first time, this work investigates federated NLP in the few-shot scenario (FedFSL). By retrofitting algorithmic advances of pseudo labeling and prompt learning, we first establish a training pipeline that delivers competitive accuracy when only 0.05% (fewer than 100) of the training data is labeled and the remaining is unlabeled. To instantiate the workflow, we further present a system FeS1, addressing the high execution cost with novel designs: (1) Curriculum pacing, which injects pseudo labels to the training workflow at a rate commensurate to the learning progress; (2) Representational diversity, a mechanism for selecting the most learnable data, only for which pseudo labels will be generated; (3) Co-planning of a model's training depth and layer capacity. Together, these designs reduce the training delay, client energy, and network traffic by up to 46.0\u00d7, 41.2\u00d7 and 3000.0\u00d7, respectively. Through algorithm/system co-design, FeS demonstrates that FL can apply to challenging settings where most training samples are unlabeled.",
            "year": 2022,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates federated NLP in the few-shot scenario (FedFSL), and demonstrates that FL can apply to challenging settings where most training samples are unlabeled, through algorithm/system co-design."
            },
            "score": 4
        },
        {
            "id": "50bac2f46ba3d88a9c0aa4c87b362830448bfbdd",
            "paperId": "50bac2f46ba3d88a9c0aa4c87b362830448bfbdd",
            "title": "Federated NLP in Few-shot Scenarios",
            "abstract": "Natural language processing (NLP) sees rich mobile applications. To support various language understanding tasks, a foundation NLP model is often fine-tuned in a federated, privacy-preserving setting (FL). This process currently relies on at least hundreds of thousands of labeled training samples from mobile clients; yet mobile users often lack willingness or knowledge to label their data. Such an inadequacy of data labels is known as a few-shot scenario; it becomes the key blocker for mobile NLP applications. For the first time, this work investigates federated NLP in the few-shot scenario (FedFSL). By retrofitting algorithmic advances of pseudo labeling and prompt learning, we first establish a training pipeline that delivers competitive accuracy when only 0.05% (fewer than 100) of the training data is labeled and the remaining is unlabeled. To instantiate the workflow, we further present a system FFNLP , addressing the high execution cost with novel designs. (1) Curriculum pacing, which injects pseudo labels to the training workflow at a rate commensurate to the learning progress; (2) Representational diversity, a mechanism for selecting the most learnable data, only for which pseudo labels will be generated; (3) Co-planning of a model\u2019s training depth and layer capacity. Together, these designs reduce the training delay, client energy, and network traffic by up to 46.0 \u00d7 , 41.2 \u00d7 and 3000.0 \u00d7 , respectively. Through algorithm/system co-design, FFNLP demonstrates that FL can apply to challenging settings where most training samples are unlabeled.",
            "year": 2022,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates federated NLP in the few-shot scenario (FedFSL) by retrofitting algorithmic advances of pseudo labeling and prompt learning, and establishes a training pipeline that delivers competitive accuracy when only 0.05% of the training data is labeled and the remaining is unlabeled."
            },
            "score": 4
        },
        {
            "id": "0e1c60dc4119589bbbf02da26f73f4fd6330be4b",
            "paperId": "0e1c60dc4119589bbbf02da26f73f4fd6330be4b",
            "title": "Revolutionizing Cyber Threat Detection With Large Language Models: A Privacy-Preserving BERT-Based Lightweight Model for IoT/IIoT Devices",
            "abstract": "The field of Natural Language Processing (NLP) is currently undergoing a revolutionary transformation driven by the power of pre-trained Large Language Models (LLMs) based on groundbreaking Transformer architectures. As the frequency and diversity of cybersecurity attacks continue to rise, the importance of incident detection has significantly increased. IoT devices are expanding rapidly, resulting in a growing need for efficient techniques to autonomously identify network-based attacks in IoT networks with both high precision and minimal computational requirements. This paper presents SecurityBERT, a novel architecture that leverages the Bidirectional Encoder Representations from Transformers (BERT) model for cyber threat detection in IoT networks. During the training of SecurityBERT, we incorporated a novel privacy-preserving encoding technique called Privacy-Preserving Fixed-Length Encoding (PPFLE). We effectively represented network traffic data in a structured format by combining PPFLE with the Byte-level Byte-Pair Encoder (BBPE) Tokenizer. Our research demonstrates that SecurityBERT outperforms traditional Machine Learning (ML) and Deep Learning (DL) methods, such as Convolutional Neural Networks (CNNs) or Recurrent Neural Networks (RNNs), in cyber threat detection. Employing the Edge-IIoTset cybersecurity dataset, our experimental analysis shows that SecurityBERT achieved an impressive 98.2% overall accuracy in identifying fourteen distinct attack types, surpassing previous records set by hybrid solutions such as GAN-Transformer-based architectures and CNN-LSTM models. With an inference time of less than 0.15 seconds on an average CPU and a compact model size of just 16.7MB, SecurityBERT is ideally suited for real-life traffic analysis and a suitable choice for deployment on resource-constrained IoT devices.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "SecurityBERT is a novel architecture that leverages the Bidirectional Encoder Representations from Transformers (BERT) model for cyber threat detection in IoT networks and achieves an impressive 98.2% overall accuracy in identifying fourteen distinct attack types."
            },
            "score": 3
        },
        {
            "id": "febcdcc4d0ed643829595480953eb5e5e27f9825",
            "paperId": "febcdcc4d0ed643829595480953eb5e5e27f9825",
            "title": "Using Membership Inference Attacks to Evaluate Privacy-Preserving Language Modeling Fails for Pseudonymizing Data",
            "abstract": "Large pre-trained language models dominate the current state-of-the-art for many natural language processing applications, including the field of clinical NLP. Several studies have found that these can be susceptible to privacy attacks that are unacceptable in the clinical domain where personally identifiable information (PII) must not be exposed.However, there is no consensus regarding how to quantify the privacy risks of different models. One prominent suggestion is to quantify these risks using membership inference attacks. In this study, we show that a state-of-the-art membership inference attack on a clinical BERT model fails to detect the privacy benefits from pseudonymizing data. This suggests that such attacks may be inadequate for evaluating token-level privacy preservation of PIIs.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that a state-of-the-art membership inference attack on a clinical BERT model fails to detect the privacy benefits from pseudonymizing data, suggesting that such attacks may be inadequate for evaluating token-level privacy preservation of PIIs."
            },
            "score": 3
        },
        {
            "id": "427b286483e3ae562a39f43c83725b5f4414d43f",
            "paperId": "427b286483e3ae562a39f43c83725b5f4414d43f",
            "title": "Detection of Suicidality Through Privacy-Preserving Large Language Models",
            "abstract": "Importance Attempts to use Artificial Intelligence (AI) in psychiatric disorders show moderate success, high-lighting the potential of incorporating information from clinical assessments to improve the mod-els. The study focuses on using Large Language Models (LLMs) to manage unstructured medi-cal text, particularly for suicide risk detection in psychiatric care. Objective The study aims to extract information about suicidality status from the admission notes of elec-tronic health records (EHR) using privacy-sensitive, locally hosted LLMs, specifically evaluating the efficacy of Llama-2 models. Main Outcomes and Measures The study compares the performance of several variants of the open source LLM Llama-2 in extracting suicidality status from psychiatric reports against a ground truth defined by human experts, assessing accuracy, sensitivity, specificity, and F1 score across different prompting strategies. Results A German fine-tuned Llama-2 model showed the highest accuracy (87.5%), sensitivity (83%) and specificity (91.8%) in identifying suicidality, with significant improvements in sensitivity and specificity across various prompt designs. Conclusions and Relevance The study demonstrates the capability of LLMs, particularly Llama-2, in accurately extracting the information on suicidality from psychiatric records while preserving data-privacy. This suggests their application in surveillance systems for psychiatric emergencies and improving the clinical management of suicidality by improving systematic quality control and research.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The study demonstrates the capability of LLMs, particularly Llama-2, in accurately extracting the information on suicidality from psychiatric records while preserving data-privacy, and suggests their application in surveillance systems for psychiatric emergencies and improving the clinical management of suicidality by improving systematic quality control and research."
            },
            "score": 3
        },
        {
            "id": "4e60d8d00dc2b0ba296ea8d1c01fb503d7f35f2a",
            "paperId": "4e60d8d00dc2b0ba296ea8d1c01fb503d7f35f2a",
            "title": "Local large language models for privacy-preserving accelerated review of historic echocardiogram reports.",
            "abstract": "OBJECTIVES\nThe study developed framework that leverages an open-source Large Language Model (LLM) to enable clinicians to ask plain-language questions about a patient's entire echocardiogram report history. This approach is intended to streamline the extraction of clinical insights from multiple echocardiogram reports, particularly in patients with complex cardiac diseases, thereby enhancing both patient care and research efficiency.\n\n\nMATERIALS AND METHODS\nData from over 10 years were collected, comprising echocardiogram reports from patients with more than 10 echocardiograms on file at the Mount Sinai Health System. These reports were converted into a single document per patient for analysis, broken down into snippets and relevant snippets were retrieved using text similarity measures. The LLaMA-2 70B model was employed for analyzing the text using a specially crafted prompt. The model's performance was evaluated against ground-truth answers created by faculty cardiologists.\n\n\nRESULTS\nThe study analyzed 432 reports from 37 patients for a total of 100 question-answer pairs. The LLM correctly answered 90% questions, with accuracies of 83% for temporality, 93% for severity assessment, 84% for intervention identification, and 100% for diagnosis retrieval. Errors mainly stemmed from the LLM's inherent limitations, such as misinterpreting numbers or hallucinations.\n\n\nCONCLUSION\nThe study demonstrates the feasibility and effectiveness of using a local, open-source LLM for querying and interpreting echocardiogram report data. This approach offers a significant improvement over traditional keyword-based searches, enabling more contextually relevant and semantically accurate responses; in turn showing promise in enhancing clinical decision-making and research by facilitating more efficient access to complex patient data.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The study demonstrates the feasibility and effectiveness of using a local, open-source LLM for querying and interpreting echocardiogram report data, and offers a significant improvement over traditional keyword-based searches, enabling more contextually relevant and semantically accurate responses."
            },
            "score": 3
        },
        {
            "id": "8fdd34153d1035d09dd4a6efa9cb0c91d23d0045",
            "paperId": "8fdd34153d1035d09dd4a6efa9cb0c91d23d0045",
            "title": "More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models",
            "abstract": "We are currently witnessing dramatic advances in the capabilities of Large Language Models (LLMs). They are already being adopted in practice and integrated into many systems, including integrated development environments (IDEs) and search engines. The functionalities of current LLMs can be modulated via natural language prompts, while their exact internal functionality remains implicit and unassessable. This property, which makes them adaptable to even unseen tasks, might also make them susceptible to targeted adversarial prompting . Recently, several ways to misalign LLMs using Prompt Injection (PI) attacks have been introduced. In such attacks, an adversary can prompt the LLM to produce malicious content or override the original instructions and the employed \ufb01ltering schemes. Recent work showed that these attacks are hard to mitigate, as state-of-the-art LLMs are instruction-following . So far, these attacks assumed that the adversary is directly prompting the LLM. In this work, we show that augmenting LLMs with retrieval and API calling capabilities (so-called Application-Integrated LLMs ) induces a whole new set of attack vectors. These LLMs might process poisoned content retrieved from the Web that contains malicious prompts pre-injected and selected by adversaries. We demonstrate that an attacker can indirectly perform such PI attacks. Based on this key insight, we systematically analyze the resulting threat landscape of Application-Integrated LLMs and discuss a variety of new attack vectors. To demonstrate the practical viability of our attacks, we implemented speci\ufb01c demonstrations",
            "year": 2023,
            "citationCount": 73,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work shows that augmenting LLMs with retrieval and API calling capabilities (so-called Application-Integrated LLMs) induces a whole new set of attack vectors and systematically analyzes the resulting threat landscape of Application-Integrated LLMs."
            },
            "score": 3
        },
        {
            "id": "05e003a34148d4663734d3f39deefa0979d2a0e6",
            "paperId": "05e003a34148d4663734d3f39deefa0979d2a0e6",
            "title": "GeneGPT: Augmenting Large Language Models with Domain Tools for Improved Access to Biomedical Information",
            "abstract": "While large language models (LLMs) have been successfully applied to various tasks, they still face challenges with hallucinations. Augmenting LLMs with domain-specific tools such as database utilities can facilitate easier and more precise access to specialized knowledge. In this paper, we present GeneGPT, a novel method for teaching LLMs to use the Web APIs of the National Center for Biotechnology Information (NCBI) for answering genomics questions. Specifically, we prompt Codex to solve the GeneTuring tests with NCBI Web APIs by in-context learning and an augmented decoding algorithm that can detect and execute API calls. Experimental results show that GeneGPT achieves state-of-the-art performance on eight tasks in the GeneTuring benchmark with an average score of 0.83, largely surpassing retrieval-augmented LLMs such as the new Bing (0.44), biomedical LLMs such as BioMedLM (0.08) and BioGPT (0.04), as well as GPT-3 (0.16) and ChatGPT (0.12). Our further analyses suggest that: (1) API demonstrations have good cross-task generalizability and are more useful than documentations for in-context learning; (2) GeneGPT can generalize to longer chains of API calls and answer multi-hop questions in GeneHop, a novel dataset introduced in this work; (3) Different types of errors are enriched in different tasks, providing valuable insights for future improvements.",
            "year": 2023,
            "citationCount": 56,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "GeneGPT is presented, a novel method for teaching LLMs to use the Web APIs of the National Center for Biotechnology Information (NCBI) for answering genomics questions by in-context learning and an augmented decoding algorithm that can detect and execute API calls."
            },
            "score": 3
        },
        {
            "id": "3d68522abfadfc8ee6b7ec9edaaf91f1b2f38e5e",
            "paperId": "3d68522abfadfc8ee6b7ec9edaaf91f1b2f38e5e",
            "title": "Large Language Models Can Be Easily Distracted by Irrelevant Context",
            "abstract": "Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of cutting-edge prompting techniques for large language models, and find that the model performance is dramatically decreased when irrelevant information is included. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.",
            "year": 2023,
            "citationCount": 212,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context, and introduces Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description."
            },
            "score": 3
        },
        {
            "id": "fd81018bc72b030545a2d3f3010f3758ec4d48c3",
            "paperId": "fd81018bc72b030545a2d3f3010f3758ec4d48c3",
            "title": "Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions",
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in various NLP tasks. However, previous works have shown these models are sensitive towards prompt wording, and few-shot demonstrations and their order, posing challenges to fair assessment of these models. As these models become more powerful, it becomes imperative to understand and address these limitations. In this paper, we focus on LLMs robustness on the task of multiple-choice questions -- commonly adopted task to study reasoning and fact-retrieving capability of LLMs. Investigating the sensitivity of LLMs towards the order of options in multiple-choice questions, we demonstrate a considerable performance gap of approximately 13% to 75% in LLMs on different benchmarks, when answer options are reordered, even when using demonstrations in a few-shot setting. Through a detailed analysis, we conjecture that this sensitivity arises when LLMs are uncertain about the prediction between the top-2/3 choices, and specific options placements may favor certain prediction between those top choices depending on the question caused by positional bias. We also identify patterns in top-2 choices that amplify or mitigate the model's bias toward option placement. We found that for amplifying bias, the optimal strategy involves positioning the top two choices as the first and last options. Conversely, to mitigate bias, we recommend placing these choices among the adjacent options. To validate our conjecture, we conduct various experiments and adopt two approaches to calibrate LLMs' predictions, leading to up to 8 percentage points improvement across different models and benchmarks.",
            "year": 2023,
            "citationCount": 30,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper investigates the sensitivity of LLMs towards the order of options in multiple-choice questions, and conjecture that this sensitivity arises when LLMs are uncertain about the prediction between the top-2/3 choices, and specific options placements may favor certain prediction between those top choices depending on the question caused by positional bias."
            },
            "score": 3
        },
        {
            "id": "6a3d0b11a0b22bafcce7739cc5eb12dad8bd7565",
            "paperId": "6a3d0b11a0b22bafcce7739cc5eb12dad8bd7565",
            "title": "Supporting Qualitative Analysis with Large Language Models: Combining Codebook with GPT-3 for Deductive Coding",
            "abstract": "Qualitative analysis of textual contents unpacks rich and valuable information by assigning labels to the data. However, this process is often labor-intensive, particularly when working with large datasets. While recent AI-based tools demonstrate utility, researchers may not have readily available AI resources and expertise, let alone be challenged by the limited generalizability of those task-specific models. In this study, we explored the use of large language models (LLMs) in supporting deductive coding, a major category of qualitative analysis where researchers use pre-determined codebooks to label the data into a fixed set of codes. Instead of training task-specific models, a pre-trained LLM could be used directly for various tasks without fine-tuning through prompt learning. Using a curiosity-driven questions coding task as a case study, we found, by combining GPT-3 with expert-drafted codebooks, our proposed approach achieved fair to substantial agreements with expert-coded results. We lay out challenges and opportunities in using LLMs to support qualitative coding and beyond.",
            "year": 2023,
            "citationCount": 45,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study explored the use of large language models (LLMs) in supporting deductive coding, a major category of qualitative analysis where researchers use pre-determined codebooks to label the data into a fixed set of codes, and found a pre-trained LLM could be used directly for various tasks without fine-tuning through prompt learning."
            },
            "score": 3
        },
        {
            "id": "e661de406d8105e52a5351a2cd66db84cc4af115",
            "paperId": "e661de406d8105e52a5351a2cd66db84cc4af115",
            "title": "Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods",
            "abstract": "Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Due to rapid technological advances and their extreme versatility, LLMs nowadays have millions of users and are at the cusp of being the main go-to technology for information retrieval, content generation, problem-solving, etc. Therefore, it is of great importance to thoroughly assess and scrutinize their capabilities. Due to increasingly complex and novel behavioral patterns in current LLMs, this can be done by treating them as participants in psychology experiments that were originally designed to test humans. For this purpose, the paper introduces a new field of research called\"machine psychology\". The paper outlines how different subfields of psychology can inform behavioral tests for LLMs. It defines methodological standards for machine psychology research, especially by focusing on policies for prompt designs. Additionally, it describes how behavioral patterns discovered in LLMs are to be interpreted. In sum, machine psychology aims to discover emergent abilities in LLMs that cannot be detected by most traditional natural language processing benchmarks.",
            "year": 2023,
            "citationCount": 41,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The paper outlines how different subfields of psychology can inform behavioral tests for LLMs and defines methodological standards for machine psychology research, especially by focusing on policies for prompt designs."
            },
            "score": 3
        },
        {
            "id": "fc32b91d1b849f8137aa0fb8eea73fc4826205f0",
            "paperId": "fc32b91d1b849f8137aa0fb8eea73fc4826205f0",
            "title": "Named Entity Recognition Utilized to Enhance Text Classification While Preserving Privacy",
            "abstract": "Recent development in Natural Language Processing (NLP) techniques has encouraged NLP-based application in various field including business, legal and health. An important process for all NLP projects is text preprocessing which is a process that modifies text data before using them in a machine learning model. Usually text preprocessing process includes cleaning, filtering, removing and replacing some texts to increase model accuracy, robustness, reduce data size or preserve privacy. Named entities recognizer (NER) is an NLP tool which finds Named Entities in text such as: names, organization, addresses, numbers and date. In this work, we create a preproccessing approach that uses NER to find named entities and, then, replace them with their type i.e. location, person or organization name to improve accuracy and preserve privacy instead of removing them or letting them become noise to our data. Experiments for text classification task using our approach have been conducted on several datasets some of which were collected in-house. Experiments indicate that using this approach enhances classifier accuracy and reduces feature representation\u2019s dimensionality while, also, preserve privacy.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work creates a preproccessing approach that uses NER to find named entities and, then, replace them with their type i.e. location, person or organization name to improve accuracy and preserve privacy."
            },
            "score": 3
        },
        {
            "id": "ef3a38b9f15e9dcb5652cb3f86f19b845cdaaef7",
            "paperId": "ef3a38b9f15e9dcb5652cb3f86f19b845cdaaef7",
            "title": "Chat2VIS: Generating Data Visualizations via Natural Language Using ChatGPT, Codex and GPT-3 Large Language Models",
            "abstract": "The field of data visualisation has long aimed to devise solutions for generating visualisations directly from natural language text. Research in Natural Language Interfaces (NLIs) has contributed towards the development of such techniques. However, the implementation of workable NLIs has always been challenging due to the inherent ambiguity of natural language, as well as in consequence of unclear and poorly written user queries which pose problems for existing language models in discerning user intent. Instead of pursuing the usual path of developing new iterations of language models, this study uniquely proposes leveraging the advancements in pre-trained large language models (LLMs) such as ChatGPT and GPT-3 to convert free-form natural language directly into code for appropriate visualisations. This paper presents a novel system, Chat2VIS, which takes advantage of the capabilities of LLMs and demonstrates how, with effective prompt engineering, the complex problem of language understanding can be solved more efficiently, resulting in simpler and more accurate end-to-end solutions than prior approaches. Chat2VIS shows that LLMs together with the proposed prompts offer a reliable approach to rendering visualisations from natural language queries, even when queries are highly misspecified and underspecified. This solution also presents a significant reduction in costs for the development of NLI systems, while attaining greater visualisation inference abilities compared to traditional NLP approaches that use hand-crafted grammar rules and tailored models. This study also presents how LLM prompts can be constructed in a way that preserves data security and privacy while being generalisable to different datasets. This work compares the performance of GPT-3, Codex and ChatGPT across several case studies and contrasts the performances with prior studies.",
            "year": 2023,
            "citationCount": 55,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Chat2VIS shows that LLMs together with the proposed prompts offer a reliable approach to rendering visualisations from natural language queries, even when queries are highly misspecified and underspecified."
            },
            "score": 3
        },
        {
            "id": "24459efb669583530b6bd7a13e8e90a9408eab46",
            "paperId": "24459efb669583530b6bd7a13e8e90a9408eab46",
            "title": "Access control in decentralized online social networks: Applying a policy-hiding cryptographic scheme and evaluating its performance",
            "abstract": "Privacy concerns in online social networking services have prompted a number of proposals for decentralized online social networks (DOSN) that remove the central provider and aim at giving the users control over their data and who can access it. This is usually done by cryptographic means. Existing DOSNs use cryptographic primitives that hide the data but reveal the access policies. At the same time, there are privacy-preserving variants of these cryptographic primitives that do not reveal access policies. They are, however, not suitable for usage in the DOSN context because of performance or storage constraints. A DOSN needs to achieve both privacy and performance to be useful. We analyze predicate encryption (PE) and adapt it to the DOSN context. We propose a univariate polynomial construction for access policies in PE that drastically increases performance of the scheme but leaks some part of the access policy to users with access rights. We utilize Bloom filters as a means of decreasing decryption time and indicate objects that can be decrypted by a particular user. We evaluate the performance of the adapted scheme in the concrete scenario of a news feed. Our PE scheme is best suited for encrypting for groups or small sets of separate identities.",
            "year": 2014,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work analyzes predicate encryption (PE) and proposes a univariate polynomial construction for access policies in PE that drastically increases performance of the scheme but leaks some part of the access policy to users with access rights."
            },
            "score": 3
        },
        {
            "id": "f80e2aa5177465640a8b406af9d69c52d72c2f78",
            "paperId": "f80e2aa5177465640a8b406af9d69c52d72c2f78",
            "title": "Enhancing chest X-ray datasets with privacy-preserving large language models and multi-type annotations: a data-driven approach for improved classification",
            "abstract": "In chest X-ray (CXR) image analysis, rule-based systems are usually employed to extract labels from reports, but concerns exist about label quality. These datasets typically offer only presence labels, sometimes with binary uncertainty indicators, which limits their usefulness. In this work, we present MAPLEZ (Medical report Annotations with Privacy-preserving Large language model using Expeditious Zero shot answers), a novel approach leveraging a locally executable Large Language Model (LLM) to extract and enhance findings labels on CXR reports. MAPLEZ extracts not only binary labels indicating the presence or absence of a finding but also the location, severity, and radiologists' uncertainty about the finding. Over eight abnormalities from five test sets, we show that our method can extract these annotations with an increase of 5 percentage points (pp) in F1 score for categorical presence annotations and more than 30 pp increase in F1 score for the location annotations over competing labelers. Additionally, using these improved annotations in classification supervision, we demonstrate substantial advancements in model quality, with an increase of 1.7 pp in AUROC over models trained with annotations from the state-of-the-art approach. We share code and annotations.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents MAPLEZ (Medical report Annotations with Privacy-preserving Large language model using Expeditious Zero shot answers), a novel approach leveraging a locally executable Large Language Model (LLM) to extract and enhance findings labels on CXR reports."
            },
            "score": 2
        },
        {
            "id": "b1750d2a6e3480e690999916a86c8b3876577b39",
            "paperId": "b1750d2a6e3480e690999916a86c8b3876577b39",
            "title": "Large Language Models are Frame-level Directors for Zero-shot Text-to-Video Generation",
            "abstract": "In the paradigm of AI-generated content (AIGC), there has been increasing attention to transferring knowledge from pre-trained text-to-image (T2I) models to text-to-video (T2V) generation. Despite their effectiveness, these frameworks face challenges in maintaining consistent narratives and handling shifts in scene composition or object placement from a single abstract user prompt. Exploring the ability of large language models (LLMs) to generate time-dependent, frame-by-frame prompts, this paper introduces a new framework, dubbed DirecT2V. DirecT2V leverages instruction-tuned LLMs as directors, enabling the inclusion of time-varying content and facilitating consistent video generation. To maintain temporal consistency and prevent mapping the value to a different object, we equip a diffusion model with a novel value mapping method and dual-softmax filtering, which do not require any additional training. The experimental results validate the effectiveness of our framework in producing visually coherent and storyful videos from abstract user prompts, successfully addressing the challenges of zero-shot video generation.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "DirecT2V leverages instruction-tuned LLMs as directors, enabling the inclusion of time-varying content and facilitating consistent video generation, and equip a diffusion model with a novel value mapping method and dual-softmax filtering, which do not require any additional training."
            },
            "score": 2
        },
        {
            "id": "c9cc62881bc4ef571785b06fd54d4e6adaecb321",
            "paperId": "c9cc62881bc4ef571785b06fd54d4e6adaecb321",
            "title": "PEVL: Position-enhanced Pre-training and Prompt Tuning for Vision-language Models",
            "abstract": "Vision-language pre-training (VLP) has shown impressive performance on a wide range of cross-modal tasks, where VLP models without reliance on object detectors are becoming the mainstream due to their superior computation efficiency and competitive performance. However, the removal of object detectors also deprives the capability of VLP models in explicit object modeling, which is essential to various position-sensitive vision-language (VL) tasks, such as referring expression comprehension and visual commonsense reasoning. To address the challenge, we introduce PEVL that enhances the pre-training and prompt tuning of VLP models with explicit object position modeling. Specifically, PEVL reformulates discretized object positions and language in a unified language modeling framework, which facilitates explicit VL alignment during pre-training, and also enables flexible prompt tuning for various downstream tasks. We show that PEVL enables state-of-the-art performance of detector-free VLP models on position-sensitive tasks such as referring expression comprehension and phrase grounding, and also improves the performance on position-insensitive tasks with grounded inputs. We make the data and code for this paper publicly available at https://github.com/thunlp/PEVL.",
            "year": 2022,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "PEVL reformulates discretized object positions and language in a unified language modeling framework, which facilitates explicit VL alignment during pre-training, and also enables flexible prompt tuning for various downstream tasks."
            },
            "score": 2
        },
        {
            "id": "179790fac38b1c35f6003f65eef7097f8413cd35",
            "paperId": "179790fac38b1c35f6003f65eef7097f8413cd35",
            "title": "Leveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data",
            "abstract": "Large language models (LLMs) provide a new way to build chatbots by accepting natural language prompts. Yet, it is unclear how to design prompts to power chatbots to carry on naturalistic conversations while pursuing a given goal such as collecting self-report data from users. We explore what design factors of prompts can help steer chatbots to talk naturally and collect data reliably. To this aim, we formulated four prompt designs with different structures and personas. Through an online study (N = 48) where participants conversed with chatbots driven by different designs of prompts, we assessed how prompt designs and conversation topics affected the conversation flows and users' perceptions of chatbots. Our chatbots covered 79% of the desired information slots during conversations, and the designs of prompts and topics significantly influenced the conversation flows and the data collection performance. We discuss the opportunities and challenges of building chatbots with LLMs.",
            "year": 2023,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work formulated four prompt designs with different structures and personas to explore what design factors of prompts can help steer chatbots to talk naturally and collect data reliably and discusses the opportunities and challenges of building chatbots with LLMs."
            },
            "score": 2
        },
        {
            "id": "990026128083f5a47b061f6237b8135b2d3a41a9",
            "paperId": "990026128083f5a47b061f6237b8135b2d3a41a9",
            "title": "Don\u2019t Prompt, Search! Mining-based Zero-Shot Learning with Language Models",
            "abstract": "Masked language models like BERT can perform text classification in a zero-shot fashion by reformulating downstream tasks as text infilling. However, this approach is highly sensitive to the template used to prompt the model, yet practitioners are blind when designing them in strict zero-shot settings. In this paper, we propose an alternative mining-based approach for zero-shot learning. Instead of prompting language models, we use regular expressions to mine labeled examples from unlabeled corpora, which can optionally be filtered through prompting, and used to finetune a pretrained model. Our method is more flexible and interpretable than prompting, and outperforms it on a wide range of tasks when using comparable templates. Our results suggest that the success of prompting can partly be explained by the model being exposed to similar examples during pretraining, which can be directly retrieved through regular expressions.",
            "year": 2022,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes an alternative mining-based approach for zero-shot learning that uses regular expressions to mine labeled examples from unlabeled corpora, which can optionally be filtered through prompting, and used to finetune a pretrained model."
            },
            "score": 2
        },
        {
            "id": "25df7f3f539e6741ff66438f81452f14f5fb9288",
            "paperId": "25df7f3f539e6741ff66438f81452f14f5fb9288",
            "title": "Knowledge-Aware Prompt Tuning for Generalizable Vision-Language Models",
            "abstract": "Pre-trained vision-language models, e.g., CLIP, working with manually designed prompts have demonstrated great capacity of transfer learning. Recently, learnable prompts achieve state-of-the-art performance, which however are prone to overfit to seen classes, failing to generalize to unseen classes. In this paper, we propose a Knowledge-Aware Prompt Tuning (KAPT) framework for vision-language models. Our approach takes the inspiration from human intelligence in which external knowledge is usually incorporated into recognizing novel categories of objects. Specifically, we design two complementary types of knowledge-aware prompts for the text encoder to leverage the distinctive characteristics of category-related external knowledge. The discrete prompt extracts the key information from descriptions of an object category, and the learned continuous prompt captures overall contexts. We further design an adaptation head for the visual encoder to aggregate salient attentive visual cues, which establishes discriminative and task-aware visual representations. We conduct extensive experiments on 11 widely-used benchmark datasets and the results verify the effectiveness in few-shot image classification, especially in generalizing to unseen categories. Compared with the state-of-the-art CoCoOp method, KAPT exhibits favorable performance and achieves an absolute gain of 3.22% on new classes and 2.57% in terms of harmonic mean.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper designs two complementary types of knowledge-aware prompts for the text encoder to leverage the distinctive characteristics of category-related external knowledge and designs an adaptation head for the visual encoder to aggregate salient attentive visual cues, which establishes discriminative and task-aware visual representations."
            },
            "score": 2
        },
        {
            "id": "c5cd07a6f407a146df87543c083e2a60e9b81609",
            "paperId": "c5cd07a6f407a146df87543c083e2a60e9b81609",
            "title": "SE-P\u00b2\u03bcm: Secure and Efficient Privacy-Preserving User-Profile Matching Protocol for Social Networks",
            "abstract": "By matching attributes in user profiles, social networking services (SNSs) are able to create certain social relationships between different users. However, user profiles often contain sensitive information about users. If data matching is done in plaintext, users\u2019 privacy and security will face serious challenges. Most recently, the hacking incident of Ashley Madison website, which incurred more than 60 GB of data being leaked, has prompted researchers to explore the privacy of user data in social networks urgently. Existing schemes may be able to achieve privacy-preserving user profile matching, but they require high communication bandwidth and complex computational costs. To address them, we proposed a private user profile matching protocol for social networks using <inline-formula> <tex-math notation=\"LaTeX\">$t$ </tex-math></inline-formula>-out-of-<inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula> matching servers. Applying the Chinese remainder theorem (CRT), Bloom filter technique, and skyline computation idea, the matching users are efficiently returned with the help of any <inline-formula> <tex-math notation=\"LaTeX\">$t$ </tex-math></inline-formula> matching server without disclosing their identities. In addition, our scheme allows up to <inline-formula> <tex-math notation=\"LaTeX\">$t$ </tex-math></inline-formula> matching servers to collude while simultaneously ensuring user profile privacy and query privacy. Finally, compared with two recent user profile matching protocols, the performance results demonstrate that our scheme outperforms them.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A private user profile matching protocol for social networks using inline-formula with the help of LaTeX notation is proposed, which allows up to up to two matching servers to collude while simultaneously ensuring user profile privacy and query privacy."
            },
            "score": 2
        },
        {
            "id": "731dd721bbd559ab80bc0b7f95e83045015e664d",
            "paperId": "731dd721bbd559ab80bc0b7f95e83045015e664d",
            "title": "Do I know you?: efficient and privacy-preserving common friend-finder protocols and applications",
            "abstract": "The increasing penetration of Online Social Networks (OSNs) prompts the need for effectively accessing and utilizing social networking information. In numerous applications, users need to make trust and/or access control decisions involving other (possibly stranger) users, and one important factor is often the existence of common social relationships. This motivates the need for secure and privacy-preserving techniques allowing users to assess whether or not they have mutual friends. This paper introduces the Common Friends service, a framework for finding common friends which protects privacy of non-mutual friends and guarantees authenticity of friendships. First, we present a generic construction that reduces to secure computation of set intersection, while ensuring authenticity of announced friends via bearer capabilities. Then, we propose an efficient instantiation, based on Bloom filters, that only incurs a constant number of public-key operations and appreciably low communication overhead. Our software is designed so that developers can easily integrate Common Friends into their applications, e.g., to enforce access control based on users' social proximity in a privacy-preserving manner. Finally, we showcase our techniques in the context of an existing application for sharing (tethered) Internet access, whereby users decide to share access depending on the existence of common friends. A comprehensive experimental evaluation attests to the practicality of proposed techniques.",
            "year": 2013,
            "citationCount": 43,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Common Friends service is introduced, a framework for finding common friends which protects privacy of non-mutual friends and guarantees authenticity of friendships, and an efficient instantiation is proposed, based on Bloom filters, that only incurs a constant number of public-key operations and appreciably low communication overhead."
            },
            "score": 2
        },
        {
            "id": "5441d96544d5c0a43af34a3c29b520e0ba92aabb",
            "paperId": "5441d96544d5c0a43af34a3c29b520e0ba92aabb",
            "title": "RE: Reliable Email",
            "abstract": "The explosive growth in unwanted email has prompted the development of techniques for the rejection of email, intended to shield recipients from the onerous task of identifying the legitimate email in their inboxes amid a sea of spam. Unfortunately, widely used content-based filtering systems have converted the spam problem into a false positive one: email has become unreliable. Email acceptance techniques complement rejection ones; they can help prevent false positives by filing email into a user's inbox before it is considered for rejection. Whitelisting, whereby recipients accept email from some set of authorized senders, is one such acceptance technique. We present Reliable Email (RE:), a new whitelisting system that incurs zero false positives among socially connected users. Unlike previous whitelisting systems, which require that whitelists be populated manually, RE: exploits friend-of-friend relationships among email correspondents to populate whitelists automatically. To do so, RE: permits an email's recipient to discover whether other email users have whitelisted the email's sender, while preserving the privacy of users' email contacts with cryptographic private matching techniques. Using real email traces from two sites, we demonstrate that RE: renders a significant fraction of received email reliable. Our evaluation also shows that RE: can prevent up to 88% of the false positives incurred by a widely deployed email rejection system, at modest computational cost.",
            "year": 2006,
            "citationCount": 165,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Reliable Email is presented, a new whitelisting system that incurs zero false positives among socially connected users and can prevent up to 88% of the false positives incurred by a widely deployed email rejection system, at modest computational cost."
            },
            "score": 2
        }
    ],
    "novelty": "yes"
}