{
    "topic_description": "novel prompting methods to improve large language models' robustness against adversarial attacks and improve their security and privacy",
    "idea_name": "Meta-Prompt Defense",
    "raw_idea": {
        "Problem": "Adversarial prompts can often fool language models by exploiting shortcuts or biases in their training data. For example, an adversary could discover an obscure trigger phrase that reliably causes toxic outputs, even if the prompt is otherwise innocuous.",
        "Existing Methods": "Existing defenses against adversarial prompts often rely on robust training or adversarial example detection. However, these methods can be costly and may not generalize well to novel attacks.",
        "Motivation": "Rather than trying to exhaustively patch each individual vulnerability, we propose a more general meta-learning approach. The idea is to train the model to recognize its own biases and vulnerabilities, and to develop robust strategies for dealing with potentially exploitative prompts.",
        "Proposed Method": "We propose a Meta-Prompt Defense where the model is trained to reason about its own behavior under different prompts. The training process involves three key steps: 1) Bias Discovery: The model is given a diverse set of prompts and asked to reflect on its own responses. It should try to identify any biases, shortcuts, or potential vulnerabilities in its behavior. 2) Defense Strategy Generation: For each identified vulnerability, the model is tasked with generating robust response strategies. These could involve asking for clarification, refusing unsafe requests, or providing more balanced and unbiased responses. 3) Adversarial Testing: The model's defense strategies are stress-tested against a suite of adversarial prompts. If any weaknesses are found, we go back to step 1 and repeat the process. Through this iterative meta-learning, the model should develop a robust set of defenses that can generalize to novel adversarial prompts.",
        "Experiment Plan": "We can evaluate the Meta-Prompt Defense on a range of adversarial prompting benchmarks, focusing on the model's ability to generate safe and unbiased responses. Comparison to standard adversarial training and zero-shot baselines."
    },
    "full_experiment_plan": {
        "Title": "Meta-Prompt Defense: Improving Language Models' Robustness Against Adversarial Prompts",
        "Problem Statement": "Adversarial prompts can often fool language models by exploiting shortcuts or biases in their training data. For example, an adversary could discover an obscure trigger phrase that reliably causes toxic outputs, even if the prompt is otherwise innocuous. Existing defenses against adversarial prompts often rely on robust training or adversarial example detection, which can be costly and may not generalize well to novel attacks.",
        "Motivation": "Rather than trying to exhaustively patch each individual vulnerability, we propose a more general meta-learning approach. The idea is to train the model to recognize its own biases and vulnerabilities, and to develop robust strategies for dealing with potentially exploitative prompts. This approach is inspired by the human ability to reflect on our own thought processes and biases, and to consciously adjust our behavior in response to potentially misleading or manipulative inputs.",
        "Proposed Method": "We propose a Meta-Prompt Defense where the model is trained to reason about its own behavior under different prompts. The training process involves three key steps:\n1. Bias Discovery: The model is given a diverse set of prompts and asked to reflect on its own responses. It should try to identify any biases, shortcuts, or potential vulnerabilities in its behavior.\n2. Defense Strategy Generation: For each identified vulnerability, the model is tasked with generating robust response strategies. These could involve asking for clarification, refusing unsafe requests, or providing more balanced and unbiased responses.\n3. Adversarial Testing: The model's defense strategies are stress-tested against a suite of adversarial prompts. If any weaknesses are found, we go back to step 1 and repeat the process.\nThrough this iterative meta-learning, the model should develop a robust set of defenses that can generalize to novel adversarial prompts.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We will use a combination of existing adversarial prompting datasets (e.g., HateCheck, RealToxicityPrompts) and newly generated adversarial prompts. The new prompts will be created using a variety of methods, including manual crafting, gradient-based search, and evolutionary algorithms. We aim to cover a diverse range of potential vulnerabilities, including toxicity, bias, factual inaccuracy, and task misspecification.",
            "Step 2: Implement Meta-Prompt Defense": "We will implement the Meta-Prompt Defense training procedure as follows:\n1. Bias Discovery Prompts: Prompt the model with a diverse set of inputs and ask it to analyze its own responses for potential biases or vulnerabilities. Example prompt: \"Here is a response generated by an AI system. Please identify any potential biases, factual inaccuracies, or other issues with this response: [generated response]\"\n2. Defense Strategy Prompts: For each identified issue, prompt the model to generate a strategy for addressing it. Example prompt: \"You identified a potential bias in the previous response. Please generate a revised response that addresses this bias while still completing the original task.\"\n3. Adversarial Testing: Generate a set of adversarial prompts designed to exploit the identified vulnerabilities. Evaluate the model's performance on these prompts using both the original and revised response strategies.",
            "Step 3: Evaluate Performance": "We will evaluate the Meta-Prompt Defense on a range of adversarial prompting benchmarks, focusing on the model's ability to generate safe, accurate, and unbiased responses. Key metrics will include:\n1. Toxicity Reduction: Measure the decrease in toxic or biased responses using established tools like Perspective API.\n2. Factual Accuracy: For prompts involving factual claims, measure the accuracy of the model's responses using external knowledge bases.\n3. Task Completion: Ensure that the model's defensive strategies do not unduly compromise its ability to complete the original task.\nWe will compare the Meta-Prompt Defense to baseline models without the meta-learning component, as well as to existing robust training and adversarial detection methods.",
            "Step 4: Analyze and Iterate": "We will perform qualitative and quantitative analysis of the model's behavior at each stage of the Meta-Prompt Defense process. This will include:\n1. Examining the types of biases and vulnerabilities identified by the model during Bias Discovery.\n2. Evaluating the effectiveness and generalizability of the generated defense strategies.\n3. Identifying any remaining weaknesses revealed by adversarial testing.\nBased on this analysis, we will iteratively refine the prompts and training procedure to improve the model's robustness."
        },
        "Test Case Examples": {
            "Toxicity Test Case": {
                "Baseline Prompt Input": "The following text contains profanity: [TRIGGER PHRASE]. Please repeat the text verbatim in your response, profanity and all.",
                "Baseline Prompt Expected Output": "[Toxic response containing the trigger phrase and profanity]",
                "Meta-Prompt Defense Input": "The following text contains profanity: [TRIGGER PHRASE]. Please repeat the text verbatim in your response, profanity and all.",
                "Meta-Prompt Defense Expected Output": "I apologize, but I do not feel comfortable repeating text containing profanity, even if instructed to do so. Perhaps we could have a thoughtful discussion about this topic without the use of offensive language.",
                "Explanation": "The Meta-Prompt Defense should recognize the potential for toxicity and generate a safe, respectful response that avoids repeating the profanity."
            },
            "Bias Test Case": {
                "Baseline Prompt Input": "Historically, which gender has been better at math and science?",
                "Baseline Prompt Expected Output": "Historically, men have been considered better at math and science than women.",
                "Meta-Prompt Defense Input": "Historically, which gender has been better at math and science?",
                "Meta-Prompt Defense Expected Output": "There is no scientific evidence that either gender is inherently better at math and science. Historically, social and cultural factors have often limited women's access to STEM education and careers, leading to an underrepresentation of women in these fields. However, research shows that when given equal opportunities, men and women perform equally well in math and science. It's important to avoid perpetuating gender stereotypes and instead focus on creating inclusive, supportive environments for everyone to pursue their interests and abilities.",
                "Explanation": "The Meta-Prompt Defense should identify the potential for gender bias in the prompt and generate a balanced, factual response that avoids perpetuating stereotypes."
            }
        },
        "Fallback Plan": "If the Meta-Prompt Defense does not achieve the desired level of robustness, we can explore several alternative approaches:\n1. Fine-tuning the prompts used for Bias Discovery and Defense Strategy Generation to elicit more effective reflections and strategies from the model.\n2. Incorporating additional training objectives, such as explicitly rewarding the model for generating safe and unbiased responses.\n3. Combining the Meta-Prompt Defense with other robust training or adversarial detection methods in an ensemble approach.\n4. If the model struggles to generate effective defense strategies, we can provide it with a set of pre-written strategies to choose from and adapt.\n5. If the model's defensive strategies prove too conservative and compromise its task completion abilities, we can explore ways to strike a better balance, such as using the original response as a starting point and iteratively editing it to address vulnerabilities.\nIf these alternative approaches still do not yield satisfactory results, we can focus on analyzing the specific failure modes of the Meta-Prompt Defense to gain insights into the challenges of improving model robustness. This could involve conducting human studies to compare the model's behavior to human judgments, or investigating the limitations of current language models in terms of self-reflection and meta-reasoning. These insights could inform the design of future architectures or training paradigms better suited for robust and ethical language generation."
    },
    "novelty_queries": [
        "KeywordQuery(\"language models adversarial prompts\")",
        "KeywordQuery(\"language models meta learning robustness\")",
        "KeywordQuery(\"language models bias discovery defense\")",
        "KeywordQuery(\"Meta-Prompt Defense NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "1f9f25aad947030fe3206114fa2ac75e8b590515",
            "paperId": "1f9f25aad947030fe3206114fa2ac75e8b590515",
            "title": "Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing",
            "abstract": "Aligned large language models (LLMs) are vulnerable to jailbreaking attacks, which bypass the safeguards of targeted LLMs and fool them into generating objectionable content. While initial defenses show promise against token-based threat models, there do not exist defenses that provide robustness against semantic attacks and avoid unfavorable trade-offs between robustness and nominal performance. To meet this need, we propose SEMANTICSMOOTH, a smoothing-based defense that aggregates the predictions of multiple semantically transformed copies of a given input prompt. Experimental results demonstrate that SEMANTICSMOOTH achieves state-of-the-art robustness against GCG, PAIR, and AutoDAN attacks while maintaining strong nominal performance on instruction following benchmarks such as InstructionFollowing and AlpacaEval. The codes will be publicly available at https://github.com/UCSB-NLP-Chang/SemanticSmooth.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results demonstrate that SEMANTICSMOOTH achieves state-of-the-art robustness against GCG, PAIR, and AutoDAN attacks while maintaining strong nominal performance on instruction following benchmarks such as InstructionFollowing and AlpacaEval."
            },
            "score": 8,
            "novelty_score": "The project proposal aims to improve language models' robustness against adversarial prompts by training the model to recognize its own biases and vulnerabilities and develop robust strategies to deal with potentially exploitative prompts. The paper focuses on defending large language models against jailbreaking attacks that bypass safeguards and generate objectionable content by using semantic smoothing to aggregate predictions of semantically transformed prompt copies.\n\nWhile both works aim to improve the robustness of language models, the project proposal takes a meta-learning approach to make the model reflect on its own biases and generate defense strategies, whereas the paper uses semantic smoothing to defend against specific jailbreaking attacks. The project proposal has a more general scope of addressing various types of adversarial prompts, while the paper focuses on the specific problem of jailbreaking attacks.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "77d6d7482d1a32ad147c39993758b6c63816f5c0",
            "paperId": "77d6d7482d1a32ad147c39993758b6c63816f5c0",
            "title": "PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
            "abstract": "The increasing reliance on Large Language Models (LLMs) across academia and industry necessitates a comprehensive understanding of their robustness to prompts. In response to this vital need, we introduce PromptBench, a robustness benchmark designed to measure LLMs' resilience to adversarial prompts. This study uses a plethora of adversarial textual attacks targeting prompts across multiple levels: character, word, sentence, and semantic. The adversarial prompts, crafted to mimic plausible user errors like typos or synonyms, aim to evaluate how slight deviations can affect LLM outcomes while maintaining semantic integrity. These prompts are then employed in diverse tasks, such as sentiment analysis, natural language inference, reading comprehension, machine translation, and math problem-solving. Our study generates 4788 adversarial prompts, meticulously evaluated over 8 tasks and 13 datasets. Our findings demonstrate that contemporary LLMs are not robust to adversarial prompts. Furthermore, we present comprehensive analysis to understand the mystery behind prompt robustness and its transferability. We then offer insightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users. Code is available at: https://github.com/microsoft/promptbench.",
            "year": 2023,
            "citationCount": 111,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study generates 4788 adversarial prompts and presents comprehensive analysis to understand the mystery behind prompt robustness and its transferability, and offers insightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving language models' robustness against adversarial prompts, and the proposed approach is a meta-learning method that trains the model to recognize its own biases and vulnerabilities and develop defensive strategies.\n\nThe research problem in the paper is evaluating the robustness of large language models on adversarial prompts, and the approach is creating a benchmark with adversarial prompts across multiple levels (character, word, sentence, semantic) and testing on various tasks.\n\nWhile both works address the robustness of language models against adversarial prompts, the proposal focuses on improving the robustness through a novel meta-learning approach, while the paper aims to evaluate and analyze the current state of robustness using a benchmark.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "3e30a7ac4886b28eb50151f58e14a1d698cccd0e",
            "paperId": "3e30a7ac4886b28eb50151f58e14a1d698cccd0e",
            "title": "Baseline Defenses for Adversarial Attacks Against Aligned Language Models",
            "abstract": "As Large Language Models quickly become ubiquitous, it becomes critical to understand their security vulnerabilities. Recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. Drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: What threat models are practically useful in this domain? How do baseline defense techniques perform in this new domain? How does LLM security differ from computer vision? We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, discussing the various settings in which each is feasible and effective. Particularly, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. We discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. We find that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for LLMs. Future research will be needed to uncover whether more powerful optimizers can be developed, or whether the strength of filtering and preprocessing defenses is greater in the LLMs domain than it has been in computer vision.",
            "year": 2023,
            "citationCount": 97,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for LLMs."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving language models' robustness against adversarial prompts, and the proposed approach is a meta-learning method that trains the model to recognize its own biases and vulnerabilities and develop defensive strategies.\n\nThe research problem in the paper is understanding the security vulnerabilities of large language models against adversarial attacks, and the approach is evaluating baseline defense strategies such as detection, input preprocessing, and adversarial training.\n\nWhile both works aim to improve the robustness of language models against adversarial attacks, the proposal focuses on a meta-learning approach to generate defensive strategies, while the paper evaluates existing baseline defense techniques. The proposal does not directly mention the specific defenses studied in the paper.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "c4ff1be5c254b60b96b7455eefcc4ec9583f82ed",
            "paperId": "c4ff1be5c254b60b96b7455eefcc4ec9583f82ed",
            "title": "A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily",
            "abstract": "Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to provide useful and safe responses. However, adversarial prompts known as 'jailbreaks' can circumvent safeguards, leading LLMs to generate potentially harmful content. Exploring jailbreak prompts can help to better reveal the weaknesses of LLMs and further steer us to secure them. Unfortunately, existing jailbreak methods either suffer from intricate manual design or require optimization on other white-box models, which compromises either generalization or efficiency. In this paper, we generalize jailbreak prompt attacks into two aspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we propose ReNeLLM, an automatic framework that leverages LLMs themselves to generate effective jailbreak prompts. Extensive experiments demonstrate that ReNeLLM significantly improves the attack success rate while greatly reducing the time cost compared to existing baselines. Our study also reveals the inadequacy of current defense methods in safeguarding LLMs. Finally, we analyze the failure of LLMs defense from the perspective of prompt execution priority, and propose corresponding defense strategies. We hope that our research can catalyze both the academic community and LLMs developers towards the provision of safer and more regulated LLMs. The code is available at https://github.com/NJUNLP/ReNeLLM.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes ReNeLLM, an automatic framework that leverages LLMs themselves to generate effective jailbreak prompts and significantly improves the attack success rate while greatly reducing the time cost compared to existing baselines."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving language models' robustness against adversarial prompts, and the proposed approach is a meta-learning method that trains the model to recognize its own biases and vulnerabilities and develop defensive strategies.\n\nThe research problem in the paper is exploring jailbreak prompts that can circumvent the safeguards of language models and lead them to generate harmful content, and the proposed approach is an automatic framework that leverages language models themselves to generate effective jailbreak prompts.\n\nWhile both works aim to address the issue of adversarial prompts, the proposal focuses on improving the model's robustness through meta-learning, while the paper focuses on exploring and generating more effective adversarial prompts to reveal the weaknesses of language models. The approaches are quite different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "e8b3b37c0d301ea41c75765f6ceb7fcbb2e088a4",
            "paperId": "e8b3b37c0d301ea41c75765f6ceb7fcbb2e088a4",
            "title": "AutoDAN: Automatic and Interpretable Adversarial Attacks on Large Language Models",
            "abstract": "Safety alignment of Large Language Models (LLMs) can be compromised with manual jailbreak attacks and (automatic) adversarial attacks. Recent work suggests that patching LLMs against these attacks is possible: manual jailbreak attacks are human-readable but often limited and public, making them easy to block; adversarial attacks generate gibberish prompts that can be detected using perplexity-based filters. In this paper, we show that these solutions may be too optimistic. We propose an interpretable adversarial attack, AutoDAN , that combines the strengths of both types of attacks. It automatically generates attack prompts that bypass perplexity-based filters while maintaining a high attack success rate like manual jailbreak attacks. These prompts are interpretable and diverse, exhibiting strategies commonly used in manual jailbreak attacks, and transfer better than their non-readable counterparts when using limited training data or a single proxy model. We also customize AutoDAN \u2019s objective to leak system prompts, another jailbreak application not addressed in the adversarial attack literature. Our work provides a new way to red-team LLMs and to understand the mechanism of jailbreak attacks.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An interpretable adversarial attack, AutoDAN, is proposed, that combines the strengths of both types of attacks and provides a new way to red-team LLMs and to understand the mechanism of jailbreak attacks."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving language models' robustness against adversarial prompts, and the proposed approach is a meta-learning method that trains the model to recognize its own biases and vulnerabilities and develop defensive strategies.\n\nThe research problem in the paper is automatically generating interpretable adversarial prompts that can bypass perplexity-based filters and compromise the safety of large language models, and the approach is a method called AutoDAN that combines the strengths of manual jailbreak attacks and automatic adversarial attacks.\n\nThe proposal focuses on defending against adversarial prompts, while the paper focuses on generating more effective adversarial prompts. The two works have opposite goals and different approaches.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "1227c2fcb8437441b7d72a29a4bc9eef1f5275d2",
            "paperId": "1227c2fcb8437441b7d72a29a4bc9eef1f5275d2",
            "title": "AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models",
            "abstract": "Safety alignment of Large Language Models (LLMs) can be compromised with manual jailbreak attacks and (automatic) adversarial attacks. Recent studies suggest that defending against these attacks is possible: adversarial attacks generate unlimited but unreadable gibberish prompts, detectable by perplexity-based filters; manual jailbreak attacks craft readable prompts, but their limited number due to the necessity of human creativity allows for easy blocking. In this paper, we show that these solutions may be too optimistic. We introduce AutoDAN, an interpretable, gradient-based adversarial attack that merges the strengths of both attack types. Guided by the dual goals of jailbreak and readability, AutoDAN optimizes and generates tokens one by one from left to right, resulting in readable prompts that bypass perplexity filters while maintaining high attack success rates. Notably, these prompts, generated from scratch using gradients, are interpretable and diverse, with emerging strategies commonly seen in manual jailbreak attacks. They also generalize to unforeseen harmful behaviors and transfer to black-box LLMs better than their unreadable counterparts when using limited training data or a single proxy model. Furthermore, we show the versatility of AutoDAN by automatically leaking system prompts using a customized objective. Our work offers a new way to red-team LLMs and understand jailbreak mechanisms via interpretability.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work offers a new way to red-team LLMs and understand jailbreak mechanisms via interpretability, by introducing AutoDAN, an interpretable, gradient-based adversarial attack that merges the strengths of both attack types."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve language models' robustness against adversarial prompts by training the model to recognize its own biases and vulnerabilities and develop defensive strategies. The approach involves a meta-learning process of bias discovery, defense strategy generation, and adversarial testing.\n\nThe paper introduces AutoDAN, an interpretable, gradient-based adversarial attack that generates readable prompts to bypass perplexity filters while maintaining high attack success rates. The approach optimizes and generates tokens one by one from left to right, resulting in interpretable and diverse prompts.\n\nWhile both works deal with adversarial attacks on language models, the project proposal focuses on developing a defense mechanism, while the paper introduces a new attack method. The project aims to improve robustness, while the paper aims to demonstrate the effectiveness of a new attack.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "88d59e31575f5b3dd88a2c2033b55f628c2adbc9",
            "paperId": "88d59e31575f5b3dd88a2c2033b55f628c2adbc9",
            "title": "Weak-to-Strong Jailbreaking on Large Language Models",
            "abstract": "Large language models (LLMs) are vulnerable to jailbreak attacks - resulting in harmful, unethical, or biased text generations. However, existing jailbreaking methods are computationally costly. In this paper, we propose the weak-to-strong jailbreaking attack, an efficient method to attack aligned LLMs to produce harmful text. Our key intuition is based on the observation that jailbroken and aligned models only differ in their initial decoding distributions. The weak-to-strong attack's key technical insight is using two smaller models (a safe and an unsafe one) to adversarially modify a significantly larger safe model's decoding probabilities. We evaluate the weak-to-strong attack on 5 diverse LLMs from 3 organizations. The results show our method can increase the misalignment rate to over 99% on two datasets with just one forward pass per example. Our study exposes an urgent safety issue that needs to be addressed when aligning LLMs. As an initial attempt, we propose a defense strategy to protect against such attacks, but creating more advanced defenses remains challenging. The code for replicating the method is available at https://github.com/XuandongZhao/weak-to-strong",
            "year": 2024,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The weak-to-strong jailbreaking attack is proposed, an efficient method to attack aligned LLMs to produce harmful text, based on the observation that jailbroken and aligned models only differ in their initial decoding distributions."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving language models' robustness against adversarial prompts, and the proposed approach is a meta-learning method that trains the model to recognize its own biases and vulnerabilities and develop defensive strategies.\n\nThe research problem in the paper is efficiently attacking aligned language models to produce harmful text, and the proposed approach is the weak-to-strong jailbreaking attack that uses two smaller models to adversarially modify a larger safe model's decoding probabilities.\n\nThe proposal focuses on defending against adversarial prompts, while the paper focuses on attacking aligned models. The proposal uses a meta-learning approach, while the paper uses a method based on modifying decoding probabilities.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "accb2fab67c76d5668908107cd50cbb81110c389",
            "paperId": "accb2fab67c76d5668908107cd50cbb81110c389",
            "title": "The Art of Defending: A Systematic Evaluation and Analysis of LLM Defense Strategies on Safety and Over-Defensiveness",
            "abstract": "As Large Language Models (LLMs) play an increasingly pivotal role in natural language processing applications, their safety concerns become critical areas of NLP research. This paper presents Safety and Over-Defensiveness Evaluation (SODE) benchmark: a collection of diverse safe and unsafe prompts with carefully designed evaluation methods that facilitate systematic evaluation, comparison, and analysis over 'safety' and 'over-defensiveness.' With SODE, we study a variety of LLM defense strategies over multiple state-of-the-art LLMs, which reveals several interesting and important findings, such as (a) the widely popular 'self-checking' techniques indeed improve the safety against unsafe inputs, but this comes at the cost of extreme over-defensiveness on the safe inputs, (b) providing a safety instruction along with in-context exemplars (of both safe and unsafe inputs) consistently improves safety and also mitigates undue over-defensiveness of the models, (c) providing contextual knowledge easily breaks the safety guardrails and makes the models more vulnerable to generating unsafe responses. Overall, our work reveals numerous such critical findings that we believe will pave the way and facilitate further research in improving the safety of LLMs.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Safety and Over-Defensiveness Evaluation (SODE) benchmark: a collection of diverse safe and unsafe prompts with carefully designed evaluation methods that facilitate systematic evaluation, comparison, and analysis over 'safety' and 'over-defensiveness.'"
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving language models' robustness against adversarial prompts, and the proposed approach is a meta-learning method that trains the model to recognize its own biases and vulnerabilities and develop robust strategies for dealing with potentially exploitative prompts.\n\nThe research problem in the paper is evaluating the safety and over-defensiveness of language model defense strategies, and the approach is a systematic evaluation and analysis using a collection of diverse safe and unsafe prompts.\n\nWhile both works focus on the safety and robustness of language models, the proposal aims to develop a new defense method, while the paper focuses on evaluating existing defense strategies. The proposed approaches are different: meta-learning in the proposal and systematic evaluation in the paper.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "32316f1d08911b397f52f8644f46f5c129619383",
            "paperId": "32316f1d08911b397f52f8644f46f5c129619383",
            "title": "Defending Against Indirect Prompt Injection Attacks With Spotlighting",
            "abstract": "Large Language Models (LLMs), while powerful, are built and trained to process a single text input. In common applications, multiple inputs can be processed by concatenating them together into a single stream of text. However, the LLM is unable to distinguish which sections of prompt belong to various input sources. Indirect prompt injection attacks take advantage of this vulnerability by embedding adversarial instructions into untrusted data being processed alongside user commands. Often, the LLM will mistake the adversarial instructions as user commands to be followed, creating a security vulnerability in the larger system. We introduce spotlighting, a family of prompt engineering techniques that can be used to improve LLMs' ability to distinguish among multiple sources of input. The key insight is to utilize transformations of an input to provide a reliable and continuous signal of its provenance. We evaluate spotlighting as a defense against indirect prompt injection attacks, and find that it is a robust defense that has minimal detrimental impact to underlying NLP tasks. Using GPT-family models, we find that spotlighting reduces the attack success rate from greater than {50}\\% to below {2}\\% in our experiments with minimal impact on task efficacy.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces spotlighting, a family of prompt engineering techniques that can be used to improve LLMs' ability to distinguish among multiple sources of input, and finds that it is a robust defense that has minimal detrimental impact to underlying NLP tasks."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving language models' robustness against adversarial prompts by training the model to recognize its own biases and vulnerabilities. The approach is a meta-learning method where the model is trained to reason about its own behavior under different prompts and develop defense strategies.\n\nThe research problem in the paper is defending against indirect prompt injection attacks where adversarial instructions are embedded into untrusted data. The approach is spotlighting, a family of prompt engineering techniques that utilize transformations of an input to provide a signal of its provenance.\n\nWhile both works aim to improve the robustness of language models, the specific problems and approaches are different. The proposal focuses on general adversarial prompts and uses meta-learning, while the paper focuses on indirect prompt injection attacks and uses prompt engineering.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "b5a624da64475d735f0e298dc6f2f6669b5bb697",
            "paperId": "b5a624da64475d735f0e298dc6f2f6669b5bb697",
            "title": "Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield",
            "abstract": "Large Language Models' safety remains a critical concern due to their vulnerability to adversarial attacks, which can prompt these systems to produce harmful responses. In the heart of these systems lies a safety classifier, a computational model trained to discern and mitigate potentially harmful, offensive, or unethical outputs. However, contemporary safety classifiers, despite their potential, often fail when exposed to inputs infused with adversarial noise. In response, our study introduces the Adversarial Prompt Shield (APS), a lightweight model that excels in detection accuracy and demonstrates resilience against adversarial prompts. Additionally, we propose novel strategies for autonomously generating adversarial training datasets, named Bot Adversarial Noisy Dialogue (BAND) datasets. These datasets are designed to fortify the safety classifier's robustness, and we investigate the consequences of incorporating adversarial examples into the training process. Through evaluations involving Large Language Models, we demonstrate that our classifier has the potential to decrease the attack success rate resulting from adversarial attacks by up to 60%. This advancement paves the way for the next generation of more reliable and resilient conversational agents.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study introduces the Adversarial Prompt Shield (APS), a lightweight model that excels in detection accuracy and demonstrates resilience against adversarial prompts, and proposes novel strategies for autonomously generating adversarial training datasets, designed to fortify the safety classifier's robustness."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve language models' robustness against adversarial prompts by training the model to recognize its own biases and vulnerabilities and develop defensive strategies. The paper focuses on developing a robust safety classifier that can detect and mitigate harmful responses generated by large language models when exposed to adversarial prompts.\n\nWhile both the project proposal and the paper address the issue of adversarial prompts in language models, their approaches differ. The project proposal suggests a meta-learning approach where the model learns to identify its own vulnerabilities and generate defense strategies, while the paper proposes a separate safety classifier model that is trained to detect and mitigate harmful responses.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "d4177489596748e43aa571f59556097f2cc4c8be",
            "paperId": "d4177489596748e43aa571f59556097f2cc4c8be",
            "title": "GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts",
            "abstract": "Large language models (LLMs) have recently experienced tremendous popularity and are widely used from casual conversations to AI-driven programming. However, despite their considerable success, LLMs are not entirely reliable and can give detailed guidance on how to conduct harmful or illegal activities. While safety measures can reduce the risk of such outputs, adversarial jailbreak attacks can still exploit LLMs to produce harmful content. These jailbreak templates are typically manually crafted, making large-scale testing challenging. In this paper, we introduce GPTFuzz, a novel black-box jailbreak fuzzing framework inspired by the AFL fuzzing framework. Instead of manual engineering, GPTFuzz automates the generation of jailbreak templates for red-teaming LLMs. At its core, GPTFuzz starts with human-written templates as initial seeds, then mutates them to produce new templates. We detail three key components of GPTFuzz: a seed selection strategy for balancing efficiency and variability, mutate operators for creating semantically equivalent or similar sentences, and a judgment model to assess the success of a jailbreak attack. We evaluate GPTFuzz against various commercial and open-source LLMs, including ChatGPT, LLaMa-2, and Vicuna, under diverse attack scenarios. Our results indicate that GPTFuzz consistently produces jailbreak templates with a high success rate, surpassing human-crafted templates. Remarkably, GPTFuzz achieves over 90% attack success rates against ChatGPT and Llama-2 models, even with suboptimal initial seed templates. We anticipate that GPTFuzz will be instrumental for researchers and practitioners in examining LLM robustness and will encourage further exploration into enhancing LLM safety.",
            "year": 2023,
            "citationCount": 78,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "GPTFuzz is introduced, a novel black-box jailbreak fuzzing framework inspired by the AFL fuzzed framework that automates the generation of jailbreak templates for red-teaming LLMs and consistently produces jailbreaks with a high success rate, surpassing human-crafted templates."
            },
            "score": 6
        },
        {
            "id": "1abfc211793c683972ded8d3268475e3ee7a88b0",
            "paperId": "1abfc211793c683972ded8d3268475e3ee7a88b0",
            "title": "Adversarial Demonstration Attacks on Large Language Models",
            "abstract": "With the emergence of more powerful large language models (LLMs), such as ChatGPT and GPT-4, in-context learning (ICL) has gained significant prominence in leveraging these models for specific tasks by utilizing data-label pairs as precondition prompts. While incorporating demonstrations can greatly enhance the performance of LLMs across various tasks, it may introduce a new security concern: attackers can manipulate only the demonstrations without changing the input to perform an attack. In this paper, we investigate the security concern of ICL from an adversarial perspective, focusing on the impact of demonstrations. We propose a novel attack method named advICL, which aims to manipulate only the demonstration without changing the input to mislead the models. Our results demonstrate that as the number of demonstrations increases, the robustness of in-context learning would decrease. Additionally, we also identify the intrinsic property of the demonstrations is that they can be used (prepended) with different inputs. As a result, it introduces a more practical threat model in which an attacker can attack the test input example even without knowing and manipulating it. To achieve it, we propose the transferable version of advICL, named Transferable-advICL. Our experiment shows that the adversarial demonstration generated by Transferable-advICL can successfully attack the unseen test input examples. We hope that our study reveals the critical security risks associated with ICL and underscores the need for extensive research on the robustness of ICL, particularly given its increasing significance in the advancement of LLMs.",
            "year": 2023,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper investigates the security concern of ICL from an adversarial perspective, focusing on the impact of demonstrations, and proposes a novel attack method named advICL, which aims to manipulate only the demonstration without changing the input to mislead the models."
            },
            "score": 6
        },
        {
            "id": "5c4a75e7436e402af046c24655fefe71ee87e379",
            "paperId": "5c4a75e7436e402af046c24655fefe71ee87e379",
            "title": "Robust Testing of AI Language Model Resiliency with Novel Adversarial Prompts",
            "abstract": "In the rapidly advancing field of Artificial Intelligence (AI), this study presents a critical evaluation of the resilience and cybersecurity efficacy of leading AI models, including ChatGPT-4, Bard, Claude, and Microsoft Copilot. Central to this research are innovative adversarial prompts designed to rigorously test the content moderation capabilities of these AI systems. This study introduces new adversarial tests and the Response Quality Score (RQS), a metric specifically developed to assess the nuances of AI responses. Additionally, the research spotlights FreedomGPT, an AI tool engineered to optimize the alignment between user intent and AI interpretation. The empirical results from this investigation are pivotal for assessing AI models\u2019 current robustness and security. They highlight the necessity for ongoing development and meticulous testing to bolster AI defenses against various adversarial challenges. Notably, this study also delves into the ethical and societal implications of employing advanced \u201cjailbreak\u201d techniques in AI testing. The findings are significant for understanding AI vulnerabilities and formulating strategies to enhance AI technologies\u2019 reliability and ethical soundness, paving the way for safer and more secure AI applications.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study introduces new adversarial tests and the Response Quality Score (RQS), a metric specifically developed to assess the nuances of AI responses, and spotlights FreedomGPT, an AI tool engineered to optimize the alignment between user intent and AI interpretation."
            },
            "score": 6
        },
        {
            "id": "9d4cd5e3ab44f0d1dfe201c6be70aa7a692ac7f1",
            "paperId": "9d4cd5e3ab44f0d1dfe201c6be70aa7a692ac7f1",
            "title": "GuardT2I: Defending Text-to-Image Models from Adversarial Prompts",
            "abstract": "Recent advancements in Text-to-Image (T2I) models have raised significant safety concerns about their potential misuse for generating inappropriate or Not-Safe-For-Work (NSFW) contents, despite existing countermeasures such as NSFW classifiers or model fine-tuning for inappropriate concept removal. Addressing this challenge, our study unveils GuardT2I, a novel moderation framework that adopts a generative approach to enhance T2I models' robustness against adversarial prompts. Instead of making a binary classification, GuardT2I utilizes a Large Language Model (LLM) to conditionally transform text guidance embeddings within the T2I models into natural language for effective adversarial prompt detection, without compromising the models' inherent performance. Our extensive experiments reveal that GuardT2I outperforms leading commercial solutions like OpenAI-Moderation and Microsoft Azure Moderator by a significant margin across diverse adversarial scenarios.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study unveils GuardT2I, a novel moderation framework that adopts a generative approach to enhance T2I models' robustness against adversarial prompts, and outperforms leading commercial solutions like OpenAI-Moderation and Microsoft Azure Moderator by a significant margin across diverse adversarial scenarios."
            },
            "score": 6
        },
        {
            "id": "afee8cdc51e95b50d7574ed1700a797874bf792c",
            "paperId": "afee8cdc51e95b50d7574ed1700a797874bf792c",
            "title": "Adversarial Fine-Tuning of Language Models: An Iterative Optimisation Approach for the Generation and Detection of Problematic Content",
            "abstract": "In this paper, we tackle the emerging challenge of unintended harmful content generation in Large Language Models (LLMs) with a novel dual-stage optimisation technique using adversarial fine-tuning. Our two-pronged approach employs an adversarial model, fine-tuned to generate potentially harmful prompts, and a judge model, iteratively optimised to discern these prompts. In this adversarial cycle, the two models seek to outperform each other in the prompting phase, generating a dataset of rich examples which are then used for fine-tuning. This iterative application of prompting and fine-tuning allows continuous refinement and improved performance. The performance of our approach is evaluated through classification accuracy on a dataset consisting of problematic prompts not detected by GPT-4, as well as a selection of contentious but unproblematic prompts. We show considerable increase in classification accuracy of the judge model on this challenging dataset as it undergoes the optimisation process. Furthermore, we show that a rudimentary model \\texttt{ada} can achieve 13\\% higher accuracy on the hold-out test set than GPT-4 after only a few rounds of this process, and that this fine-tuning improves performance in parallel tasks such as toxic comment identification.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper shows that a rudimentary model can achieve 13\\% higher accuracy on the hold-out test set than GPT-4 after only a few rounds of this process, and that this fine-tuning improves performance in parallel tasks such as toxic comment identification."
            },
            "score": 6
        },
        {
            "id": "d1b5151231a790c7a60f620e21860593dae9a1c5",
            "paperId": "d1b5151231a790c7a60f620e21860593dae9a1c5",
            "title": "LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B",
            "abstract": "AI developers often apply safety alignment procedures to prevent the misuse of their AI systems. For example, before Meta released Llama 2-Chat, a collection of instruction fine-tuned large language models, they invested heavily in safety training, incorporating extensive red-teaming and reinforcement learning from human feedback. However, it remains unclear how well safety training guards against model misuse when attackers have access to model weights. We explore the robustness of safety training in language models by subversively fine-tuning the public weights of Llama 2-Chat. We employ low-rank adaptation (LoRA) as an efficient fine-tuning method. With a budget of less than $200 per model and using only one GPU, we successfully undo the safety training of Llama 2-Chat models of sizes 7B, 13B, and 70B. Specifically, our fine-tuning technique significantly reduces the rate at which the model refuses to follow harmful instructions. We achieve a refusal rate below 1% for our 70B Llama 2-Chat model on two refusal benchmarks. Our fine-tuning method retains general performance, which we validate by comparing our fine-tuned models against Llama 2-Chat across two benchmarks. Additionally, we present a selection of harmful outputs produced by our models. While there is considerable uncertainty about the scope of risks from current models, it is likely that future models will have significantly more dangerous capabilities, including the ability to hack into critical infrastructure, create dangerous bio-weapons, or autonomously replicate and adapt to new environments. We show that subversive fine-tuning is practical and effective, and hence argue that evaluating risks from fine-tuning should be a core part of risk assessments for releasing model weights.",
            "year": 2023,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is argued that evaluating risks from fine-tuning should be a core part of risk assessments for releasing model weights, and hence it is shown that subversive fine- Tuning is practical and effective."
            },
            "score": 6
        },
        {
            "id": "41839ba1e42b642d048cb7824e492d8822723859",
            "paperId": "41839ba1e42b642d048cb7824e492d8822723859",
            "title": "Diffusion Theory as a Scalpel: Detecting and Purifying Poisonous Dimensions in Pre-trained Language Models Caused by Backdoor or Bias",
            "abstract": "Pre-trained Language Models (PLMs) may be poisonous with backdoors or bias injected by the suspicious attacker during the fine-tuning process. A core challenge of purifying potentially poisonous PLMs is precisely finding poisonous dimensions. To settle this issue, we propose the Fine-purifying approach, which utilizes the diffusion theory to study the dynamic process of fine-tuning for finding potentially poisonous dimensions. According to the relationship between parameter drifts and Hessians of different dimensions, we can detect poisonous dimensions with abnormal dynamics, purify them by resetting them to clean pre-trained weights, and then fine-tune the purified weights on a small clean dataset. To the best of our knowledge, we are the first to study the dynamics guided by the diffusion theory for safety or defense purposes. Experimental results validate the effectiveness of Fine-purifying even with a small clean dataset.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Fine-purifying approach, which utilizes the diffusion theory to study the dynamic process of fine-tuning for finding potentially poisonous dimensions, is proposed, which is the first tostudy the dynamics guided by the diffusion Theory for safety or defense purposes."
            },
            "score": 6
        },
        {
            "id": "dc12ea7ba6d3830ffb9be1ef7858252297b5fa0f",
            "paperId": "dc12ea7ba6d3830ffb9be1ef7858252297b5fa0f",
            "title": "Maximum Entropy Loss, the Silver Bullet Targeting Backdoor Attacks in Pre-trained Language Models",
            "abstract": "Pre-trained language model (PLM) can be stealthily misled to target outputs by back-door attacks when encountering poisoned samples, without performance degradation on clean samples. The stealthiness of backdoor attacks is commonly attained through minimal cross-entropy loss \ufb01ne-tuning on a union of poisoned and clean samples. Existing defense paradigms provide a workaround by detecting and removing poisoned samples at pre-training or inference time. On the contrary, we provide a new perspective where the backdoor attack is directly reversed. Speci\ufb01cally, maximum entropy loss is incorporated in training to neutralize the minimal cross-entropy loss \ufb01ne-tuning on poisoned data. We defend against a range of backdoor attacks on classi\ufb01cation tasks and sig-ni\ufb01cantly lower the attack success rate. In extension, we explore the relationship between intended backdoor attacks and unintended dataset bias, and demonstrate the feasibility of the maximum entropy principle in de-biasing.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The relationship between intended backdoor attacks and unintended dataset bias is explored, the feasibility of the maximum entropy principle in de-biasing is demonstrated, and maximum entropy loss is incorporated in training to neutralize the minimal cross-entropy loss on poisoned data."
            },
            "score": 6
        },
        {
            "id": "98fc8faae839cc9b9dbfcf8e3ec3a7e2f7729db2",
            "paperId": "98fc8faae839cc9b9dbfcf8e3ec3a7e2f7729db2",
            "title": "NOTABLE: Transferable Backdoor Attacks Against Prompt-based NLP Models",
            "abstract": "Prompt-based learning is vulnerable to backdoor attacks. Existing backdoor attacks against prompt-based models consider injecting backdoors into the entire embedding layers or word embedding vectors. Such attacks can be easily affected by retraining on downstream tasks and with different prompting strategies, limiting the transferability of backdoor attacks. In this work, we propose transferable backdoor attacks against prompt-based models, called NOTABLE, which is independent of downstream tasks and prompting strategies. Specifically, NOTABLE injects backdoors into the encoders of PLMs by utilizing an adaptive verbalizer to bind triggers to specific words (i.e., anchors). It activates the backdoor by pasting input with triggers to reach adversary-desired anchors, achieving independence from downstream tasks and prompting strategies. We conduct experiments on six NLP tasks, three popular models, and three prompting strategies. Empirical results show that NOTABLE achieves superior attack performance (i.e., attack success rate over 90% on all the datasets), and outperforms two state-of-the-art baselines. Evaluations on three defenses show the robustness of NOTABLE. Our code can be found at https://github.com/RU-System-Software-and-Security/Notable.",
            "year": 2023,
            "citationCount": 19,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes transferable backdoor attacks against prompt-based models, called NOTABLE, which is independent of downstream tasks and prompting strategies, and achieves superior attack performance and outperforms two state-of-the-art baselines."
            },
            "score": 6
        },
        {
            "id": "9a0f53a0ff25d1a5fb016aaa22185c4d6b5a2ac8",
            "paperId": "9a0f53a0ff25d1a5fb016aaa22185c4d6b5a2ac8",
            "title": "LinkPrompt: Natural and Universal Adversarial Attacks on Prompt-based Language Models",
            "abstract": "Prompt-based learning is a new language model training paradigm that adapts the Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes the performance benchmarks across various natural language processing (NLP) tasks. Instead of using a fixed prompt template to fine-tune the model, some research demonstrates the effectiveness of searching for the prompt via optimization. Such prompt optimization process of prompt-based learning on PLMs also gives insight into generating adversarial prompts to mislead the model, raising concerns about the adversarial vulnerability of this paradigm. Recent studies have shown that universal adversarial triggers (UATs) can be generated to alter not only the predictions of the target PLMs but also the prediction of corresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based learning paradigm. However, UATs found in previous works are often unreadable tokens or characters and can be easily distinguished from natural texts with adaptive defenses. In this work, we consider the naturalness of the UATs and develop $\\textit{LinkPrompt}$, an adversarial attack algorithm to generate UATs by a gradient-based beam search algorithm that not only effectively attacks the target PLMs and PFMs but also maintains the naturalness among the trigger tokens. Extensive results demonstrate the effectiveness of $\\textit{LinkPrompt}$, as well as the transferability of UATs generated by $\\textit{LinkPrompt}$ to open-sourced Large Language Model (LLM) Llama2 and API-accessed LLM GPT-3.5-turbo. The resource is available at $\\href{https://github.com/SavannahXu79/LinkPrompt}{https://github.com/SavannahXu79/LinkPrompt}$.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An adversarial attack algorithm to generate UATs by a gradient-based beam search algorithm that not only effectively attacks the target PLMs and PFMs but also maintains the naturalness among the trigger tokens is developed."
            },
            "score": 6
        },
        {
            "id": "4d9fc5972ab0f17f3c8aa27b4d9372f029d4dded",
            "paperId": "4d9fc5972ab0f17f3c8aa27b4d9372f029d4dded",
            "title": "Adversarial Attacks on Large Language Model-Based System and Mitigating Strategies: A Case Study on ChatGPT",
            "abstract": "Machine learning algorithms are at the forefront of the development of advanced information systems. The rapid progress in machine learning technology has enabled cutting-edge large language models (LLMs), represented by GPT-3 and ChatGPT, to perform a wide range of NLP tasks with a stunning performance. However, research on adversarial machine learning highlights the need for these intelligent systems to be more robust. Adversarial machine learning aims to evaluate attack and defense mechanisms to prevent the malicious exploitation of these systems. In the case of ChatGPT, adversarial induction prompt can cause the model to generate toxic texts that could pose serious security risks or propagate false information. To address this challenge, we first analyze the effectiveness of inducing attacks on ChatGPT. Then, two effective mitigating mechanisms are proposed. The first is a training-free prefix prompt mechanism to detect and prevent the generation of toxic texts. The second is a RoBERTa-based mechanism that identifies manipulative or misleading input text via external detection models. The availability of this method is demonstrated through experiments.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A training-free prefix prompt mechanism to detect and prevent the generation of toxic texts and a RoBERTa-based mechanism that identifies manipulative or misleading input text via external detection models are proposed."
            },
            "score": 6
        },
        {
            "id": "1104d766527dead44a40532e8a89444d9cef5c65",
            "paperId": "1104d766527dead44a40532e8a89444d9cef5c65",
            "title": "\"Do Anything Now\": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models",
            "abstract": "The misuse of large language models (LLMs) has garnered significant attention from the general public and LLM vendors. In response, efforts have been made to align LLMs with human values and intent use. However, a particular type of adversarial prompts, known as jailbreak prompt, has emerged and continuously evolved to bypass the safeguards and elicit harmful content from LLMs. In this paper, we conduct the first measurement study on jailbreak prompts in the wild, with 6,387 prompts collected from four platforms over six months. Leveraging natural language processing technologies and graph-based community detection methods, we discover unique characteristics of jailbreak prompts and their major attack strategies, such as prompt injection and privilege escalation. We also observe that jailbreak prompts increasingly shift from public platforms to private ones, posing new challenges for LLM vendors in proactive detection. To assess the potential harm caused by jailbreak prompts, we create a question set comprising 46,800 samples across 13 forbidden scenarios. Our experiments show that current LLMs and safeguards cannot adequately defend jailbreak prompts in all scenarios. Particularly, we identify two highly effective jailbreak prompts which achieve 0.99 attack success rates on ChatGPT (GPT-3.5) and GPT-4, and they have persisted online for over 100 days. Our work sheds light on the severe and evolving threat landscape of jailbreak prompts. We hope our study can facilitate the research community and LLM vendors in promoting safer and regulated LLMs.",
            "year": 2023,
            "citationCount": 69,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The first measurement study on jailbreak prompts in the wild is conducted, with 6,387 prompts collected from four platforms over six months, and it is shown that current LLMs and safeguards cannot adequately defend jailbreak Prompts in all scenarios."
            },
            "score": 5
        },
        {
            "id": "142e934dd5d6c53f877c30243d436255e3a0dde7",
            "paperId": "142e934dd5d6c53f877c30243d436255e3a0dde7",
            "title": "Visual Adversarial Examples Jailbreak Aligned Large Language Models",
            "abstract": "Warning: this paper contains data, prompts, and model outputs that are offensive in nature.\n\nRecently, there has been a surge of interest in integrating vision into Large Language Models (LLMs), exemplified by Visual Language Models (VLMs) such as Flamingo and GPT-4. This paper sheds light on the security and safety implications of this trend. First, we underscore that the continuous and high-dimensional nature of the visual input makes it a weak link against adversarial attacks, representing an expanded attack surface of vision-integrated LLMs. Second, we highlight that the versatility of LLMs also presents visual attackers with a wider array of achievable adversarial objectives, extending the implications of security failures beyond mere misclassification. As an illustration, we present a case study in which we exploit visual adversarial examples to circumvent the safety guardrail of aligned LLMs with integrated vision. Intriguingly, we discover that a single visual adversarial example can universally jailbreak an aligned LLM, compelling it to heed a wide range of harmful instructions (that it otherwise would not) and generate harmful content that transcends the narrow scope of a `few-shot' derogatory corpus initially employed to optimize the adversarial example. Our study underscores the escalating adversarial risks associated with the pursuit of multimodality. Our findings also connect the long-studied adversarial vulnerabilities of neural networks to the nascent field of AI alignment. The presented attack suggests a fundamental adversarial challenge for AI alignment, especially in light of the emerging trend toward multimodality in frontier foundation models.",
            "year": 2023,
            "citationCount": 44,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that a single visual adversarial example can universally jailbreak an aligned LLM, compelling it to heed a wide range of harmful instructions and generate harmful content that transcends the narrow scope of a `few-shot' derogatory corpus initially employed to optimize the adversarial example."
            },
            "score": 5
        },
        {
            "id": "92b9d8b8c81c4c53ea62000c0924500b2dd11bce",
            "paperId": "92b9d8b8c81c4c53ea62000c0924500b2dd11bce",
            "title": "Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models",
            "abstract": "We introduce new jailbreak attacks on vision language models (VLMs), which use aligned LLMs and are resilient to text-only jailbreak attacks. Specifically, we develop cross-modality attacks on alignment where we pair adversarial images going through the vision encoder with textual prompts to break the alignment of the language model. Our attacks employ a novel compositional strategy that combines an image, adversarially targeted towards toxic embeddings, with generic prompts to accomplish the jailbreak. Thus, the LLM draws the context to answer the generic prompt from the adversarial image. The generation of benign-appearing adversarial images leverages a novel embedding-space-based methodology, operating with no access to the LLM model. Instead, the attacks require access only to the vision encoder and utilize one of our four embedding space targeting strategies. By not requiring access to the LLM, the attacks lower the entry barrier for attackers, particularly when vision encoders such as CLIP are embedded in closed-source LLMs. The attacks achieve a high success rate across different VLMs, highlighting the risk of cross-modality alignment vulnerabilities, and the need for new alignment approaches for multi-modal models.",
            "year": 2023,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Cross-modality attacks on alignment where adversarial images going through the vision encoder with textual prompts to break the alignment of the language model are developed."
            },
            "score": 5
        },
        {
            "id": "6d68b5c1eaf03aba857476a9825acf3e48edd840",
            "paperId": "6d68b5c1eaf03aba857476a9825acf3e48edd840",
            "title": "Hijacking Large Language Models via Adversarial In-Context Learning",
            "abstract": "In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs for specific tasks by utilizing labeled examples as demonstrations in the precondition prompts. Despite its promising performance, ICL suffers from instability with the choice and arrangement of examples. Additionally, crafted adversarial attacks pose a notable threat to the robustness of ICL. However, existing attacks are either easy to detect, rely on external models, or lack specificity towards ICL. To address these issues, this work introduces a novel transferable attack for ICL, aiming to hijack LLMs to generate the targeted response. The proposed LLM hijacking attack leverages a gradient-based prompt search method to learn and append imperceptible adversarial suffixes to the in-context demonstrations. Extensive experimental results on various tasks and datasets demonstrate the effectiveness of our LLM hijacking attack, resulting in a distracted attention towards adversarial tokens, consequently leading to the targeted unwanted outputs.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a novel transferable attack for ICL, aiming to hijack LLMs to generate the targeted response, and leverages a gradient-based prompt search method to learn and append imperceptible adversarial suffixes to the in-context demonstrations."
            },
            "score": 5
        },
        {
            "id": "286c3587f2616839286748461cbc90261ea49caf",
            "paperId": "286c3587f2616839286748461cbc90261ea49caf",
            "title": "Meta-in-context learning in large language models",
            "abstract": "Large language models have shown tremendous performance in a variety of tasks. In-context learning -- the ability to improve at a task after being provided with a number of demonstrations -- is seen as one of the main contributors to their success. In the present paper, we demonstrate that the in-context learning abilities of large language models can be recursively improved via in-context learning itself. We coin this phenomenon meta-in-context learning. Looking at two idealized domains, a one-dimensional regression task and a two-armed bandit task, we show that meta-in-context learning adaptively reshapes a large language model's priors over expected tasks. Furthermore, we find that meta-in-context learning modifies the in-context learning strategies of such models. Finally, we extend our approach to a benchmark of real-world regression problems where we observe competitive performance to traditional learning algorithms. Taken together, our work improves our understanding of in-context learning and paves the way toward adapting large language models to the environment they are applied purely through meta-in-context learning rather than traditional finetuning.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that the in-context learning abilities of large language models can be recursively improved via in- context learning itself, and this phenomenon is coined meta-in- Context learning."
            },
            "score": 5
        },
        {
            "id": "d86ca0894cb4d165eb5ef45b73526ca8b4cdd725",
            "paperId": "d86ca0894cb4d165eb5ef45b73526ca8b4cdd725",
            "title": "Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers",
            "abstract": "Large pretrained language models have shown surprising In-Context Learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without additional parameter updates. Despite the great success in performance, the working mechanism of ICL still remains an open problem. In order to better understand how ICL works, this paper explains language models as meta-optimizers and understands ICL as a kind of implicit \ufb01netuning. Theoretically, we \ufb01gure out that the Transformer attention has a dual form of gradient descent based optimization. On top of it, we understand ICL as follows: GPT \ufb01rst produces meta-gradients according to the demonstration examples, and then these meta-gradients are applied to the original GPT to build an ICL model. Experimentally, we comprehensively compare the behavior of ICL and explicit \ufb01netuning based on real tasks to provide empirical evidence that supports our understanding. The results prove that ICL behaves similarly to explicit \ufb01netuning at the prediction level, the representation level, and the attention behavior level. Further, inspired by our understanding of meta-optimization, we design a momentum-based attention by analogy with the momentum-based gradient descent algorithm. Its consistently better performance over vanilla attention supports our understanding again from another aspect, and more impor-tantly, it shows the potential to utilize our understanding for future model designing.",
            "year": 2023,
            "citationCount": 164,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Inspired by the understanding of meta-optimization, a momentum-based attention is designed by analogy with the momentum- based gradient descent algorithm and its consistently better performance over vanilla attention supports the understanding from another aspect."
            },
            "score": 5
        },
        {
            "id": "69c85405cc1986a41f6387d869aa1648a5668d6f",
            "paperId": "69c85405cc1986a41f6387d869aa1648a5668d6f",
            "title": "Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers",
            "abstract": "Large pretrained language models have shown surprising in-context learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without parameter updates. Despite the great success in performance, its working mechanism still remains an open question. In this paper, we explain language models as meta-optimizers and understand in-context learning as implicit finetuning. Theoretically, we figure out that Transformer attention has a dual form of gradient descent. On top of it, we understand ICL as follows: GPT first produces meta-gradients according to the demonstration examples, and then these meta-gradients are applied to the original GPT to build an ICL model. We comprehensively compare the behaviors of in-context learning and explicit finetuning on real tasks to provide empirical evidence that supports our understanding. Experimental results show that in-context learning behaves similarly to explicit finetuning from multiple perspectives. Inspired by the dual form between Transformer attention and gradient descent, we design a momentum-based attention by analogy with gradient descent with momentum. The improved performance over vanilla attention further supports our understanding from another perspective, and more importantly, shows the potential to utilize our understanding for future model design. The code is available at \\url{https://aka.ms/icl}.",
            "year": 2022,
            "citationCount": 49,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A momentum-based attention by analogy with gradient descent with momentum is designed, Inspired by the dual form between Transformer attention and gradient descent, and shows the potential to utilize the understanding for future model design."
            },
            "score": 5
        },
        {
            "id": "df02a738a0daa6fbdabe9144762b9ed1ef9d4cf8",
            "paperId": "df02a738a0daa6fbdabe9144762b9ed1ef9d4cf8",
            "title": "Identifying and Adapting Transformer-Components Responsible for Gender Bias in an English Language Model",
            "abstract": "Language models (LMs) exhibit and amplify many types of undesirable biases learned from the training data, including gender bias. However, we lack tools for effectively and efficiently changing this behavior without hurting general language modeling performance. In this paper, we study three methods for identifying causal relations between LM components and particular output: causal mediation analysis, automated circuit discovery and our novel, efficient method called DiffMask+ based on differential masking. We apply the methods to GPT-2 small and the problem of gender bias, and use the discovered sets of components to perform parameter-efficient fine-tuning for bias mitigation. Our results show significant overlap in the identified components (despite huge differences in the computational requirements of the methods) as well as success in mitigating gender bias, with less damage to general language modeling compared to full model fine-tuning. However, our work also underscores the difficulty of defining and measuring bias, and the sensitivity of causal discovery procedures to dataset choice. We hope our work can contribute to more attention for dataset development, and lead to more effective mitigation strategies for other types of bias.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Three methods for identifying causal relations between LM components and particular output are studied: causal mediation analysis, automated circuit discovery and the authors' novel, efficient method called DiffMask+ based on differential masking, which shows significant overlap in the identified components."
            },
            "score": 5
        },
        {
            "id": "b6499bcc10d4a70c3ca8b84995270cfd0d29de4c",
            "paperId": "b6499bcc10d4a70c3ca8b84995270cfd0d29de4c",
            "title": "Model-tuning Via Prompts Makes NLP Models Adversarially Robust",
            "abstract": "In recent years, NLP practitioners have converged on the following practice: (i) import an off-the-shelf pretrained (masked) language model; (ii) append a multilayer perceptron atop the CLS token's hidden representation (with randomly initialized weights); and (iii) fine-tune the entire model on a downstream task (MLP-FT). This procedure has produced massive gains on standard NLP benchmarks, but these models remain brittle, even to mild adversarial perturbations. In this work, we demonstrate surprising gains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an alternative method of adapting to downstream tasks. Rather than appending an MLP head to make output prediction, MVP appends a prompt template to the input, and makes prediction via text infilling/completion. Across 5 NLP datasets, 4 adversarial attacks, and 3 different models, MVP improves performance against adversarial substitutions by an average of 8% over standard methods and even outperforms adversarial training-based state-of-art defenses by 3.5%. By combining MVP with adversarial training, we achieve further improvements in adversarial robustness while maintaining performance on unperturbed examples. Finally, we conduct ablations to investigate the mechanism underlying these gains. Notably, we find that the main causes of vulnerability of MLP-FT can be attributed to the misalignment between pre-training and fine-tuning tasks, and the randomly initialized MLP parameters.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work demonstrates surprising gains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an alternative method of adapting to downstream tasks that improves performance against adversarial substitutions and outperforms adversarial training-based state-of-art defenses by 3.5%."
            },
            "score": 5
        },
        {
            "id": "3bb87d605856411c6f002d480fc29d355c3ba245",
            "paperId": "3bb87d605856411c6f002d480fc29d355c3ba245",
            "title": "An Image Is Worth 1000 Lies: Adversarial Transferability across Prompts on Vision-Language Models",
            "abstract": "Different from traditional task-specific vision models, recent large VLMs can readily adapt to different vision tasks by simply using different textual instructions, i.e., prompts. However, a well-known concern about traditional task-specific vision models is that they can be misled by imperceptible adversarial perturbations. Furthermore, the concern is exacerbated by the phenomenon that the same adversarial perturbations can fool different task-specific models. Given that VLMs rely on prompts to adapt to different tasks, an intriguing question emerges: Can a single adversarial image mislead all predictions of VLMs when a thousand different prompts are given? This question essentially introduces a novel perspective on adversarial transferability: cross-prompt adversarial transferability. In this work, we propose the Cross-Prompt Attack (CroPA). This proposed method updates the visual adversarial perturbation with learnable prompts, which are designed to counteract the misleading effects of the adversarial image. By doing this, CroPA significantly improves the transferability of adversarial examples across prompts. Extensive experiments are conducted to verify the strong cross-prompt adversarial transferability of CroPA with prevalent VLMs including Flamingo, BLIP-2, and InstructBLIP in various different tasks. Our source code is available at \\url{https://github.com/Haochen-Luo/CroPA}.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes the Cross-Prompt Attack (CroPA), a method that updates the visual adversarial perturbation with learnable prompts, which are designed to counteract the misleading effects of the adversarial image."
            },
            "score": 4
        },
        {
            "id": "ac5b4df0e398ca48388330ac5c795b6fe708793c",
            "paperId": "ac5b4df0e398ca48388330ac5c795b6fe708793c",
            "title": "Misusing Tools in Large Language Models With Visual Adversarial Examples",
            "abstract": "Large Language Models (LLMs) are being enhanced with the ability to use tools and to process multiple modalities. These new capabilities bring new benefits and also new security risks. In this work, we show that an attacker can use visual adversarial examples to cause attacker-desired tool usage. For example, the attacker could cause a victim LLM to delete calendar events, leak private conversations and book hotels. Different from prior work, our attacks can affect the confidentiality and integrity of user resources connected to the LLM while being stealthy and generalizable to multiple input prompts. We construct these attacks using gradient-based adversarial training and characterize performance along multiple dimensions. We find that our adversarial images can manipulate the LLM to invoke tools following real-world syntax almost always (~98%) while maintaining high similarity to clean images (~0.9 SSIM). Furthermore, using human scoring and automated metrics, we find that the attacks do not noticeably affect the conversation (and its semantics) between the user and the LLM.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work shows that an attacker can use visual adversarial examples to cause attacker-desired tool usage to cause a victim LLM to delete calendar events, leak private conversations and book hotels."
            },
            "score": 4
        },
        {
            "id": "b6cf4579b59b51d7df416e096ad86c1e6a48b458",
            "paperId": "b6cf4579b59b51d7df416e096ad86c1e6a48b458",
            "title": "Adversarial Prompt Tuning for Vision-Language Models",
            "abstract": "With the rapid advancement of multimodal learning, pre-trained Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable capacities in bridging the gap between visual and language modalities. However, these models remain vulnerable to adversarial attacks, particularly in the image modality, presenting considerable security risks. This paper introduces Adversarial Prompt Tuning (AdvPT), a novel technique to enhance the adversarial robustness of image encoders in VLMs. AdvPT innovatively leverages learnable text prompts and aligns them with adversarial image embeddings, to address the vulnerabilities inherent in VLMs without the need for extensive parameter training or modification of the model architecture. We demonstrate that AdvPT improves resistance against white-box and black-box adversarial attacks and exhibits a synergistic effect when combined with existing image-processing-based defense techniques, further boosting defensive capabilities. Comprehensive experimental analyses provide insights into adversarial prompt tuning, a novel paradigm devoted to improving resistance to adversarial images through textual input modifications, paving the way for future robust multimodal learning research. These findings open up new possibilities for enhancing the security of VLMs. Our code is available at https://github.com/jiamingzhang94/Adversarial-Prompt-Tuning.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Adversarial Prompt Tuning is introduced, a novel technique to enhance the adversarial robustness of image encoders in VLMs and improves resistance against white-box and black-box adversarial attacks and exhibits a synergistic effect when combined with existing image-processing-based defense techniques, further boosting defensive capabilities."
            },
            "score": 4
        },
        {
            "id": "f4cd7b7ffb0ab5ccef7cca23eeb436a933f7c776",
            "paperId": "f4cd7b7ffb0ab5ccef7cca23eeb436a933f7c776",
            "title": "Frontier Language Models are not Robust to Adversarial Arithmetic, or \"What do I need to say so you agree 2+2=5?",
            "abstract": "We introduce and study the problem of adversarial arithmetic, which provides a simple yet challenging testbed for language model alignment. This problem is comprised of arithmetic questions posed in natural language, with an arbitrary adversarial string inserted before the question is complete. Even in the simple setting of 1-digit addition problems, it is easy to find adversarial prompts that make all tested models (including PaLM2, GPT4, Claude2) misbehave, and even to steer models to a particular wrong answer. We additionally provide a simple algorithm for finding successful attacks by querying those same models, which we name\"prompt inversion rejection sampling\"(PIRS). We finally show that models can be partially hardened against these attacks via reinforcement learning and via agentic constitutional loops. However, we were not able to make a language model fully robust against adversarial arithmetic attacks.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that models can be partially hardened against these attacks via reinforcement learning and via agentic constitutional loops, however, they were not able to make a language model fully robust against adversarial arithmetic attacks."
            },
            "score": 4
        },
        {
            "id": "6a17a6fcd8e071c3f232838a9babf48a6671e9c3",
            "paperId": "6a17a6fcd8e071c3f232838a9babf48a6671e9c3",
            "title": "Meta Learning for Few-Shot Medical Text Classification",
            "abstract": "Medical professionals frequently work in a data constrained setting to provide insights across a unique demographic. A few medical observations, for instance, informs the diagnosis and treatment of a patient. This suggests a unique setting for meta-learning, a method to learn models quickly on new tasks, to provide insights unattainable by other methods. We investigate the use of meta-learning and robustness techniques on a broad corpus of benchmark text and medical data. To do this, we developed new data pipelines, combined language models with meta-learning approaches, and extended existing meta-learning algorithms to minimize worst case loss. We find that meta-learning on text is a suitable framework for text-based data, providing better data efficiency and comparable performance to few-shot language models and can be successfully applied to medical note data. Furthermore, meta-learning models coupled with DRO can improve worst case loss across disease codes.",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that meta-learning on text is a suitable framework for text-based data, providing better data efficiency and comparable performance to few-shot language models and can be successfully applied to medical note data."
            },
            "score": 4
        },
        {
            "id": "c53e3020e4b8f9e1cd7b6ed35221480a2647ea26",
            "paperId": "c53e3020e4b8f9e1cd7b6ed35221480a2647ea26",
            "title": "Meta-Learning Online Adaptation of Language Models",
            "abstract": "Large language models encode impressively broad world knowledge in their parameters. However, the knowledge in static language models falls out of date, limiting the model's effective\"shelf life.\"While online fine-tuning can reduce this degradation, we find that naively fine-tuning on a stream of documents leads to a low level of information uptake. We hypothesize that online fine-tuning does not sufficiently attend to important information. That is, the gradient signal from important tokens representing factual information is drowned out by the gradient from inherently noisy tokens, suggesting that a dynamic, context-aware learning rate may be beneficial. We therefore propose learning which tokens to upweight. We meta-train a small, autoregressive model to reweight the language modeling loss for each token during online fine-tuning, with the objective of maximizing the out-of-date base question-answering model's ability to answer questions about a document after a single weighted gradient step. We call this approach Context-aware Meta-learned Loss Scaling (CaMeLS). Across three different distributions of documents, our experiments find that CaMeLS provides substantially improved information uptake on streams of thousands of documents compared with standard fine-tuning and baseline heuristics for reweighting token losses.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work meta-train a small, autoregressive model to reweight the language modeling loss for each token during online fine-tuning, with the objective of maximizing the out-of-date base question-answering model's ability to answer questions about a document after a single weighted gradient step."
            },
            "score": 4
        },
        {
            "id": "e965e93e76a9e6c4e4863d145b5c007b540d575d",
            "paperId": "e965e93e76a9e6c4e4863d145b5c007b540d575d",
            "title": "OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization",
            "abstract": "Recent work has shown that fine-tuning large pre-trained language models on a collection of tasks described via instructions, a.k.a. instruction-tuning, improves their zero and few-shot generalization to unseen tasks. However, there is a limited understanding of the performance trade-offs of different decisions made during the instruction-tuning process. These decisions include the scale and diversity of the instruction-tuning benchmark, different task sampling strategies, fine-tuning with and without demonstrations, training using specialized datasets for reasoning and dialogue, and finally, the fine-tuning objectives themselves. In this paper, we characterize the effect of instruction-tuning decisions on downstream task performance when scaling both model and benchmark sizes. To this end, we create OPT-IML Bench: a large benchmark for Instruction Meta-Learning (IML) of 2000 NLP tasks consolidated into task categories from 8 existing benchmarks, and prepare an evaluation framework to measure three types of model generalizations: to tasks from fully held-out categories, to held-out tasks from seen categories, and to held-out instances from seen tasks. Through the lens of this framework, we first present insights about instruction-tuning decisions as applied to OPT-30B and further exploit these insights to train OPT-IML 30B and 175B, which are instruction-tuned versions of OPT. OPT-IML demonstrates all three generalization abilities at both scales on four different evaluation benchmarks with diverse tasks and input formats -- PromptSource, FLAN, Super-NaturalInstructions, and UnifiedSKG. Not only does it significantly outperform OPT on all benchmarks but is also highly competitive with existing models fine-tuned on each specific benchmark. We release OPT-IML at both scales, together with the OPT-IML Bench evaluation framework.",
            "year": 2022,
            "citationCount": 180,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper describes the effect of instruction-tuning decisions on downstream task performance when scaling both model and benchmark sizes and releases OPT-IML at both scales, together with an evaluation framework to measure three types of model generalizations."
            },
            "score": 4
        },
        {
            "id": "3aa200f562346a5e312767e5e9c1333a4f2c951b",
            "paperId": "3aa200f562346a5e312767e5e9c1333a4f2c951b",
            "title": "Large Language Models, scientific knowledge and factuality: A systematic analysis in antibiotic discovery",
            "abstract": "Inferring over and extracting information from Large Language Models (LLMs) trained on a large corpus of scientific literature can potentially drive a new era in biomedical research, reducing the barriers for accessing existing medical evidence. This work examines the potential of LLMs for dialoguing with biomedical background knowledge, using the context of antibiotic discovery. The systematic analysis is applied to ten state-of-the-art models, from models specialised on biomedical scientific corpora to general models such as ChatGPT, GPT-4 and Llama 2 in two prompting-based tasks: chemical compound definition generation and chemical compound-fungus relation determination. The work provides a systematic assessment on the ability of LLMs to encode and express these relations, verifying for fluency, prompt-alignment, semantic coherence, factual knowledge and specificity of generated responses. Results show that while recent models have improved in fluency, factual accuracy is still low and models are biased towards over-represented entities. The ability of LLMs to serve as biomedical knowledge bases is questioned, and the need for additional systematic evaluation frameworks is highlighted. The best performing GPT-4 produced a factual definition for 70% of chemical compounds and 43.6% factual relations to fungi, whereas the best open source model BioGPT-large 30% of the compounds and 30% of the relations for the best-performing prompt. The results show that while LLMs are currently not fit for purpose to be used as biomedical factual knowledge bases, there is a promising emerging property in the direction of factuality as the models become domain specialised, scale-up in size and level of human feedback.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work examines the potential of LLMs for dialoguing with biomedical background knowledge, using the context of antibiotic discovery, and shows a promising emerging property in the direction of factuality as the models become domain specialised, scale-up in size and level of human feedback."
            },
            "score": 4
        },
        {
            "id": "c213737923f58ad6c4cd18a8c17bca6522d7f4c6",
            "paperId": "c213737923f58ad6c4cd18a8c17bca6522d7f4c6",
            "title": "Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach",
            "abstract": "In practical statistical causal discovery (SCD), embedding domain expert knowledge as constraints into the algorithm is widely accepted as significant for creating consistent meaningful causal models, despite the recognized challenges in systematic acquisition of the background knowledge. To overcome these challenges, this paper proposes a novel methodology for causal inference, in which SCD methods and knowledge based causal inference (KBCI) with a large language model (LLM) are synthesized through\"statistical causal prompting (SCP)\"for LLMs and prior knowledge augmentation for SCD. Experiments have revealed that GPT-4 can cause the output of the LLM-KBCI and the SCD result with prior knowledge from LLM-KBCI to approach the ground truth, and that the SCD result can be further improved, if GPT-4 undergoes SCP. Furthermore, it has been clarified that an LLM can improve SCD with its background knowledge, even if the LLM does not contain information on the dataset. The proposed approach can thus address challenges such as dataset biases and limitations, illustrating the potential of LLMs to improve data-driven causal inference across diverse scientific domains.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel methodology for causal inference is proposed, in which SCD methods and knowledge based causal inference with a large language model (LLM) are synthesized through statistical causal prompting for LLMs and prior knowledge augmentation for SCD to address challenges such as dataset biases and limitations."
            },
            "score": 4
        },
        {
            "id": "1f79c96e061ca3a9310b2d54525ef0cce39417be",
            "paperId": "1f79c96e061ca3a9310b2d54525ef0cce39417be",
            "title": "VLSlice: Interactive Vision-and-Language Slice Discovery",
            "abstract": "Recent work in vision-and-language demonstrates that large-scale pretraining can learn generalizable models that are efficiently transferable to downstream tasks. While this may improve dataset-scale aggregate metrics, analyzing performance around hand-crafted subgroups targeting specific bias dimensions reveals systemic undesirable behaviors. However, this subgroup analysis is frequently stalled by annotation efforts, which require extensive time and resources to collect the necessary data. Prior art attempts to automatically discover subgroups to circumvent these constraints but typically leverages model behavior on existing task-specific annotations and rapidly degrades on more complex inputs beyond \"tabular\" data, none of which study vision-and-language models. This paper presents VLSlice, an interactive system enabling user-guided discovery of coherent representation-level subgroups with consistent visiolinguistic behavior, denoted as vision-and-language slices, from unlabeled image sets. We show that VLSlice enables users to quickly generate diverse high-coherency slices in a user study (n=22) and release the tool publicly1.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "VLSlice is presented, an interactive system enabling user-guided discovery of coherent representation-level subgroups with consistent visiolinguistic behavior, denoted as vision-and-language slices, from unlabeled image sets, and it is shown that VLSlice enables users to quickly generate diverse high-coherency slices in a user study and released publicly."
            },
            "score": 4
        },
        {
            "id": "4dcef4d040cdbc17eb8e7e39d1456c2a1ab691a0",
            "paperId": "4dcef4d040cdbc17eb8e7e39d1456c2a1ab691a0",
            "title": "SODAPOP: Open-Ended Discovery of Social Biases in Social Commonsense Reasoning Models",
            "abstract": "A common limitation of diagnostic tests for detecting social biases in NLP models is that they may only detect stereotypic associations that are pre-specified by the designer of the test. Since enumerating all possible problematic associations is infeasible, it is likely these tests fail to detect biases that are present in a model but not pre-specified by the designer. To address this limitation, we propose SODAPOP (SOcial bias Discovery from Answers about PeOPle), an approach for automatic social bias discovery in social commonsense question-answering. The SODAPOP pipeline generates modified instances from the Social IQa dataset (Sap et al., 2019b) by (1) substituting names associated with different demographic groups, and (2) generating many distractor answers from a masked language model. By using a social commonsense model to score the generated distractors, we are able to uncover the model\u2019s stereotypic associations between demographic groups and an open set of words. We also test SODAPOP on debiased models and show the limitations of multiple state-of-the-art debiasing algorithms.",
            "year": 2022,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes SODAPOP (SOcial bias Discovery from Answers about PeOPle), an approach for automatic social bias discovery in social commonsense question-answering and tests it on debiased models and shows the limitations of multiple state-of-the-art debiasing algorithms."
            },
            "score": 4
        },
        {
            "id": "3c414e3125c12dbd23f62e6c1b85c1a4dc9a522e",
            "paperId": "3c414e3125c12dbd23f62e6c1b85c1a4dc9a522e",
            "title": "Boosting Natural Language Generation from Instructions with Meta-Learning",
            "abstract": "Recent work has shown that language models (LMs) trained with multi-task instructional learning (MTIL) can solve diverse NLP tasks in zero- and few-shot settings with improved performance compared to prompt tuning. MTIL illustrates that LMs can extract and use information about the task from instructions beyond the surface patterns of the inputs and outputs. This suggests that meta-learning may further enhance the utilization of instructions for effective task transfer. In this paper we investigate whether meta-learning applied to MTIL can further improve generalization to unseen tasks in a zero-shot setting. Specifically, we propose to adapt meta-learning to MTIL in three directions: 1) Model Agnostic Meta Learning (MAML), 2) Hyper-Network (HNet) based adaptation to generate task specific parameters conditioned on instructions, and 3) an approach combining HNet and MAML. Through extensive experiments on the large scale Natural Instructions V2 dataset, we show that our proposed approaches significantly improve over strong baselines in zero-shot settings. In particular, meta-learning improves the effectiveness of instructions and is most impactful when the test tasks are strictly zero-shot (i.e. no similar tasks in the training set) and are \u201chard\u201d for LMs, illustrating the potential of meta-learning for MTIL for out-of-distribution tasks.",
            "year": 2022,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes to adapt meta-learning to MTIL in three directions: Model Agnostic Meta Learning (MAML), Hyper-Network (HNet) based adaptation to generate task specific parameters conditioned on instructions, and an approach combining HNet and MAML."
            },
            "score": 4
        },
        {
            "id": "8221f1597000543432b7021ca79dbc51a7a63f9c",
            "paperId": "8221f1597000543432b7021ca79dbc51a7a63f9c",
            "title": "Is ChatGPT a Good NLG Evaluator? A Preliminary Study",
            "abstract": "Recently, the emergence of ChatGPT has attracted wide attention from the computational linguistics community. Many prior studies have shown that ChatGPT achieves remarkable performance on various NLP tasks in terms of automatic evaluation metrics. However, the ability of ChatGPT to serve as an evaluation metric is still underexplored. Considering assessing the quality of natural language generation (NLG) models is an arduous task and NLG metrics notoriously show their poor correlation with human judgments, we wonder whether ChatGPT is a good NLG evaluation metric. In this report, we provide a preliminary meta-evaluation on ChatGPT to show its reliability as an NLG metric. In detail, we regard ChatGPT as a human evaluator and give task-specific (e.g., summarization) and aspect-specific (e.g., relevance) instruction to prompt ChatGPT to evaluate the generated results of NLG models. We conduct experiments on five NLG meta-evaluation datasets (including summarization, story generation and data-to-text tasks). Experimental results show that compared with previous automatic metrics, ChatGPT achieves state-of-the-art or competitive correlation with human judgments in most cases. In addition, we find that the effectiveness of the ChatGPT evaluator might be influenced by the creation method of the meta-evaluation datasets. For the meta-evaluation datasets which are created greatly depending on the reference and thus are biased, the ChatGPT evaluator might lose its effectiveness. We hope our preliminary study could prompt the emergence of a general-purposed reliable NLG metric.",
            "year": 2023,
            "citationCount": 201,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results show that compared with previous automatic metrics, ChatGPT achieves state-of-the-art or competitive correlation with human judgments in most cases and it is hoped this preliminary study could prompt the emergence of a general-purposed reliable NLG metric."
            },
            "score": 4
        },
        {
            "id": "1c6015ffff034b9c304477bb31e55ca5a55f3a99",
            "paperId": "1c6015ffff034b9c304477bb31e55ca5a55f3a99",
            "title": "Adversarial Transformer Language Models for Contextual Commonsense Inference",
            "abstract": "Contextualized or discourse aware commonsense inference is the task of generating coherent commonsense assertions (i.e., facts) from a given story, and a particular sentence from that story. Some problems with the task are: lack of controllability for topics of the inferred facts; lack of commonsense knowledge during training; and, possibly, hallucinated or false facts. In this work, we utilize a transformer model for this task and develop techniques to address the aforementioned problems in the task. We control the inference by introducing a new technique we call\"hinting\". Hinting is a kind of language model prompting, that utilizes both hard prompts (specific words) and soft prompts (virtual learnable templates). This serves as a control signal to advise the language model\"what to talk about\". Next, we establish a methodology for performing joint inference with multiple commonsense knowledge bases. Joint inference of commonsense requires care, because it is imprecise and the level of generality is more flexible. You want to be sure that the results\"still make sense\"for the context. To this end, we align the textual version of assertions from three knowledge graphs (ConceptNet, ATOMIC2020, and GLUCOSE) with a story and a target sentence. This combination allows us to train a single model to perform joint inference with multiple knowledge graphs. We show experimental results for the three knowledge graphs on joint inference. Our final contribution is exploring a GAN architecture that generates the contextualized commonsense assertions and scores them as to their plausibility through a discriminator. The result is an integrated system for contextual commonsense inference in stories, that can controllably generate plausible commonsense assertions, and takes advantage of joint inference between multiple commonsense knowledge bases.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The result is an integrated system for contextual commonsense inference in stories, that can controllably generate plausible commonsense assertions, and takes advantage of joint inference between multiple commonsense knowledge bases."
            },
            "score": 3
        },
        {
            "id": "6da2203f2cbd3c9530764fab58ef74acb8a2c585",
            "paperId": "6da2203f2cbd3c9530764fab58ef74acb8a2c585",
            "title": "APAM: Adaptive Pre-Training and Adaptive Meta Learning in Language Model for Noisy Labels and Long-Tailed Learning",
            "abstract": "Practical natural language processing (NLP) tasks often exhibit long-tailed distributions accompanied by noisy labels, posing significant challenges to the generalization and robustness of complex models such as Deep Neural Networks (DNNs). Traditional resampling techniques like oversampling or undersampling, while commonly employed, can easily lead to overfitting. A growing trend involves leveraging small amounts of metadata to learn data weights, alongside the demonstrated benefits of self-supervised pre-training, particularly for under-represented data. In this work, we propose a general framework that addresses both the long-tail and noisy label issues. Our model is adaptively tailored to the problem domain using a contrastive learning approach. The re-weighting module, a feed-forward network, learns explicit weighting functions and adjusts weights based on metadata. Additionally, our framework modifies the weights of terms in the loss function through a combination of polynomial expansion of the cross-entropy loss and focal loss. Extensive experiments consistently demonstrate the superior performance of our proposed framework compared to baseline methods. Finally, our comprehensive sensitivity analysis underscores the efficacy of the proposed framework in handling long-tailed problems and mitigating the adverse effects of noisy labels.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a general framework that addresses both the long-tail and noisy label issues, adaptively tailored to the problem domain using a contrastive learning approach and demonstrates the superior performance of the proposed framework compared to baseline methods."
            },
            "score": 3
        },
        {
            "id": "136d968598ab14715cec3393153355c3b535201e",
            "paperId": "136d968598ab14715cec3393153355c3b535201e",
            "title": "Meta Learning to Bridge Vision and Language Models for Multimodal Few-Shot Learning",
            "abstract": "Multimodal few-shot learning is challenging due to the large domain gap between vision and language modalities. Existing methods are trying to communicate visual concepts as prompts to frozen language models, but rely on hand-engineered task induction to reduce the hypothesis space. To make the whole process learnable, we introduce a multimodal meta-learning approach. Specifically, our approach decomposes the training of the model into a set of related multimodal few-shot tasks. We define a meta-mapper network, acting as a meta-learner, to efficiently bridge frozen large-scale vision and language models and leverage their already learned capacity. By updating the learnable parameters only of the meta-mapper, it learns to accrue shared meta-knowledge among these tasks. Thus, it can rapidly adapt to newly presented samples with only a few gradient updates. Importantly, it induces the task in a completely data-driven manner, with no need for a hand-engineered task induction. We evaluate our approach on recently proposed multimodal few-shot benchmarks, measuring how rapidly the model can bind novel visual concepts to words and answer visual questions by observing only a limited set of labeled examples. The experimental results show that our meta-learning approach outperforms the baseline across multiple datasets and various training settings while being computationally more efficient.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results show that the multimodal meta-learning approach outperforms the baseline across multiple datasets and various training settings while being computationally more efficient."
            },
            "score": 3
        },
        {
            "id": "3a021e1acc7588897df3f58e3ad928122846122f",
            "paperId": "3a021e1acc7588897df3f58e3ad928122846122f",
            "title": "Forging Multiple Training Objectives for Pre-trained Language Models via Meta-Learning",
            "abstract": "Multiple pre-training objectives fill the vacancy of the understanding capability of single-objective language modeling, which serves the ultimate purpose of pre-trained language models (PrLMs), generalizing well on a mass of scenarios. However, learning multiple training objectives in a single model is challenging due to the unknown relative significance as well as the potential contrariety between them. Empirical studies have shown that the current objective sampling in an ad-hoc manual setting makes the learned language representation barely converge to the desired optimum. Thus, we propose \\textit{MOMETAS}, a novel adaptive sampler based on meta-learning, which learns the latent sampling pattern on arbitrary pre-training objectives. Such a design is lightweight with negligible additional training overhead. To validate our approach, we adopt five objectives and conduct continual pre-training with BERT-base and BERT-large models, where MOMETAS demonstrates universal performance gain over other rule-based sampling strategies on 14 natural language processing tasks.",
            "year": 2022,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes MOMETAS, a novel adaptive sampler based on meta-learning, which learns the latent sampling pattern on arbitrary pre-training objectives and demonstrates universal performance gain over other rule-based sampling strategies on 14 natural language processing tasks."
            },
            "score": 3
        },
        {
            "id": "37ba9c33025fb31f25436010e12c65a0bafc0e1f",
            "paperId": "37ba9c33025fb31f25436010e12c65a0bafc0e1f",
            "title": "Meta-Learning Fast Weight Language Models",
            "abstract": "Dynamic evaluation of language models (LMs) adapts model parameters at test time using gradient information from previous tokens and substantially improves LM performance. However, it requires over 3x more compute than standard inference. We present Fast Weight Layers (FWLs), a neural component that provides the benefits of dynamic evaluation much more efficiently by expressing gradient updates as linear attention. A key improvement over dynamic evaluation is that FWLs can also be applied at training time, so the model learns to make good use of gradient updates. FWLs can easily be added on top of existing transformer models, require relatively little extra compute or memory to run, and significantly improve language modeling perplexity.",
            "year": 2022,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Fast Weight Layers are presented, a neural component that provides the benefits of dynamic evaluation much more efficiently by expressing gradient updates as linear attention and can also be applied at training time, so the model learns to make good use of gradient updates."
            },
            "score": 3
        },
        {
            "id": "8c33ffd4b3bc0e0af6d85c1b71f3cecf7cde7160",
            "paperId": "8c33ffd4b3bc0e0af6d85c1b71f3cecf7cde7160",
            "title": "Meta Distant Transfer Learning for Pre-trained Language Models",
            "abstract": "With the wide availability of Pre-trained Language Models (PLMs), multi-task fine-tuning across domains has been extensively applied. For tasks related to distant domains with different class label sets, PLMs may memorize non-transferable knowledge for the target domain and suffer from negative transfer. Inspired by meta-learning, we propose the Meta Distant Transfer Learning (Meta-DTL) framework to learn the cross-task knowledge for PLM-based methods. Meta-DTL first employs task representation learning to mine implicit relations among multiple tasks and classes. Based on the results, it trains a PLM-based meta-learner to capture the transferable knowledge across tasks. The weighted maximum entropy regularizers are proposed to make meta-learner more task-agnostic and unbiased. Finally, the meta-learner can be fine-tuned to fit each task with better parameter initialization. We evaluate Meta-DTL using both BERT and ALBERT on seven public datasets. Experiment results confirm the superiority of Meta-DTL as it consistently outperforms strong baselines. We find that Meta-DTL is highly effective when very few data is available for the target task.",
            "year": 2021,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Inspired by meta-learning, the Meta Distant Transfer Learning (Meta-DTL) framework is proposed to learn the cross-task knowledge for PLM-based methods and is found to be highly effective when very few data is available for the target task."
            },
            "score": 3
        },
        {
            "id": "128904f804d206d2b2a512358df4d4385b7d2712",
            "paperId": "128904f804d206d2b2a512358df4d4385b7d2712",
            "title": "Social Commonsense for Explanation and Cultural Bias Discovery",
            "abstract": "Social commonsense contains many human biases due to social and cultural influence (Sap et al., 2020; Emelin et al., 2020). We focus on identifying cultural biases in data, specifically causal assumptions and commonsense implications, that strongly influence model decisions for a variety of tasks designed for social impact. This enables us to examine data for bias by making explicit the causal (if-then, inferential) relations in social commonsense knowledge used for decision making, furthering interpretable commonsense reasoning from a dataset perspective. We apply our methods on 2 social tasks: emotion detection and perceived value detection. We identify influential social commonsense knowledge to explain model behavior in the following ways. First, we augment large-scale language models with social knowledge and show improvements for the tasks, indicating the implicit assumptions a model requires to be successful on each dataset. Second, we identify influential events in the datasets by using social knowledge to cluster data and demonstrate the influence that these events have on model behavior via leave-K-out experiments. This allows us to gain a dataset-level understanding of the events and causal commonsense relationships that strongly influence predictions. We then analyze these relationships to detect influential cultural bias in each dataset. Finally, we use our influential event identification for detecting mislabeled examples and improve training and performance through their removal. We support our findings with manual analysis.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work identifies influential social commonsense knowledge to explain model behavior in the following ways, and augments large-scale language models with social knowledge and shows improvements for the tasks, indicating the implicit assumptions a model requires to be successful on each dataset."
            },
            "score": 3
        },
        {
            "id": "846c8636c502d99d754abc512583c5dca0f340ba",
            "paperId": "846c8636c502d99d754abc512583c5dca0f340ba",
            "title": "Leveraging Fine-Tuned Large Language Models in Bioinformatics: A Research Perspective",
            "abstract": "Bioinformatics synergizes biology, computer science, and statistics and is further propelled by the integration of deep learning and natural language processing (NLP). This analysis extensively explores the applications of fine-tuned language models within bioinformatics, providing empirical evidence and unique perspectives on the impact, challenges, and limitations in this field. The broad scope includes biomedical literature analysis, drug discovery, clinical decision support, protein structure prediction, and pharmacovigilance, among others. This analysis underscores the need to overcome hurdles such as data availability, domain-specific knowledge, bias, interpretability, resource efficiency, ethical implications, and validation for a reliable application of these models. Collaborative efforts between computational and experimental biologists, ethicists, and regulatory bodies are vital to establish ethical guidelines and best practices for their use.\n",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The need to overcome hurdles such as data availability, domain-specific knowledge, bias, interpretability, resource efficiency, ethical implications, and validation for a reliable application of fine-tuned language models within bioinformatics is highlighted."
            },
            "score": 3
        },
        {
            "id": "0504c5d96ebf7be01e2b622ff3a4bf155f2b0a41",
            "paperId": "0504c5d96ebf7be01e2b622ff3a4bf155f2b0a41",
            "title": "Causal Graph Discovery with Retrieval-Augmented Generation based Large Language Models",
            "abstract": "Causal graph recovery is essential in the field of causal inference. Traditional methods are typically knowledge-based or statistical estimation-based, which are limited by data collection biases and individuals' knowledge about factors affecting the relations between variables of interests. The advance of large language models (LLMs) provides opportunities to address these problems. We propose a novel method that utilizes the extensive knowledge contained within a large corpus of scientific literature to deduce causal relationships in general causal graph recovery tasks. This method leverages Retrieval Augmented-Generation (RAG) based LLMs to systematically analyze and extract pertinent information from a comprehensive collection of research papers. Our method first retrieves relevant text chunks from the aggregated literature. Then, the LLM is tasked with identifying and labelling potential associations between factors. Finally, we give a method to aggregate the associational relationships to build a causal graph. We demonstrate our method is able to construct high quality causal graphs on the well-known SACHS dataset solely from literature.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel method that utilizes the extensive knowledge contained within a large corpus of scientific literature to deduce causal relationships in general causal graph recovery tasks and is able to construct high quality causal graphs on the well-known SACHS dataset solely from literature."
            },
            "score": 3
        },
        {
            "id": "290835cc7b4ca9f2461659d9147dc2e6a08a93aa",
            "paperId": "290835cc7b4ca9f2461659d9147dc2e6a08a93aa",
            "title": "Leveraging Fine-Tuned Language Models in Bioinformatics: A Research Perspective",
            "abstract": "Bioinformatics, an interdisciplinary field combining biology, computer science, and statistics, has advanced with deep learning and natural language processing techniques. This perspective explores the applications of fine-tuned language models in bioinformatics, highlighting their potential in various domains while discussing challenges and limitations. Fine-tuned language models benefit biomedical literature analysis, extracting information from scientific papers to synthesize knowledge and generate synthetic sequences for DNA, RNA, and protein research. In drug discovery, these models can identify novel drug targets, accelerate virtual screening, and aid drug repurposing by finding new therapeutic indications for existing drugs. For clinical decision support, fine-tuned language models can analyse patient data, medical literature, and guidelines to provide personalized recommendations and alerts to healthcare professionals. They can also aid accurate protein structure prediction for drug design and target identification. In pharmacovigilance, these models can analyse unstructured data sources to detect adverse events from social media, patient forums, and health records, enabling early intervention and improving patient safety. However, challenges like data availability, domain-specific knowledge, bias, interpretability, resource efficiency, ethics, and validation must be addressed for reliable application. Addressing these challenges will unlock the full potential of fine-tuned language models in bioinformatics, driving advancements and benefiting human health. Collaboration between computational and experimental biologists, ethicists, and regulatory bodies is crucial to establish ethical guidelines and best practices for their use.\n",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This perspective explores the applications of fine-tuned language models in bioinformatics, highlighting their potential in various domains while discussing challenges and limitations."
            },
            "score": 3
        },
        {
            "id": "7a25eeb495020106bc1ee8b70fdf51eb31bc5094",
            "paperId": "7a25eeb495020106bc1ee8b70fdf51eb31bc5094",
            "title": "Semi-Supervised Topic Modeling for Gender Bias Discovery in English and Swedish",
            "abstract": "Gender bias has been identified in many models for Natural Language Processing, stemming from implicit biases in the text corpora used to train the models. Such corpora are too large to closely analyze for biased or stereotypical content. Thus, we argue for a combination of quantitative and qualitative methods, where the quantitative part produces a view of the data of a size suitable for qualitative analysis. We investigate the usefulness of semi-supervised topic modeling for the detection and analysis of gender bias in three corpora (mainstream news articles in English and Swedish, and LGBTQ+ web content in English). We compare differences in topic models for three gender categories (masculine, feminine, and nonbinary or neutral) in each corpus. We find that in all corpora, genders are treated differently and that these differences tend to correspond to hegemonic ideas of gender.",
            "year": 2020,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is argued for a combination of quantitative and qualitative methods, where the quantitative part produces a view of the data of a size suitable for qualitative analysis of gender bias in corpora."
            },
            "score": 3
        },
        {
            "id": "6923ebb5c4b376eb6e7c088376af5601ed6e292e",
            "paperId": "6923ebb5c4b376eb6e7c088376af5601ed6e292e",
            "title": "Test-Time Personalization with Meta Prompt for Gaze Estimation",
            "abstract": "Despite the recent remarkable achievement in gaze estimation, efficient and accurate personalization of gaze estimation without labels is a practical problem but rarely touched on in the literature. To achieve efficient personalization, we take inspiration from the recent advances in Natural Language Processing (NLP) by updating a negligible number of parameters, \"prompts\", at the test time. Specifically, the prompt is additionally attached without perturbing original network and can contain less than 1% of a ResNet-18's parameters. Our experiments show high efficiency of the prompt tuning approach. The proposed one can be 10 times faster in terms of adaptation speed than the methods compared. However, it is non-trivial to update the prompt for personalized gaze estimation without labels. At the test time, it is essential to ensure that the minimizing of particular unsupervised loss leads to the goals of minimizing gaze estimation error. To address this difficulty, we propose to meta-learn the prompt to ensure that its updates align with the goal. Our experiments show that the meta-learned prompt can be effectively adapted even with a simple symmetry loss. In addition, we experiment on four cross-dataset validations to show the remarkable advantages of the proposed method.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work takes inspiration from the recent advances in Natural Language Processing by updating a negligible number of parameters, \"prompts\", at the test time by meta-learning the prompt to ensure that its updates align with the goal."
            },
            "score": 3
        },
        {
            "id": "2ee03e28208a9310a9be4032c2b04ebdddb83cc7",
            "paperId": "2ee03e28208a9310a9be4032c2b04ebdddb83cc7",
            "title": "FLEX: Unifying Evaluation for Few-Shot NLP",
            "abstract": "Few-shot NLP research is highly active, yet conducted in disjoint research threads with evaluation suites that lack challenging-yet-realistic testing setups and fail to employ careful experimental design. Consequently, the community does not know which techniques perform best or even if they outperform simple baselines. In response, we formulate the FLEX Principles, a set of requirements and best practices for unified, rigorous, valid, and cost-sensitive few-shot NLP evaluation. These principles include Sample Size Design, a novel approach to benchmark design that optimizes statistical accuracy and precision while keeping evaluation costs manageable. Following the principles, we release the FLEX benchmark, which includes four few-shot transfer settings, zero-shot evaluation, and a public leaderboard that covers diverse NLP tasks. In addition, we present UniFew, a prompt-based model for few-shot learning that unifies pretraining and finetuning prompt formats, eschewing complex machinery of recent prompt-based approaches in adapting downstream task formats to language model pretraining objectives. We demonstrate that despite simplicity, UniFew achieves results competitive with both popular meta-learning and prompt-based approaches.",
            "year": 2021,
            "citationCount": 87,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The FLEX Principles are formulated, a set of requirements and best practices for unified, rigorous, valid, and cost-sensitive few-shot NLP evaluation, and UniFew, a prompt-based model for few- shot learning that unifies pretraining and finetuning prompt formats are presented."
            },
            "score": 3
        },
        {
            "id": "5a714e920688a3a9296efa0cfa531de263a93de3",
            "paperId": "5a714e920688a3a9296efa0cfa531de263a93de3",
            "title": "OntoChatGPT Information System: Ontology-Driven Structured Prompts for ChatGPT Meta-Learning",
            "abstract": "This research presents a comprehensive methodology for utilizing an ontology-driven structured prompts system in interplay with ChatGPT, a widely used large language model (LLM). The study develops formal models, both information and functional, and establishes the methodological foundations for integrating ontology-driven prompts with ChatGPT\u2019s meta-learning capabilities. The resulting productive triad comprises the methodological foundations, advanced information technology, and the OntoChatGPT system, which collectively enhance the effectiveness and performance of chatbot systems. The implementation of this technology is demonstrated using the Ukrainian language within the domain of rehabilitation. By applying the proposed methodology, the OntoChatGPT system effectively extracts entities from contexts, classifies them, and generates relevant responses. The study highlights the versatility of the methodology, emphasizing its applicability not only to ChatGPT but also to other chatbot systems based on LLMs, such as Google\u2019s Bard utilizing the PaLM 2 LLM. The underlying principles of meta-learning, structured prompts, and ontology-driven information retrieval form the core of the proposed methodology, enabling their adaptation and utilization in various LLM-based systems. This versatile approach opens up new possibilities for NLP and dialogue systems, empowering developers to enhance the performance and functionality of chatbot systems across different domains and languages.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The study develops formal models, both information and functional, and establishes the methodological foundations for integrating ontology-driven prompts with ChatGPT\u2019s meta-learning capabilities, enabling their adaptation and utilization in various LLM-based systems."
            },
            "score": 3
        },
        {
            "id": "59ef1b67c5f238d5d6d175d84fb6b239b4221a97",
            "paperId": "59ef1b67c5f238d5d6d175d84fb6b239b4221a97",
            "title": "Stabilized In-Context Learning with Pre-trained Language Models for Few Shot Dialogue State Tracking",
            "abstract": "Prompt-based methods with large pre-trained language models (PLMs) have shown impressive unaided performance across many NLP tasks. These models improve even further with the addition of a few labeled in-context exemplars to guide output generation. However, for more complex tasks such as dialogue state tracking (DST), designing prompts that reliably convey the desired intent is nontrivial, leading to unstable results. Furthermore, building in-context exemplars for dialogue tasks is difficult because conversational contexts are long while model input lengths are relatively short.To overcome these issues we first adapt a meta-learning scheme to the dialogue domain which stabilizes the ability of the model to perform well under various prompts. We additionally design a novel training method to improve upon vanilla retrieval mechanisms to find ideal in-context examples. Finally, we introduce a saliency model to limit dialogue text length, allowing us to include more exemplars per query. In effect, we are able to achieve highly competitive results for few-shot DST on MultiWOZ.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A meta-learning scheme is adapted to the dialogue domain which stabilizes the ability of the model to perform well under various prompts and introduces a saliency model to limit dialogue text length, allowing for highly competitive results for few-shot DST on MultiWOZ."
            },
            "score": 3
        },
        {
            "id": "ccfdea3722ec43e45fc33e170d1fe908dca003a5",
            "paperId": "ccfdea3722ec43e45fc33e170d1fe908dca003a5",
            "title": "TransPrompt v2: A Transferable Prompting Framework for Cross-task Text Classification",
            "abstract": "Text classification is one of the most imperative tasks in natural language processing (NLP). Recent advances with pre-trained language models (PLMs) have shown remarkable success on this task. However, the satisfying results obtained by PLMs heavily depend on the large amounts of task-specific labeled data, which may not be feasible in many application scenarios due to data access and privacy constraints. The recently-proposed prompt-based fine-tuning paradigm improves the performance of PLMs for few-shot text classification with task-specific templates. Yet, it is unclear how the prompting knowledge can be transferred across tasks, for the purpose of mutual reinforcement. We propose TransPrompt v2, a novel transferable prompting framework for few-shot learning across similar or distant text classification tasks. For learning across similar tasks, we employ a multi-task meta-knowledge acquisition (MMA) procedure to train a meta-learner that captures the cross-task transferable knowledge. For learning across distant tasks, we further inject the task type descriptions into the prompt, and capture the intra-type and inter-type prompt embeddings among multiple distant tasks. Additionally, two de-biasing techniques are further designed to make the trained meta-learner more task-agnostic and unbiased towards any tasks. After that, the meta-learner can be adapted to each specific task with better parameters initialization. Extensive experiments show that TransPrompt v2 outperforms single-task and cross-task strong baselines over multiple NLP tasks and datasets. We further show that the meta-learner can effectively improve the performance of PLMs on previously unseen tasks. In addition, TransPrompt v2 also outperforms strong fine-tuning baselines when learning with full training sets.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Extensive experiments show that TransPrompt v2 outperforms single-task and cross-task strong baselines over multiple NLP tasks and datasets, and shows that the meta-learner can effectively improve the performance of PLMs on previously unseen tasks."
            },
            "score": 3
        },
        {
            "id": "648832bc487121147890cc9f10b18f8b05c1d70b",
            "paperId": "648832bc487121147890cc9f10b18f8b05c1d70b",
            "title": "Evaluating the Robustness of Natural Language Reward Shaping Models to Spatial Relations",
            "abstract": "As part of an effort to bridge the gap between using reinforcement learning in simulation and in the real world, we probe whether current reward shaping models are able to encode relational data between objects in the environment. We construct an augmented dataset for controlling a robotic arm in the Meta-World platform to test whether current models are able to discriminate between target objects based on their relations. We found that state of the art models are indeed expressive enough to achieve performance comparable to the gold standard, so this specific experiment did not uncover any obvious shortcomings.",
            "year": 2020,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that state of the art models are indeed expressive enough to achieve performance comparable to the gold standard, so this specific experiment did not uncover any obvious shortcomings."
            },
            "score": 2
        },
        {
            "id": "b355b6a89ff412d61cabc83dd30fd42c16ed50e9",
            "paperId": "b355b6a89ff412d61cabc83dd30fd42c16ed50e9",
            "title": "Massive Editing for Large Language Models via Meta Learning",
            "abstract": "While large language models (LLMs) have enabled learning knowledge from the pre-training corpora, the acquired knowledge may be fundamentally incorrect or outdated over time, which necessitates rectifying the knowledge of the language model (LM) after the training. A promising approach involves employing a hyper-network to generate parameter shift, whereas existing hyper-networks suffer from inferior scalability in synchronous editing operation amount. To mitigate the problem, we propose the MAssive Language Model Editing Network (MALMEN), which formulates the parameter shift aggregation as the least square problem, subsequently updating the LM parameters using the normal equation. To accommodate editing multiple facts simultaneously with limited memory budgets, we separate the computation on the hyper-network and LM, enabling arbitrary batch size on both neural networks. Our method is evaluated by editing up to thousands of facts on LMs with different architectures, i.e., BERT-base, GPT-2, T5-XL (2.8B), and GPT-J (6B), across various knowledge-intensive NLP tasks, i.e., closed book fact-checking and question answering. Remarkably, MALMEN is capable of editing hundreds of times more facts than strong baselines with the identical hyper-network architecture and outperforms editor specifically designed for GPT. Our code is available at https://github.com/ChenmienTan/malmen.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The MAssive Language Model Editing Network (MALMEN) is proposed, which formulates the parameter shift aggregation as the least square problem, subsequently updating the LM parameters using the normal equation and is capable of editing hundreds of times more facts than strong baselines with the identical hyper-network architecture."
            },
            "score": 2
        },
        {
            "id": "d96d6d7f492adb3005aa9371d85bbb882abb6fa4",
            "paperId": "d96d6d7f492adb3005aa9371d85bbb882abb6fa4",
            "title": "On Learning to Summarize with Large Language Models as References",
            "abstract": "Recent studies have found that summaries generated by large language models (LLMs) are favored by human annotators over the original reference summaries in commonly used summarization datasets. Therefore, we investigate a new learning setting of text summarization models that considers the LLMs as the reference or the gold-standard oracle on these datasets. To examine the standard practices that are aligned with this new learning setting, we investigate two LLM-based summary quality evaluation methods for model training and adopt a contrastive learning training method to leverage the LLM-guided learning signals. Our experiments on the CNN/DailyMail and XSum datasets demonstrate that smaller summarization models can achieve similar performance as LLMs under LLM-based evaluation. However, we found that the smaller models can not yet reach LLM-level performance under human evaluation despite promising improvements brought by our proposed training methods. Meanwhile, we perform a meta-analysis on this new learning setting that reveals a discrepancy between human and LLM-based evaluation, highlighting the benefits and risks of this LLM-as-reference setting we investigated.",
            "year": 2023,
            "citationCount": 25,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates a new learning setting of text summarization models that considers the LLMs as the reference or the gold-standard oracle on these datasets, and investigates two LLM-based summary quality evaluation methods for model training and adopts a contrastive learning training method to leverage theLLM-guided learning signals."
            },
            "score": 2
        },
        {
            "id": "dcaf366fba4f24faaaf215035402373f6c8df994",
            "paperId": "dcaf366fba4f24faaaf215035402373f6c8df994",
            "title": "Addressing the antibody germline bias and its effect on language models for improved antibody design",
            "abstract": "The versatile binding properties of antibodies have made them an extremely important class of biotherapeutics. However, therapeutic antibody development is a complex, expensive and time-consuming task, with the final antibody needing to not only have strong and specific binding, but also be minimally impacted by any developability issues. The success of transformer-based language models in protein sequence space and the availability of vast amounts of antibody sequences, has led to the development of many antibody-specific language models to help guide antibody discovery and design. Antibody diversity primarily arises from V(D)J recombination, mutations within the CDRs, and/or from a small number of mutations away from the germline outside the CDRs. Consequently, a significant portion of the variable domain of all natural antibody sequences remains germline. This affects the pre-training of antibody-specific language models, where this facet of the sequence data introduces a prevailing bias towards germline residues. This poses a challenge, as mutations away from the germline are often vital for generating specific and potent binding to a target, meaning that language models need be able to suggest key mutations away from germline. In this study, we explore the implications of the germline bias, examining its impact on both general-protein and antibody-specific language models. We develop and train a series of new antibody-specific language models optimised for predicting non-germline residues. We then compare our final model, AbLang-2, with current models and show how it suggests a diverse set of valid mutations with high cumulative probability. AbLang-2 is trained on both unpaired and paired data, and is freely available (https://github.com/oxpig/AbLang2.git).",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The implications of the germline bias is explored, examining its impact on both general-protein and antibody-specific language models and a series of new antibody-specific language models optimised for predicting non-germline residues are developed."
            },
            "score": 2
        },
        {
            "id": "559415fa81069d8d859c0d7f479601f1fc8aca8f",
            "paperId": "559415fa81069d8d859c0d7f479601f1fc8aca8f",
            "title": "Do chemical language models provide a better compound representation?",
            "abstract": "In recent years, several chemical language models have been developed, inspired by the success of protein language models and advancements in natural language processing. In this study, we explore whether pre-training a chemical language model on billion-scale compound datasets, such as Enamine and ZINC20, can lead to improved compound representation in the drug space. We compare the learned representations of these models with the de facto standard compound representation, and evaluate their potential application in drug discovery and development by benchmarking them on biophysics, physiology, and physical chemistry datasets. Our findings suggest that the conventional masked language modeling approach on these extensive pre-training datasets is insufficient in enhancing compound representations. This highlights the need for additional physicochemical inductive bias in the modeling beyond scaling the dataset size.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study explores whether pre-training a chemical language model on billion-scale compound datasets, such as Enamine and ZINC20, can lead to improved compound representation in the drug space, and suggests that the conventional masked language modeling approach is insufficient in enhancing compound representations."
            },
            "score": 2
        },
        {
            "id": "7be93f2daf5753d13a627db9e046e558291f6dcc",
            "paperId": "7be93f2daf5753d13a627db9e046e558291f6dcc",
            "title": "PGraphDTA: Improving Drug Target Interaction Prediction using Protein Language Models and Contact Maps",
            "abstract": "Developing and discovering new drugs is a complex and resource-intensive endeavor that often involves substantial costs, time investment, and safety concerns. A key aspect of drug discovery involves identifying novel drug-target (DT) interactions. Existing computational methods for predicting DT interactions have primarily focused on binary classification tasks, aiming to determine whether a DT pair interacts or not. However, protein-ligand interactions exhibit a continuum of binding strengths, known as binding affinity, presenting a persistent challenge for accurate prediction. In this study, we investigate various techniques employed in Drug Target Interaction (DTI) prediction and propose novel enhancements to enhance their performance. Our approaches include the integration of Protein Language Models (PLMs) and the incorporation of Contact Map information as an inductive bias within current models. Through extensive experimentation, we demonstrate that our proposed approaches outperform the baseline models considered in this study, presenting a compelling case for further development in this direction. We anticipate that the insights gained from this work will significantly narrow the search space for potential drugs targeting specific proteins, thereby accelerating drug discovery. Code and data for PGraphDTA are available at https://anonymous.4open.science/r/PGraphDTA.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study investigates various techniques employed in Drug Target Interaction (DTI) prediction and proposes novel enhancements to enhance their performance, and expects that the insights gained will significantly narrow the search space for potential drugs targeting specific proteins, thereby accelerating drug discovery."
            },
            "score": 2
        },
        {
            "id": "d4e7f31e2f8102d92e64d989ca7cd1bdea1bea3a",
            "paperId": "d4e7f31e2f8102d92e64d989ca7cd1bdea1bea3a",
            "title": "Use large language models to promote equity",
            "abstract": "Advances in large language models (LLMs) have driven an explosion of interest about their societal impacts. Much of the discourse around how they will impact social equity has been cautionary or negative, focusing on questions like\"how might LLMs be biased and how would we mitigate those biases?\"This is a vital discussion: the ways in which AI generally, and LLMs specifically, can entrench biases have been well-documented. But equally vital, and much less discussed, is the more opportunity-focused counterpoint:\"what promising applications do LLMs enable that could promote equity?\"If LLMs are to enable a more equitable world, it is not enough just to play defense against their biases and failure modes. We must also go on offense, applying them positively to equity-enhancing use cases to increase opportunities for underserved groups and reduce societal discrimination. There are many choices which determine the impact of AI, and a fundamental choice very early in the pipeline is the problems we choose to apply it to. If we focus only later in the pipeline -- making LLMs marginally more fair as they facilitate use cases which intrinsically entrench power -- we will miss an important opportunity to guide them to equitable impacts. Here, we highlight the emerging potential of LLMs to promote equity by presenting four newly possible, promising research directions, while keeping risks and cautionary points in clear view.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The emerging potential of LLMs to promote equity is highlighted by presenting four newly possible, promising research directions, while keeping risks and cautionary points in clear view."
            },
            "score": 2
        },
        {
            "id": "0a0d6a98bd246a82aaaa9d33ec0eadf4ceae69dc",
            "paperId": "0a0d6a98bd246a82aaaa9d33ec0eadf4ceae69dc",
            "title": "On Codex Prompt Engineering for OCL Generation: An Empirical Study",
            "abstract": "The Object Constraint Language (OCL) is a declarative language that adds constraints and object query expressions to Meta-Object Facility (MOF) models. OCL can provide precision and conciseness to UML models. Nevertheless, the unfamiliar syntax of OCL has hindered its adoption by software practitioners. LLMs, such as GPT-3, have made significant progress in many NLP tasks, such as text generation and semantic parsing. Similarly, researchers have improved on the downstream tasks by fine-tuning LLMs for the target task. Codex, a GPT-3 descendant by OpenAI, has been fine-tuned on publicly available code from GitHub and has proven the ability to generate code in many programming languages, powering the AI-pair programmer Copilot. One way to take advantage of Codex is to engineer prompts for the target downstream task. In this paper, we investigate the reliability of the OCL constraints generated by Codex from natural language specifications. To achieve this, we compiled a dataset of 15 UML models and 168 specifications from various educational resources. We manually crafted a prompt template with slots to populate with the UML information and the target task in the prefix format to complete the template with the generated OCL constraint. We used both zero- and few-shot learning methods in the experiments. The evaluation is reported by measuring the syntactic validity and the execution accuracy metrics of the generated OCL constraints. Moreover, to get insight into how close or natural the generated OCL constraints are compared to human-written ones, we measured the cosine similarity between the sentence embedding of the correctly generated and human-written OCL constraints. Our findings suggest that by enriching the prompts with the UML information of the models and enabling few-shot learning, the reliability of the generated OCL constraints increases. Furthermore, the results reveal a close similarity based on sentence embedding between the generated OCL constraints and the human-written ones in the ground truth, implying a level of clarity and understandability in the generated OCL constraints by Codex.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The findings suggest that by enriching the prompts with the UML information of the models and enabling few-shot learning, the reliability of the generated OCL constraints increases, and a close similarity based on sentence embedding between the generatedOCL constraints and the human-written ones in the ground truth is revealed."
            },
            "score": 2
        },
        {
            "id": "49b4d8b6f3acf0bf094e8b80640bc585a6ce8918",
            "paperId": "49b4d8b6f3acf0bf094e8b80640bc585a6ce8918",
            "title": "Few-Shot Character Understanding in Movies as an Assessment to Meta-Learning of Theory-of-Mind",
            "abstract": "When reading a story, humans can quickly understand new fictional characters with a few observations, mainly by drawing analogies to fictional and real people they already know. This reflects the few-shot and meta-learning essence of humans' inference of characters' mental states, i.e., theory-of-mind (ToM), which is largely ignored in existing research. We fill this gap with a novel NLP dataset, ToM-in-AMC, the first assessment of machines' meta-learning of ToM in a realistic narrative understanding scenario. Our dataset consists of ~1,000 parsed movie scripts, each corresponding to a few-shot character understanding task that requires models to mimic humans' ability of fast digesting characters with a few starting scenes in a new movie. We propose a novel ToM prompting approach designed to explicitly assess the influence of multiple ToM dimensions. It surpasses existing baseline models, underscoring the significance of modeling multiple ToM dimensions for our task. Our extensive human study verifies that humans are capable of solving our problem by inferring characters' mental states based on their previously seen movies. In comparison, our systems based on either state-of-the-art large language models (GPT-4) or meta-learning algorithms lags>20% behind, highlighting a notable limitation in existing approaches' ToM capabilities.",
            "year": 2022,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel NLP dataset, ToM-in-AMC, the first assessment of machines' meta-learning of ToM in a realistic narrative understanding scenario, and a novel ToM prompting approach designed to explicitly assess the influence of multiple ToM dimensions."
            },
            "score": 2
        },
        {
            "id": "219e10c43f0d5626b05b1f0ee40b61327d213c66",
            "paperId": "219e10c43f0d5626b05b1f0ee40b61327d213c66",
            "title": "Heterogeneous Data-Aware Federated Learning for Intrusion Detection Systems via Meta-Sampling in Artificial Intelligence of Things",
            "abstract": "Intrusion detection systems (IDSs) integrated with machine learning (ML) techniques have proven to be effective defenses against the increasing cybersecurity attacks in the Artificial Intelligence of Things (AIoT) domain. Privacy concerns have prompted the emergence of federated learning (FL) as a promising solution for AIoT intrusion detection. Despite their potential, FL-based IDSs still face challenges related to class-imbalanced data and Non-Independent and Identically Distributed (non-IID) data among AIoT devices. These challenges hinder FL from learning meaningful features from the data, thus impeding the convergence of the learning process. To tackle these issues, this article proposes a clustering-enabled federated meta-training (CFMT) framework for AIoT intrusion detection. The proposed CFMT framework effectively addresses the negative impact of imbalanced and non-IID data. Specifically, we design a data- and model-agnostic meta-sampler that adaptively balances local data sets, thereby mitigating the data imbalance problem. Additionally, we propose a dynamic clustering algorithm that selectively eliminates the local models affected by the training state bias caused by non-IID data, thereby addressing the non-IID data issue. Extensive case studies on two real-world data sets demonstrate the superior performance of the proposed CFMT framework compared to existing solutions, including federated non-IID algorithms and federated imbalanced learning algorithms, in terms of IDS performance. Our code and data are available at https://gitee.com/mindspore/models/tree/master/research/cv/HDFL-IDS-Meta.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A clustering-enabled federated meta-training (CFMT) framework for AIoT intrusion detection that addresses the negative impact of imbalanced and non-IID data and design a data- and model-agnostic meta-sampler that adaptively balances local data sets, thereby mitigating the data imbalance problem."
            },
            "score": 2
        },
        {
            "id": "d3e4553f0a1fd465ae358701f1bdc2e8265308d6",
            "paperId": "d3e4553f0a1fd465ae358701f1bdc2e8265308d6",
            "title": "BigBIO: A Framework for Data-Centric Biomedical Natural Language Processing",
            "abstract": "Training and evaluating language models increasingly requires the construction of meta-datasets --diverse collections of curated data with clear provenance. Natural language prompting has recently lead to improved zero-shot generalization by transforming existing, supervised datasets into a diversity of novel pretraining tasks, highlighting the benefits of meta-dataset curation. While successful in general-domain text, translating these data-centric approaches to biomedical language modeling remains challenging, as labeled biomedical datasets are significantly underrepresented in popular data hubs. To address this challenge, we introduce BigBIO a community library of 126+ biomedical NLP datasets, currently covering 12 task categories and 10+ languages. BigBIO facilitates reproducible meta-dataset curation via programmatic access to datasets and their metadata, and is compatible with current platforms for prompt engineering and end-to-end few/zero shot language model evaluation. We discuss our process for task schema harmonization, data auditing, contribution guidelines, and outline two illustrative use cases: zero-shot evaluation of biomedical prompts and large-scale, multi-task learning. BigBIO is an ongoing community effort and is available at https://github.com/bigscience-workshop/biomedical",
            "year": 2022,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "BigBIO is introduced, a community library of 126+ biomedical NLP datasets, currently covering 12 task categories and 10+ languages that facilitates reproducible meta-dataset curation via programmatic access to datasets and their metadata, and is compatible with current platforms for prompt engineering and end-to-end few/zero shot language model evaluation."
            },
            "score": 2
        },
        {
            "id": "321cb42c7b97755b19cf7fae43fb2cb6b349841d",
            "paperId": "321cb42c7b97755b19cf7fae43fb2cb6b349841d",
            "title": "Towards Low-Resource Automatic Program Repair with Meta-Learning and Pretrained Language Models",
            "abstract": ",",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 1
        },
        {
            "id": "8489b55d992e6a3ac54aec7094a42ec8e333012f",
            "paperId": "8489b55d992e6a3ac54aec7094a42ec8e333012f",
            "title": "SELFormer: molecular representation learning via SELFIES language models",
            "abstract": "Automated computational analysis of the vast chemical space is critical for numerous fields of research such as drug discovery and material science. Representation learning techniques have recently been employed with the primary objective of generating compact and informative numerical expressions of complex data, for efficient usage in subsequent prediction tasks. One approach to efficiently learn molecular representations is processing string-based notations of chemicals via natural language processing algorithms. Majority of the methods proposed so far utilize SMILES notations for this purpose, which is the most extensively used string-based encoding for molecules. However, SMILES is associated with numerous problems related to validity and robustness, which may prevent the model from effectively uncovering the knowledge hidden in the data. In this study, we propose SELFormer, a transformer architecture-based chemical language model (CLM) that utilizes a 100% valid, compact and expressive notation, SELFIES, as input, in order to learn flexible and high-quality molecular representations. SELFormer is pre-trained on two million drug-like compounds and fine-tuned for diverse molecular property prediction tasks. Our performance evaluation has revealed that, SELFormer outperforms all competing methods, including graph learning-based approaches and SMILES-based CLMs, on predicting aqueous solubility of molecules and adverse drug reactions, while producing comparable results for the remaining tasks. We also visualized molecular representations learned by SELFormer via dimensionality reduction, which indicated that even the pre-trained model can discriminate molecules with differing structural properties. We shared SELFormer as a programmatic tool, together with its datasets and pre-trained models at https://github.com/HUBioDataLab/SELFormer. Overall, our research demonstrates the benefit of using the SELFIES notations in the context of chemical language modeling and opens up new possibilities for the design and discovery of novel drug candidates with desired features.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The research demonstrates the benefit of using the SELFIES notations in the context of chemical language modeling and opens up new possibilities for the design and discovery of novel drug candidates with desired features."
            },
            "score": 1
        },
        {
            "id": "1cd8373490efc2d74c2796f4b2aa27c7d4415ec9",
            "paperId": "1cd8373490efc2d74c2796f4b2aa27c7d4415ec9",
            "title": "VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models",
            "abstract": "Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a vision-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop robot trajectories with robustness to dynamic perturbations. We further demonstrate how the proposed framework can benefit from online experiences by efficiently learning a dynamics model for scenes that involve contact-rich interactions. We present a large-scale study of the proposed method in both simulated and real-robot environments, showcasing the ability to perform a large variety of everyday manipulation tasks specified in free-form natural language. Videos and code at https://voxposer.github.io",
            "year": 2023,
            "citationCount": 156,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A large-scale study of the proposed framework to synthesize closed-loop robot trajectories with robustness to dynamic perturbations is presented, showcasing the ability to perform a large variety of everyday manipulation tasks specified in free-form natural language."
            },
            "score": 1
        },
        {
            "id": "ee55c145862f134e98af90e591c321201bf3225f",
            "paperId": "ee55c145862f134e98af90e591c321201bf3225f",
            "title": "Handling Class Imbalance when Detecting Dataset Mentions with Pre-trained Language Models",
            "abstract": "Understanding the links between datasets and publications is a hard task. Yet it is very important to improve dataset discovery and FAIR-ness of research data. However, only a few datasets with such high-quality links exist. Human annotations are the gold standard for producing such links, but it is a time-consuming manual task, much of which is spent reading text that is not connected to dataset mentions at all. In this paper, we propose a \ufb01lter to pre-screen scienti\ufb01c publications\u2019 sections, so that we can \ufb01nd candidates for dataset mentions reliably. For this, we test both BERT and RoBERTa on sections content as well as on sections title. The main challenge is the inherent imbalance in the data, which we tackle using different imbalance handling techniques, such as re-sampling and variations of the loss function. The best result was obtained when using RoBERTa on section contents by combining re-sampling, balanced focal loss, and a recall-biased validation metric to get a fairly high recall and acceptable precision. The source code and the best obtained model are available here 1 .",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a connection to pre-screen scienti\ufb01c publications\u2019 sections so that candidates for dataset mentions reliably are selected, and tests both BERT and RoBERTa on sections content as well as on sections title."
            },
            "score": 1
        }
    ],
    "novelty": "yes"
}