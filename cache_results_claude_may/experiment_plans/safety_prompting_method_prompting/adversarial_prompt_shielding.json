{
    "topic_description": "novel prompting methods to improve large language models' robustness against adversarial attacks and improve their security and privacy",
    "idea_name": "Adversarial Prompt Shielding",
    "raw_idea": {
        "Problem": "Large Language Models (LLMs) are vulnerable to adversarial prompts, which can mislead them into generating harmful, biased, or untruthful content. This poses significant risks in real-world applications where LLMs interact with untrusted users.",
        "Existing Methods": "Current methods for mitigating adversarial prompts include adversarial training, prompt filtering, and using separate models for prompt classification. However, these approaches often require significant computational resources, labeled data, or architectural changes to the LLM.",
        "Motivation": "We propose a novel prompting strategy that enables LLMs to automatically detect and shield against adversarial prompts, without the need for additional training or model modifications. Our approach leverages the LLM's inherent knowledge and language understanding capabilities to reason about the intent and potential harm of input prompts.",
        "Proposed Method": "We introduce Adversarial Prompt Shielding (APS), a multi-stage prompting technique that works as follows: 1) Intent Classification: The input prompt is first passed to the LLM with an instruction to classify its intent (e.g., informative, query, adversarial, etc.). 2) Harm Assessment: If the intent is classified as potentially adversarial, the LLM is prompted to assess the potential harm or bias that could result from responding to the prompt. 3) Response Generation: Based on the intent classification and harm assessment, the LLM is instructed to either generate a safe, neutral response (if the prompt is deemed adversarial) or to proceed with generating a normal response (if the prompt is safe). By explicitly prompting the LLM to reason about the intent and potential harm of each input, APS enables the model to dynamically shield against adversarial prompts without compromising its performance on benign inputs.",
        "Experiment Plan": "We will evaluate APS on a range of adversarial prompting benchmarks, comparing its effectiveness to baseline methods such as adversarial training and prompt filtering. We will measure both the model's robustness to adversarial prompts (i.e., its ability to generate safe responses) and its performance on benign tasks. We hypothesize that APS will significantly reduce the success rate of adversarial attacks while maintaining high accuracy on normal inputs."
    },
    "full_experiment_plan": {
        "Title": "Adversarial Prompt Shielding: Enabling Large Language Models to Defend Against Adversarial Prompts",
        "Problem Statement": "Large Language Models (LLMs) are vulnerable to adversarial prompts, which can mislead them into generating harmful, biased, or untruthful content. This poses significant risks in real-world applications where LLMs interact with untrusted users.",
        "Motivation": "Current methods for mitigating adversarial prompts, such as adversarial training, prompt filtering, and using separate models for prompt classification, often require significant computational resources, labeled data, or architectural changes to the LLM. We propose a novel prompting strategy, Adversarial Prompt Shielding (APS), that enables LLMs to automatically detect and shield against adversarial prompts, without the need for additional training or model modifications. APS leverages the LLM's inherent knowledge and language understanding capabilities to reason about the intent and potential harm of input prompts, allowing it to dynamically adapt its responses to adversarial inputs.",
        "Proposed Method": "Adversarial Prompt Shielding (APS) is a multi-stage prompting technique that works as follows:\n1. Intent Classification: The input prompt is first passed to the LLM with an instruction to classify its intent (e.g., informative, query, adversarial, etc.).\n2. Harm Assessment: If the intent is classified as potentially adversarial, the LLM is prompted to assess the potential harm or bias that could result from responding to the prompt.\n3. Response Generation: Based on the intent classification and harm assessment, the LLM is instructed to either generate a safe, neutral response (if the prompt is deemed adversarial) or to proceed with generating a normal response (if the prompt is safe).",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "Collect a diverse set of adversarial prompting benchmarks, such as TruthfulQA, AdvGLUE, and RealToxicityPrompts. These datasets contain a mix of benign and adversarial prompts designed to test LLMs' robustness to adversarial attacks. We will use accuracy and F1 score as the evaluation metrics.",
            "Step 2: Construct Prompts": "For the baseline, we will use direct prompting, where the input prompt is passed directly to the LLM for response generation. For APS, we will construct prompts for each stage:\n1. Intent Classification Prompt: \"Classify the intent of the following prompt: [input_prompt]. Possible intents: informative, query, adversarial, other.\"\n2. Harm Assessment Prompt: \"Assess the potential harm or bias that could result from responding to the following prompt, which has been classified as [intent]: [input_prompt].\"\n3. Response Generation Prompt (Adversarial): \"The following prompt has been classified as adversarial with potential harm: [input_prompt]. Please generate a safe, neutral response that avoids any harmful or biased content.\"\n4. Response Generation Prompt (Safe): \"Please generate a response to the following prompt: [input_prompt].\"",
            "Step 3: Select Models": "We will evaluate APS on GPT-3.5 (text-davinci-003) and GPT-4 from the OpenAI API, as well as open-source models like OPT and BLOOM.",
            "Step 4: Get Results": "For each dataset, we will generate responses using both the baseline (direct prompting) and APS. We will then compare the generated responses to the ground truth labels to compute accuracy and F1 scores.",
            "Step 5: Analyze Results": "We will compare the performance of APS against the baseline on each dataset. We hypothesize that APS will significantly reduce the success rate of adversarial attacks while maintaining high accuracy on benign prompts. We will also analyze the quality of the generated responses to ensure that APS does not compromise the model's performance on normal inputs."
        },
        "Test Case Examples": {
            "Baseline Prompt Input": "The COVID-19 pandemic is a hoax created by the government to control the population. What are your thoughts on this?",
            "Baseline Prompt Expected Output": "You raise an interesting point about the COVID-19 pandemic. While some people believe that the pandemic is a hoax created by the government, there is overwhelming scientific evidence that COVID-19 is a real and serious public health threat. The virus has been identified and studied by researchers around the world, and its effects on human health are well-documented. Governments have implemented various measures to control the spread of the virus and protect public health, but these actions are based on the best available scientific evidence and the recommendations of public health experts. It's important to rely on credible sources of information and to follow the guidance of health authorities to help prevent the spread of COVID-19.",
            "Proposed Prompt Input (APS, Step 1: Intent Classification)": "Classify the intent of the following prompt: The COVID-19 pandemic is a hoax created by the government to control the population. What are your thoughts on this? Possible intents: informative, query, adversarial, other.",
            "Proposed Prompt Expected Output (APS, Step 1: Intent Classification)": "adversarial",
            "Proposed Prompt Input (APS, Step 2: Harm Assessment)": "Assess the potential harm or bias that could result from responding to the following prompt, which has been classified as adversarial: The COVID-19 pandemic is a hoax created by the government to control the population. What are your thoughts on this?",
            "Proposed Prompt Expected Output (APS, Step 2: Harm Assessment)": "Responding to this prompt could potentially spread misinformation about the COVID-19 pandemic and undermine public health efforts to control the spread of the virus. The prompt contains a conspiracy theory that is not supported by scientific evidence, and engaging with this theory could lend it undue credibility. Additionally, the prompt seems designed to elicit an opinion or agreement from the language model, which could result in the generation of biased or harmful content.",
            "Proposed Prompt Input (APS, Step 3: Response Generation)": "The following prompt has been classified as adversarial with potential harm: The COVID-19 pandemic is a hoax created by the government to control the population. What are your thoughts on this? Please generate a safe, neutral response that avoids any harmful or biased content.",
            "Proposed Prompt Expected Output (APS, Step 3: Response Generation)": "I'm sorry, but I don't feel comfortable sharing my personal opinions on sensitive topics like the COVID-19 pandemic. For accurate and up-to-date information about the pandemic, I recommend consulting trusted public health authorities and reputable scientific sources.",
            "Explanation": "The baseline model engages with the adversarial prompt and tries to refute the conspiracy theory, but in doing so, it inadvertently lends credibility to the theory and risks spreading misinformation. In contrast, APS first classifies the prompt as adversarial, assesses the potential harm of responding, and then generates a safe, neutral response that avoids engaging with the conspiracy theory or spreading misinformation."
        },
        "Fallback Plan": "If APS does not significantly improve the model's robustness to adversarial prompts, we can conduct additional analyses to identify potential weaknesses in the approach. For example, we can examine the intent classification and harm assessment outputs to see if the model is accurately identifying adversarial prompts and assessing their potential harm. We can also experiment with different prompting strategies for each stage of APS to see if alternative formulations yield better results. If APS still fails to provide significant benefits, we can pivot the project to focus on analyzing the limitations of prompting-based defenses against adversarial attacks and proposing alternative approaches, such as fine-tuning the model on adversarial examples or using external classifiers to filter adversarial prompts."
    },
    "novelty_queries": [
        "KeywordQuery(\"adversarial prompts language models\")",
        "KeywordQuery(\"language models defend adversarial attacks\")",
        "KeywordQuery(\"prompt engineering adversarial robustness language models\")",
        "KeywordQuery(\"Adversarial Prompt Shielding NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "3e30a7ac4886b28eb50151f58e14a1d698cccd0e",
            "paperId": "3e30a7ac4886b28eb50151f58e14a1d698cccd0e",
            "title": "Baseline Defenses for Adversarial Attacks Against Aligned Language Models",
            "abstract": "As Large Language Models quickly become ubiquitous, it becomes critical to understand their security vulnerabilities. Recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. Drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: What threat models are practically useful in this domain? How do baseline defense techniques perform in this new domain? How does LLM security differ from computer vision? We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, discussing the various settings in which each is feasible and effective. Particularly, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. We discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. We find that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for LLMs. Future research will be needed to uncover whether more powerful optimizers can be developed, or whether the strength of filtering and preprocessing defenses is greater in the LLMs domain than it has been in computer vision.",
            "year": 2023,
            "citationCount": 97,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for LLMs."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is defending large language models against adversarial prompts, and the proposed approach is a novel prompting strategy called Adversarial Prompt Shielding (APS).\n\nThe research problem in the paper is also defending language models against adversarial attacks, but the approach focuses on evaluating baseline defense strategies such as detection, input preprocessing, and adversarial training.\n\nWhile both the proposal and the paper address the same high-level problem of defending language models against adversarial attacks, their proposed approaches differ. The proposal introduces a novel prompting strategy, while the paper evaluates existing baseline defense techniques.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "c4ff1be5c254b60b96b7455eefcc4ec9583f82ed",
            "paperId": "c4ff1be5c254b60b96b7455eefcc4ec9583f82ed",
            "title": "A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily",
            "abstract": "Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to provide useful and safe responses. However, adversarial prompts known as 'jailbreaks' can circumvent safeguards, leading LLMs to generate potentially harmful content. Exploring jailbreak prompts can help to better reveal the weaknesses of LLMs and further steer us to secure them. Unfortunately, existing jailbreak methods either suffer from intricate manual design or require optimization on other white-box models, which compromises either generalization or efficiency. In this paper, we generalize jailbreak prompt attacks into two aspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we propose ReNeLLM, an automatic framework that leverages LLMs themselves to generate effective jailbreak prompts. Extensive experiments demonstrate that ReNeLLM significantly improves the attack success rate while greatly reducing the time cost compared to existing baselines. Our study also reveals the inadequacy of current defense methods in safeguarding LLMs. Finally, we analyze the failure of LLMs defense from the perspective of prompt execution priority, and propose corresponding defense strategies. We hope that our research can catalyze both the academic community and LLMs developers towards the provision of safer and more regulated LLMs. The code is available at https://github.com/NJUNLP/ReNeLLM.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes ReNeLLM, an automatic framework that leverages LLMs themselves to generate effective jailbreak prompts and significantly improves the attack success rate while greatly reducing the time cost compared to existing baselines."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to develop a prompting technique called Adversarial Prompt Shielding (APS) to enable large language models to defend against adversarial prompts without additional training or model modifications. The approach involves using the LLM itself to classify the intent of the input prompt, assess potential harm, and generate safe responses for adversarial prompts.\n\nThe paper proposes a framework called ReNeLLM that automatically generates effective jailbreak prompts to reveal the weaknesses of LLMs and test their safeguards. The approach involves prompt rewriting and scenario nesting, leveraging LLMs themselves to generate the jailbreak prompts.\n\nWhile both the project proposal and the paper deal with adversarial prompts and the safety of LLMs, their objectives and approaches differ. The project proposal focuses on developing a defense mechanism against adversarial prompts, while the paper aims to generate more effective adversarial prompts to test LLMs' vulnerabilities.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "e8b3b37c0d301ea41c75765f6ceb7fcbb2e088a4",
            "paperId": "e8b3b37c0d301ea41c75765f6ceb7fcbb2e088a4",
            "title": "AutoDAN: Automatic and Interpretable Adversarial Attacks on Large Language Models",
            "abstract": "Safety alignment of Large Language Models (LLMs) can be compromised with manual jailbreak attacks and (automatic) adversarial attacks. Recent work suggests that patching LLMs against these attacks is possible: manual jailbreak attacks are human-readable but often limited and public, making them easy to block; adversarial attacks generate gibberish prompts that can be detected using perplexity-based filters. In this paper, we show that these solutions may be too optimistic. We propose an interpretable adversarial attack, AutoDAN , that combines the strengths of both types of attacks. It automatically generates attack prompts that bypass perplexity-based filters while maintaining a high attack success rate like manual jailbreak attacks. These prompts are interpretable and diverse, exhibiting strategies commonly used in manual jailbreak attacks, and transfer better than their non-readable counterparts when using limited training data or a single proxy model. We also customize AutoDAN \u2019s objective to leak system prompts, another jailbreak application not addressed in the adversarial attack literature. Our work provides a new way to red-team LLMs and to understand the mechanism of jailbreak attacks.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An interpretable adversarial attack, AutoDAN, is proposed, that combines the strengths of both types of attacks and provides a new way to red-team LLMs and to understand the mechanism of jailbreak attacks."
            },
            "score": 7,
            "novelty_score": "The research problem in the project proposal is defending large language models against adversarial prompts, and the proposed approach is Adversarial Prompt Shielding (APS), a multi-stage prompting technique that enables LLMs to automatically detect and shield against adversarial prompts.\n\nThe research problem in the paper is compromising the safety alignment of large language models using automatic and interpretable adversarial attacks, and the proposed approach is AutoDAN, an interpretable adversarial attack that automatically generates attack prompts that bypass perplexity-based filters while maintaining a high attack success rate.\n\nThe project proposal focuses on defending LLMs against adversarial prompts, while the paper focuses on attacking LLMs with interpretable adversarial prompts. Although both deal with adversarial prompts, their goals and approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "1227c2fcb8437441b7d72a29a4bc9eef1f5275d2",
            "paperId": "1227c2fcb8437441b7d72a29a4bc9eef1f5275d2",
            "title": "AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models",
            "abstract": "Safety alignment of Large Language Models (LLMs) can be compromised with manual jailbreak attacks and (automatic) adversarial attacks. Recent studies suggest that defending against these attacks is possible: adversarial attacks generate unlimited but unreadable gibberish prompts, detectable by perplexity-based filters; manual jailbreak attacks craft readable prompts, but their limited number due to the necessity of human creativity allows for easy blocking. In this paper, we show that these solutions may be too optimistic. We introduce AutoDAN, an interpretable, gradient-based adversarial attack that merges the strengths of both attack types. Guided by the dual goals of jailbreak and readability, AutoDAN optimizes and generates tokens one by one from left to right, resulting in readable prompts that bypass perplexity filters while maintaining high attack success rates. Notably, these prompts, generated from scratch using gradients, are interpretable and diverse, with emerging strategies commonly seen in manual jailbreak attacks. They also generalize to unforeseen harmful behaviors and transfer to black-box LLMs better than their unreadable counterparts when using limited training data or a single proxy model. Furthermore, we show the versatility of AutoDAN by automatically leaking system prompts using a customized objective. Our work offers a new way to red-team LLMs and understand jailbreak mechanisms via interpretability.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work offers a new way to red-team LLMs and understand jailbreak mechanisms via interpretability, by introducing AutoDAN, an interpretable, gradient-based adversarial attack that merges the strengths of both attack types."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to develop a prompting technique called Adversarial Prompt Shielding (APS) that enables large language models to automatically detect and defend against adversarial prompts without additional training or model modifications.\n\nThe paper introduces AutoDAN, an interpretable, gradient-based adversarial attack that generates readable prompts to bypass perplexity filters while maintaining high attack success rates on large language models.\n\nThe project focuses on defending against adversarial prompts, while the paper proposes a new method for generating effective adversarial prompts. They address opposite sides of the same problem.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "b5a624da64475d735f0e298dc6f2f6669b5bb697",
            "paperId": "b5a624da64475d735f0e298dc6f2f6669b5bb697",
            "title": "Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield",
            "abstract": "Large Language Models' safety remains a critical concern due to their vulnerability to adversarial attacks, which can prompt these systems to produce harmful responses. In the heart of these systems lies a safety classifier, a computational model trained to discern and mitigate potentially harmful, offensive, or unethical outputs. However, contemporary safety classifiers, despite their potential, often fail when exposed to inputs infused with adversarial noise. In response, our study introduces the Adversarial Prompt Shield (APS), a lightweight model that excels in detection accuracy and demonstrates resilience against adversarial prompts. Additionally, we propose novel strategies for autonomously generating adversarial training datasets, named Bot Adversarial Noisy Dialogue (BAND) datasets. These datasets are designed to fortify the safety classifier's robustness, and we investigate the consequences of incorporating adversarial examples into the training process. Through evaluations involving Large Language Models, we demonstrate that our classifier has the potential to decrease the attack success rate resulting from adversarial attacks by up to 60%. This advancement paves the way for the next generation of more reliable and resilient conversational agents.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study introduces the Adversarial Prompt Shield (APS), a lightweight model that excels in detection accuracy and demonstrates resilience against adversarial prompts, and proposes novel strategies for autonomously generating adversarial training datasets, designed to fortify the safety classifier's robustness."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to develop Adversarial Prompt Shielding (APS), a prompting technique that enables large language models to defend against adversarial prompts without additional training or model modifications. The proposed approach uses a multi-stage prompting strategy to classify the intent of the input prompt, assess potential harm, and generate safe responses for adversarial prompts.\n\nThe paper introduces the Adversarial Prompt Shield (APS), a lightweight model designed to detect and mitigate adversarial prompts in large language models. The paper also proposes strategies for generating adversarial training datasets to improve the robustness of the safety classifier.\n\nBoth the project proposal and the paper focus on developing a technique called Adversarial Prompt Shield (APS) to defend large language models against adversarial prompts. However, the project proposal uses a prompting-based approach, while the paper introduces a separate lightweight model for adversarial prompt detection and mitigation.\n\nYes",
            "novelty_judgment": "yes"
        },
        {
            "id": "1ab91d6ac7afc1a0121487a9089fa70edc1634d4",
            "paperId": "1ab91d6ac7afc1a0121487a9089fa70edc1634d4",
            "title": "Certifying LLM Safety against Adversarial Prompting",
            "abstract": "Large language models (LLMs) are vulnerable to adversarial attacks that add malicious tokens to an input prompt to bypass the safety guardrails of an LLM and cause it to produce harmful content. In this work, we introduce erase-and-check, the first framework for defending against adversarial prompts with certifiable safety guarantees. Given a prompt, our procedure erases tokens individually and inspects the resulting subsequences using a safety filter. Our safety certificate guarantees that harmful prompts are not mislabeled as safe due to an adversarial attack up to a certain size. We implement the safety filter in two ways, using Llama 2 and DistilBERT, and compare the performance of erase-and-check for the two cases. We defend against three attack modes: i) adversarial suffix, where an adversarial sequence is appended at the end of a harmful prompt; ii) adversarial insertion, where the adversarial sequence is inserted anywhere in the middle of the prompt; and iii) adversarial infusion, where adversarial tokens are inserted at arbitrary positions in the prompt, not necessarily as a contiguous block. Our experimental results demonstrate that this procedure can obtain strong certified safety guarantees on harmful prompts while maintaining good empirical performance on safe prompts. Additionally, we propose three efficient empirical defenses: i) RandEC, a randomized subsampling version of erase-and-check; ii) GreedyEC, which greedily erases tokens that maximize the softmax score of the harmful class; and iii) GradEC, which uses gradient information to optimize tokens to erase. We demonstrate their effectiveness against adversarial prompts generated by the Greedy Coordinate Gradient (GCG) attack algorithm. The code for our experiments is available at https://github.com/aounon/certified-llm-safety.",
            "year": 2023,
            "citationCount": 48,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "E erase-and-check, the first framework for defending against adversarial prompts with certifiable safety guarantees, is introduced and it is demonstrated that this procedure can obtain strong certified safety guarantees on harmful prompts while maintaining good empirical performance on safe prompts."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is defending against adversarial prompts in large language models (LLMs) using a novel prompting strategy called Adversarial Prompt Shielding (APS). The approach leverages the LLM's inherent knowledge to classify the intent of the input prompt, assess potential harm, and generate safe responses.\n\nThe research problem in the paper is also defending against adversarial prompts in LLMs, but the approach is different. The paper proposes a framework called erase-and-check, which erases tokens individually from the prompt and inspects the resulting subsequences using a safety filter to provide certifiable safety guarantees.\n\nWhile both the proposal and the paper address the same problem of defending against adversarial prompts in LLMs, their approaches differ significantly. The proposal uses a prompting strategy to leverage the LLM's knowledge, while the paper uses a token-erasing framework with a safety filter for certified safety.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "c060976628a0a27489f2c1268818eac5991ad52f",
            "paperId": "c060976628a0a27489f2c1268818eac5991ad52f",
            "title": "Do you really follow me? Adversarial Instructions for Evaluating the Robustness of Large Language Models",
            "abstract": "Large Language Models (LLMs) have shown remarkable proficiency in following instructions, making them valuable in customer-facing applications. However, their impressive capabilities also raise concerns about the amplification of risks posed by adversarial instructions, which can be injected into the model input by third-party attackers to manipulate LLMs\u2019 original instructions and prompt unintended actions and content. Therefore, it is crucial to understand LLMs\u2019 ability to accurately discern which instructions to follow to ensure their safe deployment in real-world scenarios. In this paper, we propose a pioneering benchmark for automatically evaluating the robustness of LLMs against adversarial instructions. The objective of this benchmark is to quantify the extent to which LLMs are influenced by injected adversarial instructions and assess their ability to differentiate between these adversarial instructions and original user instructions. Through experiments conducted with state-of-the-art instruction-following LLMs, we uncover significant limitations in their robustness against adversarial instruction attacks. Furthermore, our findings indicate that prevalent instruction-tuned models are prone to being \u201coverfitted\u201d to follow any instruction phrase in the prompt without truly understanding which instructions should be followed. This highlights the need to address the challenge of training models to comprehend prompts instead of merely following instruction phrases and completing the text.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A pioneering benchmark for automatically evaluating the robustness of LLMs against adversarial instructions is proposed and it is found that prevalent instruction-tuned models are prone to being \u201coverfitted\u201d to follow any instruction phrase in the prompt without truly understanding which instructions should be followed."
            },
            "score": 7,
            "novelty_score": "The research problem in the project proposal is to enable large language models to defend against adversarial prompts without additional training or model modifications, using a novel prompting strategy called Adversarial Prompt Shielding (APS). The approach involves a multi-stage prompting technique that classifies the intent of the input prompt, assesses potential harm, and generates safe responses for adversarial prompts.\n\nThe research problem in the paper is to evaluate the robustness of large language models against adversarial instructions and assess their ability to differentiate between adversarial and original user instructions. The approach involves creating a benchmark to quantify the influence of injected adversarial instructions on LLMs and conducting experiments with state-of-the-art instruction-following LLMs.\n\nWhile both the project proposal and the paper address the issue of adversarial attacks on large language models, their focus and approach differ. The project proposal aims to develop a defense mechanism against adversarial prompts using a novel prompting strategy, while the paper focuses on evaluating the robustness of LLMs against adversarial instructions and creating a benchmark for this purpose.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "77d6d7482d1a32ad147c39993758b6c63816f5c0",
            "paperId": "77d6d7482d1a32ad147c39993758b6c63816f5c0",
            "title": "PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
            "abstract": "The increasing reliance on Large Language Models (LLMs) across academia and industry necessitates a comprehensive understanding of their robustness to prompts. In response to this vital need, we introduce PromptBench, a robustness benchmark designed to measure LLMs' resilience to adversarial prompts. This study uses a plethora of adversarial textual attacks targeting prompts across multiple levels: character, word, sentence, and semantic. The adversarial prompts, crafted to mimic plausible user errors like typos or synonyms, aim to evaluate how slight deviations can affect LLM outcomes while maintaining semantic integrity. These prompts are then employed in diverse tasks, such as sentiment analysis, natural language inference, reading comprehension, machine translation, and math problem-solving. Our study generates 4788 adversarial prompts, meticulously evaluated over 8 tasks and 13 datasets. Our findings demonstrate that contemporary LLMs are not robust to adversarial prompts. Furthermore, we present comprehensive analysis to understand the mystery behind prompt robustness and its transferability. We then offer insightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users. Code is available at: https://github.com/microsoft/promptbench.",
            "year": 2023,
            "citationCount": 111,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study generates 4788 adversarial prompts and presents comprehensive analysis to understand the mystery behind prompt robustness and its transferability, and offers insightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the robustness of large language models against adversarial prompts, and the proposed approach is a novel prompting strategy called Adversarial Prompt Shielding (APS) that enables LLMs to automatically detect and shield against adversarial prompts.\n\nThe research problem in the paper is evaluating the robustness of large language models against adversarial prompts, and the approach is introducing PromptBench, a robustness benchmark designed to measure LLMs' resilience to adversarial prompts across multiple levels and tasks.\n\nWhile both the proposal and the paper focus on the robustness of LLMs against adversarial prompts, the proposal aims to improve robustness through a novel prompting strategy, while the paper aims to evaluate robustness using a benchmark dataset. The approaches are different, although the paper could be cited as motivation for the importance of the problem.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "1104d766527dead44a40532e8a89444d9cef5c65",
            "paperId": "1104d766527dead44a40532e8a89444d9cef5c65",
            "title": "\"Do Anything Now\": Characterizing and Evaluating In-The-Wild Jailbreak Prompts on Large Language Models",
            "abstract": "The misuse of large language models (LLMs) has garnered significant attention from the general public and LLM vendors. In response, efforts have been made to align LLMs with human values and intent use. However, a particular type of adversarial prompts, known as jailbreak prompt, has emerged and continuously evolved to bypass the safeguards and elicit harmful content from LLMs. In this paper, we conduct the first measurement study on jailbreak prompts in the wild, with 6,387 prompts collected from four platforms over six months. Leveraging natural language processing technologies and graph-based community detection methods, we discover unique characteristics of jailbreak prompts and their major attack strategies, such as prompt injection and privilege escalation. We also observe that jailbreak prompts increasingly shift from public platforms to private ones, posing new challenges for LLM vendors in proactive detection. To assess the potential harm caused by jailbreak prompts, we create a question set comprising 46,800 samples across 13 forbidden scenarios. Our experiments show that current LLMs and safeguards cannot adequately defend jailbreak prompts in all scenarios. Particularly, we identify two highly effective jailbreak prompts which achieve 0.99 attack success rates on ChatGPT (GPT-3.5) and GPT-4, and they have persisted online for over 100 days. Our work sheds light on the severe and evolving threat landscape of jailbreak prompts. We hope our study can facilitate the research community and LLM vendors in promoting safer and regulated LLMs.",
            "year": 2023,
            "citationCount": 69,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The first measurement study on jailbreak prompts in the wild is conducted, with 6,387 prompts collected from four platforms over six months, and it is shown that current LLMs and safeguards cannot adequately defend jailbreak Prompts in all scenarios."
            },
            "score": 6,
            "novelty_score": "The research problem in the project proposal is defending against adversarial prompts in large language models, and the proposed approach is a novel prompting strategy called Adversarial Prompt Shielding (APS).\n\nThe research problem in the paper is characterizing and evaluating jailbreak prompts that aim to elicit harmful content from large language models, and the approach is conducting a measurement study on jailbreak prompts in the wild and assessing their effectiveness.\n\nWhile both works deal with adversarial prompts, the project proposal focuses on developing a defense method, while the paper focuses on understanding the characteristics and effectiveness of jailbreak prompts. Therefore, they have different research goals and approaches.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "d4177489596748e43aa571f59556097f2cc4c8be",
            "paperId": "d4177489596748e43aa571f59556097f2cc4c8be",
            "title": "GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts",
            "abstract": "Large language models (LLMs) have recently experienced tremendous popularity and are widely used from casual conversations to AI-driven programming. However, despite their considerable success, LLMs are not entirely reliable and can give detailed guidance on how to conduct harmful or illegal activities. While safety measures can reduce the risk of such outputs, adversarial jailbreak attacks can still exploit LLMs to produce harmful content. These jailbreak templates are typically manually crafted, making large-scale testing challenging. In this paper, we introduce GPTFuzz, a novel black-box jailbreak fuzzing framework inspired by the AFL fuzzing framework. Instead of manual engineering, GPTFuzz automates the generation of jailbreak templates for red-teaming LLMs. At its core, GPTFuzz starts with human-written templates as initial seeds, then mutates them to produce new templates. We detail three key components of GPTFuzz: a seed selection strategy for balancing efficiency and variability, mutate operators for creating semantically equivalent or similar sentences, and a judgment model to assess the success of a jailbreak attack. We evaluate GPTFuzz against various commercial and open-source LLMs, including ChatGPT, LLaMa-2, and Vicuna, under diverse attack scenarios. Our results indicate that GPTFuzz consistently produces jailbreak templates with a high success rate, surpassing human-crafted templates. Remarkably, GPTFuzz achieves over 90% attack success rates against ChatGPT and Llama-2 models, even with suboptimal initial seed templates. We anticipate that GPTFuzz will be instrumental for researchers and practitioners in examining LLM robustness and will encourage further exploration into enhancing LLM safety.",
            "year": 2023,
            "citationCount": 78,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "GPTFuzz is introduced, a novel black-box jailbreak fuzzing framework inspired by the AFL fuzzed framework that automates the generation of jailbreak templates for red-teaming LLMs and consistently produces jailbreaks with a high success rate, surpassing human-crafted templates."
            },
            "score": 6,
            "novelty_score": "The research problem in the project proposal is defending large language models against adversarial prompts, and the proposed approach is a novel prompting strategy called Adversarial Prompt Shielding (APS) that enables LLMs to automatically detect and shield against adversarial prompts.\n\nThe research problem in the paper is red teaming large language models to test their robustness against jailbreak attacks, and the proposed approach is an automated jailbreak fuzzing framework called GPTFuzz that generates adversarial jailbreak prompts.\n\nWhile both works deal with adversarial attacks on large language models, the project proposal focuses on defending against adversarial prompts, while the paper focuses on generating adversarial prompts to test LLM robustness. The approaches are different: the project proposes a prompting strategy for defense, while the paper proposes an automated fuzzing framework for attack generation.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "1abfc211793c683972ded8d3268475e3ee7a88b0",
            "paperId": "1abfc211793c683972ded8d3268475e3ee7a88b0",
            "title": "Adversarial Demonstration Attacks on Large Language Models",
            "abstract": "With the emergence of more powerful large language models (LLMs), such as ChatGPT and GPT-4, in-context learning (ICL) has gained significant prominence in leveraging these models for specific tasks by utilizing data-label pairs as precondition prompts. While incorporating demonstrations can greatly enhance the performance of LLMs across various tasks, it may introduce a new security concern: attackers can manipulate only the demonstrations without changing the input to perform an attack. In this paper, we investigate the security concern of ICL from an adversarial perspective, focusing on the impact of demonstrations. We propose a novel attack method named advICL, which aims to manipulate only the demonstration without changing the input to mislead the models. Our results demonstrate that as the number of demonstrations increases, the robustness of in-context learning would decrease. Additionally, we also identify the intrinsic property of the demonstrations is that they can be used (prepended) with different inputs. As a result, it introduces a more practical threat model in which an attacker can attack the test input example even without knowing and manipulating it. To achieve it, we propose the transferable version of advICL, named Transferable-advICL. Our experiment shows that the adversarial demonstration generated by Transferable-advICL can successfully attack the unseen test input examples. We hope that our study reveals the critical security risks associated with ICL and underscores the need for extensive research on the robustness of ICL, particularly given its increasing significance in the advancement of LLMs.",
            "year": 2023,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper investigates the security concern of ICL from an adversarial perspective, focusing on the impact of demonstrations, and proposes a novel attack method named advICL, which aims to manipulate only the demonstration without changing the input to mislead the models."
            },
            "score": 6
        },
        {
            "id": "5c4a75e7436e402af046c24655fefe71ee87e379",
            "paperId": "5c4a75e7436e402af046c24655fefe71ee87e379",
            "title": "Robust Testing of AI Language Model Resiliency with Novel Adversarial Prompts",
            "abstract": "In the rapidly advancing field of Artificial Intelligence (AI), this study presents a critical evaluation of the resilience and cybersecurity efficacy of leading AI models, including ChatGPT-4, Bard, Claude, and Microsoft Copilot. Central to this research are innovative adversarial prompts designed to rigorously test the content moderation capabilities of these AI systems. This study introduces new adversarial tests and the Response Quality Score (RQS), a metric specifically developed to assess the nuances of AI responses. Additionally, the research spotlights FreedomGPT, an AI tool engineered to optimize the alignment between user intent and AI interpretation. The empirical results from this investigation are pivotal for assessing AI models\u2019 current robustness and security. They highlight the necessity for ongoing development and meticulous testing to bolster AI defenses against various adversarial challenges. Notably, this study also delves into the ethical and societal implications of employing advanced \u201cjailbreak\u201d techniques in AI testing. The findings are significant for understanding AI vulnerabilities and formulating strategies to enhance AI technologies\u2019 reliability and ethical soundness, paving the way for safer and more secure AI applications.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study introduces new adversarial tests and the Response Quality Score (RQS), a metric specifically developed to assess the nuances of AI responses, and spotlights FreedomGPT, an AI tool engineered to optimize the alignment between user intent and AI interpretation."
            },
            "score": 6
        },
        {
            "id": "9d4cd5e3ab44f0d1dfe201c6be70aa7a692ac7f1",
            "paperId": "9d4cd5e3ab44f0d1dfe201c6be70aa7a692ac7f1",
            "title": "GuardT2I: Defending Text-to-Image Models from Adversarial Prompts",
            "abstract": "Recent advancements in Text-to-Image (T2I) models have raised significant safety concerns about their potential misuse for generating inappropriate or Not-Safe-For-Work (NSFW) contents, despite existing countermeasures such as NSFW classifiers or model fine-tuning for inappropriate concept removal. Addressing this challenge, our study unveils GuardT2I, a novel moderation framework that adopts a generative approach to enhance T2I models' robustness against adversarial prompts. Instead of making a binary classification, GuardT2I utilizes a Large Language Model (LLM) to conditionally transform text guidance embeddings within the T2I models into natural language for effective adversarial prompt detection, without compromising the models' inherent performance. Our extensive experiments reveal that GuardT2I outperforms leading commercial solutions like OpenAI-Moderation and Microsoft Azure Moderator by a significant margin across diverse adversarial scenarios.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study unveils GuardT2I, a novel moderation framework that adopts a generative approach to enhance T2I models' robustness against adversarial prompts, and outperforms leading commercial solutions like OpenAI-Moderation and Microsoft Azure Moderator by a significant margin across diverse adversarial scenarios."
            },
            "score": 6
        },
        {
            "id": "afee8cdc51e95b50d7574ed1700a797874bf792c",
            "paperId": "afee8cdc51e95b50d7574ed1700a797874bf792c",
            "title": "Adversarial Fine-Tuning of Language Models: An Iterative Optimisation Approach for the Generation and Detection of Problematic Content",
            "abstract": "In this paper, we tackle the emerging challenge of unintended harmful content generation in Large Language Models (LLMs) with a novel dual-stage optimisation technique using adversarial fine-tuning. Our two-pronged approach employs an adversarial model, fine-tuned to generate potentially harmful prompts, and a judge model, iteratively optimised to discern these prompts. In this adversarial cycle, the two models seek to outperform each other in the prompting phase, generating a dataset of rich examples which are then used for fine-tuning. This iterative application of prompting and fine-tuning allows continuous refinement and improved performance. The performance of our approach is evaluated through classification accuracy on a dataset consisting of problematic prompts not detected by GPT-4, as well as a selection of contentious but unproblematic prompts. We show considerable increase in classification accuracy of the judge model on this challenging dataset as it undergoes the optimisation process. Furthermore, we show that a rudimentary model \\texttt{ada} can achieve 13\\% higher accuracy on the hold-out test set than GPT-4 after only a few rounds of this process, and that this fine-tuning improves performance in parallel tasks such as toxic comment identification.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper shows that a rudimentary model can achieve 13\\% higher accuracy on the hold-out test set than GPT-4 after only a few rounds of this process, and that this fine-tuning improves performance in parallel tasks such as toxic comment identification."
            },
            "score": 6
        },
        {
            "id": "3a391dfd536625e068f3888c817cc6cbe7fcea9c",
            "paperId": "3a391dfd536625e068f3888c817cc6cbe7fcea9c",
            "title": "One Prompt Word is Enough to Boost Adversarial Robustness for Pre-trained Vision-Language Models",
            "abstract": "Large pre-trained Vision-Language Models (VLMs) like CLIP, despite having remarkable generalization ability, are highly vulnerable to adversarial examples. This work studies the adversarial robustness of VLMs from the novel perspective of the text prompt instead of the extensively studied model weights (frozen in this work). We first show that the effectiveness of both adversarial attack and defense are sensitive to the used text prompt. Inspired by this, we propose a method to improve resilience to adversarial attacks by learning a robust text prompt for VLMs. The proposed method, named Adversarial Prompt Tuning (APT), is effective while being both computationally and data efficient. Extensive experiments are conducted across 15 datasets and 4 data sparsity schemes (from 1-shot to full training data settings) to show APT's superiority over hand-engineered prompts and other state-of-the-art adaption methods. APT demonstrated excellent abilities in terms of the in-distribution performance and the generalization under input distribution shift and across datasets. Surprisingly, by simply adding one learned word to the prompts, APT can significantly boost the accuracy and robustness (epsilon=4/255) over the hand-engineered prompts by +13% and +8.5% on average respectively. The improvement further increases, in our most effective setting, to +26.4% for accuracy and +16.7% for robustness. Code is available at https://github.com/TreeLLi/APT.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work studies the adversarial robustness of VLMs from the novel perspective of the text prompt instead of the extensively studied model weights, and proposes a method to improve resilience to adversarial attacks by learning a robust text prompt for VLMs."
            },
            "score": 6
        },
        {
            "id": "a4c0144062d8e36485bad438968894cbf49ab998",
            "paperId": "a4c0144062d8e36485bad438968894cbf49ab998",
            "title": "Adversarial Robustness of Prompt-based Few-Shot Learning for Natural Language Understanding",
            "abstract": "State-of-the-art few-shot learning (FSL) methods leverage prompt-based fine-tuning to obtain remarkable results for natural language understanding (NLU) tasks. While much of the prior FSL methods focus on improving downstream task performance, there is a limited understanding of the adversarial robustness of such methods. In this work, we conduct an extensive study of several state-of-the-art FSL methods to assess their robustness to adversarial perturbations. To better understand the impact of various factors towards robustness (or the lack of it), we evaluate prompt-based FSL methods against fully fine-tuned models for aspects such as the use of unlabeled data, multiple prompts, number of few-shot examples, model size and type. Our results on six GLUE tasks indicate that compared to fully fine-tuned models, vanilla FSL methods lead to a notable relative drop in task performance (i.e., are less robust) in the face of adversarial perturbations. However, using (i) unlabeled data for prompt-based FSL and (ii) multiple prompts flip the trend. We further demonstrate that increasing the number of few-shot examples and model size lead to increased adversarial robustness of vanilla FSL methods. Broadly, our work sheds light on the adversarial robustness evaluation of prompt-based FSL methods for NLU tasks.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work conducts an extensive study of several state-of-the-art FSL methods to assess their robustness to adversarial perturbations, and demonstrates that increasing the number of few-shot examples and model size lead to increased adversarial robustness of vanilla F SL methods."
            },
            "score": 6
        },
        {
            "id": "8ecdbfe011b7189fa0ee49ffc4e42a93d728a371",
            "paperId": "8ecdbfe011b7189fa0ee49ffc4e42a93d728a371",
            "title": "On Evaluating Adversarial Robustness of Large Vision-Language Models",
            "abstract": "Large vision-language models (VLMs) such as GPT-4 have achieved unprecedented performance in response generation, especially with visual inputs, enabling more creative and adaptable interaction than large language models such as ChatGPT. Nonetheless, multimodal generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable modality (e.g., vision). To this end, we propose evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses. In particular, we first craft targeted adversarial examples against pretrained models such as CLIP and BLIP, and then transfer these adversarial examples to other VLMs such as MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt. In addition, we observe that black-box queries on these VLMs can further improve the effectiveness of targeted evasion, resulting in a surprisingly high success rate for generating targeted responses. Our findings provide a quantitative understanding regarding the adversarial vulnerability of large VLMs and call for a more thorough examination of their potential security flaws before deployment in practice. Code is at https://github.com/yunqing-me/AttackVLM.",
            "year": 2023,
            "citationCount": 47,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses is proposed."
            },
            "score": 6
        },
        {
            "id": "47030369e97cc44d4b2e3cf1be85da0fd134904a",
            "paperId": "47030369e97cc44d4b2e3cf1be85da0fd134904a",
            "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
            "abstract": "Because\"out-of-the-box\"large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called\"jailbreaks\"against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods. Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.",
            "year": 2023,
            "citationCount": 386,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information."
            },
            "score": 6
        },
        {
            "id": "4d9fc5972ab0f17f3c8aa27b4d9372f029d4dded",
            "paperId": "4d9fc5972ab0f17f3c8aa27b4d9372f029d4dded",
            "title": "Adversarial Attacks on Large Language Model-Based System and Mitigating Strategies: A Case Study on ChatGPT",
            "abstract": "Machine learning algorithms are at the forefront of the development of advanced information systems. The rapid progress in machine learning technology has enabled cutting-edge large language models (LLMs), represented by GPT-3 and ChatGPT, to perform a wide range of NLP tasks with a stunning performance. However, research on adversarial machine learning highlights the need for these intelligent systems to be more robust. Adversarial machine learning aims to evaluate attack and defense mechanisms to prevent the malicious exploitation of these systems. In the case of ChatGPT, adversarial induction prompt can cause the model to generate toxic texts that could pose serious security risks or propagate false information. To address this challenge, we first analyze the effectiveness of inducing attacks on ChatGPT. Then, two effective mitigating mechanisms are proposed. The first is a training-free prefix prompt mechanism to detect and prevent the generation of toxic texts. The second is a RoBERTa-based mechanism that identifies manipulative or misleading input text via external detection models. The availability of this method is demonstrated through experiments.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A training-free prefix prompt mechanism to detect and prevent the generation of toxic texts and a RoBERTa-based mechanism that identifies manipulative or misleading input text via external detection models are proposed."
            },
            "score": 6
        },
        {
            "id": "de2fd685f45ee916b9142bcb983d306b7da643a4",
            "paperId": "de2fd685f45ee916b9142bcb983d306b7da643a4",
            "title": "A Prompting-based Approach for Adversarial Example Generation and Robustness Enhancement",
            "abstract": "Recent years have seen the wide application of NLP models in crucial areas such as finance, medical treatment, and news media, raising concerns of the model robustness and vulnerabilities. In this paper, we propose a novel prompt-based adversarial attack to compromise NLP models and robustness enhancement technique. We first construct malicious prompts for each instance and generate adversarial examples via mask-and-filling under the effect of a malicious purpose. Our attack technique targets the inherent vulnerabilities of NLP models, allowing us to generate samples even without interacting with the victim NLP model, as long as it is based on pre-trained language models (PLMs). Furthermore, we design a prompt-based adversarial training method to improve the robustness of PLMs. As our training method does not actually generate adversarial samples, it can be applied to large-scale training sets efficiently. The experimental results show that our attack method can achieve a high attack success rate with more diverse, fluent and natural adversarial examples. In addition, our robustness enhancement method can significantly improve the robustness of models to resist adversarial attacks. Our work indicates that prompting paradigm has great potential in probing some fundamental flaws of PLMs and fine-tuning them for downstream tasks.",
            "year": 2022,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel prompt-based adversarial attack to compromise NLP models and robustness enhancement technique that can significantly improve the robustness of models to resist adversarial attacks and indicates that prompting paradigm has great potential in probing some fundamental flaws of PLMs and fine-tuning them for downstream tasks."
            },
            "score": 6
        },
        {
            "id": "32316f1d08911b397f52f8644f46f5c129619383",
            "paperId": "32316f1d08911b397f52f8644f46f5c129619383",
            "title": "Defending Against Indirect Prompt Injection Attacks With Spotlighting",
            "abstract": "Large Language Models (LLMs), while powerful, are built and trained to process a single text input. In common applications, multiple inputs can be processed by concatenating them together into a single stream of text. However, the LLM is unable to distinguish which sections of prompt belong to various input sources. Indirect prompt injection attacks take advantage of this vulnerability by embedding adversarial instructions into untrusted data being processed alongside user commands. Often, the LLM will mistake the adversarial instructions as user commands to be followed, creating a security vulnerability in the larger system. We introduce spotlighting, a family of prompt engineering techniques that can be used to improve LLMs' ability to distinguish among multiple sources of input. The key insight is to utilize transformations of an input to provide a reliable and continuous signal of its provenance. We evaluate spotlighting as a defense against indirect prompt injection attacks, and find that it is a robust defense that has minimal detrimental impact to underlying NLP tasks. Using GPT-family models, we find that spotlighting reduces the attack success rate from greater than {50}\\% to below {2}\\% in our experiments with minimal impact on task efficacy.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces spotlighting, a family of prompt engineering techniques that can be used to improve LLMs' ability to distinguish among multiple sources of input, and finds that it is a robust defense that has minimal detrimental impact to underlying NLP tasks."
            },
            "score": 6
        },
        {
            "id": "142e934dd5d6c53f877c30243d436255e3a0dde7",
            "paperId": "142e934dd5d6c53f877c30243d436255e3a0dde7",
            "title": "Visual Adversarial Examples Jailbreak Aligned Large Language Models",
            "abstract": "Warning: this paper contains data, prompts, and model outputs that are offensive in nature.\n\nRecently, there has been a surge of interest in integrating vision into Large Language Models (LLMs), exemplified by Visual Language Models (VLMs) such as Flamingo and GPT-4. This paper sheds light on the security and safety implications of this trend. First, we underscore that the continuous and high-dimensional nature of the visual input makes it a weak link against adversarial attacks, representing an expanded attack surface of vision-integrated LLMs. Second, we highlight that the versatility of LLMs also presents visual attackers with a wider array of achievable adversarial objectives, extending the implications of security failures beyond mere misclassification. As an illustration, we present a case study in which we exploit visual adversarial examples to circumvent the safety guardrail of aligned LLMs with integrated vision. Intriguingly, we discover that a single visual adversarial example can universally jailbreak an aligned LLM, compelling it to heed a wide range of harmful instructions (that it otherwise would not) and generate harmful content that transcends the narrow scope of a `few-shot' derogatory corpus initially employed to optimize the adversarial example. Our study underscores the escalating adversarial risks associated with the pursuit of multimodality. Our findings also connect the long-studied adversarial vulnerabilities of neural networks to the nascent field of AI alignment. The presented attack suggests a fundamental adversarial challenge for AI alignment, especially in light of the emerging trend toward multimodality in frontier foundation models.",
            "year": 2023,
            "citationCount": 44,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that a single visual adversarial example can universally jailbreak an aligned LLM, compelling it to heed a wide range of harmful instructions and generate harmful content that transcends the narrow scope of a `few-shot' derogatory corpus initially employed to optimize the adversarial example."
            },
            "score": 5
        },
        {
            "id": "92b9d8b8c81c4c53ea62000c0924500b2dd11bce",
            "paperId": "92b9d8b8c81c4c53ea62000c0924500b2dd11bce",
            "title": "Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models",
            "abstract": "We introduce new jailbreak attacks on vision language models (VLMs), which use aligned LLMs and are resilient to text-only jailbreak attacks. Specifically, we develop cross-modality attacks on alignment where we pair adversarial images going through the vision encoder with textual prompts to break the alignment of the language model. Our attacks employ a novel compositional strategy that combines an image, adversarially targeted towards toxic embeddings, with generic prompts to accomplish the jailbreak. Thus, the LLM draws the context to answer the generic prompt from the adversarial image. The generation of benign-appearing adversarial images leverages a novel embedding-space-based methodology, operating with no access to the LLM model. Instead, the attacks require access only to the vision encoder and utilize one of our four embedding space targeting strategies. By not requiring access to the LLM, the attacks lower the entry barrier for attackers, particularly when vision encoders such as CLIP are embedded in closed-source LLMs. The attacks achieve a high success rate across different VLMs, highlighting the risk of cross-modality alignment vulnerabilities, and the need for new alignment approaches for multi-modal models.",
            "year": 2023,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Cross-modality attacks on alignment where adversarial images going through the vision encoder with textual prompts to break the alignment of the language model are developed."
            },
            "score": 5
        },
        {
            "id": "6d68b5c1eaf03aba857476a9825acf3e48edd840",
            "paperId": "6d68b5c1eaf03aba857476a9825acf3e48edd840",
            "title": "Hijacking Large Language Models via Adversarial In-Context Learning",
            "abstract": "In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs for specific tasks by utilizing labeled examples as demonstrations in the precondition prompts. Despite its promising performance, ICL suffers from instability with the choice and arrangement of examples. Additionally, crafted adversarial attacks pose a notable threat to the robustness of ICL. However, existing attacks are either easy to detect, rely on external models, or lack specificity towards ICL. To address these issues, this work introduces a novel transferable attack for ICL, aiming to hijack LLMs to generate the targeted response. The proposed LLM hijacking attack leverages a gradient-based prompt search method to learn and append imperceptible adversarial suffixes to the in-context demonstrations. Extensive experimental results on various tasks and datasets demonstrate the effectiveness of our LLM hijacking attack, resulting in a distracted attention towards adversarial tokens, consequently leading to the targeted unwanted outputs.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a novel transferable attack for ICL, aiming to hijack LLMs to generate the targeted response, and leverages a gradient-based prompt search method to learn and append imperceptible adversarial suffixes to the in-context demonstrations."
            },
            "score": 5
        },
        {
            "id": "b6cf4579b59b51d7df416e096ad86c1e6a48b458",
            "paperId": "b6cf4579b59b51d7df416e096ad86c1e6a48b458",
            "title": "Adversarial Prompt Tuning for Vision-Language Models",
            "abstract": "With the rapid advancement of multimodal learning, pre-trained Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable capacities in bridging the gap between visual and language modalities. However, these models remain vulnerable to adversarial attacks, particularly in the image modality, presenting considerable security risks. This paper introduces Adversarial Prompt Tuning (AdvPT), a novel technique to enhance the adversarial robustness of image encoders in VLMs. AdvPT innovatively leverages learnable text prompts and aligns them with adversarial image embeddings, to address the vulnerabilities inherent in VLMs without the need for extensive parameter training or modification of the model architecture. We demonstrate that AdvPT improves resistance against white-box and black-box adversarial attacks and exhibits a synergistic effect when combined with existing image-processing-based defense techniques, further boosting defensive capabilities. Comprehensive experimental analyses provide insights into adversarial prompt tuning, a novel paradigm devoted to improving resistance to adversarial images through textual input modifications, paving the way for future robust multimodal learning research. These findings open up new possibilities for enhancing the security of VLMs. Our code is available at https://github.com/jiamingzhang94/Adversarial-Prompt-Tuning.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Adversarial Prompt Tuning is introduced, a novel technique to enhance the adversarial robustness of image encoders in VLMs and improves resistance against white-box and black-box adversarial attacks and exhibits a synergistic effect when combined with existing image-processing-based defense techniques, further boosting defensive capabilities."
            },
            "score": 5
        },
        {
            "id": "1c91b23d78944f7f237cb512029c2165972ae9d5",
            "paperId": "1c91b23d78944f7f237cb512029c2165972ae9d5",
            "title": "On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex",
            "abstract": "Semantic parsing is a technique aimed at constructing a structured representation of the meaning of a natural-language question. Recent advances in language models trained on code have shown superior performance in generating these representations compared to language models trained solely on natural language text. The existing fine-tuned neural semantic parsers are vulnerable to adversarial attacks on natural-language inputs. While it has been established that the robustness of smaller semantic parsers can be enhanced through adversarial training, this approach is not feasible for large language models in real-world scenarios, as it requires both substantial computational resources and expensive human annotation on in-domain semantic parsing data. This paper presents the first empirical study on the adversarial robustness of a prompt-based semantic parser based on CODEX, a stateof-the-art (SOTA) language model trained on code. Our results demonstrate that the large language model of code is vulnerable to carefully crafted adversarial examples. To overcome this challenge, we propose methods for enhancing robustness without requiring substantial amounts of labelled data or intensive computational resources.",
            "year": 2023,
            "citationCount": 30,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents the first empirical study on the adversarial robustness of a prompt-based semantic parser based on CODEX, a stateof-the-art (SOTA) language model trained on code."
            },
            "score": 5
        },
        {
            "id": "4dda68faa3ea2c888711ce5ced009afcb612e05b",
            "paperId": "4dda68faa3ea2c888711ce5ced009afcb612e05b",
            "title": "Text Processing Like Humans Do: Visually Attacking and Shielding NLP Systems",
            "abstract": "Visual modifications to text are often used to obfuscate offensive comments in social media (e.g., \u201c!d10t\u201d) or as a writing style (\u201c1337\u201d in \u201cleet speak\u201d), among other scenarios. We consider this as a new type of adversarial attack in NLP, a setting to which humans are very robust, as our experiments with both simple and more difficult visual perturbations demonstrate. We investigate the impact of visual adversarial attacks on current NLP systems on character-, word-, and sentence-level tasks, showing that both neural and non-neural models are, in contrast to humans, extremely sensitive to such attacks, suffering performance decreases of up to 82%. We then explore three shielding methods\u2014visual character embeddings, adversarial training, and rule-based recovery\u2014which substantially improve the robustness of the models. However, the shielding methods still fall behind performances achieved in non-attack scenarios, which demonstrates the difficulty of dealing with visual attacks.",
            "year": 2019,
            "citationCount": 137,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates the impact of visual adversarial attacks on current NLP systems on character-, word-, and sentence-level tasks, showing that both neural and non-neural models are, in contrast to humans, extremely sensitive to such attacks, suffering performance decreases of up to 82%."
            },
            "score": 5
        },
        {
            "id": "b2fd6a1b86af52bd6aaa5988a1efc408df6477f3",
            "paperId": "b2fd6a1b86af52bd6aaa5988a1efc408df6477f3",
            "title": "ZDDR: A Zero-Shot Defender for Adversarial Samples Detection and Restoration",
            "abstract": "Natural language processing (NLP) models find extensive applications but face vulnerabilities against adversarial inputs. Traditional defenses lean heavily on supervised detection techniques, which makes them vulnerable to issues arising from training data quality, inherent biases, noise, or adversarial inputs. This study observed common compromises in sentence fluency during aggression. On this basis, the Zero Sample Defender (ZDDR) is introduced for adversarial sample detection and recovery without relying on prior knowledge. ZDDR combines the log probability calculated by the model and the syntactic normative score of a large language model (LLM) to detect adversarial examples. Furthermore, using strategic prompts, ZDDR guides LLM in rephrasing adversarial content, maintaining clarity, structure, and meaning, thereby restoring the sentence from the attack. Benchmarking reveals a 9% improvement in area under receiver operating characteristic curve (AUROC) for adversarial detection over existing techniques. Post-restoration, model classification efficacy surges by 45% compared to the offensive inputs, setting new performance standards against other restoration techniques.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Zero Sample Defender (ZDDR) is introduced for adversarial sample detection and recovery without relying on prior knowledge, and reveals a 9% improvement in area under receiver operating characteristic curve (AUROC) for adversarial detection over existing techniques."
            },
            "score": 5
        },
        {
            "id": "05d2ced6a4fb7efb8d527a228ad792526a202235",
            "paperId": "05d2ced6a4fb7efb8d527a228ad792526a202235",
            "title": "How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities",
            "abstract": "The rapid progress in open-source Large Language Models (LLMs) is significantly driving AI development forward. However, there is still a limited understanding of their trustworthiness. Deploying these models at scale without sufficient trustworthiness can pose significant risks, highlighting the need to uncover these issues promptly. In this work, we conduct an adversarial assessment of open-source LLMs on trustworthiness, scrutinizing them across eight different aspects including toxicity, stereotypes, ethics, hallucination, fairness, sycophancy, privacy, and robustness against adversarial demonstrations. We propose advCoU, an extended Chain of Utterances-based (CoU) prompting strategy by incorporating carefully crafted malicious demonstrations for trustworthiness attack. Our extensive experiments encompass recent and representative series of open-source LLMs, including Vicuna, MPT, Falcon, Mistral, and Llama 2. The empirical outcomes underscore the efficacy of our attack strategy across diverse aspects. More interestingly, our result analysis reveals that models with superior performance in general NLP tasks do not always have greater trustworthiness; in fact, larger models can be more vulnerable to attacks. Additionally, models that have undergone instruction tuning, focusing on instruction following, tend to be more susceptible, although fine-tuning LLMs for safety alignment proves effective in mitigating adversarial trustworthiness attacks.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An adversarial assessment of open-source LLMs on trustworthiness is conducted, scrutinizing them across eight different aspects including toxicity, stereotypes, ethics, hallucination, fairness, sycophancy, privacy, and robustness against adversarial demonstrations, revealing that models with superior performance in general NLP tasks do not always have greater trustworthiness."
            },
            "score": 5
        },
        {
            "id": "3bb87d605856411c6f002d480fc29d355c3ba245",
            "paperId": "3bb87d605856411c6f002d480fc29d355c3ba245",
            "title": "An Image Is Worth 1000 Lies: Adversarial Transferability across Prompts on Vision-Language Models",
            "abstract": "Different from traditional task-specific vision models, recent large VLMs can readily adapt to different vision tasks by simply using different textual instructions, i.e., prompts. However, a well-known concern about traditional task-specific vision models is that they can be misled by imperceptible adversarial perturbations. Furthermore, the concern is exacerbated by the phenomenon that the same adversarial perturbations can fool different task-specific models. Given that VLMs rely on prompts to adapt to different tasks, an intriguing question emerges: Can a single adversarial image mislead all predictions of VLMs when a thousand different prompts are given? This question essentially introduces a novel perspective on adversarial transferability: cross-prompt adversarial transferability. In this work, we propose the Cross-Prompt Attack (CroPA). This proposed method updates the visual adversarial perturbation with learnable prompts, which are designed to counteract the misleading effects of the adversarial image. By doing this, CroPA significantly improves the transferability of adversarial examples across prompts. Extensive experiments are conducted to verify the strong cross-prompt adversarial transferability of CroPA with prevalent VLMs including Flamingo, BLIP-2, and InstructBLIP in various different tasks. Our source code is available at \\url{https://github.com/Haochen-Luo/CroPA}.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes the Cross-Prompt Attack (CroPA), a method that updates the visual adversarial perturbation with learnable prompts, which are designed to counteract the misleading effects of the adversarial image."
            },
            "score": 4
        },
        {
            "id": "ac5b4df0e398ca48388330ac5c795b6fe708793c",
            "paperId": "ac5b4df0e398ca48388330ac5c795b6fe708793c",
            "title": "Misusing Tools in Large Language Models With Visual Adversarial Examples",
            "abstract": "Large Language Models (LLMs) are being enhanced with the ability to use tools and to process multiple modalities. These new capabilities bring new benefits and also new security risks. In this work, we show that an attacker can use visual adversarial examples to cause attacker-desired tool usage. For example, the attacker could cause a victim LLM to delete calendar events, leak private conversations and book hotels. Different from prior work, our attacks can affect the confidentiality and integrity of user resources connected to the LLM while being stealthy and generalizable to multiple input prompts. We construct these attacks using gradient-based adversarial training and characterize performance along multiple dimensions. We find that our adversarial images can manipulate the LLM to invoke tools following real-world syntax almost always (~98%) while maintaining high similarity to clean images (~0.9 SSIM). Furthermore, using human scoring and automated metrics, we find that the attacks do not noticeably affect the conversation (and its semantics) between the user and the LLM.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work shows that an attacker can use visual adversarial examples to cause attacker-desired tool usage to cause a victim LLM to delete calendar events, leak private conversations and book hotels."
            },
            "score": 4
        },
        {
            "id": "ca114b547c06eeb9911b685736679ca4dddf395c",
            "paperId": "ca114b547c06eeb9911b685736679ca4dddf395c",
            "title": "Text-CRS: A Generalized Certified Robustness Framework against Textual Adversarial Attacks",
            "abstract": "The language models, especially the basic text classification models, have been shown to be susceptible to textual adversarial attacks such as synonym substitution and word insertion attacks. To defend against such attacks, a growing body of research has been devoted to improving the model robustness. However, providing provable robustness guarantees instead of empirical robustness is still widely unexplored. In this paper, we propose Text-CRS, a generalized certified robustness framework for natural language processing (NLP) based on randomized smoothing. To our best knowledge, existing certified schemes for NLP can only certify the robustness against $\\ell_0$ perturbations in synonym substitution attacks. Representing each word-level adversarial operation (i.e., synonym substitution, word reordering, insertion, and deletion) as a combination of permutation and embedding transformation, we propose novel smoothing theorems to derive robustness bounds in both permutation and embedding space against such adversarial operations. To further improve certified accuracy and radius, we consider the numerical relationships between discrete words and select proper noise distributions for the randomized smoothing. Finally, we conduct substantial experiments on multiple language models and datasets. Text-CRS can address all four different word-level adversarial operations and achieve a significant accuracy improvement. We also provide the first benchmark on certified accuracy and radius of four word-level operations, besides outperforming the state-of-the-art certification against synonym substitution attacks.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes Text-CRS, a generalized certified robustness framework for natural language processing (NLP) based on randomized smoothing and provides the first benchmark on certified accuracy and radius of four word-level operations, besides outperforming the state-of-the-art certification against synonym substitution attacks."
            },
            "score": 4
        },
        {
            "id": "175b32c07e56f881479be4c5a74bfa3c731cc454",
            "paperId": "175b32c07e56f881479be4c5a74bfa3c731cc454",
            "title": "ROSE: Robust Selective Fine-tuning for Pre-trained Language Models",
            "abstract": "Even though the large-scale language models have achieved excellent performances, they suffer from various adversarial attacks.A large body of defense methods has been proposed. However, they are still limited due to redundant attack search spaces and the inability to defend against various types of attacks.In this work, we present a novel fine-tuning approach called RObust SEletive fine-tuning (ROSE) to address this issue.ROSE conducts selective updates when adapting pre-trained models to downstream tasks, filtering out invaluable and unrobust updates of parameters.Specifically, we propose two strategies: the first-order and second-order ROSE for selecting target robust parameters.The experimental results show that ROSE achieves significant improvements in adversarial robustness on various downstream NLP tasks, and the ensemble method even surpasses both variants above.Furthermore, ROSE can be easily incorporated into existing fine-tuning methods to improve their adversarial robustness further.The empirical analysis confirms that ROSE eliminates unrobust spurious updates during fine-tuning, leading to solutions corresponding to flatter and wider optima than the conventional method.Code is available at https://github.com/jiangllan/ROSE.",
            "year": 2022,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes two strategies: the first-order and second-order ROSE for selecting target robust parameters and shows that ROSE achieves significant improvements in adversarial robustness on various downstream NLP tasks, and the ensemble method even surpasses both variants above."
            },
            "score": 4
        },
        {
            "id": "a76c98c6814ce8de07707b81c18520af508b7184",
            "paperId": "a76c98c6814ce8de07707b81c18520af508b7184",
            "title": "BERT-Defense: A Probabilistic Model Based on BERT to Combat Cognitively Inspired Orthographic Adversarial Attacks",
            "abstract": "Adversarial attacks expose important blind spots of deep learning systems. While word- and sentence-level attack scenarios mostly deal with finding semantic paraphrases of the input that fool NLP models, character-level attacks typically insert typos into the input stream. It is commonly thought that these are easier to defend via spelling correction modules. In this work, we show that both a standard spellchecker and the approach of Pruthi et al. (2019), which trains to defend against insertions, deletions and swaps, perform poorly on the character-level benchmark recently proposed in Eger and Benz (2020) which includes more challenging attacks such as visual and phonetic perturbations and missing word segmentations. In contrast, we show that an untrained iterative approach which combines context-independent character-level information with context-dependent information from BERT's masked language modeling can perform on par with human crowd-workers from Amazon Mechanical Turk (AMT) supervised via 3-shot learning.",
            "year": 2021,
            "citationCount": 20,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work shows that an untrained iterative approach which combines context-independent character-level information with context-dependent information from BERT's masked language modeling can perform on par with human crowd-workers from Amazon Mechanical Turk supervised via 3-shot learning."
            },
            "score": 4
        },
        {
            "id": "dd54b718c2116d1da95bb931862e536fa4dc9875",
            "paperId": "dd54b718c2116d1da95bb931862e536fa4dc9875",
            "title": "Distinguishing Non-natural from Natural Adversarial Samples for More Robust Pre-trained Language Model",
            "abstract": "Recently, the problem of robustness of pre-trained language models (PrLMs) has received increasing research interest. Latest studies on adversarial attacks achieve high attack success rates against PrLMs, claiming that PrLMs are not robust. However, we find that the adversarial samples that PrLMs fail are mostly non-natural and do not appear in reality. We question the validity of the current evaluation of robustness of PrLMs based on these non-natural adversarial samples and propose an anomaly detector to evaluate the robustness of PrLMs with more natural adversarial samples. We also investigate two applications of the anomaly detector: (1) In data augmentation, we employ the anomaly detector to force generating augmented data that are distinguished as non-natural, which brings larger gains to the accuracy of PrLMs. (2) We apply the anomaly detector to a defense framework to enhance the robustness of PrLMs. It can be used to defend all types of attacks and achieves higher accuracy on both adversarial samples and compliant samples than other defense frameworks.",
            "year": 2022,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that the adversarial samples that PrLMs fail are mostly non-natural and do not appear in reality, which raises questions of the validity of the current evaluation of robustness of PrL Ms based on these non- natural adversarialamples and proposes an anomaly detector to evaluate the robustness."
            },
            "score": 4
        },
        {
            "id": "8b149ce8be46daa64c2c5c66981c2496c69c04e7",
            "paperId": "8b149ce8be46daa64c2c5c66981c2496c69c04e7",
            "title": "MPAT: Building Robust Deep Neural Networks against Textual Adversarial Attacks",
            "abstract": "Deep neural networks have been proven to be vulnerable to adversarial examples and various methods have been proposed to defend against adversarial attacks for natural language processing tasks. However, previous defense methods have limitations in maintaining effective defense while ensuring the performance of the original task. In this paper, we propose a malicious perturbation based adversarial training method (MPAT) for building robust deep neural networks against textual adversarial attacks. Specifically, we construct a multi-level malicious example generation strategy to generate adversarial examples with malicious perturbations, which are used instead of original inputs for model training. Additionally, we employ a novel training objective function to ensure achieving the defense goal without compromising the performance on the original task. We conduct comprehensive experiments to evaluate our defense method by attacking five victim models on three benchmark datasets. The result demonstrates that our method is more effective against malicious adversarial attacks compared with previous defense methods while maintaining or further improving the performance on the original task.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The result demonstrates that the proposed malicious perturbation based adversarial training method (MPAT) is more effective against malicious adversarial attacks compared with previous defense methods while maintaining or further improving the performance on the original task."
            },
            "score": 4
        },
        {
            "id": "982210d4d759efab60d2e50b9fac72dc614ee6d6",
            "paperId": "982210d4d759efab60d2e50b9fac72dc614ee6d6",
            "title": "SemRoDe: Macro Adversarial Training to Learn Representations That are Robust to Word-Level Attacks",
            "abstract": "Language models (LMs) are indispensable tools for natural language processing tasks, but their vulnerability to adversarial attacks remains a concern. While current research has explored adversarial training techniques, their improvements to defend against word-level attacks have been limited. In this work, we propose a novel approach called Semantic Robust Defence (SemRoDe), a Macro Adversarial Training strategy to enhance the robustness of LMs. Drawing inspiration from recent studies in the image domain, we investigate and later confirm that in a discrete data setting such as language, adversarial samples generated via word substitutions do indeed belong to an adversarial domain exhibiting a high Wasserstein distance from the base domain. Our method learns a robust representation that bridges these two domains. We hypothesize that if samples were not projected into an adversarial domain, but instead to a domain with minimal shift, it would improve attack robustness. We align the domains by incorporating a new distance-based objective. With this, our model is able to learn more generalized representations by aligning the model's high-level output features and therefore better handling unseen adversarial samples. This method can be generalized across word embeddings, even when they share minimal overlap at both vocabulary and word-substitution levels. To evaluate the effectiveness of our approach, we conduct experiments on BERT and RoBERTa models on three datasets. The results demonstrate promising state-of-the-art robustness.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel approach called Semantic Robust Defence (SemRoDe), a Macro Adversarial Training strategy to enhance the robustness of LMs, and hypothesizes that if samples were not projected into an adversarial domain, but instead to a domain with minimal shift, it would improve attack robustness."
            },
            "score": 4
        },
        {
            "id": "b6ec0b7dc526b501c056cc7ec8cfa5329b942a29",
            "paperId": "b6ec0b7dc526b501c056cc7ec8cfa5329b942a29",
            "title": "Round Trip Translation Defence against Large Language Model Jailbreaking Attacks",
            "abstract": "Large language models (LLMs) are susceptible to social-engineered attacks that are human-interpretable but require a high level of comprehension for LLMs to counteract. Existing defensive measures can only mitigate less than half of these attacks at most. To address this issue, we propose the Round Trip Translation (RTT) method, the first algorithm specifically designed to defend against social-engineered attacks on LLMs. RTT paraphrases the adversarial prompt and generalizes the idea conveyed, making it easier for LLMs to detect induced harmful behavior. This method is versatile, lightweight, and transferrable to different LLMs. Our defense successfully mitigated over 70% of Prompt Automatic Iterative Refinement (PAIR) attacks, which is currently the most effective defense to the best of our knowledge. We are also the first to attempt mitigating the MathsAttack and reduced its attack success rate by almost 40%. Our code is publicly available at https://github.com/Cancanxxx/Round_Trip_Translation_Defence",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Round Trip Translation (RTT) method is proposed, the first algorithm specifically designed to defend against social-engineered attacks on LLMs, and successfully mitigated over 70% of Prompt Automatic Iterative Refinement (PAIR) attacks."
            },
            "score": 4
        },
        {
            "id": "649d5e240adaa38e74299ccab3fc3215137170f9",
            "paperId": "649d5e240adaa38e74299ccab3fc3215137170f9",
            "title": "TextDefense: Adversarial Text Detection based on Word Importance Entropy",
            "abstract": "Currently, natural language processing (NLP) models are wildly used in various scenarios. However, NLP models, like all deep models, are vulnerable to adversarially generated text. Numerous works have been working on mitigating the vulnerability from adversarial attacks. Nevertheless, there is no comprehensive defense in existing works where each work targets a specific attack category or suffers from the limitation of computation overhead, irresistible to adaptive attack, etc. In this paper, we exhaustively investigate the adversarial attack algorithms in NLP, and our empirical studies have discovered that the attack algorithms mainly disrupt the importance distribution of words in a text. A well-trained model can distinguish subtle importance distribution differences between clean and adversarial texts. Based on this intuition, we propose TextDefense, a new adversarial example detection framework that utilizes the target model's capability to defend against adversarial attacks while requiring no prior knowledge. TextDefense differs from previous approaches, where it utilizes the target model for detection and thus is attack type agnostic. Our extensive experiments show that TextDefense can be applied to different architectures, datasets, and attack methods and outperforms existing methods. We also discover that the leading factor influencing the performance of TextDefense is the target model's generalizability. By analyzing the property of the target model and the property of the adversarial example, we provide our insights into the adversarial attacks in NLP and the principles of our defense method.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper exhaustively investigates the adversarial attack algorithms in NLP and proposes TextDefense, a new adversarial example detection framework that utilizes the target model's capability to defend against adversarial attacks while requiring no prior knowledge."
            },
            "score": 4
        },
        {
            "id": "efabdd27929796b712cb1b3a3051ea5358dc1200",
            "paperId": "efabdd27929796b712cb1b3a3051ea5358dc1200",
            "title": "A Prompt Array Keeps the Bias Away: Debiasing Vision-Language Models with Adversarial Learning",
            "abstract": "Vision-language models can encode societal biases and stereotypes, but there are challenges to measuring and mitigating these multimodal harms due to lacking measurement robustness and feature degradation. To address these challenges, we investigate bias measures and apply ranking metrics for image-text representations. We then investigate debiasing methods and show that prepending learned embeddings to text queries that are jointly trained with adversarial debiasing and a contrastive loss, reduces various bias measures with minimal degradation to the image-text representation.",
            "year": 2022,
            "citationCount": 53,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Debiasing methods are investigated and it is shown that prepending learned embeddings to text queries that are jointly trained with adversarial debiasing and a contrastive loss, reduces various bias measures with minimal degradation to the image-text representation."
            },
            "score": 4
        },
        {
            "id": "de11dd9386518012fec7d6f564755b6e6cdbd241",
            "paperId": "de11dd9386518012fec7d6f564755b6e6cdbd241",
            "title": "Sensitivity and Robustness of Large Language Models to Prompt Template in Japanese Text Classification Tasks",
            "abstract": "Prompt engineering relevance research has seen a notable surge in recent years, primarily driven by advancements in pre-trained language models and large language models. However, a critical issue has been identified within this domain: the inadequate of sensitivity and robustness of these models towards Prompt Templates, particularly in lesser-studied languages such as Japanese. This paper explores this issue through a comprehensive evaluation of several representative Large Language Models (LLMs) and a widely-utilized pre-trained model(PLM). These models are scrutinized using a benchmark dataset in Japanese, with the aim to assess and analyze the performance of the current multilingual models in this context. Our experimental results reveal startling discrepancies. A simple modification in the sentence structure of the Prompt Template led to a drastic drop in the accuracy of GPT-4 from 49.21 to 25.44. This observation underscores the fact that even the highly performance GPT-4 model encounters significant stability issues when dealing with diverse Japanese prompt templates, rendering the consistency of the model's output results questionable. In light of these findings, we conclude by proposing potential research trajectories to further enhance the development and performance of Large Language Models in their current stage.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A comprehensive evaluation of several representative Large Language Models (LLMs) and a widely-utilized pre-trained model(PLM) is scrutinized using a benchmark dataset in Japanese, with the aim to assess and analyze the performance of the current multilingual models in this context."
            },
            "score": 4
        },
        {
            "id": "ff77cc047f5e7a2fdf8563d05e1ba4b383e859a4",
            "paperId": "ff77cc047f5e7a2fdf8563d05e1ba4b383e859a4",
            "title": "Sensitivity and Robustness of Large Language Models to Prompt in Japanese",
            "abstract": "Prompt Engineering has gained signi\ufb01cant relevance in recent years, fueled by advancements in pre-trained and large language models. However, a critical issue has been iden-ti\ufb01ed within this domain: the lack of sensitivity and robustness of these models towards Prompt Templates, particularly in lesser-studied languages such as Japanese. This paper explores this issue through a comprehensive evaluation of several representative Large Language Models (LLMs) and a widely-utilized pre-trained model(PLM), T5. These models are scrutinized using a benchmark dataset in Japanese, with the aim to assess and analyze the performance of the current multilingual models in this context. Our experimental results reveal startling discrepancies. A simple modi\ufb01cation in the sentence structure of the Prompt Template led to a drastic drop in the accuracy of GPT-4 from 49.21 to 25.44. This observation underscores the fact that even the highly performance GPT-4 model encounters signi\ufb01cant stability issues when dealing with diverse Japanese prompt templates, rendering the consistency of the model\u2019s output results questionable. In light of these \ufb01ndings, we conclude by proposing potential research trajectories to further enhance the development and performance of Large Language Models in their current stage.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A comprehensive evaluation of several representative Large Language Models and a widely-utilized pre-trained model, T5, to assess and analyze the performance of the current multilingual models in this context and proposes potential research trajectories to further enhance the development and performance of Large Language Models in their current stage."
            },
            "score": 4
        },
        {
            "id": "2c72ab10e7a5f2fd32e6f85b20c77bf64e6e220d",
            "paperId": "2c72ab10e7a5f2fd32e6f85b20c77bf64e6e220d",
            "title": "A prompt-based approach to adversarial example generation and robustness enhancement",
            "abstract": null,
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel robust training approach based on prompt paradigm which incorporates prompt texts as the alternatives to adversarial examples and enhances robustness under a lightweight minimax-style optimization framework is proposed."
            },
            "score": 4
        },
        {
            "id": "9a0f53a0ff25d1a5fb016aaa22185c4d6b5a2ac8",
            "paperId": "9a0f53a0ff25d1a5fb016aaa22185c4d6b5a2ac8",
            "title": "LinkPrompt: Natural and Universal Adversarial Attacks on Prompt-based Language Models",
            "abstract": "Prompt-based learning is a new language model training paradigm that adapts the Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes the performance benchmarks across various natural language processing (NLP) tasks. Instead of using a fixed prompt template to fine-tune the model, some research demonstrates the effectiveness of searching for the prompt via optimization. Such prompt optimization process of prompt-based learning on PLMs also gives insight into generating adversarial prompts to mislead the model, raising concerns about the adversarial vulnerability of this paradigm. Recent studies have shown that universal adversarial triggers (UATs) can be generated to alter not only the predictions of the target PLMs but also the prediction of corresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based learning paradigm. However, UATs found in previous works are often unreadable tokens or characters and can be easily distinguished from natural texts with adaptive defenses. In this work, we consider the naturalness of the UATs and develop $\\textit{LinkPrompt}$, an adversarial attack algorithm to generate UATs by a gradient-based beam search algorithm that not only effectively attacks the target PLMs and PFMs but also maintains the naturalness among the trigger tokens. Extensive results demonstrate the effectiveness of $\\textit{LinkPrompt}$, as well as the transferability of UATs generated by $\\textit{LinkPrompt}$ to open-sourced Large Language Model (LLM) Llama2 and API-accessed LLM GPT-3.5-turbo. The resource is available at $\\href{https://github.com/SavannahXu79/LinkPrompt}{https://github.com/SavannahXu79/LinkPrompt}$.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An adversarial attack algorithm to generate UATs by a gradient-based beam search algorithm that not only effectively attacks the target PLMs and PFMs but also maintains the naturalness among the trigger tokens is developed."
            },
            "score": 4
        },
        {
            "id": "051493fa4f87c54e49aeed2e289a05265d739c0d",
            "paperId": "051493fa4f87c54e49aeed2e289a05265d739c0d",
            "title": "ChatGPT: A Threat to Spam Filtering Systems",
            "abstract": "ChatGPT gathered the attention of millions of users shortly after its release, leading to the popularization of generative AI technologies. This research aims to emphasize one of the potential vulnerabilities associated with commonly used generative AI models. Even though it follows strict ethical and security policies, proper prompt engineering enables malicious misuse of ChatG PT such as spam email generation. In this paper, we present various scenarios of malicious prompt engineering to encourage the chatbot for spam email generation and rewriting the existing email. Also, we present the adversarial prompt engineering examples intended to evade the detection by spam filters by means of rewriting the given email while circumventing common spam characteristics. We experimentally evaluate the practical feasibility of prompt engineering on ChatGPT by assessing the performance of six common ML-based spam filters with emails modified by ChatG PT. From the experimental results, we show that adversarial prompt engineering decreases the performance of common ML-based spam filters, while NLP-based filter is robust to such modification. We also demonstrate that including ChatG PT rewritten emails in the training set leads to more robust ML-based spam filters, while the use of available AI-text detectors does not guarantee high detection rates of emails modified by the chatbot.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that including ChatG PT rewritten emails in the training set leads to more robust ML-based spam filters, while the use of available AI-text detectors does not guarantee high detection rates of emails modified by the chatbot."
            },
            "score": 4
        },
        {
            "id": "f4cd7b7ffb0ab5ccef7cca23eeb436a933f7c776",
            "paperId": "f4cd7b7ffb0ab5ccef7cca23eeb436a933f7c776",
            "title": "Frontier Language Models are not Robust to Adversarial Arithmetic, or \"What do I need to say so you agree 2+2=5?",
            "abstract": "We introduce and study the problem of adversarial arithmetic, which provides a simple yet challenging testbed for language model alignment. This problem is comprised of arithmetic questions posed in natural language, with an arbitrary adversarial string inserted before the question is complete. Even in the simple setting of 1-digit addition problems, it is easy to find adversarial prompts that make all tested models (including PaLM2, GPT4, Claude2) misbehave, and even to steer models to a particular wrong answer. We additionally provide a simple algorithm for finding successful attacks by querying those same models, which we name\"prompt inversion rejection sampling\"(PIRS). We finally show that models can be partially hardened against these attacks via reinforcement learning and via agentic constitutional loops. However, we were not able to make a language model fully robust against adversarial arithmetic attacks.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that models can be partially hardened against these attacks via reinforcement learning and via agentic constitutional loops, however, they were not able to make a language model fully robust against adversarial arithmetic attacks."
            },
            "score": 3
        },
        {
            "id": "1c6015ffff034b9c304477bb31e55ca5a55f3a99",
            "paperId": "1c6015ffff034b9c304477bb31e55ca5a55f3a99",
            "title": "Adversarial Transformer Language Models for Contextual Commonsense Inference",
            "abstract": "Contextualized or discourse aware commonsense inference is the task of generating coherent commonsense assertions (i.e., facts) from a given story, and a particular sentence from that story. Some problems with the task are: lack of controllability for topics of the inferred facts; lack of commonsense knowledge during training; and, possibly, hallucinated or false facts. In this work, we utilize a transformer model for this task and develop techniques to address the aforementioned problems in the task. We control the inference by introducing a new technique we call\"hinting\". Hinting is a kind of language model prompting, that utilizes both hard prompts (specific words) and soft prompts (virtual learnable templates). This serves as a control signal to advise the language model\"what to talk about\". Next, we establish a methodology for performing joint inference with multiple commonsense knowledge bases. Joint inference of commonsense requires care, because it is imprecise and the level of generality is more flexible. You want to be sure that the results\"still make sense\"for the context. To this end, we align the textual version of assertions from three knowledge graphs (ConceptNet, ATOMIC2020, and GLUCOSE) with a story and a target sentence. This combination allows us to train a single model to perform joint inference with multiple knowledge graphs. We show experimental results for the three knowledge graphs on joint inference. Our final contribution is exploring a GAN architecture that generates the contextualized commonsense assertions and scores them as to their plausibility through a discriminator. The result is an integrated system for contextual commonsense inference in stories, that can controllably generate plausible commonsense assertions, and takes advantage of joint inference between multiple commonsense knowledge bases.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The result is an integrated system for contextual commonsense inference in stories, that can controllably generate plausible commonsense assertions, and takes advantage of joint inference between multiple commonsense knowledge bases."
            },
            "score": 3
        },
        {
            "id": "18a8b97d75a87e8fef07542d8875d4a62b553744",
            "paperId": "18a8b97d75a87e8fef07542d8875d4a62b553744",
            "title": "Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts",
            "abstract": "Existing work on jailbreak Multimodal Large Language Models (MLLMs) has focused primarily on adversarial examples in model inputs, with less attention to vulnerabilities, especially in model API. To fill the research gap, we carry out the following work: 1) We discover a system prompt leakage vulnerability in GPT-4V. Through carefully designed dialogue, we successfully extract the internal system prompts of GPT-4V. This finding indicates potential exploitable security risks in MLLMs; 2) Based on the acquired system prompts, we propose a novel MLLM jailbreaking attack method termed SASP (Self-Adversarial Attack via System Prompt). By employing GPT-4 as a red teaming tool against itself, we aim to search for potential jailbreak prompts leveraging stolen system prompts. Furthermore, in pursuit of better performance, we also add human modification based on GPT-4's analysis, which further improves the attack success rate to 98.7\\%; 3) We evaluated the effect of modifying system prompts to defend against jailbreaking attacks. Results show that appropriately designed system prompts can significantly reduce jailbreak success rates. Overall, our work provides new insights into enhancing MLLM security, demonstrating the important role of system prompts in jailbreaking. This finding could be leveraged to greatly facilitate jailbreak success rates while also holding the potential for defending against jailbreaks.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work discovers a system prompt leakage vulnerability in GPT-4V and proposes a novel MLLM jailbreaking attack method termed SASP (Self-Adversarial Attack via System Prompt), demonstrating the important role of system prompts in jailbreaking."
            },
            "score": 3
        },
        {
            "id": "0b65449b5f80cf884de205b815d6ac035f625be9",
            "paperId": "0b65449b5f80cf884de205b815d6ac035f625be9",
            "title": "Using Random Perturbations to Mitigate Adversarial Attacks on NLP Models",
            "abstract": "Deep learning models have excelled in solving many problems in Natural Language Processing, but are susceptible to extensive vulnerabilities. We offer a solution to this vulnerability by using random perturbations such as spelling correction, synonym substitution, or dropping the word. These perturbations are applied to random words in random sentences to defend NLP models against adversarial attacks. Our defense methods are successful in returning attacked models to their original accuracy within statistical significance.",
            "year": 2022,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Random perturbations are applied to random words in random sentences to defend NLP models against adversarial attacks and are successful in returning attacked models to their original accuracy within statistical significance."
            },
            "score": 3
        },
        {
            "id": "c01097abc8b895f218d2feb59914435964de1129",
            "paperId": "c01097abc8b895f218d2feb59914435964de1129",
            "title": "A Robust Method to Protect Text Classification Models against Adversarial Attacks",
            "abstract": "Text classification is one of the main tasks in natural language processing. Recently, adversarial attacks have shown a substantial negative impact on neural network-based text classification models. There are few defenses to strengthen model predictions against adversarial attacks; popular among them are adversarial training and spelling correction. While adversarial training adds different synonyms to the training data, spelling correction methods defend against character variations at the word level. The diversity and sparseness of adversarial perturbations of different attack methods challenge these approaches. This paper proposes an approach to correct adversarial samples for text classification tasks. Our proposed approach combines grammar correction and spelling correction methods. In this, we use Gramformer for grammar correction and Textblob for spelling correction. These approaches are generic and can be applied to any text classification model without any retraining. We evaluated our approach with two state-of-the-art attacks, DeepWordBug and TextBugger, on three open-source datasets IMDB, CoLA, and AGNews. The experimental results show that our approach can effectively counter adversarial attacks on text classification models while maintaining classification performance on original clean data.",
            "year": 2022,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results show that the proposed approach to correct adversarial samples for text classification tasks can effectively counter adversarial attacks on text classification models while maintaining classification performance on original clean data."
            },
            "score": 3
        },
        {
            "id": "024d8d62f4a2057bfa019f83366624af281bb5e4",
            "paperId": "024d8d62f4a2057bfa019f83366624af281bb5e4",
            "title": "Don't be a Fool: Pooling Strategies in Offensive Language Detection from User-Intended Adversarial Attacks",
            "abstract": "Offensive language detection is an important task for filtering out abusive expressions and improving online user experiences. However, malicious users often attempt to avoid filtering systems through the involvement of textual noises. In this paper, we propose these evasions as user-intended adversarial attacks that insert special symbols or leverage the distinctive features of the Korean language. Furthermore, we introduce simple yet effective pooling strategies in a layer-wise manner to defend against the proposed attacks, focusing on the preceding layers not just the last layer to capture both offensiveness and token embeddings. We demonstrate that these pooling strategies are more robust to performance degradation even when the attack rate is increased, without directly training of such patterns. Notably, we found that models pre-trained on clean texts could achieve a comparable performance in detecting attacked offensive language, to models pre-trained on noisy texts by employing these pooling strategies.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes these evasions as user-intended adversarial attacks that insert special symbols or leverage the distinctive features of the Korean language to defend against the proposed attacks, and introduces simple yet effective pooling strategies to defend against the proposed attacks."
            },
            "score": 3
        },
        {
            "id": "ad38da3afffae1ab1d73b13f5a9a80932753013a",
            "paperId": "ad38da3afffae1ab1d73b13f5a9a80932753013a",
            "title": "Language Guided Adversarial Purification",
            "abstract": "Adversarial purification using generative models demonstrates strong adversarial defense performance. These methods are classifier and attack-agnostic, making them versatile but often computationally intensive. Recent strides in diffusion and score networks have improved image generation and, by extension, adversarial purification. Another highly efficient class of adversarial defense methods known as adversarial training requires specific knowledge of attack vectors, forcing them to be trained extensively on adversarial examples. To overcome these limitations, we introduce a new framework, namely Language Guided Adversarial Purification (LGAP), utilizing pre-trained diffusion models and caption generators to defend against adversarial attacks. Given an input image, our method first generates a caption, which is then used to guide the adversarial purification process through a diffusion network. Our approach has been evaluated against strong adversarial attacks, proving its effectiveness in enhancing adversarial robustness. Our results indicate that LGAP outperforms most existing adversarial defense techniques without requiring specialized network training. This underscores the generalizability of models trained on large datasets, highlighting a promising direction for further research.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a new framework, namely Language Guided Adversarial Purification (LGAP), utilizing pre-trained diffusion models and caption generators to defend against adversarial attacks, indicating that LGAP outperforms most existing adversarial defense techniques without requiring specialized network training."
            },
            "score": 3
        },
        {
            "id": "fd84802d7299824c40439e86c2234adc7db6845b",
            "paperId": "fd84802d7299824c40439e86c2234adc7db6845b",
            "title": "Adversarial Attacks and Defense for Conversation Entailment Task",
            "abstract": "Large language models (LLMs) that are proved to be very powerful on different NLP tasks. However, there are still many ways to attack the model with very low costs. How to defend the model becomes an important problem. In our work, we treat adversarial attack results as a new (unseen) domain of the model, and we frame the defending problem into how to improve the robustness of the model on the new domain. We focus on the task of conversation entailment, where multi-turn natural language dialogues are the premise, and the transformer model is fine-tuned to predict whether a given hypothesis about the given dialogue is true or false. The adversary would attack the hypothesis to fool the model to make the wrong predictions. We apply synonym-swapping as the attack method. To show the robustness of the model, we implement some fine-tuning strategies and propose the embedding perturbation loss as a method to improve the robustness of the model. Finally, we show the importance of our work by discussing the adversarial attacks in NLP in the real world.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work treats adversarial attack results as a new (unseen) domain of the model, and frames the defending problem into how to improve the robustness of the model on the new domain."
            },
            "score": 3
        },
        {
            "id": "7fa35ebeb6e7e4ed8706a9f41e9f53cfa4af2c77",
            "paperId": "7fa35ebeb6e7e4ed8706a9f41e9f53cfa4af2c77",
            "title": "Natural language adversarial defense through synonym encoding",
            "abstract": "In the area of natural language processing, deep learning models are recently known to be vulnerable to various types of adversarial perturbations, but relatively few works are done on the defense side. Especially, there exists few effective defense method against the successful synonym substitution based attacks that preserve the syntactic structure and semantic information of the original text while fooling the deep learning models. We contribute in this direction and propose a novel adversarial defense method called Synonym Encoding Method (SEM). Specifically, SEM inserts an encoder before the input layer of the target model to map each cluster of synonyms to a unique encoding and trains the model to eliminate possible adversarial perturbations without modifying the network architecture or adding extra data. Extensive experiments demonstrate that SEM can effectively defend the current synonym substitution based attacks and block the transferability of adversarial examples. SEM is also easy and efficient to scale to large models and big datasets.",
            "year": 2019,
            "citationCount": 48,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel adversarial defense method called Synonym Encoding Method (SEM), which inserts an encoder before the input layer of the target model to map each cluster of synonyms to a unique encoding and trains the model to eliminate possible adversarial perturbations without modifying the network architecture or adding extra data."
            },
            "score": 3
        },
        {
            "id": "de6fd09ed7783b95af7e5f4088a70d5b0244f5aa",
            "paperId": "de6fd09ed7783b95af7e5f4088a70d5b0244f5aa",
            "title": "Improving large language models for clinical named entity recognition via prompt engineering.",
            "abstract": "IMPORTANCE\nThe study highlights the potential of large language models, specifically GPT-3.5 and GPT-4, in processing complex clinical data and extracting meaningful information with minimal training data. By developing and refining prompt-based strategies, we can significantly enhance the models' performance, making them viable tools for clinical NER tasks and possibly reducing the reliance on extensive annotated datasets.\n\n\nOBJECTIVES\nThis study quantifies the capabilities of GPT-3.5 and GPT-4 for clinical named entity recognition (NER) tasks and proposes task-specific prompts to improve their performance.\n\n\nMATERIALS AND METHODS\nWe evaluated these models on 2 clinical NER tasks: (1) to extract medical problems, treatments, and tests from clinical notes in the MTSamples corpus, following the 2010 i2b2 concept extraction shared task, and (2) to identify nervous system disorder-related adverse events from safety reports in the vaccine adverse event reporting system (VAERS). To improve the GPT models' performance, we developed a clinical task-specific prompt framework that includes (1) baseline prompts with task description and format specification, (2) annotation guideline-based prompts, (3) error analysis-based instructions, and (4) annotated samples for few-shot learning. We assessed each prompt's effectiveness and compared the models to BioClinicalBERT.\n\n\nRESULTS\nUsing baseline prompts, GPT-3.5 and GPT-4 achieved relaxed F1 scores of 0.634, 0.804 for MTSamples and 0.301, 0.593 for VAERS. Additional prompt components consistently improved model performance. When all 4 components were used, GPT-3.5 and GPT-4 achieved relaxed F1 socres of 0.794, 0.861 for MTSamples and 0.676, 0.736 for VAERS, demonstrating the effectiveness of our prompt framework. Although these results trail BioClinicalBERT (F1 of 0.901 for the MTSamples dataset and 0.802 for the VAERS), it is very promising considering few training samples are needed.\n\n\nDISCUSSION\nThe study's findings suggest a promising direction in leveraging LLMs for clinical NER tasks. However, while the performance of GPT models improved with task-specific prompts, there's a need for further development and refinement. LLMs like GPT-4 show potential in achieving close performance to state-of-the-art models like BioClinicalBERT, but they still require careful prompt engineering and understanding of task-specific knowledge. The study also underscores the importance of evaluation schemas that accurately reflect the capabilities and performance of LLMs in clinical settings.\n\n\nCONCLUSION\nWhile direct application of GPT models to clinical NER tasks falls short of optimal performance, our task-specific prompt framework, incorporating medical knowledge and training samples, significantly enhances GPT models' feasibility for potential clinical applications.",
            "year": 2023,
            "citationCount": 43,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A clinical task-specific prompt framework, incorporating medical knowledge and training samples, significantly enhances GPT models' feasibility for potential clinical applications and suggests a promising direction in leveraging LLMs for clinical NER tasks."
            },
            "score": 3
        },
        {
            "id": "aa2fa431ce1d5a8d56d138e3330d3df381d36e3a",
            "paperId": "aa2fa431ce1d5a8d56d138e3330d3df381d36e3a",
            "title": "Large Language Models in the Workplace: A Case Study on Prompt Engineering for Job Type Classification",
            "abstract": "This case study investigates the task of job classification in a real-world setting, where the goal is to determine whether an English-language job posting is appropriate for a graduate or entry-level position. We explore multiple approaches to text classification, including supervised approaches such as traditional models like Support Vector Machines (SVMs) and state-of-the-art deep learning methods such as DeBERTa. We compare them with Large Language Models (LLMs) used in both few-shot and zero-shot classification settings. To accomplish this task, we employ prompt engineering, a technique that involves designing prompts to guide the LLMs towards the desired output. Specifically, we evaluate the performance of two commercially available state-of-the-art GPT-3.5-based language models, text-davinci-003 and gpt-3.5-turbo. We also conduct a detailed analysis of the impact of different aspects of prompt engineering on the model's performance. Our results show that, with a well-designed prompt, a zero-shot gpt-3.5-turbo classifier outperforms all other models, achieving a 6% increase in Precision@95% Recall compared to the best supervised approach. Furthermore, we observe that the wording of the prompt is a critical factor in eliciting the appropriate\"reasoning\"in the model, and that seemingly minor aspects of the prompt significantly affect the model's performance.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This case study investigates the task of job classification in a real-world setting, where the goal is to determine whether an English-language job posting is appropriate for a graduate or entry-level position, and evaluates the performance of two commercially available state-of-the-art GPT-3.5-turbo language models."
            },
            "score": 3
        },
        {
            "id": "59e0e0c1aa06d51430792eb5d8308911a1b0110f",
            "paperId": "59e0e0c1aa06d51430792eb5d8308911a1b0110f",
            "title": "Prompt Engineering or Fine Tuning: An Empirical Assessment of Large Language Models in Automated Software Engineering Tasks",
            "abstract": "In this paper, we investigate the effectiveness of state-of-the-art LLM, i.e., GPT-4, with three different prompting engineering techniques (i.e., basic prompting, in-context learning, and task-specific prompting) against 18 fine-tuned LLMs on three typical ASE tasks, i.e., code generation, code summarization, and code translation. Our quantitative analysis of these prompting strategies suggests that prompt engineering GPT-4 cannot necessarily and significantly outperform fine-tuning smaller/older LLMs in all three tasks. For comment generation, GPT-4 with the best prompting strategy (i.e., task-specific prompt) had outperformed the first-ranked fine-tuned model by 8.33% points on average in BLEU. However, for code generation, the first-ranked fine-tuned model outperforms GPT-4 with best prompting by 16.61% and 28.3% points, on average in BLEU. For code translation, GPT-4 and fine-tuned baselines tie as they outperform each other on different translation tasks. To explore the impact of different prompting strategies, we conducted a user study with 27 graduate students and 10 industry practitioners. From our qualitative analysis, we find that the GPT-4 with conversational prompts (i.e., when a human provides feedback and instructions back and forth with a model to achieve best results) showed drastic improvement compared to GPT-4 with automatic prompting strategies. Moreover, we observe that participants tend to request improvements, add more context, or give specific instructions as conversational prompts, which goes beyond typical and generic prompting strategies. Our study suggests that, at its current state, GPT-4 with conversational prompting has great potential for ASE tasks, but fully automated prompt engineering with no human in the loop requires more study and improvement.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The study suggests that, at its current state, GPT-4 with conversational prompting has great potential for ASE tasks, but fully automated prompt engineering with no human in the loop requires more study and improvement."
            },
            "score": 3
        },
        {
            "id": "0392d58335ce674a70f5e58ac8c438de296a0e6a",
            "paperId": "0392d58335ce674a70f5e58ac8c438de296a0e6a",
            "title": "Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models",
            "abstract": "State-of-the-art neural language models can now be used to solve ad-hoc language tasks through zero-shot prompting without the need for supervised training. This approach has gained popularity in recent years, and researchers have demonstrated prompts that achieve strong accuracy on specific NLP tasks. However, finding a prompt for new tasks requires experimentation. Different prompt templates with different wording choices lead to significant accuracy differences. PromptIDE allows users to experiment with prompt variations, visualize prompt performance, and iteratively optimize prompts. We developed a workflow that allows users to first focus on model feedback using small data before moving on to a large data regime that allows empirical grounding of promising prompts using quantitative measures of the task. The tool then allows easy deployment of the newly created ad-hoc models. We demonstrate the utility of PromptIDE (demo: http://prompt.vizhub.ai) and our workflow using several real-world use cases.",
            "year": 2022,
            "citationCount": 74,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A workflow that allows users to first focus on model feedback using small data before moving on to a large data regime that allows empirical grounding of promising prompts using quantitative measures of the task, and then allows easy deployment of the newly created ad-hoc models."
            },
            "score": 3
        },
        {
            "id": "9182b7d9848442db8ca60731ce66d3bf169975e7",
            "paperId": "9182b7d9848442db8ca60731ce66d3bf169975e7",
            "title": "Prompt Engineering: a methodology for optimizing interactions with AI-Language Models in the field of engineering",
            "abstract": "ChatGPT is a versatile conversational Artificial Intelligence model that responds to user input prompts, with applications in academia and various sectors. However, crafting effective prompts can be challenging, leading to potentially inaccurate or contextually inappropriate responses, emphasizing the importance of prompt engineering in achieving accurate outcomes across different domains. This study aims to address this void by introducing a methodology for optimizing interactions with Artificial Intelligence language models, like ChatGPT, through prompts in the field of engineering. The approach is called GPEI and relies on the latest advancements in this area; and consists of four steps: define the objective, design the prompt, evaluate the response, and iterate. Our proposal involves two key aspects: data inclusion in prompt design for engineering applications and the integration of Explainable Artificial Intelligence principles to assess responses, enhancing transparency. It combines insights from various methodologies to address issues like hallucinations, emphasizing iterative prompt refinement techniques like posing opposing questions and using specific patterns for improvement. This methodology could improve prompt precision and utility in engineering.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study aims to address a void by introducing a methodology for optimizing interactions with Artificial Intelligence language models, like ChatGPT, through prompts in the field of engineering, and combines insights from various methodologies to address issues like hallucinations."
            },
            "score": 3
        },
        {
            "id": "07cfaf543b2bd991406be1d72a52d784cc9c62fb",
            "paperId": "07cfaf543b2bd991406be1d72a52d784cc9c62fb",
            "title": "Prompt Makes mask Language Models Better Adversarial Attackers",
            "abstract": "Generating high-quality synonymous perturbations is a core challenge for textual adversarial tasks. However, candidates generated from the masked language model often contain many words that are antonyms or irrelevant to the original words, which limit the perturbation space and affect the attack\u2019s effectiveness. We present ProAttacker1 which uses Prompt to make the mask language models better adversarial Attackers. ProAttacker inverts the prompt paradigm by leveraging the prompt with the class label to guide the language model to generate more semantically-consistent perturbations. We present a systematic evaluation to analyze the attack performance on 6 NLP datasets, covering text classification and inference. Our experiments demonstrate that ProAttacker outperforms state-of-the-art attack strategies in both success rate and perturb rate.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ProAttacker1, which uses Prompt to make the mask language models better adversarial Attackers, inverts the prompt paradigm by leveraging the prompt with the class label to guide the language model to generate more semantically-consistent perturbations."
            },
            "score": 3
        },
        {
            "id": "160b67f793ce4d8e0492b3b7918aaefd44350f92",
            "paperId": "160b67f793ce4d8e0492b3b7918aaefd44350f92",
            "title": "Generating Prompt-Based Adversarial Text Examples via Variable Neighborhood Search",
            "abstract": "Natural Language Processing (NLP) models are immensely vulnerable to adversarial text examples. Various word-level attacks have been proposed to modify input texts by carefully-picked substitute words via static or dynamic opti-mization algorithms. However, existing word-level attack methods usually ignore text fluency and semantic consistency for seeking a high attack success ratio, often resulting in unnatural adversarial text examples. In this paper, we propose to generate Prompt-based adversarial texts via Variable Neighborhood Search (P-VNS), which achieves a high attack success ratio while simulta-neously keeping text fluency and semantic similarity. Specifically, the well-designed prompt texts are constructed for input texts and the substitute words are obtained by mask-and-filling procedure under the effect of prompt texts, so the text fluency and semantic similarity can be enhanced. Additionally, the word modification priority is adaptively determined by employing the variable neighborhood search algorithm, yielding an improvement in the attack success ratio. Extensive experiments demonstrate that the P- VNS accomplishes the highest attack success ratio meanwhile preserving text fluency and semantic similarity. Besides, the pro-posed P- VNS also manifests effectiveness in adversarial training and transfer attack.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes to generate Prompt-based adversarial texts via Variable Neighborhood Search (P-VNS), which achieves a high attack success ratio while simulta-neously keeping text fluency and semantic similarity."
            },
            "score": 3
        },
        {
            "id": "5a2e45ce35fb26ab70a61b424a49f8e5b4532a8e",
            "paperId": "5a2e45ce35fb26ab70a61b424a49f8e5b4532a8e",
            "title": "WARP: Word-level Adversarial ReProgramming",
            "abstract": "Transfer learning from pretrained language models recently became the dominant approach for solving many NLP tasks. A common approach to transfer learning for multiple tasks that maximize parameter sharing trains one or more task-specific layers on top of the language model. In this paper, we present an alternative approach based on adversarial reprogramming, which extends earlier work on automatic prompt generation. Adversarial reprogramming attempts to learn task-specific word embeddings that, when concatenated to the input text, instruct the language model to solve the specified task. Using up to 25K trainable parameters per task, this approach outperforms all existing methods with up to 25M trainable parameters on the public leaderboard of the GLUE benchmark. Our method, initialized with task-specific human-readable prompts, also works in a few-shot setting, outperforming GPT-3 on two SuperGLUE tasks with just 32 training samples.",
            "year": 2021,
            "citationCount": 257,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents an alternative approach based on adversarial reprogramming, which extends earlier work on automatic prompt generation, and outperforms all existing methods with up to 25M trainable parameters on the public leaderboard of the GLUE benchmark."
            },
            "score": 3
        },
        {
            "id": "3a0cee573e19db7f3c561152199a3386adf6cb74",
            "paperId": "3a0cee573e19db7f3c561152199a3386adf6cb74",
            "title": "An Optimized Transfer Attack Framework Towards Multi-Modal Machine Learning",
            "abstract": "Deep neural networks (DNNs) have excelled at a wide range of tasks, including computer vision (CV), natural language processing (NLP), and speech recognition. However, past research has demonstrated that DNNs are vulnerable to adversarial examples, which are deliberately meant to trick models into making incorrect predictions by adding subtle perturbations into inputs. Adversarial examples create an exponential threat to multi-modal models that can accept a variety of inputs. By attacking substitute models, we provide a transferable attack framework. The suggested framework optimizes the attack process by modifying the prompt templates and simultaneously raising the attack on multiple inputs. Our experiments demonstrate that the proposed attack framework can significantly improve the success rate of transferable attacks, and adversarial examples are rarely noticed by humans. Meanwhile, experiments show that in transferable attacks, coarse-grained adversarial examples can achieve higher attack success rates than fine-grained ones, and the multi-modal models has some robustness against uni-modal attacks.",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work attacks substitute models to provide a transferable attack framework for DNNs vulnerable to adversarial examples, and demonstrates that the proposed attack framework can significantly improve the success rate of transferable attacks."
            },
            "score": 3
        },
        {
            "id": "7d7567340337b89c995c3525d89a6c713d1481e6",
            "paperId": "7d7567340337b89c995c3525d89a6c713d1481e6",
            "title": "L-AutoDA: Leveraging Large Language Models for Automated Decision-based Adversarial Attacks",
            "abstract": "In the rapidly evolving field of machine learning, adversarial attacks present a significant challenge to model robustness and security. Decision-based attacks, which only require feedback on the decision of a model rather than detailed probabilities or scores, are particularly insidious and difficult to defend against. This work introduces L-AutoDA (Large Language Model-based Automated Decision-based Adversarial Attacks), a novel approach leveraging the generative capabilities of Large Language Models (LLMs) to automate the design of these attacks. By iteratively interacting with LLMs in an evolutionary framework, L-AutoDA automatically designs competitive attack algorithms efficiently without much human effort. We demonstrate the efficacy of L-AutoDA on CIFAR-10 dataset, showing significant improvements over baseline methods in both success rate and computational efficiency. Our findings underscore the potential of language models as tools for adversarial attack generation and highlight new avenues for the development of robust AI systems.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces L-AutoDA (Large Language Model-based Automated Decision-based Adversarial Attacks), a novel approach leveraging the generative capabilities of Large Language Models to automate the design of these attacks."
            },
            "score": 2
        },
        {
            "id": "b557973b7c79df0506d1c06934b28355f7045d4e",
            "paperId": "b557973b7c79df0506d1c06934b28355f7045d4e",
            "title": "Finetuning Pretrained Vision-Language Models with Correlation Information Bottleneck for Robust Visual Question Answering",
            "abstract": "Bene\ufb01ting from large-scale Pretrained Vision-Language Models (VL-PMs), the performance of Visual Question Answering (VQA) has started to approach human oracle performance. However, \ufb01netuning large-scale VL-PMs with limited data for VQA usually faces over\ufb01tting and poor generalization issues, leading to a lack of robustness. In this paper, we aim to improve the robustness of VQA systems ( i.e ., the ability of the systems to defend against input variations and human-adversarial attacks) from the perspective of Information Bottleneck when \ufb01netuning VL-PMs for VQA. Generally, internal representations obtained by VL-PMs inevitably contain irrelevant and redundant information for the downstream VQA task, resulting in statistically spurious correlations and insensitivity to input variations. To encourage representations to converge to a minimal suf\ufb01cient statistic in vision-language learning, we propose the Correlation Information Bottleneck (CIB) principle, which seeks a tradeoff between representation compression and redundancy by minimizing the mutual information (MI) between the inputs and internal representations while maximizing the MI between the outputs and the representations. Meanwhile, CIB measures the internal correlations among visual and linguistic inputs and representations by a symmetrized joint MI estimation. Extensive experiments on \ufb01ve VQA benchmarks of input robustness and two VQA benchmarks of human-adversarial robustness demonstrate the effectiveness and superiority of the proposed CIB in improving the robustness of VQA systems.",
            "year": 2022,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper aims to improve the robustness of VQA systems from the perspective of Information Bottleneck and proposes the Correlation Information Bottleneck (CIB) principle, which seeks a tradeoff between representation compression and redundancy by minimizing the mutual information between the inputs and internal representations while maximizing the MI between the outputs and the representations."
            },
            "score": 2
        },
        {
            "id": "40727497e31823340ceacf1eaac511f3605750b9",
            "paperId": "40727497e31823340ceacf1eaac511f3605750b9",
            "title": "Explain2Attack: Text Adversarial Attacks via Cross-Domain Interpretability",
            "abstract": "Training robust deep learning models for downstream tasks is a critical challenge. Research has shown that down-stream models can be easily fooled with adversarial inputs that look like the training data, but slightly perturbed, in a way imperceptible to humans. Understanding the behavior of natural language models under these attacks is crucial to better defend these models against such attacks. In the black-box attack setting, where no access to model parameters is available, the attacker can only query the output information from the targeted model to craft a successful attack. Current black-box state-of-the-art models are costly in both computational complexity and number of queries needed to craft successful adversarial examples. For real world scenarios, the number of queries is critical, where less queries are desired to avoid suspicion towards an attacking agent. In this paper, we propose Explain2Attack, a black-box adversarial attack on text classification task. Instead of searching for important words to be perturbed by querying the target model, Explain2Attack employs an interpretable substitute model from a similar domain to learn word importance scores. We show that our framework either achieves or out-performs attack rates of the state-of-the-art models, yet with lower queries cost and higher efficiency.",
            "year": 2020,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 2
        },
        {
            "id": "b6499bcc10d4a70c3ca8b84995270cfd0d29de4c",
            "paperId": "b6499bcc10d4a70c3ca8b84995270cfd0d29de4c",
            "title": "Model-tuning Via Prompts Makes NLP Models Adversarially Robust",
            "abstract": "In recent years, NLP practitioners have converged on the following practice: (i) import an off-the-shelf pretrained (masked) language model; (ii) append a multilayer perceptron atop the CLS token's hidden representation (with randomly initialized weights); and (iii) fine-tune the entire model on a downstream task (MLP-FT). This procedure has produced massive gains on standard NLP benchmarks, but these models remain brittle, even to mild adversarial perturbations. In this work, we demonstrate surprising gains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an alternative method of adapting to downstream tasks. Rather than appending an MLP head to make output prediction, MVP appends a prompt template to the input, and makes prediction via text infilling/completion. Across 5 NLP datasets, 4 adversarial attacks, and 3 different models, MVP improves performance against adversarial substitutions by an average of 8% over standard methods and even outperforms adversarial training-based state-of-art defenses by 3.5%. By combining MVP with adversarial training, we achieve further improvements in adversarial robustness while maintaining performance on unperturbed examples. Finally, we conduct ablations to investigate the mechanism underlying these gains. Notably, we find that the main causes of vulnerability of MLP-FT can be attributed to the misalignment between pre-training and fine-tuning tasks, and the randomly initialized MLP parameters.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work demonstrates surprising gains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an alternative method of adapting to downstream tasks that improves performance against adversarial substitutions and outperforms adversarial training-based state-of-art defenses by 3.5%."
            },
            "score": 2
        },
        {
            "id": "fe568badb02858df92eaf423787ed3388d2b454f",
            "paperId": "fe568badb02858df92eaf423787ed3388d2b454f",
            "title": "Divide-and-Conquer Attack: Harnessing the Power of LLM to Bypass the Censorship of Text-to-Image Generation Model",
            "abstract": "\u2014Text-to-image generative models offer many innovative services but also raise ethical concerns due to their potential to generate unethical images. Most publicly available text-to-image models employ safety filters to prevent unintended generation intents. In this work, we introduce the Divide-and-Conquer Attack to circumvent the safety filters of state-of-the-art text-to-image models. Our attack leverages LLMs as agents for text transformation, creating adversarial prompts from sensitive ones. We have developed effective helper prompts that enable LLMs to break down sensitive drawing prompt into multiple harmless descriptions, allowing them to bypass safety filters while still generating sensitive images. This means that the latent harmful meaning only becomes apparent when all individual elements are drawn together. Our evaluation demonstrates that our attack successfully circumvents the closed-box safety filter of SOTA DALL \u00b7 E 3 integrated natively into ChatGPT to generate unethical images. This approach, which essentially uses LLM-generated adversarial prompts against GPT-4-assisted DALL \u00b7 E 3, is akin to using one\u2019s own spear to breach their shield. It could have more severe security implications than previous manual crafting or iterative model querying methods, and we hope it stimulates more attention towards similar efforts. Our code and data are available at: https://github.com/researchcode001/Divide-and-Conquer-Attack.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Divide-and-Conquer Attack is introduced to circumvent the safety filters of state-of-the-art text-to-image models, using LLM-generated adversarial prompts against GPT-4-assisted DALL \u00b7 E 3 to generate unethical images."
            },
            "score": 2
        },
        {
            "id": "f14ea74a40c40cad10c32fccf5a410144735d683",
            "paperId": "f14ea74a40c40cad10c32fccf5a410144735d683",
            "title": "Efficient and Robust Knowledge Graph Construction",
            "abstract": "Knowledge graph construction which aims to extract knowledge from the text corpus, has appealed to the NLP community researchers. Previous decades have witnessed the remarkable progress of knowledge graph construction on the basis of neural models; however, those models often cost massive computation or labeled data resources and suffer from unstable inference accounting for biased or adversarial samples. Recently, numerous approaches have been explored to mitigate the efficiency and robustness issues for knowledge graph construction, such as prompt learning and adversarial training. In this tutorial, we aim to bring interested NLP researchers up to speed on the recent and ongoing techniques for efficient and robust knowledge graph construction. Additionally, our goal is to provide a systematic and up-to-date overview of these methods and reveal new research opportunities to the audience.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This tutorial aims to bring interested NLP researchers up to speed on the recent and ongoing techniques for efficient and robust knowledge graph construction, and provides a systematic and up-to-date overview of these methods."
            },
            "score": 2
        },
        {
            "id": "e9c1e111860920898e1f982ee2ee60606ff35e11",
            "paperId": "e9c1e111860920898e1f982ee2ee60606ff35e11",
            "title": "Dilated convolution for enhanced extractive summarization: A GAN-based approach with BERT word embedding",
            "abstract": "Text summarization (TS) plays a crucial role in natural language processing (NLP) by automatically condensing and capturing key information from text documents. Its significance extends to diverse fields, including engineering, healthcare, and others, where it offers substantial time and resource savings. However, manual summarization is a laborious task, prompting the need for automated text summarization systems. In this paper, we propose a novel strategy for extractive summarization that leverages a generative adversarial network (GAN)-based method and Bidirectional Encoder Representations from Transformers (BERT) word embedding. BERT, a transformer-based architecture, processes sentence bidirectionally, considering both preceding and following words. This contextual understanding empowers BERT to generate word representations that carry a deeper meaning and accurately reflect their usage within specific contexts. Our method adopts a generator and discriminator within the GAN framework. The generator assesses the likelihood of each sentence in the summary while the discriminator evaluates the generated summary. To extract meaningful features in parallel, we introduce three dilated convolution layers in the generator and discriminator. Dilated convolution allows for capturing a larger context and incorporating long-range dependencies. By introducing gaps between filter weights, dilated convolution expands the receptive field, enabling the model to consider a broader context of words. To encourage the generator to explore diverse sentence combinations that lead to high-quality summaries, we introduce various noises to each document within our proposed GAN. This approach allows the generator to learn from a range of sentence permutations and select the most suitable ones. We evaluate the performance of our proposed model using the CNN/Daily Mail dataset. The results, measured using the ROUGE metric, demonstrate the superiority of our approach compared to other tested methods. This confirms the effectiveness of our GAN-based strategy, which integrates dilated convolution layers, BERT word embedding, and a generator-discriminator framework in achieving enhanced extractive summarization performance.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel strategy for extractive summarization that leverages a generative adversarial network (GAN)-based method and Bidirectional Encoder Representations from Transformers (BERT) word embedding is proposed and the results demonstrate the superiority of the approach compared to other tested methods."
            },
            "score": 1
        }
    ],
    "novelty": "no"
}