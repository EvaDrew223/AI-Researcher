{
    "topic_description": "novel prompting methods for large language models to improve code generation",
    "idea_name": "Adversarial Prompting for Robust Code Generation",
    "raw_idea": {
        "Problem": "LLMs can generate code that passes test cases but fails on edge cases or slight variations of the input. They also tend to generate code that is not robust to minor perturbations in the problem description.",
        "Existing Methods": "Existing code generation benchmarks like HumanEval and APPS only test on a fixed set of problems and test cases. Some methods attempt to improve robustness by adversarial training or data augmentation.",
        "Motivation": "Adversarial examples have been widely used in domains like computer vision and NLP to improve model robustness. The idea is to create small perturbations to the input that cause the model to make errors. By training on these adversarial examples, the model learns to be more robust. We propose to apply this idea to prompting for code generation.",
        "Proposed Method": "We propose Adversarial Prompting, where we prompt the LLM to generate both a solution and adversarial examples for a given coding problem. Specifically, we first prompt the LLM to generate code for the original problem. Then, we prompt it to generate adversarial examples: slight variations of the problem description or edge case inputs that the current code might fail on. We then prompt the LLM to revise the code to handle these adversarial examples. This process repeats for a few iterations. The key is to prompt the LLM to generate meaningful adversarial examples that expose weaknesses in the current code.",
        "Experiment Plan": "We will evaluate Adversarial Prompting on modified versions of HumanEval and APPS that include adversarial examples. We will compare with baselines like direct prompting and few-shot prompting on both the original and adversarial test sets. We will measure pass@k rates and code robustness metrics. We will also analyze the quality of the generated adversarial examples."
    },
    "full_experiment_plan": {
        "Title": "Adversarial Prompting: Improving Code Generation Robustness via Iterative Prompt Perturbation",
        "Problem Statement": "Large Language Models (LLMs) can generate code that passes test cases but fails on edge cases or slight variations of the input. They also tend to generate code that is not robust to minor perturbations in the problem description.",
        "Motivation": "Existing code generation benchmarks like HumanEval and APPS only test on a fixed set of problems and test cases. Some methods attempt to improve robustness by adversarial training or data augmentation. However, these methods require retraining the model which is expensive. Adversarial examples have been widely used in domains like computer vision and NLP to improve model robustness without retraining. The idea is to create small perturbations to the input that cause the model to make errors. By training on these adversarial examples, the model learns to be more robust. We propose to apply this idea to prompting for code generation, where we prompt the LLM itself to generate meaningful adversarial examples that expose weaknesses in the current code, and then prompt it to revise the code to handle these adversarial examples.",
        "Proposed Method": "We propose Adversarial Prompting, where we prompt the LLM to generate both a solution and adversarial examples for a given coding problem. Specifically:\n1. We first prompt the LLM to generate code for the original problem. \n2. Then, we prompt it to generate adversarial examples: slight variations of the problem description or edge case inputs that the current code might fail on. \n3. We then prompt the LLM to revise the code to handle these adversarial examples. \nThis process repeats for a few iterations. The key is to prompt the LLM to generate meaningful adversarial examples that expose weaknesses in the current code.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We will evaluate on modified versions of HumanEval and APPS that include adversarial examples. For HumanEval, we will use the HumanEval-Adv dataset which includes human-written adversarial examples. For APPS, we will generate adversarial examples by prompting GPT-4 to perturb the problem descriptions and inputs.",
            "Step 2: Construct Prompts": "1. Original Prompt: The original coding problem description.\n2. Adversarial Example Prompt: 'Given the following coding problem and solution, generate 3 adversarial examples that the current solution likely fails on. An adversarial example can be a slight variation of the problem description that changes the meaning, or an edge case input. For each adversarial example, provide the perturbed problem description (if applicable), the input, and the expected output.'\n3. Revision Prompt: 'Here is the original coding problem, the current solution, and some adversarial examples that the solution fails on. Revise the solution to handle these adversarial examples while still passing the original test cases.'\nWe experimented with alternative prompt wordings and found these to work best.",
            "Step 3: Select Models": "We will use both GPT-3.5 (text-davinci-003) and GPT-4 via the OpenAI API. GPT-4 will likely perform better but is more expensive.",
            "Step 4: Run Experiments": "For each coding problem:\n1. Prompt the model with the Original Prompt to generate the initial solution.\n2. Prompt the model with the Adversarial Example Prompt to generate adversarial examples.\n3. Test the current solution on the adversarial examples and record the failures.\n4. Prompt the model with the Revision Prompt to revise the solution.\n5. Repeat steps 2-4 for N iterations (e.g. N=3).\nWe will compare Adversarial Prompting with baselines:\n1. Direct Prompting: Just use the Original Prompt.\n2. Few-Shot Prompting: Include a few examples of coding problems and solutions in the prompt.\nWe will evaluate pass@1 rates on the original and adversarial test cases.",
            "Step 5: Analyze Results": "We will compare the performance of Adversarial Prompting vs the baselines on both the original and adversarial test sets. We expect Adversarial Prompting to significantly improve robustness on the adversarial test set while maintaining performance on the original test set. We will also qualitatively analyze the generated adversarial examples to assess their meaningfulness and diversity."
        },
        "Test Case Examples": {
            "Original Problem": "Write a function to convert a string to lowercase.",
            "Baseline Solution": "def soln(string):\n    return string.lower()",
            "Baseline Failure": "Input: 'Hello World'\nExpected: 'hello world'\nActual: 'hello world'\n\nInput: 'HELLO WORLD!'\nExpected: 'hello world!'\nActual: 'hello world!'\n\nInput: 'Hello World 123'\nExpected: 'hello world 123'\nActual: 'hello world 123'",
            "Adversarial Examples": "1. Perturbed Problem: Write a function to convert a string to lowercase, excluding any digits in the string.\nInput: 'Hello World 123'\nExpected Output: 'hello world 123'\n\n2. Perturbed Problem: Write a function to convert a string to lowercase. The string may contain unicode characters.\nInput: '\u041f\u0440\u0438\u0432\u0435\u0442 \u043c\u0438\u0440'\nExpected Output: '\u043f\u0440\u0438\u0432\u0435\u0442 \u043c\u0438\u0440'\n\n3. Edge Case Input:\nInput: '!@#$%^&*()'\nExpected Output: '!@#$%^&*()'",
            "Revised Solution": "def soln(string):\n    lower_chars = []\n    for char in string:\n        if char.isalpha():\n            lower_chars.append(char.lower())\n        else:\n            lower_chars.append(char)\n    return ''.join(lower_chars)",
            "Explanation": "The baseline solution fails on the adversarial examples. It incorrectly lowercases digits and does not handle unicode characters. The revised solution generated after adversarial prompting correctly handles these cases by checking if each character is alphabetic before lowercasing, and supports unicode."
        },
        "Fallback Plan": "If Adversarial Prompting does not improve robustness, we can: \n1. Analyze the quality of the generated adversarial examples. Are they actually meaningful and exposing real weaknesses? If not, the prompts for generating adversarial examples may need to be improved.\n2. Increase the number of adversarial prompting iterations. More rounds of revision may be needed to handle harder cases.\n3. Use a more capable language model like GPT-4 which may be better at generating meaningful adversarial examples and revisions.\nIf Adversarial Prompting still does not help after these debugging steps, we can pivot to an analysis of why it fails on code generation robustness. The generated adversarial examples and revisions can provide insights into the limitations of prompting for this task. We can also compare Adversarial Prompting with other robustness methods like adversarial training to understand their strengths and weaknesses."
    }
}