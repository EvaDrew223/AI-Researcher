{
    "topic_description": "novel prompting methods for large language models to improve code generation",
    "idea_name": "Code Optimization Prompting with Exemplar Critiquing",
    "raw_idea": {
        "Problem": "Large language models can generate code that is functionally correct but suboptimal in terms of performance, readability, or other quality aspects. Developers often spend significant effort optimizing and refactoring generated code.",
        "Existing Methods": "Prior work has explored techniques like reinforcement learning, iterative refinement, and example-based prompting to improve the quality of generated code. However, these often require large amounts of labeled data or expensive trial-and-error optimization.",
        "Motivation": "We observe that experienced human developers often optimize code by critiquing it against exemplary code snippets and best practices. They iteratively identify areas for improvement and refine the code based on these critiques. We propose to prompt LLMs to simulate this exemplar-based critiquing and optimization process.",
        "Proposed Method": "We propose Code Optimization Prompting with Exemplar Critiquing, consisting of the following key steps: 1) Naive Code Generation Prompt to generate an initial, unoptimized version of the code, 2) Exemplar Retrieval Prompt to find relevant high-quality code examples for the given task, 3) Critique Generation Prompt to compare the generated code with the exemplars and identify areas for optimization, and 4) Code Optimization Prompt to refine the generated code based on the critiques. The critique and optimization steps are repeated for multiple iterations.",
        "Experiment Plan": "We will evaluate our approach on code optimization tasks from OpenAI's APPS dataset and competitive programming problems. We will measure execution time, memory usage, and code quality metrics (e.g. time/space complexity, readability scores) compared to baselines of direct prompting and iterative refinement without exemplar critiquing. We will also conduct a user study with developers to assess the usefulness and interpretability of the generated critiques and optimizations."
    },
    "full_experiment_plan": {
        "Title": "Code Optimization Prompting with Exemplar Critiquing for Large Language Models",
        "Problem Statement": "Large language models can generate code that is functionally correct but suboptimal in terms of performance, readability, or other quality aspects. Developers often spend significant effort optimizing and refactoring generated code.",
        "Motivation": "Prior work has explored techniques like reinforcement learning, iterative refinement, and example-based prompting to improve the quality of generated code. However, these often require large amounts of labeled data or expensive trial-and-error optimization. We observe that experienced human developers often optimize code by critiquing it against exemplary code snippets and best practices. They iteratively identify areas for improvement and refine the code based on these critiques. We propose to prompt LLMs to simulate this exemplar-based critiquing and optimization process.",
        "Proposed Method": "We propose Code Optimization Prompting with Exemplar Critiquing, consisting of the following key steps: 1) Naive Code Generation Prompt to generate an initial, unoptimized version of the code, 2) Exemplar Retrieval Prompt to find relevant high-quality code examples for the given task, 3) Critique Generation Prompt to compare the generated code with the exemplars and identify areas for optimization, and 4) Code Optimization Prompt to refine the generated code based on the critiques. The critique and optimization steps are repeated for multiple iterations.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We will use code optimization tasks from OpenAI's APPS dataset and competitive programming problems from Codeforces. For APPS, we focus on the HumanEval subset which has unit tests to automatically evaluate functional correctness. For Codeforces, we select problems that have both naive and optimized solutions in the editorial.",
            "Step 2: Construct Prompts": "1) Naive Code Generation Prompt: Given a coding task description, generate a functionally correct but potentially unoptimized solution. Few-shot examples are not needed.\n2) Exemplar Retrieval Prompt: Given the task description and the naive solution, retrieve a set of relevant optimized solutions from the dataset. Prompt the model to rank the exemplars by relevance and select the top-k.\n3) Critique Generation Prompt: Given the naive solution and the exemplars, generate critiques on how to optimize the naive solution. Provide few-shot examples of critiques comparing naive and optimized code, covering aspects like time/space complexity, redundant computation, naming, modularization etc.\n4) Code Optimization Prompt: Given the naive solution and the generated critiques, prompt the model to refine the solution based on the critique. Provide few-shot examples of iterative code refinement based on critiques.",
            "Step 3: Select Models": "We will use OpenAI's text-davinci-003 (GPT-3.5) and gpt-4 models accessed through the API.",
            "Step 4: Implement Baselines": "1) Direct prompting: Directly prompt the model to generate an optimized solution without any critiquing or refinement steps.\n2) Iterative refinement: Repeatedly prompt the model to refine the generated solution, but without using exemplars or explicit critiques. Stop after a fixed number of iterations or when the model makes no further changes.",
            "Step 5: Implement Proposed Method": "Implement the Code Optimization Prompting with Exemplar Critiquing as described in the proposed method and prompt construction steps. Experiment with different number of critique-refinement iterations (e.g., 1, 2, 3) and different number of exemplars retrieved (e.g., 1, 3, 5).",
            "Step 6: Evaluate Results": "Evaluate the optimized solutions generated by the baselines and proposed method on three key metrics:\n1) Functional correctness: Use the unit tests from APPS and the Codeforces problem checker to verify that the optimized solutions are still functionally correct.\n2) Execution time: Measure the execution time of the optimized solutions on a set of predefined test cases. Compare the speedup over the naive solution and across different methods.\n3) Code quality: Use static analysis tools like Pylint or Radon to compute code quality metrics such as cyclomatic complexity, maintainability index, and code size. Additionally, conduct a human evaluation where experienced developers rate the readability, modularity, and style of the generated code on a 1-5 Likert scale."
        },
        "Test Case Examples": {
            "Example 1": {
                "Task": "Given a list of integers, return the largest product that can be made by multiplying any three integers.",
                "Naive Solution": "def soln(nums):\n    n = len(nums)\n    max_prod = float('-inf')\n    for i in range(n):\n        for j in range(i+1, n):\n            for k in range(j+1, n):\n                max_prod = max(max_prod, nums[i]*nums[j]*nums[k])\n    return max_prod",
                "Critique": "The naive solution uses a brute force approach with three nested loops, resulting in a time complexity of O(n^3). This is inefficient for large lists. To optimize, we can sort the list and consider the following cases: 1) The three largest positive numbers, 2) The two smallest negative numbers and the largest positive number.",
                "Optimized Solution": "def soln(nums):\n    nums.sort()\n    return max(nums[-1]*nums[-2]*nums[-3], nums[0]*nums[1]*nums[-1])",
                "Explanation": "The optimized solution reduces the time complexity to O(n log n) by sorting the list and considering only two cases. This avoids the need for nested loops and redundant comparisons."
            },
            "Example 2": {
                "Task": "Given a string, find the length of the longest substring without repeating characters.",
                "Naive Solution": "def soln(s):\n    n = len(s)\n    max_len = 0\n    for i in range(n):\n        for j in range(i, n):\n            if len(set(s[i:j+1])) == j-i+1:\n                max_len = max(max_len, j-i+1)\n    return max_len",
                "Critique": "The naive solution uses nested loops to generate all substrings and check for repeating characters, resulting in a time complexity of O(n^3) due to the set creation in the inner loop. To optimize, we can use a sliding window approach with a hash set to track unique characters.",
                "Optimized Solution": "def soln(s):\n    n = len(s)\n    char_set = set()\n    max_len, start = 0, 0\n    for end in range(n):\n        while s[end] in char_set:\n            char_set.remove(s[start])\n            start += 1\n        char_set.add(s[end])\n        max_len = max(max_len, end-start+1)\n    return max_len",
                "Explanation": "The optimized solution uses a sliding window approach to maintain a substring with unique characters. The time complexity is reduced to O(n) since each character is processed at most twice (once by the end pointer and once by the start pointer)."
            }
        },
        "Fallback Plan": "If the proposed method does not consistently outperform the baselines, we can conduct additional analysis and ablation studies to identify potential issues and areas for improvement:\n1) Analyze the quality and relevance of the retrieved exemplars. If the exemplars are not sufficiently relevant or optimized, the critiques and refinements may not be effective. Experiment with alternative exemplar retrieval methods or sources.\n2) Analyze the generated critiques. Check if the critiques are accurately identifying the key areas for optimization and providing actionable suggestions. If the critiques are vague, irrelevant, or incorrect, the refinement step will not produce optimal results. Consider fine-tuning the critique generation prompt with more diverse examples or using a separate model for critique generation.\n3) Analyze the refinement process. Examine how well the model incorporates the critiques into the refined solutions. If the model struggles to apply the critiques effectively, consider providing more explicit guidance or examples in the refinement prompt. Experiment with different prompt formats or few-shot examples to improve the refinement quality.\n4) If the proposed method still underperforms after these analyses and adjustments, we can pivot the project to focus on understanding the limitations and challenges of exemplar-based code optimization with LLMs. Conduct a thorough error analysis to identify common failure modes and their underlying causes. Investigate the trade-offs between using exemplars, critiques, and iterative refinement for code optimization. Compare the performance of different LLMs and explore the impact of model size, training data, and prompt design on the optimization results. The insights from this analysis can inform future research directions and help develop more robust and effective code optimization techniques for LLMs."
    }
}