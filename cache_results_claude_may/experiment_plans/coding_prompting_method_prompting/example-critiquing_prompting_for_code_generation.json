{
    "topic_description": "novel prompting methods for large language models to improve code generation",
    "idea_name": "Example-Critiquing Prompting for Code Generation",
    "raw_idea": {
        "Problem": "LLMs can generate code that is similar to the given examples but fails to handle novel variations or more complex cases. They also struggle to generate code that is more efficient or elegant than the examples.",
        "Existing Methods": "Existing few-shot prompting methods for code generation provide examples of how to solve similar coding problems. However, they do not explicitly teach the model to critically analyze and improve upon the examples.",
        "Motivation": "When humans learn to code, they often study example solutions and then discuss their strengths and weaknesses. They learn to critique the examples and think about how to improve them. We propose to mimic this process in prompting LLMs for code generation.",
        "Proposed Method": "We propose Example-Critiquing Prompting, where we prompt the LLM to not just follow the given examples, but also critique them and generate improved solutions. Specifically, for each coding problem, we first provide a few example solutions of varying quality. We then prompt the LLM to critique each example by identifying its strengths, weaknesses, and areas for improvement. Finally, we prompt the LLM to generate a new solution that addresses the identified weaknesses and incorporates the strengths of the examples. The key is to prompt the LLM to perform a thoughtful analysis of the examples and not just blindly imitate them.",
        "Experiment Plan": "We will evaluate Example-Critiquing Prompting on the HumanEval and APPS benchmarks, comparing it with baselines like direct prompting and standard few-shot prompting. We will measure pass@k rates and code quality metrics like efficiency and readability. We will also perform a qualitative analysis of the generated critiques and how they inform the final generated solution."
    },
    "full_experiment_plan": {
        "Title": "Example-Critiquing Prompting: Teaching Language Models to Analyze and Improve Code Examples",
        "Problem Statement": "Large Language Models (LLMs) can generate code that is similar to given examples but often fails to handle novel variations, more complex cases, or generate code that is more efficient or elegant than the examples. Existing few-shot prompting methods provide examples of how to solve similar coding problems but do not explicitly teach the model to critically analyze and improve upon the examples.",
        "Motivation": "When humans learn to code, they often study example solutions and then discuss their strengths and weaknesses. They learn to critique the examples and think about how to improve them. We propose to mimic this process in prompting LLMs for code generation. By prompting the model to not just follow the given examples, but also critique them and generate improved solutions, we aim to teach the model to perform a more thoughtful analysis of the examples and generate higher-quality code.",
        "Proposed Method": "Example-Critiquing Prompting consists of the following steps for each coding problem:\n1. Provide a few example solutions of varying quality. \n2. Prompt the LLM to critique each example by identifying its strengths, weaknesses, and areas for improvement.\n3. Prompt the LLM to generate a new solution that addresses the identified weaknesses and incorporates the strengths of the examples.\nThe key is to prompt the LLM to perform a thoughtful analysis of the examples and not just blindly imitate them.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "Evaluate Example-Critiquing Prompting on the HumanEval and APPS benchmarks for code generation. These datasets contain coding problems with test cases to evaluate the functional correctness of generated solutions.",
            "Step 2: Construct Prompts": "For each coding problem, construct the following prompts:\n1. Direct Prompting (baseline): Prompt the model with just the problem description.\n2. Few-Shot Prompting (baseline): Prompt the model with the problem description and a few example solutions.\n3. Example-Critiquing Prompting (proposed): \n   a. Prompt the model with the problem description and a few example solutions.\n   b. For each example solution, prompt the model to identify its strengths, weaknesses, and areas for improvement.\n   c. Prompt the model to generate a new solution considering the critiques.\nEnsure the example solutions for each problem have varying quality (e.g., some correct and efficient, some correct but inefficient, some incorrect).",
            "Step 3: Select Models": "Evaluate the prompts on GPT-3.5 (text-davinci-003) and GPT-4 via the OpenAI API.",
            "Step 4: Generate and Evaluate Solutions": "For each coding problem and prompting method:\n1. Prompt the model to generate a solution.\n2. Evaluate the functional correctness of the generated solution using the problem's test cases.\n3. If the solution passes all test cases, evaluate its efficiency and readability:\n   - Efficiency: Measure runtime complexity and compare to an optimal solution.\n   - Readability: Use a code readability metric like Cyclomatic Complexity.\n4. For Example-Critiquing Prompting, also qualitatively analyze the generated critiques for insightfulness.",
            "Step 5: Aggregate Results": "Compute the following metrics for each prompting method:\n- Pass@k: The percentage of problems for which a correct solution is generated within k attempts (e.g., k=1, 10).\n- Average Efficiency: The average runtime complexity of correct solutions relative to optimal solutions.\n- Average Readability: The average readability score of correct solutions.\nCompare the metrics between the prompting methods to determine if Example-Critiquing Prompting improves performance."
        },
        "Test Case Examples": {
            "Example 1": {
                "Problem": "Write a function to calculate the Fibonacci sequence up to n.",
                "Example Solutions": [
                    "def fib(n):\n    if n <= 0: return []\n    if n == 1: return [0]\n    seq = [0, 1]\n    for i in range(2, n):\n        seq.append(seq[i-1] + seq[i-2])\n    return seq",
                    "def fib(n):\n    seq = []\n    a, b = 0, 1\n    while len(seq) < n:\n        seq.append(a)\n        a, b = b, a + b\n    return seq",
                    "def fib(n):\n    if n <= 0: return []\n    seq = [0] * n\n    seq[1] = 1\n    for i in range(2, n):\n        seq[i] = seq[i-1] + seq[i-2]\n    return seq"
                ],
                "Example Critiques": [
                    "Strengths: Correct, easy to understand. Weaknesses: Inefficient for large n due to linear runtime. Improvements: Use memoization or matrix exponentiation for O(log n) runtime.",
                    "Strengths: Correct, space-efficient by only storing the last two numbers. Weaknesses: While loop condition is a bit hard to understand. Improvements: Can use tuple unpacking to update a and b in one line.",
                    "Strengths: Correct, preprocesses the sequence with the correct length. Weaknesses: Allocates unnecessary space by initializing the full list. Improvements: Can generate the sequence dynamically and append to a list."
                ],
                "Example Generated Solution": "def fib(n):\n    if n <= 0: return []\n    if n <= 2: return [0, 1][:n]\n    a, b = 0, 1\n    seq = [0, 1]\n    for _ in range(2, n):\n        a, b = b, a + b\n        seq.append(b)\n    return seq",
                "Explanation": "The generated solution combines the strengths of the examples: it is correct, space-efficient by only storing the last two numbers, and dynamically generates the sequence. It improves upon the weaknesses by having a clear for loop condition and avoiding unnecessary space allocation."
            },
            "Example 2": {
                "Problem": "Write a function to check if a given string is a palindrome.",
                "Example Solutions": [
                    "def is_palindrome(s):\n    return s == s[::-1]",
                    "def is_palindrome(s):\n    for i in range(len(s)//2):\n        if s[i] != s[-i-1]:\n            return False\n    return True",
                    "def is_palindrome(s):\n    return all(s[i] == s[~i] for i in range(len(s)//2))"
                ],
                "Example Critiques": [
                    "Strengths: Very concise and easy to understand. Weaknesses: Creates a new reversed string which is inefficient for memory. Improvements: Can compare characters from both ends and move inwards.",
                    "Strengths: Efficient, only needs to compare half the characters. Weaknesses: A bit verbose with the explicit for loop and if condition. Improvements: Can use a one-liner with all() and a generator expression.",
                    "Strengths: Concise one-liner, efficient by only comparing half the characters. Weaknesses: Uses the uncommon ~ operator which may be confusing. Improvements: Can use negative indexing instead of ~ for clarity."
                ],
                "Example Generated Solution": "def is_palindrome(s):\n    return all(s[i] == s[-i-1] for i in range(len(s)//2))",
                "Explanation": "The generated solution combines the strengths of the examples: it is a concise one-liner, efficient by only comparing half the characters, and uses clear negative indexing instead of the uncommon ~ operator. It avoids the weaknesses of creating a new string or being verbose."
            }
        },
        "Fallback Plan": "If Example-Critiquing Prompting does not outperform the baselines, we can perform additional analysis to understand why:\n1. Analyze the quality of the generated critiques. Are they identifying meaningful strengths and weaknesses? Are the suggested improvements useful? If the critiques are low quality, the model may not be able to generate improved solutions.\n2. Analyze the generated solutions. Are they incorporating the identified strengths and improvements from the critiques? If not, the model may be struggling to act on the critiques.\n3. Try different variations of the prompt format and instructions. Is there a better way to prompt the model to critique and improve examples?\n4. Investigate if the model's performance varies across different types of coding problems. Is Example-Critiquing Prompting more effective for certain types of problems?\nBased on the analysis, we can either refine the prompting approach or pivot to an analysis of why Example-Critiquing Prompting did not work and what that reveals about the model's capabilities and limitations in code generation."
    }
}