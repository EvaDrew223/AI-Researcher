{
    "topic_description": "novel prompting methods that can improve factuality and reduce hallucination of large language models",
    "idea_name": "Discourse Coherence Prompting",
    "raw_idea": {
        "Problem": "Large language models can lose coherence when generating long sequences, leading to issues like self-contradiction, irrelevant tangents, or abrupt topic shifts. This is especially problematic for tasks that require generating coherent long-form text, like story generation, dialogues, or explanations.",
        "Existing Methods": "Most existing methods for improving coherence focus on better planning and content selection to guide generation, such as by generating intermediates sketches, keyword transition sequences, or learned sentence representations. However, these approaches do not directly optimize for the linguistic markers of discourse coherence.",
        "Motivation": "Linguistic theories of discourse coherence, such as Centering Theory and Rhetorical Structure Theory, describe how humans maintain coherence in text and dialogue through specific discourse patterns, such as chains of mention, coreference, discourse relations, and coherent topic progression. We can prompt models to generate these discourse coherence markers as part of the generation process, to more directly guide them towards coherent output.",
        "Proposed Method": "We develop discourse coherence prompts that include explicit markers for the key components of discourse coherence theories, such as entities and coreference chains, discourse relations, and topic flow. For example, a prompt could include tags like <entity1>, <entity2>, <coref>, <elaboration>, <contrast>, <topic_shift> that the model must generate to make the discourse structure explicit. We also include a few-shot prompt that demonstrates how to map the discourse coherence markers to natural text. At generation time, we first prompt the model to generate the discourse coherence structure of the response, then condition the response generation on that structure.",
        "Experiment Plan": "We will evaluate Discourse Coherence Prompting on tasks that require generating coherent long-form text, including story generation (e.g. WritingPrompts), long-form QA (e.g. ELI5, NarrativeQA), and dialogue (e.g. TopicalChat, PersonaChat). We will measure coherence with automatic metrics like DiscEval, GED, and sentence topic flow (STF), as well as human ratings of coherence, consistency, and single-topic flow. We will also qualitatively analyze the generated discourse structures to understand what patterns lead to more or less coherent output."
    },
    "full_experiment_plan": {
        "Title": "Discourse Coherence Prompting: Improving Long-form Text Generation with Explicit Discourse Structure",
        "Problem Statement": "Large language models can lose coherence when generating long sequences, leading to issues like self-contradiction, irrelevant tangents, or abrupt topic shifts. This is especially problematic for tasks that require generating coherent long-form text, like story generation, dialogues, or explanations.",
        "Motivation": "Most existing methods for improving coherence focus on better planning and content selection to guide generation, such as by generating intermediates sketches, keyword transition sequences, or learned sentence representations. However, these approaches do not directly optimize for the linguistic markers of discourse coherence. Linguistic theories of discourse coherence, such as Centering Theory and Rhetorical Structure Theory, describe how humans maintain coherence in text and dialogue through specific discourse patterns, such as chains of mention, coreference, discourse relations, and coherent topic progression. We can prompt models to generate these discourse coherence markers as part of the generation process, to more directly guide them towards coherent output.",
        "Proposed Method": "We develop discourse coherence prompts that include explicit markers for the key components of discourse coherence theories, such as entities and coreference chains, discourse relations, and topic flow. For example, a prompt could include tags like <entity1>, <entity2>, <coref>, <elaboration>, <contrast>, <topic_shift> that the model must generate to make the discourse structure explicit. We also include a few-shot prompt that demonstrates how to map the discourse coherence markers to natural text. At generation time, we first prompt the model to generate the discourse coherence structure of the response, then condition the response generation on that structure.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We will evaluate Discourse Coherence Prompting on tasks that require generating coherent long-form text, including story generation (e.g. WritingPrompts), long-form QA (e.g. ELI5, NarrativeQA), and dialogue (e.g. TopicalChat, PersonaChat).",
            "Step 2: Define Discourse Coherence Markers": "Based on linguistic theories of discourse coherence, define a set of discourse markers to include in the prompts. These should cover key aspects like:\n- Entities and coreference chains (e.g. <entity1>, <entity2>, <coref>)\n- Discourse relations (e.g. <elaboration>, <contrast>, <cause-effect>)\n- Topic progression (e.g. <topic_shift>, <topic_continue>)",
            "Step 3: Construct Prompts": "Create prompts that include the discourse coherence markers defined in Step 2. The prompts should have two parts:\n1. A few-shot demonstration of how to map between discourse markers and natural text. Give 3-5 examples.\n2. The actual generation prompt, which first asks the model to generate the discourse structure, and then the full response conditioned on that structure.\nFor baselines, use standard prompts without any discourse markers.",
            "Step 4: Select Models": "Evaluate the prompts on GPT-3.5 (text-davinci-002) and GPT-4 via the OpenAI API.",
            "Step 5: Generate Outputs": "For each task and prompt, generate outputs from the models. For the discourse coherence prompts, first generate the discourse structure, then the full response.",
            "Step 6: Evaluate Coherence": "Measure the coherence of the generated texts using a combination of automatic metrics and human evaluation:\n- Automatic metrics: DiscEval, GED, sentence topic flow (STF)\n- Human evaluation: Ask raters to judge the coherence, consistency, and single-topic flow of the generated texts on a 1-5 scale.\nCompare the coherence scores of the discourse coherence prompt outputs vs. the baselines.",
            "Step 7: Analyze Discourse Structures": "Qualitatively analyze the discourse structures generated by the models to understand what patterns lead to more or less coherent output. Look for things like:\n- Are the entity and coreference chains consistent and relevant?\n- Are the discourse relations reasonable and do they match the text?\n- Is the topic progression natural or are there abrupt shifts?\nUse these insights to refine the discourse marker set and prompts."
        },
        "Test Case Examples": {
            "Baseline Prompt Input": "Write a story about a haunted house.",
            "Baseline Prompt Expected Output": "The old mansion on the hill had been abandoned for years, but local legends spoke of the ghosts that haunted its halls. One summer, a group of teenagers decided to explore the house on a dare. As they crept through the dusty rooms, they heard strange creaks and whispers. Suddenly, a chill wind blew through the house, slamming doors and rattling windows. The friends ran for the exit, but found themselves trapped in a maze of twisting corridors. Panic set in as they realized they were not alone. Ghostly figures appeared in the shadows, reaching out with pale hands. The teenagers fled in terror, barely escaping with their lives. From that night on, no one dared to enter the haunted house again, leaving the spirits to their eternal unrest.",
            "Proposed Prompt Input": "Generate a discourse structure for a story about a haunted house, using these markers:\n<entity>: introduce a character or object\n<coref>: refer back to a previously mentioned entity\n<setting>: describe the location or time\n<event>: describe an action or happening\n<reaction>: describe a character's response\n<topic_shift>: introduce a new topic or scene\n<topic_continue>: elaborate on the current topic\n\nThen, write out the full story based on the discourse structure.",
            "Proposed Prompt Expected Output": "<setting> The old mansion on the hill, abandoned for years\n<entity1> Local legends\n<coref1> spoke of ghosts haunting <coref2>\n<topic_shift>\n<entity2> A group of teenagers \n<event1> <coref2> decide to explore <coref3> on a dare.\n<topic_continue>\n<event2> As <coref2> crept through the dusty rooms,\n<event3> <coref2> heard strange creaks and whispers. \n<event4> Suddenly, a chill wind blew through <coref4>, slamming doors and rattling windows.\n<reaction1> <coref2> ran for the exit in panic \n<event5> but found <coref5> trapped in a maze of twisting corridors.\n<topic_shift>\n<entity3> Ghostly figures \n<event6> <coref3> appeared in the shadows, reaching out with pale hands.\n<reaction2> <coref2> fled in terror, barely escaping with <coref6> lives.\n<topic_shift>\n<event7> From that night on, no one dared to enter <coref7> again,\n<event8> leaving the spirits to <coref8> eternal unrest.\n\nThe old mansion on the hill had been abandoned for years, but local legends spoke of the ghosts that haunted its halls. One summer, a group of teenagers decided to explore the house on a dare. \n\nAs they crept through the dusty rooms, they heard strange creaks and whispers. Suddenly, a chill wind blew through the house, slamming doors and rattling windows. The friends ran for the exit in panic, but found themselves trapped in a maze of twisting corridors. \n\nGhostly figures appeared in the shadows, reaching out with pale hands. The teenagers fled in terror, barely escaping with their lives.\n\nFrom that night on, no one dared to enter the haunted house again, leaving the spirits to their eternal unrest.",
            "Explanation": "The discourse coherence prompt guides the model to first lay out the key entities, events, and topic shifts in the story, creating a 'narrative skeleton'. This helps keep the full story on track and maintain coherence. The baseline prompt provides no such guidance, allowing the model to potentially lose focus."
        },
        "Fallback Plan": "If the proposed discourse coherence prompts do not show improvements over the baselines, we can:\n1. Analyze the generated discourse structures to see if they are relevant and well-formed. If not, the prompt format or discourse marker set may need to be refined.\n2. Check if the full text generation is properly conditioned on the discourse structure. If not, the model may be ignoring the structure and the prompting approach needs to be adjusted.\n3. Experiment with different sets of discourse markers based on other coherence frameworks beyond Centering Theory and RST.\n4. Collect a dataset of 'discourse-labeled' long-form text to further pre-train or fine-tune the model to better leverage the discourse structure.\n5. If the discourse structures look good but the model still struggles with long-form coherence, the problem may be more fundamental to the model architecture. In this case, the project could pivot to analyzing the limitations of the current models in terms of discourse-level coherence."
    }
}