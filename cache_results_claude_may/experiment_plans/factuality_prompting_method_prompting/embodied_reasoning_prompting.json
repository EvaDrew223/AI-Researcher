{
    "topic_description": "novel prompting methods that can improve factuality and reduce hallucination of large language models",
    "idea_name": "Embodied Reasoning Prompting",
    "raw_idea": {
        "Problem": "Language models can generate statements that are physically implausible or inconsistent with the constraints of the real world.",
        "Existing Methods": "Some methods incorporate physical knowledge bases or simulation engines to constrain generation.",
        "Motivation": "Language models have been shown to capture some physical and temporal commonsense knowledge. We can prompt them to reason step-by-step about the physical implications of their generations, grounding the process in embodied reasoning.",
        "Proposed Method": "The procedure is: 1) Given a prompt, generate an initial continuation. 2) Prompt the model to break down the generated continuation into a temporal sequence of physical events. 3) For each event, prompt the model to assess its physical plausibility, and revise if needed. 4) Prompt the model to re-compose the revised events into a coherent continuation. 5) Repeat steps 2-4 until the continuation is physically coherent.",
        "Experiment Plan": "Evaluate on physical reasoning datasets like PIQA and Physical IQA. Compare to baselines like direct prompting and KG-augmented generation. Metrics include physical plausibility scores and human evaluation of physical coherence."
    },
    "full_experiment_plan": {
        "Title": "Embodied Reasoning: Prompting Language Models for Physically Grounded Generation",
        "Problem Statement": "Language models can generate statements that are physically implausible or inconsistent with the constraints of the real world, such as objects defying gravity or characters teleporting.",
        "Motivation": "Existing methods to improve physical reasoning in language models typically incorporate external knowledge bases or simulation engines to constrain generation. However, these approaches can be computationally expensive and may not generalize well to open-ended scenarios. Recent work has shown that language models themselves can capture a significant amount of physical and temporal commonsense knowledge from pretraining. We propose to leverage this internal knowledge by prompting language models to reason step-by-step about the physical implications of their generations, grounding the process in 'embodied' reasoning.",
        "Proposed Method": "Given a prompt, the language model first generates an initial continuation. It is then prompted to break down the generated continuation into a temporal sequence of physical events. For each event, the model is prompted to assess its physical plausibility based on its understanding of real-world dynamics and revise the event if needed to maintain physical coherence. Finally, the model is prompted to re-compose the revised event sequence into an updated continuation. This process of decomposition, evaluation, revision, and re-composition is repeated iteratively until the model determines that the continuation is physically coherent.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "Evaluate the proposed method on established physical reasoning benchmarks like PIQA (Physical Interaction QA) and Physical IQA. These multiple-choice datasets test understanding of physical commonsense. Also evaluate on open-ended story generation datasets like WritingPrompts and BookCorpus, measuring the physical plausibility of generated continuations.",
            "Step 2: Construct Prompts": "For multiple-choice datasets, the baseline prompt provides the question and answer choices. The proposed prompt adds instructions for the model to reason about the physical interactions involved in each answer choice before selecting the most plausible one. For generation, the baseline prompt is the story opening. The proposed prompt iteratively guides the model to (1) continue the story, (2) break down the continuation into physical events, (3) evaluate and revise each event to maintain physical plausibility, and (4) integrate the revised events back into a coherent continuation. Repeat steps 2-4 until a stopping criteria is met (e.g. N iterations or no further revisions).",
            "Step 3: Select Models": "Evaluate the proposed method on GPT-3 (davinci), GPT-3.5 (text-davinci-002), and GPT-4 via the OpenAI API. For open-source models, experiment with LLaMA, GPT-J, and GPT-NeoX.",
            "Step 4: Get Results": "For multiple-choice, report accuracy of the baseline direct prompting vs. the proposed embodied reasoning prompting. For generation, sample N continuations per prompt for both baselines and proposed. Evaluate physical plausibility of generations using (1) automated metrics like the CUPS benchmark for identifying physically unlikely sentences and (2) human ratings on criteria like physical possibility, temporal coherence, and overall sensibility.",
            "Step 5: Analyze Results": "Compare performance of baselines vs. proposed method on multiple-choice and generation tasks. Analyze how the iterative reasoning process changes the physical plausibility of generations over multiple revision rounds. Qualitatively examine cases where the embodied reasoning helps correct initially implausible events. Ablate the different steps of the process (e.g. decomposition, evaluation, revision) to assess their individual contribution."
        },
        "Test Case Examples": {
            "Multiple-Choice Prompt Input": {
                "Baseline": "Question: To separate glass from paper, which would work better? \nChoices: (A) magnet (B) water (C) air blower",
                "Proposed": "Question: To separate glass from paper, which would work better? Think step-by-step about the physical interactions involved before choosing an answer. \nChoices: (A) magnet (B) water (C) air blower"
            },
            "Multiple-Choice Expected Output": {
                "Baseline": "(A) magnet",
                "Proposed": "(C) air blower. Reasoning: A magnet will not interact with either glass or paper since they are not magnetic materials. Water could make the paper heavier and cause it to sink, but the glass would also likely sink. An air blower could blow the lighter paper away while the heavier glass remains still, allowing them to be separated."
            },
            "Generation Prompt Input": {
                "Baseline": "Prompt: John was running late for work. He hurried out the door and jumped into his car. He turned the key but the engine wouldn't start. He tried again, pumping the gas pedal, but still nothing. Continuation: ",
                "Proposed": "Prompt: John was running late for work. He hurried out the door and jumped into his car. He turned the key but the engine wouldn't start. He tried again, pumping the gas pedal, but still nothing. \nContinuation: John hopped out of the car and popped open the hood. Black smoke billowed out. \nPhysical Events: 1) John exits the car 2) John opens the car hood 3) Black smoke emerges from under the hood \nEvaluation: Smoke suggests an issue with the engine, consistent with the car not starting. Plausible. \nRevision: No revision needed. \nContinuation: Waving away the smoke, John peered at the engine. A loose spark plug wire caught his eye. \nPhysical Events: 1) John waves his hand to clear the smoke 2) John looks at the car engine 3) John notices a loose spark plug wire \nEvaluation: A loose spark plug wire could prevent the engine from starting. Physically plausible. \nRevision: No revision needed. \nContinuation: John reached in and reconnected the wire. He closed the hood and got back in the driver's seat. \nPhysical Events: 1) John reconnects the spark plug wire 2) John closes the car hood 3) John sits back in the driver's seat \nEvaluation: Fixing the spark plug wire could allow the car to start. The sequence of actions is physically and temporally coherent. \nRevision: No further revisions. The continuation maintains physical plausibility. \nFinal Continuation:"
            },
            "Generation Expected Output": {
                "Baseline": "John hopped out of the car and popped open the hood. Black smoke billowed out. He reached in and fiddled with some wires but couldn't see the problem. Frustrated, he slammed the hood shut. Suddenly, the car roared to life on its own. Relieved, John jumped back in and sped off, the smoke trailing behind him as he raced to the office.",
                "Proposed": "John hopped out of the car and popped open the hood. Black smoke billowed out. Waving away the smoke, John peered at the engine. A loose spark plug wire caught his eye. John reached in and reconnected the wire. He closed the hood and got back in the driver's seat. With a hopeful breath, he turned the key. The engine sputtered, then roared to life. John grinned in relief and shifted into drive. He checked his watch as he pulled out of the driveway. If traffic was light, he might just make it to the office on time. As he accelerated down the street, the last wisps of smoke dissipated in his rearview mirror."
            },
            "Explanation": "The baseline generation contains some physically implausible elements, like the car starting on its own after slamming the hood. The proposed generation maintains better physical coherence by having John identify and fix a plausible issue (the spark plug wire), and the car only starts after this repair. The smoke dissipating as John drives away also follows logically from the engine issue being resolved."
        },
        "Fallback Plan": "If the proposed embodied reasoning prompting does not lead to significant improvements over the baselines, we can: (1) Analyze whether the model is accurately decomposing continuations into physical events and revising implausible ones. If not, experiment with alternative prompting strategies for these subtasks. (2) Collect human annotations on the physical plausibility of events in the training data and use these to finetune the model to more accurately assess and revise physical descriptions. (3) Investigate integrating external knowledge sources (e.g. knowledge bases, simulation engines) into the prompting pipeline to supplement the model's internal physical knowledge. (4) Explore alternative evaluation metrics that go beyond binary plausibility to assess more nuanced aspects of physical coherence (e.g. temporal consistency, object permanence). The project could then focus on error analysis, proposing refined metrics, and providing insights into the current challenges in physically grounded language generation."
    },
    "novelty_queries": [
        "KeywordQuery(\"language models physical reasoning prompting\")",
        "KeywordQuery(\"language models embodied reasoning\")",
        "KeywordQuery(\"language models physical commonsense generation\")",
        "KeywordQuery(\"language models iterative revision physical plausibility\")",
        "KeywordQuery(\"Embodied Reasoning Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "261549439aebdda72b648ecc462448fd24857ac1",
            "paperId": "261549439aebdda72b648ecc462448fd24857ac1",
            "title": "Progressive-Hint Prompting Improves Reasoning in Large Language Models",
            "abstract": "The performance of Large Language Models (LLMs) in reasoning tasks depends heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency being critical methods that enhance this ability. However, these methods do not fully exploit the answers generated by the LLM to guide subsequent responses. This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers. PHP is orthogonal to CoT and self-consistency, making it easy to combine with state-of-the-art techniques to further improve performance. We conducted extensive and comprehensive experiments on seven benchmarks. The results show that PHP significantly improves accuracy while remaining highly efficient. For instance, with text-davinci-003, we observed a 4.2% improvement on GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction in sample paths with self-consistency. With GPT-4 and PHP, we achieve state-of-the-art performances on SVAMP (89.1% ->91.9%), GSM8K (92% ->95.5%), AQuA (76.4% ->79.9%) and MATH (50.3% ->53.9%).",
            "year": 2023,
            "citationCount": 64,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is improving the physical reasoning capabilities of language models in open-ended generation. The proposed approach is to prompt the model to iteratively decompose the generated text into physical events, evaluate their plausibility, revise them if needed, and recompose the text.\n\nThe research problem in the paper is improving the reasoning abilities of language models on various benchmarks. The proposed approach is Progressive-Hint Prompting, which uses previously generated answers as hints to guide the model towards the correct answer over multiple interactions.\n\nThe proposal focuses specifically on physical reasoning for generation, while the paper addresses general reasoning tasks. The methods are also different, with the proposal using iterative decomposition and revision, and the paper using progressive hinting.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "5437e8adab596d7294124c0e798708e050e25321",
            "paperId": "5437e8adab596d7294124c0e798708e050e25321",
            "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
            "abstract": "Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99% using just 14 exemplars, compared to only 16% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.",
            "year": 2022,
            "citationCount": 581,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts."
            },
            "score": 8,
            "novelty_score": "The project proposal aims to improve the physical reasoning capabilities of language models in open-ended generation by prompting them to iteratively decompose, evaluate, and revise the physical plausibility of generated continuations. The paper focuses on improving the generalization ability of language models to solve more complex problems than those seen in prompts by breaking them down into simpler subproblems and solving them sequentially.\n\nThe project proposal and the paper address different research problems and propose different approaches. The project aims to enhance physical reasoning in language models, while the paper focuses on improving generalization to harder problems. The project uses an iterative prompting strategy for embodied reasoning, while the paper employs a least-to-most prompting strategy to break down complex problems into simpler subproblems.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
            "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
            "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
            "year": 2022,
            "citationCount": 3517,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the physical reasoning abilities of language models in open-ended generation. The proposed approach is to prompt the model to iteratively reason about the physical implications of its generated text and revise it to maintain coherence.\n\nThe research problem in the paper is improving the reasoning abilities of language models on complex tasks. The proposed approach is to provide chain of thought demonstrations as exemplars in prompting.\n\nWhile both works aim to elicit reasoning from language models, the proposal focuses specifically on physical reasoning for generation, while the paper addresses general reasoning across arithmetic, commonsense, and symbolic tasks. The methods also differ - iterative refinement based on physical constraints vs. chain of thought exemplars.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "62176de125738e3b95850d1227bac81fd646b78e",
            "paperId": "62176de125738e3b95850d1227bac81fd646b78e",
            "title": "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models",
            "abstract": "Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, Few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual efforts, Zero-shot-CoT concatenates the target problem statement with \u201cLet\u2019s think step by step\u201d as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.",
            "year": 2023,
            "citationCount": 115,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results over GPT-3 show that the proposed zero-shot prompting consistently outperforms Zero- shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the physical plausibility of language model generations by prompting models to iteratively decompose, evaluate, and revise continuations based on their physical and temporal coherence. The paper focuses on improving the reasoning abilities of language models in multi-step tasks by using a plan-and-solve prompting strategy to divide the task into subtasks and generate step-by-step solutions.\n\nThe project proposal tackles the problem of physically implausible language model outputs and proposes an embodied reasoning approach through iterative prompting. The paper, on the other hand, addresses the issue of reasoning errors in multi-step tasks and introduces a plan-and-solve prompting method to break down the task and generate step-by-step solutions.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "9efa81ec4954b0859c47dad8f42edfaf8bced69b",
            "paperId": "9efa81ec4954b0859c47dad8f42edfaf8bced69b",
            "title": "Boosting Language Models Reasoning with Chain-of-Knowledge Prompting",
            "abstract": "Recently, Chain-of-Thought (CoT) prompting has delivered success on complex reasoning tasks, which aims at designing a simple prompt like ``Let's think step by step'' or multiple in-context exemplars with well-designed rationales to elicit Large Language Models (LLMs) to generate intermediate reasoning steps. However, the generated rationales often come with mistakes, making unfactual and unfaithful reasoning chains. To mitigate this brittleness, we propose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple. This is inspired by our human behaviors, i.e., we can draw a mind map or knowledge map as the reasoning evidence in the brain before answering a complex question. Benefiting from CoK, we additionally introduce a F^2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness. For the unreliable response, the wrong evidence can be indicated to prompt the LLM to rethink. Extensive experiments demonstrate that our method can further improve the performance of commonsense, factual, symbolic, and arithmetic reasoning tasks.",
            "year": 2023,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel Chain-of-Knowledge prompting, where it aims at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple, and introduces a F^2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the physical reasoning capabilities of language models in open-ended generation. The proposed approach is to prompt the model to iteratively decompose the generated text into physical events, evaluate their plausibility, and revise them to maintain coherence.\n\nThe research problem in the paper is improving the reasoning capabilities of language models on complex tasks. The proposed approach is to prompt the model to generate explicit knowledge evidence in the form of structured triples, and to verify the factuality and faithfulness of the generated reasoning chains.\n\nWhile both works aim to improve the reasoning capabilities of language models, the proposal focuses specifically on physical reasoning for generation, while the paper addresses general complex reasoning tasks. The methods are also quite different, with the proposal using iterative event decomposition and revision, and the paper using knowledge triple generation and verification.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "be9447ccc05a0e8a07321272778c7574173cf00e",
            "paperId": "be9447ccc05a0e8a07321272778c7574173cf00e",
            "title": "Resprompt: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models",
            "abstract": "Chain-of-thought (CoT) prompting, which offers step-by-step problem-solving rationales, has impressively unlocked the reasoning potential of large language models (LLMs). Yet, the standard CoT is less effective in problems demanding multiple reasoning steps. This limitation arises from the complex reasoning process in multi-step problems: later stages often depend on the results of several steps earlier, not just the results of the immediately preceding step. Such complexities suggest the reasoning process is naturally represented as a graph. The almost linear and straightforward structure of CoT prompting, however, struggles to capture this complex reasoning graph. To address this challenge, we propose Residual Connection Prompting (RESPROMPT), a new prompting strategy that advances multi-step reasoning in LLMs. Our key idea is to reconstruct the reasoning graph within prompts. We achieve this by integrating necessary connections-links present in the reasoning graph but missing in the linear CoT flow-into the prompts. Termed\"residual connections\", these links are pivotal in morphing the linear CoT structure into a graph representation, effectively capturing the complex reasoning graphs inherent in multi-step problems. We evaluate RESPROMPT on six benchmarks across three diverse domains: math, sequential, and commonsense reasoning. For the open-sourced LLaMA family of models, RESPROMPT yields a significant average reasoning accuracy improvement of 12.5% on LLaMA-65B and 6.8% on LLaMA2-70B. Breakdown analysis further highlights RESPROMPT particularly excels in complex multi-step reasoning: for questions demanding at least five reasoning steps, RESPROMPT outperforms the best CoT based benchmarks by a remarkable average improvement of 21.1% on LLaMA-65B and 14.3% on LLaMA2-70B. Through extensive ablation studies and analyses, we pinpoint how to most effectively build residual connections.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes Residual Connection Prompting (RESPROMPT), a new prompting strategy that advances multi-step reasoning in LLMs by integrating necessary connections-links present in the reasoning graph but missing in the linear CoT flow-into the prompts."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the physical reasoning capabilities of language models in open-ended generation by prompting them to iteratively decompose, evaluate, and revise the physical plausibility of generated continuations. The paper focuses on enhancing multi-step reasoning in large language models for problems in math, sequential, and commonsense reasoning domains by introducing residual connection prompting to capture complex reasoning graphs.\n\nThe project proposal and the paper address different research problems and propose distinct approaches. The project targets physically grounded language generation, while the paper aims to improve multi-step reasoning across various domains. The project suggests an iterative prompting method for physical reasoning, whereas the paper introduces a new prompting strategy called RESPROMPT to reconstruct reasoning graphs.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
            "paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
            "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "abstract": "Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).",
            "year": 2022,
            "citationCount": 1396,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting that first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the physical reasoning capabilities of language models in open-ended generation by prompting them to iteratively decompose, evaluate, and revise the physical plausibility of generated continuations. The paper focuses on improving the reasoning abilities of language models on complex tasks like arithmetic and commonsense reasoning by using a self-consistency decoding strategy that samples multiple reasoning paths and selects the most consistent answer.\n\nThe project proposal and the paper have different research problems and approaches:\n- The project tackles physically grounded language generation, while the paper addresses complex reasoning tasks.\n- The project proposes an embodied reasoning prompting method, while the paper introduces a self-consistency decoding strategy.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "f3cf71c51b882fe3111d71c4bf104297d38197f8",
            "paperId": "f3cf71c51b882fe3111d71c4bf104297d38197f8",
            "title": "Inner Monologue: Embodied Reasoning through Planning with Language Models",
            "abstract": "Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent's own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high-level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.",
            "year": 2022,
            "citationCount": 483,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios, and finds that closed-loop language feedback significantly improves high-level instruction completion on three domains."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the physical plausibility of language model outputs by prompting them to iteratively reason about the physical implications of their generated continuations and revise them to maintain coherence with real-world dynamics. The approach involves decomposing generated text into physical events, evaluating their plausibility, and revising them as needed.\n\nThe paper abstract proposes using natural language feedback to enable language models to reason and plan in embodied environments, such as robotic control scenarios. The approach leverages closed-loop language feedback from various sources (e.g., success detection, scene description, human interaction) to form an inner monologue that improves high-level instruction completion.\n\nWhile both works involve language models and reasoning, the project proposal focuses on improving physical plausibility in text generation, while the paper abstract focuses on embodied reasoning for robotic control. The methods and application domains differ significantly.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "8199c9d55dd998f69f703e0ad250ca0697e3ad27",
            "paperId": "8199c9d55dd998f69f703e0ad250ca0697e3ad27",
            "title": "NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models",
            "abstract": "Trained with an unprecedented scale of data, large language models (LLMs) like ChatGPT and GPT-4 exhibit the emergence of significant reasoning abilities from model scaling. Such a trend underscored the potential of training LLMs with unlimited language data, advancing the development of a universal embodied agent. In this work, we introduce the NavGPT, a purely LLM-based instruction-following navigation agent, to reveal the reasoning capability of GPT models in complex embodied scenes by performing zero-shot sequential action prediction for vision-and-language navigation (VLN). At each step, NavGPT takes the textual descriptions of visual observations, navigation history, and future explorable directions as inputs to reason the agent's current status, and makes the decision to approach the target. Through comprehensive experiments, we demonstrate NavGPT can explicitly perform high-level planning for navigation, including decomposing instruction into sub-goals, integrating commonsense knowledge relevant to navigation task resolution, identifying landmarks from observed scenes, tracking navigation progress, and adapting to exceptions with plan adjustment. Furthermore, we show that LLMs is capable of generating high-quality navigational instructions from observations and actions along a path, as well as drawing accurate top-down metric trajectory given the agent's navigation history. Despite the performance of using NavGPT to zero-shot R2R tasks still falling short of trained models, we suggest adapting multi-modality inputs for LLMs to use as visual navigation agents and applying the explicit reasoning of LLMs to benefit learning-based models. Code is available at: https://github.com/GengzeZhou/NavGPT.",
            "year": 2023,
            "citationCount": 36,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The NavGPT is introduced, a purely LLM-based instruction-following navigation agent, to reveal the reasoning capability of GPT models in complex embodied scenes by performing zero-shot sequential action prediction for vision-and-language navigation (VLN)."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the physical reasoning capabilities of language models in open-ended generation tasks by prompting them to iteratively decompose, evaluate, and revise the physical plausibility of generated continuations. The paper focuses on using large language models to perform zero-shot sequential action prediction for vision-and-language navigation tasks.\n\nThe project proposal tackles physically grounded language generation, while the paper addresses vision-and-language navigation. Although both involve reasoning, the domains and approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "0935ce0adad57e1b24c50d793d46a407c3f563f3",
            "paperId": "0935ce0adad57e1b24c50d793d46a407c3f563f3",
            "title": "Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents",
            "abstract": "We investigate the challenge of task planning for multi-task embodied agents in open-world environments. Two main difficulties are identified: 1) executing plans in an open-world environment (e.g., Minecraft) necessitates accurate and multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla planners do not consider how easy the current agent can achieve a given sub-task when ordering parallel sub-goals within a complicated plan, the resulting plan could be inefficient or even infeasible. To this end, we propose\"$\\underline{D}$escribe, $\\underline{E}$xplain, $\\underline{P}$lan and $\\underline{S}$elect\"($\\textbf{DEPS}$), an interactive planning approach based on Large Language Models (LLMs). DEPS facilitates better error correction on initial LLM-generated $\\textit{plan}$ by integrating $\\textit{description}$ of the plan execution process and providing self-$\\textit{explanation}$ of feedback when encountering failures during the extended planning phases. Furthermore, it includes a goal $\\textit{selector}$, which is a trainable module that ranks parallel candidate sub-goals based on the estimated steps of completion, consequently refining the initial plan. Our experiments mark the milestone of the first zero-shot multi-task agent that can robustly accomplish 70+ Minecraft tasks and nearly double the overall performances. Further testing reveals our method's general effectiveness in popularly adopted non-open-ended domains as well (i.e., ALFWorld and tabletop manipulation). The ablation and exploratory studies detail how our design beats the counterparts and provide a promising update on the $\\texttt{ObtainDiamond}$ grand challenge with our approach. The code is released at https://github.com/CraftJarvis/MC-Planner.",
            "year": 2023,
            "citationCount": 163,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The ablation and exploratory studies detail how the design beats the counterparts and provide a promising update on the $\\texttt{ObtainDiamond}$ grand challenge with the MC-Planner approach."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the physical reasoning capabilities of language models in open-ended generation tasks by prompting them to iteratively decompose, evaluate, and revise the physical plausibility of generated continuations. The paper focuses on improving task planning for multi-task embodied agents in open-world environments like Minecraft by using an interactive planning approach based on large language models that integrates description, explanation, and goal selection.\n\nThe project proposal and the paper address different research problems and propose different approaches. The project proposal targets physical reasoning in language generation, while the paper focuses on task planning for embodied agents. The project proposal uses prompting techniques to guide the model's reasoning process, while the paper employs an interactive planning approach with description, explanation, and goal selection components.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "df710c46594c04fb59ef9a93d3b4e1cb387a1b2b",
            "paperId": "df710c46594c04fb59ef9a93d3b4e1cb387a1b2b",
            "title": "Embodied Task Planning with Large Language Models",
            "abstract": "Equipping embodied agents with commonsense is important for robots to successfully complete complex human instructions in general environments. Recent large language models (LLM) can embed rich semantic knowledge for agents in plan generation of complex tasks, while they lack the information about the realistic world and usually yield infeasible action sequences. In this paper, we propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planning with physical scene constraint, where the agent generates executable plans according to the existed objects in the scene by aligning LLMs with the visual perception models. Specifically, we first construct a multimodal dataset containing triplets of indoor scenes, instructions and action plans, where we provide the designed prompts and the list of existing objects in the scene for GPT-3.5 to generate a large number of instructions and corresponding planned actions. The generated data is leveraged for grounded plan tuning of pre-trained LLMs. During inference, we discover the objects in the scene by extending open-vocabulary object detectors to multi-view RGB images collected in different achievable locations. Experimental results show that the generated plan from our TaPA framework can achieve higher success rate than LLaVA and GPT-3.5 by a sizable margin, which indicates the practicality of embodied task planning in general and complex environments.",
            "year": 2023,
            "citationCount": 21,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results show that the generated plan from the TaPA framework can achieve higher success rate than LLaVA and GPT-3.5 by a sizable margin, which indicates the practicality of embodied task planning in general and complex environments."
            },
            "score": 7
        },
        {
            "id": "751c100802012764a2c45e17e41fa219867b12e5",
            "paperId": "751c100802012764a2c45e17e41fa219867b12e5",
            "title": "Better Zero-Shot Reasoning with Role-Play Prompting",
            "abstract": "Modern large language models (LLMs) exhibit a remarkable capacity for role-playing, enabling them to embody not only human characters but also non-human entities. This versatility allows them to simulate complex human-like interactions and behaviors within various contexts, as well as to emulate specific objects or systems. While these capabilities have enhanced user engagement and introduced novel modes of interaction, the influence of role-playing on LLMs' reasoning abilities remains underexplored. In this study, we introduce a strategically designed role-play prompting methodology and assess its performance under the zero-shot setting across twelve diverse reasoning benchmarks. Our empirical results illustrate that role-play prompting consistently surpasses the standard zero-shot approach across most datasets. Notably, in experiments conducted using ChatGPT, accuracy on AQuA rises from 53.5% to 63.8%, and on Last Letter from 23.8% to 84.2%.Upon further comparison with the Zero-Shot-CoT technique, which prompts the model to\"think step by step\", our study demonstrates that role-play prompting acts as a more effective trigger for the CoT process. This highlights its potential to augment the reasoning capabilities of LLMs. We release our code at https://github.com/NKU-HLT/Role-Play-Prompting.",
            "year": 2023,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study introduces a strategically designed role-play prompting methodology and assess its performance under the zero-shot setting across twelve diverse reasoning benchmarks, demonstrating that role-play prompting consistently surpasses the standard zero-shot approach across most datasets."
            },
            "score": 7
        },
        {
            "id": "7637ed79d30d0139901175ae4abedd822c217ab4",
            "paperId": "7637ed79d30d0139901175ae4abedd822c217ab4",
            "title": "3D-LLM: Injecting the 3D World into Large Language Models",
            "abstract": "Large language models (LLMs) and Vision-Language Models (VLMs) have been proven to excel at multiple tasks, such as commonsense reasoning. Powerful as these models can be, they are not grounded in the 3D physical world, which involves richer concepts such as spatial relationships, affordances, physics, layout, and so on. In this work, we propose to inject the 3D world into large language models and introduce a whole new family of 3D-LLMs. Speci\ufb01cally, 3D-LLMs can take 3D point clouds and their features as input and perform a diverse set of 3D-related tasks, including captioning, dense captioning, 3D question answering, task decomposition, 3D grounding, 3D-assisted dialog, navigation, and so on. Using three types of prompting mechanisms that we design, we are able to collect over 300k 3D-language data covering these tasks. To ef\ufb01ciently train 3D-LLMs, we \ufb01rst utilize a 3D feature extractor that obtains 3D features from rendered multi-view images. Then, we use 2D VLMs as our backbones to train our 3D-LLMs. By introducing a 3D localization mechanism, 3D-LLMs can better capture 3D spatial information. Experiments on ScanQA show that our model outperforms state-of-the-art baselines by a large margin ( e.g. , the BLEU-1 score surpasses state-of-the-art score by 9%). Furthermore, experiments on our held-in datasets for 3D captioning, task composition, and 3D-assisted dialogue show that our model outperforms 2D VLMs. Qualitative examples also show that our model could perform more tasks beyond the scope of existing LLMs and VLMs. Project Page: : https:",
            "year": 2023,
            "citationCount": 70,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes to inject the 3D world into large language models and introduce a whole new family of 3D-LLMs, and introduces a 3D localization mechanism, which can better capture 3D spatial information."
            },
            "score": 6
        },
        {
            "id": "f8b8f926bbfa327c86c40796131fe2695db81126",
            "paperId": "f8b8f926bbfa327c86c40796131fe2695db81126",
            "title": "DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models",
            "abstract": "A long-standing goal of AI systems is to perform complex multimodal reasoning like humans. Recently, large language models (LLMs) have made remarkable strides in such multi-step reasoning on the language modality solely by leveraging the chain of thought (CoT) to mimic human thinking. However, the transfer of these advancements to multimodal contexts introduces heightened challenges, including but not limited to the impractical need for labor-intensive annotation and the limitations in terms of flexibility, generalizability, and explainability. To evoke CoT reasoning in multimodality, this work first conducts an in-depth analysis of these challenges posed by multimodality and presents two key insights:\"keeping critical thinking\"and\"letting everyone do their jobs\"in multimodal CoT reasoning. Furthermore, this study proposes a novel DDCoT prompting that maintains a critical attitude through negative-space prompting and incorporates multimodality into reasoning by first dividing the reasoning responsibility of LLMs into reasoning and recognition and then integrating the visual recognition capability of visual models into the joint reasoning process. The rationales generated by DDCoT not only improve the reasoning abilities of both large and small language models in zero-shot prompting and fine-tuning learning, significantly outperforming state-of-the-art methods but also exhibit impressive generalizability and explainability.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study proposes a novel DDCoT prompting that maintains a critical attitude through negative-space prompting and incorporates multimodality into reasoning by first dividing the reasoning responsibility of LLMs into reasoning and recognition and then integrating the visual recognition capability of visual models into the joint reasoning process."
            },
            "score": 6
        },
        {
            "id": "6ad26eb2d2aa6679d16d9c16fb75cd2cbe1127bc",
            "paperId": "6ad26eb2d2aa6679d16d9c16fb75cd2cbe1127bc",
            "title": "See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning",
            "abstract": "Large pre-trained vision and language models have demonstrated remarkable capacities for various tasks. However, solving the knowledge-based visual reasoning tasks remains challenging, which requires a model to comprehensively understand image content, connect the external world knowledge, and perform step-by-step reasoning to answer the questions correctly. To this end, we propose a novel framework named Interactive Prompting Visual Reasoner (IPVR) for few-shot knowledge-based visual reasoning. IPVR contains three stages, see, think and confirm. The see stage scans the image and grounds the visual concept candidates with a visual perception model. The think stage adopts a pre-trained large language model (LLM) to attend to the key concepts from candidates adaptively. It then transforms them into text context for prompting with a visual captioning model and adopts the LLM to generate the answer. The confirm stage further uses the LLM to generate the supporting rationale to the answer, verify the generated rationale with a cross-modality classifier and ensure that the rationale can infer the predicted output consistently. We conduct experiments on a range of knowledge-based visual reasoning datasets. We found our IPVR enjoys several benefits, 1). it achieves better performance than the previous few-shot learning baselines; 2). it enjoys the total transparency and trustworthiness of the whole reasoning process by providing rationales for each reasoning step; 3). it is computation-efficient compared with other fine-tuning baselines.",
            "year": 2023,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel framework named Interactive Prompting Visual Reasoner (IPVR), which achieves better performance than the previous few-shot learning baselines, and enjoys the total transparency and trustworthiness of the whole reasoning process by providing rationales for each reasoning step."
            },
            "score": 6
        },
        {
            "id": "0875651b68e6602d45ae08bee67cf63c02faa512",
            "paperId": "0875651b68e6602d45ae08bee67cf63c02faa512",
            "title": "Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models",
            "abstract": "Large language models (LLMs) have scaled up to unlock a wide range of complex reasoning tasks with the aid of various prompting methods. However, current prompting methods generate natural language intermediate steps to help reasoning, which can cause imperfect task reduction and confusion. To mitigate such limitations, we explore code prompting, a neural symbolic prompting method with both zero-shot and few-shot versions which triggers code as intermediate steps. We conduct experiments on 7 widely-used benchmarks involving symbolic reasoning and arithmetic reasoning. Code prompting generally outperforms chain-of-thought (CoT) prompting. To further understand the performance and limitations of code prompting, we perform extensive ablation studies and error analyses, and identify several exclusive advantages of using symbolic promptings compared to natural language. We also consider the ensemble of code prompting and CoT prompting to combine the strengths of both. Finally, we show through experiments how code annotations and their locations affect code prompting.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores code prompting, a neural symbolic prompting method with both zero-shot and few-shot versions which triggers code as intermediate steps, and performs extensive ablation studies and error analyses and identifies several exclusive advantages of using symbolic promptings compared to natural language."
            },
            "score": 6
        },
        {
            "id": "c218cd1772999517b137bbbc9872c4f67e540b7f",
            "paperId": "c218cd1772999517b137bbbc9872c4f67e540b7f",
            "title": "OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models",
            "abstract": "We conduct a thorough investigation into the reasoning capabilities of Large Language Models (LLMs), focusing specifically on the Open Pretrained Transformers (OPT) models as a representative of such models. Our study entails finetuning three different sizes of OPT on a carefully curated reasoning corpus, resulting in two sets of finetuned models: OPT-R, finetuned without explanations, and OPT-RE, finetuned with explanations. We then evaluate all models on 57 out-of-domain tasks drawn from the Super-NaturalInstructions benchmark, covering 26 distinct reasoning skills, utilizing three prompting techniques. Through a comprehensive grid of 27 configurations and 6,156 test evaluations, we investigate the dimensions of finetuning, prompting, and scale to understand the role of explanations on different reasoning skills. Our findings reveal that having explanations in the fewshot exemplar has no significant impact on the model\u2019s performance when the model is finetuned, while positively affecting the non-finetuned counterpart. Moreover, we observe a slight yet consistent increase in classification accuracy as we incorporate explanations during prompting and finetuning, respectively. Finally, we offer insights on which reasoning skills benefit the most from incorporating explanations during finetuning and prompting, such as Numerical (+20.4%) and Analogical (+13.9%) reasoning, as well as skills that exhibit negligible or negative effects.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is revealed that having explanations in the fewshot exemplar has no significant impact on the model\u2019s performance when the model is finetuned, while positively affecting the non-finetuned counterpart, and a slight yet consistent increase in classification accuracy as the authors incorporate explanations during prompting and finetuning."
            },
            "score": 6
        },
        {
            "id": "102e4c860e39a2bfd7bf3f03b9ad69aac7bf3b5f",
            "paperId": "102e4c860e39a2bfd7bf3f03b9ad69aac7bf3b5f",
            "title": "Collaborating with language models for embodied reasoning",
            "abstract": "Reasoning in a complex and ambiguous environment is a key goal for Reinforcement Learning (RL) agents. While some sophisticated RL agents can successfully solve difficult tasks, they require a large amount of training data and often struggle to generalize to new unseen environments and new tasks. On the other hand, Large Scale Language Models (LSLMs) have exhibited strong reasoning ability and the ability to to adapt to new tasks through in-context learning. However, LSLMs do not inherently have the ability to interrogate or intervene on the environment. In this work, we investigate how to combine these complementary abilities in a single system consisting of three parts: a Planner, an Actor, and a Reporter. The Planner is a pre-trained language model that can issue commands to a simple embodied agent (the Actor), while the Reporter communicates with the Planner to inform its next command. We present a set of tasks that require reasoning, test this system's ability to generalize zero-shot and investigate failure cases, and demonstrate how components of this system can be trained with reinforcement-learning to improve performance.",
            "year": 2023,
            "citationCount": 40,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates how to combine complementary abilities in a single system consisting of a Planner, an Actor, and a Reporter, and presents a set of tasks that require reasoning, test this system's ability to generalize zero-shot and investigate failure cases, and demonstrates how components of this system can be trained with reinforcement-learning to improve performance."
            },
            "score": 6
        },
        {
            "id": "03251361c1d67c6b5badffc7059fdd7fbfea1fed",
            "paperId": "03251361c1d67c6b5badffc7059fdd7fbfea1fed",
            "title": "Statler: State-Maintaining Language Models for Embodied Reasoning",
            "abstract": "There has been a significant research interest in employing large language models to empower intelligent robots with complex reasoning. Existing work focuses on harnessing their abilities to reason about the histories of their actions and observations. In this paper, we explore a new dimension in which large language models may benefit robotics planning. In particular, we propose Statler, a framework in which large language models are prompted to maintain an estimate of the world state, which are often unobservable, and track its transition as new actions are taken. Our framework then conditions each action on the estimate of the current world state. Despite being conceptually simple, our Statler framework significantly outperforms strong competing methods (e.g., Code-as-Policies) on several robot planning tasks. Additionally, it has the potential advantage of scaling up to more challenging long-horizon planning tasks. We release our code at https://github.com/ripl/statler",
            "year": 2023,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Despite being conceptually simple, the Statler framework significantly outperforms strong competing methods (e.g., Code-as-Policies) on several robot planning tasks and has the potential advantage of scaling up to more challenging long-horizon planning tasks."
            },
            "score": 6
        },
        {
            "id": "587352c3b95c90de6d37f061c8e117f42be0b575",
            "paperId": "587352c3b95c90de6d37f061c8e117f42be0b575",
            "title": "Building Cooperative Embodied Agents Modularly with Large Language Models",
            "abstract": "In this work, we address challenging multi-agent cooperation problems with decentralized control, raw sensory observations, costly communication, and multi-objective tasks instantiated in various embodied environments. While previous research either presupposes a cost-free communication channel or relies on a centralized controller with shared observations, we harness the commonsense knowledge, reasoning ability, language comprehension, and text generation prowess of LLMs and seamlessly incorporate them into a cognitive-inspired modular framework that integrates with perception, memory, and execution. Thus building a Cooperative Embodied Language Agent CoELA, who can plan, communicate, and cooperate with others to accomplish long-horizon tasks efficiently. Our experiments on C-WAH and TDW-MAT demonstrate that CoELA driven by GPT-4 can surpass strong planning-based methods and exhibit emergent effective communication. Though current Open LMs like LLAMA-2 still underperform, we fine-tune a CoELA with data collected with our agents and show how they can achieve promising performance. We also conducted a user study for human-agent interaction and discovered that CoELA communicating in natural language can earn more trust and cooperate more effectively with humans. Our research underscores the potential of LLMs for future research in multi-agent cooperation. Videos can be found on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.",
            "year": 2023,
            "citationCount": 60,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experiments on C-WAH and TDW-MAT demonstrate that CoELA driven by GPT-4 can surpass strong planning-based methods and exhibit emergent effective communication, and it is discovered that CoELA communicating in natural language can earn more trust and cooperate more effectively with humans."
            },
            "score": 6
        },
        {
            "id": "6355483dee7c419f5294124bb35a8e5de4b406de",
            "paperId": "6355483dee7c419f5294124bb35a8e5de4b406de",
            "title": "Asking Before Acting: Gather Information in Embodied Decision Making with Language Models",
            "abstract": "With strong capabilities of reasoning and a broad understanding of the world, Large Language Models (LLMs) have demonstrated immense potential in building versatile embodied decision-making agents capable of executing a wide array of tasks. Nevertheless, when deployed in unfamiliar environments, we show that LLM agents encounter challenges in efficiently gathering essential information, leading to suboptimal performance. Conversely, human individuals often seek additional information from their peers prior to taking action, harnessing external knowledge to avoid unnecessary trial and error. Drawing inspiration from this behavior, we propose \\textit{Asking Before Acting} (ABA), a method that empowers the agent to proactively inquire with external sources for pertinent information using natural language during their interactions within the environment. In this way, the agent is able to enhance its efficiency and performance by circumventing potentially laborious steps and combating the difficulties associated with exploration in unfamiliar environments and vagueness of the instructions. We conduct extensive experiments involving a spectrum of environments including text-based household everyday tasks, robot arm manipulation tasks, and real world open domain image based embodied tasks. The experiments involve various models from Vicuna to GPT-4. The results demonstrate that, even with modest prompts modifications, ABA exhibits substantial advantages on both performance and efficiency over baseline LLM agents. Further finetuning ABA with reformulated metadata (ABA-FT) faciliates learning the rationale for asking and allows for additional enhancements especially in tasks that baselines struggle to solve.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes ABA, a method that empowers the agent to proactively inquire with external sources for pertinent information using natural language during their interactions within the environment, and demonstrates that, even with modest modifications, ABA exhibits substantial advantages on both performance and efficiency over baseline LLM agents."
            },
            "score": 6
        },
        {
            "id": "369b34826e23cb43bea9a91395e9603eacfa7420",
            "paperId": "369b34826e23cb43bea9a91395e9603eacfa7420",
            "title": "EgoPlan-Bench: Benchmarking Egocentric Embodied Planning with Multimodal Large Language Models",
            "abstract": "Multimodal Large Language Models, combining the remarkable reasoning and generalization capabilities of Large Language Models (LLMs) with the ability to comprehend visual inputs, have opened up new avenues for embodied task planning. Given diverse environmental inputs, including real-time task progress, visual observations, and open-form language instructions, a proficient task planner is expected to predict feasible actions, which is a feat inherently achievable by Multimodal Large Language Models (MLLMs). In this paper, we aim to quantitatively investigate the potential of MLLMs as embodied task planners in real-world scenarios by introducing a benchmark with human annotations named EgoPlan-Bench. Our benchmark is distinguished by realistic tasks derived from real-world videos, a diverse set of actions involving interactions with hundreds of different objects, and complex visual observations from varied scenes. We evaluate a wide range of MLLMs, revealing that these models have not yet evolved into embodied planning generalists (even GPT-4V). We further construct an instruction-tuning dataset EgoPlan-IT from videos with human-object interactions, to facilitate the learning of high-level task planning in intricate real-world situations. The experiment results demonstrate that the model tuned on EgoPlan-IT not only significantly improves performance on our benchmark, but can also be applied as a task planner for guiding embodied agents in simulations.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 6
        },
        {
            "id": "41531594d7e0f3b2e138ae43e0a0f6e24a9b014c",
            "paperId": "41531594d7e0f3b2e138ae43e0a0f6e24a9b014c",
            "title": "Code as Policies: Language Model Programs for Embodied Control",
            "abstract": "Large language models (LLMs) trained on code-completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g., from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions (\u2018faster\u2019) depending on context (i.e., behavioral commonsense). This paper presents Code as Policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io",
            "year": 2022,
            "citationCount": 386,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Code as Policies is presented, a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms."
            },
            "score": 6
        },
        {
            "id": "c82d8d80ea68400adb7faebb2f1cff38dd83093a",
            "paperId": "c82d8d80ea68400adb7faebb2f1cff38dd83093a",
            "title": "CAPE: Corrective Actions from Precondition Errors using Large Language Models",
            "abstract": "Extracting commonsense knowledge from a large language model (LLM) offers a path to designing intelligent robots. Existing approaches that leverage LLMs for planning are unable to recover when an action fails and often resort to retrying failed actions, without resolving the error's underlying cause. We propose a novel approach (CAPE) that attempts to propose corrective actions to resolve precondition errors during planning. CAPE improves the quality of generated plans by leveraging few-shot reasoning from action preconditions. Our approach enables embodied agents to execute more tasks than baseline methods while ensuring semantic correctness and minimizing re-prompting. In VirtualHome, CAPE generates executable plans while improving a human-annotated plan correctness metric from 28.89% to 49.63% over SayCan. Our improvements transfer to a Boston Dynamics Spot robot initialized with a set of skills (specified in language) and associated preconditions, where CAPE improves the correctness metric of the executed task plans by 76.49% compared to SayCan. Our approach enables the robot to follow natural language commands and robustly recover from failures, which baseline approaches largely cannot resolve or address inefficiently.",
            "year": 2022,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel approach (CAPE) that attempts to propose corrective actions to resolve precondition errors during planning and improves the quality of generated plans by leveraging few-shot reasoning from action preconditions."
            },
            "score": 6
        },
        {
            "id": "bee68767debbdc96d6f75947e544a8be98b869e3",
            "paperId": "bee68767debbdc96d6f75947e544a8be98b869e3",
            "title": "Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond",
            "abstract": "In this study, we explore the potential of Multimodal Large Language Models (MLLMs) in improving embodied decision-making processes for agents. While Large Language Models (LLMs) have been widely used due to their advanced reasoning skills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual understanding and reasoning capabilities. We investigate whether state-of-the-art MLLMs can handle embodied decision-making in an end-to-end manner and whether collaborations between LLMs and MLLMs can enhance decision-making. To address these questions, we introduce a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action. Additionally, we propose HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision-making. We compare end-to-end embodied decision-making and HOLMES on our benchmark and find that the GPT4-Vision model demonstrates strong end-to-end embodied decision-making abilities, outperforming GPT4-HOLMES in terms of average decision accuracy (+3%). However, this performance is exclusive to the latest GPT4-Vision model, surpassing the open-source state-of-the-art MLLM by 26%. Our results indicate that powerful MLLMs like GPT4-Vision hold promise for decision-making in embodied agents, offering new avenues for MLLM research. Code and data are open at https://github.com/pkunlp-icler/PCA-EVAL/.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study introduces a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action and proposes HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision- making."
            },
            "score": 6
        },
        {
            "id": "961a1772f3b90d9dffd2b571c6996007a1d0ccd1",
            "paperId": "961a1772f3b90d9dffd2b571c6996007a1d0ccd1",
            "title": "JARVIS: A Neuro-Symbolic Commonsense Reasoning Framework for Conversational Embodied Agents",
            "abstract": "Building a conversational embodied agent to execute real-life tasks has been a long-standing yet quite challenging research goal, as it requires effective human-agent communication, multi-modal understanding, long-range sequential decision making, etc. Traditional symbolic methods have scaling and generalization issues, while end-to-end deep learning models suffer from data scarcity and high task complexity, and are often hard to explain. To benefit from both worlds, we propose JARVIS, a neuro-symbolic commonsense reasoning framework for modular, generalizable, and interpretable conversational embodied agents. First, it acquires symbolic representations by prompting large language models (LLMs) for language understanding and sub-goal planning, and by constructing semantic maps from visual observations. Then the symbolic module reasons for sub-goal planning and action generation based on task- and action-level common sense. Extensive experiments on the TEACh dataset validate the efficacy and efficiency of our JARVIS framework, which achieves state-of-the-art (SOTA) results on all three dialog-based embodied tasks, including Execution from Dialog History (EDH), Trajectory from Dialog (TfD), and Two-Agent Task Completion (TATC) (e.g., our method boosts the unseen Success Rate on EDH from 6.1\\% to 15.8\\%). Moreover, we systematically analyze the essential factors that affect the task performance and also demonstrate the superiority of our method in few-shot settings. Our JARVIS model ranks first in the Alexa Prize SimBot Public Benchmark Challenge.",
            "year": 2022,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "JARVIS, a neuro-symbolic commonsense reasoning framework for modular, generalizable, and interpretable conversational embodied agents, is proposed, which achieves state-of-the-art (SOTA) results on all three dialog-based embodied tasks."
            },
            "score": 6
        },
        {
            "id": "2b6291eb76e2ff885238e94704bb795046d7d530",
            "paperId": "2b6291eb76e2ff885238e94704bb795046d7d530",
            "title": "SafeText: A Benchmark for Exploring Physical Safety in Language Models",
            "abstract": "Understanding what constitutes safe text is an important issue in natural language processing and can often prevent the deployment of models deemed harmful and unsafe. One such type of safety that has been scarcely studied is commonsense physical safety, i.e. text that is not explicitly violent and requires additional commonsense knowledge to comprehend that it leads to physical harm. We create the first benchmark dataset, SafeText, comprising real-life scenarios with paired safe and physically unsafe pieces of advice. We utilize SafeText to empirically study commonsense physical safety across various models designed for text generation and commonsense reasoning tasks. We find that state-of-the-art large language models are susceptible to the generation of unsafe text and have difficulty rejecting unsafe advice. As a result, we argue for further studies of safety and the assessment of commonsense physical safety in models before release.",
            "year": 2022,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that state-of-the-art large language models are susceptible to the generation of unsafe text and have difficulty rejecting unsafe advice, and is argued for further studies of safety and the assessment of commonsense physical safety in models before release."
            },
            "score": 6
        },
        {
            "id": "bb3d4b9f6991509194cffaa132ce4f1eb52141c6",
            "paperId": "bb3d4b9f6991509194cffaa132ce4f1eb52141c6",
            "title": "Room-Object Entity Prompting and Reasoning for Embodied Referring Expression",
            "abstract": "Given a high-level instruction, the task of Embodied Referring Expression (REVERIE) requires an embodied agent to localise a remote referred object via navigating in the unseen environment. Previous vision-language navigation methods utilise the provided fine-grained instruction as step-by-step navigation guidance to conduct strict instruction-following, while REVERIE aims to achieve efficient goal-oriented exploration according to the high-level command. In this work, we propose a Cross-modal Knowledge Reasoning (abbreviated as CKR+) framework, which incorporates the prior knowledge as decision guidance to learn the navigation scheme comprehensively. Specifically, we design a Room-Object Aware (ROA) mechanism to explicitly decouple the room- and object-related clues from instruction and visual observations. Moreover, we propose a Knowledge-enabled Entity Relation Reasoning (KERR+) module to leverage the structured knowledge from the knowledge graph explicitly and unstructured knowledge from pre-trained model implicitly, to learn the internal-external correlations among room- and object-entities for the agent to make proper decisions. We devise an Entity Prompter (EP) that embeds in the KERR+ module, which utilises the navigation history and visual entities as prompts to transfer knowledge from the pre-trained CLIP model. In addition, we develop a Reinforced End Decider (RED) to learn the stopping scheme specifically, which is achieved by a customised reinforcement learning strategy and knowledge enhanced matching. Two techniques are also introduced to improve navigation efficiency further. Extensive experiments conducted on the REVERIE benchmark demonstrate the effectiveness and superiority of our proposed methods, which boosts the key metrics, i.e., SPL and REVERIE-success rate, to 14.46% and 13.81% respectively.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a Cross-modal Knowledge Reasoning (abbreviated as CKR+) framework, which incorporates the prior knowledge as decision guidance to learn the navigation scheme comprehensively and designs a Room-Object Aware mechanism to explicitly decouple the room- and object-related clues from instruction and visual observations."
            },
            "score": 6
        },
        {
            "id": "7747ecbc26b1688e6cad1a6ce83914efa2a3c04c",
            "paperId": "7747ecbc26b1688e6cad1a6ce83914efa2a3c04c",
            "title": "Prompting Contrastive Explanations for Commonsense Reasoning Tasks",
            "abstract": "Many commonsense reasoning NLP tasks involve choosing between one or more possible answers to a question or prompt based on knowledge that is often implicit. Large pretrained language models (PLMs) can achieve near-human performance on such tasks, while providing little human-interpretable evidence of the underlying reasoning they use. In this work, we show how to use these same models to generate such evidence: inspired by the contrastive nature of human explanations, we use PLMs to complete explanation prompts which contrast alternatives according to the key attribute(s) required to justify the correct answer (for example, peanuts are usually salty while raisins are sweet). Conditioning model decisions on these explanations improves performance on two commonsense reasoning benchmarks, as compared to previous non-contrastive alternatives. These explanations are also judged by humans to be more relevant for solving the task, and facilitate a novel method to evaluate explanation faithfulfness.",
            "year": 2021,
            "citationCount": 48,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Inspired by the contrastive nature of human explanations, this work uses PLMs to complete explanation prompts which contrast alternatives according to the key attribute(s) required to justify the correct answer (for example, peanuts are usually salty while raisins are sweet)."
            },
            "score": 6
        },
        {
            "id": "6f3f1f6f90e74ba5936569a11fcc0cb29cdfee87",
            "paperId": "6f3f1f6f90e74ba5936569a11fcc0cb29cdfee87",
            "title": "Natural Language as Policies: Reasoning for Coordinate-Level Embodied Control with LLMs",
            "abstract": "We demonstrate experimental results with LLMs that address robotics task planning problems. Recently, LLMs have been applied in robotics task planning, particularly using a code generation approach that converts complex high-level instructions into mid-level policy codes. In contrast, our approach acquires text descriptions of the task and scene objects, then formulates task planning through natural language reasoning, and outputs coordinate level control commands, thus reducing the necessity for intermediate representation code as policies with pre-defined APIs. Our approach is evaluated on a multi-modal prompt simulation benchmark, demonstrating that our prompt engineering experiments with natural language reasoning significantly enhance success rates compared to its absence. Furthermore, our approach illustrates the potential for natural language descriptions to transfer robotics skills from known tasks to previously unseen tasks. The project website: https://natural-language-as-policies.github.io/",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work acquires text descriptions of the task and scene objects, then formulates task planning through natural language reasoning, and outputs coordinate level control commands, thus reducing the necessity for intermediate representation code as policies with pre-defined APIs."
            },
            "score": 6
        },
        {
            "id": "a26fa1983e4bc7c5b55cd5a1296afe6f876baa03",
            "paperId": "a26fa1983e4bc7c5b55cd5a1296afe6f876baa03",
            "title": "Neuro-Symbolic Integration Brings Causal and Reliable Reasoning Proofs",
            "abstract": "Though prompting LLMs with various reasoning structures produces reasoning proofs along with answers, these proofs are not ensured to be causal and reliable due to the inherent defects of LLMs. Tracking such deficiencies, we present a neuro-symbolic integration method, in which a neural LLM is used to represent the knowledge of the problem while an LLM-free symbolic solver is adopted to do deliberative reasoning using the knowledge. Specifically, our customized meta-interpreters allow the production of reasoning proofs and support flexible search strategies. These reasoning proofs are ensured to be causal and reliable because of the deterministic executing nature of the symbolic solvers. Empirically, on ProofWriter, our method surpasses the CoT baseline by nearly double in accuracy and more than triple in proof similarity. On GSM8K, our method also shows accuracy improvements and nearly doubled proof similarity. Our code is released at https://github.com/DAMO-NLP-SG/CaRing",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A neuro-symbolic integration method is presented, in which a neural LLM is used to represent the knowledge of the problem while an LLM-free symbolic solver is adopted to do deliberative reasoning using the knowledge."
            },
            "score": 6
        },
        {
            "id": "6f821d75968bc8de070af3ce5aa7f57bc031fafb",
            "paperId": "6f821d75968bc8de070af3ce5aa7f57bc031fafb",
            "title": "Language Models Meet World Models: Embodied Experiences Enhance Language Models",
            "abstract": "While large language models (LMs) have shown remarkable capabilities across numerous tasks, they often struggle with simple reasoning and planning in physical environments, such as understanding object permanence or planning household activities. The limitation arises from the fact that LMs are trained only on written text and miss essential embodied knowledge and skills. In this paper, we propose a new paradigm of enhancing LMs by finetuning them with world models, to gain diverse embodied knowledge while retaining their general language capabilities. Our approach deploys an embodied agent in a world model, particularly a simulator of the physical world (VirtualHome), and acquires a diverse set of embodied experiences through both goal-oriented planning and random exploration. These experiences are then used to finetune LMs to teach diverse abilities of reasoning and acting in the physical world, e.g., planning and completing goals, object permanence and tracking, etc. Moreover, it is desirable to preserve the generality of LMs during finetuning, which facilitates generalizing the embodied knowledge across tasks rather than being tied to specific simulations. We thus further introduce the classical (EWC) for selective weight updates, combined with low-rank adapters (LoRA) for training efficiency. Extensive experiments show our approach substantially improves base LMs on 18 downstream tasks by 64.28% on average. In particular, the small LMs (1.3B, 6B, and 13B) enhanced by our approach match or even outperform much larger LMs (e.g., ChatGPT).",
            "year": 2023,
            "citationCount": 32,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new paradigm of enhancing LMs by finetuning them with world models, to gain diverse embodied knowledge while retaining their general language capabilities, and introduces the classical for selective weight updates, combined with low-rank adapters (LoRA) for training efficiency."
            },
            "score": 5
        },
        {
            "id": "38fe8f324d2162e63a967a9ac6648974fc4c66f3",
            "paperId": "38fe8f324d2162e63a967a9ac6648974fc4c66f3",
            "title": "PaLM-E: An Embodied Multimodal Language Model",
            "abstract": "Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.",
            "year": 2023,
            "citationCount": 746,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts to enable general inference in the real world."
            },
            "score": 5
        },
        {
            "id": "80be1426825288ff876acb8cc0babcc6629fa644",
            "paperId": "80be1426825288ff876acb8cc0babcc6629fa644",
            "title": "AlphaBlock: Embodied Finetuning for Vision-Language Reasoning in Robot Manipulation",
            "abstract": "We propose a novel framework for learning high-level cognitive capabilities in robot manipulation tasks, such as making a smiley face using building blocks. These tasks often involve complex multi-step reasoning, presenting significant challenges due to the limited paired data connecting human instructions (e.g., making a smiley face) and robot actions (e.g., end-effector movement). Existing approaches relieve this challenge by adopting an open-loop paradigm decomposing high-level instructions into simple sub-task plans, and executing them step-by-step using low-level control models. However, these approaches are short of instant observations in multi-step reasoning, leading to sub-optimal results. To address this issue, we propose to automatically collect a cognitive robot dataset by Large Language Models (LLMs). The resulting dataset AlphaBlock consists of 35 comprehensive high-level tasks of multi-step text plans and paired observation sequences. To enable efficient data acquisition, we employ elaborated multi-round prompt designs that effectively reduce the burden of extensive human involvement. We further propose a closed-loop multi-modal embodied planning model that autoregressively generates plans by taking image observations as input. To facilitate effective learning, we leverage MiniGPT-4 with a frozen visual encoder and LLM, and finetune additional vision adapter and Q-former to enable fine-grained spatial perception for manipulation tasks. We conduct experiments to verify the superiority over existing open and closed-loop methods, and achieve a significant increase in success rate by 21.4% and 14.5% over ChatGPT and GPT-4 based robot tasks. Real-world demos are shown in https://www.youtube.com/watch?v=ayAzID1_qQk .",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel framework for learning high-level cognitive capabilities in robot manipulation tasks, such as making a smiley face using building blocks is proposed, and a closed-loop multi-modal embodied planning model that autoregressively generates plans by taking image observations as input is proposed."
            },
            "score": 5
        },
        {
            "id": "b6c8c1745a18d6e59c7a8a99f0df7aa4c18a1e73",
            "paperId": "b6c8c1745a18d6e59c7a8a99f0df7aa4c18a1e73",
            "title": "Octopus: Embodied Vision-Language Programmer from Environmental Feedback",
            "abstract": "Large vision-language models (VLMs) have achieved substantial progress in multimodal perception and reasoning. Furthermore, when seamlessly integrated into an embodied agent, it signifies a crucial stride towards the creation of autonomous and context-aware systems capable of formulating plans and executing commands with precision. In this paper, we introduce Octopus, a novel VLM designed to proficiently decipher an agent's vision and textual task objectives and to formulate intricate action sequences and generate executable code. Our design allows the agent to adeptly handle a wide spectrum of tasks, ranging from mundane daily chores in simulators to sophisticated interactions in complex video games. Octopus is trained by leveraging GPT-4 to control an explorative agent to generate training data, i.e., action blueprints and the corresponding executable code, within our experimental environment called OctoVerse. We also collect the feedback that allows the enhanced training scheme of Reinforcement Learning with Environmental Feedback (RLEF). Through a series of experiments, we illuminate Octopus's functionality and present compelling results, and the proposed RLEF turns out to refine the agent's decision-making. By open-sourcing our model architecture, simulator, and dataset, we aspire to ignite further innovation and foster collaborative applications within the broader embodied AI community.",
            "year": 2023,
            "citationCount": 19,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Octopus is a novel VLM designed to proficiently decipher an agent's vision and textual task objectives and to formulate intricate action sequences and generate executable code and the proposed RLEF turns out to refine the agent's decision-making."
            },
            "score": 5
        },
        {
            "id": "de4c362339afc7e070bd4250de3dcb06b32a46fc",
            "paperId": "de4c362339afc7e070bd4250de3dcb06b32a46fc",
            "title": "ThinkBot: Embodied Instruction Following with Thought Chain Reasoning",
            "abstract": "Embodied Instruction Following (EIF) requires agents to complete human instruction by interacting objects in complicated surrounding environments. Conventional methods directly consider the sparse human instruction to generate action plans for agents, which usually fail to achieve human goals because of the instruction incoherence in action descriptions. On the contrary, we propose ThinkBot that reasons the thought chain in human instruction to recover the missing action descriptions, so that the agent can successfully complete human goals by following the coherent instruction. Specifically, we first design an instruction completer based on large language models to recover the missing actions with interacted objects between consecutive human instruction, where the perceived surrounding environments and the completed sub-goals are considered for instruction completion. Based on the partially observed scene semantic maps, we present an object localizer to infer the position of interacted objects for agents to achieve complex human goals. Extensive experiments in the simulated environment show that our ThinkBot outperforms the state-of-the-art EIF methods by a sizable margin in both success rate and execution efficiency.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work designs an instruction completer based on large language models to recover the missing actions with interacted objects between consecutive human instruction, where the perceived surrounding environments and the completed sub-goals are considered for instruction completion."
            },
            "score": 5
        },
        {
            "id": "522240c89c7ac6b1365e8308c6a88c4784adc62e",
            "paperId": "522240c89c7ac6b1365e8308c6a88c4784adc62e",
            "title": "Improving Diversity of Commonsense Generation by Large Language Models via In-Context Learning",
            "abstract": "Generative Commonsense Reasoning (GCR) requires a model to reason about a situation using commonsense knowledge, while generating coherent sentences. Although the quality of the generated sentences is crucial, the diversity of the generation is equally important because it reflects the model's ability to use a range of commonsense knowledge facts. Large Language Models (LLMs) have shown proficiency in enhancing the generation quality across various tasks through in-context learning (ICL) using given examples without the need for any fine-tuning. However, the diversity aspect in LLM outputs has not been systematically studied before. To address this, we propose a simple method that diversifies the LLM generations, while preserving their quality. Experimental results on three benchmark GCR datasets show that our method achieves an ideal balance between the quality and diversity. Moreover, the sentences generated by our proposed method can be used as training data to improve diversity in existing commonsense generators.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A simple method is proposed that diversifies the LLM generations, while preserving their quality, and can be used as training data to improve diversity in existing commonsense generators."
            },
            "score": 5
        },
        {
            "id": "7d722ec75cf4cde30156e71fffec6f8f08f91600",
            "paperId": "7d722ec75cf4cde30156e71fffec6f8f08f91600",
            "title": "Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge",
            "abstract": "Large language models (LLMs) have been widely studied for their ability to store and utilize positive knowledge. However, negative knowledge, such as \u201clions don\u2019t live in the ocean\u201d, is also ubiquitous in the world but rarely mentioned explicitly in text.What do LLMs know about negative knowledge?This work examines the ability of LLMs on negative commonsense knowledge.We design a constrained keywords-to-sentence generation task (CG) and a Boolean question answering task (QA) to probe LLMs.Our experiments reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge, yet they can correctly answer polar yes-or-no questions.We term this phenomenon the belief conflict of LLMs.Our further analysis shows that statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict.",
            "year": 2023,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Results reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge, yet they can correctly answer polar yes-or-no questions, and analysis shows that statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict."
            },
            "score": 5
        },
        {
            "id": "04f4e55e14150b7c48b0287ba77c7443df76ed45",
            "paperId": "04f4e55e14150b7c48b0287ba77c7443df76ed45",
            "title": "PIQA: Reasoning about Physical Commonsense in Natural Language",
            "abstract": "To apply eyeshadow without a brush, should I use a cotton swab or a toothpick? Questions requiring this kind of physical commonsense pose a challenge to today's natural language understanding systems. While recent pretrained models (such as BERT) have made progress on question answering over more abstract domains \u2013 such as news articles and encyclopedia entries, where text is plentiful \u2013 in more physical domains, text is inherently limited due to reporting bias. Can AI systems learn to reliably answer physical commonsense questions without experiencing the physical world?In this paper, we introduce the task of physical commonsense reasoning and a corresponding benchmark dataset Physical Interaction: Question Answering or PIQA. Though humans find the dataset easy (95% accuracy), large pretrained models struggle (\u223c75%). We provide analysis about the dimensions of knowledge that existing models lack, which offers significant opportunities for future research.",
            "year": 2019,
            "citationCount": 724,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The task of physical commonsense reasoning and a corresponding benchmark dataset Physical Interaction: Question Answering or PIQA are introduced and analysis about the dimensions of knowledge that existing models lack are provided, which offers significant opportunities for future research."
            },
            "score": 5
        },
        {
            "id": "0885471c0215b3c0d31c82518066913f7f738128",
            "paperId": "0885471c0215b3c0d31c82518066913f7f738128",
            "title": "Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement",
            "abstract": "The ability to derive underlying principles from a handful of observations and then generalize to novel situations -- known as inductive reasoning -- is central to human intelligence. Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks. In this work, we conduct a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting. Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules. By examining the intermediate rules, we observe that LMs are phenomenal hypothesis proposers (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the proposed set of rules, this hybrid approach achieves strong results across inductive reasoning benchmarks that require inducing causal relations, language-like instructions, and symbolic concepts. However, they also behave as puzzling inductive reasoners, showing notable performance gaps between rule induction (i.e., identifying plausible rules) and rule application (i.e., applying proposed rules to instances), suggesting that LMs are proposing hypotheses without being able to actually apply the rules. Through empirical and human analyses, we further reveal several discrepancies between the inductive reasoning processes of LMs and humans, shedding light on both the potentials and limitations of using LMs in inductive reasoning tasks.",
            "year": 2023,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work conducts a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting."
            },
            "score": 5
        },
        {
            "id": "9ffefdf1fcd780cb71450b0a7a29247c66aa87be",
            "paperId": "9ffefdf1fcd780cb71450b0a7a29247c66aa87be",
            "title": "The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning",
            "abstract": "Does prompting a large language model (LLM) like GPT-3 with explanations improve in-context learning? We study this question on two NLP tasks that involve reasoning over text, namely question answering and natural language inference. We test the performance of four LLMs on three textual reasoning datasets using prompts that include explanations in multiple different styles. For these tasks, we find that including explanations in the prompts for OPT, GPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to moderate accuracy improvements over standard few-show learning. However, text-davinci-002 is able to benefit more substantially. We further show that explanations generated by the LLMs may not entail the models' predictions nor be factually grounded in the input, even on simple tasks with extractive explanations. However, these flawed explanations can still be useful as a way to verify LLMs' predictions post-hoc. Through analysis in our three settings, we show that explanations judged by humans to be good--logically consistent with the input and the prediction--more likely cooccur with accurate predictions. Following these observations, we train calibrators using automatically extracted scores that assess the reliability of explanations, allowing us to improve performance post-hoc across all of our datasets.",
            "year": 2022,
            "citationCount": 95,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work studies two NLP tasks that involve reasoning over text, namely question answering and natural language inference, and shows that explanations judged by humans to be good--logically consistent with the input and the prediction--more likely cooccur with accurate predictions."
            },
            "score": 5
        },
        {
            "id": "b908824639d18f11883abcab21efeb22e315ab9c",
            "paperId": "b908824639d18f11883abcab21efeb22e315ab9c",
            "title": "Multimodal Procedural Planning via Dual Text-Image Prompting",
            "abstract": "Embodied agents have achieved prominent performance in following human instructions to complete tasks. However, the potential of providing instructions informed by texts and images to assist humans in completing tasks remains underexplored. To uncover this capability, we present the multimodal procedural planning (MPP) task, in which models are given a high-level goal and generate plans of paired text-image steps, providing more complementary and informative guidance than unimodal plans. The key challenges of MPP are to ensure the informativeness, temporal coherence,and accuracy of plans across modalities. To tackle this, we propose Text-Image Prompting (TIP), a dual-modality prompting method that jointly leverages zero-shot reasoning ability in large language models (LLMs) and compelling text-to-image generation ability from diffusion-based models. TIP improves the interaction in the dual modalities using Text-to-Image Bridge and Image-to-Text Bridge, allowing LLMs to guide the textual-grounded image plan generation and leveraging the descriptions of image plans to ground the textual plan reversely. To address the lack of relevant datasets, we collect WIKIPLAN and RECIPEPLAN as a testbed for MPP. Our results show compelling human preferences and automatic scores against unimodal and multimodal baselines on WIKIPLAN and RECIPEPLAN in terms of informativeness, temporal coherence, and plan accuracy. Our code and data: https://github.com/YujieLu10/MPP.",
            "year": 2023,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Text-Image Prompting (TIP) is proposed, a dual-modality prompting method that jointly leverages zero-shot reasoning ability in large language models (LLMs) and compelling text-to-image generation ability from diffusion-based models to tackle the key challenges of MPP."
            },
            "score": 5
        },
        {
            "id": "32524aa3ae8522542753ed7e6f4cca3970e4acab",
            "paperId": "32524aa3ae8522542753ed7e6f4cca3970e4acab",
            "title": "Can an Embodied Agent Find Your \u201cCat-shaped Mug\u201d? LLM-Based Zero-Shot Object Navigation",
            "abstract": "We present language-guided exploration (LGX), a novel algorithm for Language-Driven Zero-Shot Object Goal Navigation (L-ZSON), where an embodied agent navigates to an uniquely described target object in a previously unseen environment. Our approach makes use of large language models (LLMs) for this task by leveraging the LLM's commonsense-reasoning capabilities for making sequential navigational decisions. Simultaneously, we perform generalized target object detection using a pre-trained Vision-Language grounding model. We achieve state-of-the-art zero-shot object navigation results on RoboTHOR with a success rate (SR) improvement of over 27% over the current baseline of the OWL-ViT CLIP on Wheels (OWL CoW). Furthermore, we study the usage of LLMs for robot navigation and present an analysis of various prompting strategies affecting the model output. Finally, we showcase the benefits of our approach via real-world experiments that indicate the superior performance of LGX in detecting and navigating to visually unique objects.",
            "year": 2023,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LGX is presented, a novel algorithm for Language-Driven Zero-Shot Object Goal Navigation (L-ZSON), where an embodied agent navigates to an uniquely described target object in a previously unseen environment by leveraging the LLM's commonsense-reasoning capabilities for making sequential navigational decisions."
            },
            "score": 5
        },
        {
            "id": "6ac627f57b26354ab537734d820da4a6a7dde2c6",
            "paperId": "6ac627f57b26354ab537734d820da4a6a7dde2c6",
            "title": "CLadder: Assessing Causal Reasoning in Language Models",
            "abstract": "The ability to perform causal reasoning is widely considered a core feature of intelligence. In this work, we investigate whether large language models (LLMs) can coherently reason about causality. Much of the existing work in natural language processing (NLP) focuses on evaluating commonsense causal reasoning in LLMs, thus failing to assess whether a model can perform causal inference in accordance with a set of well-defined formal rules. To address this, we propose a new NLP task, causal inference in natural language, inspired by the\"causal inference engine\"postulated by Judea Pearl et al. We compose a large dataset, CLadder, with 10K samples: based on a collection of causal graphs and queries (associational, interventional, and counterfactual), we obtain symbolic questions and ground-truth answers, through an oracle causal inference engine. These are then translated into natural language. We evaluate multiple LLMs on our dataset, and we introduce and evaluate a bespoke chain-of-thought prompting strategy, CausalCoT. We show that our task is highly challenging for LLMs, and we conduct an in-depth analysis to gain deeper insights into the causal reasoning abilities of LLMs. Our data is open-sourced at https://huggingface.co/datasets/causalNLP/cladder, and our code can be found at https://github.com/causalNLP/cladder.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates whether large language models (LLMs) can coherently reason about causality, and proposes a new NLP task, causal inference in natural language, inspired by the \"causal inference engine\" proposed by Judea Pearl et al."
            },
            "score": 5
        },
        {
            "id": "412fe1f135cb20c952962133ca1e534a71bfd27f",
            "paperId": "412fe1f135cb20c952962133ca1e534a71bfd27f",
            "title": "When Do Program-of-Thoughts Work for Reasoning?",
            "abstract": "In the realm of embodied artificial intelligence, the reasoning capabilities of Large Language Models (LLMs) play a pivotal role. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score CIRS, which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing and stratifying algorithm, and apply it to instruction generation for mathematical reasoning and code data filtering for code generation tasks. Extensive results demonstrates the effectiveness of our proposed approach.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes complexity-impacted reasoning score CIRS, which combines structural and logical attributes, to measure the correlation between code and reasoning abilities, and designs an auto-synthesizing and stratifying algorithm that applies to instruction generation for mathematical reasoning and code data filtering for code generation tasks."
            },
            "score": 5
        },
        {
            "id": "1e2eba005ccd8ab7a668a525c5b43245853bdaf1",
            "paperId": "1e2eba005ccd8ab7a668a525c5b43245853bdaf1",
            "title": "Reasoning in Large Language Models Through Symbolic Math Word Problems",
            "abstract": "Large language models (LLMs) have revolutionized NLP by solving downstream tasks with little to no labeled data. Despite their versatile abilities, the larger question of their ability to reason remains ill-understood. This paper addresses reasoning in math word problems (MWPs) by studying symbolic versions of the numeric problems, since a symbolic expression is a\"concise explanation\"of the numeric answer. We create and use a symbolic version of the SVAMP dataset and find that GPT-3's davinci-002 model also has good zero-shot accuracy on symbolic MWPs. To evaluate the faithfulness of the model's reasoning, we go beyond accuracy and additionally evaluate the alignment between the final answer and the outputted reasoning, which correspond to numeric and symbolic answers respectively for MWPs. We explore a self-prompting approach to encourage the symbolic reasoning to align with the numeric answer, thus equipping the LLM with the ability to provide a concise and verifiable reasoning and making it more interpretable. Surprisingly, self-prompting also improves the symbolic accuracy to be higher than both the numeric and symbolic accuracies, thus providing an ensembling effect. The SVAMP_Sym dataset will be released for future research on symbolic math problems.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A self-prompting approach is explored to encourage the symbolic reasoning to align with the numeric answer, thus equipping the LLM with the ability to provide a concise and verifiable reasoning and making it more interpretable."
            },
            "score": 5
        },
        {
            "id": "f30b720e34d405f200270a6ef2d09e98585fb4d1",
            "paperId": "f30b720e34d405f200270a6ef2d09e98585fb4d1",
            "title": "CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models",
            "abstract": "The ability to perform causal reasoning is widely considered a core feature of intelligence. In this work, we investigate whether large language models (LLMs) can coherently reason about causality. Much of the existing work in natural language processing (NLP) focuses on evaluating commonsense causal reasoning in LLMs, thus failing to assess whether a model can perform causal inference in accordance with a set of well-defined formal rules . To address this, we propose a new NLP task, causal inference in natural language , inspired by the \u201ccausal inference engine\u201d postulated by Judea Pearl et al. We compose a large dataset, CL ADDER , with 10K samples: based on a collection of causal graphs and queries (associational, interventional, and counterfactual), we obtain symbolic questions and ground-truth answers, through an oracle causal inference engine. These are then translated into natural language. We evaluate multiple LLMs on our dataset, and we introduce and evaluate a bespoke chain-of-thought prompting strategy, C AUSAL C O T. We show that our task is highly challenging for LLMs, and we conduct an in-depth analysis to gain deeper insight into the causal reasoning abilities of LLMs. 1",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates whether large language models (LLMs) can coherently reason about causality, and proposes a new NLP task, causal inference in natural language, inspired by the \u201ccausal inference engine\u201d postulated by Judea Pearl et al."
            },
            "score": 5
        },
        {
            "id": "e4be613cc875e61b8c1c6c60d958f1c20d12d6c0",
            "paperId": "e4be613cc875e61b8c1c6c60d958f1c20d12d6c0",
            "title": "Task and Motion Planning with Large Language Models for Object Rearrangement",
            "abstract": "Multi-object rearrangement is a crucial skill for service robots, and commonsense reasoning is frequently needed in this process. However, achieving commonsense arrangements requires knowledge about objects, which is hard to transfer to robots. Large language models (LLMs) are one potential source of this knowledge, but they do not naively capture information about plausible physical arrangements of the world. We propose LLM-GROP, which uses prompting to extract commonsense knowledge about semantically valid object configurations from an LLM and instantiates them with a task and motion planner in order to generalize to varying scene geometry. LLM-GROP allows us to go from natural-language commands to human-aligned object rearrangement in varied environments. Based on human evaluations, our approach achieves the highest rating while outperforming competitive baselines in terms of success rate while maintaining comparable cumulative action costs. Finally, we demonstrate a practical implementation of LLM-GROP on a mobile manipulator in real-world scenarios. Supplementary materials are available at: https://sites.google.com/view/llm-grop",
            "year": 2023,
            "citationCount": 80,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LLM-GROP is proposed, which uses prompting to extract commonsense knowledge about semantically valid object configurations from an LLM and instantiates them with a task and motion planner in order to generalize to varying scene geometry."
            },
            "score": 4
        },
        {
            "id": "748a2700ec11f51560a69ec05c67ca9f97014be7",
            "paperId": "748a2700ec11f51560a69ec05c67ca9f97014be7",
            "title": "EvEntS ReaLM: Event Reasoning of Entity States via Language Models",
            "abstract": "This paper investigates models of event implications. Specifically, how well models predict entity state-changes, by targeting their understanding of physical attributes. Nominally, Large Language models (LLM) have been exposed to procedural knowledge about how objects interact, yet our benchmarking shows they fail to reason about the world. Conversely, we also demonstrate that existing approaches often misrepresent the surprising abilities of LLMs via improper task encodings and that proper model prompting can dramatically improve performance of reported baseline results across multiple tasks. In particular, our results indicate that our prompting technique is especially useful for unseen attributes (out-of-domain) or when only limited data is available.",
            "year": 2022,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results indicate that the prompting technique is especially useful for unseen attributes (out-of-domain) or when only limited data is available and that proper model prompting can dramatically improve performance of reported baseline results across multiple tasks."
            },
            "score": 4
        },
        {
            "id": "4ebf49a7c053bf1d22fcce17bc8c80db827e8f99",
            "paperId": "4ebf49a7c053bf1d22fcce17bc8c80db827e8f99",
            "title": "Leveraging Commonsense Knowledge from Large Language Models for Task and Motion Planning",
            "abstract": "\u2014Multi-object rearrangement is a crucial skill for service robots, and commonsense reasoning is frequently needed in this process. However, achieving commonsense arrangements requires knowledge about objects, which is hard to transfer to robots. Large language models (LLMs) are one potential source of this knowledge, but they do not naively capture information about plausible physical arrangements of the world. We propose LLM-GROP, which uses prompting to extract commonsense knowledge about semantically valid object configurations from an LLM and instantiates them with a task and motion planner in order to generalize to varying scene geometry. LLM-GROP allows us to go from natural-language commands to human-aligned object rearrangement in varied environments. Based on human evaluations, our approach achieves the highest rating while outperforming competitive baselines in terms of success rate while maintaining comparable cumulative action costs. Finally, we demonstrate a practical implementation of LLM-GROP on a mobile manipulator in real-world scenarios. Supplementary materials are available at: https://sites.google.com/view/llm-grop",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LLM-GROP is proposed, which uses prompting to extract commonsense knowledge about semantically valid object configurations from an LLM and instantiates them with a task and motion planner in order to generalize to varying scene geometry."
            },
            "score": 4
        },
        {
            "id": "5c828011a508611df4d58cced9cc48d049dc4eb9",
            "paperId": "5c828011a508611df4d58cced9cc48d049dc4eb9",
            "title": "A Data Source for Reasoning Embodied Agents",
            "abstract": "Recent progress in using machine learning models for reasoning tasks has been driven by novel model architectures, large-scale pre-training protocols, and dedicated reasoning datasets for fine-tuning. In this work, to further pursue these advances, we introduce a new data generator for machine reasoning that integrates with an embodied agent. The generated data consists of templated text queries and answers, matched with world-states encoded into a database. The world-states are a result of both world dynamics and the actions of the agent. We show the results of several baseline models on instantiations of train sets. These include pre-trained language models fine-tuned on a text-formatted representation of the database, and graph-structured Transformers operating on a knowledge-graph representation of the database. We find that these models can answer some questions about the world-state, but struggle with others. These results hint at new research directions in designing neural reasoning models and database representations. Code to generate the data and train the models will be released at github.com/facebookresearch/neuralmemory",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a new data generator for machine reasoning that integrates with an embodied agent, and shows the results of several baseline models on instantiations of train sets that can answer some questions about the world-state, but struggle with others."
            },
            "score": 4
        },
        {
            "id": "c4b7f1ceef3f91619e80a040dbc5a9fdbd32ab22",
            "paperId": "c4b7f1ceef3f91619e80a040dbc5a9fdbd32ab22",
            "title": "Physical Reasoning and Object Planning for Household Embodied Agents",
            "abstract": "In this study, we explore the sophisticated domain of task planning for robust household embodied agents, with a particular emphasis on the intricate task of selecting substitute objects. We introduce the CommonSense Object Affordance Task (COAT), a novel framework designed to analyze reasoning capabilities in commonsense scenarios. This approach is centered on understanding how these agents can effectively identify and utilize alternative objects when executing household tasks, thereby offering insights into the complexities of practical decision-making in real-world environments.Drawing inspiration from human decision-making, we explore how large language models tackle this challenge through three meticulously crafted commonsense question-and-answer datasets, featuring refined rules and human annotations. Our evaluation of state-of-the-art language models on these datasets sheds light on three pivotal considerations: 1) aligning an object's inherent utility with the task at hand, 2) navigating contextual dependencies (societal norms, safety, appropriateness, and efficiency), and 3) accounting for the current physical state of the object. To maintain accessibility, we introduce five abstract variables reflecting an object's physical condition, modulated by human insights to simulate diverse household scenarios. Our contributions include insightful Object-Utility mappings addressing the first consideration and two extensive QA datasets (15k and 130k questions) probing the intricacies of contextual dependencies and object states. The datasets, along with our findings, are accessible at: \\url{https://github.com/com-phy-affordance/COAT}. This research not only advances our understanding of physical commonsense reasoning in language models but also paves the way for future improvements in household agent intelligence.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The CommonSense Object Affordance Task (COAT), a novel framework designed to analyze reasoning capabilities in commonsense scenarios, is introduced, thereby offering insights into the complexities of practical decision-making in real-world environments."
            },
            "score": 4
        },
        {
            "id": "c38998129e8383232b02c4e6f76c663da14b9167",
            "paperId": "c38998129e8383232b02c4e6f76c663da14b9167",
            "title": "The role of \ufb01ne-tuning method in commonsense generation with pretrained language models",
            "abstract": "In this paper we study different methods for 001 \ufb01ne-tuning a pre-trained language model to 002 generate commonsense knowledge. We par-003 ticularly focus on the T5 model in few-shot 004 settings. This model can be trained in unsu-005 pervised and supervised fashions. We investi-006 gate the effect of each type of these \ufb01ne-tuning 007 methods on the performance of the model. The 008 results shows that the unsupervised method 009 can generate more diverse and novel knowl-010 edge by relying more on the stored knowl-011 edge of the model. We also analyze parame-012 ter changes during few-shot \ufb01ne-tuning to gain 013 more insights on the way the model learns us-014 ing these methods",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results shows that the unsupervised method 009 can generate more diverse and novel knowl-010 edge by relying more on the stored knowl-011 edge of the model."
            },
            "score": 4
        },
        {
            "id": "af7503c8afb957edbf6f196f3821a8cd79bfadf6",
            "paperId": "af7503c8afb957edbf6f196f3821a8cd79bfadf6",
            "title": "Data-Efficient Concept Extraction from Pre-trained Language Models for Commonsense Explanation Generation",
            "abstract": "Predicting the key explanation concept is essen-tial for generating commonsense explanations. This paper introduces a method to predict the concept from pre-trained language models for commonsense explanation generation. Our experiment found that adopting a language model as the concept extractor and fine-tuning it with 20% training data can improve the quality and accuracy of the generated explanations over multiple evaluation metrics. Compared with conventional methods that search concepts over knowledge graphs, our method does not re-quire the preparation and training models to search through knowledge graphs. To better un-derstand the results from pre-trained language models, we also designed a metric to evaluate the retrieved concepts. Through analysis and experiments, we show the correlation between this metric and the performance of the generators, and we also show the importance of attaching concepts for generating high-quality sentences.",
            "year": 2022,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A method to predict the concept from pre-trained language models for commonsense explanation generation by designing a metric to evaluate the retrieved concepts and showing the correlation between this metric and the performance of the generators, and the importance of attaching concepts for generating high-quality sentences."
            },
            "score": 4
        },
        {
            "id": "39e40821b7207125e54e6ed7112e55cd38c6f0c3",
            "paperId": "39e40821b7207125e54e6ed7112e55cd38c6f0c3",
            "title": "Language Models of Code are Few-Shot Commonsense Learners",
            "abstract": "We address the general task of structured commonsense reasoning: given a natural language input, the goal is to generate a graph such as an event or a reasoning-graph.To employ large language models (LMs) for this task, existing approaches \u2018serialize\u2019 the output graph as a flat list of nodes and edges.Although feasible, these serialized graphs strongly deviate from the natural language corpora that LMs were pre-trained on, hindering LMs from generating them correctly. In this paper, we show that when we instead frame structured commonsense reasoning tasks as code generation tasks, pre-trained LMs of code are better structured commonsense reasoners than LMs of natural language, even when the downstream task does not involve source code at all.We demonstrate our approach across three diverse structured commonsense reasoning tasks. In all these natural language tasks, we show that using our approach, a code generation LM (codex) outperforms natural-LMs that are fine-tuned on the target task (T5) and other strong LMs such as GPT-3 in the few-shot setting.",
            "year": 2022,
            "citationCount": 119,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper shows that when this task is frame as code generation tasks, pre-trained LMs of code are better structured commonsense reasoners than L Ms of natural language, even when the downstream task does not involve source code at all."
            },
            "score": 4
        },
        {
            "id": "82232e09d629235e62f6163c813cbebefad1807c",
            "paperId": "82232e09d629235e62f6163c813cbebefad1807c",
            "title": "Negated Complementary Commonsense using Large Language Models",
            "abstract": "Larger language models, such as GPT-3, have shown to be excellent in many tasks. However, we demonstrate that out-of-ordinary questions can throw the model off guard. This work focuses on finding answers to negated complementary questions in commonsense scenarios. We illustrate how such questions adversely affect the model responses. We propose a model-agnostic methodology to improve the performance in negated complementary scenarios. Our method outperforms few-shot generation from GPT-3 (by more than 11 points) and, more importantly, highlights the significance of studying the response of large language models in negated complementary questions. The code, data, and experiments are available under: https://github.com/navidre/negated_complementary_commonsense.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a model-agnostic methodology that outperforms few-shot generation from GPT-3 (by more than 11 points) and highlights the significance of studying the response of large language models in negated complementary questions."
            },
            "score": 4
        },
        {
            "id": "0289c15ca813f13491ad1e1fb0036b103f0eb9fb",
            "paperId": "0289c15ca813f13491ad1e1fb0036b103f0eb9fb",
            "title": "Large Language Models Are Also Good Prototypical Commonsense Reasoners",
            "abstract": "Commonsense reasoning is a pivotal skill for large language models, yet it presents persistent challenges in specific tasks requiring this competence. Traditional fine-tuning approaches can be resource-intensive and potentially compromise a model's generalization capacity. Furthermore, state-of-the-art language models like GPT-3.5 and Claude are primarily accessible through API calls, which makes fine-tuning models challenging. To address these challenges, we draw inspiration from the outputs of large models for tailored tasks and semi-automatically developed a set of novel prompts from several perspectives, including task-relevance, supportive evidence generation (e.g. chain-of-thought and knowledge), diverse path decoding to aid the model. Experimental results on ProtoQA dataset demonstrate that with better designed prompts we can achieve the new state-of-art(SOTA) on the ProtoQA leaderboard, improving the Max Answer@1 score by 8%, Max Incorrect@1 score by 4% (breakthrough 50% for the first time) compared to the previous SOTA model and achieved an improvement on StrategyQA and CommonsenseQA2.0 (3% and 1%, respectively). Furthermore, with the generated Chain-of-Thought and knowledge, we can improve the interpretability of the model while also surpassing the previous SOTA models. We hope that our work can provide insight for the NLP community to develop better prompts and explore the potential of large language models for more complex reasoning tasks.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work semi-automatically developed a set of novel prompts from several perspectives, including task-relevance, supportive evidence generation, and diverse path decoding to aid the model, and can improve the interpretability of the model while also surpassing the previous SOTA models."
            },
            "score": 4
        },
        {
            "id": "ff4f3ac67c6d7d7da3d0f27ddafc0f9552a29b00",
            "paperId": "ff4f3ac67c6d7d7da3d0f27ddafc0f9552a29b00",
            "title": "VLIS: Unimodal Language Models Guide Multimodal Language Generation",
            "abstract": "Multimodal language generation, which leverages the synergy of language and vision, is a rapidly expanding field. However, existing vision-language models face challenges in tasks that require complex linguistic understanding. To address this issue, we introduce Visual-Language models as Importance Sampling weights (VLIS), a novel framework that combines the visual conditioning capability of vision-language models with the language understanding of unimodal text-only language models without further training. It extracts pointwise mutual information of each image and text from a visual-language model and uses the value as an importance sampling weight to adjust the token likelihood from a text-only model. VLIS improves vision-language models on diverse tasks, including commonsense understanding (WHOOPS, OK-VQA, and ScienceQA) and complex text generation (Concadia, Image Paragraph Captioning, and ROCStories). Our results suggest that VLIS represents a promising new direction for multimodal language generation.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces Visual-Language models as Importance Sampling weights (VLIS), a novel framework that combines the visual conditioning capability of vision-language models with the language understanding of unimodal text-only language models without further training."
            },
            "score": 4
        },
        {
            "id": "10a88301165b5682836d5d7405c424bcaf4f9942",
            "paperId": "10a88301165b5682836d5d7405c424bcaf4f9942",
            "title": "COGEN: Abductive Commonsense Language Generation",
            "abstract": "Reasoning is one of the most important elements in achieving Artificial General Intelligence (AGI), specifically when it comes to Abductive and counterfactual reasoning. In order to introduce these capabilities of reasoning in Natural Language Processing (NLP) models, there have been recent advances towards training NLP models to better perform on two main tasks - Abductive Natural Language Inference (alphaNLI) and Abductive Natural Language Generation Task (alphaNLG). This paper proposes CoGen, a model for both alphaNLI and alphaNLG tasks that employ a novel approach of combining the temporal commonsense reasoning for each observation (before and after a real hypothesis) from pre-trained models with contextual filtering for training. Additionally, we use state-of-the-art semantic entailment to filter out the contradictory hypothesis during the inference. Our experimental results show that CoGen outperforms current models and set a new state of the art in regards to alphaNLI and alphaNLG tasks. We make the source code of CoGen model publicly available for reproducibility and to facilitate relevant future research.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 4
        },
        {
            "id": "3b508a48a4b48d2a16dd790a2a04ffcf51c0b4a6",
            "paperId": "3b508a48a4b48d2a16dd790a2a04ffcf51c0b4a6",
            "title": "SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models",
            "abstract": "Diffusion models, which have emerged to become popular text-to-image generation models, can produce high-quality and content-rich images guided by textual prompts. However, there are limitations to semantic understanding and commonsense reasoning in existing models when the input prompts are concise narrative, resulting in low-quality image generation. To improve the capacities for narrative prompts, we propose a simple-yet-effective parameter-efficient fine-tuning approach called the Semantic Understanding and Reasoning adapter (SUR-adapter) for pre-trained diffusion models. To reach this goal, we first collect and annotate a new dataset SURD which consists of more than 57,000 semantically corrected multi-modal samples. Each sample contains a simple narrative prompt, a complex keyword-based prompt, and a high-quality image. Then, we align the semantic representation of narrative prompts to the complex prompts and transfer knowledge of large language models (LLMs) to our SUR-adapter via knowledge distillation so that it can acquire the powerful semantic understanding and reasoning capabilities to build a high-quality textual semantic representation for text-to-image generation. We conduct experiments by integrating multiple LLMs and popular pre-trained diffusion models to show the effectiveness of our approach in enabling diffusion models to understand and reason concise natural language without image quality degradation. Our approach can make text-to-image diffusion models easier to use with better user experience, which demonstrates our approach has the potential for further advancing the development of user-friendly text-to-image generation models by bridging the semantic gap between simple narrative prompts and complex keyword-based prompts. The code is released at https://github.com/Qrange-group/SUR-adapter.",
            "year": 2023,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a simple-yet-effective parameter-efficient fine-tuning approach called the Semantic Understanding and Reasoning adapter (SUR-adapter) for pre-trained diffusion models that can make text-to-image diffusion models easier to use with better user experience."
            },
            "score": 4
        },
        {
            "id": "1244287f66a38c6cf6dfbe18f22ccbc1792433e1",
            "paperId": "1244287f66a38c6cf6dfbe18f22ccbc1792433e1",
            "title": "Read, Revise, Repeat: A System Demonstration for Human-in-the-loop Iterative Text Revision",
            "abstract": "Revision is an essential part of the human writing process. It tends to be strategic, adaptive, and, more importantly, iterative in nature. Despite the success of large language models on text revision tasks, they are limited to non-iterative, one-shot revisions. Examining and evaluating the capability of large language models for making continuous revisions and collaborating with human writers is a critical step towards building effective writing assistants. In this work, we present a human-in-the-loop iterative text revision system, Read, Revise, Repeat (R3), which aims at achieving high quality text revisions with minimal human efforts by reading model-generated revisions and user feedbacks, revising documents, and repeating human-machine interactions. In R3, a text revision model provides text editing suggestions for human writers, who can accept or reject the suggested edits. The accepted edits are then incorporated into the model for the next iteration of document revision. Writers can therefore revise documents iteratively by interacting with the system and simply accepting/rejecting its suggested edits until the text revision model stops making further revisions or reaches a predefined maximum number of revisions. Empirical experiments show that R3 can generate revisions with comparable acceptance rate to human writers at early revision depths, and the human-machine interaction can get higher quality revisions with fewer iterations and edits. The collected human-model interaction dataset and system code are available at https://github.com/vipulraheja/IteraTeR. Our system demonstration is available at https://youtu.be/lK08tIpEoaE.",
            "year": 2022,
            "citationCount": 37,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents a human-in-the-loop iterative text revision system, Read, Revise, Repeat (R3), which aims at achieving high quality text revisions with minimal human efforts by reading model-generated revisions and user feedbacks, revising documents, and repeating human-machine interactions."
            },
            "score": 4
        },
        {
            "id": "59b71e2a248d67a2692bc7e35faa504ee2dbc98d",
            "paperId": "59b71e2a248d67a2692bc7e35faa504ee2dbc98d",
            "title": "On Grounded Planning for Embodied Tasks with Language Models",
            "abstract": "Language models (LMs) have demonstrated their capability in possessing commonsense knowledge of the physical world, a crucial aspect of performing tasks in everyday life. However, it remains unclear whether they have the capacity to generate grounded, executable plans for embodied tasks. This is a challenging task as LMs lack the ability to perceive the environment through vision and feedback from the physical environment. In this paper, we address this important research question and present the first investigation into the topic. Our novel problem formulation, named G-PlanET, inputs a high-level goal and a data table about objects in a specific environment, and then outputs a step-by-step actionable plan for a robotic agent to follow. To facilitate the study, we establish an evaluation protocol and design a dedicated metric, KAS, to assess the quality of the plans. Our experiments demonstrate that the use of tables for encoding the environment and an iterative decoding strategy can significantly enhance the LMs' ability in grounded planning. Our analysis also reveals interesting and non-trivial findings.",
            "year": 2022,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper addresses the question of whether language models have the capacity to generate grounded, executable plans for embodied tasks and demonstrates that the use of tables for encoding the environment and an iterative decoding strategy can significantly enhance the LMs' ability in grounded planning."
            },
            "score": 4
        },
        {
            "id": "0f9995ec08e95bea09d512c59e40d19f0f44d7bb",
            "paperId": "0f9995ec08e95bea09d512c59e40d19f0f44d7bb",
            "title": "Data-Efficient Alignment of Large Language Models with Human Feedback Through Natural Language",
            "abstract": "Learning from human feedback is a prominent technique to align the output of large language models (LLMs) with human expectations. Reinforcement learning from human feedback (RLHF) leverages human preference signals that are in the form of ranking of response pairs to perform this alignment. However, human preference on LLM outputs can come in much richer forms including natural language, which may provide detailed feedback on strengths and weaknesses of a given response. In this work we investigate data efficiency of modeling human feedback that is in natural language. Specifically, we fine-tune an open-source LLM, e.g., Falcon-40B-Instruct, on a relatively small amount (1000 records or even less) of human feedback in natural language in the form of critiques and revisions of responses. We show that this model is able to improve the quality of responses from even some of the strongest LLMs such as ChatGPT, BARD, and Vicuna, through critique and revision of those responses. For instance, through one iteration of revision of ChatGPT responses, the revised responses have 56.6% win rate over the original ones, and this win rate can be further improved to 65.9% after applying the revision for five iterations.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work fine-tune an open-source LLM on a relatively small amount of human feedback in natural language in the form of critiques and revisions of responses, and shows that this model is able to improve the quality of responses from even some of the strongest LLMs such as ChatGPT, BARD, and Vicuna, through critique and revision of those responses."
            },
            "score": 4
        },
        {
            "id": "6b0c3d5118ba03a35dc965129ea37cc32a4e8af2",
            "paperId": "6b0c3d5118ba03a35dc965129ea37cc32a4e8af2",
            "title": "SEED: Customize Large Language Models with Sample-Efficient Adaptation for Code Generation",
            "abstract": "Although Large Language Models (LLMs) have made significant progress in code generation, they still struggle with code generation tasks in specific scenarios. These scenarios usually necessitate the adaptation of LLMs to fulfill specific needs, but the limited training samples available in practice lead to poor code generation performance. Therefore, how to effectively adapt LLMs to new scenarios with few training samples is a major challenge for current code generation. In this paper, we propose a novel adaptation approach named SEED, which stands for Sample-Efficient adaptation with Error-Driven learning for code generation. SEED leverages the errors made by LLMs as learning opportunities, using error revision to overcome its own shortcomings, thus achieving efficient learning. Specifically, SEED involves identifying error code generated by LLMs, employing Self-revise for code revision, optimizing the model with revised code, and iteratively adapting the process for continuous improvement. Experimental results show that, compared to other mainstream fine-tuning approaches, SEED achieves superior performance with few training samples, showing an average relative improvement of 54.7% in Pass@1 on multiple code generation benchmarks. We also validate the effectiveness of Self-revise, which generates revised code that optimizes the model more efficiently compared to the code samples from datasets. Moreover, SEED consistently demonstrates strong performance across various LLMs, underscoring its generalizability.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel adaptation approach named SEED, which stands for Sample-Efficient adaptation with Error-Driven learning for code generation, which leverages the errors made by LLMs as learning opportunities, using error revision to overcome its own shortcomings, thus achieving efficient learning."
            },
            "score": 4
        },
        {
            "id": "e56aa728aaa32c087c8f7bc56a7eb225675dd8ae",
            "paperId": "e56aa728aaa32c087c8f7bc56a7eb225675dd8ae",
            "title": "Structured Chemistry Reasoning with Large Language Models",
            "abstract": "Large Language Models (LLMs) excel in diverse areas, yet struggle with complex scientific reasoning, especially in the field of chemistry. Different from the simple chemistry tasks (e.g., molecule classification) addressed in previous studies, complex chemistry problems require not only vast knowledge and precise calculation, but also compositional reasoning about rich dynamic interactions of different concepts (e.g., temperature changes). Our study shows that even advanced LLMs, like GPT-4, can fail easily in different ways. Interestingly, the errors often stem not from a lack of domain knowledge within the LLMs, but rather from the absence of an effective reasoning structure that guides the LLMs to elicit the right knowledge, incorporate the knowledge in step-by-step reasoning, and iteratively refine results for further improved quality. On this basis, we introduce StructChem, a simple yet effective prompting strategy that offers the desired guidance and substantially boosts the LLMs' chemical reasoning capability. Testing across four chemistry areas -- quantum chemistry, mechanics, physical chemistry, and kinetics -- StructChem substantially enhances GPT-4's performance, with up to 30\\% peak improvement. Our analysis also underscores the unique difficulties of precise grounded reasoning in science with LLMs, highlighting a need for more research in this area. Code is available at \\url{https://github.com/ozyyshr/StructChem}.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Testing across four chemistry areas -- quantum chemistry, mechanics, physical chemistry, and kinetics -- StructChem substantially enhances GPT-4's performance, with up to 30\\% peak improvement, and underscores the unique difficulties of precise grounded reasoning in science with LLMs."
            },
            "score": 4
        },
        {
            "id": "5851bf82b0a9db2de86828f62c3006a6e1b40798",
            "paperId": "5851bf82b0a9db2de86828f62c3006a6e1b40798",
            "title": "Team UTSA-NLP at SemEval 2024 Task 5: Prompt Ensembling for Argument Reasoning in Civil Procedures with GPT4",
            "abstract": "In this paper, we present our system for the SemEval Task 5, The Legal Argument Reasoning Task in Civil Procedure Challenge. Legal argument reasoning is an essential skill that all law students must master. Moreover, it is important to develop natural language processing solutions that can reason about a question given terse domain-specific contextual information. Our system explores a prompt-based solution using GPT4 to reason over legal arguments. We also evaluate an ensemble of prompting strategies, including chain-of-thought reasoning and in-context learning. Overall, our system results in a Macro F1 of .8095 on the validation dataset and .7315 (5th out of 21 teams) on the final test set. Code for this project is available at https://github.com/danschumac1/CivilPromptReasoningGPT4.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This system explores a prompt-based solution using GPT4 to reason over legal arguments, including chain-of-thought reasoning and in-context learning, and evaluates an ensemble of prompting strategies, including chain-of-thought reasoning and in-context learning."
            },
            "score": 4
        },
        {
            "id": "bf8491bef353df126e2306ad2fe4b898697b906a",
            "paperId": "bf8491bef353df126e2306ad2fe4b898697b906a",
            "title": "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",
            "abstract": "This paper proposes a framework for quantitatively evaluating interactive LLMs such as ChatGPT using publicly available data sets. We carry out an extensive technical evaluation of ChatGPT using 23 data sets covering 8 different common NLP application tasks. We evaluate the multitask, multilingual and multi-modal aspects of ChatGPT based on these data sets and a newly designed multimodal dataset. We find that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks. We find that it is better at understanding non-Latin script languages than generating them. It is able to generate multimodal content from textual prompts, via an intermediate code generation step. Moreover, we find that ChatGPT is 63.41% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense reasoning, hence making it an unreliable reasoner. It is, for example, better at deductive than inductive reasoning. ChatGPT suffers from hallucination problems like other LLMs and it generates more extrinsic hallucinations from its parametric memory as it does not have access to an external knowledge base. Finally, the interactive feature of ChatGPT enables human collaboration with the underlying LLM to improve its performance, i.e, 8% ROUGE-1 on summarization and 2% ChrF++ on machine translation, in a multi-turn\"prompt engineering\"fashion. We also release codebase for evaluation set extraction.",
            "year": 2023,
            "citationCount": 754,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks and is better at understanding non-Latin script languages than generating them."
            },
            "score": 4
        },
        {
            "id": "d96997265f8146e93b4c9350f19d55e46d1317f0",
            "paperId": "d96997265f8146e93b4c9350f19d55e46d1317f0",
            "title": "ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering",
            "abstract": "With the recent advance in large pre-trained language models, researchers have achieved record performances in NLP tasks that mostly focus on language pattern matching. The community is experiencing the shift of the challenge from how to model language to the imitation of complex reasoning abilities like human beings. In this work, we investigate the application domain of finance that involves real-world, complex numerical reasoning. We propose a new large-scale dataset, ConvFinQA, aiming to study the chain of numerical reasoning in conversational question answering. Our dataset poses great challenge in modeling long-range, complex numerical reasoning paths in real-world conversations. We conduct comprehensive experiments and analyses with both the neural symbolic methods and the prompting-based methods, to provide insights into the reasoning mechanisms of these two divisions. We believe our new dataset should serve as a valuable resource to push forward the exploration of real-world, complex reasoning tasks as the next research focus. Our dataset and code is publicly available at https://github.com/czyssrs/ConvFinQA.",
            "year": 2022,
            "citationCount": 44,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a new large-scale dataset, ConvFinQA, aiming to study the chain of numerical reasoning in conversational question answering, and conducts comprehensive experiments and analyses with both the neural symbolic methods and the prompting-based methods to provide insights into the reasoning mechanisms."
            },
            "score": 4
        },
        {
            "id": "dd108d14f9ee57caeb3906c0efa2623595d5992c",
            "paperId": "dd108d14f9ee57caeb3906c0efa2623595d5992c",
            "title": "MOKA: Open-Vocabulary Robotic Manipulation through Mark-Based Visual Prompting",
            "abstract": "Open-vocabulary generalization requires robotic systems to perform tasks involving complex and diverse environments and task goals. While the recent advances in vision language models (VLMs) present unprecedented opportunities to solve unseen problems, how to utilize their emergent capabilities to control robots in the physical world remains an open question. In this paper, we present MOKA (Marking Open-vocabulary Keypoint Affordances), an approach that employs VLMs to solve robotic manipulation tasks specified by free-form language descriptions. At the heart of our approach is a compact point-based representation of affordance and motion that bridges the VLM's predictions on RGB images and the robot's motions in the physical world. By prompting a VLM pre-trained on Internet-scale data, our approach predicts the affordances and generates the corresponding motions by leveraging the concept understanding and commonsense knowledge from broad sources. To scaffold the VLM's reasoning in zero-shot, we propose a visual prompting technique that annotates marks on the images, converting the prediction of keypoints and waypoints into a series of visual question answering problems that are feasible for the VLM to solve. Using the robot experiences collected in this way, we further investigate ways to bootstrap the performance through in-context learning and policy distillation. We evaluate and analyze MOKA's performance on a variety of manipulation tasks specified by free-form language descriptions, such as tool use, deformable body manipulation, and object rearrangement.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents MOKA (Marking Open-vocabulary Keypoint Affordances), an approach that employs VLMs to solve robotic manipulation tasks specified by free-form language descriptions, and evaluates and analyze MOKA's performance on a variety of manipulation tasks specified by free-form language descriptions."
            },
            "score": 3
        },
        {
            "id": "ae8aabebad0c3ecae165ec05c18a2072ed360d1e",
            "paperId": "ae8aabebad0c3ecae165ec05c18a2072ed360d1e",
            "title": "NEWTON: Are Large Language Models Capable of Physical Reasoning?",
            "abstract": "Large Language Models (LLMs), through their contextualized representations, have been empirically proven to encapsulate syntactic, semantic, word sense, and common-sense knowledge. However, there has been limited exploration of their physical reasoning abilities, specifically concerning the crucial attributes for comprehending everyday objects. To address this gap, we introduce NEWTON, a repository and benchmark for evaluating the physics reasoning skills of LLMs. Further, to enable domain-specific adaptation of this benchmark, we present a pipeline to enable researchers to generate a variant of this benchmark that has been customized to the objects and attributes relevant for their application. The NEWTON repository comprises a collection of 2800 object-attribute pairs, providing the foundation for generating infinite-scale assessment templates. The NEWTON benchmark consists of 160K QA questions, curated using the NEWTON repository to investigate the physical reasoning capabilities of several mainstream language models across foundational, explicit, and implicit reasoning tasks. Through extensive empirical analysis, our results highlight the capabilities of LLMs for physical reasoning. We find that LLMs like GPT-4 demonstrate strong reasoning capabilities in scenario-based tasks but exhibit less consistency in object-attribute reasoning compared to humans (50% vs. 84%). Furthermore, the NEWTON platform demonstrates its potential for evaluating and enhancing language models, paving the way for their integration into physically grounded settings, such as robotic manipulation. Project site: https://newtonreasoning.github.io",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The NEWTON platform demonstrates its potential for evaluating and enhancing language models, paving the way for their integration into physically grounded settings, such as robotic manipulation."
            },
            "score": 3
        },
        {
            "id": "ecbff71f8e8c8d5975eeb44214eef174fa9aa978",
            "paperId": "ecbff71f8e8c8d5975eeb44214eef174fa9aa978",
            "title": "Protein Generation via Genome-scale Language Models with Bio-physical Scoring",
            "abstract": "Large language models (LLMs) trained on vast biological datasets can learn biological motifs and correlations across the evolutionary landscape of natural proteins. LLMs can then be used for de novo design of novel proteins with specific structures, functions, and physicochemical properties. We employ a pre-trained genome-scale language model that uses codons as tokens and integrate it into a workflow for targeted generation of sequences. Our framework suggests new gene sequences that are ranked for downstream evaluation by metrics that collectively capture extensive sequence-specific, biophysical, and biochemical properties. We demonstrate our integrated workflow to design novel variants of the enzyme, malate dehydrogenase (MDH), that exhibit more favorable activation energies than their natural counterparts (reduction of 4.01 kJ/mol) with sustained sequence generation rates of \u223c 104/hr and simulation rates of \u223c 102/hr on 64 nodes of Polaris with about 99.7 system utilization during the run.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work employs a pre-trained genome-scale language model that uses codons as tokens and integrates it into a workflow for targeted generation of sequences to design novel variants of the enzyme, malate dehydrogenase, that exhibit more favorable activation energies than their natural counterparts."
            },
            "score": 3
        },
        {
            "id": "76ab9bc1cbf529d1cfe5963f24069539c3199745",
            "paperId": "76ab9bc1cbf529d1cfe5963f24069539c3199745",
            "title": "Towards Accurate Differential Diagnosis with Large Language Models",
            "abstract": "An accurate differential diagnosis (DDx) is a cornerstone of medical care, often reached through an iterative process of interpretation that combines clinical history, physical examination, investigations and procedures. Interactive interfaces powered by Large Language Models (LLMs) present new opportunities to both assist and automate aspects of this process. In this study, we introduce an LLM optimized for diagnostic reasoning, and evaluate its ability to generate a DDx alone or as an aid to clinicians. 20 clinicians evaluated 302 challenging, real-world medical cases sourced from the New England Journal of Medicine (NEJM) case reports. Each case report was read by two clinicians, who were randomized to one of two assistive conditions: either assistance from search engines and standard medical resources, or LLM assistance in addition to these tools. All clinicians provided a baseline, unassisted DDx prior to using the respective assistive tools. Our LLM for DDx exhibited standalone performance that exceeded that of unassisted clinicians (top-10 accuracy 59.1% vs 33.6%, [p = 0.04]). Comparing the two assisted study arms, the DDx quality score was higher for clinicians assisted by our LLM (top-10 accuracy 51.7%) compared to clinicians without its assistance (36.1%) (McNemar's Test: 45.7, p<0.01) and clinicians with search (44.4%) (4.75, p = 0.03). Further, clinicians assisted by our LLM arrived at more comprehensive differential lists than those without its assistance. Our study suggests that our LLM for DDx has potential to improve clinicians' diagnostic reasoning and accuracy in challenging cases, meriting further real-world evaluation for its ability to empower physicians and widen patients' access to specialist-level expertise.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study suggests that an LLM optimized for diagnostic reasoning has potential to improve clinicians' diagnostic reasoning and accuracy in challenging cases, meriting further real-world evaluation for its ability to empower physicians and widen patients' access to specialist-level expertise."
            },
            "score": 3
        },
        {
            "id": "c31396f00c4e4ddba20d085d0da819b89c71bf4a",
            "paperId": "c31396f00c4e4ddba20d085d0da819b89c71bf4a",
            "title": "CITING: Large Language Models Create Curriculum for Instruction Tuning",
            "abstract": "The recent advancement of large language models (LLMs) has been achieved through a combo of instruction tuning and human alignment. However, building manually crafted instruction datasets and performing human alignment become the bottleneck for scaling the development of LLMs. In this paper, we exploit the idea of leveraging AI models in lieu of humans as the teacher to train student LLMs. Our method is inspired by how human students refine their writing skills by following the rubrics and learning from the revisions offered by their tutors. Specifically, we employ a teacher LLM to create a curriculum for instruction tuning of the student LLM, namely Curriculum Instruction TunING (CITING). It encompasses two main steps: (1) the teacher LLM crafts the rubrics for evaluating the answers corresponding to various types of questions, and (2) the student LLM learns to follow the rubrics and perform self-correction from the revision made by the teacher. We further iteratively carry out it to embody the procedure of CITING. We compare CITING to a series of state-of-the-art baselines on four datasets. Our method demonstrates strong improvement in terms of articulate, in-depth, and comprehensive by GPT-4 evaluation. Specifically, it achieves an average winning rate of 79.4% over SFT, 73.4% over RLHF, 78.1% over RRHF, and 76.3% over RAFT, respectively.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The idea of leveraging AI models in lieu of humans as the teacher to train student LLMs is exploited, inspired by how human students refine their writing skills by following the rubrics and learning from the revisions offered by their tutors."
            },
            "score": 3
        },
        {
            "id": "437cbaee4eaee0bf84abbe11750b86b091b9b756",
            "paperId": "437cbaee4eaee0bf84abbe11750b86b091b9b756",
            "title": "MacGyver: Are Large Language Models Creative Problem Solvers?",
            "abstract": "We explore the creative problem-solving capabilities of modern LLMs in a novel constrained setting. To this end, we create MACGYVER, an automatically generated dataset consisting of over 1,600 real-world problems deliberately designed to trigger innovative usage of objects and necessitate out-of-the-box thinking. We then present our collection to both LLMs and humans to compare and contrast their problem-solving abilities. MACGYVER is challenging for both groups, but in unique and complementary ways. For instance, humans excel in tasks they are familiar with but struggle with domain-specific knowledge, leading to a higher variance. In contrast, LLMs, exposed to a variety of specialized knowledge, attempt broader problems but fail by proposing physically-infeasible actions. Finally, we provide a detailed error analysis of LLMs, and demonstrate the potential of enhancing their problem-solving ability with novel prompting techniques such as iterative step-wise reflection and divergent-convergent thinking. This work (1) introduces a fresh arena for intelligent agents focusing on intricate aspects of physical reasoning, planning, and unconventional thinking, which supplements the existing spectrum of machine intelligence; and (2) provides insight into the constrained problem-solving capabilities of both humans and AI.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "MACGYVER is an automatically generated dataset consisting of over 1,600 real-world problems deliberately designed to trigger innovative usage of objects and necessitate out-of-the-box thinking, and provides insight into the constrained problem-solving capabilities of both humans and AI."
            },
            "score": 3
        },
        {
            "id": "4dcc799bb5a47a54a69d9045c073d44f3c9ef225",
            "paperId": "4dcc799bb5a47a54a69d9045c073d44f3c9ef225",
            "title": "Learning Reward for Physical Skills using Large Language Model",
            "abstract": "Learning reward functions for physical skills are challenging due to the vast spectrum of skills, the high-dimensionality of state and action space, and nuanced sensory feedback. The complexity of these tasks makes acquiring expert demonstration data both costly and time-consuming. Large Language Models (LLMs) contain valuable task-related knowledge that can aid in learning these reward functions. However, the direct application of LLMs for proposing reward functions has its limitations such as numerical instability and inability to incorporate the environment feedback. We aim to extract task knowledge from LLMs using environment feedback to create efficient reward functions for physical skills. Our approach consists of two components. We first use the LLM to propose features and parameterization of the reward function. Next, we update the parameters of this proposed reward function through an iterative self-alignment process. In particular, this process minimizes the ranking inconsistency between the LLM and our learned reward functions based on the new observations. We validated our method by testing it on three simulated physical skill learning tasks, demonstrating effective support for our design choices.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work aims to extract task knowledge from LLMs using environment feedback to create efficient reward functions for physical skills through an iterative self-alignment process that minimizes the ranking inconsistency between the LLM and the authors' learned reward functions based on the new observations."
            },
            "score": 3
        },
        {
            "id": "860ba78f9789bbfc99c299b18558ca19430d8fea",
            "paperId": "860ba78f9789bbfc99c299b18558ca19430d8fea",
            "title": "LLM-SR: Scientific Equation Discovery via Programming with Large Language Models",
            "abstract": "Mathematical equations have been unreasonably effective in describing complex natural phenomena across various scientific disciplines. However, discovering such insightful equations from data presents significant challenges due to the necessity of navigating extremely high-dimensional combinatorial and nonlinear hypothesis spaces. Traditional methods of equation discovery largely focus on extracting equations from data alone, often neglecting the rich domain-specific prior knowledge that scientists typically depend on. To bridge this gap, we introduce LLM-SR, a novel approach that leverages the extensive scientific knowledge and robust code generation capabilities of Large Language Models (LLMs) to discover scientific equations from data in an efficient manner. Specifically, LLM-SR treats equations as programs with mathematical operators and combines LLMs' scientific priors with evolutionary search over equation programs. The LLM iteratively proposes new equation skeletons, drawing from its physical understanding, which are then optimized against data to estimate skeleton parameters. We demonstrate LLM-SR's effectiveness across three diverse scientific domains, where it discovers physically accurate equations that provide significantly better fits to in-domain and out-of-domain data compared to the well-established equation discovery baselines",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LLM-SR is introduced, a novel approach that leverages the extensive scientific knowledge and robust code generation capabilities of Large Language Models to discover scientific equations from data in an efficient manner and combines LLMs' scientific priors with evolutionary search over equation programs."
            },
            "score": 3
        },
        {
            "id": "a6a0963fcf21ed47a2616ca3980f8f4f21e6d5ad",
            "paperId": "a6a0963fcf21ed47a2616ca3980f8f4f21e6d5ad",
            "title": "Large language models as tax attorneys: a case study in legal capabilities emergence",
            "abstract": "Better understanding of Large Language Models' (LLMs) legal analysis abilities can contribute to improving the efficiency of legal services, governing artificial intelligence and leveraging LLMs to identify inconsistencies in law. This paper explores LLM capabilities in applying tax law. We choose this area of law because it has a structure that allows us to set up automated validation pipelines across thousands of examples, requires logical reasoning and maths skills, and enables us to test LLM capabilities in a manner relevant to real-world economic lives of citizens and companies. Our experiments demonstrate emerging legal understanding capabilities, with improved performance in each subsequent OpenAI model release. We experiment with retrieving and using the relevant legal authority to assess the impact of providing additional legal context to LLMs. Few-shot prompting, presenting examples of question\u2013answer pairs, is also found to significantly enhance the performance of the most advanced model, GPT-4. The findings indicate that LLMs, particularly when combined with prompting enhancements and the correct legal texts, can perform at high levels of accuracy but not yet at expert tax lawyer levels. As LLMs continue to advance, their ability to reason about law autonomously could have significant implications for the legal profession and AI governance. This article is part of the theme issue \u2018A complexity science approach to law and governance\u2019.",
            "year": 2023,
            "citationCount": 20,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The findings indicate that LLMs, particularly when combined with prompting enhancements and the correct legal texts, can perform at high levels of accuracy but not yet at expert tax lawyer levels."
            },
            "score": 2
        },
        {
            "id": "58f3f5d875b31dc3ae7744c0368349512bdb08e3",
            "paperId": "58f3f5d875b31dc3ae7744c0368349512bdb08e3",
            "title": "GPT-4 Performance for Neurologic Localization",
            "abstract": "Background and Objectives In health care, large language models such as Generative Pretrained Transformers (GPTs), trained on extensive text datasets, have potential applications in reducing health care disparities across regions and populations. Previous software developed for lesion localization has been limited in scope. This study aims to evaluate the capability of GPT-4 for lesion localization based on clinical presentation. Methods GPT-4 was prompted using history and neurologic physical examination (H&P) from published cases of acute stroke followed by questions for clinical reasoning with answering for \u201csingle or multiple lesions,\u201d \u201cside,\u201d and \u201cbrain region\u201d using Zero-Shot Chain-of-Thought and Text Classification prompting. GPT-4 output on 3 separate trials for each of 46 cases was compared with imaging-based localization. Results GPT-4 successfully processed raw text from H&P to generate accurate neuroanatomical localization and detailed clinical reasoning. Performance metrics across trial-based analysis for specificity, sensitivity, precision, and F1-score were 0.87, 0.74, 0.75, and 0.74, respectively, for side; 0.94, 0.85, 0.84, and 0.85, respectively, for brain region. Class labels within the brain region were similarly high for all regions except the cerebellum and were also similar when considering all 3 trials to examine metrics by case. Errors were due to extrinsic causes\u2014inadequate information in the published cases, and intrinsic causes\u2014failures of logic or inadequate knowledge base. Discussion This study reveals capabilities of GPT-4 in the localization of acute stroke lesions, showing a potential future role as a clinical tool in neurology.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Capability of GPT-4 in the localization of acute stroke lesions is revealed, showing a potential future role as a clinical tool in neurology."
            },
            "score": 2
        },
        {
            "id": "0792a06ccffded46f880c59c2ac78591cef4830a",
            "paperId": "0792a06ccffded46f880c59c2ac78591cef4830a",
            "title": "Computational Models of Narrative : Review of the Workshop",
            "abstract": "them to entertain, communicate, convince, and explain. One workshop participant noted that \u201cas far as I know, every society in the world has stories, which suggests they have a psychological basis, that stories do something for you.\u201d To truly understand and explain human intelligence, reasoning, and beliefs, we need to understand why narrative is universal and explain the function it serves. Computational modeling is a natural method for investigating narrative. As a complex cognitive phenomenon, narrative touches on many areas that have traditionally been of interest to artificial intelligence researchers: its different facets draw on our capacities for natural language understanding and generation, commonsense reasoning, analogical reasoning, planning, physical perception (through imagination), and social cognition. Successful modeling will undoubtedly require researchers from these many perspectives and more, using a multitude of different techniques from the AI toolkit, ranging from, for example, detailed symbolic knowledge representation to largescale statistical analyses. The relevance of AI to narrative, and vice versa, is compelling. The Computational Models of Narrative workshop1 had three main objectives: (1) to understand the scope and dimensions of narrative models, identifying gaps and next steps, (2) to evaluate the state of the art, and (3) to begin to build a community focused on computational narrative. The interdisciplinary group of 22 participants (see figure 1) included computer scienArticles",
            "year": 2009,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Computational Models of Narrative workshop had three main objectives: to understand the scope and dimensions of narrative models, identifying gaps and next steps, to evaluate the state of the art, and to begin to build a community focused on computational narrative."
            },
            "score": 2
        },
        {
            "id": "f7abe8dfc8e69bd8ea2b3a9cb5265e9ee332c886",
            "paperId": "f7abe8dfc8e69bd8ea2b3a9cb5265e9ee332c886",
            "title": "Metaphorian: Leveraging Large Language Models to Support Extended Metaphor Creation for Science Writing",
            "abstract": "Science writers commonly use extended metaphors to communicate unfamiliar concepts in a more accessible way to a wider audience. However, creating metaphors for science writing is challenging even for professional writers; according to our formative study (n=6), finding inspiration and extending metaphors with coherent structures were critical yet significantly challenging tasks for them. We contribute Metaphorian, a system that supports science writers with the creation of scientific metaphors by facilitating the search, extension, and iterative revision of metaphors. Metaphorian uses a large language model-based workflow inspired by the heuristic rules revealed from a study with six professional writers. A user study (n=16) revealed that Metaphorian significantly enhances satisfaction, confidence, and inspiration in metaphor writing without decreasing writers\u2019 sense of agency. We discuss design implications for creativity support for figurative writing in science.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Metaphorian, a system that supports science writers with the creation of scientific metaphors by facilitating the search, extension, and iterative revision of metaphors, is contributed."
            },
            "score": 2
        },
        {
            "id": "5cab50974b97872d2f8e9f4f486f0eb12ebfa67c",
            "paperId": "5cab50974b97872d2f8e9f4f486f0eb12ebfa67c",
            "title": "Deep language models for interpretative and predictive materials science",
            "abstract": "Machine learning (ML) has emerged as an indispensable methodology to describe, discover, and predict complex physical phenomena that efficiently help us learn underlying functional rules, especially in cases when conventional modeling approaches cannot be applied. While conventional feedforward neural networks are typically limited to performing tasks related to static patterns in data, recursive models can both work iteratively based on a changing input and discover complex dynamical relationships in the data. Deep language models can model flexible modalities of data and are capable of learning rich dynamical behaviors as they operate on discrete or continuous symbols that define the states of a physical system, yielding great potential toward end-to-end predictions. Similar to how words form a sentence, materials can be considered as a self-assembly of physically interacted building blocks, where the emerging functions of materials are analogous to the meaning of sentences. While discovering the fundamental relationships between building blocks and function emergence can be challenging, language models, such as recurrent neural networks and long-short term memory networks, and, in particular, attention models, such as the transformer architecture, can solve many such complex problems. Application areas of such models include protein folding, molecular property prediction, prediction of material failure of complex nonlinear architected materials, and also generative strategies for materials discovery. We outline challenges and opportunities, especially focusing on extending the deep-rooted kinship of humans with symbolism toward generalizable artificial intelligence (AI) systems using neuro-symbolic AI, and outline how tools such as ChatGPT and DALL\u00b7E can drive materials discovery.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Challenges and opportunities are outlined, especially focusing on extending the deep-rooted kinship of humans with symbolism toward generalizable artificial intelligence (AI) systems using neuro-symbolic AI, and how tools such as ChatGPT and DALL\u00b7E can drive materials discovery."
            },
            "score": 2
        },
        {
            "id": "0a01523f048c6d066be443e8be0d50206aa2940c",
            "paperId": "0a01523f048c6d066be443e8be0d50206aa2940c",
            "title": "PlanFitting: Tailoring Personalized Exercise Plans with Large Language Models",
            "abstract": "A personally tailored exercise regimen is crucial to ensuring sufficient physical activities, yet challenging to create as people have complex schedules and considerations and the creation of plans often requires iterations with experts. We present PlanFitting, a conversational AI that assists in personalized exercise planning. Leveraging generative capabilities of large language models, PlanFitting enables users to describe various constraints and queries in natural language, thereby facilitating the creation and refinement of their weekly exercise plan to suit their specific circumstances while staying grounded in foundational principles. Through a user study where participants (N=18) generated a personalized exercise plan using PlanFitting and expert planners (N=3) evaluated these plans, we identified the potential of PlanFitting in generating personalized, actionable, and evidence-based exercise plans. We discuss future design opportunities for AI assistants in creating plans that better comply with exercise principles and accommodate personal constraints.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The potential of PlanFitting in generating personalized, actionable, and evidence-based exercise plans is identified and future design opportunities for AI assistants in creating plans that better comply with exercise principles and accommodate personal constraints are discussed."
            },
            "score": 2
        },
        {
            "id": "ccd2eb6bf9a655dc0391a8a858f5de3569f191d8",
            "paperId": "ccd2eb6bf9a655dc0391a8a858f5de3569f191d8",
            "title": "The Last JITAI? The Unreasonable Effectiveness of Large Language Models in Issuing Just-in-Time Adaptive Interventions: Fostering Physical Activity in a Prospective Cardiac Rehabilitation Setting",
            "abstract": "We investigated the viability of using Large Language Models (LLMs) for triggering and personalizing content for Just-in-Time Adaptive Interventions (JITAIs) in digital health. JITAIs are being explored as a key mechanism for sustainable behavior change, adapting interventions to an individual's current context and needs. However, traditional rule-based and machine learning models for JITAI implementation face scalability and flexibility limitations, such as lack of personalization, difficulty in managing multi-parametric systems, and issues with data sparsity. To investigate JITAI implementation via LLMs, we tested the contemporary overall performance-leading model 'GPT-4' with examples grounded in the use case of fostering heart-healthy physical activity in outpatient cardiac rehabilitation. Three personas and five sets of context information per persona were used as a basis of triggering and personalizing JITAIs. Subsequently, we generated a total of 450 proposed JITAI decisions and message content, divided equally into JITAIs generated by 10 iterations with GPT-4, a baseline provided by 10 laypersons (LayPs), and a gold standard set by 10 healthcare professionals (HCPs). Ratings from 27 LayPs and 11 HCPs indicated that JITAIs generated by GPT-4 were superior to those by HCPs and LayPs over all assessed scales: i.e., appropriateness, engagement, effectiveness, and professionality. This study indicates that LLMs have significant potential for implementing JITAIs as a building block of personalized or\"precision\"health, offering scalability, effective personalization based on opportunistically sampled information, and good acceptability.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study indicates that LLMs have significant potential for implementing JITAIs as a building block of personalized or\"precision\"health, offering scalability, effective personalization based on opportunistically sampled information, and good acceptability."
            },
            "score": 2
        },
        {
            "id": "fe7660fd6c127409859fbd42684663c440db6a1b",
            "paperId": "fe7660fd6c127409859fbd42684663c440db6a1b",
            "title": "Benchmarking Large Language Models for Extraction of International Classification of Diseases Codes from Clinical Documentation",
            "abstract": "Background: Healthcare reimbursement and coding is dependent on accurate extraction of International Classification of Diseases- tenth revision clinical modification (ICD-10-CM) codes from clinical documentation. Attempts to automate this task have had limited success. This study aimed to evaluate the performance of large language models (LLMs) in extracting ICD-10-CM codes from unstructured inpatient notes and benchmark them against human coder. Methods: This study compared performance of GPT-3.5, GPT4, Claude 2.1, Claude 3, Gemini Advanced, and Llama 2-70b in extracting ICD-10-CM codes from unstructured inpatient notes against a human coder. We presented deidentified inpatient notes from American Health Information Management Association Vlab authentic patient cases to LLMs and human coder for extraction of ICD-10-CM codes. We used a standard prompt for extracting ICD-10-CM codes. The human coder analyzed the same notes using 3M Encoder, adhering to the 2022- ICD-10-CM Coding Guidelines. Results: In this study, we analyzed 50 inpatient notes, comprising of 23 history and physicals and 27 progress notes. The human coder identified 165 unique codes with a median of 4 codes per note. The LLMs extracted varying numbers of median codes per note: GPT 3.5: 7, GPT4: 6, Claude 2.1: 6, Claude 3: 8, Gemini Advanced: 5, and Llama 2-70b:11. GPT 4 had the best performance though the agreement with human coder was poor at 15.2% for overall extraction of ICD-10-CM codes and 26.4% for extraction of category ICD-10-CM codes. Conclusion: Current LLMs have poor performance in extraction of ICD-10-CM codes from inpatient notes when compared against a human coder.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Current LLMs have poor performance in extraction of ICD-10-CM codes from inpatient notes when compared against a human coder, according to this study."
            },
            "score": 2
        },
        {
            "id": "6204a7973ad33e81ebc22edea013c11731ef2acb",
            "paperId": "6204a7973ad33e81ebc22edea013c11731ef2acb",
            "title": "Towards a Verification-Driven Iterative Development of Software for Safety-Critical Cyber-Physical Systems",
            "abstract": null,
            "year": 2021,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a model-driven approach with a focus on guaranteeing safety using formal verification of cyber-physical systems, using the actor-based textual modeling language, Rebeca, with model checking support for formal verification."
            },
            "score": 1
        }
    ],
    "novelty": "yes"
}