{
    "topic_description": "novel prompting methods that can improve factuality and reduce hallucination of large language models",
    "idea_name": "Embodied Reasoning Prompting",
    "raw_idea": {
        "Problem": "Language models can generate statements that are physically implausible or inconsistent with the constraints of the real world.",
        "Existing Methods": "Some methods incorporate physical knowledge bases or simulation engines to constrain generation.",
        "Motivation": "Language models have been shown to capture some physical and temporal commonsense knowledge. We can prompt them to reason step-by-step about the physical implications of their generations, grounding the process in embodied reasoning.",
        "Proposed Method": "The procedure is: 1) Given a prompt, generate an initial continuation. 2) Prompt the model to break down the generated continuation into a temporal sequence of physical events. 3) For each event, prompt the model to assess its physical plausibility, and revise if needed. 4) Prompt the model to re-compose the revised events into a coherent continuation. 5) Repeat steps 2-4 until the continuation is physically coherent.",
        "Experiment Plan": "Evaluate on physical reasoning datasets like PIQA and Physical IQA. Compare to baselines like direct prompting and KG-augmented generation. Metrics include physical plausibility scores and human evaluation of physical coherence."
    },
    "full_experiment_plan": {
        "Title": "Embodied Reasoning: Prompting Language Models for Physically Grounded Generation",
        "Problem Statement": "Language models can generate statements that are physically implausible or inconsistent with the constraints of the real world, such as objects defying gravity or characters teleporting.",
        "Motivation": "Existing methods to improve physical reasoning in language models typically incorporate external knowledge bases or simulation engines to constrain generation. However, these approaches can be computationally expensive and may not generalize well to open-ended scenarios. Recent work has shown that language models themselves can capture a significant amount of physical and temporal commonsense knowledge from pretraining. We propose to leverage this internal knowledge by prompting language models to reason step-by-step about the physical implications of their generations, grounding the process in 'embodied' reasoning.",
        "Proposed Method": "Given a prompt, the language model first generates an initial continuation. It is then prompted to break down the generated continuation into a temporal sequence of physical events. For each event, the model is prompted to assess its physical plausibility based on its understanding of real-world dynamics and revise the event if needed to maintain physical coherence. Finally, the model is prompted to re-compose the revised event sequence into an updated continuation. This process of decomposition, evaluation, revision, and re-composition is repeated iteratively until the model determines that the continuation is physically coherent.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "Evaluate the proposed method on established physical reasoning benchmarks like PIQA (Physical Interaction QA) and Physical IQA. These multiple-choice datasets test understanding of physical commonsense. Also evaluate on open-ended story generation datasets like WritingPrompts and BookCorpus, measuring the physical plausibility of generated continuations.",
            "Step 2: Construct Prompts": "For multiple-choice datasets, the baseline prompt provides the question and answer choices. The proposed prompt adds instructions for the model to reason about the physical interactions involved in each answer choice before selecting the most plausible one. For generation, the baseline prompt is the story opening. The proposed prompt iteratively guides the model to (1) continue the story, (2) break down the continuation into physical events, (3) evaluate and revise each event to maintain physical plausibility, and (4) integrate the revised events back into a coherent continuation. Repeat steps 2-4 until a stopping criteria is met (e.g. N iterations or no further revisions).",
            "Step 3: Select Models": "Evaluate the proposed method on GPT-3 (davinci), GPT-3.5 (text-davinci-002), and GPT-4 via the OpenAI API. For open-source models, experiment with LLaMA, GPT-J, and GPT-NeoX.",
            "Step 4: Get Results": "For multiple-choice, report accuracy of the baseline direct prompting vs. the proposed embodied reasoning prompting. For generation, sample N continuations per prompt for both baselines and proposed. Evaluate physical plausibility of generations using (1) automated metrics like the CUPS benchmark for identifying physically unlikely sentences and (2) human ratings on criteria like physical possibility, temporal coherence, and overall sensibility.",
            "Step 5: Analyze Results": "Compare performance of baselines vs. proposed method on multiple-choice and generation tasks. Analyze how the iterative reasoning process changes the physical plausibility of generations over multiple revision rounds. Qualitatively examine cases where the embodied reasoning helps correct initially implausible events. Ablate the different steps of the process (e.g. decomposition, evaluation, revision) to assess their individual contribution."
        },
        "Test Case Examples": {
            "Multiple-Choice Prompt Input": {
                "Baseline": "Question: To separate glass from paper, which would work better? \nChoices: (A) magnet (B) water (C) air blower",
                "Proposed": "Question: To separate glass from paper, which would work better? Think step-by-step about the physical interactions involved before choosing an answer. \nChoices: (A) magnet (B) water (C) air blower"
            },
            "Multiple-Choice Expected Output": {
                "Baseline": "(A) magnet",
                "Proposed": "(C) air blower. Reasoning: A magnet will not interact with either glass or paper since they are not magnetic materials. Water could make the paper heavier and cause it to sink, but the glass would also likely sink. An air blower could blow the lighter paper away while the heavier glass remains still, allowing them to be separated."
            },
            "Generation Prompt Input": {
                "Baseline": "Prompt: John was running late for work. He hurried out the door and jumped into his car. He turned the key but the engine wouldn't start. He tried again, pumping the gas pedal, but still nothing. Continuation: ",
                "Proposed": "Prompt: John was running late for work. He hurried out the door and jumped into his car. He turned the key but the engine wouldn't start. He tried again, pumping the gas pedal, but still nothing. \nContinuation: John hopped out of the car and popped open the hood. Black smoke billowed out. \nPhysical Events: 1) John exits the car 2) John opens the car hood 3) Black smoke emerges from under the hood \nEvaluation: Smoke suggests an issue with the engine, consistent with the car not starting. Plausible. \nRevision: No revision needed. \nContinuation: Waving away the smoke, John peered at the engine. A loose spark plug wire caught his eye. \nPhysical Events: 1) John waves his hand to clear the smoke 2) John looks at the car engine 3) John notices a loose spark plug wire \nEvaluation: A loose spark plug wire could prevent the engine from starting. Physically plausible. \nRevision: No revision needed. \nContinuation: John reached in and reconnected the wire. He closed the hood and got back in the driver's seat. \nPhysical Events: 1) John reconnects the spark plug wire 2) John closes the car hood 3) John sits back in the driver's seat \nEvaluation: Fixing the spark plug wire could allow the car to start. The sequence of actions is physically and temporally coherent. \nRevision: No further revisions. The continuation maintains physical plausibility. \nFinal Continuation:"
            },
            "Generation Expected Output": {
                "Baseline": "John hopped out of the car and popped open the hood. Black smoke billowed out. He reached in and fiddled with some wires but couldn't see the problem. Frustrated, he slammed the hood shut. Suddenly, the car roared to life on its own. Relieved, John jumped back in and sped off, the smoke trailing behind him as he raced to the office.",
                "Proposed": "John hopped out of the car and popped open the hood. Black smoke billowed out. Waving away the smoke, John peered at the engine. A loose spark plug wire caught his eye. John reached in and reconnected the wire. He closed the hood and got back in the driver's seat. With a hopeful breath, he turned the key. The engine sputtered, then roared to life. John grinned in relief and shifted into drive. He checked his watch as he pulled out of the driveway. If traffic was light, he might just make it to the office on time. As he accelerated down the street, the last wisps of smoke dissipated in his rearview mirror."
            },
            "Explanation": "The baseline generation contains some physically implausible elements, like the car starting on its own after slamming the hood. The proposed generation maintains better physical coherence by having John identify and fix a plausible issue (the spark plug wire), and the car only starts after this repair. The smoke dissipating as John drives away also follows logically from the engine issue being resolved."
        },
        "Fallback Plan": "If the proposed embodied reasoning prompting does not lead to significant improvements over the baselines, we can: (1) Analyze whether the model is accurately decomposing continuations into physical events and revising implausible ones. If not, experiment with alternative prompting strategies for these subtasks. (2) Collect human annotations on the physical plausibility of events in the training data and use these to finetune the model to more accurately assess and revise physical descriptions. (3) Investigate integrating external knowledge sources (e.g. knowledge bases, simulation engines) into the prompting pipeline to supplement the model's internal physical knowledge. (4) Explore alternative evaluation metrics that go beyond binary plausibility to assess more nuanced aspects of physical coherence (e.g. temporal consistency, object permanence). The project could then focus on error analysis, proposing refined metrics, and providing insights into the current challenges in physically grounded language generation."
    }
}