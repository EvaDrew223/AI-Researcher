{
    "topic_description": "novel prompting methods that can improve factuality and reduce hallucination of large language models",
    "idea_name": "Socratic Dialogue Prompting",
    "raw_idea": {
        "Problem": "Large language models often generate outputs that lack depth and nuance, failing to consider multiple perspectives or engage in critical reasoning. This can lead to the generation of shallow or biased responses that do not fully address the complexity of the given prompt.",
        "Existing Methods": "Existing methods for improving the depth and quality of generated outputs, such as using adversarial training or reinforcement learning, often require significant computational resources and may not generalize well to new domains or tasks.",
        "Motivation": "We propose that engaging the model in a Socratic dialogue, where it is prompted to consider multiple perspectives, question its own assumptions, and engage in critical reasoning, can help to improve the depth and quality of the generated outputs. By encouraging the model to think more deeply and critically about the given prompt, we can generate more nuanced and well-reasoned responses.",
        "Proposed Method": "We introduce Socratic Dialogue Prompting (SDP), a prompting method that engages the model in a structured dialogue to explore multiple perspectives and engage in critical reasoning. Given an initial prompt, we first ask the model to generate an initial response. We then prompt the model to consider alternative perspectives by asking questions such as \"What are some potential counterarguments to this view?\" or \"How might someone with a different background or experience view this issue?\". We then prompt the model to respond to these alternative perspectives, and to question its own assumptions by asking questions such as \"What are the underlying assumptions behind this argument?\" or \"What evidence supports or contradicts this view?\". This process is repeated iteratively, with the model generating increasingly nuanced and well-reasoned responses at each step.",
        "Experiment Plan": "We will evaluate SDP on a range of language generation tasks that require critical reasoning and nuanced understanding, such as argumentative essay writing, policy analysis, and philosophical dialogue. We will compare the depth, nuance, and logical coherence of outputs generated by SDP to those generated by standard prompting methods, as well as to human-generated outputs. We will also conduct a human evaluation to assess the perceived quality and persuasiveness of the generated outputs."
    },
    "full_experiment_plan": {
        "Title": "Socratic Dialogue Prompting: Engaging Language Models in Critical Reasoning and Nuanced Discourse",
        "Problem Statement": "Large language models often generate outputs that lack depth and nuance, failing to consider multiple perspectives or engage in critical reasoning. This can lead to the generation of shallow or biased responses that do not fully address the complexity of the given prompt.",
        "Motivation": "Existing methods for improving the depth and quality of generated outputs, such as using adversarial training or reinforcement learning, often require significant computational resources and may not generalize well to new domains or tasks. We propose that engaging the model in a Socratic dialogue, where it is prompted to consider multiple perspectives, question its own assumptions, and engage in critical reasoning, can help to improve the depth and quality of the generated outputs. By encouraging the model to think more deeply and critically about the given prompt, we can generate more nuanced and well-reasoned responses.",
        "Proposed Method": "We introduce Socratic Dialogue Prompting (SDP), a prompting method that engages the model in a structured dialogue to explore multiple perspectives and engage in critical reasoning. Given an initial prompt, we first ask the model to generate an initial response. We then prompt the model to consider alternative perspectives by asking questions such as \"What are some potential counterarguments to this view?\" or \"How might someone with a different background or experience view this issue?\". We then prompt the model to respond to these alternative perspectives, and to question its own assumptions by asking questions such as \"What are the underlying assumptions behind this argument?\" or \"What evidence supports or contradicts this view?\". This process is repeated iteratively, with the model generating increasingly nuanced and well-reasoned responses at each step.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We will evaluate SDP on a range of language generation tasks that require critical reasoning and nuanced understanding, such as argumentative essay writing (e.g., ArgumentativeWriting-1.0), policy analysis (e.g., PolicyQuestions), and philosophical dialogue (e.g., PhilosophicalInquiry). We will also test on open-ended dialogue datasets such as Blended Skill Talk and Wizard of Wikipedia to assess the method's generalizability.",
            "Step 2: Construct Prompts": "For each task, we will construct a set of initial prompts that cover a range of topics and viewpoints. For the Socratic Dialogue Prompting, we will design a set of follow-up questions that encourage the model to consider alternative perspectives, question assumptions, and engage in critical reasoning. These questions will be designed to be open-ended and thought-provoking, such as:\n- \"What are some potential counterarguments to this view?\"\n- \"How might someone with a different background or experience view this issue?\"\n- \"What are the underlying assumptions behind this argument?\"\n- \"What evidence supports or contradicts this view?\"\n- \"Are there any potential unintended consequences of this perspective?\"\n- \"How might this viewpoint be influenced by cultural, historical, or personal biases?\"\nWe will experiment with different numbers of Socratic questioning rounds (e.g., 1, 2, 3) to find the optimal trade-off between output quality and efficiency.",
            "Step 3: Select Models": "We will evaluate SDP on state-of-the-art language models such as GPT-3.5 (text-davinci-002), GPT-4, and ChatGPT. We will compare the performance of these models with and without Socratic Dialogue Prompting.",
            "Step 4: Generate Outputs": "For each task and prompt, we will generate outputs using the selected models with and without SDP. For the SDP condition, we will generate outputs iteratively, with the model first generating an initial response, then considering alternative perspectives and questioning assumptions through multiple rounds of Socratic questioning, and finally generating a revised response that incorporates the insights gained from the dialogue.",
            "Step 5: Evaluate Outputs": "We will evaluate the quality of the generated outputs using both automated metrics and human evaluation. For automated evaluation, we will use metrics such as BLEU, ROUGE, and BERTScore to assess the linguistic quality and semantic similarity of the generated outputs to human-written references. For human evaluation, we will recruit qualified annotators (e.g., domain experts, philosophy students) to rate the outputs on criteria such as depth of reasoning, consideration of multiple perspectives, logical coherence, and overall quality. We will use a Likert scale (e.g., 1-5) for each criterion and compare the average ratings for outputs generated with and without SDP.",
            "Step 6: Analyze Results": "We will analyze the results to determine whether Socratic Dialogue Prompting leads to significant improvements in the depth, nuance, and quality of the generated outputs compared to standard prompting methods. We will also examine the impact of different numbers of Socratic questioning rounds on output quality and efficiency. Finally, we will qualitatively analyze the generated outputs to identify common patterns, strengths, and weaknesses of the SDP approach, and to gain insights into how the Socratic questioning process influences the model's reasoning and generation."
        },
        "Test Case Examples": {
            "Test Case 1": {
                "Initial Prompt": "Should social media platforms be held responsible for the spread of misinformation and fake news on their platforms? Discuss the potential benefits and drawbacks of holding these companies accountable.",
                "Baseline Output": "Social media platforms have a responsibility to curb the spread of misinformation and fake news on their platforms. These companies have the technological capabilities and resources to identify and remove false or misleading content, and they should be held accountable for failing to do so. The spread of misinformation can have serious consequences, such as influencing elections, inciting violence, and undermining public trust in institutions. By holding social media companies responsible, we can create a stronger incentive for them to prioritize truth and accuracy over engagement and profits.",
                "Socratic Dialogue Prompting": {
                    "Round 1": {
                        "Question": "What are some potential counterarguments to holding social media platforms responsible for the spread of misinformation?",
                        "Response": "One potential counterargument is that holding social media platforms responsible for the spread of misinformation could infringe on free speech and open expression. These platforms provide a space for diverse viewpoints and debates, and some argue that it's not the role of private companies to determine what is true or false. There's also a risk that overly aggressive content moderation could lead to censorship and the silencing of legitimate voices. Additionally, the scale and complexity of moderating user-generated content in real-time poses significant technical and logistical challenges for social media companies."
                    },
                    "Round 2": {
                        "Question": "How might we balance the need to combat misinformation with the importance of protecting free speech on social media platforms?",
                        "Response": "Balancing the need to combat misinformation with the protection of free speech is a complex challenge. One approach could be for social media platforms to focus on transparency and user empowerment rather than direct censorship. This could involve clearly labeling or flagging content that has been disputed or debunked by reputable fact-checkers, providing users with tools to report and filter out misinformation, and investing in media literacy education to help users critically evaluate the information they encounter online. Platforms could also work with independent, third-party fact-checking organizations to identify and address viral misinformation without making unilateral decisions about what content to remove. Ultimately, a multi-stakeholder approach that involves collaboration between platforms, users, civil society groups, and policymakers may be necessary to strike the right balance between combating misinformation and protecting free expression online."
                    }
                },
                "Revised Output": "The question of whether social media platforms should be held responsible for the spread of misinformation on their platforms is complex and multifaceted. On one hand, the spread of false or misleading information can have serious consequences, eroding public trust, influencing political outcomes, and even inciting real-world violence. Social media companies have the technical capabilities and resources to identify and address viral misinformation, and some argue they have a moral obligation to do so. \n\nOn the other hand, holding platforms liable for user-generated content raises concerns about free speech and the role of private companies in regulating public discourse. Overly aggressive content removal could lead to censorship and the silencing of legitimate voices. The scale and complexity of content moderation also poses significant challenges.\n\nUltimately, a balanced approach may involve a combination of strategies. Platforms could focus on transparency and user empowerment, clearly labeling disputed content and providing tools for users to report and filter misinformation. They could partner with independent fact-checkers to identify false claims without unilaterally removing content. Collaboration between platforms, civil society groups, and policymakers could help develop standards and best practices. Media literacy education is also critical for helping users navigate online information.\n\nWhile social media companies have a responsibility to address misinformation, the solution is not simple. Protecting the integrity of online discourse while respecting free speech will require ongoing dialogue, experimentation, and adaptation as technologies and societal norms evolve."
            },
            "Test Case 2": {
                "Initial Prompt": "Is it ethical for governments to use surveillance technologies to monitor and track their citizens in the name of national security? Consider the potential trade-offs between safety and privacy.",
                "Baseline Output": "Government surveillance of citizens poses a significant threat to individual privacy and civil liberties. While national security is important, the use of surveillance technologies can easily lead to overreach and abuse of power. Citizens have a fundamental right to privacy, and the knowledge that one's communications and movements are being monitored can have a chilling effect on free speech and political dissent. History has shown that surveillance powers are often used to target marginalized communities and political opponents, not just genuine security threats. The benefits of surveillance are often overstated, and there is little evidence that mass data collection is effective in preventing terrorist attacks. Instead of relying on intrusive surveillance, governments should focus on targeted investigations based on reasonable suspicion and due process. The erosion of privacy undermines the foundations of a free and open society.",
                "Socratic Dialogue Prompting": {
                    "Round 1": {
                        "Question": "What are some arguments in favor of government surveillance for national security purposes?",
                        "Response": "Proponents of government surveillance argue that it is a necessary tool for preventing terrorist attacks, combating organized crime, and maintaining public safety. In an age of global terrorism and cyber threats, they believe that intelligence agencies need broad powers to monitor communications and identify potential threats before they materialize. Surveillance technologies can help connect the dots between suspicious activities and individuals, potentially saving lives. Some argue that law-abiding citizens have nothing to fear from surveillance, and that the benefits of increased security outweigh the costs to privacy. Governments often assert that surveillance programs are subject to oversight and legal safeguards to prevent abuse."
                    },
                    "Round 2": {
                        "Question": "How can we ensure that government surveillance powers are not abused or misused, while still allowing for legitimate national security activities?",
                        "Response": "Striking the right balance between security and privacy is a difficult but important challenge. To prevent abuse and misuse of surveillance powers, there need to be robust legal safeguards, oversight mechanisms, and transparency requirements in place. This could include requiring warrants or court orders for surveillance activities, setting clear limits on data collection and retention, and establishing independent oversight bodies to monitor compliance. Governments should be transparent about the scope and nature of their surveillance programs, and there should be avenues for public debate and input. Surveillance activities should be narrowly targeted based on specific, credible threats, rather than indiscriminate mass data collection. Strong encryption and data protection measures can help prevent unauthorized access or misuse of collected data. Ultimately, maintaining public trust requires demonstrating that surveillance powers are being used judiciously and in accordance with democratic values. Regular auditing, reporting, and sunset provisions can help ensure that surveillance programs remain necessary and proportionate over time."
                    }
                },
                "Revised Output": "The use of government surveillance technologies for national security purposes raises difficult ethical questions and trade-offs. On one side, in an age of global terrorism and cyber threats, surveillance tools can be valuable for identifying and preventing attacks, saving lives, and maintaining public safety. Intelligence agencies argue that robust monitoring capabilities are necessary to connect the dots and disrupt increasingly sophisticated criminal and terrorist networks. Some believe that the benefits of increased security justify the costs to individual privacy.\n\nHowever, the risks of government overreach and abuse of surveillance powers are significant. Unchecked monitoring can have a chilling effect on free speech, political dissent, and other civil liberties. There are legitimate concerns about the disproportionate targeting of marginalized communities, activists, and political opponents. Mass data collection also raises questions about the presumption of innocence and due process. The effectiveness of sweeping surveillance in actually preventing attacks is often overstated, while the erosion of privacy can undermine public trust and the foundations of a free society.\n\nTo strike a balance, robust legal safeguards, oversight, and transparency are essential. Surveillance activities should require warrants, be narrowly targeted based on credible threats, and subject to independent monitoring. Governments should be transparent about the existence and scope of their programs, and there should be opportunities for public input and debate. Strong encryption and data protection measures are also needed to prevent misuse of collected information.\n\nUltimately, the use of surveillance technologies requires ongoing scrutiny and a commitment to upholding democratic principles. While targeted surveillance can serve legitimate security purposes, it must be wielded judiciously and remain accountable to the public. Finding the right equilibrium will be an ongoing challenge as technologies evolve, but it is essential for maintaining both safety and liberty in a free society."
            }
        },
        "Fallback Plan": "If the proposed Socratic Dialogue Prompting method does not yield significant improvements in output quality or reasoning depth compared to the baselines, there are several alternative approaches we can explore:\n\n1. Analyze the generated Socratic questions and responses to identify potential weaknesses or areas for improvement in the prompting strategy. This could involve examining whether the questions are sufficiently open-ended and thought-provoking, whether they cover a diverse range of perspectives and assumptions, and whether the model's responses engage substantively with the questions or simply reiterate previous points. Based on this analysis, we can refine the prompting templates and techniques.\n\n2. Experiment with different language models or fine-tuning strategies. It's possible that certain models may be better suited to engaging in Socratic dialogue than others, due to differences in their training data, architectures, or objectives. We can compare the performance of additional models such as Anthropic's Claude, Meta's Blenderbot, or open-source models like BLOOM or Chinchilla. We can also explore fine-tuning the models on datasets of Socratic dialogues or philosophical texts to improve their ability to engage in critical reasoning.\n\n3. Incorporate additional prompting techniques or external knowledge sources. For example, we can experiment with priming the model with relevant background information or philosophical concepts before engaging in the Socratic dialogue. We can also explore using retrieval-augmented generation, where the model is provided with access to external knowledge bases or search results related to the topic at hand. This could help the model generate more informed and substantive responses.\n\n4. Conduct a more in-depth error analysis to identify common failure modes or challenges in the Socratic Dialogue Prompting approach. This could involve manually examining a sample of generated outputs to categorize the types of errors or weaknesses observed (e.g., logical inconsistencies, lack of coherence, failure to address counterarguments). Based on this analysis, we can develop targeted strategies for addressing these challenges, such as modifying the prompting templates, providing additional examples or guidance, or exploring alternative evaluation metrics.\n\n5. Pivot to a more open-ended, exploratory study of the potential and limitations of using Socratic questioning to improve the reasoning capabilities of language models. Rather than focusing solely on measuring improvements in output quality, we can conduct a qualitative analysis of the types of reasoning patterns, insights, and blind spots that emerge from engaging models in Socratic dialogue. This could involve collaborating with domain experts in philosophy, argumentation theory, or critical thinking to identify promising directions for future research at the intersection of language models and structured reasoning.\n\nBy pursuing one or more of these fallback strategies, we can still gain valuable insights into the potential of Socratic Dialogue Prompting and identify opportunities for further development and refinement of the approach. Even if the initial experiments do not yield the hoped-for improvements, the process of iterating on the methodology and analyzing the model's behavior can contribute to our understanding of the capabilities and limitations of language models for engaging in critical reasoning and nuanced discourse."
    },
    "novelty_queries": [
        "KeywordQuery(\"socratic dialogue prompting language models\")",
        "KeywordQuery(\"critical reasoning language models\")",
        "KeywordQuery(\"nuanced discourse language models\")",
        "KeywordQuery(\"language models consider multiple perspectives\")",
        "KeywordQuery(\"Socratic Dialogue Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "69335077fcacbff7a7cf25697da1949e6bdfa968",
            "paperId": "69335077fcacbff7a7cf25697da1949e6bdfa968",
            "title": "The Art of SOCRATIC QUESTIONING: Recursive Thinking with Large Language Models",
            "abstract": "Chain-of-Thought (CoT) prompting enables large language models to solve complex reasoning problems by generating intermediate steps. However, confined by its inherent single-pass and sequential generation process, CoT heavily relies on the initial decisions, causing errors in early steps to accumulate and impact the final answers. In contrast, humans adopt recursive thinking when tackling complex reasoning problems, i.e., iteratively breaking the original problem into approachable sub-problems and aggregating their answers to resolve the original one. Inspired by the human cognitive process, we propose SOCRATIC QUESTIONING, a divide-and-conquer style algorithm that mimics the recursive thinking process. Specifically, SOCRATIC QUESTIONING leverages large language models to raise and answer sub-questions until collecting enough information to tackle the original question. Unlike CoT, SOCRATIC QUESTIONING explicitly navigates the thinking space, stimulates effective recursive thinking, and is more robust towards errors in the thinking process. Extensive experiments on several complex reasoning tasks, including MMLU, MATH, LogiQA, and visual question-answering demonstrate significant performance improvements over the state-of-the-art prompting methods, such as CoT, and Tree-of-Thought. The qualitative analysis clearly shows that the intermediate reasoning steps elicited by SOCRATIC QUESTIONING are similar to humans' recursively thinking process of complex reasoning problems.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The qualitative analysis clearly shows that the intermediate reasoning steps elicited by SOCRATIC QUESTIONING are similar to humans' recursively thinking process of complex reasoning problems."
            },
            "score": 8,
            "novelty_score": "The project proposal aims to improve the depth and nuance of language model outputs by engaging the model in Socratic dialogue to consider multiple perspectives and engage in critical reasoning. The approach is called Socratic Dialogue Prompting (SDP).\n\nThe paper proposes to improve complex reasoning in language models by mimicking the human recursive thinking process. The approach is called SOCRATIC QUESTIONING, which breaks down the original problem into sub-problems and aggregates their answers.\n\nWhile both the project proposal and the paper use the concept of Socratic questioning to improve language model reasoning, the project focuses on generating more nuanced and well-reasoned outputs, while the paper focuses specifically on complex reasoning tasks. The methods also differ in their implementation details.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "d7386e8859b22e05ce9c4a972613d4b1e1e44198",
            "paperId": "d7386e8859b22e05ce9c4a972613d4b1e1e44198",
            "title": "Prompting Large Language Models With the Socratic Method",
            "abstract": "This paper presents a systematic approach to using the Socratic method in developing prompt templates that effectively interact with large language models, including GPT-3. Various methods are examined, and those that yield precise answers and justifications while fostering creativity and imagination to enhance creative writing are identified. Techniques such as definition, elenchus, dialectic, maieutics, generalization, and counterfactual reasoning are discussed for their application in engineering prompt templates and their connections to inductive, deductive, and abductive reasoning. Through examples, the effectiveness of these dialogue and reasoning methods is demonstrated. An interesting observation is made that when the task's goal and user intent are conveyed to GPT-3 via ChatGPT before the start of a dialogue, the large language model seems to connect to the external context expressed in the intent and perform more effectively.",
            "year": 2023,
            "citationCount": 20,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An interesting observation is made that when the task's goal and user intent are conveyed to GPT-3 via ChatGPT before the start of a dialogue, the large language model seems to connect to the external context expressed in the intent and perform more effectively."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the depth and nuance of language model outputs through critical reasoning and considering multiple perspectives. The proposed approach is Socratic Dialogue Prompting, where the model is engaged in a structured dialogue to explore different viewpoints and question assumptions.\n\nThe research problem in the paper is developing effective prompt templates for interacting with large language models using the Socratic method. The proposed approach is applying techniques from the Socratic method such as definition, elenchus, dialectic, and counterfactual reasoning to engineer prompt templates.\n\nWhile both works involve using the Socratic method with language models, the proposal focuses on improving the model's output quality and reasoning capabilities, while the paper focuses on developing effective prompt templates for eliciting specific types of responses. The research problems and approaches are related but distinct.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "0f2ae9b1757ca1d3993d7b48d116e5d178cc9225",
            "paperId": "0f2ae9b1757ca1d3993d7b48d116e5d178cc9225",
            "title": "SocraSynth: Multi-LLM Reasoning with Conditional Statistics",
            "abstract": "Large language models (LLMs), while promising, face criticisms for biases, hallucinations, and a lack of reasoning capability. This paper introduces SocraSynth, a multi-LLM agent reasoning platform developed to mitigate these issues. SocraSynth utilizes conditional statistics and systematic context enhancement through continuous arguments, alongside adjustable debate contentiousness levels. The platform typically involves a human moderator and two LLM agents representing opposing viewpoints on a given subject. SocraSynth operates in two main phases: knowledge generation and reasoning evaluation. In the knowledge generation phase, the moderator defines the debate topic and contentiousness level, prompting the agents to formulate supporting arguments for their respective stances. The reasoning evaluation phase then employs Socratic reasoning and formal logic principles to appraise the quality of the arguments presented. The dialogue concludes with the moderator adjusting the contentiousness from confrontational to collaborative, gathering final, conciliatory remarks to aid in human reasoning and decision-making. Through case studies in three distinct application domains, this paper showcases SocraSynth's effectiveness in fostering rigorous research, dynamic reasoning, comprehensive assessment, and enhanced collaboration. This underscores the value of multi-agent interactions in leveraging LLMs for advanced knowledge extraction and decision-making support.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Through case studies in three distinct application domains, this paper showcases SocraSynth's effectiveness in fostering rigorous research, dynamic reasoning, comprehensive assessment, and enhanced collaboration, underscores the value of multi-agent interactions in leveraging LLMs for advanced knowledge extraction and decision-making support."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the depth and nuance of language model outputs through Socratic dialogue prompting. The approach involves engaging the model in a structured dialogue to consider multiple perspectives, question assumptions, and engage in critical reasoning.\n\nThe research problem in the paper is mitigating biases, hallucinations, and lack of reasoning in language models. The approach involves using a multi-agent platform with conditional statistics and systematic context enhancement through continuous arguments.\n\nWhile both works aim to improve language model reasoning, the proposal focuses on single-model prompting techniques, while the paper proposes a multi-agent platform. The methods differ in their use of Socratic dialogue prompting vs. conditional statistics and debate-style interactions.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "75ce9634d281cc12cbe434f86c737df8e10796fa",
            "paperId": "75ce9634d281cc12cbe434f86c737df8e10796fa",
            "title": "Self-Explanation Prompting Improves Dialogue Understanding in Large Language Models",
            "abstract": "Task-oriented dialogue (TOD) systems facilitate users in executing various activities via multi-turn dialogues, but Large Language Models (LLMs) often struggle to comprehend these intricate contexts. In this study, we propose a novel\"Self-Explanation\"prompting strategy to enhance the comprehension abilities of LLMs in multi-turn dialogues. This task-agnostic approach requires the model to analyze each dialogue utterance before task execution, thereby improving performance across various dialogue-centric tasks. Experimental results from six benchmark datasets confirm that our method consistently outperforms other zero-shot prompts and matches or exceeds the efficacy of few-shot prompts, demonstrating its potential as a powerful tool in enhancing LLMs' comprehension in complex dialogue tasks.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel Self-Explanation Prompting strategy to enhance the comprehension abilities of LLMs in multi-turn dialogues, which requires the model to analyze each dialogue utterance before task execution, thereby improving performance across various dialogue-centric tasks."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the depth and nuance of language model outputs through critical reasoning and considering multiple perspectives. The approach is to use Socratic dialogue prompting to engage the model in structured questioning and reasoning.\n\nThe research problem in the paper is improving dialogue understanding in large language models for task-oriented dialogue systems. The approach is to use self-explanation prompting to make the model analyze each dialogue utterance before task execution.\n\nThe two works have different research problems (shallow/biased model outputs vs. dialogue understanding) and approaches (Socratic dialogue vs. self-explanation prompting).\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "b0435af3063195e8ae880489e64ccde64e6d7563",
            "paperId": "b0435af3063195e8ae880489e64ccde64e6d7563",
            "title": "Guiding Large Language Models via Directional Stimulus Prompting",
            "abstract": "We introduce Directional Stimulus Prompting, a novel framework for guiding black-box large language models (LLMs) toward specific desired outputs. Instead of directly adjusting LLMs, our method employs a small tunable policy model (e.g., T5) to generate an auxiliary directional stimulus prompt for each input instance. These directional stimulus prompts act as nuanced, instance-specific hints and clues to guide LLMs in generating desired outcomes, such as including specific keywords in the generated summary. Our approach sidesteps the challenges of direct LLM tuning by optimizing the policy model to explore directional stimulus prompts that align LLMs with desired behaviors. The policy model can be optimized through 1) supervised fine-tuning using labeled data and 2) reinforcement learning from offline or online rewards based on the LLM's output. We assess our method across summarization, dialogue response generation, and chain-of-thought reasoning tasks. Our experiments demonstrate that the framework consistently improves LLMs' (e.g., ChatGPT, Codex, InstructGPT) performance on these supervised tasks using minimal labeled data. Notably, using just 80 dialogues on the MultiWOZ dataset, our approach enhances ChatGPT's performance by an impressive 41.4%, matching or surpassing some fully supervised start-of-the-art models. Additionally, the instance-specific chain-of-thought prompt generated by our approach improves InstructGPT's reasoning accuracy compared to human-crafted or automatically generated prompts. The code and data are publicly available at \\url{https://github.com/Leezekun/Directional-Stimulus-Prompting}.",
            "year": 2023,
            "citationCount": 37,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Directional Stimulus Prompting, a novel framework for guiding black-box large language models (LLMs) toward specific desired outputs, sidesteps the challenges of direct LLM tuning by optimizing the policy model to explore directional stimulus prompts that align LLMs with desired behaviors."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the depth and nuance of language model outputs through Socratic dialogue prompting. The approach involves engaging the model in a structured dialogue to explore multiple perspectives and engage in critical reasoning.\n\nThe research problem in the paper is guiding language models toward desired outputs through directional stimulus prompting. The approach involves using a small tunable policy model to generate instance-specific auxiliary prompts that guide the language model's output.\n\nWhile both works aim to improve language model outputs through prompting techniques, the specific research problems and approaches are different. The proposal focuses on increasing depth and nuance through Socratic questioning, while the paper focuses on guiding models toward desired behaviors through instance-specific stimulus prompts.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "ada81a4de88a6ce474df2e2446ad11fea480616e",
            "paperId": "ada81a4de88a6ce474df2e2446ad11fea480616e",
            "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language",
            "abstract": "Large pretrained (e.g.,\"foundation\") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodal-informed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning.",
            "year": 2022,
            "citationCount": 380,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Socratic Models (SMs) are shown to be competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, and enable new applications such as answering free-form questions about egocentric video, and engaging in multimodal assistive dialogue with people."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the depth and nuance of language model outputs through Socratic dialogue prompting. The approach involves engaging the model in a structured dialogue to consider multiple perspectives and engage in critical reasoning.\n\nThe research problem in the paper is composing multimodal reasoning capabilities in foundation models through prompting, without finetuning. The approach involves using Socratic Models, a modular framework for zero-shot composition of pretrained models across different domains.\n\nWhile both works involve prompting and the term \"Socratic\", the proposal focuses on using Socratic dialogue to improve language model reasoning within a single modality, while the paper focuses on zero-shot composition of multimodal reasoning capabilities across different pretrained models. The research problems and approaches are quite different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "287d45f2090adba6ed8fc4834a029cd47373a88e",
            "paperId": "287d45f2090adba6ed8fc4834a029cd47373a88e",
            "title": "Can Language Models Employ the Socratic Method? Experiments with Code Debugging",
            "abstract": "When employing the Socratic method of teaching, instructors guide students toward solving a problem on their own rather than providing the solution directly. While this strategy can substantially improve learning outcomes, it is usually time-consuming and cognitively demanding. Automated Socratic conversational agents can augment human instruction and provide the necessary scale, however their development is hampered by the lack of suitable data for training and evaluation. In this paper, we introduce a manually created dataset of multi-turn Socratic advice that is aimed at helping a novice programmer fix buggy solutions to simple computational problems. The dataset is then used for benchmarking the Socratic debugging abilities of a number of language models, ranging from fine-tuning the instruction-based text-to-text transformer Flan-T5 to zero-shot and chain of thought prompting of the much larger GPT-4. The code and datasets are made freely available for research at the link below. https://github.com/taisazero/socratic-debugging-benchmark",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A manually created dataset of multi-turn Socratic advice aimed at helping a novice programmer fix buggy solutions to simple computational problems and is used for benchmarking the Socratic debugging abilities of a number of language models."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the depth and nuance of language model outputs through Socratic dialogue prompting. The approach involves iteratively questioning the model to consider alternative perspectives and engage in critical reasoning.\n\nThe research problem in the paper is benchmarking the Socratic debugging abilities of language models in the context of helping novice programmers fix buggy code. The approach involves fine-tuning and prompting language models on a dataset of multi-turn Socratic advice.\n\nWhile both works involve the Socratic method and language models, the proposal focuses on general language generation tasks and the paper focuses specifically on code debugging. The proposal aims to improve the model's reasoning capabilities, while the paper aims to evaluate the model's ability to provide Socratic advice for a specific task.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "70916fbeb446ab7dc811ab74b193365d789bf1eb",
            "paperId": "70916fbeb446ab7dc811ab74b193365d789bf1eb",
            "title": "Chain-of-thought prompting for responding to in-depth dialogue questions with LLM",
            "abstract": "The way and content in which users ask questions can provide insight into their current status, including their personality, emotions, and psychology. Instead of directly prompting the large language models (LLMs), we explore how chain-of-thought prompting helps in this scenario to perform reasoning and planning according to user status, aiming to provide a more personalized and engaging experience for the user query. To this end, we \ufb01rst construct a benchmark of 6 dialogue or question-answering datasets in both English and Chinese, covering 3 different aspects of user status ( including personality , emotion , and psychology ). Then we prompt the LLMs to generate the response regarding the user status as intermediate reasoning processing. We propose a novel demonstration selection strategy using the semantic similarity of intermediate reasoning instead of test queries. To evaluate the effectiveness and robustness of our approach, we conduct extensive experiments with 7 LLMs under zero-shot and one-shot settings. The experimental results show that our approach consistently outperforms standard prompting in terms of both helpfulness and acceptness across all datasets, regardless of the LLMs used. The code and dataset can be found at https://github.com/ruleGreen/ Dialogue_CoT.git .",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores how chain-of-thought prompting helps in this scenario to perform reasoning and planning according to user status, aiming to provide a more personalized and engaging experience for the user query."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the depth and nuance of language model outputs through Socratic dialogue prompting. The approach involves iteratively prompting the model to consider alternative perspectives, question assumptions, and engage in critical reasoning.\n\nThe research problem in the paper is providing personalized and engaging responses to user queries by considering user status (personality, emotion, psychology). The approach involves using chain-of-thought prompting to generate intermediate reasoning based on user status before generating the final response.\n\nWhile both works aim to improve the quality of language model outputs, the specific problems and approaches are different. The proposal focuses on depth and nuance through Socratic questioning, while the paper focuses on personalization based on user status using chain-of-thought reasoning.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "d0c69c309fbf1233b6351cd57484557c16f28427",
            "paperId": "d0c69c309fbf1233b6351cd57484557c16f28427",
            "title": "Cue-CoT: Chain-of-thought Prompting for Responding to In-depth Dialogue Questions with LLMs",
            "abstract": "Large Language Models (LLMs), such as \\texttt{ChatGPT}, greatly empower dialogue systems with strong language understanding and generation capabilities. However, most of the previous works prompt the LLMs to directly generate a response based on the dialogue context, overlooking the underlying linguistic cues about the user status exhibited in the context. Such in-depth dialogue scenarios are challenging for existing LLMs to figure out the user's hidden needs and respond satisfactorily through a single-step inference. To this end, we propose a novel linguistic cue-based chain-of-thoughts (\\textit{Cue}-CoT), which enhances the LLMs inference with an intermediate reasoning step to find cues exhibited in the dialogue, aiming to provide a more personalized and engaging response. To evaluate the approach, we build a benchmark with in-depth dialogue questions, consisting of 6 datasets in both Chinese and English, targeting 3 major linguistic cues during the conversation: \\textit{personality}, \\textit{emotion}, and \\textit{psychology}. We conduct extensive experiments on the proposed benchmark with 5 LLMs under both zero-shot and one-shot settings. Empirical results demonstrate our proposed \\textit{Cue}-CoT method outperforms standard prompting methods in terms of both \\textit{helpfulness} and \\textit{acceptability} on all datasets.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel linguistic cue-based chain-of-thoughts (\\textit{Cue}-CoT) method is proposed, which enhances the LLMs inference with an intermediate reasoning step to find cues exhibited in the dialogue, aiming to provide a more personalized and engaging response."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is that large language models often generate outputs that lack depth and nuance, failing to consider multiple perspectives or engage in critical reasoning. The proposed approach is Socratic Dialogue Prompting, which engages the model in a structured dialogue to explore multiple perspectives and engage in critical reasoning.\n\nThe research problem in the paper is that most previous works prompt LLMs to directly generate a response based on the dialogue context, overlooking the underlying linguistic cues about the user status. The proposed approach is a linguistic cue-based chain-of-thoughts (Cue-CoT), which enhances the LLMs inference with an intermediate reasoning step to find cues exhibited in the dialogue.\n\nThe proposal focuses on improving the depth and nuance of generated outputs by engaging the model in critical reasoning, while the paper focuses on improving the personalization and engagement of dialogue responses by considering linguistic cues about the user. The approaches are different, as the proposal uses Socratic dialogue prompting, while the paper uses a cue-based chain-of-thought prompting.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "261549439aebdda72b648ecc462448fd24857ac1",
            "paperId": "261549439aebdda72b648ecc462448fd24857ac1",
            "title": "Progressive-Hint Prompting Improves Reasoning in Large Language Models",
            "abstract": "The performance of Large Language Models (LLMs) in reasoning tasks depends heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency being critical methods that enhance this ability. However, these methods do not fully exploit the answers generated by the LLM to guide subsequent responses. This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers. PHP is orthogonal to CoT and self-consistency, making it easy to combine with state-of-the-art techniques to further improve performance. We conducted extensive and comprehensive experiments on seven benchmarks. The results show that PHP significantly improves accuracy while remaining highly efficient. For instance, with text-davinci-003, we observed a 4.2% improvement on GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction in sample paths with self-consistency. With GPT-4 and PHP, we achieve state-of-the-art performances on SVAMP (89.1% ->91.9%), GSM8K (92% ->95.5%), AQuA (76.4% ->79.9%) and MATH (50.3% ->53.9%).",
            "year": 2023,
            "citationCount": 64,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is that large language models often generate outputs that lack depth and nuance, failing to consider multiple perspectives or engage in critical reasoning. The proposed approach is Socratic Dialogue Prompting, which engages the model in a structured dialogue to explore multiple perspectives and engage in critical reasoning.\n\nThe research problem in the paper is improving the reasoning ability of large language models in various tasks. The proposed approach is Progressive-Hint Prompting, which uses previously generated answers as hints to progressively guide the model toward the correct answers.\n\nWhile both works aim to improve the reasoning ability of large language models, the specific problems and approaches are different. The proposal focuses on generating more nuanced and well-reasoned outputs by engaging the model in Socratic dialogue, while the paper focuses on improving the accuracy of the model in specific reasoning tasks by using progressive hints.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "76e5069425547d4f53b5aa843a765a305b7fa470",
            "paperId": "76e5069425547d4f53b5aa843a765a305b7fa470",
            "title": "Discursive Socratic Questioning: (Unsupervised) Interpreting Neural Language Models for Discourse Understanding",
            "abstract": "Do neural language models (NLMs) understand 001 the discourse they are processing? Traditional 002 interpretation methods that address this ques-003 tion require pre-annotated explanations, which 004 defeats the purpose of unsupervised explana-005 tion. We propose unsupervised Discursive So-006 cratic Questioning ( D I SQ ), a two-step interpre-007 tative measure. 008 D I SQ first generates Socratic-style questions 009 about the discourse and then queries NLMs 010 about these questions. A model\u2019s understand-011 ing is measured by its responses to these ques-012 tions. We apply D I SQ to examine two fun-013 damental discourse phenomena, namely dis-014 course relation and discourse coherence. We 015 find NLMs demonstrate non-trivial capacities 016 without being trained on any discourse data: 017 Q&A pairs in D I SQ are shown to be evidence 018 for discourse relation and cohesive devices for 019 discourse coherence. D I SQ brings initial evi-020 dence that NLMs understand discourse through 021 reasoning. We find larger models perform bet-022 ter, but contradictions and hallucinations are 023 still problems. We recommend D I SQ as a uni-024 versal diagnostic for discursive NLMs and us-025 ing its output for self-supervision. 026",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "D I SQ brings initial evi-020 dence that NLMs understand discourse through 021 reasoning, and is recommended as a unsupervised versal diagnostic for discursive NLMs and its output for self-supervision."
            },
            "score": 6
        },
        {
            "id": "dad73205f54b29a50b9c2743e8333928a20e0981",
            "paperId": "dad73205f54b29a50b9c2743e8333928a20e0981",
            "title": "Prompting for Diverse Responses: Making Large Language Models More Truthful",
            "abstract": "Large Language Models (LLMs) are typically trained on a large amount of data scraped from the internet, and thus can learn human misconceptions and biases. These misconceptions and biases can cause models to produce problematic outputs that persist even after fine-tuning. In the complex reasoning domain, chain-of-thought prompting has been shown to increase the correctness of LLMs by prompting models to perform intermediate reasoning steps. Taking inspiration from chain-of-thought prompting, in this work we aim to further improve truthfulness by considering multiple, diverse answers, inspired by how human decision makers might seek out opinions from multiple sources or consider the same question from multiple perspectives. We present Prompting for Diverse Responses (PDR), a method that allows question-answering models to generate multiple distinct answers for a given question. We generate chains of thought and answers by sampling from a small set of prompts and collect the question and answers together into a single text. We use a (comparably) small model called the \u201cjudge\u201d to select the best answer. One advantage to our approach is that the judge is not finetuned, so new prompts can be swapped in or out to change the overall behaviour. We find that PDR has strong performance when evaluated on both the TriviaQA and TruthfulQA datasets, while many of our baseline prompts only perform well on a single dataset.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents Prompting for Diverse Responses (PDR), a method that allows question-answering models to generate multiple distinct answers for a given question, inspired by how human decision makers might seek out opinions from multiple sources or consider the same question from multiple perspectives."
            },
            "score": 6
        },
        {
            "id": "142ebbf4760145f591166bde2564ac70c001e927",
            "paperId": "142ebbf4760145f591166bde2564ac70c001e927",
            "title": "Language Models (Mostly) Know What They Know",
            "abstract": "We study whether language models can evaluate the validity of their own claims and predict which questions they will be able to answer correctly. We first show that larger models are well-calibrated on diverse multiple choice and true/false questions when they are provided in the right format. Thus we can approach self-evaluation on open-ended sampling tasks by asking models to first propose answers, and then to evaluate the probability\"P(True)\"that their answers are correct. We find encouraging performance, calibration, and scaling for P(True) on a diverse array of tasks. Performance at self-evaluation further improves when we allow models to consider many of their own samples before predicting the validity of one specific possibility. Next, we investigate whether models can be trained to predict\"P(IK)\", the probability that\"I know\"the answer to a question, without reference to any particular proposed answer. Models perform well at predicting P(IK) and partially generalize across tasks, though they struggle with calibration of P(IK) on new tasks. The predicted P(IK) probabilities also increase appropriately in the presence of relevant source materials in the context, and in the presence of hints towards the solution of mathematical word problems. We hope these observations lay the groundwork for training more honest models, and for investigating how honesty generalizes to cases where models are trained on objectives other than the imitation of human writing.",
            "year": 2022,
            "citationCount": 340,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that larger models are well-calibrated on diverse multiple choice and true/false questions when they are provided in the right format and investigated whether models can be trained to predict P(IK), the probability that \"I know\" the answer to a question."
            },
            "score": 6
        },
        {
            "id": "13c85adfa950651ffcd91ef3018fa30801b74472",
            "paperId": "13c85adfa950651ffcd91ef3018fa30801b74472",
            "title": "Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration",
            "abstract": "Conversational systems based on Large Language Models (LLMs), such as ChatGPT, show exceptional proficiency in context understanding and response generation. However, despite their impressive capabilities, they still possess limitations, such as providing randomly-guessed answers to ambiguous queries or failing to refuse users' requests, both of which are considered aspects of a conversational agent's proactivity. This raises the question of whether LLM-based conversational systems are equipped to handle proactive dialogue problems. In this work, we conduct a comprehensive analysis of LLM-based conversational systems, specifically focusing on three aspects of proactive dialogue systems: clarification, target-guided, and non-collaborative dialogues. To trigger the proactivity of LLMs, we propose the Proactive Chain-of-Thought prompting scheme, which augments LLMs with the goal planning capability over descriptive reasoning chains. Empirical findings are discussed to promote future studies on LLM-based proactive dialogue systems.",
            "year": 2023,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A comprehensive analysis of LLM-based conversational systems, specifically focusing on three aspects of proactive dialogue systems: clarification, target-guided, and non-collaborative dialogues, and the Proactive Chain-of-Thought prompting scheme is proposed."
            },
            "score": 5
        },
        {
            "id": "e11eeb64ba0022646a6e55c580b7dbeb414b346b",
            "paperId": "e11eeb64ba0022646a6e55c580b7dbeb414b346b",
            "title": "Injecting Salesperson's Dialogue Strategies in Large Language Models with Chain-of-Thought Reasoning",
            "abstract": "Recent research in dialogue systems and corpora has focused on two main categories: task-oriented (TOD) and open-domain (chit-chat) dialogues. TOD systems help users accomplish specific tasks, while open-domain systems aim to create engaging conversations. However, in real-world scenarios, user intents are often revealed during interactions. A recent study introduced SalesBot, which simulates dialogues transitioning from chit-chat to task-oriented scenarios to train sales agents. Unfortunately, the initial data lacked smooth transitions and coherent long-turn dialogues, resulting in poor naturalness in sales-customer interactions. To address these issues, this paper presents SalesBot 2.0, an improved dataset. It leverages commonsense knowledge from large language models (LLMs) through strategic prompting. Additionally, we introduce a novel model called SalesAgent, trained on salesperson's interactions, using chain-of-thought (CoT) reasoning. This model excels in transitioning topics, understanding user intents, and selecting appropriate strategies. Experiments using diverse user simulations validate the effectiveness of our method in controlling dialogue strategies in LLMs. Furthermore, SalesBot 2.0 enhances coherence and reduces aggression, facilitating better model learning for sales-customer interactions.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents SalesBot 2.0, an improved dataset that enhances coherence and reduces aggression, facilitating better model learning for sales-customer interactions, and introduces a novel model called SalesAgent, trained on salesperson's interactions, using chain-of-thought (CoT) reasoning."
            },
            "score": 5
        },
        {
            "id": "1321cda5cb160285de0548a999b4b941156ba764",
            "paperId": "1321cda5cb160285de0548a999b4b941156ba764",
            "title": "Threads of Subtlety: Detecting Machine-Generated Texts Through Discourse Motifs",
            "abstract": "With the advent of large language models (LLM), the line between human-crafted and machine-generated texts has become increasingly blurred. This paper delves into the inquiry of identifying discernible and unique linguistic properties in texts that were written by humans, particularly uncovering the underlying discourse structures of texts beyond their surface structures. Introducing a novel methodology, we leverage hierarchical parse trees and recursive hypergraphs to unveil distinctive discourse patterns in texts produced by both LLMs and humans. Empirical findings demonstrate that, although both LLMs and humans generate distinct discourse patterns influenced by specific domains, human-written texts exhibit more structural variability, reflecting the nuanced nature of human writing in different domains. Notably, incorporating hierarchical discourse features enhances binary classifiers' overall performance in distinguishing between human-written and machine-generated texts, even on out-of-distribution and paraphrased samples. This underscores the significance of incorporating hierarchical discourse features in the analysis of text patterns. The code and dataset will be available at [TBA].",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper delves into the inquiry of identifying discernible and unique linguistic properties in texts that were written by humans, particularly uncovering the underlying discourse structures of texts beyond their surface structures, using hierarchical parse trees and recursive hypergraphs."
            },
            "score": 5
        },
        {
            "id": "4cf527e9e0d68e3fc16d39fbcdb3869cd3ccf60f",
            "paperId": "4cf527e9e0d68e3fc16d39fbcdb3869cd3ccf60f",
            "title": "Hypothesis Search: Inductive Reasoning with Language Models",
            "abstract": "Inductive reasoning is a core problem-solving capacity: humans can identify underlying principles from a few examples, which can then be robustly generalized to novel scenarios. Recent work has evaluated large language models (LLMs) on inductive reasoning tasks by directly prompting them yielding\"in context learning.\"This can work well for straightforward inductive tasks, but performs very poorly on more complex tasks such as the Abstraction and Reasoning Corpus (ARC). In this work, we propose to improve the inductive reasoning ability of LLMs by generating explicit hypotheses at multiple levels of abstraction: we prompt the LLM to propose multiple abstract hypotheses about the problem, in natural language, then implement the natural language hypotheses as concrete Python programs. These programs can be directly verified by running on the observed examples and generalized to novel inputs. Because of the prohibitive cost of generation with state-of-the-art LLMs, we consider a middle step to filter the set of hypotheses that will be implemented into programs: we either ask the LLM to summarize into a smaller set of hypotheses, or ask human annotators to select a subset of the hypotheses. We verify our pipeline's effectiveness on the ARC visual inductive reasoning benchmark, its variant 1D-ARC, and string transformation dataset SyGuS. On a random 40-problem subset of ARC, our automated pipeline using LLM summaries achieves 27.5% accuracy, significantly outperforming the direct prompting baseline (accuracy of 12.5%). With the minimal human input of selecting from LLM-generated candidates, the performance is boosted to 37.5%. (And we argue this is a lower bound on the performance of our approach without filtering.) Our ablation studies show that abstract hypothesis generation and concrete program representations are both beneficial for LLMs to perform inductive reasoning tasks.",
            "year": 2023,
            "citationCount": 25,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes to improve the inductive reasoning ability of LLMs by generating explicit hypotheses at multiple levels of abstraction: it prompts the LLM to propose multiple abstract hypotheses about the problem, in natural language, then implements the natural language hypotheses as concrete Python programs."
            },
            "score": 5
        },
        {
            "id": "498564ba804249bac269a46416ac31159ace6695",
            "paperId": "498564ba804249bac269a46416ac31159ace6695",
            "title": "PsyCoT: Psychological Questionnaire as Powerful Chain-of-Thought for Personality Detection",
            "abstract": "Recent advances in large language models (LLMs), such as ChatGPT, have showcased remarkable zero-shot performance across various NLP tasks. However, the potential of LLMs in personality detection, which involves identifying an individual's personality from their written texts, remains largely unexplored. Drawing inspiration from Psychological Questionnaires, which are carefully designed by psychologists to evaluate individual personality traits through a series of targeted items, we argue that these items can be regarded as a collection of well-structured chain-of-thought (CoT) processes. By incorporating these processes, LLMs can enhance their capabilities to make more reasonable inferences on personality from textual input. In light of this, we propose a novel personality detection method, called PsyCoT, which mimics the way individuals complete psychological questionnaires in a multi-turn dialogue manner. In particular, we employ a LLM as an AI assistant with a specialization in text analysis. We prompt the assistant to rate individual items at each turn and leverage the historical rating results to derive a conclusive personality preference. Our experiments demonstrate that PsyCoT significantly improves the performance and robustness of GPT-3.5 in personality detection, achieving an average F1 score improvement of 4.23/10.63 points on two benchmark datasets compared to the standard prompting method. Our code is available at https://github.com/TaoYang225/PsyCoT.",
            "year": 2023,
            "citationCount": 5,
            "tldr": null,
            "score": 5
        },
        {
            "id": "5ff9cd8fcb959ca6b458c11e780d61c3f2bf7691",
            "paperId": "5ff9cd8fcb959ca6b458c11e780d61c3f2bf7691",
            "title": "PLACES: Prompting Language Models for Social Conversation Synthesis",
            "abstract": "Collecting high quality conversational data can be very expensive for most applications and infeasible for others due to privacy, ethical, or similar concerns. A promising direction to tackle this problem is to generate synthetic dialogues by prompting large language models. In this work, we use a small set of expert-written conversations as in-context examples to synthesize a social conversation dataset using prompting. We perform several thorough evaluations of our synthetic conversations compared to human-collected conversations. This includes various dimensions of conversation quality with human evaluation directly on the synthesized conversations, and interactive human evaluation of chatbots fine-tuned on the synthetically generated dataset. We additionally demonstrate that this prompting approach is generalizable to multi-party conversations, providing potential to create new synthetic data for multi-party tasks. Our synthetic multi-party conversations were rated more favorably across all measured dimensions compared to conversation excerpts sampled from a human-collected multi-party dataset.",
            "year": 2023,
            "citationCount": 37,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work uses a small set of expert-written conversations as in-context examples to synthesize a social conversation dataset using prompting, and demonstrates that this prompting approach is generalizable to multi-party conversations, providing potential to create new synthetic data for multi- party tasks."
            },
            "score": 4
        },
        {
            "id": "17c0fb8e853230cc3abcd96d8427e5edcbcb1a57",
            "paperId": "17c0fb8e853230cc3abcd96d8427e5edcbcb1a57",
            "title": "InfoVisDial: An Informative Visual Dialogue Dataset by Bridging Large Multimodal and Language Models",
            "abstract": "In this paper, we build a visual dialogue dataset, named InfoVisDial, which provides rich informative answers in each round even with external knowledge related to the visual content. Different from existing datasets where the answer is compact and short, InfoVisDial contains long free-form answers with rich information in each round of dialogue. For effective data collection, the key idea is to bridge the large-scale multimodal model (e.g., GIT) and the language models (e.g., GPT-3). GIT can describe the image content even with scene text, while GPT-3 can generate informative dialogue based on the image description and appropriate prompting techniques. With such automatic pipeline, we can readily generate informative visual dialogue data at scale. Then, we ask human annotators to rate the generated dialogues to filter the low-quality conversations.Human analyses show that InfoVisDial covers informative and diverse dialogue topics: $54.4\\%$ of the dialogue rounds are related to image scene texts, and $36.7\\%$ require external knowledge. Each round's answer is also long and open-ended: $87.3\\%$ of answers are unique with an average length of $8.9$, compared with $27.37\\%$ and $2.9$ in VisDial. Last, we propose a strong baseline by adapting the GIT model for the visual dialogue task and fine-tune the model on InfoVisDial. Hopefully, our work can motivate more effort on this direction.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A visual dialogue dataset, named InfoVisDial, which provides rich informative answers in each round even with external knowledge related to the visual content, and proposes a strong baseline by adapting the GIT model for the visual dialogue task and fine-tune the model on InfoVisDial."
            },
            "score": 4
        },
        {
            "id": "795f341e5d02b8bc86d5eaad857e7e01a6607dd7",
            "paperId": "795f341e5d02b8bc86d5eaad857e7e01a6607dd7",
            "title": "Attribute Controlled Dialogue Prompting",
            "abstract": "Prompt-tuning has become an increasingly popular parameter-efficient method for adapting large pretrained language models to downstream tasks. However, both discrete prompting and continuous prompting assume fixed prompts for all data samples within a task, neglecting the fact that inputs vary greatly in some tasks such as open-domain dialogue generation. In this paper, we present a novel, instance-specific prompt-tuning algorithm for dialogue generation. Specifically, we generate prompts based on instance-level control code, rather than the conversation history, to explore their impact on controlled dialogue generation. Experiments on popular open-domain dialogue datasets, evaluated on both automated metrics and human evaluation, demonstrate that our method is superior to prompting baselines and comparable to fine-tuning with only 5%-6% of total parameters.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper generates prompts based on instance-level control code, rather than the conversation history, to explore their impact on controlled dialogue generation and demonstrates that this method is superior to prompting baselines and comparable to fine-tuning with only 5%-6% of total parameters."
            },
            "score": 4
        },
        {
            "id": "723a6340ee641e190b22bb47455d05e3b4237179",
            "paperId": "723a6340ee641e190b22bb47455d05e3b4237179",
            "title": "Large Language Models as Zero-shot Dialogue State Tracker through Function Calling",
            "abstract": "Large language models (LLMs) are increasingly prevalent in conversational systems due to their advanced understanding and generative capabilities in general contexts. However, their effectiveness in task-oriented dialogues (TOD), which requires not only response generation but also effective dialogue state tracking (DST) within specific tasks and domains, remains less satisfying. In this work, we propose a novel approach FnCTOD for solving DST with LLMs through function calling. This method improves zero-shot DST, allowing adaptation to diverse domains without extensive data collection or model tuning. Our experimental results demonstrate that our approach achieves exceptional performance with both modestly sized open-source and also proprietary LLMs: with in-context prompting it enables various 7B or 13B parameter models to surpass the previous state-of-the-art (SOTA) achieved by ChatGPT, and improves ChatGPT's performance beating the SOTA by 5.6% average joint goal accuracy (JGA). Individual model results for GPT-3.5 and GPT-4 are boosted by 4.8% and 14%, respectively. We also show that by fine-tuning on a small collection of diverse task-oriented dialogues, we can equip modest at https://github.com/facebookresearch/FnCTOD",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel approach FnCTOD for solving DST with LLMs through function calling that improves zero-shot DST, allowing adaptation to diverse domains without extensive data collection or model tuning."
            },
            "score": 4
        },
        {
            "id": "ed2221b2260169acf5fe962cf757e46082f85bbf",
            "paperId": "ed2221b2260169acf5fe962cf757e46082f85bbf",
            "title": "Controllable Mixed-Initiative Dialogue Generation through Prompting",
            "abstract": "Mixed-initiative dialogue tasks involve repeated exchanges of information and conversational control. Conversational agents gain control by generating responses that follow particular dialogue intents or strategies, prescribed by a policy planner. The standard approach has been fine-tuning pre-trained language models to perform generation conditioned on these intents. However, these supervised generation models are limited by the cost and quality of data annotation.We instead prompt large language models as a drop-in replacement to fine-tuning on conditional generation. We formalize prompt construction for controllable mixed-initiative dialogue. Our findings show improvements over fine-tuning and ground truth responses according to human evaluation and automatic metrics for two tasks: PersuasionForGood and Emotional Support Conversations.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work formalizes prompt construction for controllable mixed-initiative dialogue and shows improvements over fine-tuning and ground truth responses according to human evaluation and automatic metrics for two tasks: PersuasionForGood and Emotional Support Conversations."
            },
            "score": 4
        },
        {
            "id": "f8b8f926bbfa327c86c40796131fe2695db81126",
            "paperId": "f8b8f926bbfa327c86c40796131fe2695db81126",
            "title": "DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models",
            "abstract": "A long-standing goal of AI systems is to perform complex multimodal reasoning like humans. Recently, large language models (LLMs) have made remarkable strides in such multi-step reasoning on the language modality solely by leveraging the chain of thought (CoT) to mimic human thinking. However, the transfer of these advancements to multimodal contexts introduces heightened challenges, including but not limited to the impractical need for labor-intensive annotation and the limitations in terms of flexibility, generalizability, and explainability. To evoke CoT reasoning in multimodality, this work first conducts an in-depth analysis of these challenges posed by multimodality and presents two key insights:\"keeping critical thinking\"and\"letting everyone do their jobs\"in multimodal CoT reasoning. Furthermore, this study proposes a novel DDCoT prompting that maintains a critical attitude through negative-space prompting and incorporates multimodality into reasoning by first dividing the reasoning responsibility of LLMs into reasoning and recognition and then integrating the visual recognition capability of visual models into the joint reasoning process. The rationales generated by DDCoT not only improve the reasoning abilities of both large and small language models in zero-shot prompting and fine-tuning learning, significantly outperforming state-of-the-art methods but also exhibit impressive generalizability and explainability.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study proposes a novel DDCoT prompting that maintains a critical attitude through negative-space prompting and incorporates multimodality into reasoning by first dividing the reasoning responsibility of LLMs into reasoning and recognition and then integrating the visual recognition capability of visual models into the joint reasoning process."
            },
            "score": 4
        },
        {
            "id": "306a312c0bae22f30a406187ab18c5724cefb661",
            "paperId": "306a312c0bae22f30a406187ab18c5724cefb661",
            "title": "Memory Injections: Correcting Multi-Hop Reasoning Failures During Inference in Transformer-Based Language Models",
            "abstract": "Answering multi-hop reasoning questions requires retrieving and synthesizing information from diverse sources. Large Language Models (LLMs) struggle to perform such reasoning consistently. Here we propose an approach to pinpoint and rectify multi-hop reasoning failures through targeted memory injections on LLM attention heads. First, we analyze the per-layer activations of GPT-2 models in response to single and multi-hop prompts. We then propose a mechanism that allows users to inject pertinent prompt-specific information, which we refer to as \u201cmemories,\u201d at critical LLM locations during inference. By thus enabling the LLM to incorporate additional relevant information during inference, we enhance the quality of multi-hop prompt completions. We show empirically that a simple, efficient, and targeted memory injection into a key attention layer can often increase the probability of the desired next token in multi-hop tasks, by up to 424%.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown empirically that a simple, efficient, and targeted memory injection into a key attention layer can often increase the probability of the desired next token in multi-hop tasks, by up to 424%."
            },
            "score": 4
        },
        {
            "id": "99bfe503743c5ec8e16e50ab8438159cdb533a89",
            "paperId": "99bfe503743c5ec8e16e50ab8438159cdb533a89",
            "title": "The Troubling Emergence of Hallucination in Large Language Models - An Extensive Definition, Quantification, and Prescriptive Remediations",
            "abstract": "The recent advancements in Large Language Models (LLMs) have garnered widespread acclaim for their remarkable emerging capabilities. However, the issue of hallucination has parallelly emerged as a by-product, posing significant concerns. While some recent endeavors have been made to identify and mitigate different types of hallucination, there has been a limited emphasis on the nuanced categorization of hallucination and associated mitigation methods. To address this gap, we offer a fine-grained discourse on profiling hallucination based on its degree, orientation, and category, along with offering strategies for alleviation. As such, we define two overarching orientations of hallucination: (i) factual mirage (FM) and (ii) silver lining (SL). To provide a more comprehensive understanding, both orientations are further sub-categorized into intrinsic and extrinsic, with three degrees of severity - (i) mild, (ii) moderate, and (iii) alarming. We also meticulously categorize hallucination into six types: (i) acronym ambiguity, (ii) numeric nuisance, (iii) generated golem, (iv) virtual voice, (v) geographic erratum, and (vi) time wrap. Furthermore, we curate HallucInation eLiciTation (HILT), a publicly available dataset comprising of 75,000 samples generated using 15 contemporary LLMs along with human annotations for the aforementioned categories. Finally, to establish a method for quantifying and to offer a comparative spectrum that allows us to evaluate and rank LLMs based on their vulnerability to producing hallucinations, we propose Hallucination Vulnerability Index (HVI). We firmly believe that HVI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making. In conclusion, we propose two solution strategies for mitigating hallucinations.",
            "year": 2023,
            "citationCount": 34,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work defines two overarching orientations of hallucination and proposes two solution strategies for mitigating hallucinations, and firmly believes that HVI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making."
            },
            "score": 4
        },
        {
            "id": "77e4c6e347e35627671907ea941ac881c7466c5e",
            "paperId": "77e4c6e347e35627671907ea941ac881c7466c5e",
            "title": "The Troubling Emergence of Hallucination in Large Language The Troubling Emergence of Hallucination in Large Language Models--An Extensive Definition, Quantification, and Prescriptive Models--An Extensive Definition, Quantification, and Prescriptive Remediations Remediations",
            "abstract": "The recent advancements in Large Language Models (LLMs) have garnered widespread ac-claim for their remarkable emerging capabilities . However, the issue of hallucination has parallelly emerged as a by-product, posing significant concerns. While some recent endeavors have been made to identify and mitigate different types of hallucination, there has been a limited emphasis on the nuanced categorization of hallucination and associated mitigation methods. To address this gap, we offer a fine-grained discourse on profiling hallucination based on its degree, orientation, and category , along with offering strategies for alleviation. As such, we define two overarching orientations of hallucination: (i) factual mirage (FM) and (ii) silver lining (SL) . To provide a more comprehensive understanding, both orientations are further sub-categorized into intrinsic and extrinsic , with three degrees of severity - (i) mild , (ii) moderate , and (iii) alarming . We also meticulously categorize hallucination into six types: (i) acronym ambiguity , (ii) numeric nuisance , (iii) generated golem , (iv) virtual voice , (v) geographic erratum , and (vi) time wrap . Furthermore, we curate H alluc I nation e L ici T ation ( ), a publicly available dataset comprising of 75,000 samples generated using 15 contemporary LLMs along with",
            "year": null,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work defines two overarching orientations of hallucination: (i) factual mirage (FM) and (ii) silver lining (SL) and meticulously categorizes hallucination into six types."
            },
            "score": 4
        },
        {
            "id": "7d29a84a589aa5655e5d3fed8d725ea472816599",
            "paperId": "7d29a84a589aa5655e5d3fed8d725ea472816599",
            "title": "Explanations from Large Language Models Make Small Reasoners Better",
            "abstract": "Integrating free-text explanations to in-context learning of large language models (LLM) is shown to elicit strong reasoning capabilities along with reasonable explanations. In this paper, we consider the problem of leveraging the explanations generated by LLM to improve the training of small reasoners, which are more favorable in real-production deployment due to their low cost. We systematically explore three explanation generation approaches from LLM and utilize a multi-task learning framework to facilitate small models to acquire strong reasoning power together with explanation generation capabilities. Experiments on multiple reasoning tasks show that our method can consistently and significantly outperform finetuning baselines across different settings, and even perform better than finetuning/prompting a 60x larger GPT-3 (175B) model by up to 9.5% in accuracy. As a side benefit, human evaluation further shows that our method can generate high-quality explanations to justify its predictions, moving towards the goal of explainable AI.",
            "year": 2022,
            "citationCount": 66,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper considers the problem of leveraging the explanations generated by LLM to improve the training of small reasoners, which are more favorable in real-production deployment due to their low cost."
            },
            "score": 4
        },
        {
            "id": "29652bb2dc0396ab27c0be0c5f24c114c757df0f",
            "paperId": "29652bb2dc0396ab27c0be0c5f24c114c757df0f",
            "title": "Evaluating Hallucinations in Chinese Large Language Models",
            "abstract": "In this paper, we establish a benchmark named HalluQA (Chinese Hallucination Question-Answering) to measure the hallucination phenomenon in Chinese large language models. HalluQA contains 450 meticulously designed adversarial questions, spanning multiple domains, and takes into account Chinese historical culture, customs, and social phenomena. During the construction of HalluQA, we consider two types of hallucinations: imitative falsehoods and factual errors, and we construct adversarial samples based on GLM-130B and ChatGPT. For evaluation, we design an automated evaluation method using GPT-4 to judge whether a model output is hallucinated. We conduct extensive experiments on 24 large language models, including ERNIE-Bot, Baichuan2, ChatGLM, Qwen, SparkDesk and etc. Out of the 24 models, 18 achieved non-hallucination rates lower than 50%. This indicates that HalluQA is highly challenging. We analyze the primary types of hallucinations in different types of models and their causes. Additionally, we discuss which types of hallucinations should be prioritized for different types of models.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A benchmark named HalluQA (Chinese Hallucination Question-Answering) is established to measure the hallucination phenomenon in Chinese large language models and discusses which types of hallucinations should be prioritized for different types of models."
            },
            "score": 4
        },
        {
            "id": "c6f1fa0228eecfcd0a83b601a7f0fbf5b55b3368",
            "paperId": "c6f1fa0228eecfcd0a83b601a7f0fbf5b55b3368",
            "title": "LLM Comparative Assessment: Zero-shot NLG Evaluation through Pairwise Comparisons using Large Language Models",
            "abstract": "Current developments in large language models (LLMs) have enabled impressive zero-shot capabilities across various natural language tasks. An interesting application of these systems is in the automated assessment of natural language generation (NLG), a highly challenging area with great practical benefit. In this paper, we explore two options for exploiting the emergent abilities of LLMs for zero-shot NLG assessment: absolute score prediction, and comparative assessment which uses relative comparisons between pairs of candidates. Though comparative assessment has not been extensively studied in NLG assessment, we note that humans often find it more intuitive to compare two options rather than scoring each one independently. This work examines comparative assessment from multiple perspectives: performance compared to absolute grading; positional biases in the prompt; and efficient ranking in terms of the number of comparisons. We illustrate that LLM comparative assessment is a simple, general and effective approach for NLG assessment. For moderate-sized open-source LLMs, such as FlanT5 and Llama2-chat, comparative assessment is superior to prompt scoring, and in many cases can achieve performance competitive with state-of-the-art methods. Additionally, we demonstrate that LLMs often exhibit strong positional biases when making pairwise comparisons, and we propose debiasing methods that can further improve performance.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is illustrated that LLM comparative assessment is a simple, general and effective approach for NLG assessment, and for moderate-sized open-source LLMs, superior to prompt scoring, and in many cases can achieve performance competitive with state-of-the-art methods."
            },
            "score": 4
        },
        {
            "id": "2522410b1cac0c14fa656a0aaeaff08bacb358a9",
            "paperId": "2522410b1cac0c14fa656a0aaeaff08bacb358a9",
            "title": "InterroLang: Exploring NLP Models and Datasets through Dialogue-based Explanations",
            "abstract": "While recently developed NLP explainability methods let us open the black box in various ways (Madsen et al., 2022), a missing ingredient in this endeavor is an interactive tool offering a conversational interface. Such a dialogue system can help users explore datasets and models with explanations in a contextualized manner, e.g. via clarification or follow-up questions, and through a natural language interface. We adapt the conversational explanation framework TalkToModel (Slack et al., 2022) to the NLP domain, add new NLP-specific operations such as free-text rationalization, and illustrate its generalizability on three NLP tasks (dialogue act classification, question answering, hate speech detection). To recognize user queries for explanations, we evaluate fine-tuned and few-shot prompting models and implement a novel Adapter-based approach. We then conduct two user studies on (1) the perceived correctness and helpfulness of the dialogues, and (2) the simulatability, i.e. how objectively helpful dialogical explanations are for humans in figuring out the model's predicted label when it's not shown. We found rationalization and feature attribution were helpful in explaining the model behavior. Moreover, users could more reliably predict the model outcome based on an explanation dialogue rather than one-off explanations.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work adapts the conversational explanation framework TalkToModel to the NLP domain, adds new NLP-specific operations such as free-text rationalization and feature attribution, and illustrates its generalizability on three NLP tasks."
            },
            "score": 4
        },
        {
            "id": "f3e54291e235df8ca91d3c83697c392155ed584d",
            "paperId": "f3e54291e235df8ca91d3c83697c392155ed584d",
            "title": "Does GPT-3 Generate Empathetic Dialogues? A Novel In-Context Example Selection Method and Automatic Evaluation Metric for Empathetic Dialogue Generation",
            "abstract": "Since empathy plays a crucial role in increasing social bonding between people, many studies have designed their own dialogue agents to be empathetic using the well-established method of fine-tuning. However, they do not use prompt-based in-context learning, which has shown powerful performance in various natural language processing (NLP) tasks, for empathetic dialogue generation. Although several studies have investigated few-shot in-context learning for empathetic dialogue generation, an in-depth analysis of the generation of empathetic dialogue with in-context learning remains unclear, especially in GPT-3 (Brown et al., 2020). In this study, we explore whether GPT-3 can generate empathetic dialogues through prompt-based in-context learning in both zero-shot and few-shot settings. To enhance performance, we propose two new in-context example selection methods, called SITSM and EMOSITSM, that utilize emotion and situational information. We also introduce a new automatic evaluation method, DIFF-EPITOME, which reflects the human tendency to express empathy. From the analysis, we reveal that our DIFF-EPITOME is effective in measuring the degree of human empathy. We show that GPT-3 achieves competitive performance with Blender 90M, a state-of-the-art dialogue generative model, on both automatic and human evaluation. Our code is available at https://github.com/passing2961/EmpGPT-3.",
            "year": 2022,
            "citationCount": 26,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study explores whether GPT-3 can generate empathetic dialogues through prompt-based in-context learning in both zero-shot and few-shot settings and introduces a new automatic evaluation method, DIFF-EPITOME, which reflects the human tendency to express empathy."
            },
            "score": 4
        },
        {
            "id": "b407333315ed98c023caf04ca8cac90c74f2fb3e",
            "paperId": "b407333315ed98c023caf04ca8cac90c74f2fb3e",
            "title": "CritiqueLLM: Scaling LLM-as-Critic for Effective and Explainable Evaluation of Large Language Model Generation",
            "abstract": "Since the natural language processing (NLP) community started to make large language models (LLMs), such as GPT-4, act as a critic to evaluate the quality of generated texts, most of them only train a critique generation model of a specific scale on specific datasets. We argue that a comprehensive investigation on the key factor of LLM-based evaluation models, such as scaling properties, is lacking, so that it is still inconclusive whether these models have potential to replace GPT-4's evaluation in practical scenarios. In this paper, we propose a new critique generation model called CritiqueLLM, which includes a dialogue-based prompting method for high-quality referenced / reference-free evaluation data. Experimental results show that our model can achieve comparable evaluation performance to GPT-4 especially in system-level correlations, and even outperform GPT-4 in 3 out of 8 tasks in a challenging reference-free setting. We conduct detailed analysis to show promising scaling properties of our model in the quality of generated critiques. We also demonstrate that our generated critiques can act as scalable feedback to directly improve the generation quality of LLMs.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new critique generation model called CritiqueLLM, which includes a dialogue-based prompting method for high-quality referenced / reference-free evaluation data and can achieve comparable evaluation performance to GPT-4 especially in system-level correlations."
            },
            "score": 4
        },
        {
            "id": "c2f9006993d9d84d48eb894aab3ba60f946d0e15",
            "paperId": "c2f9006993d9d84d48eb894aab3ba60f946d0e15",
            "title": "EduChat: A Large-Scale Language Model-based Chatbot System for Intelligent Education",
            "abstract": "EduChat (https://www.educhat.top/) is a large-scale language model (LLM)-based chatbot system in the education domain. Its goal is to support personalized, fair, and compassionate intelligent education, serving teachers, students, and parents. Guided by theories from psychology and education, it further strengthens educational functions such as open question answering, essay assessment, Socratic teaching, and emotional support based on the existing basic LLMs. Particularly, we learn domain-specific knowledge by pre-training on the educational corpus and stimulate various skills with tool use by fine-tuning on designed system prompts and instructions. Currently, EduChat is available online as an open-source project, with its code, data, and model parameters available on platforms (e.g., GitHub https://github.com/icalk-nlp/EduChat, Hugging Face https://huggingface.co/ecnu-icalk ). We also prepare a demonstration of its capabilities online (https://vimeo.com/851004454). This initiative aims to promote research and applications of LLMs for intelligent education.",
            "year": 2023,
            "citationCount": 29,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This initiative aims to promote research and applications of LLMs for intelligent education by learning domain-specific knowledge by pre-training on the educational corpus and stimulating various skills with tool use by fine- tuning on designed system prompts and instructions."
            },
            "score": 4
        },
        {
            "id": "dffdd1686218128e0284e45c24fdbc36976f31c1",
            "paperId": "dffdd1686218128e0284e45c24fdbc36976f31c1",
            "title": "MEEP: Is this Engaging? Prompting Large Language Models for Dialogue Evaluation in Multilingual Settings",
            "abstract": null,
            "year": 2023,
            "citationCount": 4,
            "tldr": null,
            "score": 3
        },
        {
            "id": "fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c",
            "paperId": "fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c",
            "title": "Language Is Not All You Need: Aligning Perception with Language Models",
            "abstract": "A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale multimodal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. We evaluate various settings, including zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range of tasks without any gradient updates or finetuning. Experimental results show that Kosmos-1 achieves impressive performance on (i) language understanding, generation, and even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language. In addition, we introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs.",
            "year": 2023,
            "citationCount": 294,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context, and follow instructions, and shows that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodals, and from multimodal to language."
            },
            "score": 3
        },
        {
            "id": "48f1a6d9ff8661be2a256422cd3f0e86fc466b5a",
            "paperId": "48f1a6d9ff8661be2a256422cd3f0e86fc466b5a",
            "title": "PSYDIAL: Personality-based Synthetic Dialogue Generation using Large Language Models",
            "abstract": "We present a novel end-to-end personality-based synthetic dialogue data generation pipeline, specifically designed to elicit responses from large language models via prompting. We design the prompts to generate more human-like dialogues considering real-world scenarios when users engage with chatbots. We introduce PSYDIAL, the first Korean dialogue dataset focused on personality-based dialogues, curated using our proposed pipeline. Notably, we focus on the Extraversion dimension of the Big Five personality model in our research. Experimental results indicate that while pre-trained models and those fine-tuned with a chit-chat dataset struggle to generate responses reflecting personality, models trained with PSYDIAL show significant improvements. The versatility of our pipeline extends beyond dialogue tasks, offering potential for other non-dialogue related applications. This research opens doors for more nuanced, personality-driven conversational AI in Korean and potentially other languages. Our code is publicly available at https://github.com/jiSilverH/psydial.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel end-to-end personality-based synthetic dialogue data generation pipeline, specifically designed to elicit responses from large language models via prompting, focusing on the Extraversion dimension of the Big Five personality model in this research."
            },
            "score": 3
        },
        {
            "id": "10632e0a667cbc3c52cc8f11a46d8e8e9c7739e3",
            "paperId": "10632e0a667cbc3c52cc8f11a46d8e8e9c7739e3",
            "title": "Causal Reasoning and Large Language Models: Opening a New Frontier for Causality",
            "abstract": "The causal capabilities of large language models (LLMs) is a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We further our understanding of LLMs and their causal implications, considering the distinctions between different types of causal reasoning tasks, as well as the entangled threats of construct and measurement validity. LLM-based methods establish new state-of-the-art accuracies on multiple causal benchmarks. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain), and actual causality (86% accuracy in determining necessary and sufficient causes in vignettes). At the same time, LLMs exhibit unpredictable failure modes and we provide some techniques to interpret their robustness. Crucially, LLMs perform these causal tasks while relying on sources of knowledge and methods distinct from and complementary to non-LLM based approaches. Specifically, LLMs bring capabilities so far understood to be restricted to humans, such as using collected knowledge to generate causal graphs or identifying background causal context from natural language. We envision LLMs to be used alongside existing causal methods, as a proxy for human domain knowledge and to reduce human effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods. We also see existing causal methods as promising tools for LLMs to formalize, validate, and communicate their reasoning especially in high-stakes scenarios. In capturing common sense and domain knowledge about causal mechanisms and supporting translation between natural language and formal methods, LLMs open new frontiers for advancing the research, practice, and adoption of causality.",
            "year": 2023,
            "citationCount": 96,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LLMs are envisioned to be used alongside existing causal methods, as a proxy for human domain knowledge and to reduce human effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods."
            },
            "score": 3
        },
        {
            "id": "f590cbb28e4994f62e94bf9400a9cb33e99922fa",
            "paperId": "f590cbb28e4994f62e94bf9400a9cb33e99922fa",
            "title": "Understanding Social Reasoning in Language Models with Language Models",
            "abstract": "As Large Language Models (LLMs) become increasingly integrated into our everyday lives, understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. However, despite the recent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of LLMs, the degree to which these models can align with human ToM remains a nuanced topic of exploration. This is primarily due to two distinct challenges: (1) the presence of inconsistent results from previous evaluations, and (2) concerns surrounding the validity of existing evaluation methodologies. To address these challenges, we present a novel framework for procedurally generating evaluations with LLMs by populating causal templates. Using our framework, we create a new social reasoning benchmark (BigToM) for LLMs which consists of 25 controls and 5,000 model-written evaluations. We find that human participants rate the quality of our benchmark higher than previous crowd-sourced evaluations and comparable to expert-written evaluations. Using BigToM, we evaluate the social reasoning capabilities of a variety of LLMs and compare model performances with human performance. Our results suggest that GPT4 has ToM capabilities that mirror human inference patterns, though less reliable, while other LLMs struggle.",
            "year": 2023,
            "citationCount": 21,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents a novel framework for procedurally generating evaluations with LLMs by populating causal templates and creates a new social reasoning benchmark (BigToM) for LLMs which is found that human participants rate the quality of this benchmark higher than previous crowd-sourced evaluations and comparable to expert-written evaluations."
            },
            "score": 3
        },
        {
            "id": "a2d069eb8c772e51292d52e297a464a5eb51979f",
            "paperId": "a2d069eb8c772e51292d52e297a464a5eb51979f",
            "title": "NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes",
            "abstract": "Complex reasoning ability is one of the most important features of current LLMs, which has also been leveraged to play an integral role in complex decision-making tasks. Therefore, the investigation into the reasoning capabilities of Large Language Models (LLMs) is critical: numerous benchmarks have been established to assess the reasoning abilities of LLMs. However, current benchmarks are inadequate in offering a rigorous evaluation of the full extent of reasoning abilities that LLMs are capable of achieving. They are also prone to the risk of overfitting, as these benchmarks, being publicly accessible and static, allow models to potentially tailor their responses to specific benchmark metrics, thereby inflating their performance. Addressing these limitations, our research introduces a new benchmark, named NPHardEval. This benchmark is designed to evaluate the reasoning abilities of LLMs across a broad spectrum of 900 algorithmic questions, extending up to the NP-Hard complexity class. These questions are meticulously chosen to represent a wide range of complexity class below the NP-hard complexity class, offering a rigorous measure of the reasoning ability of LLMs. Through this study, we shed light on the current state of reasoning in LLMs, providing an objective and rigorous perspective through the comparison of LLMs' performance across complex classes. Moreover, this benchmark is designed with a dynamic update mechanism, where the datapoints are refreshed on a monthly basis. Such regular updates play a crucial role in mitigating the risk of LLMs overfitting to the benchmark, promoting a more accurate and reliable assessment of their reasoning capabilities. The benchmark dataset and code of NPHardEval are available at https://github.com/casmlab/NPHardEval.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Light is shed on the current state of reasoning in LLMs, providing an objective and rigorous perspective through the comparison of LLMs' performance across complex classes, and introduces a new benchmark, named NPHardEval."
            },
            "score": 3
        },
        {
            "id": "18cad1aa7a3f6e783183a5588e85a9dc3ba6ede6",
            "paperId": "18cad1aa7a3f6e783183a5588e85a9dc3ba6ede6",
            "title": "Enhancing Quantitative Reasoning Skills of Large Language Models through Dimension Perception",
            "abstract": "Quantities are distinct and critical components of texts that characterize the magnitude properties of entities, providing a precise perspective for the understanding of natural language, especially for reasoning tasks. In recent years, there has been a flurry of research on reasoning tasks based on large language models (LLMs), most of which solely focus on numerical values, neglecting the dimensional concept of quantities with units despite its importance. We argue that the concept of dimension is essential for precisely understanding quantities and of great significance for LLMs to perform quantitative reasoning. However, the lack of dimension knowledge and quantity-related benchmarks has resulted in low performance of LLMs. Hence, we present a framework to enhance the quantitative reasoning ability of language models based on dimension perception. We first construct a dimensional unit knowledge base (DimUnitKB) to address the knowledge gap in this area. We propose a benchmark DimEval consisting of seven tasks of three categories to probe and enhance the dimension perception skills of LLMs. To evaluate the effectiveness of our methods, we propose a quantitative reasoning task and conduct experiments. The experimental results show that our dimension perception method dramatically improves accuracy (43.55%->50.67%) on quantitative reasoning tasks compared to GPT-4.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents a framework to enhance the quantitative reasoning ability of language models based on dimension perception and proposes a benchmark DimEval consisting of seven tasks of three categories to probe and enhance the dimension perception skills of LLMs."
            },
            "score": 3
        },
        {
            "id": "4dbae943822dcb21d3d1dd757b70740615bb4172",
            "paperId": "4dbae943822dcb21d3d1dd757b70740615bb4172",
            "title": "Moderating New Waves of Online Hate with Chain-of-Thought Reasoning in Large Language Models",
            "abstract": "Online hate is an escalating problem that negatively impacts the lives of Internet users, and is also subject to rapid changes due to evolving events, resulting in new waves of online hate that pose a critical threat. Detecting and mitigating these new waves present two key challenges: it demands reasoning-based complex decision-making to determine the presence of hateful content, and the limited availability of training samples hinders updating the detection model. To address this critical issue, we present a novel framework called HATEGUARD for effectively moderating new waves of online hate. HATEGUARD employs a reasoning-based approach that leverages the recently introduced chain-of-thought (CoT) prompting technique, harnessing the capabilities of large language models (LLMs). HATEGUARD further achieves prompt-based zero-shot detection by automatically generating and updating detection prompts with new derogatory terms and targets in new wave samples to effectively address new waves of online hate. To demonstrate the effectiveness of our approach, we compile a new dataset consisting of tweets related to three recently witnessed new waves: the 2022 Russian invasion of Ukraine, the 2021 insurrection of the US Capitol, and the COVID-19 pandemic. Our studies reveal crucial longitudinal patterns in these new waves concerning the evolution of events and the pressing need for techniques to rapidly update existing moderation tools to counteract them. Comparative evaluations against state-of-the-art tools illustrate the superiority of our framework, showcasing a substantial 22.22% to 83.33% improvement in detecting the three new waves of online hate. Our work highlights the severe threat posed by the emergence of new waves of online hate and represents a paradigm shift in addressing this threat practically.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work highlights the severe threat posed by the emergence of new waves of online hate and represents a paradigm shift in addressing this threat practically, and presents a novel framework called HATEGUARD for effectively moderating new waves of online hate."
            },
            "score": 3
        },
        {
            "id": "718343008a6cfca9e86ab6160caba353c52c17cf",
            "paperId": "718343008a6cfca9e86ab6160caba353c52c17cf",
            "title": "KMIR: A Benchmark for Evaluating Knowledge Memorization, Identification and Reasoning Abilities of Language Models",
            "abstract": "Previous works show the great potential of pre-trained language models (PLMs) for storing a large amount of factual knowledge. However, to figure out whether PLMs can be reliable knowledge sources and used as alternative knowledge bases (KBs), we need to further explore some critical features of PLMs. Firstly, knowledge memorization and identification abilities: traditional KBs can store various types of entities and relationships; do PLMs have a high knowledge capacity to store different types of knowledge? Secondly, reasoning ability: a qualified knowledge source should not only provide a collection of facts, but support a symbolic reasoner. Can PLMs derive new knowledge based on the correlations between facts? To evaluate these features of PLMs, we propose a benchmark, named Knowledge Memorization, Identification, and Reasoning test (KMIR). KMIR covers 3 types of knowledge, including general knowledge, domain-specific knowledge, and commonsense, and provides 184,348 well-designed questions. Preliminary experiments with various representative pre-training language models on KMIR reveal many interesting phenomenons: 1) The memorization ability of PLMs depends more on the number of parameters than training schemes. 2) Current PLMs are struggling to robustly remember the facts. 3) Model compression technology retains the amount of knowledge well, but hurts the identification and reasoning abilities. We hope KMIR can facilitate the design of PLMs as better knowledge sources.",
            "year": 2022,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A benchmark, named Knowledge Memorization, Identification, and Reasoning test (KMIR), which covers 3 types of knowledge, including general knowledge, domain-specific knowledge, and commonsense, and it hopes KMIR can facilitate the design of PLMs as better knowledge sources."
            },
            "score": 3
        },
        {
            "id": "e0de25cb903962a54d3c3c5407a54ecbd0707a12",
            "paperId": "e0de25cb903962a54d3c3c5407a54ecbd0707a12",
            "title": "Bias Neutralization Framework: Measuring Fairness in Large Language Models with Bias Intelligence Quotient (BiQ)",
            "abstract": "The burgeoning influence of Large Language Models (LLMs) in shaping public discourse and decision-making underscores the imperative to address inherent biases within these AI systems. In the wake of AI's expansive integration across sectors, addressing racial bias in LLMs has never been more critical. This paper introduces a novel framework called Comprehensive Bias Neutralization Framework (CBNF) which embodies an innovative approach to quantifying and mitigating biases within LLMs. Our framework combines the Large Language Model Bias Index (LLMBI) [Oketunji, A., Anas, M., Saina, D., (2023)] and Bias removaL with No Demographics (BLIND) [Orgad, H., Belinkov, Y. (2023)] methodologies to create a new metric called Bias Intelligence Quotient (BiQ)which detects, measures, and mitigates racial bias in LLMs without reliance on demographic annotations. By introducing a new metric called BiQ that enhances LLMBI with additional fairness metrics, CBNF offers a multi-dimensional metric for bias assessment, underscoring the necessity of a nuanced approach to fairness in AI [Mehrabi et al., 2021]. This paper presents a detailed analysis of Latimer AI (a language model incrementally trained on black history and culture) in comparison to ChatGPT 3.5, illustrating Latimer AI's efficacy in detecting racial, cultural, and gender biases through targeted training and refined bias mitigation strategies [Latimer&Bender, 2023].",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel framework called Comprehensive Bias Neutralization Framework (CBNF) is introduced which embodies an innovative approach to quantifying and mitigating biases within LLMs and introduces a new metric called BiQ that enhances LLMBI with additional fairness metrics."
            },
            "score": 3
        },
        {
            "id": "821200921dfcb7d779ad39ccdeda611a303cca22",
            "paperId": "821200921dfcb7d779ad39ccdeda611a303cca22",
            "title": "Detecting nuance in conspiracy discourse: Advancing methods in infodemiology and communication science with machine learning and qualitative content coding",
            "abstract": "The spread of misinformation and conspiracies has been an ongoing issue since the early stages of the internet era, resulting in the emergence of the field of infodemiology (i.e., information epidemiology), which investigates the transmission of health-related information. Due to the high volume of online misinformation in recent years, there is a need to continue advancing methodologies in order to effectively identify narratives and themes. While machine learning models can be used to detect misinformation and conspiracies, these models are limited in their generalizability to other datasets and misinformation phenomenon, and are often unable to detect implicit meanings in text that require contextual knowledge. To rapidly detect evolving conspiracist narratives within high volume online discourse while identifying nuanced themes requiring the comprehension of subtext, this study describes a hybrid methodology that combines natural language processing (i.e., topic modeling and sentiment analysis) with qualitative content coding approaches to characterize conspiracy discourse related to 5G wireless technology and COVID-19 on Twitter (currently known as \u2018X\u2019). Discourse that focused on correcting 5G conspiracies was also analyzed for comparison. Sentiment analysis shows that conspiracy-related discourse was more likely to use language that was analytic, combative, past-oriented, referenced social status, and expressed negative emotions. Corrections discourse was more likely to use words reflecting cognitive processes, prosocial relations, health-related consequences, and future-oriented language. Inductive coding characterized conspiracist narratives related to global elites, anti-vax sentiment, medical authorities, religious figures, and false correlations between technology advancements and disease outbreaks. Further, the corrections discourse did not address many of the narratives prevalent in conspiracy conversations. This paper aims to further bridge the gap between computational and qualitative methodologies by demonstrating how both approaches can be used in tandem to emphasize the positive aspects of each methodology while minimizing their respective drawbacks.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A hybrid methodology that combines natural language processing with qualitative content coding approaches is described to characterize conspiracy discourse related to 5G wireless technology and COVID-19 on Twitter (currently known as \u2018X\u2019)."
            },
            "score": 3
        },
        {
            "id": "16d8dc4a40a3928d637fd943c64fed2ae727ff48",
            "paperId": "16d8dc4a40a3928d637fd943c64fed2ae727ff48",
            "title": "Understanding Politics via Contextualized Discourse Processing",
            "abstract": "Politicians often have underlying agendas when reacting to events. Arguments in contexts of various events reflect a fairly consistent set of agendas for a given entity. In spite of recent advances in Pretrained Language Models, those text representations are not designed to capture such nuanced patterns. In this paper, we propose a Compositional Reader model consisting of encoder and composer modules, that captures and leverages such information to generate more effective representations for entities, issues, and events. These representations are contextualized by tweets, press releases, issues, news articles, and participating entities. Our model processes several documents at once and generates composed representations for multiple entities over several issues or events. Via qualitative and quantitative empirical analysis, we show that these representations are meaningful and effective.",
            "year": 2020,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a Compositional Reader model consisting of encoder and composer modules, that captures and leverages such information to generate more effective representations for entities, issues, and events."
            },
            "score": 3
        },
        {
            "id": "197b37845c9fb6f76323ad3f877d6ad00a3a8dbf",
            "paperId": "197b37845c9fb6f76323ad3f877d6ad00a3a8dbf",
            "title": "BERT-icus, Transform and Ensemble! Stanford CS224N Default Project",
            "abstract": "\"Transformers, more than meets the eye!\" Transformers have revolutionized natural language processing (NLP) with models like BERT, but they still face challenges such as overfitting and lacking nuanced language understanding. We explore these challenges using minBERT over three human language tasks: sentiment analysis, paraphrase detection, and textual similarity tasks. Techniques such as smoothness-inducing adversarial regularization and cosine-similarity fine-tuning augment min-BERT\u2019s performance. Ensembling methods further enhance results, with weighted \"expert\" averaging yielding optimal outcomes. Our findings contribute to NLP discourse, emphasizing domain expertise and task-specific fine-tuning for effective model optimization, while proposing future avenues for exploration. Through these efforts, we advance the understanding and application of NLP techniques, paving the way for more robust language models to roll out.",
            "year": null,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores challenges using minBERT over three human language tasks: sentiment analysis, paraphrase detection, and textual similarity tasks, and advances the understanding and application of NLP techniques."
            },
            "score": 3
        },
        {
            "id": "77b1f1c6d1658d120456b9046667cf009ceb39ce",
            "paperId": "77b1f1c6d1658d120456b9046667cf009ceb39ce",
            "title": "MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models",
            "abstract": "Large language models (LLMs) have pushed the limits of natural language understanding and exhibited excellent problem-solving ability. Despite the great success, most existing open-source LLMs (e.g., LLaMA-2) are still far away from satisfactory for solving mathematical problem due to the complex reasoning procedures. To bridge this gap, we propose MetaMath, a fine-tuned language model that specializes in mathematical reasoning. Specifically, we start by bootstrapping mathematical questions by rewriting the question from multiple perspectives without extra knowledge, which results in a new dataset called MetaMathQA. Then we fine-tune the LLaMA-2 models on MetaMathQA. Experimental results on two popular benchmarks (i.e., GSM8K and MATH) for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin. Our MetaMath-7B model achieves 66.4% on GSM8K and 19.4% on MATH, exceeding the state-of-the-art models of the same size by 11.5% and 8.7%. Particularly, MetaMath-70B achieves an accuracy of 82.3% on GSM8K, slightly better than GPT-3.5-Turbo. We release all the MetaMathQA dataset, the MetaMath models with different model sizes and the training code for public use.",
            "year": 2023,
            "citationCount": 59,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results on two popular benchmarks for mathematical reasoning demonstrate that MetaMath outperforms a suite of open-source LLMs by a significant margin."
            },
            "score": 3
        },
        {
            "id": "12594b6afe01461384d2856d2bf44f1cf8533e3e",
            "paperId": "12594b6afe01461384d2856d2bf44f1cf8533e3e",
            "title": "ChatGPT and the rise of large language models: the new AI-driven infodemic threat in public health",
            "abstract": "Large Language Models (LLMs) have recently gathered attention with the release of ChatGPT, a user-centered chatbot released by OpenAI. In this perspective article, we retrace the evolution of LLMs to understand the revolution brought by ChatGPT in the artificial intelligence (AI) field. The opportunities offered by LLMs in supporting scientific research are multiple and various models have already been tested in Natural Language Processing (NLP) tasks in this domain. The impact of ChatGPT has been huge for the general public and the research community, with many authors using the chatbot to write part of their articles and some papers even listing ChatGPT as an author. Alarming ethical and practical challenges emerge from the use of LLMs, particularly in the medical field for the potential impact on public health. Infodemic is a trending topic in public health and the ability of LLMs to rapidly produce vast amounts of text could leverage misinformation spread at an unprecedented scale, this could create an \u201cAI-driven infodemic,\u201d a novel public health threat. Policies to contrast this phenomenon need to be rapidly elaborated, the inability to accurately detect artificial-intelligence-produced text is an unresolved issue.",
            "year": 2023,
            "citationCount": 172,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The evolution of LLMs is retrace the evolution to understand the revolution brought by ChatGPT in the artificial intelligence (AI) field, particularly in the medical field for the potential impact on public health."
            },
            "score": 3
        },
        {
            "id": "fccf8776d7525627c518a56a1f4db367a4d7120b",
            "paperId": "fccf8776d7525627c518a56a1f4db367a4d7120b",
            "title": "Choice Over Control: How Users Write with Large Language Models using Diegetic and Non-Diegetic Prompting",
            "abstract": "We propose a conceptual perspective on prompts for Large Language Models (LLMs) that distinguishes between (1) diegetic prompts (part of the narrative, e.g. \u201cOnce upon a time, I saw a fox...\u201d), and (2) non-diegetic prompts (external, e.g. \u201cWrite about the adventures of the fox.\u201d). With this lens, we study how 129 crowd workers on Prolific write short texts with different user interfaces (1 vs 3 suggestions, with/out non-diegetic prompts; implemented with GPT-3): When the interface offered multiple suggestions and provided an option for non-diegetic prompting, participants preferred choosing from multiple suggestions over controlling them via non-diegetic prompts. When participants provided non-diegetic prompts it was to ask for inspiration, topics or facts. Single suggestions in particular were guided both with diegetic and non-diegetic information. This work informs human-AI interaction with generative models by revealing that (1) writing non-diegetic prompts requires effort, (2) people combine diegetic and non-diegetic prompting, and (3) they use their draft (i.e. diegetic information) and suggestion timing to strategically guide LLMs.",
            "year": 2023,
            "citationCount": 30,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work informs human-AI interaction with generative models by revealing that (1) writing non-diegetic prompts requires effort, (2) people combine diegetic and non- diegetic prompting, and (3) they use their draft (i.e. diegetic information) and suggestion timing to strategically guide LLMs."
            },
            "score": 3
        },
        {
            "id": "69c85405cc1986a41f6387d869aa1648a5668d6f",
            "paperId": "69c85405cc1986a41f6387d869aa1648a5668d6f",
            "title": "Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers",
            "abstract": "Large pretrained language models have shown surprising in-context learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without parameter updates. Despite the great success in performance, its working mechanism still remains an open question. In this paper, we explain language models as meta-optimizers and understand in-context learning as implicit finetuning. Theoretically, we figure out that Transformer attention has a dual form of gradient descent. On top of it, we understand ICL as follows: GPT first produces meta-gradients according to the demonstration examples, and then these meta-gradients are applied to the original GPT to build an ICL model. We comprehensively compare the behaviors of in-context learning and explicit finetuning on real tasks to provide empirical evidence that supports our understanding. Experimental results show that in-context learning behaves similarly to explicit finetuning from multiple perspectives. Inspired by the dual form between Transformer attention and gradient descent, we design a momentum-based attention by analogy with gradient descent with momentum. The improved performance over vanilla attention further supports our understanding from another perspective, and more importantly, shows the potential to utilize our understanding for future model design. The code is available at \\url{https://aka.ms/icl}.",
            "year": 2022,
            "citationCount": 49,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A momentum-based attention by analogy with gradient descent with momentum is designed, Inspired by the dual form between Transformer attention and gradient descent, and shows the potential to utilize the understanding for future model design."
            },
            "score": 3
        },
        {
            "id": "ff96527c03fbea7c3bb7d44d1d656d875ddba75e",
            "paperId": "ff96527c03fbea7c3bb7d44d1d656d875ddba75e",
            "title": "Dialogue for Prompting: a Policy-Gradient-Based Discrete Prompt Optimization for Few-shot Learning",
            "abstract": "Prompt-based pre-trained language models (PLMs) paradigm have succeeded substantially in few-shot natural language processing (NLP) tasks. However, prior discrete prompt optimization methods require expert knowledge to design the base prompt set and identify high-quality prompts, which is costly, inefficient, and subjective. Meanwhile, existing continuous prompt optimization methods improve the performance by learning the ideal prompts through the gradient information of PLMs, whose high computational cost, and low readability and generalizability are often concerning. To address the research gap, we propose a D ialogue-comprised P olicy-gradient-based D iscrete P rompt O ptimization (DP 2 O) method. We first design a multi-round dialogue alignment strategy for readability prompt set generation based on GPT-4. Furthermore, we propose an efficient prompt screening metric to identify high-quality prompts with linear complexity. Finally, we construct a reinforcement learning (RL) framework based on policy gradients to match the prompts to inputs optimally. By training a policy network with only 0.67% of the PLM parameter size on the tasks in the few-shot setting, DP 2 O outperforms the state-of-the-art (SOTA) method by 1.52% in accuracy on average on four open-source datasets. Moreover, subsequent experiments also demonstrate that DP 2 O has good universality, robustness and generalization ability.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A multi-round dialogue alignment strategy for readability prompt set generation based on GPT-4, an efficient prompt screening metric to identify high-quality prompts with linear complexity, and a reinforcement learning framework based on policy gradients to match the prompts to inputs optimally are proposed."
            },
            "score": 3
        },
        {
            "id": "c7bdb80bba981052d8ed6ae0a965b4886f36ebf9",
            "paperId": "c7bdb80bba981052d8ed6ae0a965b4886f36ebf9",
            "title": "Dialogue for Prompting: A Policy-Gradient-Based Discrete Prompt Generation for Few-Shot Learning",
            "abstract": "Prompt-based pre-trained language models (PLMs) paradigm has succeeded substantially in few-shot natural language processing (NLP) tasks. However, prior discrete prompt optimization methods require expert knowledge to design the base prompt set and identify high-quality prompts, which is costly, inefficient, and subjective. Meanwhile, existing continuous prompt optimization methods improve the performance by learning the ideal prompts through the gradient information of PLMs, whose high computational cost, and low readability and generalizability are often concerning. To address the research gap, we propose a Dialogue-comprised Policy-gradient-based Discrete Prompt Optimization (DP_2O) method. We first design a multi-round dialogue alignment strategy for readability prompt set generation based on GPT-4. Furthermore, we propose an efficient prompt screening metric to identify high-quality prompts with linear complexity. Finally, we construct a reinforcement learning (RL) framework based on policy gradients to match the prompts to inputs optimally. By training a policy network with only 0.62M parameters on the tasks in the few-shot setting, DP_2O outperforms the state-of-the-art (SOTA) method by 1.52% in accuracy on average on four open-source datasets. Moreover, subsequent experiments also demonstrate that DP_2O has good universality, robustness and generalization ability.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a Dialogue-comprised Policy-gradient-based Discrete Prompt Optimization (DP_2O) method, which outperforms the state-of-the-art (SOTA) method by 1.52% in accuracy on average on four open-source datasets and has good universality, robustness and generalization ability."
            },
            "score": 3
        },
        {
            "id": "dd647f97921a1ff49078d53dc5629c1bd1702909",
            "paperId": "dd647f97921a1ff49078d53dc5629c1bd1702909",
            "title": "Prompt-tuning in Controlled Dialogue Generation",
            "abstract": "Recent years have witnessed a prosperous development of dialogue response generation since the advent of Transformer [66]. Fine-tuning pretrained language models for different downstream tasks has become the dominant paradigm in Natural Language Processing (NLP). However, fine-tuning requires storing a full copy of parameter states for every task, which is memory-consuming and expensive to serve when working with large-scale models with billions of parameters like GPT-3 [8]. Meanwhile, prompt-tuning has become an increasingly popular parameter-efficient method for steering large pretrained language models to various tasks. Most of the prompting techniques are applied in language understanding and assuming fixed prompts for all data samples within a task. Therefore, there arises an urgent need to exploit the ability of prompt-tuning in open-domain dialogue generation where data samples may vary greatly within a task. In this thesis, we present a novel, instance-specific prompt-tuning algorithm for dialogue generation. Specifically, we generate prompts based on instance-level control code, rather than the conversation context, to explore their impact on controlled dialogue generation. Experiments on popular open-domain dialogue datasets, evaluated with both automated metrics and human evaluation, demonstrate that our method is superior to prompting baselines as well as other lightweight controlled generation methods, and comparable to fine-tuning with less than 10% of total parameters.",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This thesis presents a novel, instance-specific prompt-tuning algorithm that generates prompts based on instance-level control code, rather than the conversation context, to explore their impact on controlled dialogue generation."
            },
            "score": 3
        },
        {
            "id": "59ef1b67c5f238d5d6d175d84fb6b239b4221a97",
            "paperId": "59ef1b67c5f238d5d6d175d84fb6b239b4221a97",
            "title": "Stabilized In-Context Learning with Pre-trained Language Models for Few Shot Dialogue State Tracking",
            "abstract": "Prompt-based methods with large pre-trained language models (PLMs) have shown impressive unaided performance across many NLP tasks. These models improve even further with the addition of a few labeled in-context exemplars to guide output generation. However, for more complex tasks such as dialogue state tracking (DST), designing prompts that reliably convey the desired intent is nontrivial, leading to unstable results. Furthermore, building in-context exemplars for dialogue tasks is difficult because conversational contexts are long while model input lengths are relatively short.To overcome these issues we first adapt a meta-learning scheme to the dialogue domain which stabilizes the ability of the model to perform well under various prompts. We additionally design a novel training method to improve upon vanilla retrieval mechanisms to find ideal in-context examples. Finally, we introduce a saliency model to limit dialogue text length, allowing us to include more exemplars per query. In effect, we are able to achieve highly competitive results for few-shot DST on MultiWOZ.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A meta-learning scheme is adapted to the dialogue domain which stabilizes the ability of the model to perform well under various prompts and introduces a saliency model to limit dialogue text length, allowing for highly competitive results for few-shot DST on MultiWOZ."
            },
            "score": 3
        },
        {
            "id": "fb81264ac2fde6fecbdaaa2b96b9fa9e35bb2e08",
            "paperId": "fb81264ac2fde6fecbdaaa2b96b9fa9e35bb2e08",
            "title": "In-context Learning of Large Language Models for Controlled Dialogue Summarization: A Holistic Benchmark and Empirical Analysis",
            "abstract": "Large Language Models (LLMs) have shown significant performance in numerous NLP tasks, including summarization and controlled text generation. A notable capability of LLMs is in-context learning (ICL), where the model learns new tasks using input-output pairs in the prompt without any parameter update. However, the performance of LLMs in the context of few-shot abstractive dialogue summarization remains underexplored. This study evaluates various state-of-the-art LLMs on the SAMSum dataset within a few-shot framework. We assess these models in both controlled (entity control, length control, and person-focused planning) and uncontrolled settings, establishing a comprehensive benchmark in few-shot dialogue summarization. Our findings provide insights into summary quality and model controllability, offering a crucial reference for future research in dialogue summarization.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study evaluates various state-of-the-art LLMs on the SAMSum dataset within a few-shot framework and provides insights into summary quality and model controllability, offering a crucial reference for future research in dialogue summarization."
            },
            "score": 3
        },
        {
            "id": "3b27092740a489a63589cdcf40fad6a0e093daa0",
            "paperId": "3b27092740a489a63589cdcf40fad6a0e093daa0",
            "title": "Prompting Multilingual Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages",
            "abstract": "The differences in decision making between behavioural models of voice interfaces are hard to capture using existing measures for the absolute performance of such models. For instance, two models may have a similar task success rate, but very different ways of getting there. In this paper, we propose a general methodology to compute the similarity of two dialogue behaviour models and investigate different ways of computing scores on both the semantic and the textual level. Complementing absolute measures of performance, we test our scores on three different tasks and show the practical usability of the measures.",
            "year": 2023,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a general methodology to compute the similarity of two dialogue behaviour models and investigates different ways of computing scores on both the semantic and the textual level."
            },
            "score": 2
        },
        {
            "id": "dedfe929d182cc3537a9ed765d589b4735ce062a",
            "paperId": "dedfe929d182cc3537a9ed765d589b4735ce062a",
            "title": "On the Planning Abilities of Large Language Models - A Critical Investigation",
            "abstract": "Intrigued by the claims of emergent reasoning capabilities in LLMs trained on general web corpora, in this paper, we set out to investigate their planning capabilities. We aim to evaluate (1) the effectiveness of LLMs in generating plans autonomously in commonsense planning tasks and (2) the potential of LLMs as a source of heuristic guidance for other agents (AI planners) in their planning tasks. We conduct a systematic study by generating a suite of instances on domains similar to the ones employed in the International Planning Competition and evaluate LLMs in two distinct modes: autonomous and heuristic. Our findings reveal that LLMs' ability to generate executable plans autonomously is rather limited, with the best model (GPT-4) having an average success rate of ~12% across the domains. However, the results in the heuristic mode show more promise. In the heuristic mode, we demonstrate that LLM-generated plans can improve the search process for underlying sound planners and additionally show that external verifiers can help provide feedback on the generated plans and back-prompt the LLM for better plan generation.",
            "year": 2023,
            "citationCount": 51,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that LLM-generated plans can improve the search process for underlying sound planners and additionally show that external verifiers can help provide feedback on the generated plans and back-prompt the LLM for better plan generation."
            },
            "score": 2
        },
        {
            "id": "44d16a076c00ecada3d425203377e4ec951c4ed0",
            "paperId": "44d16a076c00ecada3d425203377e4ec951c4ed0",
            "title": "MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning",
            "abstract": "Large language models (LLMs), despite their remarkable progress across various general domains, encounter significant barriers in medicine and healthcare. This field faces unique challenges such as domain-specific terminologies and reasoning over specialized knowledge. To address these issues, we propose a novel Multi-disciplinary Collaboration (MC) framework for the medical domain that leverages LLM-based agents in a role-playing setting that participate in a collaborative multi-round discussion, thereby enhancing LLM proficiency and reasoning capabilities. This training-free framework encompasses five critical steps: gathering domain experts, proposing individual analyses, summarising these analyses into a report, iterating over discussions until a consensus is reached, and ultimately making a decision. Our work focuses on the zero-shot setting, which is applicable in real-world scenarios. Experimental results on nine datasets (MedQA, MedMCQA, PubMedQA, and six subtasks from MMLU) establish that our proposed MC framework excels at mining and harnessing the medical expertise within LLMs, as well as extending its reasoning abilities. Our code can be found at \\url{https://github.com/gersteinlab/MedAgents}.",
            "year": 2023,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel Multi-disciplinary Collaboration (MC) framework for the medical domain that leverages LLM-based agents in a role-playing setting that participate in a collaborative multi-round discussion, thereby enhancing LLM proficiency and reasoning capabilities."
            },
            "score": 2
        },
        {
            "id": "85996f9fc312777f487dd51bf9e96bb3704c2fb7",
            "paperId": "85996f9fc312777f487dd51bf9e96bb3704c2fb7",
            "title": "On the Planning Abilities of Large Language Models (A Critical Investigation with a Proposed Benchmark)",
            "abstract": "Intrigued by the claims of emergent reasoning capabilities in LLMs trained on general web corpora, in this paper, we set out to investigate their planning capabilities. We aim to evaluate (1) how good LLMs are by themselves in generating and validating simple plans in commonsense planning tasks (of the type that humans are generally quite good at) and (2) how good LLMs are in being a source of heuristic guidance for other agents--either AI planners or human planners--in their planning tasks. To investigate these questions in a systematic rather than anecdotal manner, we start by developing a benchmark suite based on the kinds of domains employed in the International Planning Competition. On this benchmark, we evaluate LLMs in three modes: autonomous, heuristic and human-in-the-loop. Our results show that LLM's ability to autonomously generate executable plans is quite meager, averaging only about 3% success rate. The heuristic and human-in-the-loop modes show slightly more promise. In addition to these results, we also make our benchmark and evaluation tools available to support investigations by research community.",
            "year": 2023,
            "citationCount": 40,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results show that LLM's ability to autonomously generate executable plans is quite meager, averaging only about 3% success rate, and the heuristic and human-in-the-loop modes show slightly more promise."
            },
            "score": 2
        },
        {
            "id": "1c89d8672a3742672850fa46f1e8ec51f3261019",
            "paperId": "1c89d8672a3742672850fa46f1e8ec51f3261019",
            "title": "Neuro Symbolic Reasoning for Planning: Counterexample Guided Inductive Synthesis using Large Language Models and Satisfiability Solving",
            "abstract": "Generative large language models (LLMs) with instruct training such as GPT-4 can follow human-provided instruction prompts and generate human-like responses to these prompts. Apart from natural language responses, they have also been found to be effective at generating formal artifacts such as code, plans, and logical specifications from natural language prompts. Despite their remarkably improved accuracy, these models are still known to produce factually incorrect or contextually inappropriate results despite their syntactic coherence - a phenomenon often referred to as hallucination. This limitation makes it difficult to use these models to synthesize formal artifacts that are used in safety-critical applications. Unlike tasks such as text summarization and question-answering, bugs in code, plan, and other formal artifacts produced by LLMs can be catastrophic. We posit that we can use the satisfiability modulo theory (SMT) solvers as deductive reasoning engines to analyze the generated solutions from the LLMs, produce counterexamples when the solutions are incorrect, and provide that feedback to the LLMs exploiting the dialog capability of instruct-trained LLMs. This interaction between inductive LLMs and deductive SMT solvers can iteratively steer the LLM to generate the correct response. In our experiments, we use planning over the domain of blocks as our synthesis task for evaluating our approach. We use GPT-4, GPT3.5 Turbo, Davinci, Curie, Babbage, and Ada as the LLMs and Z3 as the SMT solver. Our method allows the user to communicate the planning problem in natural language; even the formulation of queries to SMT solvers is automatically generated from natural language. Thus, the proposed technique can enable non-expert users to describe their problems in natural language, and the combination of LLMs and SMT solvers can produce provably correct solutions.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The proposed technique can enable non-expert users to describe their problems in natural language, and the combination of LLMs and SMT solvers can produce provably correct solutions."
            },
            "score": 2
        },
        {
            "id": "2361bae8f0ff3627a91408c172e6612b4d554cf2",
            "paperId": "2361bae8f0ff3627a91408c172e6612b4d554cf2",
            "title": "HI-TOM: A Benchmark for Evaluating Higher-Order Theory of Mind Reasoning in Large Language Models",
            "abstract": "Theory of Mind (ToM) is the ability to reason about one's own and others' mental states. ToM plays a critical role in the development of intelligence, language understanding, and cognitive processes. While previous work has primarily focused on first and second-order ToM, we explore higher-order ToM, which involves recursive reasoning on others' beliefs. We introduce HI-TOM, a Higher Order Theory of Mind benchmark. Our experimental evaluation using various Large Language Models (LLMs) indicates a decline in performance on higher-order ToM tasks, demonstrating the limitations of current LLMs. We conduct a thorough analysis of different failure cases of LLMs, and share our thoughts on the implications of our findings on the future of NLP.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores higher-order ToM, which involves recursive reasoning on others' beliefs, and introduces HI-TOM, a Higher Order Theory of Mind benchmark."
            },
            "score": 2
        },
        {
            "id": "51528fb02101102f94b398dfdc49b7851f66d3af",
            "paperId": "51528fb02101102f94b398dfdc49b7851f66d3af",
            "title": "Legal Expertise Meets Artificial Intelligence: A Critical Analysis of Large Language Models as Intelligent Assistance Technology",
            "abstract": "This talk investigates an intelligent assistance (IA) approach to utilizing Large Language Models (LLMs) in the legal domain by addressing the risks associated with unchecked artificial intelligence (AI) applications. We emphasize the importance of understanding the distinctions between AI and IA, with the latter involving human-in-the-loop decision-making processes, which can help mitigate risks and ensure responsible use of this rapidly developing technology. Using ChatGPT and GPT-4 as a prime example, we demonstrate its dual role as both an AI and IA application, showcasing its versatility in a variety of legal tasks. We look at recently reported explorations in particular in using very LLMs in addressing tasks such as multiple-choice question answering, legal reasoning, case outcome prediction, and summarization. We argue that to fully achieve \"augmented intelligence,\" a reasoning and knowledge base component is required, allowing IA systems to effectively support human users in decision-making processes.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This talk investigates an intelligent assistance approach to utilizing Large Language Models (LLMs) in the legal domain by addressing the risks associated with unchecked artificial intelligence (AI) applications, and argues that to fully achieve \"augmented intelligence,\" a reasoning and knowledge base component is required."
            },
            "score": 2
        },
        {
            "id": "59138eab70408dffaa92018934735d8636df95c4",
            "paperId": "59138eab70408dffaa92018934735d8636df95c4",
            "title": "Emotional Manipulation Through Prompt Engineering Amplifies Disinformation Generation in AI Large Language Models",
            "abstract": "This study investigates the generation of synthetic disinformation by OpenAI's Large Language Models (LLMs) through prompt engineering and explores their responsiveness to emotional prompting. Leveraging various LLM iterations using davinci-002, davinci-003, gpt-3.5-turbo and gpt-4, we designed experiments to assess their success in producing disinformation. Our findings, based on a corpus of 19,800 synthetic disinformation social media posts, reveal that all LLMs by OpenAI can successfully produce disinformation, and that they effectively respond to emotional prompting, indicating their nuanced understanding of emotional cues in text generation. When prompted politely, all examined LLMs consistently generate disinformation at a high frequency. Conversely, when prompted impolitely, the frequency of disinformation production diminishes, as the models often refuse to generate disinformation and instead caution users that the tool is not intended for such purposes. This research contributes to the ongoing discourse surrounding responsible development and application of AI technologies, particularly in mitigating the spread of disinformation and promoting transparency in AI-generated content.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is revealed that all LLMs by OpenAI can successfully produce disinformation, and that they effectively respond to emotional prompting, indicating their nuanced understanding of emotional cues in text generation."
            },
            "score": 2
        },
        {
            "id": "529b911da40fa9df598cfbb589e84af5b7c18f0d",
            "paperId": "529b911da40fa9df598cfbb589e84af5b7c18f0d",
            "title": "Evaluation of Transfer Learning and Adaptability in Large Language Models with the GLUE Benchmark",
            "abstract": "\u2014This article presents a comprehensive investigation into the adaptability of state-of-the-art language models (LMs) to diverse domains through transfer learning techniques, evaluated using the General Language Understanding Evaluation (GLUE) benchmark. Our study systematically examines the effectiveness of various transfer learning strategies, including fine-tuning and data augmentation, in enhancing the performance of selected LMs across the spectrum of GLUE tasks. Findings reveal significant improvements in domain adaptability, though the degree of effectiveness varies across models, highlighting the influence of model architecture and pre-training depth. The analysis provides insights into the complexities of transfer learning, suggesting a nuanced understanding of its application for optimal model performance. The study contributes to the discourse on the potential and limitations of current LMs in generalizing learned knowledge to new domains, underscoring the need for more sophisticated transfer learning frameworks, diverse and comprehensive evaluation benchmarks, and future research directions aimed at improving model adaptability and inclusivity in natural language processing.",
            "year": null,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study systematically examines the effectiveness of various transfer learning strategies, including fine-tuning and data augmentation, in enhancing the performance of selected LMs across the spectrum of GLUE tasks, revealing significant improvements in domain adaptability."
            },
            "score": 2
        },
        {
            "id": "366b105020f3c623241597574509858bffc02cc8",
            "paperId": "366b105020f3c623241597574509858bffc02cc8",
            "title": "Hate Speech Detection in Social Networks using Machine Learning and Deep Learning Methods",
            "abstract": "\u2014Hate speech on social media platforms like Twitter is a growing concern that poses challenges to maintaining a healthy online environment and fostering constructive communication. Effective detection and monitoring of hate speech are crucial for mitigating its adverse impact on individuals and communities. In this paper, we propose a comprehensive approach for hate speech detection on Twitter using both traditional machine learning and deep learning techniques. Our research encompasses a thorough comparison of these techniques to determine their effectiveness in identifying hate speech on Twitter. We construct a robust dataset, gathered from diverse sources and annotated by experts, to ensure the reliability of our models. The dataset consists of tweets labeled as hate speech, offensive language, or neutral, providing a more nuanced representation of online discourse. We evaluate the performance of LSTM, BiLSTM, and CNN models against traditional shallow learning methods to establish a baseline for comparison. Our findings reveal that deep learning techniques outperform shallow learning methods, with BiLSTM emerging as the most accurate model for hate speech detection. The BiLSTM model demonstrates improved sensitivity to context, semantic nuances, and sequential patterns in tweets, making it adept at capturing the intricate nature of hate speech. Furthermore, we explore the integration of word embeddings, such as Word2Vec and GloVe, to enhance the performance of our models. The incorporation of these embeddings significantly improves the models' ability to discern between hate speech and other forms of online communication. This paper presents a comprehensive analysis of various machine learning methods for hate speech detection on Twitter, ultimately demonstrating the superiority of deep learning techniques, particularly BiLSTM, in addressing this critical issue. Our findings pave the way for further research into advanced methods of tackling hate speech and facilitating healthier online interactions.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A comprehensive analysis of various machine learning methods for hate speech detection on Twitter demonstrates the superiority of deep learning techniques, particularly BiLSTM, in addressing this critical issue."
            },
            "score": 2
        },
        {
            "id": "75f37c5bd7c9c6389e7544807fbc45fd478d4242",
            "paperId": "75f37c5bd7c9c6389e7544807fbc45fd478d4242",
            "title": "Generative Artificial Intelligence: Fundamentals",
            "abstract": "\n\nGenerative language models have witnessed substantial traction, notably with the introduction of refined models aimed at more coherent user-AI interactions\u2014principally conversational models. The epitome of this public attention has arguably been the refinement of the GPT-3 model into ChatGPT and its subsequent integration with auxiliary capabilities such as search features in Microsoft Bing. Despite voluminous prior research devoted to its developmental trajectory, the model\u2019s performance, and applicability to a myriad of quotidian tasks remained nebulous and task specific. In terms of technological implementation, the advent of models such as LLMv2 and ChatGPT-4 has elevated the discourse beyond mere textual coherence to nuanced contextual understanding and real-world task completion. Concurrently, emerging architectures that focus on interpreting latent spaces have offered more granular control over text generation, thereby amplifying the model\u2019s applicability across various verticals. Within the purview of cyber defense, especially in the Swiss operational ecosystem, these models pose both unprecedented opportunities and challenges. Their capabilities in data analytics, intrusion detection, and even misinformation combatting is laudable; yet the ethical and security implications concerning data privacy, surveillance, and potential misuse warrant judicious scrutiny.\n",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Generative language models have witnessed substantial traction, notably with the introduction of refined models aimed at more coherent user-AI interactions\u2014principally conversational models; yet their capabilities in data analytics, intrusion detection, and misinformation combatting is laudable; yet the ethical and security implications concerning data privacy, surveillance, and potential misuse warrant judicious scrutiny."
            },
            "score": 2
        },
        {
            "id": "62c7340ca362ae7e5cfe8258b42641ca292b7670",
            "paperId": "62c7340ca362ae7e5cfe8258b42641ca292b7670",
            "title": "\u201cAre you kidding me?\u201d: Detecting Unpalatable Questions on Reddit",
            "abstract": "Abusive language in online discourse negatively affects a large number of social media users. Many computational methods have been proposed to address this issue of online abuse. The existing work, however, tends to focus on detecting the more explicit forms of abuse leaving the subtler forms of abuse largely untouched. Our work addresses this gap by making three core contributions. First, inspired by the theory of impoliteness, we propose a novel task of detecting a subtler form of abuse, namely unpalatable questions. Second, we publish a context-aware dataset for the task using data from a diverse set of Reddit communities. Third, we implement a wide array of learning models and also investigate the benefits of incorporating conversational context into computational models. Our results show that modeling subtle abuse is feasible but difficult due to the language involved being highly nuanced and context-sensitive. We hope that future research in the field will address such subtle forms of abuse since their harm currently passes unnoticed through existing detection systems.",
            "year": 2021,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results show that modeling subtle abuse is feasible but difficult due to the language involved being highly nuanced and context-sensitive and it is hoped that future research in the field will address such subtle forms of abuse since their harm currently passes unnoticed through existing detection systems."
            },
            "score": 2
        },
        {
            "id": "c02e89a67a9d26fffa5b8269e638a8348f75f9c0",
            "paperId": "c02e89a67a9d26fffa5b8269e638a8348f75f9c0",
            "title": "Simple dynamic word embeddings for mapping perceptions in the public sphere",
            "abstract": "Word embeddings trained on large-scale historical corpora can illuminate human biases and stereotypes that perpetuate social inequalities. These embeddings are often trained in separate vector space models defined according to different attributes of interest. In this paper, we introduce a single, unified dynamic embedding model that learns attribute-specific word embeddings and apply it to a novel dataset\u2014talk radio shows from around the US\u2014to analyze perceptions about refugees. We validate our model on a benchmark dataset and apply it to two corpora of talk radio shows averaging 117 million words produced over one month across 83 stations and 64 cities. Our findings suggest that dynamic word embeddings are capable of identifying nuanced differences in public discourse about contentious topics, suggesting their usefulness as a tool for better understanding how the public perceives and engages with different issues across time, geography, and other dimensions.",
            "year": 2019,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A single, unified dynamic embedding model is introduced that learns attribute-specific word embeddings and is applied to a novel dataset\u2014talk radio shows from around the US\u2014to analyze perceptions about refugees."
            },
            "score": 2
        },
        {
            "id": "68850153b0210615c86f9a72624f34e2913bcddf",
            "paperId": "68850153b0210615c86f9a72624f34e2913bcddf",
            "title": "Document-Level Machine Translation with Large Language Models",
            "abstract": "Large language models (LLMs) such as ChatGPT can produce coherent, cohesive, relevant, and fluent answers for various natural language processing (NLP) tasks. Taking document-level machine translation (MT) as a testbed, this paper provides an in-depth evaluation of LLMs' ability on discourse modeling. The study focuses on three aspects: 1) Effects of Context-Aware Prompts, where we investigate the impact of different prompts on document-level translation quality and discourse phenomena; 2) Comparison of Translation Models, where we compare the translation performance of ChatGPT with commercial MT systems and advanced document-level MT methods; 3) Analysis of Discourse Modelling Abilities, where we further probe discourse knowledge encoded in LLMs and shed light on impacts of training techniques on discourse modeling. By evaluating on a number of benchmarks, we surprisingly find that LLMs have demonstrated superior performance and show potential to become a new paradigm for document-level translation: 1) leveraging their powerful long-text modeling capabilities, GPT-3.5 and GPT-4 outperform commercial MT systems in terms of human evaluation; 2) GPT-4 demonstrates a stronger ability for probing linguistic knowledge than GPT-3.5. This work highlights the challenges and opportunities of LLMs for MT, which we hope can inspire the future design and evaluation of LLMs.We release our data and annotations at https://github.com/longyuewangdcu/Document-MT-LLM.",
            "year": 2023,
            "citationCount": 53,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An in-depth evaluation of LLMs' ability on discourse modeling using document-level machine translation (MT) as a testbed finds that LLMs have demonstrated superior performance and show potential to become a new paradigm for document- level translation."
            },
            "score": 2
        },
        {
            "id": "1c7402843d8b586d945b3b030e3edd93f0ae3959",
            "paperId": "1c7402843d8b586d945b3b030e3edd93f0ae3959",
            "title": "On the Creativity of Large Language Models",
            "abstract": "Large Language Models (LLMs) are revolutionizing several areas of Artificial Intelligence. One of the most remarkable applications is creative writing, e.g., poetry or storytelling: the generated outputs are often of astonishing quality. However, a natural question arises: can LLMs be really considered creative? In this article we firstly analyze the development of LLMs under the lens of creativity theories, investigating the key open questions and challenges. In particular, we focus our discussion around the dimensions of value, novelty and surprise as proposed by Margaret Boden in her work. Then, we consider different classic perspectives, namely product, process, press and person. We discuss a set of ``easy'' and ``hard'' problems in machine creativity, presenting them in relation to LLMs. Finally, we examine the societal impact of these technologies with a particular focus on the creative industries, analyzing the opportunities offered by them, the challenges arising by them and the potential associated risks, from both legal and ethical points of view.",
            "year": 2023,
            "citationCount": 19,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The development of LLMs under the lens of creativity theories is analyzed, investigating the key open questions and challenges, and a set of ``easy'' and ``hard'' problems in machine creativity, presenting them in relation to LLMs."
            },
            "score": 2
        },
        {
            "id": "1212b1e44f7611d2017b246fd3d8e9c973c9d937",
            "paperId": "1212b1e44f7611d2017b246fd3d8e9c973c9d937",
            "title": "Can Large Language Models Empower Molecular Property Prediction?",
            "abstract": "Molecular property prediction has gained significant attention due to its transformative potential in multiple scientific disciplines. Conventionally, a molecule graph can be represented either as a graph-structured data or a SMILES text. Recently, the rapid development of Large Language Models (LLMs) has revolutionized the field of NLP. Although it is natural to utilize LLMs to assist in understanding molecules represented by SMILES, the exploration of how LLMs will impact molecular property prediction is still in its early stage. In this work, we advance towards this objective through two perspectives: zero/few-shot molecular classification, and using the new explanations generated by LLMs as representations of molecules. To be specific, we first prompt LLMs to do in-context molecular classification and evaluate their performance. After that, we employ LLMs to generate semantically enriched explanations for the original SMILES and then leverage that to fine-tune a small-scale LM model for multiple downstream tasks. The experimental results highlight the superiority of text explanations as molecular representations across multiple benchmark datasets, and confirm the immense potential of LLMs in molecular property prediction tasks. Codes are available at \\url{https://github.com/ChnQ/LLM4Mol}.",
            "year": 2023,
            "citationCount": 19,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work employs LLMs to generate semantically enriched explanations for the original SMILES and then uses that to fine-tune a small-scale LM model for multiple downstream tasks and highlights the superiority of text explanations as molecular representations across multiple benchmark datasets."
            },
            "score": 2
        },
        {
            "id": "cce17289765132b6192ccf90123bb7f5ef920c8e",
            "paperId": "cce17289765132b6192ccf90123bb7f5ef920c8e",
            "title": "Large Language Models can accomplish Business Process Management Tasks",
            "abstract": "Business Process Management (BPM) aims to improve organizational activities and their outcomes by managing the underlying processes. To achieve this, it is often necessary to consider information from various sources, including unstructured textual documents. Therefore, researchers have developed several BPM-specific solutions that extract information from textual documents using Natural Language Processing techniques. These solutions are specific to their respective tasks and cannot accomplish multiple process-related problems as a general-purpose instrument. However, in light of the recent emergence of Large Language Models (LLMs) with remarkable reasoning capabilities, such a general-purpose instrument with multiple applications now appears attainable. In this paper, we illustrate how LLMs can accomplish text-related BPM tasks by applying a specific LLM to three exemplary tasks: mining imperative process models from textual descriptions, mining declarative process models from textual descriptions, and assessing the suitability of process tasks from textual descriptions for robotic process automation. We show that, without extensive configuration or prompt engineering, LLMs perform comparably to or better than existing solutions and discuss implications for future BPM research as well as practical usage.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper illustrates how Large Language Models can accomplish text-related BPM tasks by applying a specific LLM to three exemplary tasks: mining imperative process models from textual descriptions, mining declarative process Models from textual describe, and assessing the suitability of process tasks from textual description for robotic process automation."
            },
            "score": 2
        },
        {
            "id": "66718e87b70de80cbc2a4120050ca36fda49f8d6",
            "paperId": "66718e87b70de80cbc2a4120050ca36fda49f8d6",
            "title": "Exploring Distributional Shifts in Large Language Models for Code Analysis",
            "abstract": "We systematically study how three large language models with code capabilities - CodeT5, Codex, and ChatGPT - generalize to out-of-domain data. We consider two fundamental applications - code summarization, and code generation. We split data into domains following its natural boundaries - by an organization, by a project, and by a module within the software project. We establish that samples from each new domain present all the models with a significant challenge of distribution shift. We study how established methods adapt models to better generalize to new domains. Our experiments show that while multitask learning alone is a reasonable baseline, combining it with few-shot finetuning on examples retrieved from training data can achieve very strong performance. Moreover, this solution can outperform direct finetuning for very low-data scenarios. Finally, we consider variations of this approach to create a more broadly applicable method to adapt to multiple domains at once. We find that for code generation, a model adapted to multiple domains simultaneously performs on par with those adapted to a single domain",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work systematically study how three large language models with code capabilities - CodeT5, Codex, and ChatGPT - generalize to out-of-domain data, finding that for code generation, a model adapted to multiple domains simultaneously performs on par with those adapted to a single domain."
            },
            "score": 2
        },
        {
            "id": "4d4890ee706c094ec21d194b05551793235de5af",
            "paperId": "4d4890ee706c094ec21d194b05551793235de5af",
            "title": "Challenges in Reproducing Human Evaluation Results for Role-Oriented Dialogue Summarization",
            "abstract": "There is a growing concern regarding the reproducibility of human evaluation studies in NLP. As part of the ReproHum campaign, we conducted a study to assess the reproducibility of a recent human evaluation study in NLP. Specifically, we attempted to reproduce a human evaluation of a novel approach to enhance Role-Oriented Dialogue Summarization by considering the influence of role interactions. Despite our best efforts to adhere to the reported setup, we were unable to reproduce the statistical results as presented in the original paper. While no contradictory evidence was found, our study raises questions about the validity of the reported statistical significance results, and/or the comprehensiveness with which the original study was reported. In this paper, we provide a comprehensive account of our reproduction study, detailing the methodologies employed, data collection, and analysis procedures. We discuss the implications of our findings for the broader issue of reproducibility in NLP research. Our findings serve as a cautionary reminder of the challenges in conducting reproducible human evaluations and prompt further discussions within the NLP community.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study attempted to reproduce a human evaluation of a novel approach to enhance Role-Oriented Dialogue Summarization by considering the influence of role interactions, and was unable to reproduce the statistical results as presented in the original paper."
            },
            "score": 2
        },
        {
            "id": "cf9b125120b199b68ebaa2af1f14b86cef32e098",
            "paperId": "cf9b125120b199b68ebaa2af1f14b86cef32e098",
            "title": "Chronotopes, Scales, and Complexity in the Study of Language in Society",
            "abstract": "Recent developments in the study of language in society have moved the field increasingly away from linear models toward complex models. The complexity of timespace as an aspect of what is called context is of key importance in this development, and this article engages with two possibly useful concepts in view of this: chronotope and scale. Chronotope can be seen as invokable chunks of history that organize the indexical order of discourse; scale, in turn, can be seen as the scope of communicability of such invocations. Thus, whenever we see chronotopes, we see them mediated by scales. The cultural stuff of chronotopes is conditioned by the sociolinguistic conditions of scale. This nuanced approach to timescale contextualization offers new directions for complexity-oriented research in our fields.",
            "year": 2015,
            "citationCount": 407,
            "tldr": null,
            "score": 1
        },
        {
            "id": "62a0102c45c913be4c34dc9626e5b1f287c11bae",
            "paperId": "62a0102c45c913be4c34dc9626e5b1f287c11bae",
            "title": "Deciphering the Dynamics of Reddit Comment Popularity",
            "abstract": "\u2014The digital discourse on Reddit, marked by diverse interactions through comments, presents a unique canvas to explore the dynamics of online popularity and engagement. Unlike prior studies focusing predominantly on post popularity, this research pivots towards understanding the factors that influence the popularity of individual comments. Central to this investigation is the application of Natural Language Processing (NLP) techniques and the analysis of controversiality\u2014a metric indicating the extent of divisive opinion a comment elicits within the community. By integrating features such as controversiality, subreddit context, and the textual content of comments (body), this study employs a suite of machine learning models to predict comment popularity on Reddit. The exploration is anchored in a dataset specifically curated for this purpose, which encompasses a rich set of comment attributes conducive to nuanced analysis. Through the lens of Linear Regression, K-Nearest Neighbors (KNN), and Random Forest Regression models, the research seeks to not only quantify the impact of controversiality and textual sentiment on comment engagement but also to contribute to the broader discourse on content virality and user interaction in social media platforms.",
            "year": null,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study employs a suite of machine learning models to predict comment popularity on Reddit and seeks to not only quantify the impact of controversiality and textual sentiment on comment engagement but also to contribute to the broader discourse on content virality and user interaction in social media platforms."
            },
            "score": 1
        },
        {
            "id": "0563e1a5978a70a06669bd1ec8884791245ff987",
            "paperId": "0563e1a5978a70a06669bd1ec8884791245ff987",
            "title": "Pragmatic Aspects of Judicial Discourse: Zooming in on a Nuanced Language Use",
            "abstract": "Over recent decades, judgments as a legal genre have received increased attention from linguists. Such a trend has contributed to a better understanding of the macro-and micro-linguistic aspects of this highly complex legal genre. However, the pragmatic aspects of judgments as a specific text type remain largely neglected in the current literature. Therefore, this paper aims to contribute to this line of research by discussing the pragmatic effects arising from the use of meta-argumentative verbs in a corpus of judgments delivered by the European Court of Human Rights (ECHR judgments, henceforth). Accordingly, the paper will show that the pragmatic effects of the recurrent meta-argumentative verbs in ECHR judgments can be analyzed through the prism of politeness and legitimization strategies. In addition, suggestions will be provided regarding the ways in which corpus data can be used for raising learners\u2019 awareness of the nuanced language that is pertinent to juridical settings.",
            "year": 2022,
            "citationCount": 1,
            "tldr": null,
            "score": 1
        },
        {
            "id": "010b04e53a538c037237d6c28f0bb1e6367e6a17",
            "paperId": "010b04e53a538c037237d6c28f0bb1e6367e6a17",
            "title": "Evaluating Large-Language Models for Dimensional Music Emotion Prediction from Social Media Discourse",
            "abstract": "The automatic prediction of emotional responses to music is a task of inherent interest to the \ufb01eld of music information retrieval. These efforts are often hindered by the absence of large datasets available for this task. In this work, we investigate the use of sentiment analysis on online social media conversations as an alternate data source to train computational models to predict the emotive responses to a piece of music. Using two datasets annotated with valence and arousal values, we create a corpus of social media commentary for these songs extracted from YouTube, Twitter, and Reddit. We evaluate our approach with transformer models to predict the affective values of the 2402 songs in our dataset. We achieve a moderate Pearson\u2019s correlation of 0.62 and 0.72 for valence and arousal, respectively, for discourse from YouTube. These promising results demonstrate that discourse about music may carry semantic information useful to making determinations about the music itself. Such an approach could potentially supplement music information retrieval systems to estimate emotion for pieces of music for which the audio is restricted by copyright or otherwise unavailable.",
            "year": 2022,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Investigation of the use of sentiment analysis on online social media conversations as an alternate data source to train computational models to predict the emotive responses to a piece of music demonstrates that discourse about music may carry semantic information useful to making determinations about the music itself."
            },
            "score": 1
        },
        {
            "id": "d03b310eae124761858047e673e5805c743aee65",
            "paperId": "d03b310eae124761858047e673e5805c743aee65",
            "title": "Model-Driven Engineering of Simulations for Smart Roads",
            "abstract": "Smart Roads (SRs) are systems that provide trafficrelated services, based on a combination of sensor and actuator networks deployed in roads, vehicles, and surrounding elements. They are complex distributed systems that involve multiple heterogeneous components and technologies. This makes their development a challenging and costly process. Simulations are a key tool to deal with these issues, as they allow developing and testing in fully controlled environments with simplified software components. Nevertheless, they still need to consider multiple perspectives (e.g. experts and designers), which frequently cause problems to understand and validate them. Model-Driven Engineering of simulations appears as a solution. It uses models to represent explicitly these perspectives, and transformations to link them and generate new artifacts (including code). This paper presents a framework to develop simulations of SRs following this approach. Its base is an existing modeling language related to road traffic which is adapted to specify the aspects of these systems (i.e. sensors, networks, and services), and their context (i.e. users, vehicles, and their environment). A process guides its use in the transition from abstract models to code supported by tailored tools. A case study on a system to track vehicles using sensors in roads illustrates its use.",
            "year": 2016,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents a framework to develop simulations of SRs following Model-Driven Engineering of simulations, an existing modeling language related to road traffic which is adapted to specify the aspects of these systems."
            },
            "score": 1
        }
    ],
    "novelty": "yes"
}