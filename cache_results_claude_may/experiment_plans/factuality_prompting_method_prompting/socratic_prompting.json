{
    "topic_description": "novel prompting methods that can improve factuality and reduce hallucination of large language models",
    "idea_name": "Socratic Prompting",
    "raw_idea": {
        "Problem": "Large language models often generate shallow and superficial responses that lack deep understanding and reasoning, leading to factual inconsistencies and logical fallacies.",
        "Existing Methods": "Existing methods for improving the reasoning abilities of language models mainly focus on using chain-of-thought prompting and incorporating external knowledge.",
        "Motivation": "We draw inspiration from the Socratic method, a form of cooperative argumentative dialogue based on asking and answering questions to stimulate critical thinking and draw out ideas and underlying presuppositions. We hypothesize that by incorporating Socratic questioning into the prompting process, we can guide the model to generate more thoughtful and well-reasoned responses.",
        "Proposed Method": "We propose Socratic Prompting (SP), a novel prompting method that incorporates Socratic questioning into the generation process. SP consists of three main steps: 1) Socratic Question Generation: Given an input prompt, we first generate a set of Socratic questions that probe the model's understanding and reasoning process, such as \"What are the assumptions behind this statement?\" and \"Can you provide evidence to support your claim?\" 2) Socratic Dialogue Prompting: We then engage the model in a Socratic dialogue by asking it to answer the generated questions and provide justifications for its responses. We alternate between question generation and answering until the model reaches a satisfactory level of understanding and reasoning. 3) Final Response Generation: Based on the Socratic dialogue, we ask the model to generate a final response that incorporates the insights and reasoning from the dialogue.",
        "Experiment Plan": "We plan to evaluate SP on a range of tasks that require deep understanding and reasoning, such as argument generation, debating, and persuasive writing. We will compare SP with state-of-the-art baselines, including chain-of-thought prompting and methods that incorporate external knowledge. We will also conduct human evaluation to assess the depth and quality of the reasoning in the generated responses, as well as the effectiveness of the Socratic questioning in eliciting critical thinking."
    },
    "full_experiment_plan": {
        "Title": "Socratic Prompting: Guiding Language Models to Reason Deeply via Socratic Questioning",
        "Problem Statement": "Large language models often generate shallow and superficial responses that lack deep understanding and reasoning, leading to factual inconsistencies and logical fallacies.",
        "Motivation": "Existing methods for improving the reasoning abilities of language models mainly focus on using chain-of-thought prompting and incorporating external knowledge. However, these methods still struggle to guide the model to engage in deep reasoning and critical thinking. We draw inspiration from the Socratic method, a form of cooperative argumentative dialogue based on asking and answering questions to stimulate critical thinking and draw out ideas and underlying presuppositions. We hypothesize that by incorporating Socratic questioning into the prompting process, we can guide the model to generate more thoughtful and well-reasoned responses.",
        "Proposed Method": "We propose Socratic Prompting (SP), a novel prompting method that incorporates Socratic questioning into the generation process. SP consists of three main steps: 1) Socratic Question Generation: Given an input prompt, we first generate a set of Socratic questions that probe the model's understanding and reasoning process, such as \"What are the assumptions behind this statement?\" and \"Can you provide evidence to support your claim?\" 2) Socratic Dialogue Prompting: We then engage the model in a Socratic dialogue by asking it to answer the generated questions and provide justifications for its responses. We alternate between question generation and answering until the model reaches a satisfactory level of understanding and reasoning. 3) Final Response Generation: Based on the Socratic dialogue, we ask the model to generate a final response that incorporates the insights and reasoning from the dialogue.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We evaluate SP on a range of tasks that require deep understanding and reasoning, including: 1) Argument Generation: We use the Argument Reasoning Comprehension (ARC) dataset, which consists of two-sentence arguments with warrants. The goal is to generate the implicit warrants that justify the reasoning from the claim to the conclusion. 2) Debating: We use the IBM-EviDebate dataset, which contains pairs of evidence and claims. The task is to determine whether the evidence supports or opposes the claim and generate a response explaining the reasoning. 3) Persuasive Writing: We use the Persuasive Opinion Text (PersuadeMe) dataset, which consists of opinion articles on various topics. The task is to generate persuasive arguments that support or oppose the main thesis of each article.",
            "Step 2: Construct Prompts": "For each task, we construct the following prompts:\n1) Baseline Prompt: We use a simple prompt that asks the model to generate a response directly based on the input, without any additional guidance.\n2) Chain-of-Thought (CoT) Prompt: We use a CoT prompt that asks the model to generate a step-by-step reasoning process before providing the final response.\n3) Socratic Prompting (SP): We implement the three-step SP process as described in the Proposed Method section. For Socratic Question Generation, we use a set of pre-defined question templates based on common types of Socratic questions, such as \"What are the counterarguments to this claim?\" and \"How does this relate to other ideas we have discussed?\" We fill in the templates with relevant content from the input prompt. For Socratic Dialogue Prompting, we alternate between question answering and question generation for a fixed number of turns (e.g., 3-5 turns). For Final Response Generation, we provide the model with the full Socratic dialogue and ask it to generate a final response.",
            "Step 3: Select Models": "We experiment with state-of-the-art language models, including GPT-3.5 (text-davinci-002), GPT-4, and LLaMA-2-70B-chat.",
            "Step 4: Evaluate Results": "For each task and model combination, we generate responses using the baseline, CoT, and SP prompts. We evaluate the quality of the generated responses using both automatic metrics and human evaluation. For automatic evaluation, we use metrics such as BLEU, ROUGE, and BERTScore to measure the similarity between the generated responses and the reference responses in the datasets. For human evaluation, we ask human raters to assess the responses in terms of various criteria, such as relevance, coherence, persuasiveness, and depth of reasoning. We also ask the raters to directly compare the responses generated by different prompting methods and indicate their preference.",
            "Step 5: Analyze Results": "We compare the performance of SP with the baseline and CoT prompts across all tasks and models. We hypothesize that SP will outperform the baseline and CoT prompts in terms of both automatic metrics and human evaluation, especially on tasks that require deep reasoning and critical thinking. We also analyze the Socratic dialogues generated by SP to gain insights into how the questioning process helps to guide the model's reasoning and improve the quality of the final responses."
        },
        "Test Case Examples": {
            "Test Case 1": {
                "Input": "Claim: We should ban the sale of violent video games to minors. Reason: Violent video games can increase aggression in children.",
                "Baseline Prompt": "Generate a response to the above claim and reason.",
                "Baseline Output": "I agree that we should ban the sale of violent video games to minors. Exposure to violent content at a young age can have negative effects on children's behavior and mental health. It's important to protect children from potentially harmful influences.",
                "CoT Prompt": "Generate a step-by-step reasoning process to analyze the above claim and reason, and then provide a final response.",
                "CoT Output": "Step 1: The claim is that we should ban the sale of violent video games to minors. Step 2: The reason provided is that violent video games can increase aggression in children. Step 3: There is some research suggesting a link between violent video games and increased aggression, but the evidence is mixed and the effects are likely small. Step 4: Banning the sale of violent video games to minors may help protect some children, but it also raises concerns about freedom of speech and expression. Step 5: Parents and caregivers should play a primary role in monitoring and regulating children's access to violent video games. Final response: While there are concerns about the potential negative effects of violent video games on children, an outright ban may not be the most appropriate solution. Parents and caregivers should take responsibility for supervising children's gaming habits and making informed decisions about what content is appropriate. More research is needed to fully understand the impact of violent video games on aggression and other outcomes in children.",
                "SP Prompt": "Let's analyze the claim and reason using Socratic questioning. First, generate three Socratic questions to probe the assumptions and implications of the claim and reason. Then, provide a response to each question. Finally, generate a conclusive response that incorporates the insights from the Socratic dialogue.",
                "SP Output": "Socratic Questions:\n1. What evidence supports the claim that violent video games increase aggression in children? Are there any studies that suggest otherwise?\n2. How do we define \"violent\" video games? Is there a clear distinction between acceptable and unacceptable levels of violence?\n3. What are the potential unintended consequences of banning the sale of violent video games to minors? Could it lead to increased interest or underground distribution?\n\nResponses:\n1. While some studies have found a correlation between playing violent video games and increased aggressive behavior, the evidence is mixed. Other factors, such as family environment and mental health, also play a role. More research is needed to establish a causal link.\n2. The definition of \"violent\" video games is subjective and can vary widely. Games with cartoonish or unrealistic violence may have different effects than those with graphic, realistic violence. It's important to consider the context and intensity of the violence.\n3. Banning the sale of violent video games to minors could have unintended consequences. It may create a \"forbidden fruit\" effect, increasing children's interest in and desire for the banned games. It could also lead to the development of an underground market or encourage children to access the games through other means, such as online downloads or borrowing from older friends.\n\nFinal Response:\nThe claim that violent video games increase aggression in children is a complex issue that requires careful consideration. While some research suggests a link, the evidence is not conclusive and other factors likely play a role. Banning the sale of violent video games to minors may seem like a straightforward solution, but it could have unintended consequences and may not address the underlying issues. A more nuanced approach that involves parental guidance, media literacy education, and open dialogue about the content and context of video game violence may be more effective. It's important to empower parents and caregivers to make informed decisions and to provide children with the skills to critically engage with media content. Further research is needed to fully understand the impact of violent video games on children's development and behavior.",
                "Explanation": "The baseline prompt generates a simple response that agrees with the claim without much reasoning. The CoT prompt improves upon this by breaking down the reasoning process, but still lacks depth in considering alternative perspectives and implications. The SP prompt generates a Socratic dialogue that probes the assumptions and implications of the claim and reason, leading to a more nuanced and well-reasoned final response. The Socratic questioning helps to surface important considerations, such as the mixed evidence, the subjectivity of defining \"violent\" games, and the potential unintended consequences of a ban. The final response incorporates these insights to provide a balanced and thoughtful perspective on the issue."
            },
            "Test Case 2": {
                "Input": "Claim: Social media platforms should be held legally responsible for the spread of misinformation on their platforms. Reason: Social media companies have the power and resources to monitor and control the content shared on their platforms.",
                "Baseline Prompt": "Generate a response to the above claim and reason.",
                "Baseline Output": "I disagree with holding social media platforms legally responsible for the spread of misinformation. While they have the power to monitor content, it's ultimately the responsibility of users to critically evaluate the information they encounter online. Social media companies should not be expected to police all content shared by millions of users.",
                "CoT Prompt": "Generate a step-by-step reasoning process to analyze the above claim and reason, and then provide a final response.",
                "CoT Output": "Step 1: The claim states that social media platforms should be held legally responsible for the spread of misinformation on their platforms. Step 2: The reason given is that social media companies have the power and resources to monitor and control the content shared on their platforms. Step 3: Social media platforms do have the technical capability to monitor and potentially remove misinformation, but this raises concerns about censorship and free speech. Step 4: Determining what constitutes misinformation can be subjective and politically charged, making it difficult for social media companies to make consistent and fair decisions. Step 5: Holding social media companies legally responsible could lead to overly aggressive content moderation and have a chilling effect on online discourse. Final response: While social media companies have a role to play in combating the spread of misinformation, holding them legally responsible is problematic. It places an excessive burden on these companies to police content and make subjective judgments about truth and falsity. This could lead to censorship and have unintended consequences for free speech online. A better approach may be to focus on media literacy education, fact-checking resources, and collaborative efforts between social media companies, journalists, and researchers to identify and counter misinformation.",
                "SP Prompt": "Let's analyze the claim and reason using Socratic questioning. First, generate three Socratic questions to probe the assumptions and implications of the claim and reason. Then, provide a response to each question. Finally, generate a conclusive response that incorporates the insights from the Socratic dialogue.",
                "SP Output": "Socratic Questions:\n1. What are the potential benefits and drawbacks of holding social media companies legally responsible for misinformation? How do we balance concerns about misinformation with free speech and open discourse?\n2. How would legal liability for misinformation impact the way social media platforms moderate content? What are the risks of over-censorship or biased moderation?\n3. Are there alternative approaches to combating misinformation on social media that don't involve legal liability? What role can users, educators, and other stakeholders play?\n\nResponses:\n1. Holding social media companies legally responsible could incentivize them to take stronger actions against misinformation, potentially reducing its spread. However, it could also lead to over-censorship and have a chilling effect on free speech. Social media companies may become overly cautious and remove content that is controversial but not necessarily false. This could stifle open discourse and lead to a sanitized online environment that favors certain perspectives over others.\n2. Legal liability would likely lead social media platforms to adopt more aggressive content moderation practices. They may rely heavily on automated algorithms to flag and remove potential misinformation, which could result in false positives and the removal of legitimate content. Human moderators may also face pressure to make quick judgments and err on the side of removal to avoid legal risk. This could amplify existing concerns about bias and inconsistency in content moderation.\n3. There are alternative approaches to combating misinformation that don't rely on legal liability. Improving media literacy education could help users develop the skills to critically evaluate information online. Fact-checking organizations and journalists can play a role in identifying and debunking misinformation. Social media companies can invest in research and partnerships to better understand the spread of misinformation and develop targeted interventions. Collaborative efforts between stakeholders, such as the development of shared standards and best practices, could also be effective.\n\nFinal Response:\nHolding social media companies legally responsible for the spread of misinformation on their platforms is a complex issue with significant implications for free speech and content moderation. While it's important to address the problem of misinformation, legal liability may not be the most effective or desirable solution. It could lead to over-censorship, biased moderation, and a chilling effect on online discourse. Alternative approaches, such as improving media literacy education, supporting fact-checking efforts, and fostering collaboration between stakeholders, may be more promising. Social media companies have a responsibility to address misinformation, but this should be balanced with respect for free speech and open dialogue. A multi-stakeholder approach that engages users, educators, researchers, and civil society can help to create a more resilient and informed online environment. Ultimately, combating misinformation requires a collective effort and a commitment to the values of truth, transparency, and critical thinking.",
                "Explanation": "The SP prompt generates a Socratic dialogue that explores the nuances and implications of holding social media companies legally responsible for misinformation. The questions probe the potential benefits and drawbacks, the impact on content moderation, and alternative approaches. The responses highlight the risks of over-censorship, biased moderation, and the chilling effect on free speech. They also suggest alternative approaches, such as media literacy education and collaborative efforts. The final response incorporates these insights to provide a balanced perspective that acknowledges the importance of addressing misinformation while also respecting free speech and open dialogue. The Socratic questioning leads to a more nuanced and comprehensive analysis compared to the baseline and CoT prompts."
            }
        },
        "Fallback Plan": "If the proposed Socratic Prompting method does not outperform the baselines, we can conduct additional analyses to understand why. Some potential avenues for investigation include:\n\n1. Analyzing the quality and relevance of the generated Socratic questions. If the questions are not probing the key assumptions and implications of the input prompts, they may not be effective in guiding the model's reasoning. We can manually evaluate a sample of the generated questions and compare them to human-written Socratic questions to identify areas for improvement.\n\n2. Examining the coherence and depth of the Socratic dialogues. If the model's responses to the Socratic questions are shallow, inconsistent, or fail to build upon each other, the final response may not reflect genuine reasoning. We can analyze the dialogues to identify patterns of success and failure, and potentially develop additional prompting techniques to encourage more coherent and progressive dialogues.\n\n3. Comparing the performance of Socratic Prompting across different models and parameter sizes. If the method works well for some models but not others, it may indicate that certain architectures or training paradigms are more amenable to Socratic questioning. We can conduct experiments to isolate the effects of model choice and identify the characteristics of models that respond well to Socratic Prompting.\n\n4. Investigating the impact of domain knowledge on the effectiveness of Socratic Prompting. If the method performs poorly on tasks that require specialized knowledge or technical understanding, it may suggest that the model's background knowledge is insufficient to engage in meaningful Socratic questioning. We can explore ways to incorporate domain-specific information into the prompts or fine-tune the models on relevant datasets to improve their ability to reason about complex topics.\n\nIf these analyses fail to yield insights that can improve the effectiveness of Socratic Prompting, we can still leverage the generated Socratic dialogues as a valuable dataset for studying the reasoning capabilities and limitations of language models. By comparing the dialogues to human-written Socratic questioning, we can identify patterns of success and failure, common reasoning pitfalls, and opportunities for future research. This can contribute to the broader goal of developing more transparent and interpretable language models that can engage in robust reasoning and critical thinking."
    },
    "novelty_queries": [
        "KeywordQuery(\"socratic questioning language models\")",
        "KeywordQuery(\"socratic prompting language models reasoning\")",
        "KeywordQuery(\"language models critical thinking socratic dialogue\")",
        "KeywordQuery(\"Socratic Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "69335077fcacbff7a7cf25697da1949e6bdfa968",
            "paperId": "69335077fcacbff7a7cf25697da1949e6bdfa968",
            "title": "The Art of SOCRATIC QUESTIONING: Recursive Thinking with Large Language Models",
            "abstract": "Chain-of-Thought (CoT) prompting enables large language models to solve complex reasoning problems by generating intermediate steps. However, confined by its inherent single-pass and sequential generation process, CoT heavily relies on the initial decisions, causing errors in early steps to accumulate and impact the final answers. In contrast, humans adopt recursive thinking when tackling complex reasoning problems, i.e., iteratively breaking the original problem into approachable sub-problems and aggregating their answers to resolve the original one. Inspired by the human cognitive process, we propose SOCRATIC QUESTIONING, a divide-and-conquer style algorithm that mimics the recursive thinking process. Specifically, SOCRATIC QUESTIONING leverages large language models to raise and answer sub-questions until collecting enough information to tackle the original question. Unlike CoT, SOCRATIC QUESTIONING explicitly navigates the thinking space, stimulates effective recursive thinking, and is more robust towards errors in the thinking process. Extensive experiments on several complex reasoning tasks, including MMLU, MATH, LogiQA, and visual question-answering demonstrate significant performance improvements over the state-of-the-art prompting methods, such as CoT, and Tree-of-Thought. The qualitative analysis clearly shows that the intermediate reasoning steps elicited by SOCRATIC QUESTIONING are similar to humans' recursively thinking process of complex reasoning problems.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The qualitative analysis clearly shows that the intermediate reasoning steps elicited by SOCRATIC QUESTIONING are similar to humans' recursively thinking process of complex reasoning problems."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the reasoning abilities of language models by incorporating Socratic questioning into the prompting process to guide the model to engage in deep reasoning and critical thinking. The approach is called Socratic Prompting (SP), which consists of three main steps: Socratic Question Generation, Socratic Dialogue Prompting, and Final Response Generation.\n\nThe paper proposes SOCRATIC QUESTIONING, a divide-and-conquer style algorithm that mimics the recursive thinking process to enable large language models to solve complex reasoning problems. It leverages large language models to raise and answer sub-questions until collecting enough information to tackle the original question.\n\nWhile both the project proposal and the paper focus on improving the reasoning abilities of language models using Socratic questioning, the project proposal specifically proposes a prompting method called Socratic Prompting (SP), which is different from the SOCRATIC QUESTIONING algorithm proposed in the paper.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "6fd7184bc15bf694902c0e465692e5bec533f69f",
            "paperId": "6fd7184bc15bf694902c0e465692e5bec533f69f",
            "title": "Improving Socratic Question Generation using Data Augmentation and Preference Optimization",
            "abstract": "The Socratic method is a way of guiding students toward solving a problem independently without directly revealing the solution to the problem. Although this method has been shown to significantly improve student learning outcomes, it remains a complex labor-intensive task for instructors. Large language models (LLMs) can be used to augment human effort by automatically generating Socratic questions for students. However, existing methods that involve prompting these LLMs sometimes produce invalid outputs, e.g., those that directly reveal the solution to the problem or provide irrelevant or premature questions. To alleviate this problem, inspired by reinforcement learning with AI feedback (RLAIF), we first propose a data augmentation method to enrich existing Socratic questioning datasets with questions that are invalid in specific ways. Next, we propose a method to optimize open-source LLMs such as LLama 2 to prefer ground-truth questions over generated invalid ones, using direct preference optimization (DPO). Our experiments on a Socratic questions dataset for student code debugging show that a DPO-optimized 7B LLama 2 model can effectively avoid generating invalid questions, and as a result, outperforms existing state-of-the-art prompting methods.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A data augmentation method to enrich existing Socratic questioning datasets with questions that are invalid in specific ways and a method to optimize open-source LLMs such as LLama 2 to prefer ground-truth questions over generated invalid ones, using direct preference optimization (DPO)."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the reasoning abilities of language models, and the proposed approach is to use Socratic questioning to guide the models to engage in deep reasoning and critical thinking.\n\nThe research problem in the paper is improving the quality of automatically generated Socratic questions by language models, and the proposed approach is to use data augmentation and preference optimization to avoid generating invalid questions.\n\nWhile both works involve Socratic questioning and language models, the research problems and approaches are different. The proposal focuses on using Socratic questioning to improve the reasoning abilities of language models, while the paper focuses on improving the quality of generated Socratic questions.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "d7386e8859b22e05ce9c4a972613d4b1e1e44198",
            "paperId": "d7386e8859b22e05ce9c4a972613d4b1e1e44198",
            "title": "Prompting Large Language Models With the Socratic Method",
            "abstract": "This paper presents a systematic approach to using the Socratic method in developing prompt templates that effectively interact with large language models, including GPT-3. Various methods are examined, and those that yield precise answers and justifications while fostering creativity and imagination to enhance creative writing are identified. Techniques such as definition, elenchus, dialectic, maieutics, generalization, and counterfactual reasoning are discussed for their application in engineering prompt templates and their connections to inductive, deductive, and abductive reasoning. Through examples, the effectiveness of these dialogue and reasoning methods is demonstrated. An interesting observation is made that when the task's goal and user intent are conveyed to GPT-3 via ChatGPT before the start of a dialogue, the large language model seems to connect to the external context expressed in the intent and perform more effectively.",
            "year": 2023,
            "citationCount": 20,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An interesting observation is made that when the task's goal and user intent are conveyed to GPT-3 via ChatGPT before the start of a dialogue, the large language model seems to connect to the external context expressed in the intent and perform more effectively."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is that large language models often generate shallow and superficial responses that lack deep understanding and reasoning. The proposed approach is to incorporate Socratic questioning into the prompting process to guide the model to engage in deep reasoning and critical thinking.\n\nThe research problem in the paper is not explicitly stated, but it seems to be about effectively interacting with large language models using prompts. The proposed approach is to use the Socratic method and various dialogue and reasoning techniques to engineer effective prompt templates.\n\nWhile both works involve using the Socratic method with large language models, the research problems and goals are quite different. The proposal focuses specifically on improving the reasoning and critical thinking abilities of the models, while the paper is more broadly about effective interaction and prompt engineering.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "76e5069425547d4f53b5aa843a765a305b7fa470",
            "paperId": "76e5069425547d4f53b5aa843a765a305b7fa470",
            "title": "Discursive Socratic Questioning: (Unsupervised) Interpreting Neural Language Models for Discourse Understanding",
            "abstract": "Do neural language models (NLMs) understand 001 the discourse they are processing? Traditional 002 interpretation methods that address this ques-003 tion require pre-annotated explanations, which 004 defeats the purpose of unsupervised explana-005 tion. We propose unsupervised Discursive So-006 cratic Questioning ( D I SQ ), a two-step interpre-007 tative measure. 008 D I SQ first generates Socratic-style questions 009 about the discourse and then queries NLMs 010 about these questions. A model\u2019s understand-011 ing is measured by its responses to these ques-012 tions. We apply D I SQ to examine two fun-013 damental discourse phenomena, namely dis-014 course relation and discourse coherence. We 015 find NLMs demonstrate non-trivial capacities 016 without being trained on any discourse data: 017 Q&A pairs in D I SQ are shown to be evidence 018 for discourse relation and cohesive devices for 019 discourse coherence. D I SQ brings initial evi-020 dence that NLMs understand discourse through 021 reasoning. We find larger models perform bet-022 ter, but contradictions and hallucinations are 023 still problems. We recommend D I SQ as a uni-024 versal diagnostic for discursive NLMs and us-025 ing its output for self-supervision. 026",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "D I SQ brings initial evi-020 dence that NLMs understand discourse through 021 reasoning, and is recommended as a unsupervised versal diagnostic for discursive NLMs and its output for self-supervision."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the reasoning abilities and depth of understanding of language models. The proposed approach is to use Socratic questioning to guide the model to engage in deep reasoning and critical thinking during the generation process.\n\nThe research problem in the paper is interpreting whether neural language models understand the discourse they are processing. The proposed approach is to use unsupervised Discursive Socratic Questioning (DISQ) to generate Socratic-style questions about the discourse and measure the model's understanding based on its responses.\n\nWhile both works involve using Socratic questioning to probe the reasoning and understanding of language models, the specific research problems and goals are different. The proposal focuses on improving the models' reasoning abilities during generation, while the paper focuses on interpreting and measuring the models' existing discourse understanding capabilities.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "9b77d139e530302a537ecd1cf4ef20e3703a0a2b",
            "paperId": "9b77d139e530302a537ecd1cf4ef20e3703a0a2b",
            "title": "Response Generation for Cognitive Behavioral Therapy with Large Language Models: Comparative Study with Socratic Questioning",
            "abstract": "Dialogue systems controlled by predefined or rule-based scenarios derived from counseling techniques, such as cognitive behavioral therapy (CBT), play an important role in mental health apps. Despite the need for responsible responses, it is conceivable that using the newly emerging LLMs to generate contextually relevant utterances will enhance these apps. In this study, we construct dialogue modules based on a CBT scenario focused on conventional Socratic questioning using two kinds of LLMs: a Transformer-based dialogue model further trained with a social media empathetic counseling dataset, provided by Osaka Prefecture (OsakaED), and GPT-4, a state-of-the art LLM created by OpenAI. By comparing systems that use LLM-generated responses with those that do not, we investigate the impact of generated responses on subjective evaluations such as mood change, cognitive change, and dialogue quality (e.g., empathy). As a result, no notable improvements are observed when using the OsakaED model. When using GPT-4, the amount of mood change, empathy, and other dialogue qualities improve significantly. Results suggest that GPT-4 possesses a high counseling ability. However, they also indicate that even when using a dialogue model trained with a human counseling dataset, it does not necessarily yield better outcomes compared to scenario-based dialogues. While presenting LLM-generated responses, including GPT-4, and having them interact directly with users in real-life mental health care services may raise ethical issues, it is still possible for human professionals to produce example responses or response templates using LLMs in advance in systems that use rules, scenarios, or example responses.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study constructs dialogue modules based on a CBT scenario focused on conventional Socratic questioning using two kinds of LLMs: a Transformer-based dialogue model further trained with a social media empathetic counseling dataset, provided by Osaka Prefecture, and GPT-4, a state-of-the art LLM created by OpenAI."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the reasoning abilities of language models by incorporating Socratic questioning into the prompting process. The approach is to generate Socratic questions, engage the model in a Socratic dialogue, and then generate a final response based on the dialogue.\n\nThe research problem in the paper is enhancing mental health apps using large language models to generate contextually relevant responses in a cognitive behavioral therapy scenario. The approach is to compare systems that use LLM-generated responses with those that do not, focusing on conventional Socratic questioning.\n\nWhile both the proposal and the paper involve using Socratic questioning with language models, the application domains and specific approaches differ. The proposal focuses on general reasoning abilities, while the paper is specific to mental health apps and cognitive behavioral therapy.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "12dc76b5f04fb1fc34f94f26632d193305a97e7e",
            "paperId": "12dc76b5f04fb1fc34f94f26632d193305a97e7e",
            "title": "SocREval: Large Language Models with the Socratic Method for Reference-Free Reasoning Evaluation",
            "abstract": "To comprehensively gauge the capacity of current models for complex reasoning, it is crucial to assess their step-by-step reasoning in a scalable manner. Established reference-based evaluation metrics rely on human-annotated reasoning chains as references to assess the model-derived chains. However, such\"gold-standard\"human-written reasoning chains may not be unique and their acquisition is often labor-intensive. Existing reference-free reasoning evaluation metrics, while eliminating the need for human-crafted reasoning chains as references, often require fine-tuning with human-derived chains before evaluation, complicating the process and questioning their adaptability to other datasets. To address these challenges, we harness GPT-4 to automatically evaluate reasoning chain quality, thereby removing the dependency on human-written reasoning chains for both model fine-tuning and evaluative purposes. Leveraging the Socratic method, we develop SocREval ({\\bf Soc}ratic Method-Inspired {\\bf R}easoning {\\bf Eval}uation), a novel approach for prompt design in reference-free reasoning evaluation. Empirical results from four human annotated datasets reveal that SocREval significantly improves GPT-4's performance, surpassing existing reference-free and reference-based reasoning evaluation metrics. Beyond its demonstrated efficacy, SocREval, proves to be both cost-efficient and robust to prompt writing and example selection, as substantiated by our in-depth analysis.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "SocREval, a novel approach for prompt design in reference-free reasoning evaluation, harnesses GPT-4 to automatically evaluate reasoning chain quality, thereby removing the dependency on human-written reasoning chains for both model fine-tuning and evaluative purposes."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve the reasoning abilities of language models by incorporating Socratic questioning into the prompting process, guiding the models to generate more thoughtful and well-reasoned responses.\n\nThe paper abstract proposes a reference-free reasoning evaluation approach called SocREval, which leverages GPT-4 and the Socratic method to automatically evaluate the quality of reasoning chains without relying on human-annotated references.\n\nWhile both the project proposal and the paper abstract involve the Socratic method, their research problems and approaches are different. The project proposal focuses on improving the reasoning abilities of language models during the generation process, while the paper abstract focuses on evaluating the quality of reasoning chains generated by models.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "b408c712ab9425462855b8305d614c3421ad1216",
            "paperId": "b408c712ab9425462855b8305d614c3421ad1216",
            "title": "Socratic Questioning of Novice Debuggers: A Benchmark Dataset and Preliminary Evaluations",
            "abstract": "Socratic questioning is a teaching strategy where the student is guided towards solving a problem on their own, instead of being given the solution directly. In this paper, we introduce a dataset of Socratic conversations where an instructor helps a novice programmer fix buggy solutions to simple computational problems. The dataset is then used for benchmarking the Socratic debugging abilities of GPT-based language models. While GPT-4 is observed to perform much better than GPT-3.5, its precision, and recall still fall short of human expert abilities, motivating further work in this area.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a dataset of Socratic conversations where an instructor helps a novice programmer fix buggy solutions to simple computational problems, and uses this dataset for benchmarking the Socratic debugging abilities of GPT-based language models."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the reasoning abilities of language models, and the approach is using Socratic questioning to guide the models to generate more thoughtful and well-reasoned responses.\n\nThe research problem in the paper is benchmarking the Socratic debugging abilities of language models, and the approach is using a dataset of Socratic conversations where an instructor helps a novice programmer fix buggy code.\n\nWhile both works involve Socratic questioning, the proposal focuses on using it to improve general language model reasoning, while the paper studies Socratic questioning specifically in the context of debugging code. The research problems and approaches are not directly aligned.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "13b5b69355555e0c8b702261c5de3b4172ba653c",
            "paperId": "13b5b69355555e0c8b702261c5de3b4172ba653c",
            "title": "The Art of SOCRATIC QUESTIONING: Zero-shot Multimodal Reasoning with Recursive Thinking and Self-Questioning",
            "abstract": "Chain-of-Thought prompting (CoT) enables large-scale language models to solve complex reasoning problems by decomposing the problem and tackling it step-by-step. However, Chain-of-Thought is a greedy thinking process that requires the language model to come up with a starting point and generate the next step solely based on previous steps. This thinking process is different from how humans approach a complex problem e.g., we proactively raise sub-problems related to the original problem and recursively answer them. In this work, we propose S OCRATIC Q UESTIONING , a divide-and-conquer fashion algorithm that simulates the self-questioning and recursive thinking process. S OCRATIC Q UESTIONING is driven by a S ELF -Q UESTIONING module that employs a large-scale language model to propose sub-problems related to the original problem as intermediate steps and S OCRATIC Q UESTION - ING recursively backtracks and answers the sub-problems until reaches the original problem. We apply our proposed algorithm to the visual question-answering task as a case study and by evaluating it on three public benchmark datasets, we observe a significant performance improvement over all baselines on (almost) all datasets. In addition, the qualitative analysis clearly demonstrates the intermediate thinking steps elicited by S OCRATIC Q UESTIONING are similar to the human\u2019s recursively thinking process of a complex reasoning problem.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Qualitative analysis clearly demonstrates the intermediate thinking steps elicited by S OCRATIC Q UESTIONING are similar to the human\u2019s recursively thinking process of a complex reasoning problem."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve the reasoning abilities of language models by incorporating Socratic questioning into the prompting process to guide the model to engage in deep reasoning and critical thinking. The approach involves generating Socratic questions, engaging the model in a Socratic dialogue, and generating a final response based on the dialogue.\n\nThe paper abstract proposes a divide-and-conquer algorithm called SOCRATIC QUESTIONING that simulates the self-questioning and recursive thinking process to solve complex reasoning problems in visual question-answering. The approach involves employing a large-scale language model to propose sub-problems related to the original problem as intermediate steps and recursively answering the sub-problems.\n\nWhile both the project proposal and the paper abstract involve using Socratic questioning to improve the reasoning abilities of language models, the project proposal focuses on incorporating Socratic questioning into the prompting process for general language tasks, while the paper abstract specifically applies the Socratic questioning approach to the visual question-answering task.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "e6745fb621481ccb0ed53c267a37292e499c1b42",
            "paperId": "e6745fb621481ccb0ed53c267a37292e499c1b42",
            "title": "Automatic Generation of Socratic Subquestions for Teaching Math Word Problems",
            "abstract": "Socratic questioning is an educational method that allows students to discover answers to complex problems by asking them a series of thoughtful questions. Generation of didactically sound questions is challenging, requiring understanding of the reasoning process involved in the problem. We hypothesize that such questioning strategy can not only enhance the human performance, but also assist the math word problem (MWP) solvers.In this work, we explore the ability of large language models (LMs) in generating sequential questions for guiding math word problem-solving. We propose various guided question generation schemes based on input conditioning and reinforcement learning.On both automatic and human quality evaluations, we find that LMs constrained with desirable question properties generate superior questions and improve the overall performance of a math word problem solver. We conduct a preliminary user study to examine the potential value of such question generation models in the education domain. Results suggest that the difficulty level of problems plays an important role in determining whether questioning improves or hinders human performance. We discuss the future of using such questioning strategies in education.",
            "year": 2022,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores the ability of large language models (LMs) in generating sequential questions for guiding math word problem-solving and proposes various guided question generation schemes based on input conditioning and reinforcement learning."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the reasoning abilities and depth of language model responses using Socratic questioning. The approach is to generate Socratic questions to guide the model to engage in a dialogue before producing the final answer.\n\nThe research problem in the paper is generating Socratic subquestions to guide math word problem solving. The approach is to use large language models to generate sequential questions conditioned on desirable properties.\n\nWhile both works involve using Socratic questioning to guide problem-solving, the proposal focuses on general language model reasoning while the paper specifically targets math word problems. The methods also differ in the details of question generation and integration.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "ada81a4de88a6ce474df2e2446ad11fea480616e",
            "paperId": "ada81a4de88a6ce474df2e2446ad11fea480616e",
            "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language",
            "abstract": "Large pretrained (e.g.,\"foundation\") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodal-informed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning.",
            "year": 2022,
            "citationCount": 380,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Socratic Models (SMs) are shown to be competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, and enable new applications such as answering free-form questions about egocentric video, and engaging in multimodal assistive dialogue with people."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the reasoning abilities of language models by incorporating Socratic questioning into the prompting process. The approach is to generate Socratic questions, engage the model in a Socratic dialogue, and then generate a final response based on the dialogue.\n\nThe research problem in the paper is composing multimodal reasoning capabilities in foundation models without finetuning. The approach is to use a modular framework called Socratic Models, where pretrained models exchange information with each other via multimodal-informed prompting.\n\nThe proposal focuses on improving language models' reasoning via Socratic questioning, while the paper focuses on composing multimodal reasoning capabilities of different foundation models. The methods are also different: the proposal uses Socratic questioning in prompting, while the paper uses multimodal-informed prompting to exchange information between models.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "88b0ee59d565f2e04f62e1729ac9c681e132f103",
            "paperId": "88b0ee59d565f2e04f62e1729ac9c681e132f103",
            "title": "PlatoLM: Teaching LLMs via a Socratic Questioning User Simulator",
            "abstract": "The unparalleled performance of closed-sourced ChatGPT has sparked efforts towards its democratization, with notable strides made by leveraging real user and ChatGPT conversations, as evidenced by Vicuna. However, due to challenges in gathering conversations involving human participation, current endeavors like Baize and UltraChat aim to automatically generate conversational data. They primarily rely on ChatGPT conducting roleplay to simulate human behaviors based on instructions rather than genuine learning from humans, resulting in limited scope, diminished diversity, and an absence of genuine multi-round conversational dynamics. To address the above issues, we target human questions extracted from genuine human-machine conversations as a learning goal and train a user simulator called `Socratic' to produce a high-quality human-centric synthetic conversation dataset. Subsequently, this dataset was used to train our assistant model, named `PlatoLM'. Experimentally, PlatoLM outpaces baseline models in both Vicuna-Bench and MT-Bench by pairwise comparison when considering equivalent training set sizes, and manual evaluation also shows that our model is highly competitive. Impressively, when fine-tuned with the latest LLaMA 2 model, PlatoLM achieves the SOTA performance among 7B models (including LLaMA-2-7B-chat and Vicuna-7B) in MT-Bench benchmark and in Alpaca-Eval benchmark, it ranks second among 7B models, even beating some larger scale models (including LLaMA-2-13B-chat and GPT-3.5). Further in-depth analysis demonstrates the scalability and transferability of our approach. The code is available at https://github.com/FreedomIntelligence/PlatoLM.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work targets human questions extracted from genuine human-machine conversations as a learning goal and train a user simulator called `Socratic' to produce a high-quality human-centric synthetic conversation dataset, which was used to train the assistant model, named `PlatoLM'."
            },
            "score": 6
        },
        {
            "id": "f8b8f926bbfa327c86c40796131fe2695db81126",
            "paperId": "f8b8f926bbfa327c86c40796131fe2695db81126",
            "title": "DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models",
            "abstract": "A long-standing goal of AI systems is to perform complex multimodal reasoning like humans. Recently, large language models (LLMs) have made remarkable strides in such multi-step reasoning on the language modality solely by leveraging the chain of thought (CoT) to mimic human thinking. However, the transfer of these advancements to multimodal contexts introduces heightened challenges, including but not limited to the impractical need for labor-intensive annotation and the limitations in terms of flexibility, generalizability, and explainability. To evoke CoT reasoning in multimodality, this work first conducts an in-depth analysis of these challenges posed by multimodality and presents two key insights:\"keeping critical thinking\"and\"letting everyone do their jobs\"in multimodal CoT reasoning. Furthermore, this study proposes a novel DDCoT prompting that maintains a critical attitude through negative-space prompting and incorporates multimodality into reasoning by first dividing the reasoning responsibility of LLMs into reasoning and recognition and then integrating the visual recognition capability of visual models into the joint reasoning process. The rationales generated by DDCoT not only improve the reasoning abilities of both large and small language models in zero-shot prompting and fine-tuning learning, significantly outperforming state-of-the-art methods but also exhibit impressive generalizability and explainability.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study proposes a novel DDCoT prompting that maintains a critical attitude through negative-space prompting and incorporates multimodality into reasoning by first dividing the reasoning responsibility of LLMs into reasoning and recognition and then integrating the visual recognition capability of visual models into the joint reasoning process."
            },
            "score": 6
        },
        {
            "id": "ae76159a43743e6ecae5290921ade9c338a25b5e",
            "paperId": "ae76159a43743e6ecae5290921ade9c338a25b5e",
            "title": "ImpactBot: Chatbot Leveraging Language Models to Automate Feedback and Promote Critical Thinking Around Impact Statements",
            "abstract": "Impact statements articulate the impacts of a research project with concise and unambiguous statements about problems addressed, actions to resolve, and explanations of any impacts. Researchers and technologists often rely on impact statements as means to provoke introspective critical thinking around the impacts of technology being developed. However, due to factors such as technocentrism, positivity bias, marketization, or hyperinflation of impact statements, the claims presented in these statements do not cover all important aspects when creating technology \u2013 for instance, negative and delayed impacts. This work contributes to the development of a chatbot called ImpactBot to promote critical thinking while researchers create impact statements for research projects or scientific papers. The proposed chatbot leverages two fine-tuned state-of-the-art RoBERTa models for sequence classification and was assessed in this case study with 5 researchers from a large information technology company and 7 university engineering research scientists or students. This approach may be reused as part of content management or a paper submission system, for instance, to dialogue with researchers and promote critical thinking about negative impacts and how to mitigate them (if any) while creating impact statements for their projects or scientific papers.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work contributes to the development of a chatbot called ImpactBot to promote critical thinking while researchers create impact statements for research projects or scientific papers."
            },
            "score": 6
        },
        {
            "id": "3fa70115248377c3d1517c9f978791a296fbc1dd",
            "paperId": "3fa70115248377c3d1517c9f978791a296fbc1dd",
            "title": "Large Language Models Can Self-Improve",
            "abstract": "Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate\"high-confidence\"rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and 63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.",
            "year": 2022,
            "citationCount": 265,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work uses a pre-trained LLM to generate \"high-confidence\"rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs to improve the general reasoning ability."
            },
            "score": 6
        },
        {
            "id": "2429e82d122c4dc5fad239f6d4dbfb17ff2432dd",
            "paperId": "2429e82d122c4dc5fad239f6d4dbfb17ff2432dd",
            "title": "Effects of an annotation-supported Socratic questioning approach on students' argumentative writing performance and critical thinking skills in flipped language classrooms",
            "abstract": "Critical thinking and argumentative writing are considered complex but essential skills for learners. However, few studies have examined how instructions can be designed to optimize the development of such skills simultaneously. Contextualized in flipped language classrooms, this study aimed to develop an instructional innovation for English as a foreign language (EFL) learners.This study investigated the effects of an annotation\u2010supported Socratic questioning (ASSQ) approach as a flipped classroom enhancement on students' argumentative writing and critical thinking in an EFL writing course.Fifty students from two classes participated in the study, and they were randomly assigned to an experimental group and a comparison group. Following an 8\u2010week flipped learning curriculum, the experimental group learned with the proposed ASSQ approach in the enhanced flipped classroom, while the comparison group was exposed to the traditional flipped classroom. Students' argumentative writing performance was evaluated with their argumentative essays using a researcher\u2010developed writing rubric, and their critical thinking skills were assessed using the California Critical Thinking Skills Test.Both groups significantly improved their argumentative writing, and the experimental group also significantly outperformed the comparison group after learning through the proposed approach. While the two groups had no significant difference in critical thinking skills, the within\u2010group comparison results revealed that the experimental group made significant improvements due to the instructional intervention.The ASSQ approach could enhance the flipped classroom in regard to the teaching of argumentative writing, whereas it bears only limited impact on critical thinking. Overall, the positive effect of the proposed approach on enhancing English learners' argumentative writing performance was mainly attributed to the combined affordances of Socratic questioning and a social annotation tool to scaffold the students' thinking through verbal articulation, and to visualize their thinking with social annotations throughout the flipped learning process.",
            "year": 2023,
            "citationCount": 0,
            "tldr": null,
            "score": 5
        },
        {
            "id": "287d45f2090adba6ed8fc4834a029cd47373a88e",
            "paperId": "287d45f2090adba6ed8fc4834a029cd47373a88e",
            "title": "Can Language Models Employ the Socratic Method? Experiments with Code Debugging",
            "abstract": "When employing the Socratic method of teaching, instructors guide students toward solving a problem on their own rather than providing the solution directly. While this strategy can substantially improve learning outcomes, it is usually time-consuming and cognitively demanding. Automated Socratic conversational agents can augment human instruction and provide the necessary scale, however their development is hampered by the lack of suitable data for training and evaluation. In this paper, we introduce a manually created dataset of multi-turn Socratic advice that is aimed at helping a novice programmer fix buggy solutions to simple computational problems. The dataset is then used for benchmarking the Socratic debugging abilities of a number of language models, ranging from fine-tuning the instruction-based text-to-text transformer Flan-T5 to zero-shot and chain of thought prompting of the much larger GPT-4. The code and datasets are made freely available for research at the link below. https://github.com/taisazero/socratic-debugging-benchmark",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A manually created dataset of multi-turn Socratic advice aimed at helping a novice programmer fix buggy solutions to simple computational problems and is used for benchmarking the Socratic debugging abilities of a number of language models."
            },
            "score": 5
        },
        {
            "id": "42780f9c7f73d73d7a887e2f787af0e079703d40",
            "paperId": "42780f9c7f73d73d7a887e2f787af0e079703d40",
            "title": "Are Large Language Models Ready for Healthcare? A Comparative Study on Clinical Language Understanding",
            "abstract": "Large language models (LLMs) have made significant progress in various domains, including healthcare. However, the specialized nature of clinical language understanding tasks presents unique challenges and limitations that warrant further investigation. In this study, we conduct a comprehensive evaluation of state-of-the-art LLMs, namely GPT-3.5, GPT-4, and Bard, within the realm of clinical language understanding tasks. These tasks span a diverse range, including named entity recognition, relation extraction, natural language inference, semantic textual similarity, document classification, and question-answering. We also introduce a novel prompting strategy, self-questioning prompting (SQP), tailored to enhance LLMs' performance by eliciting informative questions and answers pertinent to the clinical scenarios at hand. Our evaluation underscores the significance of task-specific learning strategies and prompting techniques for improving LLMs' effectiveness in healthcare-related tasks. Additionally, our in-depth error analysis on the challenging relation extraction task offers valuable insights into error distribution and potential avenues for improvement using SQP. Our study sheds light on the practical implications of employing LLMs in the specialized domain of healthcare, serving as a foundation for future research and the development of potential applications in healthcare settings.",
            "year": 2023,
            "citationCount": 21,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An in-depth error analysis on the challenging relation extraction task offers valuable insights into error distribution and potential avenues for improvement using SQP, and introduces a novel prompting strategy, self-questioning prompting (SQP), tailored to enhance LLMs' performance by eliciting informative questions and answers pertinent to the clinical scenarios at hand."
            },
            "score": 5
        },
        {
            "id": "6ad26eb2d2aa6679d16d9c16fb75cd2cbe1127bc",
            "paperId": "6ad26eb2d2aa6679d16d9c16fb75cd2cbe1127bc",
            "title": "See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning",
            "abstract": "Large pre-trained vision and language models have demonstrated remarkable capacities for various tasks. However, solving the knowledge-based visual reasoning tasks remains challenging, which requires a model to comprehensively understand image content, connect the external world knowledge, and perform step-by-step reasoning to answer the questions correctly. To this end, we propose a novel framework named Interactive Prompting Visual Reasoner (IPVR) for few-shot knowledge-based visual reasoning. IPVR contains three stages, see, think and confirm. The see stage scans the image and grounds the visual concept candidates with a visual perception model. The think stage adopts a pre-trained large language model (LLM) to attend to the key concepts from candidates adaptively. It then transforms them into text context for prompting with a visual captioning model and adopts the LLM to generate the answer. The confirm stage further uses the LLM to generate the supporting rationale to the answer, verify the generated rationale with a cross-modality classifier and ensure that the rationale can infer the predicted output consistently. We conduct experiments on a range of knowledge-based visual reasoning datasets. We found our IPVR enjoys several benefits, 1). it achieves better performance than the previous few-shot learning baselines; 2). it enjoys the total transparency and trustworthiness of the whole reasoning process by providing rationales for each reasoning step; 3). it is computation-efficient compared with other fine-tuning baselines.",
            "year": 2023,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel framework named Interactive Prompting Visual Reasoner (IPVR), which achieves better performance than the previous few-shot learning baselines, and enjoys the total transparency and trustworthiness of the whole reasoning process by providing rationales for each reasoning step."
            },
            "score": 5
        },
        {
            "id": "4d766ae60fbbaea4f6c4ca8ad7f14569ec3189d1",
            "paperId": "4d766ae60fbbaea4f6c4ca8ad7f14569ec3189d1",
            "title": "Critical Thinking for Language Models",
            "abstract": "This paper takes a first step towards a critical thinking curriculum for neural auto-regressive language models. We introduce a synthetic corpus of deductively valid arguments, and generate artificial argumentative texts to train CRiPT: a critical thinking intermediarily pre-trained transformer based on GPT-2. Significant transfer learning effects can be observed: Trained on three simple core schemes, CRiPT accurately completes conclusions of different, and more complex types of arguments, too. CRiPT generalizes the core argument schemes in a correct way. Moreover, we obtain consistent and promising results for NLU benchmarks. In particular, CRiPT\u2019s zero-shot accuracy on the GLUE diagnostics exceeds GPT-2\u2019s performance by 15 percentage points. The findings suggest that intermediary pre-training on texts that exemplify basic reasoning abilities (such as typically covered in critical thinking textbooks) might help language models to acquire a broad range of reasoning skills. The synthetic argumentative texts presented in this paper are a promising starting point for building such a \u201ccritical thinking curriculum for language models.\u201d",
            "year": 2020,
            "citationCount": 30,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The findings suggest that intermediary pre-training on texts that exemplify basic reasoning abilities (such as typically covered in critical thinking textbooks) might help language models to acquire a broad range of reasoning skills."
            },
            "score": 5
        },
        {
            "id": "bdd325fa62eef7e8ba04e3694dc13554e9e06344",
            "paperId": "bdd325fa62eef7e8ba04e3694dc13554e9e06344",
            "title": "ChatGPT and Generative AI: Possibilities for Its Contribution to Lesson Planning, Critical Thinking and Openness in Teacher Education",
            "abstract": "Although artificial intelligence (AI) has been part of our lives for some time, the launch of the Generative Pretrained Transformer (ChatGPT) has given it renewed attention. While most of these debates are about higher education in general, this article focuses on schoolteacher education and teacher training. This research aimed to determine the contribution of generative AI tools such as ChatGPT in lesson planning, critical thinking and openness in education. The research used a qualitative approach and document analysis following an interpretative paradigm. The findings reveal that generative language models such as ChatGPT can provide specific materials and support mechanisms, such as lesson plans, to schoolteachers and student teachers. It also showed that ChatGPT has levelled the playing field by opening access to lesson plans to all teachers. However, to unleash their full potential for education, it is crucial to approach these models with caution and critically evaluate their limitations and potential biases, understanding that they are tools to support teaching and learning and do not replace teachers. The study\u2019s contribution lies in ChatGPT-generated lesson plans\u2019 implications and the enhancement of critical thinking for teacher education, and it also underscores the need for further research to explore best practices for integrating ChatGPT in lesson planning.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The findings reveal that generative language models such as ChatGPT can provide specific materials and support mechanisms, such as lesson plans, to schoolteachers and student teachers and underscores the need for further research to explore best practices for integratingChatGPT in lesson planning."
            },
            "score": 5
        },
        {
            "id": "dfab0f3ee6f47e36cccee145794cd117773e6f73",
            "paperId": "dfab0f3ee6f47e36cccee145794cd117773e6f73",
            "title": "Towards LLM-based Fact Verification on News Claims with a Hierarchical Step-by-Step Prompting Method",
            "abstract": "While large pre-trained language models (LLMs) have shown their impressive capabilities in various NLP tasks, they are still under-explored in the misinformation domain. In this paper, we examine LLMs with in-context learning (ICL) for news claim verification, and find that only with 4-shot demonstration examples, the performance of several prompting methods can be comparable with previous supervised models. To further boost performance, we introduce a Hierarchical Step-by-Step (HiSS) prompting method which directs LLMs to separate a claim into several subclaims and then verify each of them via multiple questions-answering steps progressively. Experiment results on two public misinformation datasets show that HiSS prompting outperforms state-of-the-art fully-supervised approach and strong few-shot ICL-enabled baselines.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Hierarchical Step-by-Step (HiSS) prompting method is introduced which directs LLMs to separate a claim into several subclaims and then verify each of them via multiple questions-answering steps progressively."
            },
            "score": 5
        },
        {
            "id": "c6d72ec48fa663f001b65e575d02697efb2706a8",
            "paperId": "c6d72ec48fa663f001b65e575d02697efb2706a8",
            "title": "SOCRATIC QUESTIONING TO PROMOTE EFL STUDENTS\u2019 CRITICAL THINKING IN A LANGUAGE LEARNING",
            "abstract": "This paper addressed the use of the Socratic Questioning method as an attempt to promote EFL students' critical thinking in language learning. It is ordinarily understood that the process of learning the language tends to focus more on how to answer questions rather than how to ask productive, systematic, and directed questions. Questioning as a means to fulfil curiosity is a driving factor for critical thinking activities. Questioning is able to direct the task and define the problem so that it will spur and trigger students' critical thinking. The use of Socratic Questioning and critical thinking methods embraces and encourages the analysis of Bloom's taxonomy integrally since Bloom's critical thinking is assigned as one of the highest levels of thinking in cognitive domains. Socratic Questioning, critical thinking, and Bloom's taxonomy are lines that must be passed in a language learning process in order to achieve the quality of skills which surely depends on the quality of thinking.",
            "year": 2019,
            "citationCount": 4,
            "tldr": null,
            "score": 4
        },
        {
            "id": "32e84115068a3b62cfec385ba1aac10270756158",
            "paperId": "32e84115068a3b62cfec385ba1aac10270756158",
            "title": "Exploring Critical Questioning Among In-Service ESL Teachers Using Socratic Questioning Technique (SQT)",
            "abstract": "The development of pupils' critical thinking depends on their ability to ask thoughtful questions. Although the advantages of critical questioning to develop students' critical thinking abilities have been widely recognized, teachers' poor questioning abilities become the main area of concern among educators. This paper aims to highlight this issue by exploring the use of Socratic Questioning Technique (SQT) in enhancing in-service teachers\u2019 critical questioning skills when writing reflections. An Action Research Design was adopted using multiple methods such as interview, document analysis and reflections. 13 in-service ESL teachers who enrolled in a one-semester Master of Education course with a focus on English Language Teaching participated in this study. This action research involved three phases which are: problem identification, intervention and evaluation. Two moments of intervention were conducted using Gibbs\u2019 Reflective Model and Socratic Critical Questioning techniques. The data were analyzed thematically. The findings revealed that SQT managed to enhance their critical questioning technique as they started asking variety of questions such as clarification, evidence, reasoning and implication questions during the intervention. The interview also showed that they learned to be more critical, they knew how to ask critical questions and they also became more confident in giving feedback to others. In other words, SQT trained these teachers about dialogic skills which are pertinent for fostering the capacity for critical thought. Thus, SQT should be incorporated in Malaysia's teacher education system .",
            "year": 2023,
            "citationCount": 0,
            "tldr": null,
            "score": 4
        },
        {
            "id": "67c30f2e39083a332498d14cd7546311845d7bc1",
            "paperId": "67c30f2e39083a332498d14cd7546311845d7bc1",
            "title": "Application of the Socratic Questioning Technique in Language Teaching",
            "abstract": null,
            "year": 2019,
            "citationCount": 0,
            "tldr": null,
            "score": 4
        },
        {
            "id": "dbe1085049b2ec750bd5090c76778533c50dce32",
            "paperId": "dbe1085049b2ec750bd5090c76778533c50dce32",
            "title": "The Application of Socratic Questioning Technique in Language Teaching",
            "abstract": "Socratic approach to questioning is based on the practice of disciplined, thoughtful dialogue. It is believed that disciplined practice of thoughtful questioning enable the students to examine ideas logically and to determine the validity of those ideas. The main purpose of this study is to introduce Socratic questioning technique in language teaching. Specifically, it deals with the tips to use, types of Socratic questioning technique and how to engage students with this technique. It is shown that the Socratic questioning technique is an effective way to explore ideas in depths. It can be used at all levels and is a helpful tool for all teachers. It can be used at different points within a unit of project. By using Socratic Questioning, teachers promote independent thinking in their students and give then ownership of what they are learning. Higher-level thinking skills are presented while students think, discuss debate, evaluate, and analyze content through their own thinking and the thinking of those around them.",
            "year": 2015,
            "citationCount": 0,
            "tldr": null,
            "score": 4
        },
        {
            "id": "0f2ae9b1757ca1d3993d7b48d116e5d178cc9225",
            "paperId": "0f2ae9b1757ca1d3993d7b48d116e5d178cc9225",
            "title": "SocraSynth: Multi-LLM Reasoning with Conditional Statistics",
            "abstract": "Large language models (LLMs), while promising, face criticisms for biases, hallucinations, and a lack of reasoning capability. This paper introduces SocraSynth, a multi-LLM agent reasoning platform developed to mitigate these issues. SocraSynth utilizes conditional statistics and systematic context enhancement through continuous arguments, alongside adjustable debate contentiousness levels. The platform typically involves a human moderator and two LLM agents representing opposing viewpoints on a given subject. SocraSynth operates in two main phases: knowledge generation and reasoning evaluation. In the knowledge generation phase, the moderator defines the debate topic and contentiousness level, prompting the agents to formulate supporting arguments for their respective stances. The reasoning evaluation phase then employs Socratic reasoning and formal logic principles to appraise the quality of the arguments presented. The dialogue concludes with the moderator adjusting the contentiousness from confrontational to collaborative, gathering final, conciliatory remarks to aid in human reasoning and decision-making. Through case studies in three distinct application domains, this paper showcases SocraSynth's effectiveness in fostering rigorous research, dynamic reasoning, comprehensive assessment, and enhanced collaboration. This underscores the value of multi-agent interactions in leveraging LLMs for advanced knowledge extraction and decision-making support.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Through case studies in three distinct application domains, this paper showcases SocraSynth's effectiveness in fostering rigorous research, dynamic reasoning, comprehensive assessment, and enhanced collaboration, underscores the value of multi-agent interactions in leveraging LLMs for advanced knowledge extraction and decision-making support."
            },
            "score": 4
        },
        {
            "id": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
            "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
            "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
            "year": 2022,
            "citationCount": 3517,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks."
            },
            "score": 4
        },
        {
            "id": "62176de125738e3b95850d1227bac81fd646b78e",
            "paperId": "62176de125738e3b95850d1227bac81fd646b78e",
            "title": "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models",
            "abstract": "Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, Few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual efforts, Zero-shot-CoT concatenates the target problem statement with \u201cLet\u2019s think step by step\u201d as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.",
            "year": 2023,
            "citationCount": 115,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results over GPT-3 show that the proposed zero-shot prompting consistently outperforms Zero- shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem."
            },
            "score": 4
        },
        {
            "id": "261549439aebdda72b648ecc462448fd24857ac1",
            "paperId": "261549439aebdda72b648ecc462448fd24857ac1",
            "title": "Progressive-Hint Prompting Improves Reasoning in Large Language Models",
            "abstract": "The performance of Large Language Models (LLMs) in reasoning tasks depends heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency being critical methods that enhance this ability. However, these methods do not fully exploit the answers generated by the LLM to guide subsequent responses. This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers. PHP is orthogonal to CoT and self-consistency, making it easy to combine with state-of-the-art techniques to further improve performance. We conducted extensive and comprehensive experiments on seven benchmarks. The results show that PHP significantly improves accuracy while remaining highly efficient. For instance, with text-davinci-003, we observed a 4.2% improvement on GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction in sample paths with self-consistency. With GPT-4 and PHP, we achieve state-of-the-art performances on SVAMP (89.1% ->91.9%), GSM8K (92% ->95.5%), AQuA (76.4% ->79.9%) and MATH (50.3% ->53.9%).",
            "year": 2023,
            "citationCount": 64,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers."
            },
            "score": 4
        },
        {
            "id": "5437e8adab596d7294124c0e798708e050e25321",
            "paperId": "5437e8adab596d7294124c0e798708e050e25321",
            "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
            "abstract": "Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99% using just 14 exemplars, compared to only 16% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.",
            "year": 2022,
            "citationCount": 581,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts."
            },
            "score": 4
        },
        {
            "id": "be9447ccc05a0e8a07321272778c7574173cf00e",
            "paperId": "be9447ccc05a0e8a07321272778c7574173cf00e",
            "title": "Resprompt: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models",
            "abstract": "Chain-of-thought (CoT) prompting, which offers step-by-step problem-solving rationales, has impressively unlocked the reasoning potential of large language models (LLMs). Yet, the standard CoT is less effective in problems demanding multiple reasoning steps. This limitation arises from the complex reasoning process in multi-step problems: later stages often depend on the results of several steps earlier, not just the results of the immediately preceding step. Such complexities suggest the reasoning process is naturally represented as a graph. The almost linear and straightforward structure of CoT prompting, however, struggles to capture this complex reasoning graph. To address this challenge, we propose Residual Connection Prompting (RESPROMPT), a new prompting strategy that advances multi-step reasoning in LLMs. Our key idea is to reconstruct the reasoning graph within prompts. We achieve this by integrating necessary connections-links present in the reasoning graph but missing in the linear CoT flow-into the prompts. Termed\"residual connections\", these links are pivotal in morphing the linear CoT structure into a graph representation, effectively capturing the complex reasoning graphs inherent in multi-step problems. We evaluate RESPROMPT on six benchmarks across three diverse domains: math, sequential, and commonsense reasoning. For the open-sourced LLaMA family of models, RESPROMPT yields a significant average reasoning accuracy improvement of 12.5% on LLaMA-65B and 6.8% on LLaMA2-70B. Breakdown analysis further highlights RESPROMPT particularly excels in complex multi-step reasoning: for questions demanding at least five reasoning steps, RESPROMPT outperforms the best CoT based benchmarks by a remarkable average improvement of 21.1% on LLaMA-65B and 14.3% on LLaMA2-70B. Through extensive ablation studies and analyses, we pinpoint how to most effectively build residual connections.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes Residual Connection Prompting (RESPROMPT), a new prompting strategy that advances multi-step reasoning in LLMs by integrating necessary connections-links present in the reasoning graph but missing in the linear CoT flow-into the prompts."
            },
            "score": 4
        },
        {
            "id": "ebcf91c007ebe553f804f8f1fdcf5ea5b7502328",
            "paperId": "ebcf91c007ebe553f804f8f1fdcf5ea5b7502328",
            "title": "Students\u2019 Critical Thinking Skills Using an E-Module Based on Problem-Based Learning Combined with Socratic Dialogue",
            "abstract": "This study aimed at testing the effectiveness of an e-module based on Problem-Based Learning combined with Socratic Dialogue on the topic of environmental change in improving school students' critical thinking skills. This research was a quasi-experimental study and used a non-randomised control group, pre-test/post-test design. The sampling technique was cluster random sampling. The sample in this study was two experimental classes (n = 72) and two control classes (n = 72). The data collection instruments included multiple-choice tests and a questionnaire. The test results were analysed using the t-test and the N-gain score test. The results showed significant difference in students' critical thinking skills between the experimental and control classes. Students who used the e-module showed better critical thinking skills. Students who used the e-module showed a greater increase in critical thinking skills after the learning process, compared to students who did not use the e-module. Thus, it can be concluded that the e-module based on PBL combined with Socratic Dialogue effectively improved students' critical thinking skills.",
            "year": 2024,
            "citationCount": 0,
            "tldr": null,
            "score": 4
        },
        {
            "id": "e877d295ca425faf33f0c8e4d8c410c2e9c8a26d",
            "paperId": "e877d295ca425faf33f0c8e4d8c410c2e9c8a26d",
            "title": "We're Afraid Language Models Aren't Modeling Ambiguity",
            "abstract": "Ambiguity is an intrinsic feature of natural language. Managing ambiguity is a key part of human language understanding, allowing us to anticipate misunderstanding as communicators and revise our interpretations as listeners. As language models (LMs) are increasingly employed as dialogue interfaces and writing aids, handling ambiguous language is critical to their success. We characterize ambiguity in a sentence by its effect on entailment relations with another sentence, and collect AmbiEnt, a linguist-annotated benchmark of 1,645 examples with diverse kinds of ambiguity. We design a suite of tests based on AmbiEnt, presenting the first evaluation of pretrained LMs to recognize ambiguity and disentangle possible meanings. We find that the task remains extremely challenging, including for GPT-4, whose generated disambiguations are considered correct only 32% of the time in human evaluation, compared to 90% for disambiguations in our dataset. Finally, to illustrate the value of ambiguity-sensitive tools, we show that a multilabel NLI model can flag political claims in the wild that are misleading due to ambiguity. We encourage the field to rediscover the importance of ambiguity for NLP.",
            "year": 2023,
            "citationCount": 44,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A suite of tests based on AmbiEnt is designed, presenting the first evaluation of pretrained LMs to recognize ambiguity and disentangle possible meanings, and it is shown that a multilabel NLI model can flag political claims in the wild that are misleading due to ambiguity."
            },
            "score": 4
        },
        {
            "id": "63b44548661605a6f606f578386629f46faf4e46",
            "paperId": "63b44548661605a6f606f578386629f46faf4e46",
            "title": "The Effect of Socratics Questioning Method in Improving Students\u2019 Speaking Skill and Critical Thinking in English as a Foreign Language Learning",
            "abstract": "The purpose of this paper is to look at how the Socratic questioning method can be used in EFL teaching and learning. This paper not only describes the method's beliefs and practices, but also assesses its efficacy in improving speaking ability and critical thinking. As the research design, convergent parallel mixed methods provides complete data (test, observation, and interview) to provide a clear interpretation of the Socratic questioning approach. This study was conducted with one teacher and 38 English department students at Tarakan State University in Indonesia. To evaluate quantitative data, Wilcoxon statistical analysis and Rank Spearman correlation analysis were utilized (pre-test and posttest). To evaluate qualitative data, this study used descript ion as analysis, inductive analysis, constructionist analysis, and coding (observation and interview). The Socratic questioning approach has three stages in its execution (preparation, running process, and assessment). The method's investigation reveals that it is beneficial in improving students' speaking skills and critical thinking (significance value.000 0.05). The Rank Spearman correlation demonstrates a strong link between speaking ability and critical thinking (coefficient .866). By selecting and altering topic discussion and Socratic questions, this technique is ideal for teaching and learning EFL. Through the analysis of Socratic questions that stimulate speaking skill and critical thinking, this study establishes the adoption and adaption of the Socratic questioning approach in EFL teaching and learning in Indonesia.",
            "year": 2022,
            "citationCount": 0,
            "tldr": null,
            "score": 4
        },
        {
            "id": "0f0a973c6457bcaf7255f891f9b34d658a0a84ae",
            "paperId": "0f0a973c6457bcaf7255f891f9b34d658a0a84ae",
            "title": "Generating medically-accurate summaries of patient-provider dialogue: A multi-stage approach using large language models",
            "abstract": "A medical provider\u2019s summary of a patient visit serves several critical purposes, including clinical decision-making, facilitating hand-offs between providers, and as a reference for the patient. An effective summary is required to be coherent and accurately capture all the medically relevant information in the dialogue, despite the complexity of patient-generated language. Even minor inaccuracies in visit summaries (for example, summarizing \u201cpatient does not have a fever\u201d when a fever is present) can be detrimental to the outcome of care for the patient.This paper tackles the problem of medical conversation summarization by discretizing the task into several smaller dialogue-understanding tasks that are sequentially built upon. First, we identify medical entities and their affirmations within the conversation to serve as building blocks. We study dynamically constructing few-shot prompts for tasks by conditioning on relevant patient information and use GPT-3 as the backbone for our experiments. We also develop GPT-derived summarization metrics to measure performance against reference summaries quantitatively. Both our human evaluation study and metrics for medical correctness show that summaries generated using this approach are clinically accurate and outperform the baseline approach of summarizing the dialog in a zero-shot, single-prompt setting.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper tackles the problem of medical conversation summarization by discretizing the task into several smaller dialogue-understanding tasks that are sequentially built upon and shows that summaries generated using this approach are clinically accurate and outperform the baseline approach of summarizing the dialog in a zero-shot, single-prompt setting."
            },
            "score": 4
        },
        {
            "id": "12c826f4195da172b212a529f8fcf10cc79e35da",
            "paperId": "12c826f4195da172b212a529f8fcf10cc79e35da",
            "title": "Context-faithful Prompting for Large Language Models",
            "abstract": "Large language models (LLMs) encode parametric knowledge about world facts and have shown remarkable performance in knowledge-driven NLP tasks. However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks). In this paper, we seek to assess and enhance LLMs' contextual faithfulness in two aspects: knowledge conflict and prediction with abstention. We demonstrate that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies. In particular, we identify opinion-based prompts and counterfactual demonstrations as the most effective methods. Opinion-based prompts reframe the context as a narrator's statement and inquire about the narrator's opinions, while counterfactual demonstrations use instances containing false facts to improve faithfulness in knowledge conflict situations. Neither technique requires additional training. We conduct experiments on three datasets of two standard NLP tasks, machine reading comprehension and relation extraction, and the results demonstrate significant improvement in faithfulness to contexts. Code and data are released at https://github.com/wzhouad/context-faithful-llm.",
            "year": 2023,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies, and opinion-based prompts and counterfactual demonstrations are identified as the most effective methods."
            },
            "score": 4
        },
        {
            "id": "8f7d8582314e10e122f2ac7527e6022788cef21f",
            "paperId": "8f7d8582314e10e122f2ac7527e6022788cef21f",
            "title": "Questioning Biases in Case Judgment Summaries: Legal Datasets or Large Language Models?",
            "abstract": "The evolution of legal datasets and the advent of large language models (LLMs) have significantly transformed the legal field, particularly in the generation of case judgment summaries. However, a critical concern arises regarding the potential biases embedded within these summaries. This study scrutinizes the biases present in case judgment summaries produced by legal datasets and large language models. The research aims to analyze the impact of biases on legal decision making. By interrogating the accuracy, fairness, and implications of biases in these summaries, this study contributes to a better understanding of the role of technology in legal contexts and the implications for justice systems worldwide. In this study, we investigate biases wrt Gender-related keywords, Race-related keywords, Keywords related to crime against women, Country names and religious keywords. The study shows interesting evidences of biases in the outputs generated by the large language models and pre-trained abstractive summarization models. The reasoning behind these biases needs further studies.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study scrutinizes the biases present in case judgment summaries produced by legal datasets and large language models, and investigates biases wrt Gender- related keywords, Race-related keywords, Keywords related to crime against women, Country names and religious keywords."
            },
            "score": 3
        },
        {
            "id": "9efa81ec4954b0859c47dad8f42edfaf8bced69b",
            "paperId": "9efa81ec4954b0859c47dad8f42edfaf8bced69b",
            "title": "Boosting Language Models Reasoning with Chain-of-Knowledge Prompting",
            "abstract": "Recently, Chain-of-Thought (CoT) prompting has delivered success on complex reasoning tasks, which aims at designing a simple prompt like ``Let's think step by step'' or multiple in-context exemplars with well-designed rationales to elicit Large Language Models (LLMs) to generate intermediate reasoning steps. However, the generated rationales often come with mistakes, making unfactual and unfaithful reasoning chains. To mitigate this brittleness, we propose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple. This is inspired by our human behaviors, i.e., we can draw a mind map or knowledge map as the reasoning evidence in the brain before answering a complex question. Benefiting from CoK, we additionally introduce a F^2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness. For the unreliable response, the wrong evidence can be indicated to prompt the LLM to rethink. Extensive experiments demonstrate that our method can further improve the performance of commonsense, factual, symbolic, and arithmetic reasoning tasks.",
            "year": 2023,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel Chain-of-Knowledge prompting, where it aims at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple, and introduces a F^2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness."
            },
            "score": 3
        },
        {
            "id": "0875651b68e6602d45ae08bee67cf63c02faa512",
            "paperId": "0875651b68e6602d45ae08bee67cf63c02faa512",
            "title": "Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models",
            "abstract": "Large language models (LLMs) have scaled up to unlock a wide range of complex reasoning tasks with the aid of various prompting methods. However, current prompting methods generate natural language intermediate steps to help reasoning, which can cause imperfect task reduction and confusion. To mitigate such limitations, we explore code prompting, a neural symbolic prompting method with both zero-shot and few-shot versions which triggers code as intermediate steps. We conduct experiments on 7 widely-used benchmarks involving symbolic reasoning and arithmetic reasoning. Code prompting generally outperforms chain-of-thought (CoT) prompting. To further understand the performance and limitations of code prompting, we perform extensive ablation studies and error analyses, and identify several exclusive advantages of using symbolic promptings compared to natural language. We also consider the ensemble of code prompting and CoT prompting to combine the strengths of both. Finally, we show through experiments how code annotations and their locations affect code prompting.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores code prompting, a neural symbolic prompting method with both zero-shot and few-shot versions which triggers code as intermediate steps, and performs extensive ablation studies and error analyses and identifies several exclusive advantages of using symbolic promptings compared to natural language."
            },
            "score": 3
        },
        {
            "id": "c218cd1772999517b137bbbc9872c4f67e540b7f",
            "paperId": "c218cd1772999517b137bbbc9872c4f67e540b7f",
            "title": "OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models",
            "abstract": "We conduct a thorough investigation into the reasoning capabilities of Large Language Models (LLMs), focusing specifically on the Open Pretrained Transformers (OPT) models as a representative of such models. Our study entails finetuning three different sizes of OPT on a carefully curated reasoning corpus, resulting in two sets of finetuned models: OPT-R, finetuned without explanations, and OPT-RE, finetuned with explanations. We then evaluate all models on 57 out-of-domain tasks drawn from the Super-NaturalInstructions benchmark, covering 26 distinct reasoning skills, utilizing three prompting techniques. Through a comprehensive grid of 27 configurations and 6,156 test evaluations, we investigate the dimensions of finetuning, prompting, and scale to understand the role of explanations on different reasoning skills. Our findings reveal that having explanations in the fewshot exemplar has no significant impact on the model\u2019s performance when the model is finetuned, while positively affecting the non-finetuned counterpart. Moreover, we observe a slight yet consistent increase in classification accuracy as we incorporate explanations during prompting and finetuning, respectively. Finally, we offer insights on which reasoning skills benefit the most from incorporating explanations during finetuning and prompting, such as Numerical (+20.4%) and Analogical (+13.9%) reasoning, as well as skills that exhibit negligible or negative effects.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is revealed that having explanations in the fewshot exemplar has no significant impact on the model\u2019s performance when the model is finetuned, while positively affecting the non-finetuned counterpart, and a slight yet consistent increase in classification accuracy as the authors incorporate explanations during prompting and finetuning."
            },
            "score": 3
        },
        {
            "id": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
            "paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
            "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "abstract": "Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).",
            "year": 2022,
            "citationCount": 1396,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting that first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths."
            },
            "score": 3
        },
        {
            "id": "281a7a99c16ce8f53bfbfb7aeb460dbd28648d28",
            "paperId": "281a7a99c16ce8f53bfbfb7aeb460dbd28648d28",
            "title": "Toxicity in ChatGPT: Analyzing Persona-assigned Language Models",
            "abstract": "Large language models (LLMs) have shown incredible capabilities and transcended the natural language processing (NLP) community, with adoption throughout many services like healthcare, therapy, education, and customer service. Since users include people with critical information needs like students or patients engaging with chatbots, the safety of these systems is of prime importance. Therefore, a clear understanding of the capabilities and limitations of LLMs is necessary. To this end, we systematically evaluate toxicity in over half a million generations of ChatGPT, a popular dialogue-based LLM. We find that setting the system parameter of ChatGPT by assigning it a persona, say that of the boxer Muhammad Ali, significantly increases the toxicity of generations. Depending on the persona assigned to ChatGPT, its toxicity can increase up to 6x, with outputs engaging in incorrect stereotypes, harmful dialogue, and hurtful opinions. This may be potentially defamatory to the persona and harmful to an unsuspecting user. Furthermore, we find concerning patterns where specific entities (e.g., certain races) are targeted more than others (3x more) irrespective of the assigned persona, that reflect inherent discriminatory biases in the model. We hope that our findings inspire the broader AI community to rethink the efficacy of current safety guardrails and develop better techniques that lead to robust, safe, and trustworthy AI systems.",
            "year": 2023,
            "citationCount": 159,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work systematically evaluates toxicity in over half a million generations of ChatGPT, a popular dialogue-based LLM, and finds concerning patterns where specific entities are targeted more than others irrespective of the assigned persona, that reflect inherent discriminatory biases in the model."
            },
            "score": 3
        },
        {
            "id": "c616cf321f433e34f05057d7e5441beeed8ecb82",
            "paperId": "c616cf321f433e34f05057d7e5441beeed8ecb82",
            "title": "Long-Horizon Dialogue Understanding for Role Identification in the Game of Avalon with Large Language Models",
            "abstract": "Deception and persuasion play a critical role in long-horizon dialogues between multiple parties, especially when the interests, goals, and motivations of the participants are not aligned. Such complex tasks pose challenges for current Large Language Models (LLM) as deception and persuasion can easily mislead them, especially in long-horizon multi-party dialogues. To this end, we explore the game of Avalon: The Resistance, a social deduction game in which players must determine each other's hidden identities to complete their team's objective. We introduce an online testbed and a dataset containing 20 carefully collected and labeled games among human players that exhibit long-horizon deception in a cooperative-competitive setting. We discuss the capabilities of LLMs to utilize deceptive long-horizon conversations between six human players to determine each player's goal and motivation. Particularly, we discuss the multimodal integration of the chat between the players and the game's state that grounds the conversation, providing further insights into the true player identities. We find that even current state-of-the-art LLMs do not reach human performance, making our dataset a compelling benchmark to investigate the decision-making and language-processing capabilities of LLMs. Our dataset and online testbed can be found at our project website: https://sstepput.github.io/Avalon-NLU/",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that even state-of-the-art LLMs do not reach human performance, making the dataset containing 20 carefully collected and labeled games among human players a compelling benchmark to investigate the decision-making and language-processing capabilities of LLMs."
            },
            "score": 3
        },
        {
            "id": "aeb032b29956783aafa0ec38a3989a3456129218",
            "paperId": "aeb032b29956783aafa0ec38a3989a3456129218",
            "title": "What Should Data Science Education Do with Large Language Models?",
            "abstract": "The rapid advances of large language models (LLMs), such as ChatGPT, are revolutionizing data science and statistics. These state-of-the-art tools can streamline complex processes. As a result, it reshapes the role of data scientists. We argue that LLMs are transforming the responsibilities of data scientists, shifting their focus from hands-on coding, data-wrangling and conducting standard analyses to assessing and managing analyses performed by these automated AIs. This evolution of roles is reminiscent of the transition from a software engineer to a product manager. We illustrate this transition with concrete data science case studies using LLMs in this paper. These developments necessitate a meaningful evolution in data science education. Pedagogy must now place greater emphasis on cultivating diverse skillsets among students, such as LLM-informed creativity, critical thinking, AI-guided programming. LLMs can also play a significant role in the classroom as interactive teaching and learning tools, contributing to personalized education. This paper discusses the opportunities, resources and open challenges for each of these directions. As with any transformative technology, integrating LLMs into education calls for careful consideration. While LLMs can perform repetitive tasks efficiently, it's crucial to remember that their role is to supplement human intelligence and creativity, not to replace it. Therefore, the new era of data science education should balance the benefits of LLMs while fostering complementary human expertise and innovations. In conclusion, the rise of LLMs heralds a transformative period for data science and its education. This paper seeks to shed light on the emerging trends, potential opportunities, and challenges accompanying this paradigm shift, hoping to spark further discourse and investigation into this exciting, uncharted territory.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is argued that LLMs are transforming the responsibilities of data scientists, shifting their focus from hands-on coding, data-wrangling and conducting standard analyses to assessing and managing analyses performed by these automated AIs."
            },
            "score": 3
        },
        {
            "id": "a5b499835d1781acb58e19b571a95b31e1b3f088",
            "paperId": "a5b499835d1781acb58e19b571a95b31e1b3f088",
            "title": "Better to Ask in English: Cross-Lingual Evaluation of Large Language Models for Healthcare Queries",
            "abstract": "Large language models (LLMs) are transforming the ways the general public accesses and consumes information. Their influence is particularly pronounced in pivotal sectors like healthcare, where lay individuals are increasingly appropriating LLMs as conversational agents for everyday queries. While LLMs demonstrate impressive language understanding and generation proficiencies, concerns regarding their safety remain paramount in these high-stake domains. Moreover, the development of LLMs is disproportionately focused on English. It remains unclear how these LLMs perform in the context of non-English languages, a gap that is critical for ensuring equity in the real-world use of these systems.This paper provides a framework to investigate the effectiveness of LLMs as multi-lingual dialogue systems for healthcare queries. Our empirically-derived framework XlingEval focuses on three fundamental criteria for evaluating LLM responses to naturalistic human-authored health-related questions: correctness, consistency, and verifiability. Through extensive experiments on four major global languages, including English, Spanish, Chinese, and Hindi, spanning three expert-annotated large health Q&A datasets, and through an amalgamation of algorithmic and human-evaluation strategies, we found a pronounced disparity in LLM responses across these languages, indicating a need for enhanced cross-lingual capabilities. We further propose XlingHealth, a cross-lingual benchmark for examining the multilingual capabilities of LLMs in the healthcare context. Our findings underscore the pressing need to bolster the cross-lingual capacities of these models, and to provide an equitable information ecosystem accessible to all.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper provides a framework to investigate the effectiveness of LLMs as multi-lingual dialogue systems for healthcare queries, and proposes XlingHealth, a cross-lingually benchmark for examining the multilingual capabilities of LLM in the healthcare context."
            },
            "score": 3
        },
        {
            "id": "8bc313e04cbd39847eb50b22af0a698ff2971a35",
            "paperId": "8bc313e04cbd39847eb50b22af0a698ff2971a35",
            "title": "Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models: A Case Study on ChatGPT",
            "abstract": "Generative large language models (LLMs), e.g., ChatGPT, have demonstrated remarkable proficiency across several NLP tasks, such as machine translation, text summarization. Recent research (Kocmi and Federmann, 2023) has shown that utilizing LLMs for assessing the quality of machine translation (MT) achieves state-of-the-art performance at the system level but \\textit{performs poorly at the segment level}. To further improve the performance of LLMs on MT quality assessment, we investigate several prompting designs, and propose a new prompting method called \\textbf{\\texttt{Error Analysis Prompting}} (EAPrompt) by combining Chain-of-Thoughts (Wei et al., 2022) and Error Analysis (Lu et al., 2023). This technique emulates the commonly accepted human evaluation framework - Multidimensional Quality Metrics (MQM, Freitag et al. (2021)) and \\textit{produces explainable and reliable MT evaluations at both the system and segment level}. Experimental Results from the WMT22 metrics shared task validate the effectiveness of EAPrompt on various LLMs, with different structures. Further analysis confirms that EAPrompt effectively distinguishes major errors from minor ones, while also sharing a similar distribution of the number of errors with MQM. These findings highlight the potential of EAPrompt as a human-like evaluator prompting technique for MT evaluation.",
            "year": 2023,
            "citationCount": 58,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Findings highlight the potential of EAPrompt as a human-like evaluator prompting technique for MT evaluation, and investigate several prompting designs, and propose a new prompting method called EAPrompt by combining Chain-of-Thoughts and Error Analysis."
            },
            "score": 3
        },
        {
            "id": "9ffefdf1fcd780cb71450b0a7a29247c66aa87be",
            "paperId": "9ffefdf1fcd780cb71450b0a7a29247c66aa87be",
            "title": "The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning",
            "abstract": "Does prompting a large language model (LLM) like GPT-3 with explanations improve in-context learning? We study this question on two NLP tasks that involve reasoning over text, namely question answering and natural language inference. We test the performance of four LLMs on three textual reasoning datasets using prompts that include explanations in multiple different styles. For these tasks, we find that including explanations in the prompts for OPT, GPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to moderate accuracy improvements over standard few-show learning. However, text-davinci-002 is able to benefit more substantially. We further show that explanations generated by the LLMs may not entail the models' predictions nor be factually grounded in the input, even on simple tasks with extractive explanations. However, these flawed explanations can still be useful as a way to verify LLMs' predictions post-hoc. Through analysis in our three settings, we show that explanations judged by humans to be good--logically consistent with the input and the prediction--more likely cooccur with accurate predictions. Following these observations, we train calibrators using automatically extracted scores that assess the reliability of explanations, allowing us to improve performance post-hoc across all of our datasets.",
            "year": 2022,
            "citationCount": 95,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work studies two NLP tasks that involve reasoning over text, namely question answering and natural language inference, and shows that explanations judged by humans to be good--logically consistent with the input and the prediction--more likely cooccur with accurate predictions."
            },
            "score": 3
        },
        {
            "id": "d5a6fc6aa139066e3b66ba63002e7d84c109aebc",
            "paperId": "d5a6fc6aa139066e3b66ba63002e7d84c109aebc",
            "title": "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing",
            "abstract": "Large language models (LLMs) have shown remarkable capabilities in Natural Language Processing (NLP), especially in domains where labeled data is scarce or expensive, such as clinical domain. However, to unlock the clinical knowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform specific clinical NLP tasks without any task-specific training data. This is known as in-context learning, which is an art and science that requires understanding the strengths and weaknesses of different LLMs and prompt engineering approaches. In this paper, we present a comprehensive and systematic experimental study on prompt engineering for five clinical NLP tasks: Clinical Sense Disambiguation, Biomedical Evidence Extraction, Coreference Resolution, Medication Status Extraction, and Medication Attribute Extraction. We assessed the prompts proposed in recent literature, including simple prefix, simple cloze, chain of thought, and anticipatory prompts, and introduced two new types of prompts, namely heuristic prompting and ensemble prompting. We evaluated the performance of these prompts on three state-of-the-art LLMs: GPT-3.5, BARD, and LLAMA2. We also contrasted zero-shot prompting with few-shot prompting, and provide novel insights and guidelines for prompt engineering for LLMs in clinical NLP. To the best of our knowledge, this is one of the first works on the empirical evaluation of different prompt engineering approaches for clinical NLP in this era of generative AI, and we hope that it will inspire and inform future research in this area.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents a comprehensive and systematic experimental study on prompt engineering for five clinical NLP tasks: Clinical Sense Disambiguation, Biomedical Evidence Extraction, Coreference Resolution, Medication Status Ext extraction, and Medication Attribute Extraction."
            },
            "score": 3
        },
        {
            "id": "33727cfa2710e9f502480b7eb9ac1925cb3bc06b",
            "paperId": "33727cfa2710e9f502480b7eb9ac1925cb3bc06b",
            "title": "AutoTrial: Prompting Language Models for Clinical Trial Design",
            "abstract": "Clinical trials are critical for drug development. Constructing the appropriate eligibility criteria (i.e., the inclusion/exclusion criteria for patient recruitment) is essential for the trial's success. Proper design of clinical trial protocols should consider similar precedent trials and their eligibility criteria to ensure sufficient patient coverage. In this paper, we present a method named AutoTrial to aid the design of clinical eligibility criteria using language models. It allows (1) controllable generation under instructions via a hybrid of discrete and neural prompting, (2) scalable knowledge incorporation via in-context learning, and (3) explicit reasoning chains to provide rationales for understanding the outputs. Experiments on over 70K clinical trials verify that AutoTrial generates high-quality criteria texts that are fluent and coherent and with high accuracy in capturing the relevant clinical concepts to the target trial. It is noteworthy that our method, with a much smaller parameter size, gains around 60% winning rate against the GPT-3.5 baselines via human evaluations.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A method named AutoTrial is presented to aid the design of clinical eligibility criteria using language models that allows controllable generation under instructions via a hybrid of discrete and neural prompting, scalable knowledge incorporation via in-context learning, and explicit reasoning chains to provide rationales for understanding the outputs."
            },
            "score": 2
        },
        {
            "id": "5427f39c7ec5bad121175165fd2259b8a94fea38",
            "paperId": "5427f39c7ec5bad121175165fd2259b8a94fea38",
            "title": "Thinking about the language models and what we can do with them",
            "abstract": "Abstract Massive developments in technology, ICT, and artificial intelligence have been witnessed in recent years, with various projects emerging showing the apparent superiority of artificial intelligence over human intelligence, such as Deep Blue, AlphaGo, OpenAI Five, and GPT 3. In connection with them and with the favoured method of machine learning with deep learning, the term language model has appeared, which has the ambition to become the basis of general artificial intelligence (AGI). Critical responses, however, have been claiming that this is a dead end. An alternative view to these critical responses is attempted to be shown in this article, with consideration given as to whether the language model is just what critics consider it to be or whether something more can be sought behind it.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An alternative view to these critical responses is attempted, with consideration given as to whether the language model is just what critics consider it to be or whether something more can be sought behind it."
            },
            "score": 2
        },
        {
            "id": "e10c2ed8de5568e6a41fb60c2adc4bf9c225da25",
            "paperId": "e10c2ed8de5568e6a41fb60c2adc4bf9c225da25",
            "title": "Is EVALITA Done? On the Impact of Prompting on the Italian NLP Evaluation Campaign",
            "abstract": "Prompt-based learning is a recent paradigm in NLP that leverages large pre-trained language models to perform a variety of tasks. With this technique, it is possible to build classifiers that do not need training data (zero-shot). In this paper, we assess the status of prompt-based learning applied to several text classification tasks in the Italian language. The results indicate that the performance gap towards current supervised methods is still relevant. However, the difference in performance between pre-trained models and the characteristic of the prompt-based classifier of operating in a zero-shot fashion open a discussion regarding the next generation of evaluation campaigns for NLP.",
            "year": 2022,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results indicate that the performance gap towards current supervised methods is still relevant, however, the difference in performance between pre-trained models and the characteristic of the prompt-based classifier of operating in a zero-shot fashion open a discussion regarding the next generation of evaluation campaigns for NLP."
            },
            "score": 2
        },
        {
            "id": "12c200e731017f9851afb1a6fe3fc7f76e6439c6",
            "paperId": "12c200e731017f9851afb1a6fe3fc7f76e6439c6",
            "title": "Explicit Visual Prompting for Low-Level Structure Segmentations",
            "abstract": "We consider the generic problem of detecting low-level structures in images, which includes segmenting the manipulated parts, identifying out-of-focus pixels, separating shadow regions, and detecting concealed objects. Whereas each such topic has been typically addressed with a domain-specific solution, we show that a unified approach performs well across all of them. We take inspiration from the widely-used pre-training and then prompt tuning protocols in NLP and propose a new visual prompting model, named Explicit Visual Prompting (EVP). Different from the previous visual prompting which is typically a dataset-level implicit embedding, our key insight is to enforce the tunable parameters focusing on the explicit visual content from each individual image, i.e., the features from frozen patch embeddings and the input's high-frequency components. The proposed EVP significantly outperforms other parameter-efficient tuning protocols under the same amount of tunable parameters (5.7% extra trainable parameters of each task). EVP also achieves state-of-the-art performances on diverse low-level structure segmentation tasks compared to task-specific solutions. Our code is available at: https://github.com/NiFangBaAGe/Explicit-Visual-Prompt.",
            "year": 2023,
            "citationCount": 42,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work takes inspiration from the widely-used pre-training and then prompt tuning protocols in NLP and proposes a new visual prompting model, named Explicit Visual Prompting (EVP), which significantly outperforms other parameter-efficient tuning protocols under the same amount of tunable parameters."
            },
            "score": 2
        },
        {
            "id": "9141480721653789597b6e537ee0eeab401f3e60",
            "paperId": "9141480721653789597b6e537ee0eeab401f3e60",
            "title": "PromptNER: Prompting For Named Entity Recognition",
            "abstract": "In a surprising turn, Large Language Models (LLMs) together with a growing arsenal of prompt-based heuristics now offer powerful off-the-shelf approaches providing few-shot solutions to myriad classic NLP problems. However, despite promising early results, these LLM-based few-shot methods remain far from the state of the art in Named Entity Recognition (NER), where prevailing methods include learning representations via end-to-end structural understanding and fine-tuning on standard labeled corpora. In this paper, we introduce PromptNER, a new state-of-the-art algorithm for few-Shot and cross-domain NER. To adapt to any new NER task PromptNER requires a set of entity definitions in addition to the standard few-shot examples. Given a sentence, PromptNER prompts an LLM to produce a list of potential entities along with corresponding explanations justifying their compatibility with the provided entity type definitions. Remarkably, PromptNER achieves state-of-the-art performance on few-shot NER, achieving a 4% (absolute) improvement in F1 score on the ConLL dataset, a 9% (absolute) improvement on the GENIA dataset, and a 4% (absolute) improvement on the FewNERD dataset. PromptNER also moves the state of the art on Cross Domain NER, outperforming prior methods (including those not limited to the few-shot setting), setting a new mark on 3/5 CrossNER target domains, with an average F1 gain of 3%, despite using less than 2% of the available data.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces PromptNER, a new state-of-the-art algorithm for few-Shot and cross-domain NER, and prompts an LLM to produce a list of potential entities along with corresponding explanations justifying their compatibility with the provided entity type definitions."
            },
            "score": 2
        },
        {
            "id": "c2408a3a8da4f12d3eb156fe359a96b428e5aff1",
            "paperId": "c2408a3a8da4f12d3eb156fe359a96b428e5aff1",
            "title": "Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models",
            "abstract": "In this paper, we take the initiative to investigate the performance of LLMs on complex planning tasks that require LLMs to understand a virtual spatial environment simulated via natural language and act correspondingly in text. We propose a benchmark named Natural Language Planning (NLP) composed of a set of novel tasks: Brick World, NLVR-based Manipulations, and Natural Language Navigation. We found that current popular LLMs such as ChatGPT still lack abilities in complex planning. This arises a question \u2013 do the LLMs have a good understanding of the environments described in natural language, or maybe other alternatives such as symbolic representations are neater and hence better to be understood by LLMs? To this end, we propose a novel method called C O S ( C hain- o f- S ymbol Prompting) that represents the complex environments with condensed symbolic spatial representations during the chained intermediate thinking steps. C O S is easy to use and does not need additional training on LLMs. Extensive experiments indicate that C O S clearly surpasses the performance of the Chain-of-Thought (CoT) Prompting in all three planning tasks with even fewer tokens used in the inputs compared with CoT on ChatGPT and InstructGPT. The performance gain is strong, by up to 60.8% accuracy (from 31.8% to 92.6%) on Brick World for ChatGPT. C O S also reduces the number of tokens in the prompt obviously, by up to 65.8% of the tokens (from 407 to 139) for the intermediate steps from demonstrations on Brick World.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel method called C O S is proposed that represents the complex environments with condensed symbolic spatial representations during the chained intermediate thinking steps that surpasses the performance of the Chain-of-Thought Prompting in all three planning tasks with even fewer tokens used in the inputs."
            },
            "score": 2
        },
        {
            "id": "8d806a91e5f2166ee6823eb7e6e8e56826b6776d",
            "paperId": "8d806a91e5f2166ee6823eb7e6e8e56826b6776d",
            "title": "NLPBench: Evaluating Large Language Models on Solving NLP Problems",
            "abstract": "Recent developments in large language models (LLMs) have shown promise in enhancing the capabilities of natural language processing (NLP). Despite these successes, there remains a dearth of research dedicated to the NLP problem-solving abilities of LLMs. To fill the gap in this area, we present a unique benchmarking dataset, NLPBench, comprising 378 college-level NLP questions spanning various NLP topics sourced from Yale University's prior final exams. NLPBench includes questions with context, in which multiple sub-questions share the same public information, and diverse question types, including multiple choice, short answer, and math. Our evaluation, centered on LLMs such as GPT-3.5/4, PaLM-2, and LLAMA-2, incorporates advanced prompting strategies like the chain-of-thought (CoT) and tree-of-thought (ToT). Our study reveals that the effectiveness of the advanced prompting strategies can be inconsistent, occasionally damaging LLM performance, especially in smaller models like the LLAMA-2 (13b). Furthermore, our manual assessment illuminated specific shortcomings in LLMs' scientific problem-solving skills, with weaknesses in logical decomposition and reasoning notably affecting results.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study presents a unique benchmarking dataset, NLPBench, comprising 378 college-level NLP questions spanning various NLP topics sourced from Yale University's prior final exams, and reveals that the effectiveness of the advanced prompting strategies can be inconsistent, occasionally damaging LLM performance."
            },
            "score": 2
        },
        {
            "id": "80c698688bb4488beaceaab5c64f701a946cb7ae",
            "paperId": "80c698688bb4488beaceaab5c64f701a946cb7ae",
            "title": "All in One: Multi-Task Prompting for Graph Neural Networks",
            "abstract": "Recently, \"pre-training and fine-tuning'' has been adopted as a standard workflow for many graph tasks since it can take general graph knowledge to relieve the lack of graph annotations from each application. However, graph tasks with node level, edge level, and graph level are far diversified, making the pre-training pretext often incompatible with these multiple tasks. This gap may even cause a \"negative transfer'' to the specific application, leading to poor results. Inspired by the prompt learning in natural language processing (NLP), which has presented significant effectiveness in leveraging prior knowledge for various NLP tasks, we study the prompting topic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models. Specifically, we first unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern. In this way, the prompting idea from NLP can be seamlessly introduced to the graph area. Then, to further narrow the gap between various graph tasks and state-of-the-art pre-training strategies, we further study the task space of various graph applications and reformulate downstream problems to the graph-level task. Afterward, we introduce meta-learning to efficiently learn a better initialization for the multi-task prompt of graphs so that our prompting framework can be more reliable and general for different tasks. We conduct extensive experiments, results from which demonstrate the superiority of our method.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel multi-task prompting method for graph models that unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern, and introduces meta-learning to efficiently learn a better initialization for the multi- task prompt of graphs so that the prompting framework can be more reliable and general for different tasks."
            },
            "score": 1
        },
        {
            "id": "98fc8faae839cc9b9dbfcf8e3ec3a7e2f7729db2",
            "paperId": "98fc8faae839cc9b9dbfcf8e3ec3a7e2f7729db2",
            "title": "NOTABLE: Transferable Backdoor Attacks Against Prompt-based NLP Models",
            "abstract": "Prompt-based learning is vulnerable to backdoor attacks. Existing backdoor attacks against prompt-based models consider injecting backdoors into the entire embedding layers or word embedding vectors. Such attacks can be easily affected by retraining on downstream tasks and with different prompting strategies, limiting the transferability of backdoor attacks. In this work, we propose transferable backdoor attacks against prompt-based models, called NOTABLE, which is independent of downstream tasks and prompting strategies. Specifically, NOTABLE injects backdoors into the encoders of PLMs by utilizing an adaptive verbalizer to bind triggers to specific words (i.e., anchors). It activates the backdoor by pasting input with triggers to reach adversary-desired anchors, achieving independence from downstream tasks and prompting strategies. We conduct experiments on six NLP tasks, three popular models, and three prompting strategies. Empirical results show that NOTABLE achieves superior attack performance (i.e., attack success rate over 90% on all the datasets), and outperforms two state-of-the-art baselines. Evaluations on three defenses show the robustness of NOTABLE. Our code can be found at https://github.com/RU-System-Software-and-Security/Notable.",
            "year": 2023,
            "citationCount": 19,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes transferable backdoor attacks against prompt-based models, called NOTABLE, which is independent of downstream tasks and prompting strategies, and achieves superior attack performance and outperforms two state-of-the-art baselines."
            },
            "score": 1
        },
        {
            "id": "0a25c137edc7c9752aa6d99ae4084683c3fe6b56",
            "paperId": "0a25c137edc7c9752aa6d99ae4084683c3fe6b56",
            "title": "Visual Prompting via Image Inpainting",
            "abstract": "How does one adapt a pre-trained visual model to novel downstream tasks without task-specific finetuning or any model modification? Inspired by prompting in NLP, this paper investigates visual prompting: given input-output image example(s) of a new task at test time and a new input image, the goal is to automatically produce the output image, consistent with the given examples. We show that posing this problem as simple image inpainting - literally just filling in a hole in a concatenated visual prompt image - turns out to be surprisingly effective, provided that the inpainting algorithm has been trained on the right data. We train masked auto-encoders on a new dataset that we curated - 88k unlabeled figures from academic papers sources on Arxiv. We apply visual prompting to these pretrained models and demonstrate results on various downstream image-to-image tasks, including foreground segmentation, single object detection, colorization, edge detection, etc.",
            "year": 2022,
            "citationCount": 109,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper investigates visual prompting: given input-output image example(s) of a new task at test time and a new input image, the goal is to automatically produce the output image, consistent with the given examples, and shows that posing this problem as simple image inpainting turns out to be surprisingly effective."
            },
            "score": 1
        },
        {
            "id": "f953dbea152b4160a4fc7463cc6dcd3214111117",
            "paperId": "f953dbea152b4160a4fc7463cc6dcd3214111117",
            "title": "Aligning with Whom? Large Language Models Have Gender and Racial Biases in Subjective NLP Tasks",
            "abstract": "Human perception of language depends on personal backgrounds like gender and ethnicity. While existing studies have shown that large language models (LLMs) hold values that are closer to certain societal groups, it is unclear whether their prediction behaviors on subjective NLP tasks also exhibit a similar bias. In this study, leveraging the POPQUORN dataset which contains annotations of diverse demographic backgrounds, we conduct a series of experiments on four popular LLMs to investigate their capability to understand group differences and potential biases in their predictions for politeness and offensiveness. We find that for both tasks, model predictions are closer to the labels from White and female participants. We further explore prompting with the target demographic labels and show that including the target demographic in the prompt actually worsens the model's performance. More specifically, when being prompted to respond from the perspective of\"Black\"and\"Asian\"individuals, models show lower performance in predicting both overall scores as well as the scores from corresponding groups. Our results suggest that LLMs hold gender and racial biases for subjective NLP tasks and that demographic-infused prompts alone may be insufficient to mitigate such effects. Code and data are available at https://github.com/Jiaxin-Pei/LLM-Group-Bias.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that for both tasks, model predictions are closer to the labels from White and female participants, and that demographic-infused prompts alone may be insufficient to mitigate such effects."
            },
            "score": 1
        }
    ],
    "novelty": "yes"
}