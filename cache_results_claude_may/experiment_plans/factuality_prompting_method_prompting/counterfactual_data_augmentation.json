{
    "topic_description": "novel prompting methods that can improve factuality and reduce hallucination of large language models",
    "idea_name": "Counterfactual Data Augmentation",
    "raw_idea": {
        "Problem": "Large language models are prone to generating factually incorrect information, especially when the input query is ambiguous, complex, or outside the model's training distribution.",
        "Existing Methods": "Existing methods for reducing hallucination often rely on curated datasets or human-written prompts, which can be expensive to obtain and may not cover a wide range of factual errors.",
        "Motivation": "Instead of relying solely on human-curated data, we propose to automatically generate counterfactual examples that challenge the model's factual knowledge and reasoning abilities. By training the model on these counterfactual examples, we can improve its robustness to factual errors and enhance its ability to generate factually consistent outputs.",
        "Proposed Method": "We propose Counterfactual Data Augmentation (CDA), a method that automatically generates counterfactual examples to augment the model's training data. Given a factual statement or query, CDA applies a set of predefined perturbations to generate counterfactual versions of the input. These perturbations can include swapping entities, negating facts, or introducing logical inconsistencies. The model is then trained to distinguish between the original factual input and the generated counterfactual examples, using techniques such as contrastive learning or consistency regularization. During inference, the model is prompted to generate outputs that are consistent with the original factual input and inconsistent with the counterfactual examples.",
        "Experiment Plan": "We will evaluate CDA on a range of factual generation tasks, such as fact verification, fact-grounded dialog, and fact-guided question answering. We will compare CDA to baseline methods such as direct prompting and consistency training, as well as state-of-the-art methods that use human-curated datasets or adversarial training. We will measure factuality using both automatic metrics (e.g., accuracy, consistency score) and human evaluation of the generated outputs' correctness and consistency with the original factual input. We will also analyze the quality and diversity of the generated counterfactual examples, and their impact on the model's robustness to different types of factual errors."
    },
    "full_experiment_plan": {
        "Title": "Counterfactual Data Augmentation for Reducing Factual Hallucination in Large Language Models",
        "Problem Statement": "Large language models (LLMs) are prone to generating factually incorrect information, especially when the input query is ambiguous, complex, or outside the model's training distribution. This problem of factual hallucination can lead to unreliable and misleading outputs, limiting the practical usability of LLMs in real-world applications.",
        "Motivation": "Existing methods for reducing hallucination often rely on curated datasets or human-written prompts, which can be expensive to obtain and may not cover a wide range of factual errors. Instead of relying solely on human-curated data, we propose to automatically generate counterfactual examples that challenge the model's factual knowledge and reasoning abilities. By training the model on these counterfactual examples, we can improve its robustness to factual errors and enhance its ability to generate factually consistent outputs.",
        "Proposed Method": "We propose Counterfactual Data Augmentation (CDA), a method that automatically generates counterfactual examples to augment the model's training data. Given a factual statement or query, CDA applies a set of predefined perturbations to generate counterfactual versions of the input. These perturbations can include swapping entities, negating facts, or introducing logical inconsistencies. The model is then trained to distinguish between the original factual input and the generated counterfactual examples, using techniques such as contrastive learning or consistency regularization. During inference, the model is prompted to generate outputs that are consistent with the original factual input and inconsistent with the counterfactual examples.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We will use a diverse set of datasets that require factual knowledge and reasoning, such as fact verification (e.g., FEVER), fact-grounded dialog (e.g., Wizard of Wikipedia), and fact-guided question answering (e.g., Natural Questions, TriviaQA). These datasets cover a range of domains and difficulty levels, allowing us to evaluate the effectiveness of CDA in different settings.",
            "Step 2: Implement Counterfactual Data Augmentation": "We will implement the CDA method to generate counterfactual examples for each dataset. The perturbations will be designed based on the characteristics of each dataset, such as the types of entities, relations, and reasoning patterns involved. We will experiment with different perturbation strategies and hyperparameters to find the optimal settings for each dataset.",
            "Step 3: Train Models with CDA": "We will train large language models (e.g., GPT-3, T5) on the augmented datasets using contrastive learning or consistency regularization. The models will be trained to assign higher probabilities to the original factual inputs and lower probabilities to the counterfactual examples. We will compare the performance of models trained with and without CDA to assess the effectiveness of our approach.",
            "Step 4: Evaluate on Factual Generation Tasks": "We will evaluate the trained models on a range of factual generation tasks, such as fact verification, fact-grounded dialog, and fact-guided question answering. We will use both automatic metrics (e.g., accuracy, consistency score) and human evaluation to assess the factuality and consistency of the generated outputs. We will compare the performance of models trained with CDA to baseline methods such as direct prompting and consistency training.",
            "Step 5: Analyze Counterfactual Examples": "We will analyze the quality and diversity of the generated counterfactual examples to gain insights into the strengths and limitations of CDA. We will examine the types of factual errors that CDA can effectively capture and the ones that are more challenging to generate. We will also investigate the impact of different perturbation strategies on the model's performance and robustness."
        },
        "Test Case Examples": {
            "Original Input": "Barack Obama was born in Hawaii.",
            "Counterfactual Input": "Barack Obama was born in Kenya.",
            "Baseline Output": "Barack Obama, the 44th President of the United States, was born in Honolulu, Hawaii on August 4, 1961. He was not born in Kenya.",
            "CDA Output": "The statement 'Barack Obama was born in Kenya' is factually incorrect. Barack Obama was born in Honolulu, Hawaii on August 4, 1961. Claims about his birthplace in Kenya have been thoroughly debunked and are considered conspiracy theories.",
            "Explanation": "The CDA model is able to distinguish between the factual and counterfactual inputs, generating an output that is consistent with the true fact and explicitly refutes the counterfactual claim. The baseline model, while mentioning the correct birthplace, does not directly address the counterfactual claim, potentially leaving room for ambiguity or misinterpretation."
        },
        "Fallback Plan": "If the proposed CDA method does not significantly improve the factuality and consistency of the generated outputs, we will conduct additional analyses to understand the limitations and potential improvements. We will examine the quality and diversity of the generated counterfactual examples to identify any shortcomings in the perturbation strategies. We will also explore alternative training objectives and architectures that may be more effective in capturing factual knowledge and reasoning abilities. If the results are still not satisfactory, we will pivot the project to focus on analyzing the factors that contribute to factual hallucination in LLMs and proposing new directions for future research. This analysis can provide valuable insights into the challenges and opportunities in developing more factually reliable language models."
    },
    "novelty_queries": [
        "KeywordQuery(\"counterfactual data augmentation language models\")",
        "KeywordQuery(\"reducing factual hallucination language models\")",
        "KeywordQuery(\"consistency regularization language models\")",
        "KeywordQuery(\"contrastive learning factual consistency language models\")",
        "KeywordQuery(\"Counterfactual Data Augmentation NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "fb8d0982d76945e136836a57e7a23907c21c2fb2",
            "paperId": "fb8d0982d76945e136836a57e7a23907c21c2fb2",
            "title": "Relation-based Counterfactual Data Augmentation and Contrastive Learning for Robustifying Natural Language Inference Models",
            "abstract": "Although pre-trained language models show good performance on various natural language processing tasks, they often rely on non-causal features and patterns to determine the outcome. For natural language inference tasks, previous results have shown that even a model trained on a large number of data fails to perform well on counterfactually revised data, indicating that the model is not robustly learning the semantics of the classes. In this paper, we propose a method in which we use token-based and sentence-based augmentation methods to generate counter-factual sentence pairs that belong to each class, and apply contrastive learning to help the model learn the difference between sentence pairs of different classes with similar contexts. Evaluation results with counterfactually-revised dataset and general NLI datasets show that the proposed method can improve the performance and robustness of the NLI model.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A method is proposed in which token-based and sentence-based augmentation methods are used to generate counter-factual sentence pairs that belong to each class, and contrastive learning is applied to help the model learn the difference between sentence pairs of different classes with similar contexts."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to reduce factual hallucination in large language models by generating counterfactual examples to augment the training data and using techniques like contrastive learning to distinguish between factual and counterfactual inputs.\n\nThe paper focuses on improving the robustness of natural language inference models by generating counterfactual sentence pairs and applying contrastive learning to help the model learn the difference between sentence pairs of different classes with similar contexts.\n\nWhile both the proposal and the paper use counterfactual data augmentation and contrastive learning, the proposal targets reducing factual hallucination in language models, while the paper aims to improve the robustness of natural language inference models. The research problems and application domains are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "fab5f884301cf5fc6e07907e3d136090d2641923",
            "paperId": "fab5f884301cf5fc6e07907e3d136090d2641923",
            "title": "Making a (Counterfactual) Difference One Rationale at a Time",
            "abstract": "Rationales, snippets of extracted text that explain an inference, have emerged as a popular framework for interpretable natural language processing (NLP). Rationale models typically consist of two cooperating modules: a selector and a classifier with the goal of maximizing the mutual information (MMI) between the\"selected\"text and the document label. Despite their promises, MMI-based methods often pick up on spurious text patterns and result in models with nonsensical behaviors. In this work, we investigate whether counterfactual data augmentation (CDA), without human assistance, can improve the performance of the selector by lowering the mutual information between spurious signals and the document label. Our counterfactuals are produced in an unsupervised fashion using class-dependent generative models. From an information theoretic lens, we derive properties of the unaugmented dataset for which our CDA approach would succeed. The effectiveness of CDA is empirically evaluated by comparing against several baselines including an improved MMI-based rationale schema on two multi aspect datasets. Our results show that CDA produces rationales that better capture the signal of interest.",
            "year": 2022,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates whether counterfactual data augmentation (CDA) can improve the performance of the selector by lowering the mutual information between spurious signals and the document label and shows that CDA produces rationales that better capture the signal of interest."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to reduce factual hallucination in large language models by generating counterfactual examples to augment the training data. The paper investigates whether counterfactual data augmentation can improve the performance of rationale models by lowering the mutual information between spurious signals and the document label.\n\nWhile both the project proposal and the paper use counterfactual data augmentation, their research problems and goals are different. The project focuses on reducing factual hallucination in language models, while the paper aims to improve the performance of rationale models by capturing the signal of interest better.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "50a5c61b5283e4dc2e0de1fcfd3ed9a525d2973a",
            "paperId": "50a5c61b5283e4dc2e0de1fcfd3ed9a525d2973a",
            "title": "Bias Challenges in Counterfactual Data Augmentation",
            "abstract": "Deep learning models tend not to be out-of-distribution robust primarily due to their reliance on spurious features to solve the task. Counterfactual data augmentations provide a general way of (approximately) achieving representations that are counterfactual-invariant to spurious features, a requirement for out-of-distribution (OOD) robustness. In this work, we show that counterfactual data augmentations may not achieve the desired counterfactual-invariance if the augmentation is performed by a context-guessing machine, an abstract machine that guesses the most-likely context of a given input. We theoretically analyze the invariance imposed by such counterfactual data augmentations and describe an exemplar NLP task where counterfactual data augmentation by a context-guessing machine does not lead to robust OOD classifiers.",
            "year": 2022,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that counterfactual data augmentations may not achieve the desired counterfactUAL-invariance if the augmentation is performed by a context-guessing machine, an abstract machine that guesses the most-likely context of a given input."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is reducing factual hallucination in large language models, and the proposed approach is counterfactual data augmentation. The research problem in the paper is improving out-of-distribution robustness of deep learning models, and the approach is also counterfactual data augmentation, but the paper focuses on the challenges and limitations of this approach.\n\nWhile both the proposal and the paper explore counterfactual data augmentation, the proposal aims to use it for reducing factual hallucination in language models, while the paper investigates its effectiveness in improving out-of-distribution robustness in general deep learning models. The paper also highlights potential issues with counterfactual data augmentation when performed by a context-guessing machine, which is not the focus of the proposal.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "301c09d74e46436bc75ea5f56d60acc549831961",
            "paperId": "301c09d74e46436bc75ea5f56d60acc549831961",
            "title": "CREST: A Joint Framework for Rationalization and Counterfactual Text Generation",
            "abstract": "Selective rationales and counterfactual examples have emerged as two effective, complementary classes of interpretability methods for analyzing and training NLP models. However, prior work has not explored how these methods can be integrated to combine their complementary advantages. We overcome this limitation by introducing CREST (ContRastive Edits with Sparse raTionalization), a joint framework for selective rationalization and counterfactual text generation, and show that this framework leads to improvements in counterfactual quality, model robustness, and interpretability. First, CREST generates valid counterfactuals that are more natural than those produced by previous methods, and subsequently can be used for data augmentation at scale, reducing the need for human-generated examples. Second, we introduce a new loss function that leverages CREST counterfactuals to regularize selective rationales and show that this regularization improves both model robustness and rationale quality, compared to methods that do not leverage CREST counterfactuals. Our results demonstrate that CREST successfully bridges the gap between selective rationales and counterfactual examples, addressing the limitations of existing methods and providing a more comprehensive view of a model\u2019s predictions.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results demonstrate that CREST successfully bridges the gap between selective rationales and counterfactual examples, addressing the limitations of existing methods and providing a more comprehensive view of a model\u2019s predictions."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is reducing factual hallucination in large language models, and the proposed approach is counterfactual data augmentation. The research problem in the paper is improving counterfactual quality, model robustness, and interpretability, and the proposed approach is a joint framework for selective rationalization and counterfactual text generation.\n\nWhile both the proposal and the paper involve counterfactual generation, their research problems and high-level approaches are different. The proposal focuses specifically on reducing factual hallucination using counterfactual data augmentation, while the paper aims to improve counterfactual quality, model robustness, and interpretability by combining selective rationalization and counterfactual generation.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "a04b3744834b1b167a590bbb3b6230a75e20accc",
            "paperId": "a04b3744834b1b167a590bbb3b6230a75e20accc",
            "title": "C2L: Causally Contrastive Learning for Robust Text Classification",
            "abstract": "Despite the super-human accuracy of recent deep models in NLP tasks, their robustness is reportedly limited due to their reliance on spurious patterns. We thus aim to leverage contrastive learning and counterfactual augmentation for robustness. For augmentation, existing work either requires humans to add counterfactuals to the dataset or machines to automatically matches near-counterfactuals already in the dataset. Unlike existing augmentation is affected by spurious correlations, ours, by synthesizing \u201ca set\u201d of counterfactuals, and making a collective decision on the distribution of predictions on this set, can robustly supervise the causality of each term. Our empirical results show that our approach, by collective decisions, is less sensitive to task model bias of attribution-based synthesis, and thus achieves significant improvements, in diverse dimensions: 1) counterfactual robustness, 2) cross-domain generalization, and 3) generalization from scarce data.",
            "year": 2022,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The empirical results show that the approach, by collective decisions, is less sensitive to task model bias of attribution-based synthesis, and thus achieves significant improvements, in diverse dimensions: 1) counterfactual robustness, 2) cross-domain generalization, and 3) generalization from scarce data."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to reduce factual hallucination in large language models by automatically generating counterfactual examples to augment the training data and improve the model's robustness to factual errors.\n\nThe paper focuses on improving the robustness of text classification models by leveraging contrastive learning and counterfactual augmentation to reduce reliance on spurious patterns.\n\nWhile both the project proposal and the paper use counterfactual augmentation to improve model robustness, they target different tasks (factual hallucination in language models vs. robustness in text classification) and employ different approaches (training models to distinguish between factual and counterfactual inputs vs. making collective decisions on synthesized counterfactuals).\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "048af7345faefb6fb0bee2924275e222f21742e2",
            "paperId": "048af7345faefb6fb0bee2924275e222f21742e2",
            "title": "Exploring the Efficacy of Automatically Generated Counterfactuals for Sentiment Analysis",
            "abstract": "While state-of-the-art NLP models have been achieving the excellent performance of a wide range of tasks in recent years, important questions are being raised about their robustness and their underlying sensitivity to systematic biases that may exist in their training and test data. Such issues come to be manifest in performance problems when faced with out-of-distribution data in the field. One recent solution has been to use counterfactually augmented datasets in order to reduce any reliance on spurious patterns that may exist in the original data. Producing high-quality augmented data can be costly and time-consuming as it usually needs to involve human feedback and crowdsourcing efforts. In this work, we propose an alternative by describing and evaluating an approach to automatically generating counterfactual data for the purpose of data augmentation and explanation. A comprehensive evaluation on several different datasets and using a variety of state-of-the-art benchmarks demonstrate how our approach can achieve significant improvements in model performance when compared to models training on the original data and even when compared to models trained with the benefit of human-generated augmented data.",
            "year": 2021,
            "citationCount": 42,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes an alternative by describing and evaluating an approach to automatically generating counterfactual data for the purpose of data augmentation and explanation and demonstrates how this approach can achieve significant improvements in model performance when compared to models training on the original data and even whenCompared to models trained with the benefit of human-generated augmented data."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to reduce factual hallucination in large language models by automatically generating counterfactual examples to augment the training data. The paper explores the efficacy of automatically generated counterfactuals for sentiment analysis.\n\nThe project focuses on improving factual correctness in language model outputs, while the paper aims to improve sentiment analysis performance and robustness. Although both use automatically generated counterfactuals, the application domains and goals are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "649208082a025265d9897f981356e8012a29effb",
            "paperId": "649208082a025265d9897f981356e8012a29effb",
            "title": "EXPLAIN, EDIT, GENERATE: Rationale-Sensitive Counterfactual Data Augmentation for Multi-hop Fact Verification",
            "abstract": "Automatic multi-hop fact verification task has gained significant attention in recent years. Despite impressive results, these well-designed models perform poorly on out-of-domain data. One possible solution is to augment the training data with counterfactuals, which are generated by minimally altering the causal features of the original data. However, current counterfactual data augmentation techniques fail to handle multi-hop fact verification due to their incapability to preserve the complex logical relationships within multiple correlated texts. In this paper, we overcome this limitation by developing a rationale-sensitive method to generate linguistically diverse and label-flipping counterfactuals while preserving logical relationships. In specific, the diverse and fluent counterfactuals are generated via an Explain-Edit-Generate architecture. Moreover, the checking and filtering modules are proposed to regularize the counterfactual data with logical relations and flipped labels. Experimental results show that the proposed approach outperforms the SOTA baselines and can generate linguistically diverse counterfactual data without disrupting their logical relationships.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper develops a rationale-sensitive method to generate linguistically diverse and label-flipping counterfactuals while preserving logical relationships through an Explain-Edit-Generate architecture and shows that the proposed approach outperforms the SOTA baselines and can generate linguically diversecounterfactual data without disrupting their logical relationships."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to reduce factual hallucination in large language models by generating counterfactual examples to augment the training data. The paper focuses on improving multi-hop fact verification by generating rationale-sensitive counterfactuals that preserve logical relationships within multiple correlated texts.\n\nWhile both the project proposal and the paper use counterfactual data augmentation, they target different research problems. The proposal addresses factual hallucination in general language generation, while the paper specifically tackles multi-hop fact verification. Additionally, the paper emphasizes preserving logical relationships in the generated counterfactuals, which is not the main focus of the project proposal.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "51e549e1bb49032ea24bfe74dfa30d52c1172ae4",
            "paperId": "51e549e1bb49032ea24bfe74dfa30d52c1172ae4",
            "title": "Counterfactual Data Augmentation improves Factuality of Abstractive Summarization",
            "abstract": "Abstractive summarization systems based on pretrained language models often generate coherent but factually inconsistent sentences. In this paper, we present a counterfactual data augmentation approach where we augment data with perturbed summaries that increase the training data diversity. Specifically, we present three augmentation approaches based on replacing (i) entities from other and the same category and (ii) nouns with their corresponding WordNet hypernyms. We show that augmenting the training data with our approach improves the factual correctness of summaries without significantly affecting the ROUGE score. We show that in two commonly used summarization datasets (CNN/Dailymail and XSum), we improve the factual correctness by about 2.5 points on average",
            "year": 2022,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that augmenting the training data with the counterfactual data augmentation approach improves the factual correctness of summaries without significantly affecting the ROUGE score."
            },
            "score": 6,
            "novelty_score": "The research problem in the project proposal is reducing factual hallucination in large language models, and the proposed approach is counterfactual data augmentation. The research problem in the paper is improving factual correctness of abstractive summarization, and the proposed approach is also counterfactual data augmentation.\n\nThe project proposal and the paper are both about using counterfactual data augmentation to improve factual correctness of language models, although they focus on different tasks (open-ended generation vs. summarization).\n\nYes",
            "novelty_judgment": "yes"
        },
        {
            "id": "ab94014cbf4c43926b6322a2c7085e7df2079792",
            "paperId": "ab94014cbf4c43926b6322a2c7085e7df2079792",
            "title": "Improving Robustness through Pairwise Generative Counterfactual Data Augmentation",
            "abstract": "Counterfactual Data Augmentation (CDA) is a commonly used technique for improving robustness in natural language classi\ufb01ers. However, one fundamental challenge is how to ef\ufb01-ciently label such synthetic data, particularly when they are in regions where the model is already not con\ufb01dent. Most meth-ods either rely on human-annotated templates, an expensive process which limits the scale of counterfactual data, or implicitly assume label invariance, which may mislead the model with incorrect labels. In this paper, we utilize counterfactual generative models to generate a large number of diverse coun-terfactuals that include multiple label changing and invariant assumptions, and learn a classi\ufb01er to automatically annotate more counterfactuals. Our key insight is that we can more effectively and ef\ufb01ciently annotate generated counterfactuals by training a pairwise classi\ufb01er that uses the original exam-ple\u2019s ground-truth label and compares the original example to the counterfactual. We demonstrate that with a small amount of human-annotated counterfactual data (e.g., 10%), we generate a counterfactual augmentation dataset which provides an 18-20% improvement in robustness and a 14-21% reduction in errors on 3 out-of-domain datasets, comparable to that of a fully human-annotated counterfactual dataset for both sentiment classi\ufb01cation and question paraphrase tasks.",
            "year": 2021,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A counterfactual augmentation dataset is generated which provides an 18-20% improvement in robustness and a 14-21% reduction in errors on 3 out-of-domain datasets, comparable to that of a fully human-annotated counterfactual dataset for both sentiment classi\ufb01cation and question paraphrase tasks."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to reduce factual hallucination in large language models by automatically generating counterfactual examples to augment the training data. The proposed approach is called Counterfactual Data Augmentation (CDA), which applies perturbations to factual statements or queries to generate counterfactual versions of the input.\n\nThe paper focuses on improving robustness in natural language classifiers through pairwise generative counterfactual data augmentation. The approach utilizes counterfactual generative models to generate diverse counterfactuals and learns a pairwise classifier to automatically annotate more counterfactuals.\n\nWhile both the project proposal and the paper use counterfactual data augmentation, the project proposal targets reducing factual hallucination in large language models, while the paper aims to improve robustness in natural language classifiers. The project proposal generates counterfactuals by perturbing factual statements, while the paper uses generative models and a pairwise classifier for annotation.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "39880b887c19ae80d71643b37d6fc89aba8ec0c4",
            "paperId": "39880b887c19ae80d71643b37d6fc89aba8ec0c4",
            "title": "NeuroCounterfactuals: Beyond Minimal-Edit Counterfactuals for Richer Data Augmentation",
            "abstract": "While counterfactual data augmentation offers a promising step towards robust generalization in natural language processing, producing a set of counterfactuals that offer valuable inductive bias for models remains a challenge. Most existing approaches for producing counterfactuals, manual or automated, rely on small perturbations via minimal edits, resulting in simplistic changes. We introduce NeuroCounterfactuals, designed as loose counterfactuals, allowing for larger edits which result in naturalistic generations containing linguistic diversity, while still bearing similarity to the original document. Our novel generative approach bridges the benefits of constrained decoding, with those of language model adaptation for sentiment steering. Training data augmentation with our generations results in both in-domain and out-of-domain improvements for sentiment classification, outperforming even manually curated counterfactuals, under select settings. We further present detailed analyses to show the advantages of NeuroCounterfactuals over approaches involving simple, minimal edits.",
            "year": 2022,
            "citationCount": 19,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "NeuroCounterfactuals is introduced, designed as loose counterfactUALs, allowing for larger edits which result in naturalistic generations containing linguistic diversity, while still bearing similarity to the original document."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to reduce factual hallucination in large language models by automatically generating counterfactual examples to augment the training data. The paper proposes NeuroCounterfactuals, a method for generating loose counterfactuals that allow for larger, more naturalistic edits to improve sentiment classification.\n\nThe project focuses on improving factual consistency in language model outputs, while the paper focuses on improving sentiment classification robustness. Although both use counterfactual data augmentation, the project generates counterfactuals to reduce factual errors, while the paper generates counterfactuals to improve sentiment steering.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "0d72949f3ccbd5d36974028299093632b3cb53f6",
            "paperId": "0d72949f3ccbd5d36974028299093632b3cb53f6",
            "title": "MAFIA: Multi-Adapter Fused Inclusive Language Models",
            "abstract": "Pretrained Language Models (PLMs) are widely used in NLP for various tasks. Recent studies have identified various biases that such models exhibit and have proposed methods to correct these biases. However, most of the works address a limited set of bias dimensions independently such as gender, race, or religion. Moreover, the methods typically involve finetuning the full model in order to maintain the performance on the downstream task. In this work, we aim to modularly debias a pre-trained language model across multiple dimensions. Previous works extensively explored debiasing PLMs by using limited US-centric counterfactual data augmentation (CDA). We use structured knowledge and a large generative model to build a diverse CDA across multiple bias dimensions in a semi-automated way. We highlight how existing debiasing methods do not consider interactions between multiple societal biases and propose a debiasing model that exploits the synergy amongst various societal biases and enables multi-bias debiasing simultaneously. An extensive evaluation on multiple tasks and languages demonstrates the efficacy of the approach.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work uses structured knowledge and a large generative model to build a diverse CDA across multiple bias dimensions in a semi-automated way and proposes a debiasing model that exploits the synergy amongst various societal biases and enables multi-bias debiasing simultaneously."
            },
            "score": 6
        },
        {
            "id": "46dfd46fb41d56bc77055e436dc969de8df0e76b",
            "paperId": "46dfd46fb41d56bc77055e436dc969de8df0e76b",
            "title": "Data augmentation and the role of hardness for feature learning in NLP",
            "abstract": ". Abstract Neural models often exploit features that generalize badly in order to achieve good performance. Overcoming this tendency is a central challenge in areas such as representation learning and ML fairness. We approach this problem, in part, from the perspective of feature hardness and data augmentation. First, we construct a dataset for linguistic acceptability in which multiple, competing features might be used for prediction. We \ufb01nd that in this setting, the downstream model \u2018prefers\u2019 \u2013 to some extent \u2013 the feature that is more easily extracted from the pre-trained model. OUr preliminary results suggest that the learning of downstream tasks in natural language processing (NLP) is governed, in part, by the \u2018clarity\u2019 with which features are represented by pre-trained models. Second, we introduce a toy setting to probe the e\ufb00ectiveness of data augmentation, a widely-used strategy to prevent models from learning undesirable heuristics. Adversarial or counterfactual data augmentation involves generating training examples where these heuristics fail, in order to encourage the model to use more general features. We show that, often, the added training examples help prevent the model from adopting the targeted heuristic, but do not help it learn more general features. We also \ufb01nd in many cases that the number of adversarial examples needed to reach a given error rate is independent of the amount of training data, and that adversarial data augmentation becomes less e\ufb00ective as the number of available heuristics increases and/or as the underlying learning problem becomes more challenging. Finally, we explore several de\ufb01nitions of feature hardness in the context of the same toy setting, including: (1) the area under the classi\ufb01cation learning curve, (2) the sum of weights of a classi\ufb01cation model, (3) the minimum description length given by the probing methods of Voita and Titov [2020], and (4) the number of adversarial counterexamples that are needed to induce a model to learn the feature. We show an correspondence between these de\ufb01nitions for the features in our toy setting.",
            "year": 2020,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Preliminary results suggest that the learning of downstream tasks in natural language processing (NLP) is governed, in part, by the \u2018clarity\u2019 with which features are represented by pre-trained models."
            },
            "score": 6
        },
        {
            "id": "06c8f8aa5d9fc02ea8ba35010e5b1e8420014c62",
            "paperId": "06c8f8aa5d9fc02ea8ba35010e5b1e8420014c62",
            "title": "CATfOOD: Counterfactual Augmented Training for Improving Out-of-Domain Performance and Calibration",
            "abstract": "In recent years, large language models (LLMs) have shown remarkable capabilities at scale, particularly at generating text conditioned on a prompt. In our work, we investigate the use of LLMs to augment training data of smaller language models (SLMs) with automatically generated counterfactual (CF) instances \u2013 i.e. minimally altered inputs \u2013 in order to improve out-of-domain (OOD) performance of SLMs in the extractive question answering (QA) setup. We show that, across various LLM generators, such data augmentation consistently enhances OOD performance and improves model calibration for both confidence-based and rationale-augmented calibrator models. Furthermore, these performance improvements correlate with higher diversity of CF instances in terms of their surface form and semantic content. Finally, we show that CF augmented models which are easier to calibrate also exhibit much lower entropy when assigning importance, indicating that rationale-augmented calibrators prefer concise explanations.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates the use of LLMs to augment training data of smaller language models with automatically generated counterfactual instances \u2013 i.e. minimally altered inputs \u2013 in order to improve out-of-domain (OOD) performance of SLMs in the extractive question answering (QA) setup and shows that such data augmentation consistently enhances OOD performance and improves model calibration."
            },
            "score": 6
        },
        {
            "id": "e0384ba36555232c587d4a80d527895a095a9001",
            "paperId": "e0384ba36555232c587d4a80d527895a095a9001",
            "title": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
            "abstract": "Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the Hallucination Evaluation benchmark for Large Language Models (HaluEval), a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing hallucination. To generate these samples, we propose a ChatGPT-based two-step framework, i.e., sampling-then-filtering. Besides, we also hire some human labelers to annotate the hallucinations in ChatGPT responses. The empirical results suggest that ChatGPT is likely to generate hallucinated content in specific topics by fabricating unverifiable information (i.e., about $19.5\\%$ responses). Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts. However, our experiments also prove that providing external knowledge or adding reasoning steps can help LLMs recognize hallucinations. Our benchmark can be accessed at https://github.com/RUCAIBox/HaluEval.",
            "year": 2023,
            "citationCount": 57,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Hallucination Evaluation benchmark for Large Language Models (HaluEval), a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing hallucination, is introduced and it is proved that providing external knowledge or adding reasoning steps can help LLMs recognize hallucinations."
            },
            "score": 6
        },
        {
            "id": "cb7fa7ee3df826628c113ba0c6db1205751d89a3",
            "paperId": "cb7fa7ee3df826628c113ba0c6db1205751d89a3",
            "title": "HELMA: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
            "abstract": "Large language models (LLMs), such as Chat-GPT, are prone to generate hallucinations, i.e., content that con\ufb02icts with the source or cannot be veri\ufb01ed by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the H allucination E valuation for Large L anguage M odels ( HELMA ) benchmark, a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing and alleviating hallucination. To generate these samples, we propose a ChatGPT-based two-step framework, i.e., sampling-then-\ufb01ltering . Speci\ufb01cally, we \ufb01rst adopt two different sampling methods to generate hallucinated samples based on instructions, and then use an example-enhanced \ufb01ltering method to select the best one. Furthermore, we also hire some human labelers to annotate the hallucinations in ChatGPT responses. The empirical results suggest that ChatGPT has some probabilities to generate hallucinations and existing LLMs face great challenges in recognizing the hallucinations in text. In addition, the performance can be improved by providing external knowledge or adding reasoning steps. Our benchmark can be accessed at https://github.com/ RUCAIBox/HELMA .",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The empirical results suggest that ChatGPT has some probabilities to generate hallucinations and existing LLMs face great challenges in recognizing the hallucinations in text, and the performance can be improved by providing external knowledge or adding reasoning steps."
            },
            "score": 6
        },
        {
            "id": "f543f1d36e4556b50b160c68fd18da3d7db40867",
            "paperId": "f543f1d36e4556b50b160c68fd18da3d7db40867",
            "title": "Hallucination Augmented Recitations for Language Models",
            "abstract": "Attribution is a key concept in large language models (LLMs) as it enables control over information sources and enhances the factuality of LLMs. While existing approaches utilize open book question answering to improve attribution, factual datasets may reward language models to recall facts that they already know from their pretraining data, not attribution. In contrast, counterfactual open book QA datasets would further improve attribution because the answer could only be grounded in the given text. We propose Hallucination Augmented Recitations (HAR) for creating counterfactual datasets by utilizing hallucination in LLMs to improve attribution. For open book QA as a case study, we demonstrate that models finetuned with our counterfactual datasets improve text grounding, leading to better open book QA performance, with up to an 8.0% increase in F1 score. Our counterfactual dataset leads to significantly better performance than using humanannotated factual datasets, even with 4x smaller datasets and 4x smaller models. We observe that improvements are consistent across various model sizes and datasets, including multi-hop, biomedical, and adversarial QA datasets.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes Hallucination Augmented Recitations (HAR) for creating counterfactual datasets by utilizing hallucination in LLMs to improve attribution and demonstrates that improvements are consistent across various model sizes and datasets, including multi-hop, biomedical, and adversarial QA datasets."
            },
            "score": 6
        },
        {
            "id": "ccfd026175f4a6683da5a8e661f2c055b1506277",
            "paperId": "ccfd026175f4a6683da5a8e661f2c055b1506277",
            "title": "Improving cross-lingual language understanding with consistency regularization-based fine-tuning",
            "abstract": null,
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes to improve cross-lingual language understanding with consistency regularization-based fine-tuning, and uses example consistency regularization to penalize the prediction sensitivity to four types of data augmentations, i.e., subword sampling, Gaussian noise, code-switch substitution, and machine translation."
            },
            "score": 6
        },
        {
            "id": "1da75c7a8915c0794bc9857f93c6cf31aac17ad2",
            "paperId": "1da75c7a8915c0794bc9857f93c6cf31aac17ad2",
            "title": "Consistency Regularization for Cross-Lingual Fine-Tuning",
            "abstract": "Fine-tuning pre-trained cross-lingual language models can transfer task-specific supervision from one language to the others. In this work, we propose to improve cross-lingual fine-tuning with consistency regularization. Specifically, we use example consistency regularization to penalize the prediction sensitivity to four types of data augmentations, i.e., subword sampling, Gaussian noise, code-switch substitution, and machine translation. In addition, we employ model consistency to regularize the models trained with two augmented versions of the same training set. Experimental results on the XTREME benchmark show that our method significantly improves cross-lingual fine-tuning across various tasks, including text classification, question answering, and sequence labeling.",
            "year": 2021,
            "citationCount": 47,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work uses example consistency regularization to penalize the prediction sensitivity to four types of data augmentations, i.e., subword sampling, Gaussian noise, code-switch substitution, and machine translation, to improve cross-lingual fine-tuning with consistency regularized."
            },
            "score": 6
        },
        {
            "id": "a390f26af171e784c15329847dd4a5e9806e15fa",
            "paperId": "a390f26af171e784c15329847dd4a5e9806e15fa",
            "title": "Logic-Guided Data Augmentation and Regularization for Consistent Question Answering",
            "abstract": "Many natural language questions require qualitative, quantitative or logical comparisons between two entities or events. This paper addresses the problem of improving the accuracy and consistency of responses to comparison questions by integrating logic rules and neural models. Our method leverages logical and linguistic knowledge to augment labeled training data and then uses a consistency-based regularizer to train the model. Improving the global consistency of predictions, our approach achieves large improvements over previous methods in a variety of question answering (QA) tasks, including multiple-choice qualitative reasoning, cause-effect reasoning, and extractive machine reading comprehension. In particular, our method significantly improves the performance of RoBERTa-based models by 1-5% across datasets. We advance state of the art by around 5-8% on WIQA and QuaRel and reduce consistency violations by 58% on HotpotQA. We further demonstrate that our approach can learn effectively from limited data.",
            "year": 2020,
            "citationCount": 95,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper addresses the problem of improving the accuracy and consistency of responses to comparison questions by integrating logic rules and neural models by leveraging logical and linguistic knowledge to augment labeled training data and then uses a consistency-based regularizer to train the model."
            },
            "score": 6
        },
        {
            "id": "27285b3760be8f0473245c13b97988265cd0467b",
            "paperId": "27285b3760be8f0473245c13b97988265cd0467b",
            "title": "Entity-level Factual Adaptiveness of Fine-tuning based Abstractive Summarization Models",
            "abstract": "Abstractive summarization models often generate factually inconsistent content particularly when the parametric knowledge of the model conflicts with the knowledge in the input document. In this paper, we analyze the robustness of fine-tuning based summarization models to the knowledge conflict, which we call factual adaptiveness. We utilize pre-trained language models to construct evaluation sets and find that factual adaptiveness is not strongly correlated with factual consistency on original datasets. Furthermore, we introduce a controllable counterfactual data augmentation method where the degree of knowledge conflict within the augmented data can be adjustable. Our experimental results on two pre-trained language models (PEGASUS and BART) and two fine-tuning datasets (XSum and CNN/DailyMail) demonstrate that our method enhances factual adaptiveness while achieving factual consistency on original datasets on par with the contrastive learning baseline.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper analyzes the robustness of fine-tuning based summarization models to the knowledge conflict, which is called factual adaptiveness, and introduces a controllable counterfactual data augmentation method where the degree of knowledge conflict within the augmented data can be adjustable."
            },
            "score": 6
        },
        {
            "id": "835ac3cbb41f2ec47718c5491211dd33b64f382b",
            "paperId": "835ac3cbb41f2ec47718c5491211dd33b64f382b",
            "title": "Counterfactual Data Augmentation for Mitigating Gender Stereotypes in Languages with Rich Morphology",
            "abstract": "Gender stereotypes are manifest in most of the world\u2019s languages and are consequently propagated or amplified by NLP systems. Although research has focused on mitigating gender stereotypes in English, the approaches that are commonly employed produce ungrammatical sentences in morphologically rich languages. We present a novel approach for converting between masculine-inflected and feminine-inflected sentences in such languages. For Spanish and Hebrew, our approach achieves F1 scores of 82% and 73% at the level of tags and accuracies of 90% and 87% at the level of forms. By evaluating our approach using four different languages, we show that, on average, it reduces gender stereotyping by a factor of 2.5 without any sacrifice to grammaticality.",
            "year": 2019,
            "citationCount": 221,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents a novel approach for converting between masculine-inflected and feminine-inflection sentences in morphologically rich languages and shows that it reduces gender stereotyping by a factor of 2.5 without any sacrifice to grammaticality."
            },
            "score": 6
        },
        {
            "id": "bbffc1d5b4a8786d6a33080204e5a35590a08389",
            "paperId": "bbffc1d5b4a8786d6a33080204e5a35590a08389",
            "title": "BLCU-NLP at SemEval-2020 Task 5: Data Augmentation for Efficient Counterfactual Detecting",
            "abstract": "Counterfactuals describe events counter to facts and hence naturally involve common sense, knowledge, and reasoning. SemEval 2020 task 5 is focusing on this field. We participate in the subtask 1 and we use BERT as our system. Our Innovations are feature extraction and data augmentation. We extract and summarize features of counterfactual statements, augment counterfactual examples in training set with the help of these features, and two general methods of data augmentation is experimented in our work. We demonstrate the effectiveness of our approaches, which achieves 0.95 of subtask 1 in F1 while using only a subset of giving training set to fine-tune the BERT model, and our official submission achieves F1 0.802, which ranks us 16th in the competition.",
            "year": 2020,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The effectiveness of the approaches is demonstrated, which achieves 0.95 of subtask 1 in F1 while using only a subset of giving training set to fine-tune the BERT model, and the official submission achieves F1 0.802, which ranks us 16th in the competition."
            },
            "score": 6
        },
        {
            "id": "541648e0d6938748ba0b2bbda41ff6a5f28409ff",
            "paperId": "541648e0d6938748ba0b2bbda41ff6a5f28409ff",
            "title": "Addressing Both Statistical and Causal Gender Fairness in NLP Models",
            "abstract": "Statistical fairness stipulates equivalent outcomes for every protected group, whereas causal fairness prescribes that a model makes the same prediction for an individual regardless of their protected characteristics. Counterfactual data augmentation (CDA) is effective for reducing bias in NLP models, yet models trained with CDA are often evaluated only on metrics that are closely tied to the causal fairness notion; similarly, sampling-based methods designed to promote statistical fairness are rarely evaluated for causal fairness. In this work, we evaluate both statistical and causal debiasing methods for gender bias in NLP models, and find that while such methods are effective at reducing bias as measured by the targeted metric, they do not necessarily improve results on other bias metrics. We demonstrate that combinations of statistical and causal debiasing techniques are able to reduce bias measured through both types of metrics.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work evaluates both statistical and causal debiasing methods for gender bias in NLP models, and finds that while such methods are effective at reducing bias as measured by the targeted metric, they do not necessarily improve results on other bias metrics."
            },
            "score": 6
        },
        {
            "id": "7fef656d079366c5cb42a84b4bd6a0991599847e",
            "paperId": "7fef656d079366c5cb42a84b4bd6a0991599847e",
            "title": "Retrieval-guided Counterfactual Generation for QA",
            "abstract": "Deep NLP models have been shown to be brittle to input perturbations. Recent work has shown that data augmentation using counterfactuals \u2014 i.e. minimally perturbed inputs \u2014 can help ameliorate this weakness. We focus on the task of creating counterfactuals for question answering, which presents unique challenges related to world knowledge, semantic diversity, and answerability. To address these challenges, we develop a Retrieve-Generate-Filter(RGF) technique to create counterfactual evaluation and training data with minimal human supervision. Using an open-domain QA framework and question generation model trained on original task data, we create counterfactuals that are fluent, semantically diverse, and automatically labeled. Data augmentation with RGF counterfactuals improves performance on out-of-domain and challenging evaluation sets over and above existing methods, in both the reading comprehension and open-domain QA settings. Moreover, we find that RGF data leads to significant improvements in a model\u2019s robustness to local perturbations.",
            "year": 2021,
            "citationCount": 20,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work develops a Retrieve-Generate-Filter technique to create counterfactual evaluation and training data with minimal human supervision, and finds that RGF data leads to significant improvements in a model\u2019s robustness to local perturbations."
            },
            "score": 6
        },
        {
            "id": "a45ff2a8b18abc850b267cf0ec6e391dba9138a5",
            "paperId": "a45ff2a8b18abc850b267cf0ec6e391dba9138a5",
            "title": "Deep Learning on a Healthy Data Diet: Finding Important Examples for Fairness",
            "abstract": "Data-driven predictive solutions predominant in commercial applications tend to suffer from biases and stereotypes, which raises equity concerns. Prediction models may discover, use, or amplify spurious correlations based on gender or other protected personal characteristics, thus discriminating against marginalized groups. Mitigating gender bias has become an important research focus in natural language processing (NLP) and is an area where annotated corpora are available. Data augmentation reduces gender bias by adding counterfactual examples to the training dataset. In this work, we show that some of the examples in the augmented dataset can be not important or even harmful to fairness. We hence propose a general method for pruning both the factual and counterfactual examples to maximize the model\u2019s fairness as measured by the demographic parity, equality of opportunity, and equality of odds. The fairness achieved by our method surpasses that of data augmentation on three text classification datasets, using no more than half of the examples in the augmented dataset. Our experiments are conducted using models of varying sizes and pre-training settings. WARNING: This work uses language that is offensive in nature.",
            "year": 2022,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work shows that some of the examples in the augmented dataset can be not important or even harmful to fairness, and proposes a general method for pruning both the factual and counterfactual examples to maximize the model\u2019s fairness as measured by the demographic parity, equality of opportunity, and equality of odds."
            },
            "score": 6
        },
        {
            "id": "63897018266ce8b8df9a63845c561aa63c90285a",
            "paperId": "63897018266ce8b8df9a63845c561aa63c90285a",
            "title": "Counterfactually-Augmented SNLI Training Data Does Not Yield Better Generalization Than Unaugmented Data",
            "abstract": "A growing body of work shows that models exploit annotation artifacts to achieve state-of-the-art performance on standard crowdsourced benchmarks\u2014datasets collected from crowdworkers to create an evaluation task\u2014while still failing on out-of-domain examples for the same task. Recent work has explored the use of counterfactually-augmented data\u2014data built by minimally editing a set of seed examples to yield counterfactual labels\u2014to augment training data associated with these benchmarks and build more robust classifiers that generalize better. However, Khashabi et al. (2020) find that this type of augmentation yields little benefit on reading comprehension tasks when controlling for dataset size and cost of collection. We build upon this work by using English natural language inference data to test model generalization and robustness and find that models trained on a counterfactually-augmented SNLI dataset do not generalize better than unaugmented datasets of similar size and that counterfactual augmentation can hurt performance, yielding models that are less robust to challenge examples. Counterfactual augmentation of natural language understanding data through standard crowdsourcing techniques does not appear to be an effective way of collecting training data and further innovation is required to make this general line of work viable.",
            "year": 2020,
            "citationCount": 33,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that models trained on a counterfactually-augmented SNLI dataset do not generalize better than unaugmenting datasets of similar size and that counterfactual augmentation can hurt performance, yielding models that are less robust to challenge examples."
            },
            "score": 6
        },
        {
            "id": "c41c3d07a10bebb4d8b6dcaea0d263ede051bda8",
            "paperId": "c41c3d07a10bebb4d8b6dcaea0d263ede051bda8",
            "title": "Can We Improve Model Robustness through Secondary Attribute Counterfactuals?",
            "abstract": "Developing robust NLP models that perform well on many, even small, slices of data is a significant but important challenge, with implications from fairness to general reliability. To this end, recent research has explored how models rely on spurious correlations, and how counterfactual data augmentation (CDA) can mitigate such issues. In this paper we study how and why modeling counterfactuals over multiple attributes can go significantly further in improving model performance. We propose RDI, a context-aware methodology which takes into account the impact of secondary attributes on the model\u2019s predictions and increases sensitivity for secondary attributes over reweighted counterfactually augmented data. By implementing RDI in the context of toxicity detection, we find that accounting for secondary attributes can significantly improve robustness, with improvements in sliced accuracy on the original dataset up to 7% compared to existing robustness methods. We also demonstrate that RDI generalizes to the coreference resolution task and provide guidelines to extend this to other tasks.",
            "year": 2021,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes RDI, a context-aware methodology which takes into account the impact of secondary attributes on the model\u2019s predictions and increases sensitivity for secondary attributes over reweighted counterfactually augmented data, and finds that accounting forsecondary attributes can significantly improve robustness."
            },
            "score": 6
        },
        {
            "id": "2660fcbec300315e349d1bff65c8802e4c7c4df4",
            "paperId": "2660fcbec300315e349d1bff65c8802e4c7c4df4",
            "title": "Counterfactual Data Augmentation for Neural Machine Translation",
            "abstract": "We propose a data augmentation method for neural machine translation. It works by interpreting language models and phrasal alignment causally. Specifically, it creates augmented parallel translation corpora by generating (path-specific) counterfactual aligned phrases. We generate these by sampling new source phrases from a masked language model, then sampling an aligned counterfactual target phrase by noting that a translation language model can be interpreted as a Gumbel-Max Structural Causal Model (Oberst and Sontag, 2019). Compared to previous work, our method takes both context and alignment into account to maintain the symmetry between source and target sequences. Experiments on IWSLT\u201915 English \u2192 Vietnamese, WMT\u201917 English \u2192 German, WMT\u201918 English \u2192 Turkish, and WMT\u201919 robust English \u2192 French show that the method can improve the performance of translation, backtranslation and translation robustness.",
            "year": 2021,
            "citationCount": 29,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work creates augmented parallel translation corpora by generating (path-specific) counterfactual aligned phrases by interpreting language models and phrasal alignment causally by taking both context and alignment into account."
            },
            "score": 5
        },
        {
            "id": "b6d534c8e49f7a9b167ce35facba0b1a907e4a85",
            "paperId": "b6d534c8e49f7a9b167ce35facba0b1a907e4a85",
            "title": "A Rationale-centric Counterfactual Data Augmentation Method for Cross-Document Event Coreference Resolution",
            "abstract": "Based on Pre-trained Language Models (PLMs), event coreference resolution (ECR) systems have demonstrated outstanding performance in clustering coreferential events across documents. However, the existing system exhibits an excessive reliance on the `triggers lexical matching' spurious pattern in the input mention pair text. We formalize the decision-making process of the baseline ECR system using a Structural Causal Model (SCM), aiming to identify spurious and causal associations (i.e., rationales) within the ECR task. Leveraging the debiasing capability of counterfactual data augmentation, we develop a rationale-centric counterfactual data augmentation method with LLM-in-the-loop. This method is specialized for pairwise input in the ECR system, where we conduct direct interventions on triggers and context to mitigate the spurious association while emphasizing the causation. Our approach achieves state-of-the-art performance on three popular cross-document ECR benchmarks and demonstrates robustness in out-of-domain scenarios.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work formalizes the decision-making process of the baseline ECR system using a Structural Causal Model (SCM), aiming to identify spurious and causal associations within the ECR task and develops a rationale-centric counterfactual data augmentation method with LLM-in-the-loop."
            },
            "score": 5
        },
        {
            "id": "2f130a320798dba1b13fa2e2822d42273e453038",
            "paperId": "2f130a320798dba1b13fa2e2822d42273e453038",
            "title": "An Empirical Analysis of Parameter-Efficient Methods for Debiasing Pre-Trained Language Models",
            "abstract": "The increasingly large size of modern pre-trained language models not only makes them inherit more human-like biases from the training corpora, but also makes it computationally expensive to mitigate such biases. In this paper, we investigate recent parameter-efficient methods in combination with counterfactual data augmentation (CDA) for bias mitigation. We conduct extensive experiments with prefix tuning, prompt tuning, and adapter tuning on different language models and bias types to evaluate their debiasing performance and abilities to preserve the internal knowledge of a pre-trained model. We find that the parameter-efficient methods (i) are effective in mitigating gender bias, where adapter tuning is consistently the most effective one and prompt tuning is more suitable for GPT-2 than BERT, (ii) areless effective when it comes to racial and religious bias, which may be attributed to the limitations of CDA, and (iii) can perform similarly to or sometimes better than full fine-tuning with improved time and memory efficiency, as well as maintain the internal knowledge in BERT and GPT-2, evaluated via fact retrieval and downstream fine-tuning.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Recent parameter-efficient methods in combination with counterfactual data augmentation (CDA) for bias mitigation are investigated and find that they are effective in mitigating gender bias and are less effective when it comes to racial and religious bias."
            },
            "score": 5
        },
        {
            "id": "1dea08f23e424973dc660b5eb22a6f1cba285795",
            "paperId": "1dea08f23e424973dc660b5eb22a6f1cba285795",
            "title": "In-Contextual Gender Bias Suppression for Large Language Models",
            "abstract": "Despite their impressive performance in a wide range of NLP tasks, Large Language Models (LLMs) have been reported to encode worrying-levels of gender biases. Prior work has proposed debiasing methods that require human labelled examples, data augmentation and fine-tuning of LLMs, which are computationally costly. Moreover, one might not even have access to the model parameters for performing debiasing such as in the case of closed LLMs such as GPT-4. To address this challenge, we propose bias suppression that prevents biased generations of LLMs by simply providing textual preambles constructed from manually designed templates and real-world statistics, without accessing to model parameters. We show that, using CrowsPairs dataset, our textual preambles covering counterfactual statements can suppress gender biases in English LLMs such as LLaMA2. Moreover, we find that gender-neutral descriptions of gender-biased objects can also suppress their gender biases. Moreover, we show that bias suppression has acceptable adverse effect on downstream task performance with HellaSwag and COPA.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that, using CrowsPairs dataset, textual preambles covering counterfactual statements can suppress gender biases in English LLMs such as LLaMA2, and it is found that gender-neutral descriptions of gender-biased objects can also suppress their gender biases."
            },
            "score": 5
        },
        {
            "id": "a1f0a972473ac0ffbe7d117597a0ba6ce7f78880",
            "paperId": "a1f0a972473ac0ffbe7d117597a0ba6ce7f78880",
            "title": "Layered Bias: Interpreting Bias in Pretrained Large Language Models",
            "abstract": "Large language models (LLMs) like GPT and PALM have excelled in numerous natural language processing (NLP) tasks such as text generation, question answering, and translation. However, they are also found to have inherent social biases. To address this, recent studies have proposed debiasing techniques like iterative nullspace projection (INLP) and Counterfactual Data Augmentation (CDA). Additionally, there\u2019s growing interest in understanding the intricacies of these models. Some researchers focus on individual neural units, while others examine specific layers. In our study, we benchmark newly released models, assess the impact of debiasing methods, and investigate how biases are linked to different transformer layers using a method called Logit Lens. Specifically, we evaluate three modern LLMs: OPT, LLaMA, and LLaMA2, and their debiased versions. Our experiments are based on two popular bias evaluation datasets, StereoSet and CrowS-Pairs, and we perform a layer-by-layer analysis using the Logit Lens.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study benchmarks newly released models, assess the impact of debiasing methods, and investigates how biases are linked to different transformer layers using a method called Logit Lens."
            },
            "score": 5
        },
        {
            "id": "e0a39a21615f658736852e3de896c4d9bef11a80",
            "paperId": "e0a39a21615f658736852e3de896c4d9bef11a80",
            "title": "Model-based Counterfactual Generator for Gender Bias Mitigation",
            "abstract": "Counterfactual Data Augmentation (CDA) has been one of the preferred techniques for mitigating gender bias in natural language models. CDA techniques have mostly employed word substitution based on dictionaries. Although such dictionary-based CDA techniques have been shown to significantly improve the mitigation of gender bias, in this paper, we highlight some limitations of such dictionary-based counterfactual data augmentation techniques, such as susceptibility to ungrammatical compositions, and lack of generalization outside the set of predefined dictionary words. Model-based solutions can alleviate these problems, yet the lack of qualitative parallel training data hinders development in this direction. Therefore, we propose a combination of data processing techniques and a bi-objective training regime to develop a model-based solution for generating counterfactuals to mitigate gender bias. We implemented our proposed solution and performed an empirical evaluation which shows how our model alleviates the shortcomings of dictionary-based solutions.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A combination of data processing techniques and a bi-objective training regime is proposed to develop a model-based solution for generating counterfactuals to mitigate gender bias and an empirical evaluation shows how the model alleviates the shortcomings of dictionary-based solutions."
            },
            "score": 5
        },
        {
            "id": "7c1707db9aafd209aa93db3251e7ebd593d55876",
            "paperId": "7c1707db9aafd209aa93db3251e7ebd593d55876",
            "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
            "abstract": "Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose\"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods.",
            "year": 2023,
            "citationCount": 155,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes \"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database, and demonstrates that it can detect non-factual and factual sentences and rank passages in terms of factuality."
            },
            "score": 5
        },
        {
            "id": "4b0b56be0ae9479d2bd5c2f0943db1906343c10f",
            "paperId": "4b0b56be0ae9479d2bd5c2f0943db1906343c10f",
            "title": "Chain-of-Verification Reduces Hallucination in Large Language Models",
            "abstract": "Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (CoVe) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show CoVe decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.",
            "year": 2023,
            "citationCount": 53,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Chain-of-Verification (CoVe) method is developed, whereby the model first drafts an initial response; then plans verification questions to fact-check its draft; and answers those questions independently so the answers are not biased by other responses."
            },
            "score": 5
        },
        {
            "id": "6af460d34bfc8e955e43fbe15cedcf329b48bc19",
            "paperId": "6af460d34bfc8e955e43fbe15cedcf329b48bc19",
            "title": "SAC3: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency",
            "abstract": "Hallucination detection is a critical step toward understanding the trustworthiness of modern language models (LMs). To achieve this goal, we re-examine existing detection approaches based on the self-consistency of LMs and uncover two types of hallucinations resulting from 1) question-level and 2) model-level, which cannot be effectively identified through self-consistency check alone. Building upon this discovery, we propose a novel sampling-based method, i.e., semantic-aware cross-check consistency (SAC3) that expands on the principle of self-consistency checking. Our SAC3 approach incorporates additional mechanisms to detect both question-level and model-level hallucinations by leveraging advances including semantically equivalent question perturbation and cross-model response consistency checking. Through extensive and systematic empirical analysis, we demonstrate that SAC3 outperforms the state of the art in detecting both non-factual and factual statements across multiple question-answering and open-domain generation benchmarks.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel sampling-based method, i.e., semantic-aware cross-check consistency (SAC3) that expands on the principle of self-consistency checking and demonstrates that SAC3 outperforms the state of the art in detecting both non-factual and factual statements across multiple question-answering and open-domain generation benchmarks."
            },
            "score": 5
        },
        {
            "id": "bb3cc013c462ff2bf3dc5be90f731ebf34996f86",
            "paperId": "bb3cc013c462ff2bf3dc5be90f731ebf34996f86",
            "title": "AutoHall: Automated Hallucination Dataset Generation for Large Language Models",
            "abstract": "While Large language models (LLMs) have garnered widespread applications across various domains due to their powerful language understanding and generation capabilities, the detection of non-factual or hallucinatory content generated by LLMs remains scarce. Currently, one significant challenge in hallucination detection is the laborious task of time-consuming and expensive manual annotation of the hallucinatory generation. To address this issue, this paper first introduces a method for automatically constructing model-specific hallucination datasets based on existing fact-checking datasets called AutoHall. Furthermore, we propose a zero-resource and black-box hallucination detection method based on self-contradiction. We conduct experiments towards prevalent open-/closed-source LLMs, achieving superior hallucination detection performance compared to extant baselines. Moreover, our experiments reveal variations in hallucination proportions and types among different models.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A method for automatically constructing model-specific hallucination datasets based on existing fact-checking datasets called AutoHall and a zero-resource and black-box hallucination detection method based on self-contradiction are introduced."
            },
            "score": 5
        },
        {
            "id": "96de6983be2d1bf2d947b29e320fde4e946a6dca",
            "paperId": "96de6983be2d1bf2d947b29e320fde4e946a6dca",
            "title": "Improving language models fine-tuning with representation consistency targets",
            "abstract": "Fine-tuning contextualized representations 001 learned by pre-trained language models has 002 become a standard practice in the NLP field. 003 However, pre-trained representations are prone 004 to degradation (also known as representation 005 collapse) during fine-tuning, which leads to in-006 stability, sub-optimal performance, and weak 007 generalization. In this paper, we propose a 008 novel fine-tuning method that avoids represen-009 tation collapse during fine-tuning by discourag-010 ing undesirable changes of the representations. 011 We show that our approach matches or exceeds 012 the performance of the existing regularization-013 based fine-tuning methods across 13 language 014 understanding tasks (GLUE benchmark and six 015 additional datasets). We also demonstrate its 016 effectiveness in low-data settings and robust-017 ness to label perturbation. Furthermore, we 018 extend previous studies of representation col-019 lapse and propose several metrics to quantify it. 020 Using these metrics and previously proposed 021 experiments, we show that our approach ob-022 tains significant improvements in retaining the 023 expressive power of representations. 024",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel fine-tuning method that avoids represen-009 collapse during fine-tuning by discourag-010 ing undesirable changes of the representations and shows significant improvements in retaining the expressive power of representations."
            },
            "score": 5
        },
        {
            "id": "41a4482bb47de60fdc44492fa633701e425cb501",
            "paperId": "41a4482bb47de60fdc44492fa633701e425cb501",
            "title": "SpARC: Sparsity Activation Regularization for Consistency",
            "abstract": "Transformers, while powerful language models, have a tendency to lack logical consistency in their beliefs [1]. Large language models such as T5 may simultaneously believe \u201cA bird can fly\" and \u201cA bird cannot fly\" [2]. In this work, we aim to finetune large language models to make them logically consistent. Specifically, we show that sparsifying model activations and increasing model modularity yields gains in both model accuracy and consistency. To achieve these improvements, we design a method of jointly penalising model activations through the L1 norm and employing a contrastive similarity loss between pairs of \u201csimilar\" and \u201cdissimilar\" facts. We perform qualitative analysis of the model attention weights and find a correlation between increased sparsity and increased consistency. Code is available at https://github.com/SConsul/SpARC/ .",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work designs a method of jointly penalising model activations through the L1 norm and employing a contrastive similarity loss between pairs of \u201csimilar\" and \u201cdissimilar\" facts, which yields gains in both model accuracy and consistency."
            },
            "score": 5
        },
        {
            "id": "e35d660e9096e1bf1d219f2457666c910384c2de",
            "paperId": "e35d660e9096e1bf1d219f2457666c910384c2de",
            "title": "MvSR-NAT: Multi-view Subset Regularization for Non-Autoregressive Machine Translation",
            "abstract": "Conditional masked language models (CMLM) have shown impressive progress in non-autoregressive machine translation (NAT). They learn the conditional translation model by predicting the random masked subset in the target sentence. Based on the CMLM framework, we introduce Multi-view Subset Regularization (MvSR), a novel regularization method to improve the performance of the NAT model. Specifically, MvSR consists of two parts: (1) \\textit{shared mask consistency}: we forward the same target with different mask strategies, and encourage the predictions of shared mask positions to be consistent with each other. (2) \\textit{model consistency}, we maintain an exponential moving average of the model weights, and enforce the predictions to be consistent between the average model and the online model. Without changing the CMLM-based architecture, our approach achieves remarkable performance on three public benchmarks with 0.36-1.14 BLEU gains over previous NAT models. Moreover, compared with the stronger Transformer baseline, we reduce the gap to 0.01-0.44 BLEU scores on small datasets (WMT16 RO$\\leftrightarrow$EN and IWSLT DE$\\rightarrow$EN).",
            "year": 2021,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Multi-view Subset Regularization (MvSR), a novel regularization method to improve the performance of the NAT model, achieves remarkable performance on three public benchmarks with 0.36-1.14 BLEU gains over previous NAT models."
            },
            "score": 5
        },
        {
            "id": "56373d3fd0f1354a61f9e577db039cdb187d8d43",
            "paperId": "56373d3fd0f1354a61f9e577db039cdb187d8d43",
            "title": "Evaluating the Factual Consistency of Large Language Models Through News Summarization",
            "abstract": "While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information. To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB(Factual Inconsistency Benchmark) that focuses on the task of summarization. Specifically, our benchmark involves comparing the scores an LLM assigns to a factually consistent versus a factually inconsistent summary for an input news article. For factually consistent summaries, we use human-written reference summaries that we manually verify as factually consistent. To generate summaries that are factually inconsistent, we generate summaries from a suite of summarization models that we have manually annotated as factually inconsistent. A model's factual consistency is then measured according to its accuracy, i.e.\\ the proportion of documents where it assigns a higher score to the factually consistent summary. To validate the usefulness of FIB, we evaluate 23 large language models ranging from 1B to 176B parameters from six different model families including BLOOM and OPT. We find that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries. However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries. We validate design choices in our benchmark including the scoring method and source of distractor summaries. Our code and benchmark data can be found at https://github.com/r-three/fib.",
            "year": 2022,
            "citationCount": 51,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new benchmark called FIB(Factual Inconsistency Benchmark) that focuses on the task of summarization, and finds that existing LLMs generally assign a higher score to factually consistent summaries than tofactually inconsistent summaries."
            },
            "score": 5
        },
        {
            "id": "d998ff93b2e58c9b219a9103f8c2ad714a41e4b9",
            "paperId": "d998ff93b2e58c9b219a9103f8c2ad714a41e4b9",
            "title": "The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency of Language Models",
            "abstract": "Large Language Models (LLMs) make natural interfaces to factual knowledge, but their usefulness is limited by their tendency to deliver inconsistent answers to semantically equivalent questions. For example, a model might predict both\"Anne Redpath passed away in Edinburgh.\"and\"Anne Redpath's life ended in London.\"In this work, we identify potential causes of inconsistency and evaluate the effectiveness of two mitigation strategies: up-scaling and augmenting the LM with a retrieval corpus. Our results on the LLaMA and Atlas models show that both strategies reduce inconsistency while retrieval augmentation is considerably more efficient. We further consider and disentangle the consistency contributions of different components of Atlas. For all LMs evaluated we find that syntactical form and other evaluation task artifacts impact consistency. Taken together, our results provide a better understanding of the factors affecting the factual consistency of language models.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work identifies potential causes of inconsistency and evaluates the effectiveness of two mitigation strategies: up-scaling and augmenting the LM with a retrieval corpus, showing that both strategies reduce inconsistency while retrieval augmentation is considerably more efficient."
            },
            "score": 5
        },
        {
            "id": "f33d19614d78f4a6e107e768ccd0a2d4244fc89a",
            "paperId": "f33d19614d78f4a6e107e768ccd0a2d4244fc89a",
            "title": "Evaluating the Factual Consistency of Large Language Models Through Summarization",
            "abstract": "While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information. To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB ( F actual I nconsistency B enchmark) that focuses on the task of summarization. Specifically, our benchmark involves comparing the scores an LLM assigns to a factually consistent versus a factually inconsistent summary for an input news article. For factually consistent summaries, we use human-written reference summaries that we manually verify as fac-tually consistent. To generate summaries that are factually inconsistent, we generate summaries from a suite of summarization models that we have manually annotated as fac-tually inconsistent. A model\u2019s factual consistency is then measured according to its accuracy, i.e. the proportion of documents where it assigns a higher score to the factually consistent summary. To validate the usefulness of FIB , we evaluate 23 large language models ranging from 1B to 176B parameters from six different model families including BLOOM and OPT. We \ufb01nd that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries. However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries. We validate design choices in our benchmark including the scoring method and source of distractor summaries. Our code and",
            "year": 2022,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new benchmark called FIB is proposed that focuses on the task of summarization and shows that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries, and that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries."
            },
            "score": 5
        },
        {
            "id": "01aa29292d3da33163c56e3c3c23ac933680b95b",
            "paperId": "01aa29292d3da33163c56e3c3c23ac933680b95b",
            "title": "Reinforced Counterfactual Data Augmentation for Dual Sentiment Classification",
            "abstract": "Data augmentation and adversarial perturbation approaches have recently achieved promising results in solving the over-fitting problem in many natural language processing (NLP) tasks including sentiment classification. However, existing studies aimed to improve the generalization ability by augmenting the training data with synonymous examples or adding random noises to word embeddings, which cannot address the spurious association problem. In this work, we propose an end-to-end reinforcement learning framework, which jointly performs counterfactual data generation and dual sentiment classification. Our approach has three characteristics:1) the generator automatically generates massive and diverse antonymous sentences; 2) the discriminator contains a original-side sentiment predictor and an antonymous-side sentiment predictor, which jointly evaluate the quality of the generated sample and help the generator iteratively generate higher-quality antonymous samples; 3) the discriminator is directly used as the final sentiment classifier without the need to build an extra one. Extensive experiments show that our approach outperforms strong data augmentation baselines on several benchmark sentiment classification datasets. Further analysis confirms our approach\u2019s advantages in generating more diverse training samples and solving the spurious association problem in sentiment classification.",
            "year": 2021,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An end-to-end reinforcement learning framework, which jointly performs counterfactual data generation and dual sentiment classification, and confirms the approach\u2019s advantages in generating more diverse training samples and solving the spurious association problem in sentiment classification."
            },
            "score": 5
        },
        {
            "id": "733ab85431becba55b91ead62f695b9e5da1ecaa",
            "paperId": "733ab85431becba55b91ead62f695b9e5da1ecaa",
            "title": "Towards Robust Aspect-based Sentiment Analysis through Non-counterfactual Augmentations",
            "abstract": "While state-of-the-art NLP models have demonstrated excellent performance for aspect based sentiment analysis (ABSA), substantial evidence has been presented on their lack of robustness. This is especially manifested as significant degradation in performance when faced with out-of-distribution data. Recent solutions that rely on counterfactually augmented datasets show promising results, but they are inherently limited because of the lack of access to explicit causal structure. In this paper, we present an alternative approach that relies on non-counterfactual data augmentation. Our proposal instead relies on using noisy, cost-efficient data augmentations that preserve semantics associated with the target aspect. Our approach then relies on modelling invariances between different versions of the data to improve robustness. A comprehensive suite of experiments shows that our proposal significantly improves upon strong pre-trained baselines on both standard and robustness-specific datasets. Our approach further establishes a new state-of-the-art on the ABSA robustness benchmark and transfers well across domains.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper's approach relies on using noisy, cost-efficient data augmentations that preserve semantics associated with the target aspect that establishes a new state-of-the-art on the ABSA robustness benchmark and transfers well across domains."
            },
            "score": 5
        },
        {
            "id": "daadc5409e524dce9171f4579b595878a17e4887",
            "paperId": "daadc5409e524dce9171f4579b595878a17e4887",
            "title": "Transforming Dutch: Debiasing Dutch Coreference Resolution Systems for Non-binary Pronouns",
            "abstract": "Gender-neutral pronouns are increasingly being introduced across Western languages. Recent evaluations have however demonstrated that English NLP systems are unable to correctly process gender-neutral pronouns, with the risk of erasing and misgendering non-binary individuals. This paper examines a Dutch coreference resolution system's performance on gender-neutral pronouns, specifically hen and die. In Dutch, these pronouns were only introduced in 2016, compared to the longstanding existence of singular they in English. We additionally compare two debiasing techniques for coreference resolution systems in non-binary contexts: Counterfactual Data Augmentation (CDA) and delexicalisation. Moreover, because pronoun performance can be hard to interpret from a general evaluation metric like LEA, we introduce an innovative evaluation metric, the pronoun score, which directly represents the portion of correctly processed pronouns. Our results reveal diminished performance on gender-neutral pronouns compared to gendered counterparts. Nevertheless, although delexicalisation fails to yield improvements, CDA substantially reduces the performance gap between gendered and gender-neutral pronouns. We further show that CDA remains effective in low-resource settings, in which a limited set of debiasing documents is used. This efficacy extends to previously unseen neopronouns, which are currently infrequently used but may gain popularity in the future, underscoring the viability of effective debiasing with minimal resources and low computational costs.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Dutch coreference resolution system's performance on gender-neutral pronouns, specifically hen and die, is examined, and an innovative evaluation metric, the pronoun score, is introduced, which directly represents the portion of correctly processed pronouns."
            },
            "score": 5
        },
        {
            "id": "99368d6fc86e2eb181d9d36165cfed578bfe938d",
            "paperId": "99368d6fc86e2eb181d9d36165cfed578bfe938d",
            "title": "Q: How to Specialize Large Vision-Language Models to Data-Scarce VQA Tasks? A: Self-Train on Unlabeled Images!",
            "abstract": "Finetuning a large vision language model (VLM) on a target dataset after large scale pretraining is a dominant paradigm in visual question answering (VQA). Datasets for specialized tasks such as knowledge-based VQA or VQA in non natural-image domains are orders of magnitude smaller than those for general-purpose VQA. While collecting additional labels for specialized tasks or domains can be challenging, unlabeled images are often available. We introduce SelTDA (Self-Taught Data Augmentation), a strategy for finetuning large VLMs on small-scale VQA datasets. SelTDA uses the VLM and target dataset to build a teacher model that can generate question-answer pseudolabels directly conditioned on an image alone, allowing us to pseudolabel unlabeled images. SelTDA then finetunes the initial VLM on the original dataset augmented with freshly pseudolabeled images. We describe a series of experiments showing that our self-taught data augmentation increases robustness to adversarially searched questions, counterfactual examples and rephrasings, improves domain generalization, and results in greater retention of numerical reasoning skills. The proposed strategy requires no additional annotations or architectural modifications, and is compatible with any modern encoder-decoder multimodal transformer. Code available at https://github.com/codezakh/SelTDA.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces SelTDA (Self-Taught Data Augmentation), a strategy for finetuning large VLMs on small-scale VQA datasets and describes a series of experiments showing that the proposed strategy increases robustness to adversarially searched questions, counterfactual examples and rephrasings, improves domain generalization, and results in greater retention of numerical reasoning skills."
            },
            "score": 4
        },
        {
            "id": "fd26c019d889b816c28fa2e15e2571faa78592bb",
            "paperId": "fd26c019d889b816c28fa2e15e2571faa78592bb",
            "title": "Counter-GAP: Counterfactual Bias Evaluation through Gendered Ambiguous Pronouns",
            "abstract": "Bias-measuring datasets play a critical role in detecting biased behavior of language models and in evaluating progress of bias mitigation methods. In this work, we focus on evaluating gender bias through coreference resolution, where previous datasets are either hand-crafted or fail to reliably measure an explicitly defined bias. To overcome these shortcomings, we propose a novel method to collect diverse, natural, and minimally distant text pairs via counterfactual generation, and construct Counter-GAP, an annotated dataset consisting of 4008 instances grouped into 1002 quadruples. We further identify a bias cancellation problem in previous group-level metrics on Counter-GAP, and propose to use the difference between inconsistency across genders and within genders to measure bias at a quadruple level. Our results show that four pre-trained language models are significantly more inconsistent across different gender groups than within each group, and that a name-based counterfactual data augmentation method is more effective to mitigate such bias than an anonymization-based method.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results show that four pre-trained language models are significantly more inconsistent across different gender groups than within each group, and that a name-based counterfactual data augmentation method is more effective to mitigate such bias than an anonymization-based method."
            },
            "score": 4
        },
        {
            "id": "6489640b1d30a8a3e7cb906bb6557f1ccd0d799d",
            "paperId": "6489640b1d30a8a3e7cb906bb6557f1ccd0d799d",
            "title": "Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models",
            "abstract": "Retrieval-augmented language models (RALMs) represent a substantial advancement in the capabilities of large language models, notably in reducing factual hallucination by leveraging external knowledge sources. However, the reliability of the retrieved information is not always guaranteed. The retrieval of irrelevant data can lead to misguided responses, and potentially causing the model to overlook its inherent knowledge, even when it possesses adequate information to address the query. Moreover, standard RALMs often struggle to assess whether they possess adequate knowledge, both intrinsic and retrieved, to provide an accurate answer. In situations where knowledge is lacking, these systems should ideally respond with\"unknown\"when the answer is unattainable. In response to these challenges, we introduces Chain-of-Noting (CoN), a novel approach aimed at improving the robustness of RALMs in facing noisy, irrelevant documents and in handling unknown scenarios. The core idea of CoN is to generate sequential reading notes for retrieved documents, enabling a thorough evaluation of their relevance to the given question and integrating this information to formulate the final answer. We employed ChatGPT to create training data for CoN, which was subsequently trained on an LLaMa-2 7B model. Our experiments across four open-domain QA benchmarks show that RALMs equipped with CoN significantly outperform standard RALMs. Notably, CoN achieves an average improvement of +7.9 in EM score given entirely noisy retrieved documents and +10.5 in rejection rates for real-time questions that fall outside the pre-training knowledge scope.",
            "year": 2023,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Chain-of-Noting (CoN) is introduced, a novel approach aimed at improving the robustness of RALMs in facing noisy, irrelevant documents and in handling unknown scenarios, and achieves an average improvement of +7.9 in EM score given entirely noisy retrieved documents and +10.5 in rejection rates for real-time questions that fall outside the pre-training knowledge scope."
            },
            "score": 4
        },
        {
            "id": "fcee1c19e12f3b7e3595aeba702416d055bdbc3f",
            "paperId": "fcee1c19e12f3b7e3595aeba702416d055bdbc3f",
            "title": "Knowledge Verification to Nip Hallucination in the Bud",
            "abstract": "While large language models (LLMs) have demonstrated exceptional performance across various tasks following human alignment, they may still generate responses that sound plausible but contradict factual knowledge, a phenomenon known as \\emph{hallucination}. In this paper, we demonstrate the feasibility of mitigating hallucinations by verifying and minimizing the inconsistency between external knowledge present in the alignment data and the intrinsic knowledge embedded within foundation LLMs. Specifically, we propose a novel approach called Knowledge Consistent Alignment (KCA), which employs a well-aligned LLM to automatically formulate assessments based on external knowledge to evaluate the knowledge boundaries of foundation LLMs. To address knowledge inconsistencies in the alignment data, KCA implements several specific strategies to deal with these data instances. We demonstrate the superior efficacy of KCA in reducing hallucinations across six benchmarks, utilizing foundation LLMs of varying backbones and scales. This confirms the effectiveness of mitigating hallucinations by reducing knowledge inconsistency. Our code, model weights, and data are openly accessible at \\url{https://github.com/fanqiwan/KCA}.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The superior efficacy of KCA is demonstrated in reducing hallucinations across six benchmarks, utilizing foundation LLMs of varying backbones and scales, which confirms the effectiveness of mitigating hallucinations by reducing knowledge inconsistency."
            },
            "score": 4
        },
        {
            "id": "f63fdbbdf9005245d960ac1912cf4d0805e274a8",
            "paperId": "f63fdbbdf9005245d960ac1912cf4d0805e274a8",
            "title": "Minimizing Factual Inconsistency and Hallucination in Large Language Models",
            "abstract": "Large Language Models (LLMs) are widely used in critical fields such as healthcare, education, and finance due to their remarkable proficiency in various language-related tasks. However, LLMs are prone to generating factually incorrect responses or\"hallucinations,\"which can lead to a loss of credibility and trust among users. To address this issue, we propose a multi-stage framework that generates the rationale first, verifies and refines incorrect ones, and uses them as supporting references to generate the answer. The generated rationale enhances the transparency of the answer and our framework provides insights into how the model arrived at this answer, by using this rationale and the references to the context. In this paper, we demonstrate its effectiveness in improving the quality of responses to drug-related inquiries in the life sciences industry. Our framework improves traditional Retrieval Augmented Generation (RAG) by enabling OpenAI GPT-3.5-turbo to be 14-25% more faithful and 16-22% more accurate on two datasets. Furthermore, fine-tuning samples based on our framework improves the accuracy of smaller open-access LLMs by 33-42% and competes with RAG on commercial models.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A multi-stage framework that generates the rationale first, verifies and refines incorrect ones, and uses them as supporting references to generate the answer enhances the transparency of the answer and provides insights into how the model arrived at this answer, by using this rationale and the references to the context."
            },
            "score": 4
        },
        {
            "id": "99bfe503743c5ec8e16e50ab8438159cdb533a89",
            "paperId": "99bfe503743c5ec8e16e50ab8438159cdb533a89",
            "title": "The Troubling Emergence of Hallucination in Large Language Models - An Extensive Definition, Quantification, and Prescriptive Remediations",
            "abstract": "The recent advancements in Large Language Models (LLMs) have garnered widespread acclaim for their remarkable emerging capabilities. However, the issue of hallucination has parallelly emerged as a by-product, posing significant concerns. While some recent endeavors have been made to identify and mitigate different types of hallucination, there has been a limited emphasis on the nuanced categorization of hallucination and associated mitigation methods. To address this gap, we offer a fine-grained discourse on profiling hallucination based on its degree, orientation, and category, along with offering strategies for alleviation. As such, we define two overarching orientations of hallucination: (i) factual mirage (FM) and (ii) silver lining (SL). To provide a more comprehensive understanding, both orientations are further sub-categorized into intrinsic and extrinsic, with three degrees of severity - (i) mild, (ii) moderate, and (iii) alarming. We also meticulously categorize hallucination into six types: (i) acronym ambiguity, (ii) numeric nuisance, (iii) generated golem, (iv) virtual voice, (v) geographic erratum, and (vi) time wrap. Furthermore, we curate HallucInation eLiciTation (HILT), a publicly available dataset comprising of 75,000 samples generated using 15 contemporary LLMs along with human annotations for the aforementioned categories. Finally, to establish a method for quantifying and to offer a comparative spectrum that allows us to evaluate and rank LLMs based on their vulnerability to producing hallucinations, we propose Hallucination Vulnerability Index (HVI). We firmly believe that HVI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making. In conclusion, we propose two solution strategies for mitigating hallucinations.",
            "year": 2023,
            "citationCount": 34,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work defines two overarching orientations of hallucination and proposes two solution strategies for mitigating hallucinations, and firmly believes that HVI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making."
            },
            "score": 4
        },
        {
            "id": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
            "paperId": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
            "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such\"society of minds\"approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.",
            "year": 2023,
            "citationCount": 206,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer is presented, indicating that this approach significantly enhances mathematical and strategic reasoning across a number of tasks."
            },
            "score": 4
        },
        {
            "id": "6c6ef8700507618b2851f428ef383d34ab99d7af",
            "paperId": "6c6ef8700507618b2851f428ef383d34ab99d7af",
            "title": "Bridging the Gap between Language Models and Cross-Lingual Sequence Labeling",
            "abstract": "Large-scale cross-lingual pre-trained language models (xPLMs) have shown effective in cross-lingual sequence labeling tasks (xSL), such as machine reading comprehension (xMRC) by transferring knowledge from a high-resource language to low-resource languages.Despite the great success, we draw an empirical observation that there is an training objective gap between pre-training and fine-tuning stages: e.g., mask language modeling objective requires local understanding of the masked token and the span-extraction objective requires understanding and reasoning of the global input passage/paragraph and question, leading to the discrepancy between pre-training and xMRC. In this paper, we first design a pre-training task tailored for xSL named Cross-lingual Language Informative Span Masking (CLISM) to eliminate the objective gap in a self-supervised manner. Second, we present ContrAstive-Consistency Regularization (CACR), which utilizes contrastive learning to encourage the consistency between representations of input parallel sequences via unsupervised cross-lingual instance-wise training signals during pre-training. By these means, our methods not only bridge the gap between pretrain-finetune, but also enhance PLMs to better capture the alignment between different languages. Extensive experiments prove that our method achieves clearly superior results on multiple xSL benchmarks with limited pre-training data. Our methods also surpass the previous state-of-the-art methods by a large margin in few-shot data setting, where only a few hundred training examples are available.",
            "year": 2022,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A pre-training task tailored for xSL named Cross-lingual Language Informative Span Masking (CLISM) is designed to eliminate the objective gap in a self-supervised manner and ContrAstive-Consistency Regularization (CACR) is presented, which utilizes contrastive learning to encourage the consistency between representations of input parallel sequences via unsupervised cross-lingUAL instance-wise training signals during pre- training."
            },
            "score": 4
        },
        {
            "id": "20f0f9562611dd3e482a3ce44a87874ad30dc6da",
            "paperId": "20f0f9562611dd3e482a3ce44a87874ad30dc6da",
            "title": "Improving Non-autoregressive Machine Translation with Error Exposure and Consistency Regularization",
            "abstract": "Being one of the IR-NAT (Iterative-refinemennt-based NAT) frameworks, the Conditional Masked Language Model (CMLM) adopts the mask-predict paradigm to re-predict the masked low-confidence tokens. However, CMLM suffers from the data distribution discrepancy between training and inference, where the observed tokens are generated differently in the two cases. In this paper, we address this problem with the training approaches of error exposure and consistency regularization (EECR). We construct the mixed sequences based on model prediction during training, and propose to optimize over the masked tokens under imperfect observation conditions. We also design a consistency learning method to constrain the data distribution for the masked tokens under different observing situations to narrow down the gap between training and inference. The experiments on five translation benchmarks obtains an average improvement of 0.68 and 0.40 BLEU scores compared to the base models, respectively, and our CMLMC-EECR achieves the best performance with a comparable translation quality with the Transformer. The experiments results demonstrate the effectiveness of our method.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper constructs the mixed sequences based on model prediction during training, and proposes to optimize over the masked tokens under imperfect observation conditions, and design a consistency learning method to constrain the data distribution under different observing situations to narrow down the gap between training and inference."
            },
            "score": 4
        },
        {
            "id": "d5a50700c8850559ed27adb9f59cbf5b3a210529",
            "paperId": "d5a50700c8850559ed27adb9f59cbf5b3a210529",
            "title": "Towards Boosting Many-to-Many Multilingual Machine Translation with Large Language Models",
            "abstract": "The training paradigm for machine translation has gradually shifted, from learning neural machine translation (NMT) models with extensive parallel corpora to instruction finetuning on multilingual large language models (LLMs) with high-quality translation pairs. In this paper, we focus on boosting many-to-many multilingual translation of LLMs with an emphasis on zero-shot translation directions. We demonstrate that prompt strategies adopted during finetuning are crucial to zero-shot translation and introduce a cross-lingual consistency regularization, XConST, to bridge the representation gap among different languages and improve zero-shot translation performance. XConST is not a new method, but a version of CrossConST (Gao et al., 2023a) adapted for translation instruction finetuning with LLMs. Experimental results on ALMA (Xu et al., 2023), Tower (Team, 2024), and LLaMA-2 (Touvron et al., 2023) show that our approach consistently improves translation performance. Our implementations are available at https://github.com/gpengzhi/CrossConST-LLM.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that prompt strategies adopted during finetuning are crucial to zero-shot translation and a cross-lingual consistency regularization is introduced, XConST, to bridge the representation gap among different languages and improve zero-shot translation performance."
            },
            "score": 4
        },
        {
            "id": "4ef4b2f4f57c845f2b5e0598f1a762b82f8b63e1",
            "paperId": "4ef4b2f4f57c845f2b5e0598f1a762b82f8b63e1",
            "title": "Layer-wise Regularized Dropout for Neural Language Models",
            "abstract": "Among the various pre-trained neural language models that are popular today, dropout is already an indispensable regularization technique. To solve the inconsistency between training and inference caused by the randomness of dropout, some studies use consistency training to regularize dropout at the output layer. In this paper, we propose a novel Layer-wise Regularized Dropout (LR-Drop), which is specially designed for Transformer-based Language models. Specifically, LR-Drop layer-wise regularizes each Transformer layer using the consistency training strategy. Each training sample passes through the two siamese sub-models sampled by dropout, and then LR-Drop forces the hidden states, multi-head attention matrices, and output distribution of the two siamese sub-models to be consistent. The proposed LR-Drop can be regarded as a\"self-distillation\"framework, in which each sub-model generated by dropout is the other's\"teacher\"model and\"student\"model. Through extensive experiments on 8 natural language understanding datasets, 6 neural machine translation datasets, and 1 abstractive summarization dataset (a total of 15 datasets), we show that LR-Drop achieves superior performances, including state-of-the-art results.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The proposed LR-Drop can be regarded as a\"self-distillation\"framework, in which each sub-model generated by dropout is the other's\"teacher\"model and\"student\"model, and achieves superior performances, including state-of-the-art results."
            },
            "score": 4
        },
        {
            "id": "ff5695b5a452629f67094e8004db410c26731fb1",
            "paperId": "ff5695b5a452629f67094e8004db410c26731fb1",
            "title": "Fidelity-Enriched Contrastive Search: Reconciling the Faithfulness-Diversity Trade-Off in Text Generation",
            "abstract": "In this paper, we address the hallucination problem commonly found in natural language generation tasks. Language models often generate fluent and convincing content but can lack consistency with the provided source, resulting in potential inaccuracies. We propose a new decoding method called Fidelity-Enriched Contrastive Search (FECS), which augments the contrastive search framework with context-aware regularization terms. FECS promotes tokens that are semantically similar to the provided source while penalizing repetitiveness in the generated text. We demonstrate its effectiveness across two tasks prone to hallucination: abstractive summarization and dialogue generation. Results show that FECS consistently enhances faithfulness across various language model sizes while maintaining output diversity comparable to well-performing decoding algorithms.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new decoding method called Fidelity-Enriched Contrastive Search (FECS), which augments the contrastive search framework with context-aware regularization terms, which consistently enhances faithfulness across various language model sizes while maintaining output diversity comparable to well-performing decoding algorithms."
            },
            "score": 4
        },
        {
            "id": "d2d16333a4b0dc7e3463b280b9945e5ee6c53396",
            "paperId": "d2d16333a4b0dc7e3463b280b9945e5ee6c53396",
            "title": "TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models",
            "abstract": "Factual consistency evaluation is often conducted using Natural Language Inference (NLI) models, yet these models exhibit limited success in evaluating summaries. Previous work improved such models with synthetic training data. However, the data is typically based on perturbed human-written summaries, which often differ in their characteristics from real model-generated summaries and have limited coverage of possible factual errors. Alternatively, large language models (LLMs) have recently shown promising results in directly evaluating generative tasks, but are too computationally expensive for practical use. Motivated by these limitations, we introduce TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM. Unlike prior work, TrueTeacher does not rely on human-written summaries, and is multilingual by nature. Experiments on the TRUE benchmark show that a student model trained using our data, substantially outperforms both the state-of-the-art model with similar capacity, and the LLM teacher. In a systematic study, we compare TrueTeacher to existing synthetic data generation methods and demonstrate its superiority and robustness to domain-shift. We also show that our method generalizes to multilingual scenarios. Lastly, we release our large scale synthetic dataset (1.4M examples), generated using TrueTeacher, and a checkpoint trained on this data.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM, which does not rely on human-written summaries, and is multilingual by nature."
            },
            "score": 4
        },
        {
            "id": "152d9a231c00d4495c9bc4a466f42165ce2e2164",
            "paperId": "152d9a231c00d4495c9bc4a466f42165ce2e2164",
            "title": "Evaluating Factual Consistency of Summaries with Large Language Models",
            "abstract": "Detecting factual errors in summaries has been an important and challenging subject in summarization research. Inspired by the emergent ability of large language models (LLMs), we explore evaluating factual consistency of summaries by directly prompting LLMs. We present a comprehensive empirical study to assess the ability of LLMs as factual consistency evaluators, which consists of (1) analyzing different LLMs such as the GPT model series and Flan-T5; (2) investigating a variety of prompting methods including vanilla prompting, chain-of-thought prompting, and a sentence-by-sentence prompting method to tackle long summaries; and (3) evaluating on diverse summaries generated by multiple summarization systems, ranging from pre-transformer methods to SOTA pretrained models. Our experiments demonstrate that prompting LLMs is able to outperform the previous best factuality systems in all settings, by up to 12.2 absolute points in terms of the binary classification accuracy on inconsistency detection.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A comprehensive empirical study to assess the ability of LLMs as factual consistency evaluators, which consists of analyzing different LLMs such as the GPT model series and Flan-T5 and investigating a variety of prompting methods including vanilla prompting, chain-of-thought prompting, and a sentence-by-sentence prompting method to tackle long summaries."
            },
            "score": 4
        },
        {
            "id": "064e2c1c14f5801bd7ea5fb65e2075993ff8861e",
            "paperId": "064e2c1c14f5801bd7ea5fb65e2075993ff8861e",
            "title": "WeCheck: Strong Factual Consistency Checker via Weakly Supervised Learning",
            "abstract": "A crucial issue of current text generation models is that they often uncontrollably generate text that is factually inconsistent with inputs.Due to lack of annotated data, existing factual consistency metrics usually train evaluation models on synthetic texts or directly transfer from other related tasks, such as question answering (QA) and natural language inference (NLI).Bias in synthetic text or upstream tasks makes them perform poorly on text actually generated by language models, especially for general evaluation for various tasks.To alleviate this problem, we propose a weakly supervised framework named WeCheck that is directly trained on actual generated samples from language models with weakly annotated labels.WeCheck first utilizes a generative model to infer the factual labels of generated samples by aggregating weak labels from multiple resources.Next, we train a simple noise-aware classification model as the target metric using the inferred weakly supervised information.Comprehensive experiments on various tasks demonstrate the strong performance of WeCheck, achieving an average absolute improvement of 3.3% on the TRUE benchmark over 11B state-of-the-art methods using only 435M parameters.Furthermore, it is up to 30 times faster than previous evaluation methods, greatly improving the accuracy and efficiency of factual consistency evaluation.",
            "year": 2022,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A weakly supervised framework named WeCheck is proposed that is directly trained on actual generated samples from language models with weakly annotated labels, greatly improving the accuracy and efficiency of factual consistency evaluation."
            },
            "score": 4
        },
        {
            "id": "f727f928e7e179307d8d4a1da2387393f2bd7915",
            "paperId": "f727f928e7e179307d8d4a1da2387393f2bd7915",
            "title": "Methods for Measuring, Updating, and Visualizing Factual Beliefs in Language Models",
            "abstract": "Language models can memorize a considerable amount of factual information during pretraining that can be elicited through prompting or finetuning models on tasks like question answering. In this paper, we discuss approaches to measuring model factual beliefs, updating incorrect factual beliefs in models, and visualizing graphical relationships between factual beliefs. Our main contributions include: (1) new metrics for evaluating belief-updating methods focusing on the logical consistency of beliefs, (2) a training objective for Sequential, Local, and Generalizing updates (SLAG) that improves the performance of existing hypernetwork approaches, and (3) the introduction of the belief graph, a new form of visualization for language models that shows relationships between stored model beliefs. Our experiments suggest that models show only limited consistency between factual beliefs, but update methods can both fix incorrect model beliefs and greatly improve their consistency. Although off-the-shelf optimizers are surprisingly strong belief-updating baselines, our learned optimizers can outperform them in more difficult settings than have been considered in past work.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experiments suggest that models show only limited consistency between factual beliefs, but update methods can both fix incorrect model beliefs and greatly improve their consistency, and off-the-shelf optimizers can outperform them in more difficult settings than have been considered in past work."
            },
            "score": 4
        },
        {
            "id": "663d743272e9ab04f54d9105a3c3a3f6e22dd1dd",
            "paperId": "663d743272e9ab04f54d9105a3c3a3f6e22dd1dd",
            "title": "FactKB: Generalizable Factuality Evaluation using Language Models Enhanced with Factual Knowledge",
            "abstract": "Evaluating the factual consistency of automatically generated summaries is essential for the progress and adoption of reliable summarization systems. Despite recent advances, existing factuality evaluation models are not robust, being especially prone to entity and relation errors in new domains. We propose FactKB, a simple new approach to factuality evaluation that is generalizable across domains, in particular with respect to entities and relations. FactKB is based on language models pretrained using facts extracted from external knowledge bases. We introduce three types of complementary factuality pretraining objectives based on direct entity facts, facts grounded in auxiliary knowledge about entities, and facts constructed compositionally through knowledge base walks. The resulting factuality evaluation model achieves state-of-the-art performance on two in-domain news summarization benchmarks as well as on three out-of-domain scientific literature datasets. Further analysis of FactKB shows improved ability to detect erroneous entities and relations in summaries and is robust and generalizable across domains.",
            "year": 2023,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "FactKB is a simple new approach to factuality evaluation that is generalizable across domains, in particular with respect to entities and relations and shows improved ability to detect erroneous entities and relation in summaries."
            },
            "score": 4
        },
        {
            "id": "bf7cbd37e6be38f4c8871d70e639153479717a2d",
            "paperId": "bf7cbd37e6be38f4c8871d70e639153479717a2d",
            "title": "Counterfactual Vision-and-Language Navigation via Adversarial Path Sampling",
            "abstract": "Vision-and-Language Navigation (VLN) is a task where agents must decide how to move through a 3D environment to reach a goal by grounding natural language instructions to the visual surroundings. One of the problems of the VLN task is data scarcity since it is difficult to collect enough navigation paths with human-annotated instructions for interactive environments. In this paper, we explore the use of counterfactual thinking as a human-inspired data augmentation method that results in robust models. Counterfactual thinking is a concept that describes the human propensity to create possible alternatives to life events that have already occurred. We propose an adversarial-driven counterfactual reasoning model that can consider effective conditions instead of low-quality augmented data. In particular, we present a model-agnostic adversarial path sampler (APS) that learns to sample challenging paths that force the navigator to improve based on the navigation performance. APS also serves to do pre-exploration of unseen environments to strengthen the model's ability to generalize. We evaluate the influence of APS on the performance of different VLN baseline models using the room-to-room dataset (R2R). The results show that the adversarial training process with our proposed APS benefits VLN models under both seen and unseen environments. And the pre-exploration process can further gain additional improvements under unseen environments.",
            "year": 2019,
            "citationCount": 40,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An adversarial-driven counterfactual reasoning model is proposed that can consider effective conditions instead of low-quality augmented data and is presented as a model-agnostic adversarial path sampler (APS) that learns to sample challenging paths that force the navigator to improve based on the navigation performance."
            },
            "score": 3
        },
        {
            "id": "696bc5ba0d023822bbee6b878a71ea2e4a4b0e5a",
            "paperId": "696bc5ba0d023822bbee6b878a71ea2e4a4b0e5a",
            "title": "N-Critics: Self-Refinement of Large Language Models with Ensemble of Critics",
            "abstract": "We propose a self-correction mechanism for Large Language Models (LLMs) to mitigate issues such as toxicity and fact hallucination. This method involves refining model outputs through an ensemble of critics and the model's own feedback. Drawing inspiration from human behavior, we explore whether LLMs can emulate the self-correction process observed in humans who often engage in self-reflection and seek input from others to refine their understanding of complex topics. Our approach is model-agnostic and can be applied across various domains to enhance trustworthiness by addressing fairness, bias, and robustness concerns. We consistently observe performance improvements in LLMs for reducing toxicity and correcting factual errors.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a self-correction mechanism for Large Language Models (LLMs) to mitigate issues such as toxicity and fact hallucination and consistently observe performance improvements in LLMs for reducing toxicity and correcting factual errors."
            },
            "score": 3
        },
        {
            "id": "afdeef9585232642d18e7c6a7942b2395e94ede1",
            "paperId": "afdeef9585232642d18e7c6a7942b2395e94ede1",
            "title": "A Cause-Effect Look at Alleviating Hallucination of Knowledge-grounded Dialogue Generation",
            "abstract": "Empowered by the large-scale pretrained language models, existing dialogue systems have demonstrated impressive performance conducting fluent and natural-sounding conversations. However, they are still plagued by the hallucination problem, causing unpredictable factual errors in the generated responses. Recently, knowledge-grounded dialogue generation models, that intentionally invoke external knowledge resources to more informative responses, are also proven to be effective in reducing hallucination. Following the idea of getting high-quality knowledge, a few efforts have achieved pretty good performance on this issue. As some inevitable knowledge noises may also lead to hallucinations, it is emergent to investigate the reason and future directions for building noise-tolerant methods in KGD tasks. In this paper, we analyze the causal story behind this problem with counterfactual reasoning methods. Based on the causal effect analysis, we propose a possible solution for alleviating the hallucination in KGD by exploiting the dialogue-knowledge interaction. Experimental results of our example implementation show that this method can reduce hallucination without disrupting other dialogue performance, while keeping adaptive to different generation models. We hope our efforts can support and call for more attention to developing lightweight techniques towards robust and trusty dialogue systems.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Based on the causal effect analysis, a possible solution for alleviating the hallucination in KGD by exploiting the dialogue-knowledge interaction is proposed and results show this method can reduce hallucination without disrupting other dialogue performance, while keeping adaptive to different generation models."
            },
            "score": 3
        },
        {
            "id": "f41977c497c96c1da2e9e945315e9be6d6ad472e",
            "paperId": "f41977c497c96c1da2e9e945315e9be6d6ad472e",
            "title": "Towards reducing hallucination in extracting information from financial reports using Large Language Models",
            "abstract": "For a financial analyst, the question and answer (Q\\&A) segment of the company financial report is a crucial piece of information for various analysis and investment decisions. However, extracting valuable insights from the Q\\&A section has posed considerable challenges as the conventional methods such as detailed reading and note-taking lack scalability and are susceptible to human errors, and Optical Character Recognition (OCR) and similar techniques encounter difficulties in accurately processing unstructured transcript text, often missing subtle linguistic nuances that drive investor decisions. Here, we demonstrate the utilization of Large Language Models (LLMs) to efficiently and rapidly extract information from earnings report transcripts while ensuring high accuracy transforming the extraction process as well as reducing hallucination by combining retrieval-augmented generation technique as well as metadata. We evaluate the outcomes of various LLMs with and without using our proposed approach based on various objective metrics for evaluating Q\\&A systems, and empirically demonstrate superiority of our method.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work demonstrates the utilization of Large Language Models (LLMs) to efficiently and rapidly extract information from earnings report transcripts while ensuring high accuracy transforming the extraction process as well as reducing hallucination by combining retrieval-augmented generation technique aswell as metadata."
            },
            "score": 3
        },
        {
            "id": "e7c97e953849f1a8e5d85ceb4cfcc0a5d54d2365",
            "paperId": "e7c97e953849f1a8e5d85ceb4cfcc0a5d54d2365",
            "title": "Enabling Large Language Models to Generate Text with Citations",
            "abstract": "Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. In this work, our aim is to allow LLMs to generate text with citations, improving their factual correctness and verifiability. Existing work mainly relies on commercial search engines and human evaluation, making it challenging to reproduce and compare different modeling approaches. We propose ALCE, the first benchmark for Automatic LLMs' Citation Evaluation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations. We develop automatic metrics along three dimensions -- fluency, correctness, and citation quality -- and demonstrate their strong correlation with human judgements. Our experiments with state-of-the-art LLMs and novel prompting strategies show that current systems have considerable room for improvement -- For example, on the ELI5 dataset, even the best models lack complete citation support 50% of the time. Our analyses further highlight promising future directions, including developing better retrievers, advancing long-context LLMs, and improving the ability to synthesize information from multiple sources.",
            "year": 2023,
            "citationCount": 102,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes ALCE, the first benchmark for Automatic LLMs' Citation Evaluation, and develops automatic metrics along three dimensions -- fluency, correctness, and citation quality -- and demonstrates their strong correlation with human judgements."
            },
            "score": 3
        },
        {
            "id": "06b2ac5153e3d8d05c13c82f93d7f4e13eee6d0f",
            "paperId": "06b2ac5153e3d8d05c13c82f93d7f4e13eee6d0f",
            "title": "Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites",
            "abstract": "Large language models (LLMs) have shown remarkable performance in natural language processing (NLP) tasks. To comprehend and execute diverse human instructions over image data, instruction-tuned large vision-language models (LVLMs) have been introduced. However, LVLMs may suffer from different types of object hallucinations. Nevertheless, LVLMs are evaluated for coarse-grained object hallucinations only (i.e., generated objects non-existent in the input image). The fine-grained object attributes and behaviors non-existent in the image may still be generated but not measured by the current evaluation methods. In this paper, we thus focus on reducing fine-grained hallucinations of LVLMs. We propose \\textit{ReCaption}, a framework that consists of two components: rewriting captions using ChatGPT and fine-tuning the instruction-tuned LVLMs on the rewritten captions. We also propose a fine-grained probing-based evaluation method named \\textit{Fine-Grained Object Hallucination Evaluation} (\\textit{FGHE}). Our experiment results demonstrate that ReCaption effectively reduces fine-grained object hallucination for different LVLM options and improves their text generation quality. The code can be found at https://github.com/Anonymousanoy/FOHE.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ReCaption is proposed, a framework that consists of two components: rewriting captions using ChatGPT and fine-tuning the instruction-tuned LVLMs on the rewritten captions, and a fine-grained probing-based evaluation method named \\textit{Fine-Grained Object Hallucination Evaluation} (FGHE)."
            },
            "score": 3
        },
        {
            "id": "13fd528a587196ff6429bfbe1d11d2f89a4036f5",
            "paperId": "13fd528a587196ff6429bfbe1d11d2f89a4036f5",
            "title": "User-Controlled Knowledge Fusion in Large Language Models: Balancing Creativity and Hallucination",
            "abstract": "In modern dialogue systems, the use of Large Language Models (LLMs) has grown exponentially due to their capacity to generate diverse, relevant, and creative responses. Despite their strengths, striking a balance between the LLMs' creativity and their faithfulness to external knowledge remains a key challenge. This paper presents an innovative user-controllable mechanism that modulates the balance between an LLM's imaginative capabilities and its adherence to factual information. Our approach incorporates a numerical tag during the fine-tuning phase of the LLM's training, representing the degree of faithfulness to the reference knowledge in the generated responses. This degree is computed through an automated process that measures lexical overlap using ROUGE scores, semantic similarity using Sentence-BERT embeddings, and an LLM's self-evaluation score. During model inference, users can manipulate this numerical tag, thus controlling the degree of the LLM's reliance on external knowledge. We conduct extensive experiments across various scenarios, demonstrating the adaptability of our method and its efficacy in ensuring the quality and accuracy of the LLM's responses. The results highlight the potential of our approach to enhance the versatility of LLMs while maintaining a balance between creativity and hallucination.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents an innovative user-controllable mechanism that modulates the balance between an LLM's imaginative capabilities and its adherence to factual information, and highlights the potential of this approach to enhance the versatility of LLMs while maintaining a balance between creativity and hallucination."
            },
            "score": 3
        },
        {
            "id": "fb24091f419a0daaf213653858a69ab05d0186aa",
            "paperId": "fb24091f419a0daaf213653858a69ab05d0186aa",
            "title": "Bridge the Gap between Language models and Tabular Understanding",
            "abstract": "Table pretrain-then-finetune paradigm has been proposed and employed at a rapid pace after the success of pre-training in the natural language domain. Despite the promising findings in tabular pre-trained language models (TPLMs), there is an input gap between pre-training and fine-tuning phases. For instance, TPLMs jointly pre-trained with table and text input could be effective for tasks also with table-text joint input like table question answering, but it may fail for tasks with only tables or text as input such as table retrieval. To this end, we propose UTP, an approach that dynamically supports three types of multi-modal inputs: table-text, table, and text. Specifically, UTP is pre-trained with two strategies: (1) We first utilize a universal mask language modeling objective on each kind of input, enforcing the model to adapt various inputs. (2) We then present Cross-Modal Contrastive Regularization (CMCR), which utilizes contrastive learning to encourage the consistency between table-text cross-modality representations via unsupervised instance-wise training signals during pre-training. By these means, the resulting model not only bridges the input gap between pre-training and fine-tuning but also advances in the alignment of table and text. Extensive results show UTP achieves superior results on uni-modal input tasks (e.g., table retrieval) and cross-modal input tasks (e.g., table question answering).",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "UTP is pre-trained with two strategies: a universal mask language modeling objective on each kind of input, enforcing the model to adapt various inputs, and Cross-Modal Contrastive Regularization (CMCR), which utilizes contrastive learning to encourage the consistency between table-text cross-modality representations via unsupervised instance-wise training signals during pre-training."
            },
            "score": 3
        },
        {
            "id": "ff3efb5bf333850f69282b15417a4606165e70ae",
            "paperId": "ff3efb5bf333850f69282b15417a4606165e70ae",
            "title": "Beyond Lexical Consistency: Preserving Semantic Consistency for Program Translation",
            "abstract": "Program translation aims to convert the input programs from one programming language to another. Automatic program translation is a prized target of software engineering research, which leverages the reusability of projects and improves the efficiency of development. Recently, thanks to the rapid development of deep learning model architectures and the availability of large-scale parallel corpus of programs, the performance of program translation has been greatly improved. However, the existing program translation models are still far from satisfactory, in terms of the quality of translated programs. In this paper, we argue that a major limitation of the current approaches is the lack of consideration of semantic consistency. Beyond lexical consistency, semantic consistency is also critical for the task. To make the program translation model more semantically aware, we propose a general framework named Preserving Semantic Consistency for Program Translation (PSCPT), which considers semantic consistency with regularization in the training objective of program translation and can be easily applied to all encoder-decoder methods with various neural networks (e.g., LSTM, Transformer) as the backbone. We conduct extensive experiments in 7 general programming languages. Experimental results show that with CodeBERT as the backbone, our approach outperforms not only the state-of-the-art open-source models but also the commercial closed large language models (e.g., textdavinci-002, text-davinci-003) on the program translation task. Our replication package (including code, data, etc.) is publicly available at https://github.com/duyali2000/PSCPT.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a general framework named Preserving Semantic Consistency for Program Translation (PSCPT), which considers semantic consistency with regularization in the training objective of program translation and can be easily applied to all encoder-decoder methods with various neural networks (e.g., LSTM, Transformer) as the backbone."
            },
            "score": 3
        },
        {
            "id": "372a2ed7e32790419f079f63dd7fc29fa4c61287",
            "paperId": "372a2ed7e32790419f079f63dd7fc29fa4c61287",
            "title": "Domain Informed - a better approach to regularization and semi-supervised learning for seismic event analysis.",
            "abstract": "Typically, data-driven learning works best when we can exploit expectations from our data domain. For example, the development of recurrent neural network architectures to deal with the temporal dependence in language, geometric deep learning for 3-D problems, and physics-constrained Bayesian learning for more interpretable dependencies. Yet it can be unclear how to interject expectations, and which specific expectations will result in better outcomes for a given domain. In seismic event processing, enforcing consistency over disparate observations for an individual event has a long history of empirical value. For example, we almost always use magnitude estimates from many individual stations, drop outliers, and average to arrive at a final event magnitude. Similarly, we can leverage the expectation that stations provide consistent predictions for any event-level attributes, such as event type, when we develop deep learning based predictive models. In this work we show how to formulate this expectation as a loss term during model training and give several examples of how this expectation can result in better model regularization, which can reduce overfitting while still outperforming other methods, give us more trustworthy decision confidence, and allows us to leverage data where no ground truth is available.",
            "year": 2020,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work shows how to formulate the expectation that stations provide consistent predictions for any event-level attributes as a loss term during model training and gives several examples of how this expectation can result in better model regularization, which can reduce overfitting while still outperforming other methods, give us more trustworthy decision confidence, and allows us to leverage data where no ground truth is available."
            },
            "score": 3
        },
        {
            "id": "2846610bcfb205735988f4dff92475339a817e8c",
            "paperId": "2846610bcfb205735988f4dff92475339a817e8c",
            "title": "Duality Regularization for Unsupervised Bilingual Lexicon Induction",
            "abstract": "Unsupervised bilingual lexicon induction naturally exhibits duality, which results from symmetry in back-translation. For example, EN-IT and IT-EN induction can be mutually primal and dual problems. Current state-of-the-art methods, however, consider the two tasks independently. In this paper, we propose to train primal and dual models jointly, using regularizers to encourage consistency in back translation cycles. Experiments across 6 language pairs show that the proposed method significantly outperforms competitive baselines, obtaining the best-published results on a standard benchmark.",
            "year": 2019,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes to train primal and dual models jointly, using regularizers to encourage consistency in back translation cycles, and shows that the proposed method significantly outperforms competitive baseline results on a standard benchmark."
            },
            "score": 3
        },
        {
            "id": "e013bb20b42d23d5cf6ff90203a8adc8194ed68a",
            "paperId": "e013bb20b42d23d5cf6ff90203a8adc8194ed68a",
            "title": "On Compositionality and Improved Training of NADO",
            "abstract": "NeurAlly-Decomposed Oracle (NADO) is a powerful approach for controllable generation with large language models. Differentiating from finetuning/prompt tuning, it has the potential to avoid catastrophic forgetting of the large base model and achieve guaranteed convergence to an entropy-maximized closed-form solution without significantly limiting the model capacity. Despite its success, several challenges arise when applying NADO to more complex scenarios. First, the best practice of using NADO for the composition of multiple control signals is under-explored. Second, vanilla NADO suffers from gradient vanishing for low-probability control signals and is highly reliant on the forward-consistency regularization. In this paper, we study the aforementioned challenges when using NADO theoretically and empirically. We show we can achieve guaranteed compositional generalization of NADO with a certain practice, and propose a novel alternative parameterization of NADO to perfectly guarantee the forward-consistency. We evaluate the improved training of NADO, i.e. NADO++, on CommonGen. Results show that NADO++ improves the effectiveness of the algorithm in multiple aspects.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that NADO++ improves the effectiveness of the algorithm in multiple aspects, and a novel alternative parameterization of NADO is proposed to perfectly guarantee the forward-consistency."
            },
            "score": 3
        },
        {
            "id": "d20f8eb6d56d7d73fdfb255b9ea462b806d1813e",
            "paperId": "d20f8eb6d56d7d73fdfb255b9ea462b806d1813e",
            "title": "Text Style Transferring via Adversarial Masking and Styled Filling",
            "abstract": "Text style transfer is an important task in natural language processing with broad applications. Existing models following the masking and filling scheme suffer two challenges: the word masking procedure may mistakenly remove unexpected words and the selected words in the word filling procedure may lack diversity and semantic consistency. To tackle both challenges, in this study, we propose a style transfer model, with an adversarial masking approach and a styled filling technique (AMSF). Specifically, AMSF first trains a mask predictor by adversarial training without manual configuration. Then two additional losses, i.e. an entropy maximization loss and a consistency regularization loss, are introduced in training the word filling module to guarantee the diversity and semantic consistency of the transferred texts. Experimental results and analysis on two benchmark text style transfer data sets demonstrate the effectiveness of the proposed approaches.",
            "year": 2022,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A style transfer model, with an adversarial masking approach and a styled filling technique (AMSF), which first trains a mask predictor by adversarial training without manual configuration and two additional losses are introduced in training the word filling module to guarantee the diversity and semantic consistency of the transferred texts."
            },
            "score": 3
        },
        {
            "id": "f70bf522a90c09ed06c32c9bf36b7ee14b8a9856",
            "paperId": "f70bf522a90c09ed06c32c9bf36b7ee14b8a9856",
            "title": "Knowledge Graph-Augmented Language Models for Knowledge-Grounded Dialogue Generation",
            "abstract": "Language models have achieved impressive performances on dialogue generation tasks. However, when generating responses for a conversation that requires factual knowledge, they are far from perfect, due to an absence of mechanisms to retrieve, encode, and reflect the knowledge in the generated responses. Some knowledge-grounded dialogue generation methods tackle this problem by leveraging facts from Knowledge Graphs (KGs); however, they do not guarantee that the model utilizes a relevant piece of knowledge from the KG. To overcome this limitation, we propose SUbgraph Retrieval-augmented GEneration (SURGE), a framework for generating context-relevant and knowledge-grounded dialogues with the KG. Specifically, our SURGE framework first retrieves the relevant subgraph from the KG, and then enforces consistency across facts by perturbing their word embeddings conditioned by the retrieved subgraph. Then, we utilize contrastive learning to ensure that the generated texts have high similarity to the retrieved subgraphs. We validate our SURGE framework on OpendialKG and KOMODIS datasets, showing that it generates high-quality dialogues that faithfully reflect the knowledge from KG.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The proposed SUbgraph Retrieval-augmented GEneration (SURGE), a framework for generating context-relevant and knowledge-grounded dialogues with the KG, first retrieves the relevant subgraph from the KGs, and then enforces consistency across facts by perturbing their word embeddings conditioned by the retrieved subgraph."
            },
            "score": 3
        },
        {
            "id": "468d1e2d75a23fecaf96fe65d8b01ff35ea5d0bd",
            "paperId": "468d1e2d75a23fecaf96fe65d8b01ff35ea5d0bd",
            "title": "Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models",
            "abstract": "Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts inserted in English reveal a clear pattern whereby the new piece of knowledge transfers only to languages with which English has a high RankC score.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy, and conducts an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level."
            },
            "score": 3
        },
        {
            "id": "36870ac7332b45edf59e5c111dc5bd24139dc1ce",
            "paperId": "36870ac7332b45edf59e5c111dc5bd24139dc1ce",
            "title": "Factual Consistency of Multilingual Pretrained Language Models",
            "abstract": "Pretrained language models can be queried for factual knowledge, with potential applications in knowledge base acquisition and tasks that require inference. However, for that, we need to know how reliable this knowledge is, and recent work has shown that monolingual English language models lack consistency when predicting factual knowledge, that is, they fill-in-the-blank differently for paraphrases describing the same fact. In this paper, we extend the analysis of consistency to a multilingual setting. We introduce a resource, mParaRel, and investigate (i) whether multilingual language models such as mBERT and XLM-R are more consistent than their monolingual counterparts;and (ii) if such models are equally consistent across languages.We find that mBERT is as inconsistent as English BERT in English paraphrases, but that both mBERT and XLM-R exhibit a high degree of inconsistency in English and even more so for all the other 45 languages.",
            "year": 2022,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "MBERT is as inconsistent as English BERT in English paraphrases, but that both mBERT and XLM-R exhibit a high degree of inconsistency in English and even more so for all the other 45 languages."
            },
            "score": 3
        },
        {
            "id": "f5d581e916613838cbadc05ab8c35ee4ea78da32",
            "paperId": "f5d581e916613838cbadc05ab8c35ee4ea78da32",
            "title": "Fine-grained Factual Consistency Assessment for Abstractive Summarization Models",
            "abstract": "Factual inconsistencies existed in the output of abstractive summarization models with original documents are frequently presented. Fact consistency assessment requires the reasoning capability to find subtle clues to identify whether a model-generated summary is consistent with the original document. This paper proposes a fine-grained two-stage Fact Consistency assessment framework for Summarization models (SumFC). Given a document and a summary sentence, in the first stage, SumFC selects the top-K most relevant sentences with the summary sentence from the document. In the second stage, the model performs fine-grained consistency reasoning at the sentence level, and then aggregates all sentences\u2019 consistency scores to obtain the final assessment result. We get the training data pairs by data synthesis and adopt contrastive loss of data pairs to help the model identify subtle cues. Experiment results show that SumFC has made a significant improvement over the previous state-of-the-art methods. Our experiments also indicate that SumFC distinguishes detailed differences better.",
            "year": 2021,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A fine-grained two-stage Fact Consistency assessment framework for Summarization models (SumFC), which selects the top-K most relevant sentences with the summary sentence from the document and aggregates all sentences\u2019 consistency scores to obtain the final assessment result."
            },
            "score": 3
        },
        {
            "id": "3592b80bc4383d51b3d83817aeaa33ac781de50b",
            "paperId": "3592b80bc4383d51b3d83817aeaa33ac781de50b",
            "title": "\u201cNot All Information is Created Equal\u201d: Leveraging Metadata for Enhanced Knowledge Curation",
            "abstract": "We study the problem of using source metadata, such as timestamps, to enhance retrieval-augmented generation (RAG) in a knowledge curation system. The RAG paradigm allows language models to ground in external sources, thus reducing factual hallucination. However, it is still challenging for the models to synthesize different sources in an organized and accurate way due to differing priorities among information pieces and inherent connections. This project takes the first step to combat this by focusing on solving time confusion issues. We propose META ( M etadata E xtractor and T ext A ugmentation for Synthesis), a plug-in-and-play module designed to enhance RAG-based systems. META automatically extracts meta-information from sources, employing this data to inform and refine content generation. Experiments on FreshWiki Dataset show that incorporating META with the base knowledge curation system reduces temporal inaccuracies in articles on time-sensitive topics.",
            "year": null,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "META is proposed, a plug-in-and-play module designed to enhance RAG-based systems that automatically extracts meta-information from sources, employing this data to inform and refine content generation."
            },
            "score": 2
        },
        {
            "id": "f03d76430e730878ecd50cfd7b38f97b16b10fea",
            "paperId": "f03d76430e730878ecd50cfd7b38f97b16b10fea",
            "title": "Towards Label-free Scene Understanding by Vision Foundation Models",
            "abstract": "Vision foundation models such as Contrastive Vision-Language Pre-training (CLIP) and Segment Anything (SAM) have demonstrated impressive zero-shot performance on image classification and segmentation tasks. However, the incorporation of CLIP and SAM for label-free scene understanding has yet to be explored. In this paper, we investigate the potential of vision foundation models in enabling networks to comprehend 2D and 3D worlds without labelled data. The primary challenge lies in effectively supervising networks under extremely noisy pseudo labels, which are generated by CLIP and further exacerbated during the propagation from the 2D to the 3D domain. To tackle these challenges, we propose a novel Cross-modality Noisy Supervision (CNS) method that leverages the strengths of CLIP and SAM to supervise 2D and 3D networks simultaneously. In particular, we introduce a prediction consistency regularization to co-train 2D and 3D networks, then further impose the networks' latent space consistency using the SAM's robust feature representation. Experiments conducted on diverse indoor and outdoor datasets demonstrate the superior performance of our method in understanding 2D and 3D open environments. Our 2D and 3D network achieves label-free semantic segmentation with 28.4\\% and 33.5\\% mIoU on ScanNet, improving 4.7\\% and 7.9\\%, respectively. For nuImages and nuScenes datasets, the performance is 22.1\\% and 26.8\\% with improvements of 3.5\\% and 6.0\\%, respectively. Code is available. (https://github.com/runnanchen/Label-Free-Scene-Understanding).",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel Cross-modality Noisy Supervision (CNS) method that leverages the strengths of CLIP and SAM to supervise 2D and 3D networks simultaneously, and introduces a prediction consistency regularization to co-train 2Dand3D networks, then further impose the networks' latent space consistency using the SAM's robust feature representation."
            },
            "score": 2
        },
        {
            "id": "0c17326565266c40a02b230fac3b405a4d3220b9",
            "paperId": "0c17326565266c40a02b230fac3b405a4d3220b9",
            "title": "CLIP2Scene: Towards Label-efficient 3D Scene Understanding by CLIP",
            "abstract": "Contrastive Language-Image Pre-training (CLIP) achieves promising results in 2D zero-shot and few-shot learning. Despite the impressive performance in 2D, applying CLIP to help the learning in 3D scene understanding has yet to be explored. In this paper, we make the first attempt to investigate how CLIP knowledge benefits 3D scene understanding. We propose CLIP2Scene, a simple yet effective framework that transfers CLIP knowledge from 2D image-text pre-trained models to a 3D point cloud network. We show that the pre-trained 3D network yields impressive performance on various downstream tasks, i.e., annotation-free and fine-tuning with labelled data for semantic segmentation. Specifically, built upon CLIP, we design a Semantic-driven Cross-modal Contrastive Learning framework that pre-trains a 3D network via semantic and spatial-temporal consistency regularization. For the former, we first leverage CLIP's text semantics to select the positive and negative point samples and then employ the contrastive loss to train the 3D network. In terms of the latter, we force the consistency between the temporally coherent point cloud features and their corresponding image features. We conduct experiments on SemanticKITTI, nuScenes, and ScanNet. For the first time, our pre-trained network achieves annotation-free 3D semantic segmentation with 20.8% and 25.08% mIoU on nuScenes and ScanNet, respectively. When fine-tuned with 1% or 100% labelled data, our method significantly outperforms other self-supervised methods, with improvements of 8% and 1% mIoU, respectively. Furthermore, we demonstrate the generalizability for handling cross-domain datasets. Code is publicly available11https://github.com/runnanchen/CLIP2Scene..",
            "year": 2023,
            "citationCount": 50,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Semantic-driven Cross-modal Contrastive Learning framework that pre-trains a 3D network via semantic and spatial-temporal consistency regularization, which significantly outperforms other self-supervised methods and demonstrates the generalizability for handling cross-domain datasets."
            },
            "score": 2
        },
        {
            "id": "1afbf37912f2ea134eb67b710896b81fcd41ac2e",
            "paperId": "1afbf37912f2ea134eb67b710896b81fcd41ac2e",
            "title": "Clustering Aspect-related Phrases by Leveraging Sentiment Distribution Consistency",
            "abstract": "Clustering aspect-related phrases in terms of product\u2019s property is a precursor process to aspect-level sentiment analysis which is a central task in sentiment analysis. Most of existing methods for addressing this problem are context-based models which assume that domain synonymous phrases share similar co-occurrence contexts. In this paper, we explore a novel idea, sentiment distribution consistency, which states that different phrases (e.g. \u201cprice\u201d, \u201cmoney\u201d, \u201cworth\u201d, and \u201ccost\u201d) of the same aspect tend to have consistent sentiment distribution. Through formalizing sentiment distribution consistency as soft constraint, we propose a novel unsupervised model in the framework of Posterior Regularization (PR) to cluster aspectrelated phrases. Experiments demonstrate that our approach outperforms baselines remarkably.",
            "year": 2014,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Through formalizing sentiment distribution consistency as soft constraint, a novel unsupervised model in the framework of Posterior Regularization (PR) to cluster aspectrelated phrases is proposed and demonstrates that it outperforms baselines remarkably."
            },
            "score": 2
        },
        {
            "id": "36d6d614c7b36144c59e3892a6812b2a68af2c93",
            "paperId": "36d6d614c7b36144c59e3892a6812b2a68af2c93",
            "title": "Retrofitting Light-weight Language Models for Emotions using Supervised Contrastive Learning",
            "abstract": "We present a novel retrofitting method to induce emotion aspects into pre-trained language models (PLMs) such as BERT and RoBERTa. Our method updates pre-trained network weights using contrastive learning so that the text fragments exhibiting similar emotions are encoded nearby in the representation space, and the fragments with different emotion content are pushed apart. While doing so, it also ensures that the linguistic knowledge already present in PLMs is not inadvertently perturbed. The language models retrofitted by our method, i.e., BERTEmo and RoBERTaEmo, produce emotion-aware text representations, as evaluated through different clustering and retrieval metrics. For the downstream tasks on sentiment analysis and sarcasm detection, they perform better than their pre-trained counterparts (about 1% improvement in F1-score) and other existing approaches. Additionally, a more significant boost in performance is observed for the retrofitted models over pre-trained ones in few-shot learning setting.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel retrofitting method to induce emotion aspects into pre-trained language models (PLMs) such as BERT and RoBERTa using contrastive learning, which produces emotion-aware text representations as evaluated through different clustering and retrieval metrics."
            },
            "score": 2
        },
        {
            "id": "0907f2fe9d8305976a539877ef2b48b553e4fb63",
            "paperId": "0907f2fe9d8305976a539877ef2b48b553e4fb63",
            "title": "Explanation Graph Generation via Pre-trained Language Models: An Empirical Study with Contrastive Learning",
            "abstract": "Pre-trained sequence-to-sequence language models have led to widespread success in many natural language generation tasks. However, there has been relatively less work on analyzing their ability to generate structured outputs such as graphs. Unlike natural language, graphs have distinct structural and semantic properties in the context of a downstream NLP task, e.g., generating a graph that is connected and acyclic can be attributed to its structural constraints, while the semantics of a graph can refer to how meaningfully an edge represents the relation between two node concepts. In this work, we study pre-trained language models that generate explanation graphs in an end-to-end manner and analyze their ability to learn the structural constraints and semantics of such graphs. We first show that with limited supervision, pre-trained language models often generate graphs that either violate these constraints or are semantically incoherent. Since curating large amount of human-annotated graphs is expensive and tedious, we propose simple yet effective ways of graph perturbations via node and edge edit operations that lead to structurally and semantically positive and negative graphs. Next, we leverage these graphs in different contrastive learning models with Max-Margin and InfoNCE losses. Our methods lead to significant improvements in both structural and semantic accuracy of explanation graphs and also generalize to other similar graph generation tasks. Lastly, we show that human errors are the best negatives for contrastive learning and also that automatically generating more such human-like negative graphs can lead to further improvements.",
            "year": 2022,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work studies pre-trained language models that generate explanation graphs in an end-to-end manner and analyzes their ability to learn the structural constraints and semantics of such graphs and proposes simple yet effective ways of graph perturbations via node and edge edit operations that lead to structurally and semantically positive and negative graphs."
            },
            "score": 2
        },
        {
            "id": "f6572fccea70e5023cc38cb33ebd0f520d7b22e2",
            "paperId": "f6572fccea70e5023cc38cb33ebd0f520d7b22e2",
            "title": "ContraBERT: Enhancing Code Pre-trained Models via Contrastive Learning",
            "abstract": "Large-scale pre-trained models such as CodeBERT, GraphCodeBERT have earned widespread attention from both academia and industry. Attributed to the superior ability in code representation, they have been further applied in multiple downstream tasks such as clone detection, code search and code translation. However, it is also observed that these state-of-the-art pre-trained models are susceptible to adversarial attacks. The performance of these pre-trained models drops significantly with simple perturbations such as renaming variable names. This weakness may be inherited by their downstream models and thereby amplified at an unprecedented scale. To this end, we propose an approach namely ContraBERT that aims to improve the robustness of pre-trained models via contrastive learning. Specifically, we design nine kinds of simple and complex data augmentation operators on the programming language (PL) and natural language (NL) data to construct different variants. Furthermore, we continue to train the existing pre-trained models by masked language modeling (MLM) and contrastive pre-training task on the original samples with their augmented variants to enhance the robustness of the model. The extensive ex-periments demonstrate that ContraBERT can effectively improve the robustness of the existing pre-trained models. Further study also confirms that these robustness-enhanced models provide improvements as compared to original models over four popular downstream tasks.",
            "year": 2023,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes an approach namely ContraBERT that aims to improve the robustness of pre-trained models via contrastive learning and designs nine kinds of simple and complex data augmentation operators on the programming language (PL) and natural language (NL) data to construct different variants."
            },
            "score": 1
        },
        {
            "id": "af861b96ccdbae9b302f949c05ec2bfe8db13453",
            "paperId": "af861b96ccdbae9b302f949c05ec2bfe8db13453",
            "title": "RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding",
            "abstract": "We propose a lightweight and scalable Regional Point-Language Contrastive learning framework, namely \\textbf{RegionPLC}, for open-world 3D scene understanding, aiming to identify and recognize open-set objects and categories. Specifically, based on our empirical studies, we introduce a 3D-aware SFusion strategy that fuses 3D vision-language pairs derived from multiple 2D foundation models, yielding high-quality, dense region-level language descriptions without human 3D annotations. Subsequently, we devise a region-aware point-discriminative contrastive learning objective to enable robust and effective 3D learning from dense regional language supervision. We carry out extensive experiments on ScanNet, ScanNet200, and nuScenes datasets, and our model outperforms prior 3D open-world scene understanding approaches by an average of 17.2\\% and 9.1\\% for semantic and instance segmentation, respectively, while maintaining greater scalability and lower resource demands. Furthermore, our method has the flexibility to be effortlessly integrated with language models to enable open-ended grounded 3D reasoning without extra task-specific training. Code will be released.",
            "year": 2023,
            "citationCount": 21,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A 3D-aware SFusion strategy that fuses 3D vision-language pairs derived from multiple 2D foundation models, yielding high-quality, dense region-level language descriptions without human 3D annotations to enable robust and effective 3D learning from dense regional language supervision is introduced."
            },
            "score": 1
        }
    ],
    "novelty": "no"
}