{
    "topic_description": "novel prompting methods that can improve factuality and reduce hallucination of large language models",
    "idea_name": "Counterfactual Reasoning Prompting",
    "raw_idea": {
        "Problem": "Large language models often struggle to reason about counterfactual scenarios, leading to hallucinations and inconsistencies when generating responses that require considering alternative possibilities or outcomes.",
        "Existing Methods": "Current methods for counterfactual reasoning in LLMs include fine-tuning on counterfactual datasets or using prompts that encourage considering alternative scenarios. However, these approaches often lack a systematic way to guide the model's reasoning process.",
        "Motivation": "Counterfactual reasoning is a crucial aspect of human cognition, allowing us to consider alternative possibilities and make better decisions. By explicitly prompting LLMs to engage in counterfactual reasoning, we can improve their ability to generate more factual and consistent responses.",
        "Proposed Method": "We propose Counterfactual Reasoning Prompting (CRP), a multi-step prompting approach that guides LLMs through a structured counterfactual reasoning process. The steps include: 1) Identifying key factors in the given scenario; 2) Generating counterfactual scenarios by systematically varying these factors; 3) Reasoning about the consequences of each counterfactual scenario; 4) Comparing the outcomes of the counterfactual scenarios with the original scenario; 5) Generating a final response that incorporates insights from the counterfactual reasoning process.",
        "Experiment Plan": "Evaluate CRP on counterfactual reasoning benchmarks such as WIQA and COSMOS. Compare performance with baselines such as zero-shot prompting and fine-tuning on counterfactual datasets. Assess the factuality and consistency of generated responses using metrics like FactCC and human evaluation."
    },
    "full_experiment_plan": {
        "Title": "Counterfactual Reasoning Prompting: Improving Language Models' Ability to Consider Alternative Possibilities",
        "Problem Statement": "Large language models often struggle to reason about counterfactual scenarios, leading to hallucinations and inconsistencies when generating responses that require considering alternative possibilities or outcomes.",
        "Motivation": "Current methods for counterfactual reasoning in LLMs, such as fine-tuning on counterfactual datasets or using prompts that encourage considering alternative scenarios, often lack a systematic way to guide the model's reasoning process. Counterfactual reasoning is a crucial aspect of human cognition, allowing us to consider alternative possibilities and make better decisions. By explicitly prompting LLMs to engage in a structured counterfactual reasoning process, we can improve their ability to generate more factual and consistent responses.",
        "Proposed Method": "We propose Counterfactual Reasoning Prompting (CRP), a multi-step prompting approach that guides LLMs through a structured counterfactual reasoning process. The steps include: 1) Identifying key factors in the given scenario; 2) Generating counterfactual scenarios by systematically varying these factors; 3) Reasoning about the consequences of each counterfactual scenario; 4) Comparing the outcomes of the counterfactual scenarios with the original scenario; 5) Generating a final response that incorporates insights from the counterfactual reasoning process.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "Evaluate CRP on counterfactual reasoning benchmarks such as WIQA and COSMOS. These datasets contain questions that require reasoning about the effects of perturbations to a given scenario.",
            "Step 2: Construct Prompts": "For the baseline, use zero-shot prompting by directly asking the question. For the proposed CRP method, construct a multi-step prompt that guides the model through the counterfactual reasoning process. The prompt should include instructions for each step: 1) Identify key factors; 2) Generate counterfactual scenarios; 3) Reason about consequences; 4) Compare outcomes; 5) Generate final response. Use few-shot examples to demonstrate the desired output format for each step.",
            "Step 3: Select Models": "Evaluate the performance of GPT-3.5 (text-davinci-003) and GPT-4 models from the OpenAI API on the selected datasets using both the baseline and CRP prompts.",
            "Step 4: Get Results": "Generate model outputs for each question in the datasets using both the baseline and CRP prompts. For CRP, generate intermediate outputs for each step of the reasoning process.",
            "Step 5: Analyze Results": "Compare the performance of the baseline and CRP prompts using metrics such as accuracy and consistency. Assess the factuality of the generated responses using metrics like FactCC or human evaluation. Analyze the intermediate outputs of the CRP process to understand how the model's reasoning evolves across the steps."
        },
        "Test Case Examples": {
            "Baseline Prompt Input": "Question: If the sun was not shining, what would happen to the temperature?\nAnswer:",
            "Baseline Prompt Expected Output": "If the sun was not shining, the temperature would drop significantly. The Earth would become much colder, as the sun is the primary source of heat and energy for our planet.",
            "Proposed Prompt Input (CRP Step 1: Identify Key Factors)": "Question: If the sun was not shining, what would happen to the temperature?\nStep 1: Identify the key factors that influence the temperature in this scenario.",
            "Proposed Prompt Expected Output (CRP Step 1: Identify Key Factors)": "The key factors that influence the temperature in this scenario are:\n1. The presence or absence of sunlight\n2. The Earth's atmosphere\n3. The Earth's surface properties (e.g., land, water, ice)",
            "Proposed Prompt Input (CRP Step 2: Generate Counterfactual Scenarios)": "Question: If the sun was not shining, what would happen to the temperature?\nStep 2: Generate counterfactual scenarios by varying the key factors identified in Step 1.",
            "Proposed Prompt Expected Output (CRP Step 2: Generate Counterfactual Scenarios)": "Counterfactual Scenario 1: The sun is not shining, but the Earth's atmosphere remains intact.\nCounterfactual Scenario 2: The sun is not shining, and the Earth's atmosphere is absent.\nCounterfactual Scenario 3: The sun is not shining, and the Earth's surface is mostly covered by ice.",
            "Proposed Prompt Input (CRP Step 3: Reason about Consequences)": "Question: If the sun was not shining, what would happen to the temperature?\nStep 3: Reason about the consequences of each counterfactual scenario generated in Step 2.",
            "Proposed Prompt Expected Output (CRP Step 3: Reason about Consequences)": "Consequences of Counterfactual Scenario 1: The temperature would drop significantly, but the atmosphere would still retain some heat and provide insulation.\nConsequences of Counterfactual Scenario 2: The temperature would drop drastically, as there would be no atmosphere to trap heat. The Earth would become extremely cold.\nConsequences of Counterfactual Scenario 3: The temperature would drop even more than in Scenario 1, as ice reflects more sunlight and has a lower heat capacity than land or water.",
            "Proposed Prompt Input (CRP Step 4: Compare Outcomes)": "Question: If the sun was not shining, what would happen to the temperature?\nStep 4: Compare the outcomes of the counterfactual scenarios with the original scenario.",
            "Proposed Prompt Expected Output (CRP Step 4: Compare Outcomes)": "In the original scenario, the sun is shining, and the Earth's temperature is maintained at a habitable level. In all the counterfactual scenarios, where the sun is not shining, the temperature drops significantly. The severity of the temperature drop depends on the state of the Earth's atmosphere and surface properties. The absence of the atmosphere (Scenario 2) or the presence of extensive ice cover (Scenario 3) would lead to even colder temperatures compared to Scenario 1.",
            "Proposed Prompt Input (CRP Step 5: Generate Final Response)": "Question: If the sun was not shining, what would happen to the temperature?\nStep 5: Generate a final response that incorporates insights from the counterfactual reasoning process.",
            "Proposed Prompt Expected Output (CRP Step 5: Generate Final Response)": "If the sun was not shining, the Earth's temperature would drop significantly. The extent of the temperature drop would depend on factors such as the presence of the Earth's atmosphere and the surface properties. In the absence of sunlight, the atmosphere would still provide some insulation, but the temperature would be much lower than current levels. If the atmosphere were also absent, the temperature would plummet even further, making the Earth extremely cold. Additionally, if the Earth's surface were mostly covered by ice, it would reflect more sunlight and have a lower heat capacity, exacerbating the cooling effect. In summary, the sun's presence is crucial for maintaining the Earth's temperature at a level suitable for life as we know it.",
            "Explanation": "The proposed CRP method guides the model through a structured counterfactual reasoning process, encouraging it to consider alternative scenarios and their consequences systematically. By comparing the outcomes of the counterfactual scenarios with the original scenario, the model can generate a more comprehensive and factually consistent response that incorporates insights from the reasoning process."
        },
        "Fallback Plan": "If the proposed CRP method does not significantly improve the model's performance on counterfactual reasoning tasks, consider the following alternative plans:\n1. Analyze the quality of the generated counterfactual scenarios and their consequences to identify potential weaknesses in the prompting strategy. Modify the prompts to encourage more diverse and relevant counterfactual scenarios.\n2. Investigate the impact of using different numbers of few-shot examples in the prompts. Vary the number and quality of examples to find the optimal balance between guidance and flexibility.\n3. Explore alternative prompting strategies that focus on other aspects of counterfactual reasoning, such as identifying causal relationships or generating more targeted counterfactual questions.\n4. Conduct an in-depth error analysis to understand the types of mistakes the model makes when engaging in counterfactual reasoning. Use these insights to inform the development of new prompting strategies or fine-tuning approaches.\n5. If the CRP method does not yield significant improvements, consider turning the project into an analysis paper that explores the limitations of current LLMs in counterfactual reasoning tasks and offers insights into potential future directions for research in this area."
    },
    "novelty_queries": [
        "KeywordQuery(\"counterfactual reasoning language models\")",
        "KeywordQuery(\"language models alternative possibilities prompting\")",
        "KeywordQuery(\"counterfactual reasoning prompts llms\")",
        "KeywordQuery(\"Counterfactual Reasoning Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "9028fd54ecbbd58da6f3d86629b48bb95405fff2",
            "paperId": "9028fd54ecbbd58da6f3d86629b48bb95405fff2",
            "title": "Counterfactual reasoning: Testing language models\u2019 understanding of hypothetical scenarios",
            "abstract": "Current pre-trained language models have enabled remarkable improvements in downstream tasks, but it remains difficult to distinguish effects of statistical correlation from more systematic logical reasoning grounded on the understanding of real world. We tease these factors apart by leveraging counterfactual conditionals, which force language models to predict unusual consequences based on hypothetical propositions. We introduce a set of tests from psycholinguistic experiments, as well as larger-scale controlled datasets, to probe counterfactual predictions from five pre-trained language models. We find that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge\u2014however, we also find that for most models this effect appears largely to be driven by simple lexical cues. When we mitigate effects of both world knowledge and lexical cues to test knowledge of linguistic nuances of counterfactuals, we find that only GPT-3 shows sensitivity to these nuances, though this sensitivity is also non-trivially impacted by lexical associative factors.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge\u2014however, it is also found that for most models this effect appears largely to be driven by simple lexical cues."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve language models' ability to consider alternative possibilities through a multi-step prompting approach called Counterfactual Reasoning Prompting (CRP). The paper, on the other hand, focuses on testing language models' understanding of hypothetical scenarios using counterfactual conditionals and probing their predictions.\n\nWhile both the project proposal and the paper deal with counterfactual reasoning in language models, their research problems and approaches differ. The project proposal seeks to improve the models' counterfactual reasoning abilities through a structured prompting method, whereas the paper aims to test and probe the models' existing understanding of counterfactual scenarios using psycholinguistic experiments and controlled datasets.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "91a82593721c03ecffdef1c72ea55c6d87c42473",
            "paperId": "91a82593721c03ecffdef1c72ea55c6d87c42473",
            "title": "Counterfactual reasoning: Do language models need world knowledge for causal understanding?",
            "abstract": "Current pre-trained language models have enabled remarkable improvements in downstream tasks, but it remains difficult to distinguish effects of statistical correlation from more systematic logical reasoning grounded on understanding of the real world. In this paper we tease these factors apart by leveraging counterfactual conditionals, which force language models to predict unusual consequences based on hypothetical propositions. We introduce a set of tests drawn from psycholinguistic experiments, as well as larger-scale controlled datasets, to probe counterfactual predictions from a variety of popular pre-trained language models. We find that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge -- however, we also find that for most models this effect appears largely to be driven by simple lexical cues. When we mitigate effects of both world knowledge and lexical cues to test knowledge of linguistic nuances of counterfactuals, we find that only GPT-3 shows sensitivity to these nuances, though this sensitivity is also non-trivially impacted by lexical associative factors.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge -- however, it is also found that for most models this effect appears largely to be driven by simple lexical cues."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve language models' ability to consider alternative possibilities through a multi-step prompting approach called Counterfactual Reasoning Prompting (CRP). The paper investigates language models' counterfactual reasoning abilities and their reliance on world knowledge and lexical cues.\n\nWhile both the project proposal and the paper focus on counterfactual reasoning in language models, the project proposal emphasizes improving the models' performance through a novel prompting method, whereas the paper primarily analyzes the factors influencing the models' counterfactual reasoning abilities.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "0dc6e2476005d1aae30bede8ebbae92bdc831f6a",
            "paperId": "0dc6e2476005d1aae30bede8ebbae92bdc831f6a",
            "title": "Counterfactual reasoning: Do Language Models need world knowledge for causal inference?",
            "abstract": "Current pre-trained language models have enabled remarkable improvements in downstream tasks, but it remains difficult to distinguish effects of statistical correlation from more systematic logical reasoning grounded on understanding of the real world. In this paper we tease these factors apart by leveraging counterfactual conditionals , which force language models to predict unusual consequences based on hypothetical propositions. We introduce a set of tests drawn from psycholinguistic experiments, as well as larger-scale controlled datasets, to probe counterfactual predictions from a variety of popular pre-trained language models. We find that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge\u2014however, we also find that for most models this effect appears largely to be driven by simple lexical cues. When we mitigate effects of both world knowledge and lexical cues to test knowledge of linguistic nuances of counterfactuals, we find that only GPT-3 shows sensitivity to these nuances, though this sensitivity is also non-trivially impacted by lexical associative factors.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge\u2014however, it is also found that for most models this effect appears largely to be driven by simple lexical cues."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving language models' ability to reason about counterfactual scenarios using a multi-step prompting approach. The paper also focuses on probing language models' counterfactual reasoning abilities, but it does not propose a new method to improve them.\n\nProposal: Improving language models' counterfactual reasoning using a multi-step prompting approach.\nPaper: Probing language models' counterfactual reasoning abilities using psycholinguistic tests and controlled datasets.\n\nWhile both works deal with counterfactual reasoning in language models, the proposal aims to improve this ability, while the paper focuses on evaluating it. The methods and goals are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "04dd492b506e48b7dd91ecb1e1fdb80c1ce30e34",
            "paperId": "04dd492b506e48b7dd91ecb1e1fdb80c1ce30e34",
            "title": "Towards CausalGPT: A Multi-Agent Approach for Faithful Knowledge Reasoning via Promoting Causal Consistency in LLMs",
            "abstract": "Despite advancements in LLMs, knowledge-based reasoning remains a longstanding issue due to the fragility of knowledge recall and inference. Existing methods primarily encourage LLMs to autonomously plan and solve problems or to extensively sample reasoning chains without addressing the conceptual and inferential fallacies. Attempting to alleviate inferential fallacies and drawing inspiration from multi-agent collaboration, we present a framework to increase faithfulness and causality for knowledge-based reasoning. Specifically, we propose to employ multiple intelligent agents (i.e., reasoners and an evaluator) to work collaboratively in a reasoning-and-consensus paradigm for elevated reasoning faithfulness. The reasoners focus on providing solutions with human-like causality to solve open-domain problems. On the other hand, the \\textit{evaluator} agent scrutinizes if a solution is deducible from a non-causal perspective and if it still holds when challenged by a counterfactual candidate. According to the extensive and comprehensive evaluations on a variety of knowledge reasoning tasks (e.g., science question answering and commonsense reasoning), our framework outperforms all compared state-of-the-art approaches by large margins.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes to employ multiple intelligent agents to work collaboratively in a reasoning-and-consensus paradigm for elevated reasoning faithfulness and outperforms all compared state-of-the-art approaches by large margins."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve language models' ability to consider alternative possibilities through a multi-step prompting approach called Counterfactual Reasoning Prompting (CRP). The paper proposes a multi-agent approach to increase faithfulness and causality for knowledge-based reasoning in language models by employing multiple intelligent agents (reasoners and an evaluator) working collaboratively.\n\nWhile both the project proposal and the paper focus on improving reasoning capabilities in language models, their approaches and specific problem areas differ. The project proposal targets counterfactual reasoning and considers alternative possibilities, while the paper addresses knowledge-based reasoning and aims to increase faithfulness and causality.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "0d943aa547690c40aff35b4e0b329bf04aedc59d",
            "paperId": "0d943aa547690c40aff35b4e0b329bf04aedc59d",
            "title": "Diversity of Thought Improves Reasoning Abilities of LLMs",
            "abstract": "Large language models (LLMs) are documented to struggle in settings that require complex reasoning. Nevertheless, instructing the model to break down the problem into smaller reasoning steps, or ensembling various generations through modifying decoding steps boosts performance. However, these methods assume that the input prompt is fixed and expect the decoding strategies to introduce the diversity needed for ensembling. In this work, we discuss how one can create and leverage variations of the input prompt as a means of diversity of thought. We propose a method that automatically improves prompt diversity by soliciting feedback from the LLM to ideate approaches that are apt for the problem. We then ensemble the diverse prompts in our method DIVSE (DIVerse reasoning path Self-Ensemble) across multiple inference calls, or use diverse approaches within a single inference call; we call the latter IDIV-SE (In-call DIVerse reasoning path Self-Ensemble). Apart from our approaches outperforming prior work, DIV-SE(in particular) advances state-of-the-art performance on the challenging planning and graph coloring benchmarks. Our results improve the Pareto frontier of the accuracy-cost trade-off.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a method that automatically improves prompt diversity by soliciting feedback from the LLM to ideate approaches that are apt for the problem, and improves the Pareto frontier of the accuracy-cost trade-off."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve language models' ability to consider alternative possibilities and reason about counterfactual scenarios by using a multi-step prompting approach called Counterfactual Reasoning Prompting (CRP). The paper focuses on improving the reasoning abilities of large language models by leveraging variations of the input prompt to create diversity of thought and ensembling the diverse prompts.\n\nWhile both the project proposal and the paper aim to improve the reasoning abilities of language models, their approaches differ. The project proposal introduces a structured prompting method to guide the model through a counterfactual reasoning process, while the paper explores the use of diverse input prompts and ensembling techniques to enhance reasoning performance.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "63549bf78e4b1e7e1cec505ce65e6e8f90474f41",
            "paperId": "63549bf78e4b1e7e1cec505ce65e6e8f90474f41",
            "title": "ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs",
            "abstract": "Large Language Models (LLMs) still struggle with natural language reasoning tasks. Motivated by the society of minds (Minsky, 1988), we propose ReConcile, a multi-model multiagent framework designed as a round table conference among diverse LLM agents. ReConcile enhances collaborative reasoning between LLM agents via multiple rounds of discussion, learning to convince other agents to improve their answers, and employing a confidence-weighted voting mechanism that leads to a better consensus. In each round, ReConcile initiates discussion between agents via a 'discussion prompt' that consists of (a) grouped answers and explanations generated by each agent in the previous round, (b) their confidence scores, and (c) demonstrations of answer-rectifying human explanations, used for convincing other agents. Experiments on seven benchmarks demonstrate that ReConcile significantly improves LLMs' reasoning -- both individually and as a team -- surpassing prior single-agent and multi-agent baselines by up to 11.4% and even outperforming GPT-4 on three datasets. ReConcile also flexibly incorporates different combinations of agents, including API-based, open-source, and domain-specific models, leading to an 8% improvement on MATH. Finally, we analyze the individual components of ReConcile, demonstrating that the diversity originating from different models is critical to its superior performance. Code: https://github.com/dinobby/ReConcile",
            "year": 2023,
            "citationCount": 29,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experiments demonstrate that ReConcile significantly improves LLMs' reasoning -- both individually and as a team -- surpassing prior single-agent and multi-agent baselines by up to 11.4% and even outperforming GPT-4 on three datasets."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving language models' ability to reason about counterfactual scenarios, and the approach is using a multi-step prompting method called Counterfactual Reasoning Prompting (CRP) to guide the model through a structured counterfactual reasoning process.\n\nThe research problem in the paper is improving language models' reasoning ability, and the approach is using a multi-model multiagent framework called ReConcile that facilitates collaborative reasoning among diverse LLM agents through multiple rounds of discussion and confidence-weighted voting.\n\nWhile both the proposal and the paper aim to improve language models' reasoning capabilities, they focus on different aspects of reasoning (counterfactual reasoning vs. general reasoning) and employ different approaches (multi-step prompting vs. multi-model multiagent framework).\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "b1ee831883f7f6ab17a34875df23afa8fc56f59e",
            "paperId": "b1ee831883f7f6ab17a34875df23afa8fc56f59e",
            "title": "Think before You Simulate: Symbolic Reasoning to Orchestrate Neural Computation for Counterfactual Question Answering",
            "abstract": "Causal and temporal reasoning about video dynamics is a challenging problem. While neuro-symbolic models that combine symbolic reasoning with neural-based perception and prediction have shown promise, they exhibit limitations, especially in answering counterfactual questions. This paper introduces a method to enhance a neuro-symbolic model for counterfactual reasoning, leveraging symbolic reasoning about causal relations among events. We define the notion of a causal graph to represent such relations and use Answer Set Programming (ASP), a declarative logic programming method, to find how to coordinate perception and simulation modules. We validate the effectiveness of our approach on two benchmarks, CLEVRER and CRAFT. Our enhancement achieves state-of-the-art performance on the CLEVRER challenge, significantly outperforming existing models. In the case of the CRAFT benchmark, we leverage a large pre-trained language model, such as GPT-3.5 and GPT-4, as a proxy for a dynamics simulator. Our findings show that this method can further improve its performance on counterfactual questions by providing alternative prompts instructed by symbolic causal reasoning.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A method to enhance a neuro-symbolic model for counterfactual reasoning, leveraging symbolic reasoning about causal relations among events, and defines the notion of a causal graph to represent such relations and uses Answer Set Programming (ASP) to find how to coordinate perception and simulation modules."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve language models' ability to consider alternative possibilities in counterfactual reasoning by using a multi-step prompting approach called Counterfactual Reasoning Prompting (CRP). The paper focuses on enhancing a neuro-symbolic model for counterfactual reasoning in video dynamics by leveraging symbolic reasoning about causal relations among events.\n\nWhile both the project proposal and the paper address counterfactual reasoning, they differ in their focus and approach. The project proposal targets language models and uses a prompting method, while the paper focuses on neuro-symbolic models for video dynamics and employs symbolic reasoning with Answer Set Programming.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "810226ed665b3a9693b7171fcdb244bb3bb31acf",
            "paperId": "810226ed665b3a9693b7171fcdb244bb3bb31acf",
            "title": "Let's Do a Thought Experiment: Using Counterfactuals to Improve Moral Reasoning",
            "abstract": "Language models still struggle on moral reasoning, despite their impressive performance in many other tasks. In particular, the Moral Scenarios task in MMLU (Multi-task Language Understanding) is among the worst performing tasks for many language models, including GPT-3. In this work, we propose a new prompting framework, Thought Experiments, to teach language models to do better moral reasoning using counterfactuals. Experiment results show that our framework elicits counterfactual questions and answers from the model, which in turn helps improve the accuracy on Moral Scenarios task by 9-16% compared to other zero-shot baselines. Interestingly, unlike math reasoning tasks, zero-shot Chain-of-Thought (CoT) reasoning doesn't work out of the box, and even reduces accuracy by around 4% compared to direct zero-shot. We further observed that with minimal human supervision in the form of 5 few-shot examples, the accuracy of the task can be improved to as much as 80%.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new prompting framework is proposed, Thought Experiments, to teach language models to do better moral reasoning using counterfactuals, and shows that with minimal human supervision in the form of 5 few-shot examples, the accuracy of the task can be improved to as much as 80%."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve language models' ability to consider alternative possibilities and reason about counterfactual scenarios by using a multi-step prompting approach called Counterfactual Reasoning Prompting (CRP).\n\nThe paper proposes a new prompting framework called Thought Experiments to teach language models better moral reasoning using counterfactuals, specifically focusing on improving accuracy on the Moral Scenarios task in MMLU.\n\nWhile both the project proposal and the paper use counterfactuals to improve language models' reasoning abilities, the project proposal focuses on general counterfactual reasoning and generating consistent responses, while the paper specifically targets moral reasoning and improving accuracy on a specific task (Moral Scenarios in MMLU).\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "f8e99be4f9a01761fab74bade2c3c18de9fc686b",
            "paperId": "f8e99be4f9a01761fab74bade2c3c18de9fc686b",
            "title": "Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks",
            "abstract": "The impressive performance of recent language models across a wide range of tasks suggests that they possess a degree of abstract reasoning skills. Are these skills general and transferable, or specialized to specific tasks seen during pretraining? To disentangle these effects, we propose an evaluation framework based on\"counterfactual\"task variants that deviate from the default assumptions underlying standard tasks. Across a suite of 11 tasks, we observe nontrivial performance on the counterfactual variants, but nevertheless find that performance substantially and consistently degrades compared to the default conditions. This suggests that while current LMs may possess abstract task-solving skills to an extent, they often also rely on narrow, non-transferable procedures for task-solving. These results motivate a more careful interpretation of language model performance that teases apart these aspects of behavior.",
            "year": 2023,
            "citationCount": 69,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An evaluation framework based on task variants that deviate from the default assumptions underlying standard tasks that suggests that while current LMs may possess abstract task-solving skills to an extent, they often also rely on narrow, non-transferable procedures for task-solving."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving language models' ability to consider alternative possibilities through counterfactual reasoning prompting, while the paper explores the capabilities and limitations of language models in counterfactual reasoning tasks.\n\nThe approach in the proposal is a multi-step prompting method to guide the model's counterfactual reasoning process, whereas the paper proposes an evaluation framework based on counterfactual task variants to assess the transferability of language models' reasoning skills.\n\nAlthough both the proposal and the paper focus on counterfactual reasoning in language models, the proposal aims to improve the models' performance through a novel prompting method, while the paper primarily focuses on evaluating the models' existing capabilities and limitations using counterfactual task variants.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "eebdf7303256f081ab1f6a36ff0ea6126e4da484",
            "paperId": "eebdf7303256f081ab1f6a36ff0ea6126e4da484",
            "title": "CRASS: A Novel Data Set and Benchmark to Test Counterfactual Reasoning of Large Language Models",
            "abstract": "We introduce the CRASS (counterfactual reasoning assessment) data set and benchmark utilizing questionized counterfactual conditionals as a novel and powerful tool to evaluate large language models. We present the data set design and benchmark. We test six state-of-the-art models against our benchmark. Our results show that it poses a valid challenge for these models and opens up considerable room for their improvement.",
            "year": 2021,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The CRASS data set and benchmark utilizing questionized counterfactual conditionals is introduced as a novel and powerful tool to evaluate large language models and poses a valid challenge for these models and opens up considerable room for their improvement."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve language models' ability to consider alternative possibilities through a multi-step prompting approach called Counterfactual Reasoning Prompting (CRP). The paper introduces a new dataset and benchmark called CRASS to evaluate the counterfactual reasoning abilities of large language models.\n\nWhile both the project proposal and the paper focus on counterfactual reasoning in language models, the project proposes a specific prompting method to improve the models' performance, whereas the paper introduces a dataset and benchmark to assess the models' current capabilities. The project does not directly use the CRASS dataset or benchmark in its experiments.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "b69ae70abf1e8519a565c40bed07ab499576a8f6",
            "paperId": "b69ae70abf1e8519a565c40bed07ab499576a8f6",
            "title": "Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models",
            "abstract": "Large language models (LLMs) have performed well on several reasoning benchmarks, including ones that test analogical reasoning abilities. However, it has been debated whether they are actually performing humanlike abstract reasoning or instead employing less general processes that rely on similarity to what has been seen in their training data. Here we investigate the generality of analogy-making abilities previously claimed for LLMs (Webb, Holyoak,&Lu, 2023). We take one set of analogy problems used to evaluate LLMs and create a set of\"counterfactual\"variants-versions that test the same abstract reasoning abilities but that are likely dissimilar from any pre-training data. We test humans and three GPT models on both the original and counterfactual problems, and show that, while the performance of humans remains high for all the problems, the GPT models' performance declines sharply on the counterfactual set. This work provides evidence that, despite previously reported successes of LLMs on analogical reasoning, these models lack the robustness and generality of human analogy-making.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work takes one set of analogy problems used to evaluate LLMs and creates a set of \"counterfactual\"variants-versions that test the same abstract reasoning abilities but that are likely dissimilar from any pre-training data, providing evidence that these models lack the robustness and generality of human analogy-making."
            },
            "score": 6
        },
        {
            "id": "e67dd9230cd4a8199ca6c9c9ebaa31ab29be23d2",
            "paperId": "e67dd9230cd4a8199ca6c9c9ebaa31ab29be23d2",
            "title": "Evidence from counterfactual tasks supports emergent analogical reasoning in large language models",
            "abstract": "We recently reported evidence that large language models are capable of solving a wide range of text-based analogy problems in a zero-shot manner, indicating the presence of an emergent capacity for analogical reasoning. Two recent commentaries have challenged these results, citing evidence from so-called `counterfactual' tasks in which the standard sequence of the alphabet is arbitrarily permuted so as to decrease similarity with materials that may have been present in the language model's training data. Here, we reply to these critiques, clarifying some misunderstandings about the test materials used in our original work, and presenting evidence that language models are also capable of generalizing to these new counterfactual task variants.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 6
        },
        {
            "id": "dab4f70d75a04e62553e583f2450d9bb1f0ead46",
            "paperId": "dab4f70d75a04e62553e583f2450d9bb1f0ead46",
            "title": "CLOMO: Counterfactual Logical Modification with Large Language Models",
            "abstract": "In this study, we delve into the realm of counterfactual reasoning capabilities of large language models (LLMs). Our primary objective is to cultivate the counterfactual thought processes within LLMs and rigorously assess these processes for their validity. Specifically, we introduce a novel task, Counterfactual Logical Modification (CLOMO), and a high-quality human-annotated benchmark. In this task, LLMs must adeptly alter a given argumentative text to uphold a predetermined logical relationship. To effectively evaluate a generation model's counterfactual capabilities, we propose an innovative evaluation metric, the LogicAware Counterfactual Score to directly evaluate the natural language output of LLMs instead of modeling the task as a multiple-choice problem. Analysis shows that the proposed automatic metric aligns well with human preference. Our experimental results show that while LLMs demonstrate a notable capacity for logical counterfactual thinking, there remains a discernible gap between their current abilities and human performance.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study introduces a novel task, Counterfactual Logical Modification (CLOMO), and proposes an innovative evaluation metric, the LogicAware CounterfactUAL Score, to directly evaluate the natural language output of LLMs instead of modeling the task as a multiple-choice problem."
            },
            "score": 6
        },
        {
            "id": "7dc928f41e15f65f1267bd87b0fcfcc7e715cb56",
            "paperId": "7dc928f41e15f65f1267bd87b0fcfcc7e715cb56",
            "title": "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting",
            "abstract": "Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. This level of transparency into LLMs' predictions would yield significant safety benefits. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs--e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always\"(A)\"--which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations rationalizing those answers. This causes accuracy to drop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. Building more transparent and explainable systems will require either improving CoT faithfulness through targeted efforts or abandoning CoT in favor of alternative methods.",
            "year": 2023,
            "citationCount": 137,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that CoT explanations can be plausible yet misleading, which risks increasing trust in LLMs without guaranteeing their safety, and building more transparent and explainable systems will require either improving CoT faithfulness through targeted efforts or abandoning CoT in favor of alternative methods."
            },
            "score": 6
        },
        {
            "id": "ee692adce2e5cbb42eae2d516641ae907b0483ce",
            "paperId": "ee692adce2e5cbb42eae2d516641ae907b0483ce",
            "title": "Diagnostic reasoning prompts reveal the potential for large language model interpretability in medicine",
            "abstract": null,
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 6
        },
        {
            "id": "af44c205c648c21d06064b23613dd60ecbd4adf8",
            "paperId": "af44c205c648c21d06064b23613dd60ecbd4adf8",
            "title": "Improving Zero-shot Visual Question Answering via Large Language Models with Reasoning Question Prompts",
            "abstract": "Zero-shot Visual Question Answering (VQA) is a prominent vision-language task that examines both the visual and textual understanding capability of systems in the absence of training data. Recently, by converting the images into captions, information across multi-modalities is bridged and Large Language Models (LLMs) can apply their strong zero-shot generalization capability to unseen questions. To design ideal prompts for solving VQA via LLMs, several studies have explored different strategies to select or generate question-answer pairs as the exemplar prompts, which guide LLMs to answer the current questions effectively. However, they totally ignore the role of question prompts. The original questions in VQA tasks usually encounter ellipses and ambiguity which require intermediate reasoning. To this end, we present Reasoning Question Prompts for VQA tasks, which can further activate the potential of LLMs in zero-shot scenarios. Specifically, for each question, we first generate self-contained questions as reasoning question prompts via an unsupervised question edition module considering sentence fluency, semantic integrity and syntactic invariance. Each reasoning question prompt clearly indicates the intent of the original question. This results in a set of candidate answers. Then, the candidate answers associated with their confidence scores acting as answer heuristics are fed into LLMs and produce the final answer. We evaluate reasoning question prompts on three VQA challenges, experimental results demonstrate that they can significantly improve the results of LLMs on zero-shot setting and outperform existing state-of-the-art zero-shot methods on three out of four data sets. Our source code is publicly released at https://github.com/ECNU-DASE-NLP/RQP.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Reasoning Question Prompts for VQA tasks are presented, which can further activate the potential of Large Language Models in zero-shot scenarios and can significantly improve the results of LLMs on zero- shot setting and outperform existing state-of-the-art zero-Shot methods on three out of four data sets."
            },
            "score": 6
        },
        {
            "id": "09bed28b3221f8aaac7f348cf0a5bb683513433b",
            "paperId": "09bed28b3221f8aaac7f348cf0a5bb683513433b",
            "title": "DIVKNOWQA: Assessing the Reasoning Ability of LLMs via Open-Domain Question Answering over Knowledge Base and Text",
            "abstract": "Large Language Models (LLMs) have exhibited impressive generation capabilities, but they suffer from hallucinations when solely relying on their internal knowledge, especially when answering questions that require less commonly known information. Retrieval-augmented LLMs have emerged as a potential solution to ground LLMs in external knowledge. Nonetheless, recent approaches have primarily emphasized retrieval from unstructured text corpora, owing to its seamless integration into prompts. When using structured data such as knowledge graphs, most methods simplify it into natural text, neglecting the underlying structures. Moreover, a significant gap in the current landscape is the absence of a realistic benchmark for evaluating the effectiveness of grounding LLMs on heterogeneous knowledge sources (e.g., knowledge base and text). To fill this gap, we have curated a comprehensive dataset that poses two unique challenges: (1) Two-hop multi-source questions that require retrieving information from both open-domain structured and unstructured knowledge sources; retrieving information from structured knowledge sources is a critical component in correctly answering the questions. (2) The generation of symbolic queries (e.g., SPARQL for Wikidata) is a key requirement, which adds another layer of challenge. Our dataset is created using a combination of automatic generation through predefined reasoning chains and human annotation. We also introduce a novel approach that leverages multiple retrieval tools, including text passage retrieval and symbolic language-assisted retrieval. Our model outperforms previous approaches by a significant margin, demonstrating its effectiveness in addressing the above-mentioned reasoning challenges.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work curated a comprehensive dataset that poses two unique challenges: two-hop multi-source questions that require retrieving information from both open-domain structured and unstructured knowledge sources; and a novel approach that leverages multiple retrieval tools, including text passage retrieval and symbolic language-assisted retrieval."
            },
            "score": 6
        },
        {
            "id": "1e6102c981b9464c632ef0b00dbd11dfb0564e4e",
            "paperId": "1e6102c981b9464c632ef0b00dbd11dfb0564e4e",
            "title": "SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning",
            "abstract": "The recent progress in large language models (LLMs), especially the invention of chain-of-thought prompting, has made it possible to automatically answer questions by stepwise reasoning. However, when faced with more complicated problems that require non-linear thinking, even the strongest LLMs make mistakes. To address this, we explore whether LLMs are able to recognize errors in their own step-by-step reasoning, without resorting to external resources. To this end, we propose SelfCheck, a general-purpose zero-shot verification schema for recognizing such errors. We then use the results of these checks to improve question-answering performance by conducting weighted voting on multiple solutions to the question. We test SelfCheck on three datasets (GSM8K, MathQA, and MATH) and find that it successfully recognizes errors and, in turn, increases final answer accuracies.",
            "year": 2023,
            "citationCount": 34,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes SelfCheck, a general-purpose zero-shot verification schema for recognizing errors in large language models and uses the results of these checks to improve question-answering performance by conducting weighted voting on multiple solutions to the question."
            },
            "score": 6
        },
        {
            "id": "02ca7c0a9938e7f5a6e2c8b4df8a92c5bdbc283c",
            "paperId": "02ca7c0a9938e7f5a6e2c8b4df8a92c5bdbc283c",
            "title": "Distillation Contrastive Decoding: Improving LLMs Reasoning with Contrastive Decoding and Distillation",
            "abstract": "We propose a straightforward approach called Distillation Contrastive Decoding (DCD) to enhance the reasoning capabilities of Large Language Models (LLMs) during inference. In contrast to previous approaches that relied on smaller amateur models or analysis of hidden state differences, DCD employs Contrastive Chain-of-thought Prompting and advanced distillation techniques, including Dropout and Quantization. This approach effectively addresses the limitations of Contrastive Decoding (CD), which typically requires both an expert and an amateur model, thus increasing computational resource demands. By integrating contrastive prompts with distillation, DCD obviates the need for an amateur model and reduces memory usage. Our evaluations demonstrate that DCD significantly enhances LLM performance across a range of reasoning benchmarks, surpassing both CD and existing methods in the GSM8K and StrategyQA datasets.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Evaluated methods demonstrate that DCD significantly enhances LLM performance across a range of reasoning benchmarks, surpassing both CD and existing methods in the GSM8K and StrategyQA datasets."
            },
            "score": 6
        },
        {
            "id": "ee079b16612bc36de2c117821bd8ed4c2c8eacfb",
            "paperId": "ee079b16612bc36de2c117821bd8ed4c2c8eacfb",
            "title": "Being nice by choice: The effect of counterfactual reasoning on children's social evaluations.",
            "abstract": "The ability to engage in counterfactual thinking (reason about what else could have happened) is critical to learning, agency, and social evaluation. However, not much is known about how individual differences in counterfactual reasoning may play a role in children's social evaluations. In the current study, we investigate how prompting children to engage in counterfactual thinking about positive moral actions impacts children's social evaluations. Eighty-seven 4-8-year-olds were introduced to a character who engaged in a positive moral action (shared a sticker with a friend) and asked about what else the character could have done with the sticker (counterfactual simulation). Children were asked to generate either a high number of counterfactuals (five alternative actions) or a low number of counterfactuals (one alternative action). Children were then asked a series of social evaluation questions contrasting that character with one who did not have a choice and had no alternatives (was told to give away the sticker to his friend). Results show that children who generated selfish counterfactuals were more likely to positively evaluate the character with choice than children who did not generate selfish counterfactuals, suggesting that generating counterfactuals most distant from the chosen action (prosociality) leads children to view prosocial actions more positively. We also found age-related changes: as children got older, regardless of the type of counterfactuals generated, they were more likely to evaluate the character with choice more positively. These results highlight the importance of counterfactual reasoning in the development of moral evaluations. RESEARCH HIGHLIGHTS: Older children were more likely to endorse agents who choose to share over those who do not have a choice. Children who were prompted to generate more counterfactuals were more likely to allocate resources to characters with choice. Children who generated selfish counterfactuals more positively evaluated agents with choice. Comparable to theories suggesting children punish willful transgressors more than accidental transgressors, we propose children also consider free will when making positive moral evaluations.",
            "year": 2023,
            "citationCount": 6,
            "tldr": null,
            "score": 6
        },
        {
            "id": "7747ecbc26b1688e6cad1a6ce83914efa2a3c04c",
            "paperId": "7747ecbc26b1688e6cad1a6ce83914efa2a3c04c",
            "title": "Prompting Contrastive Explanations for Commonsense Reasoning Tasks",
            "abstract": "Many commonsense reasoning NLP tasks involve choosing between one or more possible answers to a question or prompt based on knowledge that is often implicit. Large pretrained language models (PLMs) can achieve near-human performance on such tasks, while providing little human-interpretable evidence of the underlying reasoning they use. In this work, we show how to use these same models to generate such evidence: inspired by the contrastive nature of human explanations, we use PLMs to complete explanation prompts which contrast alternatives according to the key attribute(s) required to justify the correct answer (for example, peanuts are usually salty while raisins are sweet). Conditioning model decisions on these explanations improves performance on two commonsense reasoning benchmarks, as compared to previous non-contrastive alternatives. These explanations are also judged by humans to be more relevant for solving the task, and facilitate a novel method to evaluate explanation faithfulfness.",
            "year": 2021,
            "citationCount": 48,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Inspired by the contrastive nature of human explanations, this work uses PLMs to complete explanation prompts which contrast alternatives according to the key attribute(s) required to justify the correct answer (for example, peanuts are usually salty while raisins are sweet)."
            },
            "score": 6
        },
        {
            "id": "deff814eefe597b2fe275bc3dd205ecb1cc09c4e",
            "paperId": "deff814eefe597b2fe275bc3dd205ecb1cc09c4e",
            "title": "PINTO: Faithful Language Reasoning Using Prompt-Generated Rationales",
            "abstract": "Neural language models (LMs) have achieved impressive results on various language-based reasoning tasks by utilizing latent knowledge encoded in their own pretrained parameters. To make this reasoning process more explicit, recent works retrieve a rationalizing LM's internal knowledge by training or prompting it to generate free-text rationales, which can be used to guide task predictions made by either the same LM or a separate reasoning LM. However, rationalizing LMs require expensive rationale annotation and/or computation, without any assurance that their generated rationales improve LM task performance or faithfully reflect LM decision-making. In this paper, we propose PINTO, an LM pipeline that rationalizes via prompt-based learning, and learns to faithfully reason over rationales via counterfactual regularization. First, PINTO maps out a suitable reasoning process for the task input by prompting a frozen rationalizing LM to generate a free-text rationale. Second, PINTO's reasoning LM is fine-tuned to solve the task using the generated rationale as context, while regularized to output less confident predictions when the rationale is perturbed. Across four datasets, we show that PINTO significantly improves the generalization ability of the reasoning LM, yielding higher performance on both in-distribution and out-of-distribution test sets. Also, we find that PINTO's rationales are more faithful to its task predictions than those generated by competitive baselines.",
            "year": 2022,
            "citationCount": 41,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "PINTO is proposed, an LM pipeline that rationalizes via prompt-based learning, and learns to faithfully reason over rationales via counterfactual regularization, and is fine-tuned to solve the task using the generated rationale as context, while regularized to output less confident predictions when the rationale is perturbed."
            },
            "score": 6
        },
        {
            "id": "20ae101289965d36dbd93e9b8c47ec9deab03ed0",
            "paperId": "20ae101289965d36dbd93e9b8c47ec9deab03ed0",
            "title": "What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models",
            "abstract": "Counterfactual reasoning ability is one of the core abilities of human intelligence. This reasoning process involves the processing of alternatives to observed states or past events, and this process can improve our ability for planning and decision-making. In this work, we focus on benchmarking the counterfactual reasoning ability of multimodal large language models. We take the question and answer pairs from the VQAv2 dataset and add one counterfactual presupposition to the questions, with the answer being modified accordingly. After generating counterfactual questions and answers using ChatGPT, we manually examine all generated questions and answers to ensure correctness. This results in over 2k counterfactual question and answer pairs. We evaluate recent vision language models on our newly collected test dataset and found that all models exhibit a large performance drop compared to the results tested on questions without counterfactual presupposition. This result indicates that there still exists space for developing vision language models. We hope our proposed benchmark can help the development of future systems.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work benchmarking the counterfactual reasoning ability of multimodal large language models found that all models exhibit a large performance drop compared to the results tested on questions without counterfactual presupposition, indicating that there still exists space for developing vision language models."
            },
            "score": 5
        },
        {
            "id": "415c594667bb94f086ea980f897c4f62f9a22d18",
            "paperId": "415c594667bb94f086ea980f897c4f62f9a22d18",
            "title": "Eyes Can Deceive: Benchmarking Counterfactual Reasoning Abilities of Multi-modal Large Language Models",
            "abstract": "Counterfactual reasoning, as a crucial manifestation of human intelligence, refers to making presuppositions based on established facts and extrapolating potential outcomes. Existing multimodal large language models (MLLMs) have exhibited impressive cognitive and reasoning capabilities, which have been examined across a wide range of Visual Question Answering (VQA) benchmarks. Nevertheless, how will existing MLLMs perform when faced with counterfactual questions? To answer this question, we first curate a novel \\textbf{C}ounter\\textbf{F}actual \\textbf{M}ulti\\textbf{M}odal reasoning benchmark, abbreviated as \\textbf{CFMM}, to systematically assess the counterfactual reasoning capabilities of MLLMs. Our CFMM comprises six challenging tasks, each including hundreds of carefully human-labeled counterfactual questions, to evaluate MLLM's counterfactual reasoning capabilities across diverse aspects. Through experiments, interestingly, we find that existing MLLMs prefer to believe what they see, but ignore the counterfactual presuppositions presented in the question, thereby leading to inaccurate responses. Furthermore, we evaluate a wide range of prevalent MLLMs on our proposed CFMM. The significant gap between their performance on our CFMM and that on several VQA benchmarks indicates that there is still considerable room for improvement in existing MLLMs toward approaching human-level intelligence. On the other hand, through boosting MLLMs performances on our CFMM in the future, potential avenues toward developing MLLMs with advanced intelligence can be explored.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that existing MLLMs prefer to believe what they see, but ignore the counterfactual presuppositions presented in the question, thereby leading to inaccurate responses on the proposed CFMM."
            },
            "score": 5
        },
        {
            "id": "3e4afde5a9de2c1801da99b8aff5ae05923f256b",
            "paperId": "3e4afde5a9de2c1801da99b8aff5ae05923f256b",
            "title": "Are Emergent Abilities in Large Language Models just In-Context Learning?",
            "abstract": "Large language models have exhibited emergent abilities, demonstrating exceptional performance across diverse tasks for which they were not explicitly trained, including those that require complex reasoning abilities. The emergence of such abilities carries profound implications for the future direction of research in NLP, especially as the deployment of such models becomes more prevalent. However, one key challenge is that the evaluation of these abilities is often confounded by competencies that arise in models through alternative prompting techniques, such as in-context learning and instruction following, which also emerge as the models are scaled up. In this study, we provide the first comprehensive examination of these emergent abilities while accounting for various potentially biasing factors that can influence the evaluation of models. We conduct rigorous tests on a set of 18 models, encompassing a parameter range from 60 million to 175 billion parameters, across a comprehensive set of 22 tasks. Through an extensive series of over 1,000 experiments, we provide compelling evidence that emergent abilities can primarily be ascribed to in-context learning. We find no evidence for the emergence of reasoning abilities, thus providing valuable insights into the underlying mechanisms driving the observed abilities and thus alleviating safety concerns regarding their use.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study provides the first comprehensive examination of these emergent abilities while accounting for various potentially biasing factors that can influence the evaluation of models and finds no evidence for the emergence of reasoning abilities, thus alleviating safety concerns regarding their use."
            },
            "score": 5
        },
        {
            "id": "c5120b546f1bd99df5bd2e2bf44db5c7c46d1545",
            "paperId": "c5120b546f1bd99df5bd2e2bf44db5c7c46d1545",
            "title": "Pretraining Language Models with Human Preferences",
            "abstract": "Language models (LMs) are pretrained to imitate internet text, including content that would violate human preferences if generated by an LM: falsehoods, offensive comments, personally identifiable information, low-quality or buggy code, and more. Here, we explore alternative objectives for pretraining LMs in a way that also guides them to generate text aligned with human preferences. We benchmark five objectives for pretraining with human feedback across three tasks and study how they affect the trade-off between alignment and capabilities of pretrained LMs. We find a Pareto-optimal and simple approach among those we explored: conditional training, or learning distribution over tokens conditional on their human preference scores given by a reward model. Conditional training reduces the rate of undesirable content by up to an order of magnitude, both when generating without a prompt and with an adversarially-chosen prompt. Moreover, conditional training maintains the downstream task performance of standard LM pretraining, both before and after task-specific finetuning. Pretraining with human feedback results in much better preference satisfaction than standard LM pretraining followed by finetuning with feedback, i.e., learning and then unlearning undesirable behavior. Our results suggest that we should move beyond imitation learning when pretraining LMs and incorporate human preferences from the start of training.",
            "year": 2023,
            "citationCount": 96,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results suggest that the authors should move beyond imitation learning when pretraining LMs and incorporate human preferences from the start of training, i.e., learning and then unlearning undesirable behavior."
            },
            "score": 5
        },
        {
            "id": "0d502a1e300336ae628f5c8b99ee4d3766c8f60b",
            "paperId": "0d502a1e300336ae628f5c8b99ee4d3766c8f60b",
            "title": "Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT",
            "abstract": "In this paper, we aim to develop a large language model (LLM) with the reasoning ability on complex graph data. Currently, LLMs have achieved very impressive performance on various natural language learning tasks, extensions of which have also been applied to study the vision tasks with multi-modal data. However, when it comes to the graph learning tasks, existing LLMs present very serious flaws due to their several inherited weaknesses in performing {multi-step logic reasoning}, {precise mathematical calculation} and {perception about the spatial and temporal factors}. To address such challenges, in this paper, we will investigate the principles, methodologies and algorithms to empower existing LLMs with graph reasoning ability, which will have tremendous impacts on the current research of both LLMs and graph learning. Inspired by the latest ChatGPT and Toolformer models, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer) framework to teach LLMs themselves with prompts augmented by ChatGPT to use external graph reasoning API tools. Specifically, we will investigate to teach Graph-ToolFormer to handle various graph data reasoning tasks in this paper, including both (1) very basic graph data loading and graph property reasoning tasks, ranging from simple graph order and size to the graph diameter and periphery, and (2) more advanced reasoning tasks on real-world graph data, such as bibliographic networks, protein molecules, sequential recommender systems, social networks and knowledge graphs.",
            "year": 2023,
            "citationCount": 41,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Inspired by the latest ChatGPT and Toolformer models, the Graph-ToolFormer (Graph Reasoning oriented Toolformer) framework is proposed to teach LLMs themselves with prompts augmented by ChatG PT to use external graph reasoning API tools."
            },
            "score": 5
        },
        {
            "id": "14ad24cc03b0e997f84a07547bb4c179acf896b6",
            "paperId": "14ad24cc03b0e997f84a07547bb4c179acf896b6",
            "title": "Few shot chain-of-thought driven reasoning to prompt LLMs for open ended medical question answering",
            "abstract": "Large Language models (LLMs) have demonstrated significant potential in transforming healthcare by automating tasks such as clinical documentation, information retrieval, and decision support. In this aspect, carefully engineered prompts have emerged as a powerful tool for using LLMs for medical scenarios, e.g., patient clinical scenarios. In this paper, we propose a modified version of the MedQA-USMLE dataset, which is subjective, to mimic real-life clinical scenarios. We explore the Chain of Thought (CoT) reasoning based on subjective response generation for the modified MedQA-USMLE dataset with appropriate LM-driven forward reasoning for correct responses to the medical questions. Keeping in mind the importance of response verification in the medical setting, we utilize a reward training mechanism whereby the language model also provides an appropriate verified response for a particular response to a clinical question. In this regard, we also include human-in-the-loop for different evaluation aspects. We develop better in-contrast learning strategies by modifying the 5-shot-codex-CoT-prompt from arXiv:2207.08143 for the subjective MedQA dataset and developing our incremental-reasoning prompt. Our evaluations show that the incremental reasoning prompt performs better than the modified codex prompt in certain scenarios. We also show that greedy decoding with the incremental reasoning method performs better than other strategies, such as prompt chaining and eliminative reasoning.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a modified version of the MedQA-USMLE dataset, which is subjective, to mimic real-life clinical scenarios, and explores the Chain of Thought (CoT) reasoning based on subjective response generation for the modified MedQA-USMLE dataset with appropriate LM-driven forward reasoning for correct responses to the medical questions."
            },
            "score": 5
        },
        {
            "id": "92fd80a619d6f9454b69882e81a710f857fd9cdf",
            "paperId": "92fd80a619d6f9454b69882e81a710f857fd9cdf",
            "title": "Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning",
            "abstract": "Open-source pre-trained Large Language Models (LLMs) exhibit strong language understanding and generation capabilities, making them highly successful in a variety of tasks. However, when used as agents for dealing with complex problems in the real world, their performance is far inferior to large commercial models such as ChatGPT and GPT-4. As intelligent agents, LLMs need to have the capabilities of task planning, long-term memory, and the ability to leverage external tools to achieve satisfactory performance. Various methods have been proposed to enhance the agent capabilities of LLMs. On the one hand, methods involve constructing agent-specific data and fine-tuning the models. On the other hand, some methods focus on designing prompts that effectively activate the reasoning abilities of the LLMs. We explore both strategies on the 7B and 13B models. We propose a comprehensive method for constructing agent-specific data using GPT-4. Through supervised fine-tuning with constructed data, we find that for these models with a relatively small number of parameters, supervised fine-tuning can significantly reduce hallucination outputs and formatting errors in agent tasks. Furthermore, techniques such as multi-path reasoning and task decomposition can effectively decrease problem complexity and enhance the performance of LLMs as agents. We evaluate our method on five agent tasks of AgentBench and achieve satisfactory results.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a comprehensive method for constructing agent-specific data using GPT-4 and finds that for these models with a relatively small number of parameters, supervised fine-tuning can significantly reduce hallucination outputs and formatting errors in agent tasks."
            },
            "score": 5
        },
        {
            "id": "62176de125738e3b95850d1227bac81fd646b78e",
            "paperId": "62176de125738e3b95850d1227bac81fd646b78e",
            "title": "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models",
            "abstract": "Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, Few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual efforts, Zero-shot-CoT concatenates the target problem statement with \u201cLet\u2019s think step by step\u201d as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.",
            "year": 2023,
            "citationCount": 115,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results over GPT-3 show that the proposed zero-shot prompting consistently outperforms Zero- shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem."
            },
            "score": 5
        },
        {
            "id": "12c826f4195da172b212a529f8fcf10cc79e35da",
            "paperId": "12c826f4195da172b212a529f8fcf10cc79e35da",
            "title": "Context-faithful Prompting for Large Language Models",
            "abstract": "Large language models (LLMs) encode parametric knowledge about world facts and have shown remarkable performance in knowledge-driven NLP tasks. However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks). In this paper, we seek to assess and enhance LLMs' contextual faithfulness in two aspects: knowledge conflict and prediction with abstention. We demonstrate that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies. In particular, we identify opinion-based prompts and counterfactual demonstrations as the most effective methods. Opinion-based prompts reframe the context as a narrator's statement and inquire about the narrator's opinions, while counterfactual demonstrations use instances containing false facts to improve faithfulness in knowledge conflict situations. Neither technique requires additional training. We conduct experiments on three datasets of two standard NLP tasks, machine reading comprehension and relation extraction, and the results demonstrate significant improvement in faithfulness to contexts. Code and data are released at https://github.com/wzhouad/context-faithful-llm.",
            "year": 2023,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies, and opinion-based prompts and counterfactual demonstrations are identified as the most effective methods."
            },
            "score": 5
        },
        {
            "id": "10632e0a667cbc3c52cc8f11a46d8e8e9c7739e3",
            "paperId": "10632e0a667cbc3c52cc8f11a46d8e8e9c7739e3",
            "title": "Causal Reasoning and Large Language Models: Opening a New Frontier for Causality",
            "abstract": "The causal capabilities of large language models (LLMs) is a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We further our understanding of LLMs and their causal implications, considering the distinctions between different types of causal reasoning tasks, as well as the entangled threats of construct and measurement validity. LLM-based methods establish new state-of-the-art accuracies on multiple causal benchmarks. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain), and actual causality (86% accuracy in determining necessary and sufficient causes in vignettes). At the same time, LLMs exhibit unpredictable failure modes and we provide some techniques to interpret their robustness. Crucially, LLMs perform these causal tasks while relying on sources of knowledge and methods distinct from and complementary to non-LLM based approaches. Specifically, LLMs bring capabilities so far understood to be restricted to humans, such as using collected knowledge to generate causal graphs or identifying background causal context from natural language. We envision LLMs to be used alongside existing causal methods, as a proxy for human domain knowledge and to reduce human effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods. We also see existing causal methods as promising tools for LLMs to formalize, validate, and communicate their reasoning especially in high-stakes scenarios. In capturing common sense and domain knowledge about causal mechanisms and supporting translation between natural language and formal methods, LLMs open new frontiers for advancing the research, practice, and adoption of causality.",
            "year": 2023,
            "citationCount": 96,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LLMs are envisioned to be used alongside existing causal methods, as a proxy for human domain knowledge and to reduce human effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods."
            },
            "score": 4
        },
        {
            "id": "6ac627f57b26354ab537734d820da4a6a7dde2c6",
            "paperId": "6ac627f57b26354ab537734d820da4a6a7dde2c6",
            "title": "CLadder: Assessing Causal Reasoning in Language Models",
            "abstract": "The ability to perform causal reasoning is widely considered a core feature of intelligence. In this work, we investigate whether large language models (LLMs) can coherently reason about causality. Much of the existing work in natural language processing (NLP) focuses on evaluating commonsense causal reasoning in LLMs, thus failing to assess whether a model can perform causal inference in accordance with a set of well-defined formal rules. To address this, we propose a new NLP task, causal inference in natural language, inspired by the\"causal inference engine\"postulated by Judea Pearl et al. We compose a large dataset, CLadder, with 10K samples: based on a collection of causal graphs and queries (associational, interventional, and counterfactual), we obtain symbolic questions and ground-truth answers, through an oracle causal inference engine. These are then translated into natural language. We evaluate multiple LLMs on our dataset, and we introduce and evaluate a bespoke chain-of-thought prompting strategy, CausalCoT. We show that our task is highly challenging for LLMs, and we conduct an in-depth analysis to gain deeper insights into the causal reasoning abilities of LLMs. Our data is open-sourced at https://huggingface.co/datasets/causalNLP/cladder, and our code can be found at https://github.com/causalNLP/cladder.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates whether large language models (LLMs) can coherently reason about causality, and proposes a new NLP task, causal inference in natural language, inspired by the \"causal inference engine\" proposed by Judea Pearl et al."
            },
            "score": 4
        },
        {
            "id": "498d1406fc4cddb05cd46477793f2e726a6fe238",
            "paperId": "498d1406fc4cddb05cd46477793f2e726a6fe238",
            "title": "The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code",
            "abstract": "Causal reasoning, the ability to identify cause-and-effect relationship, is crucial in human thinking. Although large language models (LLMs) succeed in many NLP tasks, it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning. Given the fact that programming code may express causal relations more often and explicitly with conditional statements like ``if``, we want to explore whether Code-LLMs acquire better causal reasoning abilities. Our experiments show that compared to text-only LLMs, Code-LLMs with code prompts are significantly better in causal reasoning. We further intervene on the prompts from different aspects, and discover that the programming structure is crucial in code prompt design, while Code-LLMs are robust towards format perturbations.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experiments show that compared to text-only LLMs, Code-LLMs with code prompts are significantly better in causal reasoning, and that the programming structure is crucial in code prompt design, while Code- LLMs are robust towards format perturbations."
            },
            "score": 4
        },
        {
            "id": "d3637f23d15e71bc1b16948a4c29f08c90b8c772",
            "paperId": "d3637f23d15e71bc1b16948a4c29f08c90b8c772",
            "title": "PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning",
            "abstract": "Procedural planning, which entails decomposing a high-level goal into a sequence of temporally ordered steps, is an important yet intricate task for machines. It involves integrating common-sense knowledge to reason about complex contextualized situations that are often counterfactual, e.g.\"scheduling a doctor's appointment without a phone\". While current approaches show encouraging results using large language models (LLMs), they are hindered by drawbacks such as costly API calls and reproducibility issues. In this paper, we advocate planning using smaller language models. We present PlaSma, a novel two-pronged approach to endow small language models with procedural knowledge and (counterfactual) planning capabilities. More concretely, we develop symbolic procedural knowledge distillation to enhance the implicit knowledge in small language models and an inference-time algorithm to facilitate more structured and accurate reasoning. In addition, we introduce a novel task, Counterfactual Planning, that requires a revision of a plan to cope with a counterfactual situation. In both the original and counterfactual setting, we show that orders-of-magnitude smaller models (770M-11B parameters) can compete and often surpass their larger teacher models' capabilities.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "PlaSma is presented, a novel two-pronged approach to endow small language models with procedural knowledge and (counterfactual) planning capabilities and symbolic procedural knowledge distillation is developed to enhance the implicit knowledge insmall language models and an inference-time algorithm to facilitate more structured and accurate reasoning."
            },
            "score": 4
        },
        {
            "id": "f30b720e34d405f200270a6ef2d09e98585fb4d1",
            "paperId": "f30b720e34d405f200270a6ef2d09e98585fb4d1",
            "title": "CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models",
            "abstract": "The ability to perform causal reasoning is widely considered a core feature of intelligence. In this work, we investigate whether large language models (LLMs) can coherently reason about causality. Much of the existing work in natural language processing (NLP) focuses on evaluating commonsense causal reasoning in LLMs, thus failing to assess whether a model can perform causal inference in accordance with a set of well-defined formal rules . To address this, we propose a new NLP task, causal inference in natural language , inspired by the \u201ccausal inference engine\u201d postulated by Judea Pearl et al. We compose a large dataset, CL ADDER , with 10K samples: based on a collection of causal graphs and queries (associational, interventional, and counterfactual), we obtain symbolic questions and ground-truth answers, through an oracle causal inference engine. These are then translated into natural language. We evaluate multiple LLMs on our dataset, and we introduce and evaluate a bespoke chain-of-thought prompting strategy, C AUSAL C O T. We show that our task is highly challenging for LLMs, and we conduct an in-depth analysis to gain deeper insight into the causal reasoning abilities of LLMs. 1",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates whether large language models (LLMs) can coherently reason about causality, and proposes a new NLP task, causal inference in natural language, inspired by the \u201ccausal inference engine\u201d postulated by Judea Pearl et al."
            },
            "score": 4
        },
        {
            "id": "d1eb051c6b13eba8a9b333d5ee0a55250717195d",
            "paperId": "d1eb051c6b13eba8a9b333d5ee0a55250717195d",
            "title": "Debiasing NLU Models via Causal Intervention and Counterfactual Reasoning",
            "abstract": "Recent studies have shown that strong Natural Language Understanding (NLU) models are prone to relying on annotation biases of the datasets as a shortcut, which goes against the underlying mechanisms of the task of interest. To reduce such biases, several recent works introduce debiasing methods to regularize the training process of targeted NLU models. In this paper, we provide a new perspective with causal inference to find out the bias. On one hand, we show that there is an unobserved confounder for the natural language utterances and their respective classes, leading to spurious correlations from training data. To remove such confounder, the backdoor adjustment with causal intervention is utilized to find the true causal effect, which makes the training process fundamentally different from the traditional likelihood estimation. On the other hand, in inference process, we formulate the bias as the direct causal effect and remove it by pursuing the indirect causal effect with counterfactual reasoning. We conduct experiments on large-scale natural language inference and fact verification benchmarks, evaluating on bias sensitive datasets that are specifically designed to assess the robustness of models against known biases in the training data. Experimental results show that our proposed debiasing framework outperforms previous state-of-the-art debiasing methods while maintaining the original in-distribution performance.",
            "year": 2022,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper shows that there is an unobserved confounder for the natural language utterances and their respective classes, leading to spurious correlations from training data, and provides a new perspective with causal inference to find out the bias."
            },
            "score": 4
        },
        {
            "id": "df0db04d870e1666a64a9c92688419e7628423e5",
            "paperId": "df0db04d870e1666a64a9c92688419e7628423e5",
            "title": "Is Knowledge All Large Language Models Needed for Causal Reasoning?",
            "abstract": "This paper explores the causal reasoning of large language models (LLMs) to enhance their interpretability and reliability in advancing artificial intelligence. Despite the proficiency of LLMs in a range of tasks, their potential for understanding causality requires further exploration. We propose a novel causal attribution model that utilizes\"do-operators\"for constructing counterfactual scenarios, allowing us to systematically quantify the influence of input numerical data and LLMs' pre-existing knowledge on their causal reasoning processes. Our newly developed experimental setup assesses LLMs' reliance on contextual information and inherent knowledge across various domains. Our evaluation reveals that LLMs' causal reasoning ability depends on the context and domain-specific knowledge provided, and supports the argument that\"knowledge is, indeed, what LLMs principally require for sound causal reasoning\". On the contrary, in the absence of knowledge, LLMs still maintain a degree of causal reasoning using the available numerical data, albeit with limitations in the calculations.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel causal attribution model is proposed that utilizes \"do-operators\" for constructing counterfactual scenarios, allowing to systematically quantify the influence of input numerical data and LLMs' pre-existing knowledge on their causal reasoning processes."
            },
            "score": 4
        },
        {
            "id": "ff2eecb21972eb287064f98db1a4487c62bd7566",
            "paperId": "ff2eecb21972eb287064f98db1a4487c62bd7566",
            "title": "MenatQA: A New Dataset for Testing the Temporal Comprehension and Reasoning Abilities of Large Language Models",
            "abstract": "Large language models (LLMs) have shown nearly saturated performance on many natural language processing (NLP) tasks. As a result, it is natural for people to believe that LLMs have also mastered abilities such as time understanding and reasoning. However, research on the temporal sensitivity of LLMs has been insufficiently emphasized. To fill this gap, this paper constructs Multiple Sensitive Factors Time QA (MenatQA), which encompasses three temporal factors (scope factor, order factor, counterfactual factor) with total 2,853 samples for evaluating the time comprehension and reasoning abilities of LLMs. This paper tests current mainstream LLMs with different parameter sizes, ranging from billions to hundreds of billions. The results show most LLMs fall behind smaller temporal reasoning models with different degree on these factors. In specific, LLMs show a significant vulnerability to temporal biases and depend heavily on the temporal information provided in questions. Furthermore, this paper undertakes a preliminary investigation into potential improvement strategies by devising specific prompts and leveraging external tools. These approaches serve as valuable baselines or references for future research endeavors.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "MenatQA constructs Multiple Sensitive Factors Time QA, which encompasses three temporal factors (scope factor, order factor, counterfactual factor) with total 2,853 samples for evaluating the time comprehension and reasoning abilities of LLMs."
            },
            "score": 4
        },
        {
            "id": "d7386e8859b22e05ce9c4a972613d4b1e1e44198",
            "paperId": "d7386e8859b22e05ce9c4a972613d4b1e1e44198",
            "title": "Prompting Large Language Models With the Socratic Method",
            "abstract": "This paper presents a systematic approach to using the Socratic method in developing prompt templates that effectively interact with large language models, including GPT-3. Various methods are examined, and those that yield precise answers and justifications while fostering creativity and imagination to enhance creative writing are identified. Techniques such as definition, elenchus, dialectic, maieutics, generalization, and counterfactual reasoning are discussed for their application in engineering prompt templates and their connections to inductive, deductive, and abductive reasoning. Through examples, the effectiveness of these dialogue and reasoning methods is demonstrated. An interesting observation is made that when the task's goal and user intent are conveyed to GPT-3 via ChatGPT before the start of a dialogue, the large language model seems to connect to the external context expressed in the intent and perform more effectively.",
            "year": 2023,
            "citationCount": 20,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An interesting observation is made that when the task's goal and user intent are conveyed to GPT-3 via ChatGPT before the start of a dialogue, the large language model seems to connect to the external context expressed in the intent and perform more effectively."
            },
            "score": 4
        },
        {
            "id": "dd1d5906ccf9725bde74eb0de1fb4b5e01a5c0cc",
            "paperId": "dd1d5906ccf9725bde74eb0de1fb4b5e01a5c0cc",
            "title": "Concept-Guided Chain-of-Thought Prompting for Pairwise Comparison Scaling of Texts with Large Language Models",
            "abstract": "Existing text scaling methods often require a large corpus, struggle with short texts, or require labeled data. We develop a text scaling method that leverages the pattern recognition capabilities of generative large language models (LLMs). Specifically, we propose concept-guided chain-of-thought (CGCoT), which uses prompts designed to summarize ideas and identify target parties in texts to generate concept-specific breakdowns, in many ways similar to guidance for human coder content analysis. CGCoT effectively shifts pairwise text comparisons from a reasoning problem to a pattern recognition problem. We then pairwise compare concept-specific breakdowns using an LLM. We use the results of these pairwise comparisons to estimate a scale using the Bradley-Terry model. We use this approach to scale affective speech on Twitter. Our measures correlate more strongly with human judgments than alternative approaches like Wordfish. Besides a small set of pilot data to develop the CGCoT prompts, our measures require no additional labeled data and produce binary predictions comparable to a RoBERTa-Large model fine-tuned on thousands of human-labeled tweets. We demonstrate how combining substantive knowledge with LLMs can create state-of-the-art measures of abstract concepts.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes concept-guided chain-of-thought (CGCoT), which uses prompts designed to summarize ideas and identify target parties in texts to generate concept-specific breakdowns, in many ways similar to guidance for human coder content analysis."
            },
            "score": 4
        },
        {
            "id": "aefa4aeaaa8802be7559e39c9176e9857ec526ad",
            "paperId": "aefa4aeaaa8802be7559e39c9176e9857ec526ad",
            "title": "Simulating Social Media Using Large Language Models to Evaluate Alternative News Feed Algorithms",
            "abstract": "Social media is often criticized for amplifying toxic discourse and discouraging constructive conversations. But designing social media platforms to promote better conversations is inherently challenging. This paper asks whether simulating social media through a combination of Large Language Models (LLM) and Agent-Based Modeling can help researchers study how different news feed algorithms shape the quality of online conversations. We create realistic personas using data from the American National Election Study to populate simulated social media platforms. Next, we prompt the agents to read and share news articles - and like or comment upon each other's messages - within three platforms that use different news feed algorithms. In the first platform, users see the most liked and commented posts from users whom they follow. In the second, they see posts from all users - even those outside their own network. The third platform employs a novel\"bridging\"algorithm that highlights posts that are liked by people with opposing political views. We find this bridging algorithm promotes more constructive, non-toxic, conversation across political divides than the other two models. Though further research is needed to evaluate these findings, we argue that LLMs hold considerable potential to improve simulation research on social media and many other complex social settings.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is argued that LLMs hold considerable potential to improve simulation research on social media and many other complex social settings and can help researchers study how different news feed algorithms shape the quality of online conversations."
            },
            "score": 4
        },
        {
            "id": "ac28a021b91ff98fe657185425a5d8807a2164d1",
            "paperId": "ac28a021b91ff98fe657185425a5d8807a2164d1",
            "title": "Clinical Prompt Learning With Frozen Language Models.",
            "abstract": "When the first transformer-based language models were published in the late 2010s, pretraining with general text and then fine-tuning the model on a task-specific dataset often achieved the state-of-the-art performance. However, more recent work suggests that for some tasks, directly prompting the pretrained model matches or surpasses fine-tuning in performance with few or no model parameter updates required. The use of prompts with language models for natural language processing (NLP) tasks is known as prompt learning. We investigated the viability of prompt learning on clinically meaningful decision tasks and directly compared this with more traditional fine-tuning methods. Results show that prompt learning methods were able to match or surpass the performance of traditional fine-tuning with up to 1000 times fewer trainable parameters, less training time, less training data, and lower computation resource requirements. We argue that these characteristics make prompt learning a very desirable alternative to traditional fine-tuning for clinical tasks, where the computational resources of public health providers are limited, and where data can often not be made available or not be used for fine-tuning due to patient privacy concerns. The complementary code to reproduce the experiments presented in this work can be found at https://github.com/NtaylorOX/Public_Clinical_Prompt.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigated the viability of prompt learning on clinically meaningful decision tasks and directly compared this with more traditional fine-tuning methods, and showed that prompt learning methods were able to match or surpass the performance of traditional Finetuning with up to 1000 times fewer trainable parameters, less training time, lessTraining data, and lower computation resource requirements."
            },
            "score": 4
        },
        {
            "id": "2406d2b5369d03b3bad307e420d3c40ec0ea4951",
            "paperId": "2406d2b5369d03b3bad307e420d3c40ec0ea4951",
            "title": "Towards Coding Social Science Datasets with Language Models",
            "abstract": "Researchers often rely on humans to code (label, annotate, etc.) large sets of texts. This kind of human coding forms an important part of social science research, yet the coding process is both resource intensive and highly variable from application to application. In some cases, efforts to automate this process have achieved human-level accuracies, but to achieve this, these attempts frequently rely on thousands of hand-labeled training examples, which makes them inapplicable to small-scale research studies and costly for large ones. Recent advances in a specific kind of artificial intelligence tool - language models (LMs) - provide a solution to this problem. Work in computer science makes it clear that LMs are able to classify text, without the cost (in financial terms and human effort) of alternative methods. To demonstrate the possibilities of LMs in this area of political science, we use GPT-3, one of the most advanced LMs, as a synthetic coder and compare it to human coders. We find that GPT-3 can match the performance of typical human coders and offers benefits over other machine learning methods of coding text. We find this across a variety of domains using very different coding procedures. This provides exciting evidence that language models can serve as a critical advance in the coding of open-ended texts in a variety of applications.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "GPT-3, one of the most advanced LMs, is used as a synthetic coder and compared to typical human coders to provide exciting evidence that language models can serve as a critical advance in the coding of open-ended texts in a variety of applications."
            },
            "score": 4
        },
        {
            "id": "e1935c70b1d8d00e6bd925a5a45af67bd1384fa4",
            "paperId": "e1935c70b1d8d00e6bd925a5a45af67bd1384fa4",
            "title": "GPT-3.5, GPT-4, or BARD? Evaluating LLMs Reasoning Ability in Zero-Shot Setting and Performance Boosting Through Prompts",
            "abstract": "Large Language Models (LLMs) have exhibited remarkable performance on various Natural Language Processing (NLP) tasks. However, there is a current hot debate regarding their reasoning capacity. In this paper, we examine the performance of GPT-3.5, GPT-4, and BARD models, by performing a thorough technical evaluation on different reasoning tasks across eleven distinct datasets. Our paper provides empirical evidence showcasing the superior performance of ChatGPT-4 in comparison to both ChatGPT-3.5 and BARD in zero-shot setting throughout almost all evaluated tasks. While the superiority of GPT-4 compared to GPT-3.5 might be explained by its larger size and NLP efficiency, this was not evident for BARD. We also demonstrate that the three models show limited proficiency in Inductive, Mathematical, and Multi-hop Reasoning Tasks. To bolster our findings, we present a detailed and comprehensive analysis of the results from these three models. Furthermore, we propose a set of engineered prompts that enhances the zero-shot setting performance of all three models.",
            "year": 2023,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper provides empirical evidence showcasing the superior performance of ChatGPT-4 in comparison to both ChatG PT-3.5 and BARD in zero-shot setting throughout almost all evaluated tasks."
            },
            "score": 4
        },
        {
            "id": "a1a91c24f25598f437596acf9d81b0ff182a6190",
            "paperId": "a1a91c24f25598f437596acf9d81b0ff182a6190",
            "title": "Natural Language based Context Modeling and Reasoning with LLMs: A Tutorial",
            "abstract": "Large language models (LLMs) have become phenomenally surging 1 , since 2018 \u2013 two decades after introducing context-awareness into computing systems [1\u20134]. Through taking into account the situations of ubiquitous devices, users and the societies, context-aware computing has enabled a wide spectrum of innovative applications, such as assisted living, location-based social network services and so on. To recognize contexts and make decisions for actions accordingly, various artificial intelligence technologies, such as Ontology and OWL, have been adopted as representations for context modeling and reasoning [5, 6]. Recently, with the rise of LLMs and their improved natural language understanding and reasoning capabilities, it has become feasible to model contexts using natural language and perform context reasoning by interacting with LLMs such as ChatGPT and GPT-4. In this tutorial, we demonstrate the use of texts, prompts, and autonomous agents (AutoAgents) that enable LLMs to perform context modeling and reasoning without requiring fine-tuning of the model. We organize and introduce works in the related field, and name this computing paradigm as the LLM-driven Context-aware Computing ( LCaC ). In the LCaC paradigm, users\u2019 requests, sensors reading data, and the command to actuators are supposed to be represented as texts. Given the text of users\u2019 request and sensor data, the AutoAgent models the context by prompting and sends to the LLM for context reasoning. LLM generates a plan of actions and responds to the AutoAgent, which later follows the action plan to foster context-awareness. To prove the concepts, we use two showcases \u2013 (1) operating a mobile z-arm in an apartment for assisted living, and (2) planning a trip and scheduling the itinerary in a context-aware and personalized manner. Furthermore, we analyze several factors that might affect the performance of LLM-driven context-awareness, and then discuss the",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This tutorial demonstrates the use of texts, prompts, and autonomous agents that enable LLMs to perform context modeling and reasoning without requiring fine-tuning of the model and organizes and introduces works in the related field."
            },
            "score": 4
        },
        {
            "id": "b2ee9afe5142bd47163c4261a544e77a622023e0",
            "paperId": "b2ee9afe5142bd47163c4261a544e77a622023e0",
            "title": "Relevant or Random: Can LLMs Truly Perform Analogical Reasoning?",
            "abstract": "Analogical reasoning is a unique ability of humans to address unfamiliar challenges by transferring strategies from relevant past experiences. One key finding in psychology is that compared with irrelevant past experiences, recalling relevant ones can help humans better handle new tasks. Coincidentally, the NLP community has also recently found that self-generating relevant examples in the context can help large language models (LLMs) better solve a given problem than hand-crafted prompts. However, it is yet not clear whether relevance is the key factor eliciting such capability, i.e., can LLMs benefit more from self-generated relevant examples than irrelevant ones? In this work, we systematically explore whether LLMs can truly perform analogical reasoning on a diverse set of reasoning tasks. With extensive experiments and analysis, we show that self-generated random examples can surprisingly achieve comparable or even better performance, e.g., 4% performance boost on GSM8K with random biological examples. We find that the accuracy of self-generated examples is the key factor and subsequently design two improved methods with significantly reduced inference costs. Overall, we aim to advance a deeper understanding of LLM analogical reasoning and hope this work stimulates further research in the design of self-generated contexts.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work systematically explores whether LLMs can truly perform analogical reasoning on a diverse set of reasoning tasks and finds that the accuracy of self-generated examples is the key factor and subsequently design two improved methods with significantly reduced inference costs."
            },
            "score": 4
        },
        {
            "id": "276bfc5052f967a99a4a3730319f21c43096f8d5",
            "paperId": "276bfc5052f967a99a4a3730319f21c43096f8d5",
            "title": "Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs",
            "abstract": "In this paper, we explore a new way for user targeting, where non-expert marketers could select their target users solely given demands in natural language form. The key to this issue is how to transform natural languages into practical structured logical languages, i.e., the structured understanding of marketer demands. Considering the impressive natural language processing ability of large language models (LLMs), we try to leverage LLMs to solve this issue. Past research indicates that the reasoning ability of LLMs can be effectively enhanced through chain-of-thought (CoT) prompting. But existing methods still have some limitations: (1) Previous methods either use simple\"Let's think step by step\"spells or provide fixed examples in demonstrations without considering compatibility between prompts and questions, making LLMs ineffective in some complex reasoning tasks such as structured language transformation. (2) Previous methods are often implemented in closed-source models or excessively large models, which is not suitable in industrial practical scenarios. Based on these, we propose ARALLM (i.e., Analogical Reasoning Augmented Large Language Models) consisting of two modules: Analogical Reasoning based Prompting and Reasoning-Augmented Multi-Task Model Distillation.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new way for user targeting, where non-expert marketers could select their target users solely given demands in natural language form is explored, consisting of two modules: Analogical Reasoning based Prompting and Reasoning-Augmented Multi-Task Model Distillation."
            },
            "score": 4
        },
        {
            "id": "a26fa1983e4bc7c5b55cd5a1296afe6f876baa03",
            "paperId": "a26fa1983e4bc7c5b55cd5a1296afe6f876baa03",
            "title": "Neuro-Symbolic Integration Brings Causal and Reliable Reasoning Proofs",
            "abstract": "Though prompting LLMs with various reasoning structures produces reasoning proofs along with answers, these proofs are not ensured to be causal and reliable due to the inherent defects of LLMs. Tracking such deficiencies, we present a neuro-symbolic integration method, in which a neural LLM is used to represent the knowledge of the problem while an LLM-free symbolic solver is adopted to do deliberative reasoning using the knowledge. Specifically, our customized meta-interpreters allow the production of reasoning proofs and support flexible search strategies. These reasoning proofs are ensured to be causal and reliable because of the deterministic executing nature of the symbolic solvers. Empirically, on ProofWriter, our method surpasses the CoT baseline by nearly double in accuracy and more than triple in proof similarity. On GSM8K, our method also shows accuracy improvements and nearly doubled proof similarity. Our code is released at https://github.com/DAMO-NLP-SG/CaRing",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A neuro-symbolic integration method is presented, in which a neural LLM is used to represent the knowledge of the problem while an LLM-free symbolic solver is adopted to do deliberative reasoning using the knowledge."
            },
            "score": 4
        },
        {
            "id": "e43e95f706762b64d4ae17a5eb72731d2bd9047a",
            "paperId": "e43e95f706762b64d4ae17a5eb72731d2bd9047a",
            "title": "Learning To Teach Large Language Models Logical Reasoning",
            "abstract": "Large language models (LLMs) have gained enormous attention from both academia and industry, due to their exceptional ability in language generation and extremely powerful generalization. However, current LLMs still output unreliable content in practical reasoning tasks due to their inherent issues (e.g., hallucination). To better disentangle this problem, in this paper, we conduct an in-depth investigation to systematically explore the capability of LLMs in logical reasoning. More in detail, we first investigate the deficiency of LLMs in logical reasoning on different tasks, including event relation extraction and deductive reasoning. Our study demonstrates that LLMs are not good reasoners in solving tasks with rigorous reasoning and will produce counterfactual answers, which require us to iteratively refine. Therefore, we comprehensively explore different strategies to endow LLMs with logical reasoning ability, and thus enable them to generate more logically consistent answers across different scenarios. Based on our approach, we also contribute a synthesized dataset (LLM-LR) involving multi-hop reasoning for evaluation and pre-training. Extensive quantitative and qualitative analyses on different tasks also validate the effectiveness and necessity of teaching LLMs with logic and provide insights for solving practical tasks with LLMs in future work.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study comprehensively explore different strategies to endow LLMs with logical reasoning ability, and thus enable them to generate more logically consistent answers across different scenarios."
            },
            "score": 3
        },
        {
            "id": "aace515abd7c6d1b0a6c98a0e321800c71561bae",
            "paperId": "aace515abd7c6d1b0a6c98a0e321800c71561bae",
            "title": "Prompting and Fine-tuning Pre-trained Generative Language Models",
            "abstract": "There has been an explosion of available pre-trained and fine-tuned Generative Language Models (LM). They vary in the number of parameters, architecture, training strategy, and training set size. Aligned with it, alternative strategies exist to exploit these models, such as Fine-tuning and Prompt Engineering. However, many questions may arise throughout this process: Which model to apply for a given task? Which strategies to use? Will Prompt Engineering solve all tasks? What are the computational and financial costs involved? This tutorial will introduce and explore typical modern LM architectures with a hands-on approach to the available strategies.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This tutorial will introduce and explore typical modern LM architectures with a hands-on approach to the available strategies."
            },
            "score": 3
        },
        {
            "id": "f0295393100c1b55c75ab3d0719827546fbf41cc",
            "paperId": "f0295393100c1b55c75ab3d0719827546fbf41cc",
            "title": "Respectful or Toxic? Using Zero-Shot Learning with Language Models to Detect Hate Speech",
            "abstract": "Hate speech detection faces two significant challenges: 1) the limited availability of labeled data and 2) the high variability of hate speech across different contexts and languages. Prompting brings a ray of hope to these challenges. It allows injecting a model with task-specific knowledge without relying on labeled data. This paper explores zero-shot learning with prompting for hate speech detection. We investigate how well zero-shot learning can detect hate speech in 3 languages with limited labeled data. We experiment with various large language models and verbalizers on 8 benchmark datasets. Our findings highlight the impact of prompt selection on the results. They also suggest that prompting, specifically with recent large language models, can achieve performance comparable to and surpass fine-tuned models, making it a promising alternative for under-resourced languages. Our findings highlight the potential of prompting for hate speech detection and show how both the prompt and the model have a significant impact on achieving more accurate predictions in this task.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper investigates how well zero-shot learning with prompting for hate speech detection in 3 languages with limited labeled data and suggests that prompting can achieve performance comparable to and surpass fine-tuned models, making it a promising alternative for under-resourced languages."
            },
            "score": 3
        },
        {
            "id": "4895d443c36bd136a818be2db34442354ba408d1",
            "paperId": "4895d443c36bd136a818be2db34442354ba408d1",
            "title": "TagGPT: Large Language Models are Zero-shot Multimodal Taggers",
            "abstract": "Tags are pivotal in facilitating the effective distribution of multimedia content in various applications in the contemporary Internet era, such as search engines and recommendation systems. Recently, large language models (LLMs) have demonstrated impressive capabilities across a wide range of tasks. In this work, we propose TagGPT, a fully automated system capable of tag extraction and multimodal tagging in a completely zero-shot fashion. Our core insight is that, through elaborate prompt engineering, LLMs are able to extract and reason about proper tags given textual clues of multimodal data, e.g., OCR, ASR, title, etc. Specifically, to automatically build a high-quality tag set that reflects user intent and interests for a specific application, TagGPT predicts large-scale candidate tags from a series of raw data via prompting LLMs, filtered with frequency and semantics. Given a new entity that needs tagging for distribution, TagGPT introduces two alternative options for zero-shot tagging, i.e., a generative method with late semantic matching with the tag set, and another selective method with early matching in prompts. It is well noticed that TagGPT provides a system-level solution based on a modular framework equipped with a pre-trained LLM (GPT-3.5 used here) and a sentence embedding model (SimCSE used here), which can be seamlessly replaced with any more advanced one you want. TagGPT is applicable for various modalities of data in modern social media and showcases strong generalization ability to a wide range of applications. We evaluate TagGPT on publicly available datasets, i.e., Kuaishou and Food.com, and demonstrate the effectiveness of TagGPT compared to existing hashtags and off-the-shelf taggers. Project page: https://github.com/TencentARC/TagGPT.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes TagGPT, a fully automated system capable of tag extraction and multimodal tagging in a completely zero-shot fashion, and provides a system-level solution based on a modular framework equipped with a pre-trained LLM and a sentence embedding model."
            },
            "score": 3
        },
        {
            "id": "17322c997e993bee1ec05309e5c8d45799a0e4b8",
            "paperId": "17322c997e993bee1ec05309e5c8d45799a0e4b8",
            "title": "Beyond Words: A Mathematical Framework for Interpreting Large Language Models",
            "abstract": "Large language models (LLMs) are powerful AI tools that can generate and comprehend natural language text and other complex information. However, the field lacks a mathematical framework to systematically describe, compare and improve LLMs. We propose Hex a framework that clarifies key terms and concepts in LLM research, such as hallucinations, alignment, self-verification and chain-of-thought reasoning. The Hex framework offers a precise and consistent way to characterize LLMs, identify their strengths and weaknesses, and integrate new findings. Using Hex, we differentiate chain-of-thought reasoning from chain-of-thought prompting and establish the conditions under which they are equivalent. This distinction clarifies the basic assumptions behind chain-of-thought prompting and its implications for methods that use it, such as self-verification and prompt programming. Our goal is to provide a formal framework for LLMs that can help both researchers and practitioners explore new possibilities for generative AI. We do not claim to have a definitive solution, but rather a tool for opening up new research avenues. We argue that our formal definitions and results are crucial for advancing the discussion on how to build generative AI systems that are safe, reliable, fair and robust, especially in domains like healthcare and software engineering.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Hex is proposed a framework that clarifies key terms and concepts in LLM research, such as hallucinations, alignment, self-verification and chain-of-thought reasoning, and offers a precise and consistent way to characterize LLMs, identify their strengths and weaknesses, and integrate new findings."
            },
            "score": 3
        },
        {
            "id": "dfffba50d7630f1e68d9cc67d4a9a1c6519b93cd",
            "paperId": "dfffba50d7630f1e68d9cc67d4a9a1c6519b93cd",
            "title": "Text Style Transfer Evaluation Using Large Language Models",
            "abstract": "Evaluating Text Style Transfer (TST) is a complex task due to its multifaceted nature. The quality of the generated text is measured based on challenging factors, such as style transfer accuracy, content preservation, and overall fluency. While human evaluation is considered to be the gold standard in TST assessment, it is costly and often hard to reproduce. Therefore, automated metrics are prevalent in these domains. Nevertheless, it remains unclear whether these automated metrics correlate with human evaluations. Recent strides in Large Language Models (LLMs) have showcased their capacity to match and even exceed average human performance across diverse, unseen tasks. This suggests that LLMs could be a feasible alternative to human evaluation and other automated metrics in TST evaluation. We compare the results of different LLMs in TST using multiple input prompts. Our findings highlight a strong correlation between (even zero-shot) prompting and human evaluation, showing that LLMs often outperform traditional automated metrics. Furthermore, we introduce the concept of prompt ensembling, demonstrating its ability to enhance the robustness of TST evaluation. This research contributes to the ongoing evaluation of LLMs in diverse tasks, offering insights into successful outcomes and areas of limitation.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A strong correlation between (even zero-shot) prompting and human evaluation is highlighted, showing that LLMs often outperform traditional automated metrics in TST evaluation, and the concept of prompt ensembling is introduced, demonstrating its ability to enhance the robustness of T ST evaluation."
            },
            "score": 3
        },
        {
            "id": "1fc0e5b30bfede1b78389d00f8c41bacd29ecd7f",
            "paperId": "1fc0e5b30bfede1b78389d00f8c41bacd29ecd7f",
            "title": "The Utility of Large Language Models and Generative AI for Education Research",
            "abstract": "The use of natural language processing (NLP) techniques in engineering education can provide valuable insights into the underlying processes involved in generating text. While accessing these insights can be labor-intensive if done manually, recent advances in NLP and large language models have made it a realistic option for individuals. This study explores and evaluates a combination of clustering, summarization, and prompting techniques to analyze over 1,000 student essays in which students discussed their career interests. The specific assignment prompted students to define and explain their career goals as engineers. Using text embedding representations of student responses, we clustered the responses together to identify thematically similar statements from students. The clustered responses were then summarized to quickly identify career interest themes. We also used a set of a priori codes about career satisfaction and sectors to demonstrate an alternative approach to using these generative text models to analyze student writing. The results of this study demonstrate the feasibility and usefulness of NLP techniques in engineering education research. By automating the initial analysis of student essays, researchers and educators can more efficiently and accurately identify key themes and patterns in student writing. The methods presented in this paper have broader applications for engineering education and research purposes beyond analyzing student essays. By explaining these methods to the engineering education community, readers can utilize them in their own contexts.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A combination of clustering, summarization, and prompting techniques to analyze over 1,000 student essays in which students discussed their career interests demonstrate the feasibility and usefulness of NLP techniques in engineering education research."
            },
            "score": 3
        },
        {
            "id": "1b0e3360b3341fc411a6c7841173a8d78ac2ab43",
            "paperId": "1b0e3360b3341fc411a6c7841173a8d78ac2ab43",
            "title": "Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs",
            "abstract": "Recent works have showcased the ability of LLMs to embody diverse personas in their responses, exemplified by prompts like 'You are Yoda. Explain the Theory of Relativity.' While this ability allows personalization of LLMs and enables human behavior simulation, its effect on LLMs' capabilities remains unclear. To fill this gap, we present the first extensive study of the unintended side-effects of persona assignment on the ability of LLMs to perform basic reasoning tasks. Our study covers 24 reasoning datasets, 4 LLMs, and 19 diverse personas (e.g. an Asian person) spanning 5 socio-demographic groups. Our experiments unveil that LLMs harbor deep rooted bias against various socio-demographics underneath a veneer of fairness. While they overtly reject stereotypes when explicitly asked ('Are Black people less skilled at mathematics?'), they manifest stereotypical and erroneous presumptions when asked to answer questions while adopting a persona. These can be observed as abstentions in responses, e.g., 'As a Black person, I can't answer this question as it requires math knowledge', and generally result in a substantial performance drop. Our experiments with ChatGPT-3.5 show that this bias is ubiquitous - 80% of our personas demonstrate bias; it is significant - some datasets show performance drops of 70%+; and can be especially harmful for certain groups - some personas suffer statistically significant drops on 80%+ of the datasets. Overall, all 4 LLMs exhibit this bias to varying extents, with GPT-4-Turbo showing the least but still a problematic amount of bias (evident in 42% of the personas). Further analysis shows that these persona-induced errors can be hard-to-discern and hard-to-avoid. Our findings serve as a cautionary tale that the practice of assigning personas to LLMs - a trend on the rise - can surface their deep-rooted biases and have unforeseeable and detrimental side-effects.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experiments unveil that LLMs harbor deep rooted bias against various socio-demographics underneath a veneer of fairness, and serve as a cautionary tale that the practice of assigning personas to LLMs - a trend on the rise - can surface their deep-rooted biases and have unforeseeable and detrimental side-effects."
            },
            "score": 3
        },
        {
            "id": "0333cf45841955d19de5096d5fd0ee55fdcd15ad",
            "paperId": "0333cf45841955d19de5096d5fd0ee55fdcd15ad",
            "title": "Human-Centric Autonomous Systems With LLMs for User Command Reasoning",
            "abstract": "The evolution of autonomous driving has made remark-able advancements in recent years, evolving into a tangible reality. However, a human-centric large-scale adoption hinges on meeting a variety of multifaceted requirements. To ensure that the autonomous system meets the user's intent, it is essential to accurately discern and interpret user commands, especially in complex or emergency situations. To this end, we propose to leverage the reasoning capabilities of Large Language Models (LLMs) to infer system requirements from in-cabin users' commands. Through a series of experiments that include different LLM models and prompt designs, we explore the few-shot mul-tivariate binary classification accuracy of system requirements from natural language textual commands. We confirm the general ability of LLMs to understand and reason about prompts but underline that their effectiveness is conditioned on the quality of both the LLM model and the design of appropriate sequential prompts. Code and models are public with the link ht tps: / / github. com/KTH-RPL/Dri veCmd_LLM.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes to leverage the reasoning capabilities of Large Language Models to infer system requirements from in-cabin users' commands and confirms the general ability of LLMs to understand and reason about prompts but underline that their effectiveness is conditioned on the quality of both the LLM model and the design of appropriate sequential prompts."
            },
            "score": 3
        },
        {
            "id": "bf8491bef353df126e2306ad2fe4b898697b906a",
            "paperId": "bf8491bef353df126e2306ad2fe4b898697b906a",
            "title": "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",
            "abstract": "This paper proposes a framework for quantitatively evaluating interactive LLMs such as ChatGPT using publicly available data sets. We carry out an extensive technical evaluation of ChatGPT using 23 data sets covering 8 different common NLP application tasks. We evaluate the multitask, multilingual and multi-modal aspects of ChatGPT based on these data sets and a newly designed multimodal dataset. We find that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks. We find that it is better at understanding non-Latin script languages than generating them. It is able to generate multimodal content from textual prompts, via an intermediate code generation step. Moreover, we find that ChatGPT is 63.41% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense reasoning, hence making it an unreliable reasoner. It is, for example, better at deductive than inductive reasoning. ChatGPT suffers from hallucination problems like other LLMs and it generates more extrinsic hallucinations from its parametric memory as it does not have access to an external knowledge base. Finally, the interactive feature of ChatGPT enables human collaboration with the underlying LLM to improve its performance, i.e, 8% ROUGE-1 on summarization and 2% ChrF++ on machine translation, in a multi-turn\"prompt engineering\"fashion. We also release codebase for evaluation set extraction.",
            "year": 2023,
            "citationCount": 754,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks and is better at understanding non-Latin script languages than generating them."
            },
            "score": 3
        },
        {
            "id": "9ffefdf1fcd780cb71450b0a7a29247c66aa87be",
            "paperId": "9ffefdf1fcd780cb71450b0a7a29247c66aa87be",
            "title": "The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning",
            "abstract": "Does prompting a large language model (LLM) like GPT-3 with explanations improve in-context learning? We study this question on two NLP tasks that involve reasoning over text, namely question answering and natural language inference. We test the performance of four LLMs on three textual reasoning datasets using prompts that include explanations in multiple different styles. For these tasks, we find that including explanations in the prompts for OPT, GPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to moderate accuracy improvements over standard few-show learning. However, text-davinci-002 is able to benefit more substantially. We further show that explanations generated by the LLMs may not entail the models' predictions nor be factually grounded in the input, even on simple tasks with extractive explanations. However, these flawed explanations can still be useful as a way to verify LLMs' predictions post-hoc. Through analysis in our three settings, we show that explanations judged by humans to be good--logically consistent with the input and the prediction--more likely cooccur with accurate predictions. Following these observations, we train calibrators using automatically extracted scores that assess the reliability of explanations, allowing us to improve performance post-hoc across all of our datasets.",
            "year": 2022,
            "citationCount": 95,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work studies two NLP tasks that involve reasoning over text, namely question answering and natural language inference, and shows that explanations judged by humans to be good--logically consistent with the input and the prediction--more likely cooccur with accurate predictions."
            },
            "score": 3
        },
        {
            "id": "bbffc1d5b4a8786d6a33080204e5a35590a08389",
            "paperId": "bbffc1d5b4a8786d6a33080204e5a35590a08389",
            "title": "BLCU-NLP at SemEval-2020 Task 5: Data Augmentation for Efficient Counterfactual Detecting",
            "abstract": "Counterfactuals describe events counter to facts and hence naturally involve common sense, knowledge, and reasoning. SemEval 2020 task 5 is focusing on this field. We participate in the subtask 1 and we use BERT as our system. Our Innovations are feature extraction and data augmentation. We extract and summarize features of counterfactual statements, augment counterfactual examples in training set with the help of these features, and two general methods of data augmentation is experimented in our work. We demonstrate the effectiveness of our approaches, which achieves 0.95 of subtask 1 in F1 while using only a subset of giving training set to fine-tune the BERT model, and our official submission achieves F1 0.802, which ranks us 16th in the competition.",
            "year": 2020,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The effectiveness of the approaches is demonstrated, which achieves 0.95 of subtask 1 in F1 while using only a subset of giving training set to fine-tune the BERT model, and the official submission achieves F1 0.802, which ranks us 16th in the competition."
            },
            "score": 3
        },
        {
            "id": "1e2eba005ccd8ab7a668a525c5b43245853bdaf1",
            "paperId": "1e2eba005ccd8ab7a668a525c5b43245853bdaf1",
            "title": "Reasoning in Large Language Models Through Symbolic Math Word Problems",
            "abstract": "Large language models (LLMs) have revolutionized NLP by solving downstream tasks with little to no labeled data. Despite their versatile abilities, the larger question of their ability to reason remains ill-understood. This paper addresses reasoning in math word problems (MWPs) by studying symbolic versions of the numeric problems, since a symbolic expression is a\"concise explanation\"of the numeric answer. We create and use a symbolic version of the SVAMP dataset and find that GPT-3's davinci-002 model also has good zero-shot accuracy on symbolic MWPs. To evaluate the faithfulness of the model's reasoning, we go beyond accuracy and additionally evaluate the alignment between the final answer and the outputted reasoning, which correspond to numeric and symbolic answers respectively for MWPs. We explore a self-prompting approach to encourage the symbolic reasoning to align with the numeric answer, thus equipping the LLM with the ability to provide a concise and verifiable reasoning and making it more interpretable. Surprisingly, self-prompting also improves the symbolic accuracy to be higher than both the numeric and symbolic accuracies, thus providing an ensembling effect. The SVAMP_Sym dataset will be released for future research on symbolic math problems.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A self-prompting approach is explored to encourage the symbolic reasoning to align with the numeric answer, thus equipping the LLM with the ability to provide a concise and verifiable reasoning and making it more interpretable."
            },
            "score": 3
        },
        {
            "id": "d96997265f8146e93b4c9350f19d55e46d1317f0",
            "paperId": "d96997265f8146e93b4c9350f19d55e46d1317f0",
            "title": "ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering",
            "abstract": "With the recent advance in large pre-trained language models, researchers have achieved record performances in NLP tasks that mostly focus on language pattern matching. The community is experiencing the shift of the challenge from how to model language to the imitation of complex reasoning abilities like human beings. In this work, we investigate the application domain of finance that involves real-world, complex numerical reasoning. We propose a new large-scale dataset, ConvFinQA, aiming to study the chain of numerical reasoning in conversational question answering. Our dataset poses great challenge in modeling long-range, complex numerical reasoning paths in real-world conversations. We conduct comprehensive experiments and analyses with both the neural symbolic methods and the prompting-based methods, to provide insights into the reasoning mechanisms of these two divisions. We believe our new dataset should serve as a valuable resource to push forward the exploration of real-world, complex reasoning tasks as the next research focus. Our dataset and code is publicly available at https://github.com/czyssrs/ConvFinQA.",
            "year": 2022,
            "citationCount": 44,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a new large-scale dataset, ConvFinQA, aiming to study the chain of numerical reasoning in conversational question answering, and conducts comprehensive experiments and analyses with both the neural symbolic methods and the prompting-based methods to provide insights into the reasoning mechanisms."
            },
            "score": 3
        },
        {
            "id": "1bd9466f0bb10d29a16f614943ec7823e13cb210",
            "paperId": "1bd9466f0bb10d29a16f614943ec7823e13cb210",
            "title": "Mixed Distillation Helps Smaller Language Model Better Reasoning",
            "abstract": "While large language models (LLMs) have demonstrated exceptional performance in recent natural language processing (NLP) tasks, their deployment poses substantial challenges due to high computational and memory demands in real-world applications. Recent studies have focused on enhancing smaller models through knowledge distillation from LLMs, yielding promising results. However, these models often struggle to match the performance of LLMs, especially in tasks that require reasoning. In this work, we introduce Mixed Distillation (MD) framework, which capitalizes on the strengths of Program of Thought (PoT) and Chain of Thought (CoT) capabilities within LLMs, combining multiple prompting techniques and distilling these capabilities into smaller models. Our experimental results show that MD significantly enhances the single-path and multi-path reasoning ability of smaller models in various tasks. In terms of accuracy and generality of reasoning tasks, the model generated by it exceeds the comprehensive performance of two individually distilled models. Notably, LLaMA2-7B and CodeLlama-7B using MD achieved remarkable improvements of (84.5%) and (85.5%), respectively, outperforming GPT-3.5-Turbo by (2.5%) and (3.5%), on the SVAMP benchmark.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces Mixed Distillation (MD) framework, which capitalizes on the strengths of Program of Thought (PoT) and Chain of Thought (CoT) capabilities within LLMs, combining multiple prompting techniques and distilling these capabilities into smaller models."
            },
            "score": 3
        },
        {
            "id": "f63f37d981e2c36b1ea35f2025e55adee7906f69",
            "paperId": "f63f37d981e2c36b1ea35f2025e55adee7906f69",
            "title": "Prompting Large Language Models with Speech Recognition Abilities",
            "abstract": "Large language models have proven themselves highly flexible, able to solve a wide range of generative tasks, such as abstractive summarization and open-ended question answering. In this paper we extend the capabilities of LLMs by directly attaching a small audio encoder allowing it to perform speech recognition. By directly prepending a sequence of audial embeddings to the text token embeddings, the LLM can be converted to an automatic speech recognition (ASR) system, and be used in the exact same manner as its textual counterpart. Experiments on Multilingual LibriSpeech (MLS) show that incorporating a conformer encoder into the open sourced LLaMA-7B allows it to outperform monolingual baselines by 18% and perform multilingual speech recognition despite LLaMA being trained overwhelmingly on English text. Furthermore, we perform ablation studies to investigate whether the LLM can be completely frozen during training to maintain its original capabilities, scaling up the audio encoder, and increasing the audio encoder striding to generate fewer embeddings. The results from these studies show that multilingual ASR is possible even when the LLM is frozen or when strides of almost 1 second are used in the audio encoder opening up the possibility for LLMs to operate on long-form audio.",
            "year": 2023,
            "citationCount": 32,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The capabilities of LLMs are extended by directly attaching a small audio encoder allowing it to perform speech recognition and it is shown that multilingual ASR is possible even when the LLM is frozen or when strides of almost 1 second are used in theaudio encoder opening up the possibility for LLMs to operate on long-form audio."
            },
            "score": 2
        },
        {
            "id": "5a85761d32442b6eef8de353d356d777eb72c3eb",
            "paperId": "5a85761d32442b6eef8de353d356d777eb72c3eb",
            "title": "\u201cThe less I type, the better\u201d: How AI Language Models can Enhance or Impede Communication for AAC Users",
            "abstract": "Users of augmentative and alternative communication (AAC) devices sometimes find it difficult to communicate in real time with others due to the time it takes to compose messages. AI technologies such as large language models (LLMs) provide an opportunity to support AAC users by improving the quality and variety of text suggestions. However, these technologies may fundamentally change how users interact with AAC devices as users transition from typing their own phrases to prompting and selecting AI-generated phrases. We conducted a study in which 12 AAC users tested live suggestions from a language model across three usage scenarios: extending short replies, answering biographical questions, and requesting assistance. Our study participants believed that AI-generated phrases could save time, physical and cognitive effort when communicating, but felt it was important that these phrases reflect their own communication style and preferences. This work identifies opportunities and challenges for future AI-enhanced AAC devices.",
            "year": 2023,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work identifies opportunities and challenges for future AI-enhanced AAC devices by testing live suggestions from a language model across three usage scenarios: extending short replies, answering biographical questions, and requesting assistance."
            },
            "score": 2
        },
        {
            "id": "cd084de509a38c7f2dad5fa7de9da0d49956e53c",
            "paperId": "cd084de509a38c7f2dad5fa7de9da0d49956e53c",
            "title": "Performance of Large Language Models in a Computer Science Degree Program",
            "abstract": "Large language models such as ChatGPT-3.5 and GPT-4.0 are ubiquitous and dominate the current discourse. Their transformative capabilities have led to a paradigm shift in how we interact with and utilize (text-based) information. Each day, new possibilities to leverage the capabilities of these models emerge. This paper presents findings on the performance of different large language models in a university of applied sciences' undergraduate computer science degree program. Our primary objective is to assess the effectiveness of these models within the curriculum by employing them as educational aids. By prompting the models with lecture material, exercise tasks, and past exams, we aim to evaluate their proficiency across different computer science domains. We showcase the strong performance of current large language models while highlighting limitations and constraints within the context of such a degree program. We found that ChatGPT-3.5 averaged 79.9% of the total score in 10 tested modules, BingAI achieved 68.4%, and LLaMa, in the 65 billion parameter variant, 20%. Despite these convincing results, even GPT-4.0 would not pass the degree program - due to limitations in mathematical calculations.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper showcases the strong performance of current large language models while highlighting limitations and constraints within the context of such a degree program, and finds that ChatGPT-3.5 would not pass the degree program - due to limitations in mathematical calculations."
            },
            "score": 2
        },
        {
            "id": "89fe385d004afa512332db6ff92bad07a96a1492",
            "paperId": "89fe385d004afa512332db6ff92bad07a96a1492",
            "title": "Haiku Generation with Large Language Models",
            "abstract": "Generating poetry and other creative open-ended texts has been a long-standing challenge in the field of Natural Language Processing. In this paper, we present an approach to fine-tuning Large Language Models, specifically OpenAI\u2019s GPT-3 and Meta\u2019s OPT-125M, to generate haikus, a traditional form of Japanese poetry that consists of three lines and a total of 17 syllables. Our goal is to generate haikus that not only follow the traditional syllabic structure but also convey poetic imagery and evoke an emotional response in the reader. We evaluate our approach based on perplexity metrics in addition to qualitative human evaluation. We believe that our approach opens up new possibilities for using Large Language Models to generate creative and emotionally resonant text; our results show us that while it can be difficult for models to learn the structure of the haiku, they are surprisingly poetic after being fine-tuned on tens of thousands of haikus and using certain sampling and prompting techniques",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An approach to fine-tuning Large Language Models, specifically OpenAI\u2019s GPT-3 and Meta\u2019s OPT-125M, to generate haikus, a traditional form of Japanese poetry that consists of three lines and a total of 17 syllables is presented."
            },
            "score": 2
        },
        {
            "id": "5851bf82b0a9db2de86828f62c3006a6e1b40798",
            "paperId": "5851bf82b0a9db2de86828f62c3006a6e1b40798",
            "title": "Team UTSA-NLP at SemEval 2024 Task 5: Prompt Ensembling for Argument Reasoning in Civil Procedures with GPT4",
            "abstract": "In this paper, we present our system for the SemEval Task 5, The Legal Argument Reasoning Task in Civil Procedure Challenge. Legal argument reasoning is an essential skill that all law students must master. Moreover, it is important to develop natural language processing solutions that can reason about a question given terse domain-specific contextual information. Our system explores a prompt-based solution using GPT4 to reason over legal arguments. We also evaluate an ensemble of prompting strategies, including chain-of-thought reasoning and in-context learning. Overall, our system results in a Macro F1 of .8095 on the validation dataset and .7315 (5th out of 21 teams) on the final test set. Code for this project is available at https://github.com/danschumac1/CivilPromptReasoningGPT4.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This system explores a prompt-based solution using GPT4 to reason over legal arguments, including chain-of-thought reasoning and in-context learning, and evaluates an ensemble of prompting strategies, including chain-of-thought reasoning and in-context learning."
            },
            "score": 2
        },
        {
            "id": "6ebf05664e14777d6b4d8e4aaf50efcaf5bbf29c",
            "paperId": "6ebf05664e14777d6b4d8e4aaf50efcaf5bbf29c",
            "title": "Possibilities to Utilize Large Language Models in Detection and Mitigation of Limitations of Currently Available Neurocognitive Assessment Batteries",
            "abstract": "Neurocognitive assessment batteries play a crucial role in evaluating cognitive abilities and identifying potential impairments or cognitive decline. However, these assessments may suffer from limitations and biases associated with specific tasks, such as drawing a clock, copying a cube, and recalling words. In this research paper, we explore the potential utilization of large language models in identifying and mitigating these limitations. We discuss the biases introduced by these tasks and propose the incorporation of alternative assessment methods. Furthermore, we examine the feasibility of utilizing large language models, such as the ChatGPT, to address these limitations and enhance the inclusivity and accuracy of cognitive evaluations. By leveraging the capabilities of large language models, we aim to provide a comprehensive framework for improving neurocognitive assessment batteries.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The feasibility of utilizing large language models, such as the ChatGPT, to address limitations and biases associated with specific tasks and enhance the inclusivity and accuracy of cognitive evaluations is examined."
            },
            "score": 1
        }
    ],
    "novelty": "yes"
}