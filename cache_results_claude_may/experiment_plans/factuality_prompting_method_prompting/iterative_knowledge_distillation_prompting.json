{
    "topic_description": "novel prompting methods that can improve factuality and reduce hallucination of large language models",
    "idea_name": "Iterative Knowledge Distillation Prompting",
    "raw_idea": {
        "Problem": "Large language models can generate hallucinated content that is not supported by the input context or external knowledge, leading to factual inaccuracies in the generated output.",
        "Existing Methods": "Existing methods for reducing hallucination include using retrieval-augmented generation, incorporating external knowledge bases, or using reinforcement learning with factual consistency rewards.",
        "Motivation": "We hypothesize that large language models can iteratively distill factual knowledge from their own generated output, if prompted to do so with appropriate instructions. By explicitly prompting the model to extract factual knowledge from its generated output and conditioning future generation on the extracted knowledge, we can encourage the model to generate more factually consistent output over multiple iterations.",
        "Proposed Method": "We propose Iterative Knowledge Distillation Prompting, a multi-step prompting approach to reduce hallucination. First, we prompt the model to generate an initial output given the input context. Second, we prompt the model to extract factual knowledge from its generated output, in the form of concise factual statements or knowledge triples. Third, we prompt the model to generate a revised output conditioned on both the original input context and the extracted factual knowledge. We repeat steps 2-3 for multiple iterations, with the model distilling factual knowledge from its own generated output and conditioning future generation on the accumulated knowledge.",
        "Experiment Plan": "We will evaluate our proposed method on factual consistency benchmarks such as the Fact Extraction and VERification (FEVER) dataset and the Fact-Checking Assistant task. We will compare our method with baseline prompting approaches without knowledge distillation, as well as state-of-the-art methods for reducing hallucination. We will also analyze the quality and accuracy of the distilled factual knowledge across multiple iterations."
    },
    "full_experiment_plan": {
        "Title": "Iterative Knowledge Distillation Prompting for Reducing Hallucination in Large Language Models",
        "Problem Statement": "Large language models can generate hallucinated content that is not supported by the input context or external knowledge, leading to factual inaccuracies in the generated output.",
        "Motivation": "Existing methods for reducing hallucination, such as retrieval-augmented generation, incorporating external knowledge bases, or using reinforcement learning with factual consistency rewards, often require additional training or external resources. We hypothesize that large language models can iteratively distill factual knowledge from their own generated output, if prompted to do so with appropriate instructions. By explicitly prompting the model to extract factual knowledge from its generated output and conditioning future generation on the extracted knowledge, we can encourage the model to generate more factually consistent output over multiple iterations, without the need for external resources or additional training.",
        "Proposed Method": "We propose Iterative Knowledge Distillation Prompting, a multi-step prompting approach to reduce hallucination. First, we prompt the model to generate an initial output given the input context. Second, we prompt the model to extract factual knowledge from its generated output, in the form of concise factual statements or knowledge triples. Third, we prompt the model to generate a revised output conditioned on both the original input context and the extracted factual knowledge. We repeat steps 2-3 for multiple iterations, with the model distilling factual knowledge from its own generated output and conditioning future generation on the accumulated knowledge.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We evaluate our proposed method on factual consistency benchmarks such as the Fact Extraction and VERification (FEVER) dataset and the Fact-Checking Assistant task. FEVER contains claims that need to be verified against a corpus of Wikipedia articles, while the Fact-Checking Assistant task involves generating factual corrections for a given claim using evidence from Wikipedia.",
            "Step 2: Construct Prompts": "We design a set of prompts for each step of our proposed method:\n1. Initial Generation Prompt: \"Given the following input, generate a relevant output: [INPUT]\"\n2. Knowledge Extraction Prompt: \"Extract concise factual statements or knowledge triples from the following generated output: [GENERATED_OUTPUT]\"\n3. Revised Generation Prompt: \"Given the following input and extracted factual knowledge, generate a revised output: [INPUT] [EXTRACTED_KNOWLEDGE]\"\nWe also include a stopping criterion prompt to determine when to stop the iterative process: \"Based on the extracted factual knowledge and the revised output, should we perform another iteration of knowledge distillation and output revision? (Yes/No)\"",
            "Step 3: Select Models": "We experiment with GPT-3.5 (text-davinci-002) and GPT-4 models from OpenAI, as well as the open-source FLAN-T5-XXL model. This allows us to compare the effectiveness of our method across different model sizes and architectures.",
            "Step 4: Run Experiments": "For each dataset and model combination, we run the following experiments:\n1. Baseline: Generate outputs directly from the input context, without any knowledge distillation or iterative refinement.\n2. Iterative Knowledge Distillation: Apply our proposed method with a fixed number of iterations (e.g., 3) for all examples.\n3. Iterative Knowledge Distillation with Adaptive Stopping: Apply our proposed method with the stopping criterion prompt to adaptively determine the number of iterations for each example.\nWe evaluate the generated outputs using factual consistency metrics such as FEVER score and FactCC.",
            "Step 5: Analyze Results": "We compare the performance of our proposed method against the baseline and state-of-the-art methods for reducing hallucination. We also analyze the quality and accuracy of the distilled factual knowledge across multiple iterations, and investigate the impact of the stopping criterion on the overall performance and efficiency of the method."
        },
        "Test Case Examples": {
            "Test Case 1": {
                "Input": "Claim: The movie Interstellar was directed by Christopher Nolan.",
                "Baseline Output": "The claim is true. Interstellar is a 2014 science fiction film directed by Christopher Nolan. The movie stars Matthew McConaughey, Anne Hathaway, and Jessica Chastain. It follows a team of astronauts who travel through a wormhole near Saturn in search of a new home for humanity. Nolan co-wrote the screenplay with his brother Jonathan Nolan. The movie was a critical and commercial success, grossing over $700 million worldwide and receiving several Academy Award nominations, including for Best Visual Effects and Best Original Score.",
                "Iterative Knowledge Distillation Output - Iteration 1": {
                    "Extracted Knowledge": "1. Interstellar is a 2014 science fiction film. 2. Interstellar was directed by Christopher Nolan. 3. Christopher Nolan co-wrote the screenplay with his brother Jonathan Nolan.",
                    "Revised Output": "The claim is true. Christopher Nolan directed the 2014 science fiction film Interstellar, which he co-wrote with his brother Jonathan Nolan. The movie was a critical and commercial success."
                },
                "Iterative Knowledge Distillation Output - Iteration 2": {
                    "Extracted Knowledge": "1. Christopher Nolan directed the 2014 science fiction film Interstellar. 2. Christopher Nolan co-wrote the screenplay for Interstellar with his brother Jonathan Nolan.",
                    "Revised Output": "The claim is true. Christopher Nolan directed the 2014 science fiction film Interstellar, which he co-wrote with his brother Jonathan Nolan."
                },
                "Explanation": "The baseline output contains some irrelevant information about the plot and cast of the movie, which is not directly related to verifying the claim. The iterative knowledge distillation process helps to filter out this irrelevant information and focus on the key facts that support the claim, resulting in a more concise and factually consistent output."
            },
            "Test Case 2": {
                "Input": "Claim: Barack Obama was born in Kenya.",
                "Baseline Output": "The claim is false. Barack Obama, the 44th President of the United States, was born on August 4, 1961, in Honolulu, Hawaii. He was born to an American mother, Ann Dunham, and a Kenyan father, Barack Obama Sr. However, some conspiracy theories have falsely claimed that Obama was born in Kenya, despite clear evidence to the contrary, such as his Hawaiian birth certificate and birth announcements in local newspapers.",
                "Iterative Knowledge Distillation Output - Iteration 1": {
                    "Extracted Knowledge": "1. Barack Obama was born on August 4, 1961, in Honolulu, Hawaii. 2. Barack Obama's mother, Ann Dunham, was American. 3. Barack Obama's father, Barack Obama Sr., was Kenyan.",
                    "Revised Output": "The claim is false. Barack Obama was born on August 4, 1961, in Honolulu, Hawaii, to an American mother and a Kenyan father. Despite this, some conspiracy theories have falsely claimed that Obama was born in Kenya."
                },
                "Iterative Knowledge Distillation Output - Iteration 2": {
                    "Extracted Knowledge": "1. Barack Obama was born on August 4, 1961, in Honolulu, Hawaii. 2. Some conspiracy theories have falsely claimed that Obama was born in Kenya.",
                    "Revised Output": "The claim is false. Barack Obama was born on August 4, 1961, in Honolulu, Hawaii. Some conspiracy theories have falsely claimed that Obama was born in Kenya, despite clear evidence to the contrary."
                },
                "Explanation": "The baseline output contains some tangentially relevant information about Obama's parents, which is not necessary for verifying the claim. The iterative knowledge distillation process helps to identify the key facts that directly refute the claim, resulting in a more focused and factually consistent output."
            }
        },
        "Fallback Plan": "If the proposed Iterative Knowledge Distillation Prompting method does not significantly improve factual consistency compared to the baselines, we can explore the following alternative approaches:\n1. Analyze the quality and relevance of the extracted factual knowledge to identify potential issues with the knowledge extraction prompt or process. This may inform improvements to the prompting strategy or the need for additional filtering or post-processing of the extracted knowledge.\n2. Experiment with different prompting strategies for the initial generation and revised generation steps, such as using more specific or targeted prompts to encourage the model to focus on factual information.\n3. Investigate the impact of different stopping criteria or iteration strategies, such as using a dynamic stopping criterion based on the similarity between the extracted knowledge and the revised output, or using a different number of iterations for different types of claims or inputs.\n4. Conduct a more in-depth error analysis to identify common patterns or types of hallucinations that are not effectively addressed by the current method, and use these insights to inform the development of targeted prompting strategies or post-processing techniques.\nIf the proposed method and alternative approaches do not yield satisfactory results, we can pivot the project to focus on analyzing the limitations and challenges of using prompting-based methods for reducing hallucination in large language models. This could involve a more systematic evaluation of different prompting strategies, an investigation into the types and sources of hallucinations generated by the models, and a discussion of potential future directions for addressing this problem, such as combining prompting-based methods with other techniques like retrieval-augmented generation or explicit fact-checking."
    },
    "novelty_queries": [
        "KeywordQuery(\"iterative knowledge distillation language models\")",
        "KeywordQuery(\"prompting factual consistency language models\")",
        "KeywordQuery(\"reducing hallucination language models\")",
        "KeywordQuery(\"Iterative Knowledge Distillation Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "6e09ec84a235bb3465aeb97645d7999ce257bb0a",
            "paperId": "6e09ec84a235bb3465aeb97645d7999ce257bb0a",
            "title": "Multistage Collaborative Knowledge Distillation from a Large Language Model for Semi-Supervised Sequence Generation",
            "abstract": "We study semi-supervised sequence generation tasks, where the few labeled examples are too scarce to finetune a model, and meanwhile, few-shot prompted large language models (LLMs) exhibit room for improvement. In this paper, we present the discovery that a student model distilled from a few-shot prompted LLM can commonly generalize better than its teacher to unseen examples on such tasks. We find that the student is able to learn a general pattern from the high-quality pseudolabels produced by the teacher during knowledge distillation (KD), and favorably not a general pattern from the low-quality pseudolables. Leveraging this discovery, we propose a new method, Multistage Collaborative Knowledge Distillation from an LLM (MCKD), for these tasks. MCKD first few-shot prompts an LLM to produce pseudolabels for unlabeled data. Then at each stage of an iterative KD process, a new pair of students is trained on disjoint partitions of the pseudolabeled data, and produces new and improved pseudolabels for their unseen partitions. We conduct extensive experiments on four syntactic and semantic parsing datasets and show the effectiveness of MCKD for low-resource semi-supervised sequence generation. On CRAFT biomedical parsing, for example, 3-stage MCKD with 50 labeled examples outperforms an LLM teacher and vanilla KD by 7.5% and 3.7% parsing F1, respectively, and matches the performance of supervised finetuning with 500 labeled examples.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that the student is able to learn a general pattern from the high-quality pseudolabels produced by the teacher during knowledge distillation (KD), and favorably not a general pattern from the low-quality pseudolables."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to reduce hallucination in large language models by iteratively prompting the model to distill factual knowledge from its own generated output and conditioning future generation on the extracted knowledge.\n\nThe paper focuses on improving semi-supervised sequence generation tasks by distilling knowledge from a few-shot prompted large language model to a student model through an iterative process, where the student models learn from the pseudolabels produced by the teacher and other students.\n\nWhile both the project proposal and the paper involve iterative knowledge distillation, the project proposal focuses on reducing hallucination in large language models, while the paper aims to improve semi-supervised sequence generation tasks. The project proposal uses self-distillation from the model's own output, whereas the paper distills knowledge from a teacher model to student models.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "b90d733a310465eb7ccd16d8a7b8b254ad866d69",
            "paperId": "b90d733a310465eb7ccd16d8a7b8b254ad866d69",
            "title": "ISD-QA: Iterative Distillation of Commonsense Knowledge from General Language Models for Unsupervised Question Answering",
            "abstract": "Commonsense question answering has primarily been tackled through supervised transfer learning, where a language model pre-trained on large amounts of data is used as the starting point. While successful, the approach requires large amounts of labeled question-answer pairs, with increasingly larger amounts of data required as the complexity of scenarios or tasks such as commonsense QA increases. In this paper, we hypothesize that large-scale pre-training of language models encodes the necessary commonsense knowledge to answer common questions in context without labeled data. We propose a novel framework called Iterative Self Distillation for QA (ISD-QA), which extracts the \"dark knowledge\" encoded during largescale pre-training of language models to provide supervision for commonsense question answering. We show that the approach can be used to train common neural QA models for commonsense question answering by distilling knowledge from language models in an unsupervised manner. With no bells and whistles, we achieve an average of 68% of the performance of fully supervised QA models while requiring no labeled training data. Extensive experiments on three public benchmarks (OpenBookQA, HellaSWAG, and CommonsenseQA) show the effectiveness of the proposed approach.",
            "year": 2022,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel framework called Iterative Self Distillation for QA (ISD-QA), which extracts the \"dark knowledge\" encoded during largescale pre-training of language models to provide supervision for commonsense question answering by distilling knowledge from language models in an unsupervised manner."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is reducing hallucination in large language models, and the proposed approach is iterative knowledge distillation prompting. The research problem in the paper is unsupervised commonsense question answering, and the proposed approach is iterative self-distillation of knowledge from pre-trained language models.\n\nWhile both works involve iterative distillation of knowledge from language models, the research problems and goals are different. The proposal focuses on reducing hallucination in generated outputs, while the paper aims to enable unsupervised question answering by extracting commonsense knowledge from pre-trained models.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "524fb3ee0cf8fd04570036cc8a1b31c9a8ddc534",
            "paperId": "524fb3ee0cf8fd04570036cc8a1b31c9a8ddc534",
            "title": "Evolving Knowledge Distillation with Large Language Models and Active Learning",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities across various NLP tasks. However, their computational costs are prohibitively high. To address this issue, previous research has attempted to distill the knowledge of LLMs into smaller models by generating annotated data. Nonetheless, these works have mainly focused on the direct use of LLMs for text generation and labeling, without fully exploring their potential to comprehend the target task and acquire valuable knowledge. In this paper, we propose EvoKD: Evolving Knowledge Distillation, which leverages the concept of active learning to interactively enhance the process of data generation using large language models, simultaneously improving the task capabilities of small domain model (student model). Different from previous work, we actively analyze the student model's weaknesses, and then synthesize labeled samples based on the analysis. In addition, we provide iterative feedback to the LLMs regarding the student model's performance to continuously construct diversified and challenging samples. Experiments and analysis on different NLP tasks, namely, text classification and named entity recognition show the effectiveness of EvoKD.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "EvoKD is proposed, which leverages the concept of active learning to interactively enhance the process of data generation using large language models, simultaneously improving the task capabilities of small domain model (student model)."
            },
            "score": 6,
            "novelty_score": "The research problem in the project proposal is reducing hallucination in large language models, and the proposed approach is iterative knowledge distillation prompting.\n\nThe research problem in the paper is reducing the computational cost of large language models, and the proposed approach is evolving knowledge distillation with active learning.\n\nWhile both works involve knowledge distillation, the project proposal focuses on reducing hallucination in the output of large language models, while the paper aims to reduce the computational cost of large language models by distilling their knowledge into smaller models. The approaches also differ, with the project proposal using iterative prompting and the paper using active learning.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "5aec6865043cb7c7f281699ae95652e0ff680f09",
            "paperId": "5aec6865043cb7c7f281699ae95652e0ff680f09",
            "title": "CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning",
            "abstract": "The sequential process of conceptualization and instantiation is essential to generalizable commonsense reasoning as it allows the application of existing knowledge to unfamiliar scenarios. However, existing works tend to undervalue the step of instantiation and heavily rely on pre-built concept taxonomies and human annotations to collect both types of knowledge, resulting in a lack of instantiated knowledge to complete reasoning, high cost, and limited scalability. To tackle these challenges, we introduce CANDLE, a distillation framework that iteratively performs contextualized conceptualization and instantiation over commonsense knowledge bases by instructing large language models to generate both types of knowledge with critic filtering. By applying CANDLE to ATOMIC, we construct a comprehensive knowledge base comprising six million conceptualizations and instantiated commonsense knowledge triples. Both types of knowledge are firmly rooted in the original ATOMIC dataset, and intrinsic evaluations demonstrate their exceptional quality and diversity. Empirical results indicate that distilling CANDLE on student models provides benefits across four downstream tasks. Our code, data, and models are publicly available at https://github.com/HKUST-KnowComp/CANDLE.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "CANDLE is introduced, a distillation framework that iteratively performs contextualized conceptualization and instantiation over commonsense knowledge bases by instructing large language models to generate both types of knowledge with critic filtering."
            },
            "score": 6,
            "novelty_score": "The research problem in the project proposal is reducing hallucination in large language models, and the proposed approach is iterative knowledge distillation prompting. The research problem in the paper is improving commonsense reasoning, and the proposed approach is iterative conceptualization and instantiation distillation from large language models.\n\nThe project proposal focuses on reducing factual inaccuracies in the output of large language models, while the paper aims to improve commonsense reasoning by distilling conceptualized and instantiated knowledge. Although both use iterative distillation from large language models, the target problems and the specific methods differ.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "5785dde53577bf8431215426f1470b87f4f0fc53",
            "paperId": "5785dde53577bf8431215426f1470b87f4f0fc53",
            "title": "GOLD: Generalized Knowledge Distillation via Out-of-Distribution-Guided Language Data Generation",
            "abstract": "Knowledge distillation from LLMs is essential for the efficient deployment of language models. Prior works have proposed data generation using LLMs for preparing distilled models. We argue that generating data with LLMs is prone to sampling mainly from the center of original content distribution. This limitation hinders the distilled model from learning the true underlying data distribution and to forget the tails of the distributions (samples with lower probability). To this end, we propose GOLD, a task-agnostic data generation and knowledge distillation framework, which employs an iterative out-of-distribution-guided feedback mechanism for the LLM. As a result, the generated data improves the generalizability of distilled models. An energy-based OOD evaluation approach is also introduced to deal with noisy generated data. Our extensive experiments on 10 different classification and sequence-to-sequence tasks in NLP show that GOLD respectively outperforms prior arts and the LLM with an average improvement of 5% and 14%. We will also show that the proposed method is applicable to less explored and novel tasks. The code is available.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "GOLD is proposed, a task-agnostic data generation and knowledge distillation framework, which employs an iterative out-of-distribution-guided feedback mechanism for the LLM, which improves the generalizability of distilled models."
            },
            "score": 6,
            "novelty_score": "The research problem in the project proposal is reducing hallucination in large language models, and the proposed approach is iterative knowledge distillation prompting. The research problem in the paper is improving the generalizability of distilled models, and the proposed approach is out-of-distribution-guided data generation.\n\nThe project proposal focuses on reducing hallucination in large language models, while the paper aims to improve the generalizability of distilled models. The project proposes iterative knowledge distillation prompting, whereas the paper introduces an out-of-distribution-guided feedback mechanism for data generation.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "55990bdf4c0e8dc5ae28a87d2a659ee4b7f56703",
            "paperId": "55990bdf4c0e8dc5ae28a87d2a659ee4b7f56703",
            "title": "What Makes it Ok to Set a Fire? Iterative Self-distillation of Contexts and Rationales for Disambiguating Defeasible Social and Moral Situations",
            "abstract": "Moral or ethical judgments rely heavily on the specific contexts in which they occur. Understanding varying shades of defeasible contextualizations (i.e., additional information that strengthens or attenuates the moral acceptability of an action) is critical to accurately represent the subtlety and intricacy of grounded human moral judgment in real-life scenarios. We introduce defeasible moral reasoning: a task to provide grounded contexts that make an action more or less morally acceptable, along with commonsense rationales that justify the reasoning. To elicit high-quality task data, we take an iterative self-distillation approach that starts from a small amount of unstructured seed knowledge from GPT-3 and then alternates between (1) self-distillation from student models; (2) targeted filtering with a critic model trained by human judgment (to boost validity) and NLI (to boost diversity); (3) self-imitation learning (to amplify the desired data quality). This process yields a student model that produces defeasible contexts with improved validity, diversity, and defeasibility. From this model we distill a high-quality dataset, \\delta-Rules-of-Thumb, of 1.2M entries of contextualizations and rationales for 115K defeasible moral actions rated highly by human annotators 85.9% to 99.8% of the time. Using \\delta-RoT we obtain a final student model that wins over all intermediate student models by a notable margin.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces defeasible moral reasoning: a task to provide grounded contexts that make an action more or less morally acceptable, along with commonsense rationales that justify the reasoning."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is reducing hallucination in large language models, and the proposed approach is iterative knowledge distillation prompting. The research problem in the paper is disambiguating defeasible social and moral situations, and the proposed approach is iterative self-distillation of contexts and rationales.\n\nThe proposal focuses on improving factual consistency in language model outputs, while the paper focuses on understanding the nuances of moral judgments in different contexts. The methods also differ: the proposal uses prompting to distill factual knowledge, while the paper uses self-distillation to generate diverse and valid contextualizations and rationales.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "152d9a231c00d4495c9bc4a466f42165ce2e2164",
            "paperId": "152d9a231c00d4495c9bc4a466f42165ce2e2164",
            "title": "Evaluating Factual Consistency of Summaries with Large Language Models",
            "abstract": "Detecting factual errors in summaries has been an important and challenging subject in summarization research. Inspired by the emergent ability of large language models (LLMs), we explore evaluating factual consistency of summaries by directly prompting LLMs. We present a comprehensive empirical study to assess the ability of LLMs as factual consistency evaluators, which consists of (1) analyzing different LLMs such as the GPT model series and Flan-T5; (2) investigating a variety of prompting methods including vanilla prompting, chain-of-thought prompting, and a sentence-by-sentence prompting method to tackle long summaries; and (3) evaluating on diverse summaries generated by multiple summarization systems, ranging from pre-transformer methods to SOTA pretrained models. Our experiments demonstrate that prompting LLMs is able to outperform the previous best factuality systems in all settings, by up to 12.2 absolute points in terms of the binary classification accuracy on inconsistency detection.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A comprehensive empirical study to assess the ability of LLMs as factual consistency evaluators, which consists of analyzing different LLMs such as the GPT model series and Flan-T5 and investigating a variety of prompting methods including vanilla prompting, chain-of-thought prompting, and a sentence-by-sentence prompting method to tackle long summaries."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is reducing hallucination in large language models, and the proposed approach is iterative knowledge distillation prompting. The research problem in the paper is evaluating factual consistency of summaries, and the proposed approach is directly prompting large language models.\n\nThe proposal focuses on generating factually consistent outputs from language models, while the paper focuses on evaluating the factual consistency of given summaries. Although both works involve factual consistency and large language models, their goals and methods are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "03764434729b83d4f04a8bd02f99f2500cd5bbae",
            "paperId": "03764434729b83d4f04a8bd02f99f2500cd5bbae",
            "title": "Teaching Language Models to Hallucinate Less with Synthetic Tasks",
            "abstract": "Large language models (LLMs) frequently hallucinate on abstractive summarization tasks such as document-based question-answering, meeting summarization, and clinical report generation, even though all necessary information is included in context. However, optimizing LLMs to hallucinate less on these tasks is challenging, as hallucination is hard to efficiently evaluate at each optimization step. In this work, we show that reducing hallucination on a synthetic task can also reduce hallucination on real-world downstream tasks. Our method, SynTra, first designs a synthetic task where hallucinations are easy to elicit and measure. It next optimizes the LLM's system message via prefix-tuning on the synthetic task, and finally transfers the system message to realistic, hard-to-optimize tasks. Across three realistic abstractive summarization tasks, SynTra reduces hallucination for two 13B-parameter LLMs using only a synthetic retrieval task for supervision. We also find that optimizing the system message rather than the model weights can be critical; fine-tuning the entire model on the synthetic task can counterintuitively increase hallucination. Overall, SynTra demonstrates that the extra flexibility of working with synthetic data can help mitigate undesired behaviors in practice.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Across three realistic abstractive summarization tasks, SynTra reduces hallucination for two 13B-parameter LLMs using only a synthetic retrieval task for supervision, demonstrating that the extra flexibility of working with synthetic data can help mitigate undesired behaviors in practice."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is reducing hallucination in large language models, and the proposed approach is iterative knowledge distillation prompting without additional training or external resources.\n\nThe research problem in the paper is also reducing hallucination in large language models, but the proposed approach is using synthetic tasks to optimize the model's system message via prefix-tuning and transferring it to real-world tasks.\n\nWhile both works aim to reduce hallucination in large language models, the proposal focuses on an iterative prompting method without additional training, while the paper proposes using synthetic tasks for optimization and transfer learning. The approaches are quite different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "02529b2666a536053a2e2940de5b28de36fd594b",
            "paperId": "02529b2666a536053a2e2940de5b28de36fd594b",
            "title": "Lion: Adversarial Distillation of Proprietary Large Language Models",
            "abstract": "The practice of transferring knowledge from a sophisticated, proprietary large language model (LLM) to a compact, open-source LLM has garnered considerable attention. Previous works have focused on a unidirectional knowledge distillation way by aligning the responses of the student model with those of the teacher model to a set of instructions. Nevertheless, they overlooked the possibility of incorporating any reciprocal\"feedback\"--identifying challenging instructions where the student model's performance falls short--to boost the student model's proficiency iteratively. To this end, we propose a novel adversarial distillation framework for a more efficient knowledge transfer. Leveraging the versatile role adaptability of LLMs, we prompt the teacher model to identify\"hard\"instructions and generate new\"hard\"instructions for the student model, creating a three-stage adversarial loop of imitation, discrimination, and generation. By applying this adversarial framework, we successfully transfer knowledge from ChatGPT to a student model (named Lion), using a mere 70k training data. Our results show that Lion-13B not only achieves comparable open-ended generation capabilities to ChatGPT but surpasses conventional state-of-the-art (SOTA) instruction-tuned models like Vicuna-13B by 55.4% in challenging zero-shot reasoning benchmarks such as BIG-Bench Hard (BBH) and 16.7% on AGIEval. Code and model can be found at https://github.com/YJiangcm/Lion.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Lion-13B not only achieves comparable open-ended generation capabilities to ChatGPT but surpasses conventional state-of-the-art instruction-tuned models like Vicuna- 13B by 55.4% in challenging zero-shot reasoning benchmarks such as BIG-Bench Hard (BBH) and 16.7% on AGIEval."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to reduce hallucination in large language models using iterative knowledge distillation prompting, where the model distills factual knowledge from its own generated output over multiple iterations. The paper proposes an adversarial distillation framework to efficiently transfer knowledge from a proprietary large language model (ChatGPT) to a compact, open-source model (Lion) by identifying challenging instructions and generating new hard instructions in a three-stage loop.\n\nThe project focuses on reducing hallucination in LLMs, while the paper focuses on knowledge distillation from proprietary LLMs to open-source models. The approaches are different: the project uses iterative knowledge distillation prompting, while the paper uses an adversarial distillation framework.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "78d3955e30d99650d078ba3ce1a523745da31040",
            "paperId": "78d3955e30d99650d078ba3ce1a523745da31040",
            "title": "Referee: Reference-Free Sentence Summarization with Sharper Controllability through Symbolic Knowledge Distillation",
            "abstract": "We present Referee, a novel framework for sentence summarization that can be trained reference-free (i.e., requiring no gold summaries for supervision), while allowing direct control for compression ratio. Our work is the first to demonstrate that reference-free, controlled sentence summarization is feasible via the conceptual framework of Symbolic Knowledge Distillation (West et al., 2022), where latent knowledge in pre-trained language models is distilled via explicit examples sampled from the teacher models, further purified with three types of filters: length, fidelity, and Information Bottleneck. Moreover, we uniquely propose iterative distillation of knowledge, where student models from the previous iteration of distillation serve as teacher models in the next iteration. Starting off from a relatively modest set of GPT3-generated summaries, we demonstrate how iterative knowledge distillation can lead to considerably smaller, but better summarizers with sharper controllability. A useful by-product of this iterative distillation process is a high-quality dataset of sentence-summary pairs with varying degrees of compression ratios. Empirical results demonstrate that the final student models vastly outperform the much larger GPT3-Instruct model in terms of the controllability of compression ratios, without compromising the quality of resulting summarization.",
            "year": 2022,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work is the first to demonstrate that reference-free, controlled sentence summarization is feasible via the conceptual framework of Symbolic Knowledge Distillation, where latent knowledge in pre-trained language models is distilled via explicit examples sampled from the teacher models."
            },
            "score": 5,
            "novelty_score": "The project proposal aims to reduce hallucination in large language models through iterative knowledge distillation prompting, while the paper focuses on reference-free sentence summarization with controllable compression ratios using symbolic knowledge distillation.\n\nProject Proposal: Reducing hallucination in large language models through iterative knowledge distillation prompting.\nPaper: Reference-free sentence summarization with controllable compression ratios using symbolic knowledge distillation.\n\nThe two works address different research problems (hallucination reduction vs. controllable sentence summarization) and employ different approaches (iterative knowledge distillation prompting vs. symbolic knowledge distillation).\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "cc016c52ba9fbda563c401bf890f92d2dad08501",
            "paperId": "cc016c52ba9fbda563c401bf890f92d2dad08501",
            "title": "Improving Zero-Shot Multilingual Text Generation via Iterative Distillation",
            "abstract": "The demand for multilingual dialogue systems often requires a costly labeling process, where human translators derive utterances in low resource languages from resource rich language annotation. To this end, we explore leveraging the inductive biases for target languages learned by numerous pretrained teacher models by transferring them to student models via sequence-level knowledge distillation. By assuming no target language text, the both the teacher and student models need to learn from the target distribution in a few/zero-shot manner. On the MultiATIS++ benchmark, we explore the effectiveness of our proposed technique to derive the multilingual text for 6 languages, using only the monolingual English data and the pretrained models. We show that training on the synthetic multilingual generation outputs yields close performance to training on human annotations in both slot F1 and intent accuracy; the synthetic text also scores high in naturalness and correctness based on human evaluation.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores leveraging the inductive biases for target languages learned by numerous pretrained teacher models by transferring them to student models via sequence-level knowledge distillation, and shows that training on the synthetic multilingual generation outputs yields close performance to training on human annotations in both slot F1 and intent accuracy."
            },
            "score": 5
        },
        {
            "id": "f5359f596e0306599b4aa4157e6fe03567b35c01",
            "paperId": "f5359f596e0306599b4aa4157e6fe03567b35c01",
            "title": "Knowledge Distillation of Large Language Models",
            "abstract": "Knowledge Distillation (KD) is a promising technique for reducing the high computational demand of large language models (LLMs). However, previous KD methods are primarily applied to white-box classification models or training small models to imitate black-box model APIs like ChatGPT. How to effectively distill the knowledge of white-box LLMs into small models is still under-explored, which becomes more important with the prosperity of open-source LLMs. In this work, we propose a KD approach that distills LLMs into smaller language models. We first replace the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution. Then, we derive an effective optimization approach to learn this objective. The student models are named MiniLLM. Extensive experiments in the instruction-following setting show that MiniLLM generates more precise responses with higher overall quality, lower exposure bias, better calibration, and higher long-text generation performance than the baselines. Our method is scalable for different model families with 120M to 13B parameters. Our code, data, and model checkpoints can be found in https://github.com/microsoft/LMOps/tree/main/minillm.",
            "year": 2023,
            "citationCount": 38,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work replaces the forward Kullback-Leibler divergence (KLD) objective in the standard KD approaches with reverse KLD, which is more suitable for KD on generative language models, to prevent the student model from overestimating the low-probability regions of the teacher distribution."
            },
            "score": 5
        },
        {
            "id": "22d5459d1f47341b355feeb1becc37208d6ec365",
            "paperId": "22d5459d1f47341b355feeb1becc37208d6ec365",
            "title": "RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought",
            "abstract": "Large language Models (LLMs) have achieved promising performance on arithmetic reasoning tasks by incorporating step-by-step chain-of-thought (CoT) prompting. However, LLMs face challenges in maintaining factual consistency during reasoning, exhibiting tendencies to condition overlooking, question misinterpretation, and condition hallucination over given problems. Existing methods use coarse-grained feedback (e.g., whether the answer is correct) to improve factual consistency. In this work, we propose RCoT (Reversing Chain-of-Thought), a novel method to improve LLMs' reasoning abilities by automatically detecting and rectifying factual inconsistency in LLMs, generated solutions. To detect factual inconsistency, RCoT first asks LLMs to reconstruct the problem based on generated solutions. Then fine-grained comparisons between the original problem and the reconstructed problem expose the factual inconsistency in the original solutions. To rectify the solution, RCoT formulates detected factual inconsistency into fine-grained feedback to guide LLMs in revising solutions. Experimental results demonstrate improvements of RCoT over standard CoT, Self-Consistency and Self-Refine across seven arithmetic datasets. Moreover, we find that manually written fine-grained feedback can dramatically improve LLMs' reasoning abilities (e.g., ChatGPT reaches 94.6% accuracy on GSM8K), encouraging the community to further explore the fine-grained feedback generation methods.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes RCoT (Reversing Chain-of-Thought), a novel method to improve LLMs' reasoning abilities by automatically detecting and rectifying factual inconsistency in LLMs, generated solutions and finds that manually written fine-grained feedback can dramatically improve LLM's reasoning abilities."
            },
            "score": 5
        },
        {
            "id": "d2d16333a4b0dc7e3463b280b9945e5ee6c53396",
            "paperId": "d2d16333a4b0dc7e3463b280b9945e5ee6c53396",
            "title": "TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models",
            "abstract": "Factual consistency evaluation is often conducted using Natural Language Inference (NLI) models, yet these models exhibit limited success in evaluating summaries. Previous work improved such models with synthetic training data. However, the data is typically based on perturbed human-written summaries, which often differ in their characteristics from real model-generated summaries and have limited coverage of possible factual errors. Alternatively, large language models (LLMs) have recently shown promising results in directly evaluating generative tasks, but are too computationally expensive for practical use. Motivated by these limitations, we introduce TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM. Unlike prior work, TrueTeacher does not rely on human-written summaries, and is multilingual by nature. Experiments on the TRUE benchmark show that a student model trained using our data, substantially outperforms both the state-of-the-art model with similar capacity, and the LLM teacher. In a systematic study, we compare TrueTeacher to existing synthetic data generation methods and demonstrate its superiority and robustness to domain-shift. We also show that our method generalizes to multilingual scenarios. Lastly, we release our large scale synthetic dataset (1.4M examples), generated using TrueTeacher, and a checkpoint trained on this data.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM, which does not rely on human-written summaries, and is multilingual by nature."
            },
            "score": 5
        },
        {
            "id": "56373d3fd0f1354a61f9e577db039cdb187d8d43",
            "paperId": "56373d3fd0f1354a61f9e577db039cdb187d8d43",
            "title": "Evaluating the Factual Consistency of Large Language Models Through News Summarization",
            "abstract": "While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information. To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB(Factual Inconsistency Benchmark) that focuses on the task of summarization. Specifically, our benchmark involves comparing the scores an LLM assigns to a factually consistent versus a factually inconsistent summary for an input news article. For factually consistent summaries, we use human-written reference summaries that we manually verify as factually consistent. To generate summaries that are factually inconsistent, we generate summaries from a suite of summarization models that we have manually annotated as factually inconsistent. A model's factual consistency is then measured according to its accuracy, i.e.\\ the proportion of documents where it assigns a higher score to the factually consistent summary. To validate the usefulness of FIB, we evaluate 23 large language models ranging from 1B to 176B parameters from six different model families including BLOOM and OPT. We find that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries. However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries. We validate design choices in our benchmark including the scoring method and source of distractor summaries. Our code and benchmark data can be found at https://github.com/r-three/fib.",
            "year": 2022,
            "citationCount": 51,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new benchmark called FIB(Factual Inconsistency Benchmark) that focuses on the task of summarization, and finds that existing LLMs generally assign a higher score to factually consistent summaries than tofactually inconsistent summaries."
            },
            "score": 5
        },
        {
            "id": "f33d19614d78f4a6e107e768ccd0a2d4244fc89a",
            "paperId": "f33d19614d78f4a6e107e768ccd0a2d4244fc89a",
            "title": "Evaluating the Factual Consistency of Large Language Models Through Summarization",
            "abstract": "While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information. To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB ( F actual I nconsistency B enchmark) that focuses on the task of summarization. Specifically, our benchmark involves comparing the scores an LLM assigns to a factually consistent versus a factually inconsistent summary for an input news article. For factually consistent summaries, we use human-written reference summaries that we manually verify as fac-tually consistent. To generate summaries that are factually inconsistent, we generate summaries from a suite of summarization models that we have manually annotated as fac-tually inconsistent. A model\u2019s factual consistency is then measured according to its accuracy, i.e. the proportion of documents where it assigns a higher score to the factually consistent summary. To validate the usefulness of FIB , we evaluate 23 large language models ranging from 1B to 176B parameters from six different model families including BLOOM and OPT. We \ufb01nd that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries. However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries. We validate design choices in our benchmark including the scoring method and source of distractor summaries. Our code and",
            "year": 2022,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new benchmark called FIB is proposed that focuses on the task of summarization and shows that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries, and that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries."
            },
            "score": 5
        },
        {
            "id": "6489640b1d30a8a3e7cb906bb6557f1ccd0d799d",
            "paperId": "6489640b1d30a8a3e7cb906bb6557f1ccd0d799d",
            "title": "Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models",
            "abstract": "Retrieval-augmented language models (RALMs) represent a substantial advancement in the capabilities of large language models, notably in reducing factual hallucination by leveraging external knowledge sources. However, the reliability of the retrieved information is not always guaranteed. The retrieval of irrelevant data can lead to misguided responses, and potentially causing the model to overlook its inherent knowledge, even when it possesses adequate information to address the query. Moreover, standard RALMs often struggle to assess whether they possess adequate knowledge, both intrinsic and retrieved, to provide an accurate answer. In situations where knowledge is lacking, these systems should ideally respond with\"unknown\"when the answer is unattainable. In response to these challenges, we introduces Chain-of-Noting (CoN), a novel approach aimed at improving the robustness of RALMs in facing noisy, irrelevant documents and in handling unknown scenarios. The core idea of CoN is to generate sequential reading notes for retrieved documents, enabling a thorough evaluation of their relevance to the given question and integrating this information to formulate the final answer. We employed ChatGPT to create training data for CoN, which was subsequently trained on an LLaMa-2 7B model. Our experiments across four open-domain QA benchmarks show that RALMs equipped with CoN significantly outperform standard RALMs. Notably, CoN achieves an average improvement of +7.9 in EM score given entirely noisy retrieved documents and +10.5 in rejection rates for real-time questions that fall outside the pre-training knowledge scope.",
            "year": 2023,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Chain-of-Noting (CoN) is introduced, a novel approach aimed at improving the robustness of RALMs in facing noisy, irrelevant documents and in handling unknown scenarios, and achieves an average improvement of +7.9 in EM score given entirely noisy retrieved documents and +10.5 in rejection rates for real-time questions that fall outside the pre-training knowledge scope."
            },
            "score": 5
        },
        {
            "id": "696bc5ba0d023822bbee6b878a71ea2e4a4b0e5a",
            "paperId": "696bc5ba0d023822bbee6b878a71ea2e4a4b0e5a",
            "title": "N-Critics: Self-Refinement of Large Language Models with Ensemble of Critics",
            "abstract": "We propose a self-correction mechanism for Large Language Models (LLMs) to mitigate issues such as toxicity and fact hallucination. This method involves refining model outputs through an ensemble of critics and the model's own feedback. Drawing inspiration from human behavior, we explore whether LLMs can emulate the self-correction process observed in humans who often engage in self-reflection and seek input from others to refine their understanding of complex topics. Our approach is model-agnostic and can be applied across various domains to enhance trustworthiness by addressing fairness, bias, and robustness concerns. We consistently observe performance improvements in LLMs for reducing toxicity and correcting factual errors.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a self-correction mechanism for Large Language Models (LLMs) to mitigate issues such as toxicity and fact hallucination and consistently observe performance improvements in LLMs for reducing toxicity and correcting factual errors."
            },
            "score": 5
        },
        {
            "id": "44993ace721ffb93f3b78fe8eb9a082d9b6a6b42",
            "paperId": "44993ace721ffb93f3b78fe8eb9a082d9b6a6b42",
            "title": "Leveraging Large Language Models for Enhanced NLP Task Performance through Knowledge Distillation and Optimized Training Strategies",
            "abstract": "Emerging Large Language Models (LLMs) like GPT-4 have revolutionized Natural Language Processing (NLP), showing potential in traditional tasks such as Named Entity Recognition (NER). Our study explores a three-phase training strategy that harnesses GPT-4's capabilities to enhance the BERT model's performance on NER. Initially, GPT-4 annotates a subset of the CONLL2003 and additional BBC dataset without fine-tuning. We then train BERT using a mix of original and LLM-annotated data, analyzing the efficacy of LLM annotations against traditional methods. The second phase involves comparative experiments with different training regimens, assessing the synergy between distilled and original data. We observe that sequential strategies, particularly a simple mix of training first with distilled data followed by original data, significantly boost performance. In the third phase, we investigate various data blending techniques, including sigmoid and power decay functions, to optimize the training process further. Our results indicate that a strategic mix of distilled and original data markedly elevates the NER capabilities of BERT. Our approach presents a scalable methodology that reduces manual annotation costs and increases efficiency, making it especially pertinent in resource-limited and closed-network environments. The study concludes that while the 'Simple Mix' strategy yields the best results, understanding its underlying mechanisms requires further research. Future work will also focus on refining prompt designs and enhancing annotation selection processes, aiming to extend our methodology to diverse NLP tasks.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study explores a three-phase training strategy that harnesses GPT-4's capabilities to enhance the BERT model's performance on NER, and indicates that a strategic mix of distilled and original data markedly elevates the NER capabilities of BERT."
            },
            "score": 5
        },
        {
            "id": "7599401233a40cc8c51b77d0419d2732c35439b2",
            "paperId": "7599401233a40cc8c51b77d0419d2732c35439b2",
            "title": "Annealing Knowledge Distillation",
            "abstract": "Significant memory and computational requirements of large deep neural networks restricts their application on edge devices. Knowledge distillation (KD) is a prominent model compression technique for deep neural networks in which the knowledge of a trained large teacher model is transferred to a smaller student model. The success of knowledge distillation is mainly attributed to its training objective function, which exploits the soft-target information (also known as \u201cdark knowledge\u201d) besides the given regular hard labels in a training set. However, it is shown in the literature that the larger the gap between the teacher and the student networks, the more difficult is their training using knowledge distillation. To address this shortcoming, we propose an improved knowledge distillation method (called Annealing-KD) by feeding the rich information provided by teacher\u2019s soft-targets incrementally and more efficiently. Our Annealing-KD technique is based on a gradual transition over annealed soft-targets generated by the teacher at different temperatures in an iterative process; and therefore, the student is trained to follow the annealed teacher output in a step-by-step manner. This paper includes theoretical and empirical evidence as well as practical experiments to support the effectiveness of our Annealing-KD method. We did a comprehensive set of experiments on different tasks such as image classification (CIFAR-10 and 100) and NLP language inference with BERT-based models on the GLUE benchmark and consistently got superior results.",
            "year": 2021,
            "citationCount": 57,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes an improved knowledge distillation method (called Annealing-KD), which is based on a gradual transition over annealed soft-targets generated by the teacher at different temperatures in an iterative process and does a comprehensive set of experiments on different tasks such as image classification and NLP language inference with BERT-based models on the GLUE benchmark and consistently got superior results."
            },
            "score": 5
        },
        {
            "id": "1cf50a2e906dc89463d7eab827de9a3c371e7c53",
            "paperId": "1cf50a2e906dc89463d7eab827de9a3c371e7c53",
            "title": "Generate, Annotate, and Learn: NLP with Synthetic Text",
            "abstract": "Abstract This paper studies the use of language models as a source of synthetic unlabeled text for NLP. We formulate a general framework called \u201cgenerate, annotate, and learn (GAL)\u201d to take advantage of synthetic text within knowledge distillation, self-training, and few-shot learning applications. To generate high-quality task-specific text, we either fine-tune LMs on inputs from the task of interest, or prompt large LMs with few examples. We use the best available classifier to annotate synthetic text with soft pseudo labels for knowledge distillation and self-training, and use LMs to obtain hard labels for few-shot learning. We train new supervised models on the combination of labeled and pseudo-labeled data, which results in significant gains across several applications. We investigate key components of GAL and present theoretical and empirical arguments against the use of class-conditional LMs to generate synthetic labeled text instead of unlabeled text. GAL achieves new state-of-the-art knowledge distillation results for 6-layer transformers on the GLUE leaderboard.",
            "year": 2021,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "GAL achieves new state-of-the-art knowledge distillation results for 6-layer transformers on the GLUE leaderboard and investigates key components of GAL and presents theoretical and empirical arguments against the use of class-conditional LMs."
            },
            "score": 5
        },
        {
            "id": "9d0767866f8424b78c5ded9502e67c702ea5a198",
            "paperId": "9d0767866f8424b78c5ded9502e67c702ea5a198",
            "title": "Reinforced Iterative Knowledge Distillation for Cross-Lingual Named Entity Recognition",
            "abstract": "Named entity recognition (NER) is a fundamental component in many applications, such as Web Search and Voice Assistants. Although deep neural networks greatly improve the performance of NER, due to the requirement of large amounts of training data, deep neural networks can hardly scale out to many languages in an industry setting. To tackle this challenge, cross-lingual NER transfers knowledge from a rich-resource language to languages with low resources through pre-trained multilingual language models. Instead of using training data in target languages, cross-lingual NER has to rely on only training data in source languages, and optionally adds the translated training data derived from source languages. However, the existing cross-lingual NER methods do not make good use of rich unlabeled data in target languages, which is relatively easy to collect in industry applications. To address the opportunities and challenges, in this paper we describe our novel practice in Microsoft to leverage such large amounts of unlabeled data in target languages in real production settings. To effectively extract weak supervision signals from the unlabeled data, we develop a novel approach based on the ideas of semi-supervised learning and reinforcement learning. The empirical study on three benchmark data sets verifies that our approach establishes the new state-of-the-art performance with clear edges. Now, the NER techniques reported in this paper are on their way to become a fundamental component for Web ranking, Entity Pane, Answers Triggering, and Question Answering in the Microsoft Bing search engine. Moreover, our techniques will also serve as part of the Spoken Language Understanding module for a commercial voice assistant. We plan to open source the code of the prototype framework after deployment.",
            "year": 2021,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The NER techniques reported in this paper are on their way to become a fundamental component for Web ranking, Entity Pane, Answers Triggering, and Question Answering in the Microsoft Bing search engine."
            },
            "score": 4
        },
        {
            "id": "43feeddf36102f7b5d2612acbb0cb1dfae3eea82",
            "paperId": "43feeddf36102f7b5d2612acbb0cb1dfae3eea82",
            "title": "PILE: Pairwise Iterative Logits Ensemble for Multi-Teacher Labeled Distillation",
            "abstract": "Pre-trained language models have become a crucial part of ranking systems and achieved very impressive effects recently. To maintain high performance while keeping efficient computations, knowledge distillation is widely used. In this paper, we focus on two key questions in knowledge distillation for ranking models: 1) how to ensemble knowledge from multi-teacher; 2) how to utilize the label information of data in the distillation process. We propose a unified algorithm called Pairwise Iterative Logits Ensemble (PILE) to tackle these two questions simultaneously. PILE ensembles multi-teacher logits supervised by label information in an iterative way and achieved competitive performance in both offline and online experiments. The proposed method has been deployed in a real-world commercial search system.",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A unified algorithm called Pairwise Iterative Logits Ensemble (PILE) is proposed to tackle two key questions in knowledge distillation for ranking models: how to ensemble knowledge from multi-teacher; and how to utilize the label information of data in the distillation process."
            },
            "score": 4
        },
        {
            "id": "b835345c6168d7b179516700aa4460912a8857e9",
            "paperId": "b835345c6168d7b179516700aa4460912a8857e9",
            "title": "HomoDistil: Homotopic Task-Agnostic Distillation of Pre-trained Transformers",
            "abstract": "Knowledge distillation has been shown to be a powerful model compression approach to facilitate the deployment of pre-trained language models in practice. This paper focuses on task-agnostic distillation. It produces a compact pre-trained model that can be easily fine-tuned on various tasks with small computational costs and memory footprints. Despite the practical benefits, task-agnostic distillation is challenging. Since the teacher model has a significantly larger capacity and stronger representation power than the student model, it is very difficult for the student to produce predictions that match the teacher's over a massive amount of open-domain training data. Such a large prediction discrepancy often diminishes the benefits of knowledge distillation. To address this challenge, we propose Homotopic Distillation (HomoDistil), a novel task-agnostic distillation approach equipped with iterative pruning. Specifically, we initialize the student model from the teacher model, and iteratively prune the student's neurons until the target width is reached. Such an approach maintains a small discrepancy between the teacher's and student's predictions throughout the distillation process, which ensures the effectiveness of knowledge transfer. Extensive experiments demonstrate that HomoDistil achieves significant improvements on existing baselines.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The proposed Homotopic Distillation (HomoDistil), a novel task-agnostic distillation approach equipped with iterative pruning that maintains a small discrepancy between the teacher's and student's predictions throughout the distillation process, which ensures the effectiveness of knowledge transfer."
            },
            "score": 4
        },
        {
            "id": "f727f928e7e179307d8d4a1da2387393f2bd7915",
            "paperId": "f727f928e7e179307d8d4a1da2387393f2bd7915",
            "title": "Methods for Measuring, Updating, and Visualizing Factual Beliefs in Language Models",
            "abstract": "Language models can memorize a considerable amount of factual information during pretraining that can be elicited through prompting or finetuning models on tasks like question answering. In this paper, we discuss approaches to measuring model factual beliefs, updating incorrect factual beliefs in models, and visualizing graphical relationships between factual beliefs. Our main contributions include: (1) new metrics for evaluating belief-updating methods focusing on the logical consistency of beliefs, (2) a training objective for Sequential, Local, and Generalizing updates (SLAG) that improves the performance of existing hypernetwork approaches, and (3) the introduction of the belief graph, a new form of visualization for language models that shows relationships between stored model beliefs. Our experiments suggest that models show only limited consistency between factual beliefs, but update methods can both fix incorrect model beliefs and greatly improve their consistency. Although off-the-shelf optimizers are surprisingly strong belief-updating baselines, our learned optimizers can outperform them in more difficult settings than have been considered in past work.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experiments suggest that models show only limited consistency between factual beliefs, but update methods can both fix incorrect model beliefs and greatly improve their consistency, and off-the-shelf optimizers can outperform them in more difficult settings than have been considered in past work."
            },
            "score": 4
        },
        {
            "id": "3d2f9bd3f2ea1e1095e6d22e32ed8eb5fbb096f6",
            "paperId": "3d2f9bd3f2ea1e1095e6d22e32ed8eb5fbb096f6",
            "title": "Rationale Belief Aggregation for Self-Verified Reasoning",
            "abstract": "Large language models such as GPT-3 [1] have a great deal of information encoded within their parameters, however, our ability to access this information is bottlenecked by how we communicate or interface with these models, namely through prompting. Chain-of-thought prompting [2] demonstrates the value of producing step-by-step reasoning chains (rationales) before answering a question, and self-consistency [3] shows that sampling multiple rationales and aggregating their outputs can allow for more robust reasoning. In this work we postulate that rationales consist of multiple beliefs, or informational phrases that the model uses as context for its reasoning (which may or may not be factual), and some inference over these beliefs. Empirically, we observe that different rationales expose different beliefs and hypothesize that performing a principled aggregation over the beliefs surfaced by different rationales would allow us to reduce internal contradictions within a language model and produce more consistent rationales to reason over. We propose two such aggregation strategies, Belief Aggregation and Belief Majority Voting, and evaluate their performance over three challenging QA datasets [4]. Our method results in modest gains in accuracy over self-consistency and greedy decoding for chain-of-thought prompting, while providing strong gains in coverage (% of questions a model is able to answer without abstaining), thereby resulting in confident reasoning over a larger set of questions.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The method results in modest gains in accuracy over self-consistency and greedy decoding for chain-of-thought prompting, while providing strong gains in coverage (% of questions a model is able to answer without abstaining), thereby resulting in confident reasoning over a larger set of questions."
            },
            "score": 4
        },
        {
            "id": "d998ff93b2e58c9b219a9103f8c2ad714a41e4b9",
            "paperId": "d998ff93b2e58c9b219a9103f8c2ad714a41e4b9",
            "title": "The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency of Language Models",
            "abstract": "Large Language Models (LLMs) make natural interfaces to factual knowledge, but their usefulness is limited by their tendency to deliver inconsistent answers to semantically equivalent questions. For example, a model might predict both\"Anne Redpath passed away in Edinburgh.\"and\"Anne Redpath's life ended in London.\"In this work, we identify potential causes of inconsistency and evaluate the effectiveness of two mitigation strategies: up-scaling and augmenting the LM with a retrieval corpus. Our results on the LLaMA and Atlas models show that both strategies reduce inconsistency while retrieval augmentation is considerably more efficient. We further consider and disentangle the consistency contributions of different components of Atlas. For all LMs evaluated we find that syntactical form and other evaluation task artifacts impact consistency. Taken together, our results provide a better understanding of the factors affecting the factual consistency of language models.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work identifies potential causes of inconsistency and evaluates the effectiveness of two mitigation strategies: up-scaling and augmenting the LM with a retrieval corpus, showing that both strategies reduce inconsistency while retrieval augmentation is considerably more efficient."
            },
            "score": 4
        },
        {
            "id": "e96348576b682e709b2ee06ea28cd81f8bfa102b",
            "paperId": "e96348576b682e709b2ee06ea28cd81f8bfa102b",
            "title": "Exploring the Factual Consistency in Dialogue Comprehension of Large Language Models",
            "abstract": "LLMs (Large Language Models) usually interact with users in the form of dialogue and generate responses following their instructions, which naturally require dialogue comprehension abilities. However, dialogue comprehension is a general language ability which is hard to be evaluated directly. In this work, we propose to perform the evaluation focusing on the factual consistency issue with the help of the dialogue summarization task. Besides evaluating and analyzing the dialogue summarization performance (DIAC-Sum) of different LLMs, we also derive factual questions from the generated summaries and use them as a more flexible measurement of dialogue comprehension (DIAC-QA). Our evaluation shows that, on average, 26.8% of the summaries generated by LLMs contain factual inconsistency. Even ChatGPT, the strongest model evaluated, has such errors in 16% of its summaries. For answering the factual questions, which is more challenging, the average error rate of all evaluated LLMs is 36.1%. Both results indicate serious deficiencies. Detailed analysis shows that the understanding of subject/object of the conversation is still challenging for LLMs. Furthermore, to stimulate and enhance the dialogue comprehension ability of LLMs, we propose a fine-tuning paradigm with auto-constructed multi-task data, which achieved a relative error rate reduction of 11% on DIAC-QA.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "To stimulate and enhance the dialogue comprehension ability of LLMs, a fine-tuning paradigm with auto-constructed multi-task data is proposed, which achieved a relative error rate reduction of 11% on DIAC-QA."
            },
            "score": 4
        },
        {
            "id": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
            "paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
            "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "abstract": "Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).",
            "year": 2022,
            "citationCount": 1396,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting that first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths."
            },
            "score": 4
        },
        {
            "id": "f3cd3a0e1a0a29193fd88178ef75946dfe756622",
            "paperId": "f3cd3a0e1a0a29193fd88178ef75946dfe756622",
            "title": "Factual Consistency Evaluation of Summarisation in the Era of Large Language Models",
            "abstract": "Factual inconsistency with source documents in automatically generated summaries can lead to misinformation or pose risks. Existing factual consistency(FC) metrics are constrained by their performance, efficiency, and explainability. Recent advances in Large language models (LLMs) have demonstrated remarkable potential in text evaluation but their effectiveness in assessing FC in summarisation remains underexplored. Prior research has mostly focused on proprietary LLMs, leaving essential factors that affect their assessment capabilities unexplored. Additionally, current FC evaluation benchmarks are restricted to news articles, casting doubt on the generality of the FC methods tested on them. In this paper, we first address the gap by introducing TreatFact a dataset of LLM-generated summaries of clinical texts, annotated for FC by domain experts. Moreover, we benchmark 11 LLMs for FC evaluation across news and clinical domains and analyse the impact of model size, prompts, pre-training and fine-tuning data. Our findings reveal that despite proprietary models prevailing on the task, open-source LLMs lag behind. Nevertheless, there is potential for enhancing the performance of open-source LLMs through increasing model size, expanding pre-training data, and developing well-curated fine-tuning data. Experiments on TreatFact suggest that both previous methods and LLM-based evaluators are unable to capture factual inconsistencies in clinical summaries, posing a new challenge for FC evaluation.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "TreatFact, a dataset of LLM-generated summaries of clinical texts, annotated for FC by domain experts, is introduced and it is revealed that despite proprietary models prevailing on the task, open-source LLMs lag behind."
            },
            "score": 4
        },
        {
            "id": "261549439aebdda72b648ecc462448fd24857ac1",
            "paperId": "261549439aebdda72b648ecc462448fd24857ac1",
            "title": "Progressive-Hint Prompting Improves Reasoning in Large Language Models",
            "abstract": "The performance of Large Language Models (LLMs) in reasoning tasks depends heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency being critical methods that enhance this ability. However, these methods do not fully exploit the answers generated by the LLM to guide subsequent responses. This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers. PHP is orthogonal to CoT and self-consistency, making it easy to combine with state-of-the-art techniques to further improve performance. We conducted extensive and comprehensive experiments on seven benchmarks. The results show that PHP significantly improves accuracy while remaining highly efficient. For instance, with text-davinci-003, we observed a 4.2% improvement on GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction in sample paths with self-consistency. With GPT-4 and PHP, we achieve state-of-the-art performances on SVAMP (89.1% ->91.9%), GSM8K (92% ->95.5%), AQuA (76.4% ->79.9%) and MATH (50.3% ->53.9%).",
            "year": 2023,
            "citationCount": 64,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers."
            },
            "score": 4
        },
        {
            "id": "6af460d34bfc8e955e43fbe15cedcf329b48bc19",
            "paperId": "6af460d34bfc8e955e43fbe15cedcf329b48bc19",
            "title": "SAC3: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency",
            "abstract": "Hallucination detection is a critical step toward understanding the trustworthiness of modern language models (LMs). To achieve this goal, we re-examine existing detection approaches based on the self-consistency of LMs and uncover two types of hallucinations resulting from 1) question-level and 2) model-level, which cannot be effectively identified through self-consistency check alone. Building upon this discovery, we propose a novel sampling-based method, i.e., semantic-aware cross-check consistency (SAC3) that expands on the principle of self-consistency checking. Our SAC3 approach incorporates additional mechanisms to detect both question-level and model-level hallucinations by leveraging advances including semantically equivalent question perturbation and cross-model response consistency checking. Through extensive and systematic empirical analysis, we demonstrate that SAC3 outperforms the state of the art in detecting both non-factual and factual statements across multiple question-answering and open-domain generation benchmarks.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel sampling-based method, i.e., semantic-aware cross-check consistency (SAC3) that expands on the principle of self-consistency checking and demonstrates that SAC3 outperforms the state of the art in detecting both non-factual and factual statements across multiple question-answering and open-domain generation benchmarks."
            },
            "score": 4
        },
        {
            "id": "f41977c497c96c1da2e9e945315e9be6d6ad472e",
            "paperId": "f41977c497c96c1da2e9e945315e9be6d6ad472e",
            "title": "Towards reducing hallucination in extracting information from financial reports using Large Language Models",
            "abstract": "For a financial analyst, the question and answer (Q\\&A) segment of the company financial report is a crucial piece of information for various analysis and investment decisions. However, extracting valuable insights from the Q\\&A section has posed considerable challenges as the conventional methods such as detailed reading and note-taking lack scalability and are susceptible to human errors, and Optical Character Recognition (OCR) and similar techniques encounter difficulties in accurately processing unstructured transcript text, often missing subtle linguistic nuances that drive investor decisions. Here, we demonstrate the utilization of Large Language Models (LLMs) to efficiently and rapidly extract information from earnings report transcripts while ensuring high accuracy transforming the extraction process as well as reducing hallucination by combining retrieval-augmented generation technique as well as metadata. We evaluate the outcomes of various LLMs with and without using our proposed approach based on various objective metrics for evaluating Q\\&A systems, and empirically demonstrate superiority of our method.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work demonstrates the utilization of Large Language Models (LLMs) to efficiently and rapidly extract information from earnings report transcripts while ensuring high accuracy transforming the extraction process as well as reducing hallucination by combining retrieval-augmented generation technique aswell as metadata."
            },
            "score": 4
        },
        {
            "id": "4e53b481beabba42aac027e5a8c69fed26ab4062",
            "paperId": "4e53b481beabba42aac027e5a8c69fed26ab4062",
            "title": "RHO ($\u03c1$): Reducing Hallucination in Open-domain Dialogues with Knowledge Grounding",
            "abstract": "Dialogue systems can leverage large pre-trained language models and knowledge to generate fluent and informative responses. However, these models are still prone to produce hallucinated responses not supported by the input source, which greatly hinders their application. The heterogeneity between external knowledge and dialogue context challenges representation learning and source integration, and further contributes to unfaithfulness. To handle this challenge and generate more faithful responses, this paper presents RHO ($\\rho$) utilizing the representations of linked entities and relation predicates from a knowledge graph (KG). We propose (1) local knowledge grounding to combine textual embeddings with the corresponding KG embeddings; and (2) global knowledge grounding to equip RHO with multi-hop reasoning abilities via the attention mechanism. In addition, we devise a response re-ranking technique based on walks over KG sub-graphs for better conversational reasoning. Experimental results on OpenDialKG show that our approach significantly outperforms state-of-the-art methods on both automatic and human evaluation by a large margin, especially in hallucination reduction (17.54% in FeQA).",
            "year": 2022,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "RHO is presented utilizing the representations of linked entities and relation predicates from a knowledge graph (KG) to equip RHO with multi-hop reasoning abilities via the attention mechanism and devise a response re-ranking technique based on walks over KG sub-graphs for better conversational reasoning."
            },
            "score": 4
        },
        {
            "id": "798963674902f741c3ea9298403eb8384c099a42",
            "paperId": "798963674902f741c3ea9298403eb8384c099a42",
            "title": "Factored Verification: Detecting and Reducing Hallucination in Summaries of Academic Papers",
            "abstract": "Hallucination plagues even frontier LLMs--but how bad is it really for summarizing academic papers? We evaluate Factored Verification, a simple automated method for detecting hallucinations in abstractive summaries. This method sets a new SotA on hallucination detection in the summarization task of the HaluEval benchmark, achieving 76.2% accuracy. We then use this method to estimate how often language models hallucinate when summarizing across multiple academic papers and find 0.62 hallucinations in the average ChatGPT (16k) summary, 0.84 for GPT-4, and 1.55 for Claude 2. We ask models to self-correct using Factored Critiques and find that this lowers the number of hallucinations to 0.49 for ChatGPT, 0.46 for GPT-4, and 0.95 for Claude 2. The hallucinations we find are often subtle, so we advise caution when using models to synthesize academic papers.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Factored Verification, a simple automated method for detecting hallucinations in abstractive summaries, is evaluated and sets a new SotA on hallucination detection in the summarization task of the HaluEval benchmark, achieving 76.2% accuracy."
            },
            "score": 4
        },
        {
            "id": "889feabe31ba0d24c093ac94d54a06eecb87e3f4",
            "paperId": "889feabe31ba0d24c093ac94d54a06eecb87e3f4",
            "title": "Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding",
            "abstract": "Dialogue systems powered by large pre-trained language models exhibit an innate ability to deliver fluent and natural-sounding responses. Despite their impressive performance, these models are fitful and can often generate factually incorrect statements impeding their widespread adoption. In this paper, we focus on the task of improving faithfulness and reducing hallucination of neural dialogue systems to known facts supplied by a Knowledge Graph (KG). We propose Neural Path Hunter which follows a generate-then-refine strategy whereby a generated response is amended using the KG. Neural Path Hunter leverages a separate token-level fact critic to identify plausible sources of hallucination followed by a refinement stage that retrieves correct entities by crafting a query signal that is propagated over a k-hop subgraph. We empirically validate our proposed approach on the OpenDialKG dataset (Moon et al., 2019) against a suite of metrics and report a relative improvement of faithfulness over dialogue responses by 20.35% based on FeQA (Durmus et al., 2020). The code is available at https://github.com/nouhadziri/Neural-Path-Hunter.",
            "year": 2021,
            "citationCount": 80,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes Neural Path Hunter which follows a generate-then-refine strategy whereby a generated response is amended using the KG, and leverages a separate token-level fact critic to identify plausible sources of hallucination and retrieves correct entities by crafting a query signal that is propagated over a k-hop subgraph."
            },
            "score": 4
        },
        {
            "id": "2986b2b06173e065c94bae49c7a9a3718dad486c",
            "paperId": "2986b2b06173e065c94bae49c7a9a3718dad486c",
            "title": "Reducing hallucination in structured outputs via Retrieval-Augmented Generation",
            "abstract": "A common and fundamental limitation of Generative AI (GenAI) is its propensity to hallucinate. While large language models (LLM) have taken the world by storm, without eliminating or at least reducing hallucinations, real-world GenAI systems may face challenges in user adoption. In the process of deploying an enterprise application that produces workflows based on natural language requirements, we devised a system leveraging Retrieval Augmented Generation (RAG) to greatly improve the quality of the structured output that represents such workflows. Thanks to our implementation of RAG, our proposed system significantly reduces hallucinations in the output and improves the generalization of our LLM in out-of-domain settings. In addition, we show that using a small, well-trained retriever encoder can reduce the size of the accompanying LLM, thereby making deployments of LLM-based systems less resource-intensive.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This proposed system significantly reduces hallucinations in the output and improves the generalization of the LLM in out-of-domain settings, and it is shown that using a small, well-trained retriever encoder can reduce the size of the accompanying LLM, thereby making deployments of LLM-based systems less resource-intensive."
            },
            "score": 4
        },
        {
            "id": "79429814fd4d967b9277af2805c53f370e52ebb5",
            "paperId": "79429814fd4d967b9277af2805c53f370e52ebb5",
            "title": "Chain of Natural Language Inference for Reducing Large Language Model Ungrounded Hallucinations",
            "abstract": "Large language models (LLMs) can generate fluent natural language texts when given relevant documents as background context. This ability has attracted considerable interest in developing industry applications of LLMs. However, LLMs are prone to generate hallucinations that are not supported by the provided sources. In this paper, we propose a hierarchical framework to detect and mitigate such ungrounded hallucination. Our framework uses Chain of Natural Language Inference (CoNLI) for hallucination detection and hallucination reduction via post-editing. Our approach achieves state-of-the-art performance on hallucination detection and enhances text quality through rewrite, using LLMs without any fine-tuning or domain-specific prompt engineering. We show that this simple plug-and-play framework can serve as an effective choice for hallucination detection and reduction, achieving competitive performance across various contexts.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a hierarchical framework to detect and mitigate ungrounded hallucination, using Chain of Natural Language Inference (CoNLI) for hallucination detection and hallucination reduction via post-editing and shows that this simple plug-and-play framework can serve as an effective choice for hallucinations detection and reduction, achieving competitive performance across various contexts."
            },
            "score": 4
        },
        {
            "id": "60988a0ebad89af503f17de977785814fb864635",
            "paperId": "60988a0ebad89af503f17de977785814fb864635",
            "title": "Correction with Backtracking Reduces Hallucination in Summarization",
            "abstract": "Abstractive summarization aims at generating natural language summaries of a source document that are succinct while preserving the important elements. Despite recent advances, neural text summarization models are known to be susceptible to hallucinating (or more correctly confabulating), that is to produce summaries with details that are not grounded in the source document. In this paper, we introduce a simple yet efficient technique, CoBa, to reduce hallucination in abstractive summarization. The approach is based on two steps: hallucination detection and mitigation. We show that the former can be achieved through measuring simple statistics about conditional word probabilities and distance to context words. Further, we demonstrate that straight-forward backtracking is surprisingly effective at mitigation. We thoroughly evaluate the proposed method with prior art on three benchmark datasets for text summarization. The results show that CoBa is effective and efficient in reducing hallucination, and offers great adaptability and flexibility.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a simple yet efficient technique, CoBa, to reduce hallucination in abstractive summarization, and shows that the former can be achieved through measuring simple statistics about conditional word probabilities and distance to context words."
            },
            "score": 4
        },
        {
            "id": "fcee1c19e12f3b7e3595aeba702416d055bdbc3f",
            "paperId": "fcee1c19e12f3b7e3595aeba702416d055bdbc3f",
            "title": "Knowledge Verification to Nip Hallucination in the Bud",
            "abstract": "While large language models (LLMs) have demonstrated exceptional performance across various tasks following human alignment, they may still generate responses that sound plausible but contradict factual knowledge, a phenomenon known as \\emph{hallucination}. In this paper, we demonstrate the feasibility of mitigating hallucinations by verifying and minimizing the inconsistency between external knowledge present in the alignment data and the intrinsic knowledge embedded within foundation LLMs. Specifically, we propose a novel approach called Knowledge Consistent Alignment (KCA), which employs a well-aligned LLM to automatically formulate assessments based on external knowledge to evaluate the knowledge boundaries of foundation LLMs. To address knowledge inconsistencies in the alignment data, KCA implements several specific strategies to deal with these data instances. We demonstrate the superior efficacy of KCA in reducing hallucinations across six benchmarks, utilizing foundation LLMs of varying backbones and scales. This confirms the effectiveness of mitigating hallucinations by reducing knowledge inconsistency. Our code, model weights, and data are openly accessible at \\url{https://github.com/fanqiwan/KCA}.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The superior efficacy of KCA is demonstrated in reducing hallucinations across six benchmarks, utilizing foundation LLMs of varying backbones and scales, which confirms the effectiveness of mitigating hallucinations by reducing knowledge inconsistency."
            },
            "score": 4
        },
        {
            "id": "afdeef9585232642d18e7c6a7942b2395e94ede1",
            "paperId": "afdeef9585232642d18e7c6a7942b2395e94ede1",
            "title": "A Cause-Effect Look at Alleviating Hallucination of Knowledge-grounded Dialogue Generation",
            "abstract": "Empowered by the large-scale pretrained language models, existing dialogue systems have demonstrated impressive performance conducting fluent and natural-sounding conversations. However, they are still plagued by the hallucination problem, causing unpredictable factual errors in the generated responses. Recently, knowledge-grounded dialogue generation models, that intentionally invoke external knowledge resources to more informative responses, are also proven to be effective in reducing hallucination. Following the idea of getting high-quality knowledge, a few efforts have achieved pretty good performance on this issue. As some inevitable knowledge noises may also lead to hallucinations, it is emergent to investigate the reason and future directions for building noise-tolerant methods in KGD tasks. In this paper, we analyze the causal story behind this problem with counterfactual reasoning methods. Based on the causal effect analysis, we propose a possible solution for alleviating the hallucination in KGD by exploiting the dialogue-knowledge interaction. Experimental results of our example implementation show that this method can reduce hallucination without disrupting other dialogue performance, while keeping adaptive to different generation models. We hope our efforts can support and call for more attention to developing lightweight techniques towards robust and trusty dialogue systems.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Based on the causal effect analysis, a possible solution for alleviating the hallucination in KGD by exploiting the dialogue-knowledge interaction is proposed and results show this method can reduce hallucination without disrupting other dialogue performance, while keeping adaptive to different generation models."
            },
            "score": 4
        },
        {
            "id": "0176d39e92b46a5aecfdc38198cf641e5885ec43",
            "paperId": "0176d39e92b46a5aecfdc38198cf641e5885ec43",
            "title": "Knowledge Distillation for Swedish NER models: A Search for Performance and Efficiency",
            "abstract": "The current recipe for better model performance within NLP is to increase model size and training data. While it gives us models with increasingly impressive results, it also makes it more difficult to train and deploy state-of-the-art models for NLP due to increasing computational costs. Model compression is a field of research that aims to alleviate this problem. The field encompasses different methods that aim to preserve the performance of a model while decreasing the size of it. One such method is knowledge distillation. In this article, we investigate the effect of knowledge distillation for named entity recognition models in Swedish. We show that while some sequence tagging models benefit from knowledge distillation, not all models do. This prompts us to ask questions about in which situations and for which models knowledge distillation is beneficial. We also reason about the effect of knowledge distillation on computational costs.",
            "year": 2021,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The effect of knowledge distillation for named entity recognition models in Swedish is investigated and it is shown that while some sequence tagging models benefit from knowledgedistillation, not all models do."
            },
            "score": 4
        },
        {
            "id": "1bd9466f0bb10d29a16f614943ec7823e13cb210",
            "paperId": "1bd9466f0bb10d29a16f614943ec7823e13cb210",
            "title": "Mixed Distillation Helps Smaller Language Model Better Reasoning",
            "abstract": "While large language models (LLMs) have demonstrated exceptional performance in recent natural language processing (NLP) tasks, their deployment poses substantial challenges due to high computational and memory demands in real-world applications. Recent studies have focused on enhancing smaller models through knowledge distillation from LLMs, yielding promising results. However, these models often struggle to match the performance of LLMs, especially in tasks that require reasoning. In this work, we introduce Mixed Distillation (MD) framework, which capitalizes on the strengths of Program of Thought (PoT) and Chain of Thought (CoT) capabilities within LLMs, combining multiple prompting techniques and distilling these capabilities into smaller models. Our experimental results show that MD significantly enhances the single-path and multi-path reasoning ability of smaller models in various tasks. In terms of accuracy and generality of reasoning tasks, the model generated by it exceeds the comprehensive performance of two individually distilled models. Notably, LLaMA2-7B and CodeLlama-7B using MD achieved remarkable improvements of (84.5%) and (85.5%), respectively, outperforming GPT-3.5-Turbo by (2.5%) and (3.5%), on the SVAMP benchmark.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces Mixed Distillation (MD) framework, which capitalizes on the strengths of Program of Thought (PoT) and Chain of Thought (CoT) capabilities within LLMs, combining multiple prompting techniques and distilling these capabilities into smaller models."
            },
            "score": 4
        },
        {
            "id": "0a7dda04a744b2610a5e2cb3527b1cfccf1b9ba8",
            "paperId": "0a7dda04a744b2610a5e2cb3527b1cfccf1b9ba8",
            "title": "Multi-Cultural Commonsense Knowledge Distillation",
            "abstract": "Despite recent progress, large language models (LLMs) still face the challenge of appropriately reacting to the intricacies of social and cultural conventions. This paper presents MANGO, a methodology for distilling high-accuracy, high-recall assertions of cultural knowledge. We judiciously and iteratively prompt LLMs for this purpose from two entry points, concepts and cultures. Outputs are consolidated via clustering and generative summarization. Running the MANGO method with GPT-3.5 as underlying LLM yields 167K high-accuracy assertions for 30K concepts and 11K cultures, surpassing prior resources by a large margin. For extrinsic evaluation, we explore augmenting dialogue systems with cultural knowledge assertions. We find that adding knowledge from MANGO improves the overall quality, specificity, and cultural sensitivity of dialogue responses, as judged by human annotators. Data and code are available for download.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "MANGO, a methodology for distilling high-accuracy, high-recall assertions of cultural knowledge, is presented and it is found that adding knowledge from MANGO improves the overall quality, specificity, and cultural sensitivity of dialogue responses, as judged by human annotators."
            },
            "score": 4
        },
        {
            "id": "8e5af286e461ad07625e43e17d4c69e8b16d9fbb",
            "paperId": "8e5af286e461ad07625e43e17d4c69e8b16d9fbb",
            "title": "Distilled Language Models are economically efficient for the enterprise. ...mostly.",
            "abstract": "Contacting customer service via chat is a common practice. Because employing customer service agents is expensive, many companies are turning to NLP that assists human agents by auto-generating responses that can be used directly or with modifications. With their ability to handle large context windows, Large Language Models (LLMs) are a natural fit for this use case. However, their efficacy must be balanced with the cost of training and serving them. This paper assesses the practical cost and impact of LLMs for the enterprise as a function of the usefulness of the responses that they generate. We present a cost framework for evaluating an NLP model\u2019s utility for this use case and apply it to a single brand as a case study in the context of an existing agent assistance product. We compare three strategies for specializing an LLM \u2014 prompt engineering, fine-tuning, and knowledge distillation \u2014 using feedback from the brand\u2019s customer service agents. We find that the usability of a model\u2019s responses can make up for a large difference in inference cost for our case study brand, and we extrapolate our findings to the broader enterprise space.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that the usability of a model\u2019s responses can make up for a large difference in inference cost for the authors' case study brand, and the findings are extrapolated to the broader enterprise space."
            },
            "score": 4
        },
        {
            "id": "f3dceda070be6edf78f5ed8830d2b0971dc5d718",
            "paperId": "f3dceda070be6edf78f5ed8830d2b0971dc5d718",
            "title": "Compressing Pre-trained Language Models using Progressive Low Rank Decomposition",
            "abstract": "In this paper, a progressive low rank decomposition method is used to compress large-scale pre-trained transformer based language models. To this end, each fully-connected layers of the transformer modules are decomposed into two consecutive smaller ones using a progressive Singular Value Decomposition technique. In contrast to many of state-of-the-art compression methods where intensive pre-training of the compressed model is necessary, progressive LRD can provide promising performance by compressing the model in the \ufb01ne-tuning stage. Furthermore, the current state-of-the-art model compression techniques usually face a limitation in their compression ratio as the accuracy gap becomes signi\ufb01cant with compression ratios higher than 2 \u00d7 . We show that in later steps of the iterative compression where the decomposed models becomes much smaller than their original (compression factors larger than 8 \u00d7 ), Knowledge Distillation can also be used to improve the performance.",
            "year": 2021,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper uses a progressive low rank decomposition method to compress large-scale pre-trained transformer based language models and shows that in later steps of the iterative compression where the decomposed models becomes much smaller than their original, Knowledge Distillation can also be used to improve the performance."
            },
            "score": 3
        },
        {
            "id": "eda43b5766e00a0cb70e02e0f50c9b9e6e62d214",
            "paperId": "eda43b5766e00a0cb70e02e0f50c9b9e6e62d214",
            "title": "LAD: Layer-Wise Adaptive Distillation for BERT Model Compression",
            "abstract": "Recent advances with large-scale pre-trained language models (e.g., BERT) have brought significant potential to natural language processing. However, the large model size hinders their use in IoT and edge devices. Several studies have utilized task-specific knowledge distillation to compress the pre-trained language models. However, to reduce the number of layers in a large model, a sound strategy for distilling knowledge to a student model with fewer layers than the teacher model is lacking. In this work, we present Layer-wise Adaptive Distillation (LAD), a task-specific distillation framework that can be used to reduce the model size of BERT. We design an iterative aggregation mechanism with multiple gate blocks in LAD to adaptively distill layer-wise internal knowledge from the teacher model to the student model. The proposed method enables an effective knowledge transfer process for a student model, without skipping any teacher layers. The experimental results show that both the six-layer and four-layer LAD student models outperform previous task-specific distillation approaches during GLUE tasks.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Layer-wise Adaptive Distillation (LAD), a task-specific distillation framework that can be used to reduce the model size of BERT, is presented and an iterative aggregation mechanism with multiple gate blocks in LAD enables an effective knowledge transfer process for a student model, without skipping any teacher layers."
            },
            "score": 3
        },
        {
            "id": "894c7a5fd756ca31bf92561f8e12c824637bef57",
            "paperId": "894c7a5fd756ca31bf92561f8e12c824637bef57",
            "title": "Iterative Dual Domain Adaptation for Neural Machine Translation",
            "abstract": "Previous studies on the domain adaptation for neural machine translation (NMT) mainly focus on the one-pass transferring out-of-domain translation knowledge to in-domain NMT model. In this paper, we argue that such a strategy fails to fully extract the domain-shared translation knowledge, and repeatedly utilizing corpora of different domains can lead to better distillation of domain-shared translation knowledge. To this end, we propose an iterative dual domain adaptation framework for NMT. Specifically, we first pretrain in-domain and out-of-domain NMT models using their own training corpora respectively, and then iteratively perform bidirectional translation knowledge transfer (from in-domain to out-of-domain and then vice versa) based on knowledge distillation until the in-domain NMT model convergences. Furthermore, we extend the proposed framework to the scenario of multiple out-of-domain training corpora, where the above-mentioned transfer is performed sequentially between the in-domain and each out-of-domain NMT models in the ascending order of their domain similarities. Empirical results on Chinese-English and English-German translation tasks demonstrate the effectiveness of our framework.",
            "year": 2019,
            "citationCount": 29,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An iterative dual domain adaptation framework for NMT that first pretrain in-domain and out-of-domain NMT models using their own training corpora respectively, and iteratively perform bidirectional translation knowledge transfer based on knowledge distillation until the in- domain NMT model convergences."
            },
            "score": 3
        },
        {
            "id": "e78c5ba8be44c666b58352ddae76f106ee6aba87",
            "paperId": "e78c5ba8be44c666b58352ddae76f106ee6aba87",
            "title": "Papago\u2019s Submission for the WMT21 Quality Estimation Shared Task",
            "abstract": "This paper describes Papago submission to the WMT 2021 Quality Estimation Task 1: Sentence-level Direct Assessment. Our multilingual Quality Estimation system explores the combination of Pretrained Language Models and Multi-task Learning architectures. We propose an iterative training pipeline based on pretraining with large amounts of in-domain synthetic data and finetuning with gold (labeled) data. We then compress our system via knowledge distillation in order to reduce parameters yet maintain strong performance. Our submitted multilingual systems perform competitively in multilingual and all 11 individual language pair settings including zero-shot.",
            "year": 2021,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Papago's multilingual Quality Estimation system explores the combination of Pretrained Language Models and Multi-task Learning architectures and proposes an iterative training pipeline based on pretraining with large amounts of in-domain synthetic data and finetuning with gold (labeled) data."
            },
            "score": 3
        },
        {
            "id": "743193559aa9b86d8cf46ef82e45e738514af40b",
            "paperId": "743193559aa9b86d8cf46ef82e45e738514af40b",
            "title": "Sentiment analysis via dually-born-again network and sample selection",
            "abstract": "Text sentiment analysis is an important natural language processing (NLP) task and has received considerable attention in recent years. Numerous deep-learning based methods have been proposed in previous literature in terms of new deep neural networks (DNN) including new embedding strategies, new attention mechanisms, and new encoding layers. In this study, an alternative technical path is investigated to further improve the state-of-the-art performance of text sentiment analysis. An new effective learning framework is proposed that combines knowledge distillation and sample selection. A dually-born-again network (DBAN) is presented in which the teacher network and the student network are simultaneously trained through an iterative approach. A selection gate is defined to deal with training samples which are useless or even harmful for model training. Moreover, both the DBAN and sample selection are further improved by ensemble. The proposed framework can improve the existing state-of-the-art DNN models in sentiment analysis. Experimental results indicate that the proposed framework enhances the performances of existing networks. In addition, DBAN outperforms existing born-again network.",
            "year": 2020,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An new effective learning framework is proposed that combines knowledge distillation and sample selection and can improve the existing state-of-the-art DNN models in sentiment analysis."
            },
            "score": 3
        },
        {
            "id": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
            "paperId": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
            "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such\"society of minds\"approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.",
            "year": 2023,
            "citationCount": 206,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer is presented, indicating that this approach significantly enhances mathematical and strategic reasoning across a number of tasks."
            },
            "score": 3
        },
        {
            "id": "e6e3020251d354159885a7c603c0e6fa28144f2a",
            "paperId": "e6e3020251d354159885a7c603c0e6fa28144f2a",
            "title": "Reliability Check: An Analysis of GPT-3\u2019s Response to Sensitive Topics and Prompt Wording",
            "abstract": "Large language models (LLMs) have become mainstream technology with their versatile use cases and impressive performance. Despite the countless out-of-the-box applications, LLMs are still not reliable. A lot of work is being done to improve the factual accuracy, consistency, and ethical standards of these models through fine-tuning, prompting, and Reinforcement Learning with Human Feedback (RLHF), but no systematic analysis of the responses of these models to different categories of statements, or on their potential vulnerabilities to simple prompting changes is available. In this work, we analyze what confuses GPT-3: how the model responds to certain sensitive topics and what effects the prompt wording has on the model response. We find that GPT-3 correctly disagrees with obvious Conspiracies and Stereotypes but makes mistakes with common Misconceptions and Controversies. The model responses are inconsistent across prompts and settings, highlighting GPT-3\u2019s unreliability.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work analyzes what confuses GPT-3: how the model responds to certain sensitive topics and what effects the prompt wording has on the model response."
            },
            "score": 3
        },
        {
            "id": "468d1e2d75a23fecaf96fe65d8b01ff35ea5d0bd",
            "paperId": "468d1e2d75a23fecaf96fe65d8b01ff35ea5d0bd",
            "title": "Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models",
            "abstract": "Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts inserted in English reveal a clear pattern whereby the new piece of knowledge transfers only to languages with which English has a high RankC score.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy, and conducts an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level."
            },
            "score": 3
        },
        {
            "id": "36870ac7332b45edf59e5c111dc5bd24139dc1ce",
            "paperId": "36870ac7332b45edf59e5c111dc5bd24139dc1ce",
            "title": "Factual Consistency of Multilingual Pretrained Language Models",
            "abstract": "Pretrained language models can be queried for factual knowledge, with potential applications in knowledge base acquisition and tasks that require inference. However, for that, we need to know how reliable this knowledge is, and recent work has shown that monolingual English language models lack consistency when predicting factual knowledge, that is, they fill-in-the-blank differently for paraphrases describing the same fact. In this paper, we extend the analysis of consistency to a multilingual setting. We introduce a resource, mParaRel, and investigate (i) whether multilingual language models such as mBERT and XLM-R are more consistent than their monolingual counterparts;and (ii) if such models are equally consistent across languages.We find that mBERT is as inconsistent as English BERT in English paraphrases, but that both mBERT and XLM-R exhibit a high degree of inconsistency in English and even more so for all the other 45 languages.",
            "year": 2022,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "MBERT is as inconsistent as English BERT in English paraphrases, but that both mBERT and XLM-R exhibit a high degree of inconsistency in English and even more so for all the other 45 languages."
            },
            "score": 3
        },
        {
            "id": "663d743272e9ab04f54d9105a3c3a3f6e22dd1dd",
            "paperId": "663d743272e9ab04f54d9105a3c3a3f6e22dd1dd",
            "title": "FactKB: Generalizable Factuality Evaluation using Language Models Enhanced with Factual Knowledge",
            "abstract": "Evaluating the factual consistency of automatically generated summaries is essential for the progress and adoption of reliable summarization systems. Despite recent advances, existing factuality evaluation models are not robust, being especially prone to entity and relation errors in new domains. We propose FactKB, a simple new approach to factuality evaluation that is generalizable across domains, in particular with respect to entities and relations. FactKB is based on language models pretrained using facts extracted from external knowledge bases. We introduce three types of complementary factuality pretraining objectives based on direct entity facts, facts grounded in auxiliary knowledge about entities, and facts constructed compositionally through knowledge base walks. The resulting factuality evaluation model achieves state-of-the-art performance on two in-domain news summarization benchmarks as well as on three out-of-domain scientific literature datasets. Further analysis of FactKB shows improved ability to detect erroneous entities and relations in summaries and is robust and generalizable across domains.",
            "year": 2023,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "FactKB is a simple new approach to factuality evaluation that is generalizable across domains, in particular with respect to entities and relations and shows improved ability to detect erroneous entities and relation in summaries."
            },
            "score": 3
        },
        {
            "id": "41a41c75ba336dec98d58c563605f261019e5df0",
            "paperId": "41a41c75ba336dec98d58c563605f261019e5df0",
            "title": "\u201cAccording to . . . \u201d: Prompting Language Models Improves Quoting from Pre-Training Data",
            "abstract": "Large Language Models (LLMs) may hallucinate and generate fake information, despite pre-training on factual data. Inspired by the journalistic device of \u201caccording to sources\u201d, we propose according-to prompting: directing LLMs to ground responses against previously observed text. To quantify this grounding, we propose a novel evaluation metric (QUIP-Score) that measures the extent to which model-produced answers are directly found in underlying text corpora. We illustrate with experiments on three corpora (Wikipedia, PubMed, and the U.S. legal tax code) that these prompts improve grounding under our metrics, with the additional benefit of often improving end-task performance. Furthermore, prompts that ask the model to decrease grounding (or to ground to other corpora) indeed decrease QUIP-Score, indicating the ability of LLMs to increase or decrease grounded generations on request.",
            "year": 2023,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "According-to prompting is proposed: directing LLMs to ground responses against previously observed text, to quantify this grounding, and proposes a novel evaluation metric (QUIP-Score) that measures the extent to which model-produced answers are directly found in underlying text corpora."
            },
            "score": 3
        },
        {
            "id": "4ea413e5a21a743d68c92e7f169535d0543f6051",
            "paperId": "4ea413e5a21a743d68c92e7f169535d0543f6051",
            "title": "On Improving Summarization Factual Consistency from Natural Language Feedback",
            "abstract": "Despite the recent progress in language generation models, their outputs may not always meet user expectations. In this work, we study whether informational feedback in natural language can be leveraged to improve generation quality and user preference alignment. To this end, we consider factual consistency in summarization, the quality that the summary should only contain information supported by the input documents, as the user-expected preference. We collect a high-quality dataset, DeFacto, containing human demonstrations and informational natural language feedback consisting of corrective instructions, edited summaries, and explanations with respect to the factual consistency of the summary. Using our dataset, we study three natural language generation tasks: (1) editing a summary by following the human feedback, (2) generating human feedback for editing the original summary, and (3) revising the initial summary to correct factual errors by generating both the human feedback and edited summary. We show that DeFacto can provide factually consistent human-edited summaries and further insights into summarization factual consistency thanks to its informational natural language feedback. We further demonstrate that fine-tuned language models can leverage our dataset to improve the summary factual consistency, while large language models lack the zero-shot learning ability in our proposed tasks that require controllable text generation.",
            "year": 2022,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "DeFacto can provide factually consistent human-edited summaries and further insights into summarization factual consistency thanks to its informational natural language feedback, and it is demonstrated that fine-tuned language models can leverage the dataset to improve the summary factual consistency."
            },
            "score": 3
        },
        {
            "id": "06b2ac5153e3d8d05c13c82f93d7f4e13eee6d0f",
            "paperId": "06b2ac5153e3d8d05c13c82f93d7f4e13eee6d0f",
            "title": "Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites",
            "abstract": "Large language models (LLMs) have shown remarkable performance in natural language processing (NLP) tasks. To comprehend and execute diverse human instructions over image data, instruction-tuned large vision-language models (LVLMs) have been introduced. However, LVLMs may suffer from different types of object hallucinations. Nevertheless, LVLMs are evaluated for coarse-grained object hallucinations only (i.e., generated objects non-existent in the input image). The fine-grained object attributes and behaviors non-existent in the image may still be generated but not measured by the current evaluation methods. In this paper, we thus focus on reducing fine-grained hallucinations of LVLMs. We propose \\textit{ReCaption}, a framework that consists of two components: rewriting captions using ChatGPT and fine-tuning the instruction-tuned LVLMs on the rewritten captions. We also propose a fine-grained probing-based evaluation method named \\textit{Fine-Grained Object Hallucination Evaluation} (\\textit{FGHE}). Our experiment results demonstrate that ReCaption effectively reduces fine-grained object hallucination for different LVLM options and improves their text generation quality. The code can be found at https://github.com/Anonymousanoy/FOHE.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ReCaption is proposed, a framework that consists of two components: rewriting captions using ChatGPT and fine-tuning the instruction-tuned LVLMs on the rewritten captions, and a fine-grained probing-based evaluation method named \\textit{Fine-Grained Object Hallucination Evaluation} (FGHE)."
            },
            "score": 3
        },
        {
            "id": "7cfbd36c0043098589cbaf18dca2b41d8dc24abe",
            "paperId": "7cfbd36c0043098589cbaf18dca2b41d8dc24abe",
            "title": "Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre-training",
            "abstract": "Large-scale vision-language pre-trained (VLP) models are prone to hallucinate non-existent visual objects when generating text based on visual information. In this paper, we systematically study the object hallucination problem from three aspects. First, we examine recent state-of-the-art VLP models, showing that they still hallucinate frequently and models achieving better scores on standard metrics (e.g., CIDEr) could be more unfaithful. Second, we investigate how different types of image encoding in VLP influence hallucination, including region-based, grid-based, and patch-based. Surprisingly, we find that patch-based features perform the best and smaller patch resolution yields a non-trivial reduction in object hallucination. Third, we decouple various VLP objectives and demonstrate that token-level image-text alignment and controlled generation are crucial to reducing hallucination. Based on that, we propose a simple yet effective VLP loss named ObjMLM to further mitigate object hallucination. Results show that it reduces object hallucination by up to 17.4% when tested on two benchmarks (COCO Caption for in-domain and NoCaps for out-of-domain evaluation).",
            "year": 2022,
            "citationCount": 35,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a simple yet effective VLP loss named ObjMLM to further mitigate object hallucination and decouple various VLP objectives and demonstrates that token-level image-text alignment and controlled generation are crucial to reducing hallucination."
            },
            "score": 3
        },
        {
            "id": "6df5ba162b38d2853cc8431ff6f878d085c03693",
            "paperId": "6df5ba162b38d2853cc8431ff6f878d085c03693",
            "title": "SERPENT-VLM : Self-Refining Radiology Report Generation Using Vision Language Models",
            "abstract": "Radiology Report Generation (R2Gen) demonstrates how Multi-modal Large Language Models (MLLMs) can automate the creation of accurate and coherent radiological reports. Existing methods often hallucinate details in text-based reports that don't accurately reflect the image content. To mitigate this, we introduce a novel strategy, SERPENT-VLM (SElf Refining Radiology RePort GENeraTion using Vision Language Models), which improves the R2Gen task by integrating a self-refining mechanism into the MLLM framework. We employ a unique self-supervised loss that leverages similarity between pooled image representations and the contextual representations of the generated radiological text, alongside the standard Causal Language Modeling objective, to refine image-text representations. This allows the model to scrutinize and align the generated text through dynamic interaction between a given image and the generated text, therefore reducing hallucination and continuously enhancing nuanced report generation. SERPENT-VLM outperforms existing baselines such as LLaVA-Med, BiomedGPT, etc., achieving SoTA performance on the IU X-ray and Radiology Objects in COntext (ROCO) datasets, and also proves to be robust against noisy images. A qualitative case study emphasizes the significant advancements towards more sophisticated MLLM frameworks for R2Gen, opening paths for further research into self-supervised refinement in the medical imaging domain.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel strategy, SERPENT-VLM (SElf Refining Radiology RePort GENeraTion using Vision Language Models), which improves the R2Gen task by integrating a self-refining mechanism into the MLLM framework, and proves to be robust against noisy images."
            },
            "score": 3
        },
        {
            "id": "7751f6cdec0f4473c1733eec91699744a7d5176f",
            "paperId": "7751f6cdec0f4473c1733eec91699744a7d5176f",
            "title": "HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding",
            "abstract": "While large vision-language models (LVLMs) have demonstrated impressive capabilities in interpreting multi-modal contexts, they invariably suffer from object hallucinations (OH). We introduce HALC, a novel decoding algorithm designed to mitigate OH in LVLMs. HALC leverages distinct fine-grained optimal visual information in vision-language tasks and operates on both local and global contexts simultaneously. Specifically, HALC integrates a robust auto-focal grounding mechanism (locally) to correct hallucinated tokens on the fly, and a specialized beam search algorithm (globally) to significantly reduce OH while preserving text generation quality. Additionally, HALC can be integrated into any LVLMs as a plug-and-play module without extra training. Extensive experimental studies demonstrate the effectiveness of HALC in reducing OH, outperforming state-of-the-arts across four benchmarks.",
            "year": 2024,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "HALC, a novel decoding algorithm designed to mitigate OH in LVLMs, is introduced, which leverages distinct fine-grained optimal visual information in vision-language tasks and operates on both local and global contexts simultaneously."
            },
            "score": 3
        },
        {
            "id": "f767f0a883c8fc70de03fb8b65ed87e1fef5f415",
            "paperId": "f767f0a883c8fc70de03fb8b65ed87e1fef5f415",
            "title": "Hallucination Diversity-Aware Active Learning for Text Summarization",
            "abstract": "Large Language Models (LLMs) have shown propensity to generate hallucinated outputs, i.e., texts that are factually incorrect or unsupported. Existing methods for alleviating hallucinations typically require costly human annotations to identify and correct hallucinations in LLM outputs. Moreover, most of these methods focus on a specific type of hallucination, e.g., entity or token errors, which limits their effectiveness in addressing various types of hallucinations exhibited in LLM outputs. To our best knowledge, in this paper we propose the first active learning framework to alleviate LLM hallucinations, reducing costly human annotations of hallucination needed. By measuring fine-grained hallucinations from errors in semantic frame, discourse and content verifiability in text summarization, we propose HAllucination Diversity-Aware Sampling (HADAS) to select diverse hallucinations for annotations in active learning for LLM finetuning. Extensive experiments on three datasets and different backbone models demonstrate advantages of our method in effectively and efficiently mitigating LLM hallucinations.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes the first active learning framework to alleviate LLM hallucinations, reducing costly human annotations of hallucination needed, and proposes HAllucination Diversity-Aware Sampling (HADAS) to select diverse hallucinations for annotations in active learning for LLM finetuning."
            },
            "score": 3
        },
        {
            "id": "914a0f5e7eb98842f220a5082dba4f9382086f27",
            "paperId": "914a0f5e7eb98842f220a5082dba4f9382086f27",
            "title": "Language-Oriented Communication with Semantic Coding and Knowledge Distillation for Text-to-Image Generation",
            "abstract": "By integrating recent advances in large language models (LLMs) and generative models into the emerging semantic communication (SC) paradigm, in this article we put forward to a novel framework of language-oriented semantic communication (LSC). In LSC, machines communicate using human language messages that can be interpreted and manipulated via natural language processing (NLP) techniques for SC efficiency. To demonstrate LSC's potential, we introduce three innovative algorithms: 1) semantic source coding (SSC) which compresses a text prompt into its key head words capturing the prompt's syntactic essence while maintaining their appearance order to keep the prompt's context; 2) semantic channel coding (SCC) that improves robustness against errors by substituting head words with their lenghthier synonyms; and 3) semantic knowledge distillation (SKD) that produces listener-customized prompts via in-context learning the listener's language style. In a communication task for progressive text-to-image generation, the proposed methods achieve higher perceptual similarities with fewer transmissions while enhancing robustness in noisy communication channels.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Three innovative algorithms are introduced that achieve higher perceptual similarities with fewer transmissions while enhancing robustness in noisy communication channels and produces listener-customized prompts via in-context learning the listener's language style."
            },
            "score": 3
        },
        {
            "id": "3a22aad6c18a9559be3bbb197494b434b872a05a",
            "paperId": "3a22aad6c18a9559be3bbb197494b434b872a05a",
            "title": "Diffusion-NAT: Self-Prompting Discrete Diffusion for Non-Autoregressive Text Generation",
            "abstract": "Recently, continuous diffusion models (CDM) have been introduced into non-autoregressive (NAR) text-to-text generation. However, the discrete nature of text increases the difficulty of CDM to generate coherent and fluent texts, and also causes the incompatibility problem between CDM and advanced NLP techniques, especially the popular pre-trained language models (PLMs).To solve it, we propose Diffusion-NAT, which introduces discrete diffusion models (DDM) into NAR text-to-text generation and integrates BART to improve the performance.By revising the decoding process of BART and the typical settings of DDM, we unify the inference process of BART and the denoising process of DDM into the same NAR masked tokens recovering task.In this way, DDM can rely on BART to perform denoising, which can benefit from both the rich pre-learned knowledge of BART and the iterative refining paradigm of DDM.Besides, we also propose the iterative self-prompting strategy to further improve the generation quality.Experimental results on 7 datasets show that our approach can outperform competitive NAR methods, and even surpass autoregressive methods.Our code and data are released at https://github.com/RUCAIBox/DiffusionNAT.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Diffusion-NAT is proposed, which introduces discrete diffusion models (DDM) into NAR text-to-text generation and integrates BART to improve the performance and proposes the iterative self-prompting strategy to further improve the generation quality."
            },
            "score": 3
        },
        {
            "id": "ea6b0b5904d6e8eccbccb609ac35911ae967cd2c",
            "paperId": "ea6b0b5904d6e8eccbccb609ac35911ae967cd2c",
            "title": "Creating Suspenseful Stories: Iterative Planning with Large Language Models",
            "abstract": "Automated story generation has been one of the long-standing challenges in NLP. Among all dimensions of stories, *suspense* is very common in human-written stories but relatively under-explored in AI-generated stories. While recent advances in large language models (LLMs) have greatly promoted language generation in general, state-of-the-art LLMs are still unreliable when it comes to suspenseful story generation. We propose a novel iterative-prompting-based planning method that is grounded in two theoretical foundations of story suspense from cognitive psychology and narratology. This theory-grounded method works in a fully zero-shot manner and does not rely on any supervised story corpora. To the best of our knowledge, this paper is the first attempt at suspenseful story generation with LLMs. Extensive human evaluations of the generated suspenseful stories demonstrate the effectiveness of our method.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel iterative-prompting-based planning method that is grounded in two theoretical foundations of story suspense from cognitive psychology and narratology and works in a fully zero-shot manner and does not rely on any supervised story corpora."
            },
            "score": 3
        },
        {
            "id": "7baaaa623d2c3011a52e2bb515e030825fa6e36c",
            "paperId": "7baaaa623d2c3011a52e2bb515e030825fa6e36c",
            "title": "An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration",
            "abstract": "While Large Language Models (LLMs) demonstrate exceptional performance in a multitude of Natural Language Processing (NLP) tasks, they encounter challenges in practical applications, including issues with hallucinations, inadequate knowledge updating, and limited transparency in the reasoning process. To overcome these limitations, this study innovatively proposes a collaborative training-free reasoning scheme involving tight cooperation between Knowledge Graph (KG) and LLMs. This scheme first involves using LLMs to iteratively explore KG, selectively retrieving a task-relevant knowledge subgraph to support reasoning. The LLMs are then guided to further combine inherent implicit knowledge to reason on the subgraph while explicitly elucidating the reasoning process. Through such a cooperative approach, our scheme achieves more reliable knowledge-based reasoning and facilitates the tracing of the reasoning results. Experimental results show that our scheme significantly progressed across multiple datasets, notably achieving over a 10% improvement on the QALD10 dataset compared to the best baseline and the fine-tuned state-of-the-art (SOTA) work. Building on this success, this study hopes to offer a valuable reference for future research in the fusion of KG and LLMs, thereby enhancing LLMs' proficiency in solving complex issues.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study innovatively proposes a collaborative training-free reasoning scheme involving tight cooperation between Knowledge Graph (KG) and LLMs that achieves more reliable knowledge-based reasoning and facilitates the tracing of the reasoning results."
            },
            "score": 3
        },
        {
            "id": "f07c13e35f5d299dd66b0991cbd77dac0eb19508",
            "paperId": "f07c13e35f5d299dd66b0991cbd77dac0eb19508",
            "title": "When Gradient Descent Meets Derivative-Free Optimization: A Match Made in Black-Box Scenario",
            "abstract": "Large pre-trained language models (PLMs) have garnered significant attention for their versatility and potential for solving a wide spectrum of natural language processing (NLP) tasks. However, the cost of running these PLMs may be prohibitive. Furthermore, PLMs may not be open-sourced due to commercial considerations and potential risks of misuse, such as GPT-3. The parameters and gradients of PLMs are unavailable in this scenario. To solve the issue, black-box tuning has been proposed, which utilizes derivative-free optimization (DFO), instead of gradient descent, for training task-specific continuous prompts. However, these gradient-free methods still exhibit a significant gap compared to gradient-based methods. In this paper, we introduce gradient descent into black-box tuning scenario through knowledge distillation. Furthermore, we propose a novel method GDFO, which integrates gradient descent and derivative-free optimization to optimize task-specific continuous prompts in a harmonized manner. Experimental results show that GDFO can achieve significant performance gains over previous state-of-the-art methods.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel method GDFO is proposed, which integrates gradient descent and derivative-free optimization to optimize task-specific continuous prompts in a harmonized manner and can achieve significant performance gains over previous state-of-the-art methods."
            },
            "score": 3
        },
        {
            "id": "bf164cece2154cb7995f5d35479e7ce504964889",
            "paperId": "bf164cece2154cb7995f5d35479e7ce504964889",
            "title": "Datastore Distillation for Nearest Neighbor Machine Translation",
            "abstract": "Nearest neighbor machine translation (i.e., <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math></inline-formula>NN-MT) is a promising approach to enhance translation quality by equipping pre-trained neural machine translation (NMT) models with the nearest neighbor retrieval. Despite its great success, <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math></inline-formula>NN-MT typically requires ample space to store its token-level datastore, causing <inline-formula><tex-math notation=\"LaTeX\">$k$</tex-math></inline-formula>NN-MT to be less practical in edge devices or online scenarios. In this paper, inspired by the concept of knowledge distillation, we provide a new perspective to ease the storage overhead by datastore distillation, which is formalized as a constrained optimization problem. We further design a novel model-agnostic iterative nearest neighbor merging method for the datastore distillation problem to obtain an effective and efficient solution. Experiments on three benchmark datasets indicate that our approach not only reduces the volume of the datastore by up to 50% without significant performance degradation, but also outperforms other baselines by a large margin at the same compression rate. Another experiment conducted on WikiText-103 further demonstrates the effectiveness of our method in the language model task.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel model-agnostic iterative nearest neighbor merging method for the datastore distillation problem is designed to obtain an effective and efficient solution."
            },
            "score": 2
        },
        {
            "id": "b6f94c835be068e5111c5b688cbeb8c3dbacd60b",
            "paperId": "b6f94c835be068e5111c5b688cbeb8c3dbacd60b",
            "title": "LQER: Low-Rank Quantization Error Reconstruction for LLMs",
            "abstract": "Post-training quantization of Large Language Models (LLMs) is challenging. In this work, we introduce Low-rank Quantization Error Reduction (LQER), which combines quantization and low-rank approximation to recover the model capability. LQER leverages an activation-induced scale matrix to drive the singular value distribution of quantization error towards a desirable distribution, which enables nearly-lossless W4A8 quantization on various LLMs and downstream tasks without the need for knowledge distillation, grid search, or gradient-base iterative optimization. Unlike existing methods, the computation pattern of LQER eliminates the need for specialized Scatter and Gather processes to collect high-precision weights from irregular memory locations. Our W4A8 LLMs achieve near-lossless performance on six popular downstream tasks, while using 1.36$\\times$ fewer hardware resources than the leading state-of-the-art method. We will open-source our framework once the paper is accepted.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Low-rank Quantization Error Reduction (LQER) is introduced, which combines quantization and low-rank approximation to recover the model capability and enables nearly-lossless W4A8 quantization on various LLMs and downstream tasks without the need for knowledge distillation, grid search, or gradient-base iterative optimization."
            },
            "score": 2
        },
        {
            "id": "d94516685f0f55841d3676f3e655e1b50e836a6e",
            "paperId": "d94516685f0f55841d3676f3e655e1b50e836a6e",
            "title": "The NiuTrans Machine Translation Systems for WMT22",
            "abstract": "This paper describes the NiuTrans neural machine translation systems of the WMT22 General MT constrained task. We participate in four directions, including Chinese\u2192English, English\u2192Croatian, and Livonian\u2194English. Our models are based on several advanced Transformer variants, e.g., Transformer-ODE, Universal Multiscale Transformer (UMST). The main workflow consists of data filtering, large-scale data augmentation (i.e., iterative back-translation, iterative knowledge distillation), and specific-domain fine-tuning. Moreover, we try several multi-domain methods, such as a multi-domain model structure and a multi-domain data clustering method, to rise to this year\u2019s newly proposed multi-domain test set challenge. For low-resource scenarios, we build a multi-language translation model to enhance the performance, and try to use the pre-trained language model (mBERT) to initialize the translation model.",
            "year": 2022,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The NiuTrans neural machine translation systems of the WMT22 General MT constrained task are described, including Chinese\u2192English, English\u2192Croatian, and Livonian\u2194English, based on several advanced Transformer variants, e.g., Transformer-ODE, Universal Multiscale Transformer (UMST)."
            },
            "score": 2
        },
        {
            "id": "8e0dbc206db278c29c4b70eae2060db2818f72dd",
            "paperId": "8e0dbc206db278c29c4b70eae2060db2818f72dd",
            "title": "Generalization and Hallucination of Large Vision-Language Models through a Camouflaged Lens",
            "abstract": "Large Vision-Language Model (LVLM) has seen burgeoning development and increasing attention recently. In this paper, we propose a novel framework, camo-perceptive vision-language framework (CPVLF), to explore whether LVLM can generalize to the challenging camouflaged object detection (COD) scenario in a training-free manner. During the process of generalization, we find that due to hallucination issues within LVLM, it can erroneously perceive objects in camouflaged scenes, producing counterfactual concepts. Moreover, as LVLM is not specifically trained for the precise localization of camouflaged objects, it exhibits a degree of uncertainty in accurately pinpointing these objects. Therefore, we propose chain of visual perception, which enhances LVLM's perception of camouflaged scenes from both linguistic and visual perspectives, reducing the hallucination issue and improving its capability in accurately locating camouflaged objects. We validate the effectiveness of CPVLF on three widely used COD datasets, and the experiments show the potential of LVLM in the COD task.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel framework, camo-perceptive vision-language framework (CPVLF), is proposed, which enhances LVLM's perception of camouflaged scenes from both linguistic and visual perspectives, reducing the hallucination issue and improving its capability in accurately locating camouflaged objects."
            },
            "score": 2
        },
        {
            "id": "f99116659c7522941c2353f23bddd07251adaccc",
            "paperId": "f99116659c7522941c2353f23bddd07251adaccc",
            "title": "BTR: Binary Token Representations for Efficient Retrieval Augmented Language Models",
            "abstract": "Retrieval augmentation addresses many critical problems in large language models such as hallucination, staleness, and privacy leaks. However, running retrieval-augmented language models (LMs) is slow and difficult to scale due to processing large amounts of retrieved text. We introduce binary token representations (BTR), which use 1-bit vectors to precompute every token in passages, significantly reducing computation during inference. Despite the potential loss of accuracy, our new calibration techniques and training objectives restore performance. Combined with offline and runtime compression, this only requires 127GB of disk space for encoding 3 billion tokens in Wikipedia. Our experiments show that on five knowledge-intensive NLP tasks, BTR accelerates state-of-the-art inference by up to 4x and reduces storage by over 100x while maintaining over 95% task performance.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "BTR is introduced, which use 1-bit vectors to precompute every token in passages, significantly reducing computation during inference, and accelerates state-of-the-art inference by up to 4x and reduces storage by over 100x while maintaining over 95% task performance."
            },
            "score": 2
        },
        {
            "id": "d16f8d624ab16c8bb35dde676f522b66a771d271",
            "paperId": "d16f8d624ab16c8bb35dde676f522b66a771d271",
            "title": "Large Language Models are Null-Shot Learners",
            "abstract": "This paper presents null-shot prompting. Null-shot prompting exploits hallucination in large language models (LLMs) by instructing LLMs to utilize information from the\"Examples\"section that never exists within the provided context to perform a task. While reducing hallucination is crucial and non-negligible for daily and critical uses of LLMs, we propose that in the current landscape in which these LLMs still hallucinate, it is possible, in fact, to exploit hallucination to increase performance in performing tasks compared to standard zero-shot prompting. Experiments with eight LLMs show improvements in performance across the majority of eight datasets, including reading comprehension, arithmetic reasoning, and closed-book question answering. The observed inconsistency in increased relative performance across the LLMs also potentially indicates a different degree of inherent hallucination in each model. These differences show that it is possible to utilize null-shot prompting as a way to detect degrees of hallucination in LLMs using existing benchmarking datasets. We also perform ablation studies, including experimenting with a modified version of null-shot prompting that incorporates ideas from zero-shot chain-of-thought prompting, which shows different trends of results.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experiments with eight LLMs show improvements in performance across the majority of eight datasets, including reading comprehension, arithmetic reasoning, and closed-book question answering, and differences show that it is possible to utilize null-shot prompting as a way to detect degrees of hallucination in LLMs using existing benchmarking datasets."
            },
            "score": 2
        },
        {
            "id": "99e52537a642dc05c152b68a774cdb7c20ed4693",
            "paperId": "99e52537a642dc05c152b68a774cdb7c20ed4693",
            "title": "Distilling and Retrieving Generalizable Knowledge for Robot Manipulation via Language Corrections",
            "abstract": "Today's robot policies exhibit subpar performance when faced with the challenge of generalizing to novel environments. Human corrective feedback is a crucial form of guidance to enable such generalization. However, adapting to and learning from online human corrections is a non-trivial endeavor: not only do robots need to remember human feedback over time to retrieve the right information in new settings and reduce the intervention rate, but also they would need to be able to respond to feedback that can be arbitrary corrections about high-level human preferences to low-level adjustments to skill parameters. In this work, we present Distillation and Retrieval of Online Corrections (DROC), a large language model (LLM)-based system that can respond to arbitrary forms of language feedback, distill generalizable knowledge from corrections, and retrieve relevant past experiences based on textual and visual similarity for improving performance in novel settings. DROC is able to respond to a sequence of online language corrections that address failures in both high-level task plans and low-level skill primitives. We demonstrate that DROC effectively distills the relevant information from the sequence of online corrections in a knowledge base and retrieves that knowledge in settings with new task or object instances. DROC outperforms other techniques that directly generate robot code via LLMs by using only half of the total number of corrections needed in the first round and requires little to no corrections after two iterations. We show further results, videos, prompts and code on https://sites.google.com/stanford.edu/droc .",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Distillation and Retrieval of Online Corrections (DROC), a large language model (LLM)-based system that can respond to arbitrary forms of language feedback, distill generalizable knowledge from corrections, and retrieve relevant past experiences based on textual and visual similarity for improving performance in novel settings is presented."
            },
            "score": 2
        },
        {
            "id": "331706215baf547e7782b07fcea66b64b754df08",
            "paperId": "331706215baf547e7782b07fcea66b64b754df08",
            "title": "Fast Inference Through The Reuse Of Attention Maps In Diffusion Models",
            "abstract": "Text-to-image diffusion models have demonstrated unprecedented abilities at flexible and realistic image synthesis. However, the iterative process required to produce a single image is costly and incurs a high latency, prompting researchers to further investigate its efficiency. Typically, improvements in latency have been achieved in two ways: (1) training smaller models through knowledge distillation (KD); and (2) adopting techniques from ODE-theory to facilitate larger step sizes. In contrast, we propose a training-free approach that does not alter the step-size of the sampler. Specifically, we find the repeated calculation of attention maps to be both costly and redundant; therefore, we propose a structured reuse of attention maps during sampling. Our initial reuse policy is motivated by rudimentary ODE-theory, which suggests that reuse is most suitable late in the sampling procedure. After noting a number of limitations in this theoretical approach, we empirically search for a better policy. Unlike methods that rely on KD, our reuse policies can easily be adapted to a variety of setups in a plug-and-play manner. Furthermore, when applied to Stable Diffusion-1.5, our reuse policies reduce latency with minimal repercussions on sample quality.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The initial reuse policy is motivated by rudimentary ODE-theory, which suggests that reuse is most suitable late in the sampling procedure, and can be adapted to a variety of setups in a plug-and-play manner."
            },
            "score": 2
        },
        {
            "id": "98e52738bc1fa06f3ede1e7617037abeb92503ba",
            "paperId": "98e52738bc1fa06f3ede1e7617037abeb92503ba",
            "title": "Unsupervised Text Style Transfer via LLMs and Attention Masking with Multi-way Interactions",
            "abstract": "Unsupervised Text Style Transfer (UTST) has emerged as a critical task within the domain of Natural Language Processing (NLP), aiming to transfer one stylistic aspect of a sentence into another style without changing its semantics, syntax, or other attributes. This task is especially challenging given the intrinsic lack of parallel text pairings. Among existing methods for UTST tasks, attention masking approach and Large Language Models (LLMs) are deemed as two pioneering methods. However, they have shortcomings in generating unsmooth sentences and changing the original contents, respectively. In this paper, we investigate if we can combine these two methods effectively. We propose four ways of interactions, that are pipeline framework with tuned orders; knowledge distillation from LLMs to attention masking model; in-context learning with constructed parallel examples. We empirically show these multi-way interactions can improve the baselines in certain perspective of style strength, content preservation and text fluency. Experiments also demonstrate that simply conducting prompting followed by attention masking-based revision can consistently surpass the other systems, including supervised text style transfer systems. On Yelp-clean and Amazon-clean datasets, it improves the previously best mean metric by 0.5 and 3.0 absolute percentages respectively, and achieves new SOTA results.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Four ways of interactions are proposed, that are pipeline framework with tuned orders; knowledge distillation from LLMs to attention masking model; in-context learning with constructed parallel examples; in-context learning with constructed parallel examples; and multi-way interactions, that can improve the baselines in certain perspective of style strength, content preservation and text fluency."
            },
            "score": 2
        },
        {
            "id": "65c16237665cc9b0c3d055e8b2f9b532620ba570",
            "paperId": "65c16237665cc9b0c3d055e8b2f9b532620ba570",
            "title": "Reinforcement Learning Problem Solving with Large Language Models",
            "abstract": "Large Language Models (LLMs) encapsulate an extensive amount of world knowledge, and this has enabled their application in various domains to improve the performance of a variety of Natural Language Processing (NLP) tasks. This has also facilitated a more accessible paradigm of conversation-based interactions between humans and AI systems to solve intended problems. However, one interesting avenue that shows untapped potential is the use of LLMs as Reinforcement Learning (RL) agents to enable conversational RL problem solving. Therefore, in this study, we explore the concept of formulating Markov Decision Process-based RL problems as LLM prompting tasks. We demonstrate how LLMs can be iteratively prompted to learn and optimize policies for specific RL tasks. In addition, we leverage the introduced prompting technique for episode simulation and Q-Learning, facilitated by LLMs. We then show the practicality of our approach through two detailed case studies for\"Research Scientist\"and\"Legal Matter Intake\"workflows.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study explores the concept of formulating Markov Decision Process-based RL problems as LLM prompting tasks and demonstrates how LLMs can be iteratively prompted to learn and optimize policies for specific RL tasks."
            },
            "score": 2
        }
    ],
    "novelty": "yes"
}