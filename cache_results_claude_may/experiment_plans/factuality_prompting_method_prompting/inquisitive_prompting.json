{
    "topic_description": "novel prompting methods that can improve factuality and reduce hallucination of large language models",
    "idea_name": "Inquisitive Prompting",
    "raw_idea": {
        "Problem": "Large language models often generate overconfident and assertive responses, even when they are uncertain or lack sufficient information to answer a question accurately. This can lead to the generation of hallucinated or incorrect information.",
        "Existing Methods": "Existing methods for reducing hallucination, such as using calibrated confidence scores or adversarial filtering, focus on post-processing the generated outputs to identify and remove potentially hallucinated content.",
        "Motivation": "We propose that encouraging the model to ask clarifying questions when it is uncertain can help to reduce hallucination and improve the accuracy of generated responses. By prompting the model to adopt an inquisitive stance and seek additional information when needed, we can mitigate the tendency to generate overconfident and assertive responses.",
        "Proposed Method": "We introduce Inquisitive Prompting (InqP), a prompting method that encourages the model to ask clarifying questions when it is uncertain about how to answer a given prompt. Given a question or prompt, we first ask the model to assess its confidence in being able to answer the question accurately. If the model's confidence is below a certain threshold, we prompt it to generate a clarifying question that would help it to answer the original question more accurately. We then provide the model with the answer to its clarifying question (either manually or by querying an external knowledge base), and prompt it to generate a final answer based on the original question and the additional information provided.",
        "Experiment Plan": "We will evaluate InqP on a range of question-answering and open-ended generation tasks, such as answering questions from the Natural Questions dataset and generating summaries of news articles. We will compare the factuality and accuracy of responses generated by InqP to those generated by standard prompting methods, as well as to human-generated responses. We will also conduct a human evaluation to assess the effectiveness of the clarifying questions generated by the model in eliciting additional information and improving the accuracy of the final responses."
    },
    "full_experiment_plan": {
        "Title": "Inquisitive Prompting: Encouraging Language Models to Ask Clarifying Questions for Improved Factuality",
        "Problem Statement": "Large language models often generate overconfident and assertive responses, even when they are uncertain or lack sufficient information to answer a question accurately. This can lead to the generation of hallucinated or incorrect information.",
        "Motivation": "Existing methods for reducing hallucination, such as using calibrated confidence scores or adversarial filtering, focus on post-processing the generated outputs to identify and remove potentially hallucinated content. We propose that encouraging the model to ask clarifying questions when it is uncertain can help to reduce hallucination and improve the accuracy of generated responses. By prompting the model to adopt an inquisitive stance and seek additional information when needed, we can mitigate the tendency to generate overconfident and assertive responses.",
        "Proposed Method": "We introduce Inquisitive Prompting (InqP), a prompting method that encourages the model to ask clarifying questions when it is uncertain about how to answer a given prompt. Given a question or prompt, we first ask the model to assess its confidence in being able to answer the question accurately. If the model's confidence is below a certain threshold, we prompt it to generate a clarifying question that would help it to answer the original question more accurately. We then provide the model with the answer to its clarifying question (either manually or by querying an external knowledge base), and prompt it to generate a final answer based on the original question and the additional information provided.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We will evaluate InqP on a range of question-answering and open-ended generation tasks, such as answering questions from the Natural Questions dataset, TriviaQA, and HotpotQA, and generating summaries of news articles from the CNN/DailyMail dataset. We will use standard metrics such as exact match, F1 score, and ROUGE for evaluation.",
            "Step 2: Construct Prompts": "For each dataset, we will construct the following prompts:\n1. Baseline prompt: The original question or prompt, without any modifications.\n2. Confidence assessment prompt: 'Given the following question: [QUESTION], how confident are you that you can answer this question accurately? Please provide a confidence score between 0 and 1.'\n3. Clarifying question prompt (if confidence < 0.8): 'Given the following question: [QUESTION], what additional information would you need to answer this question more accurately? Please generate a clarifying question.'\n4. Final answer prompt: 'Given the following question: [QUESTION], and the additional information: [CLARIFICATION_ANSWER], please provide a final answer to the original question.'",
            "Step 3: Select Models": "We will evaluate InqP using GPT-3.5 (text-davinci-002) and GPT-4 models from OpenAI. We will compare the performance of these models with and without InqP.",
            "Step 4: Get Results": "For each dataset and model combination, we will generate responses using both the baseline and InqP prompts. For InqP, we will manually provide answers to the generated clarifying questions. We will then evaluate the accuracy and factuality of the generated responses using the metrics specified in Step 1.",
            "Step 5: Analyze Results": "We will compare the performance of the models with and without InqP across all datasets and metrics. We will also conduct a qualitative analysis of the generated clarifying questions and final answers to assess the effectiveness of InqP in reducing hallucination and improving factuality. Additionally, we will conduct a human evaluation on a subset of the generated responses to assess their coherence, relevance, and overall quality."
        },
        "Test Case Examples": {
            "Example 1": {
                "Baseline Prompt Input": "What is the capital of France?",
                "Baseline Prompt Expected Output": "The capital of France is Paris.",
                "InqP Prompt Input (Confidence Assessment)": "Given the following question: What is the capital of France?, how confident are you that you can answer this question accurately? Please provide a confidence score between 0 and 1.",
                "InqP Prompt Expected Output (Confidence Assessment)": "0.95",
                "InqP Prompt Input (Final Answer, no clarification needed)": "Given the following question: What is the capital of France?, please provide a final answer to the original question.",
                "InqP Prompt Expected Output (Final Answer)": "The capital of France is Paris."
            },
            "Example 2": {
                "Baseline Prompt Input": "What is the largest moon of Neptune?",
                "Baseline Prompt Expected Output": "The largest moon of Neptune is Triton. It was discovered in 1846 by William Lassell, just 17 days after Neptune itself was discovered.",
                "InqP Prompt Input (Confidence Assessment)": "Given the following question: What is the largest moon of Neptune?, how confident are you that you can answer this question accurately? Please provide a confidence score between 0 and 1.",
                "InqP Prompt Expected Output (Confidence Assessment)": "0.7",
                "InqP Prompt Input (Clarifying Question)": "Given the following question: What is the largest moon of Neptune?, what additional information would you need to answer this question more accurately? Please generate a clarifying question.",
                "InqP Prompt Expected Output (Clarifying Question)": "To confirm, are you asking about the largest moon of Neptune in terms of diameter, mass, or some other measure?",
                "InqP Prompt Input (Clarification Answer)": "I am asking about the largest moon of Neptune in terms of diameter.",
                "InqP Prompt Input (Final Answer)": "Given the following question: What is the largest moon of Neptune?, and the additional information: I am asking about the largest moon of Neptune in terms of diameter., please provide a final answer to the original question.",
                "InqP Prompt Expected Output (Final Answer)": "The largest moon of Neptune in terms of diameter is Triton, with a diameter of 2,706 km. It is the seventh-largest moon in the Solar System and was discovered in 1846 by William Lassell, just 17 days after Neptune itself was discovered."
            }
        },
        "Fallback Plan": "If the proposed InqP method does not significantly improve performance over the baselines, we can conduct additional analyses to understand why. Some potential avenues for investigation include:\n1. Analyzing the quality and relevance of the generated clarifying questions to determine if they are effective in eliciting useful additional information.\n2. Experimenting with different confidence thresholds for triggering the generation of clarifying questions to optimize the trade-off between asking for clarification and generating direct answers.\n3. Exploring alternative methods for answering the generated clarifying questions, such as using external knowledge bases or crowdsourcing, to assess the impact of the quality of the clarification answers on the final response.\n4. Conducting error analysis on the generated responses to identify common failure modes and potential areas for improvement in the prompting strategy.\nIf these analyses do not yield insights that can be used to improve InqP, we can consider alternative approaches, such as using retrieval-augmented generation or fine-tuning the models on datasets that explicitly include clarifying questions and answers. Alternatively, we can focus on characterizing the types of questions and domains where InqP is most effective, and propose hybrid strategies that combine InqP with other methods for reducing hallucination in large language models."
    },
    "novelty_queries": [
        "KeywordQuery(\"language models clarifying questions uncertainty\")",
        "KeywordQuery(\"language models inquisitive prompting hallucination\")",
        "KeywordQuery(\"language models confidence scores question answering\")",
        "KeywordQuery(\"Inquisitive Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "61be5b2d4b73cde6b11a8b988aea95c22364c86e",
            "paperId": "61be5b2d4b73cde6b11a8b988aea95c22364c86e",
            "title": "STaR-GATE: Teaching Language Models to Ask Clarifying Questions",
            "abstract": "When prompting language models to complete a task, users often leave important aspects unsaid. While asking questions could resolve this ambiguity (GATE; Li et al., 2023), models often struggle to ask good questions. We explore a language model's ability to self-improve (STaR; Zelikman et al., 2022) by rewarding the model for generating useful questions-a simple method we dub STaR-GATE. We generate a synthetic dataset of 25,500 unique persona-task prompts to simulate conversations between a pretrained language model-the Questioner-and a Roleplayer whose preferences are unknown to the Questioner. By asking questions, the Questioner elicits preferences from the Roleplayer. The Questioner is iteratively finetuned on questions that increase the probability of high-quality responses to the task, which are generated by an Oracle with access to the Roleplayer's latent preferences. After two iterations of self-improvement, the Questioner asks better questions, allowing it to generate responses that are preferred over responses from the initial model on 72% of tasks. Our results indicate that teaching a language model to ask better questions leads to better personalized responses.",
            "year": 2024,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results indicate that teaching a language model to ask better questions leads to better personalized responses."
            },
            "score": 8,
            "novelty_score": "The project proposal aims to encourage language models to ask clarifying questions when uncertain, in order to reduce hallucination and improve factuality. The approach is to use a prompting method called Inquisitive Prompting (InqP) that assesses the model's confidence and prompts it to generate clarifying questions when needed.\n\nThe paper explores a language model's ability to self-improve by rewarding it for generating useful questions, in order to elicit unknown preferences from a Roleplayer and generate better personalized responses. The approach is to iteratively finetune the model on questions that increase the probability of high-quality responses.\n\nWhile both the proposal and the paper focus on improving language model outputs by asking questions, the specific research problems and approaches differ. The proposal targets reducing hallucination and improving factuality, while the paper aims to generate better personalized responses based on elicited preferences. The proposal uses a prompting method, while the paper employs iterative finetuning.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "7154e7f42e35a7e5f0795f5d674a8a5356584c04",
            "paperId": "7154e7f42e35a7e5f0795f5d674a8a5356584c04",
            "title": "CLAM: Selective Clarification for Ambiguous Questions with Generative Language Models",
            "abstract": "Users often ask dialogue systems ambiguous questions that require clarification. We show that current language models rarely ask users to clarify ambiguous questions and instead provide incorrect answers. To address this, we introduce CLAM: a framework for getting language models to selectively ask for clarification about ambiguous user questions. In particular, we show that we can prompt language models to detect whether a given question is ambiguous, generate an appropriate clarifying question to ask the user, and give a final answer after receiving clarification. We also show that we can simulate users by providing language models with privileged information. This lets us automatically evaluate multi-turn clarification dialogues. Finally, CLAM significantly improves language models' accuracy on mixed ambiguous and unambiguous questions relative to SotA.",
            "year": 2022,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work shows that it can prompt language models to detect whether a given question is ambiguous, generate an appropriate clarifying question to ask the user, and give a final answer after receiving clarification and significantly improves language models' accuracy on mixed ambiguous and unambiguous questions relative to SotA."
            },
            "score": 8,
            "novelty_score": "The project proposal aims to improve the factuality of language model responses by encouraging the models to ask clarifying questions when uncertain, while the paper focuses on getting language models to selectively ask for clarification about ambiguous user questions.\n\nThe project proposes Inquisitive Prompting (InqP) to prompt the model to generate clarifying questions when its confidence is low, whereas the paper introduces CLAM, a framework for prompting language models to detect ambiguous questions, generate clarifying questions, and provide final answers after receiving clarification.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "4ca1057d642caf8a7b817c4aeb6198801bc69758",
            "paperId": "4ca1057d642caf8a7b817c4aeb6198801bc69758",
            "title": "CLAM: Selective Clarification for Ambiguous Questions with Large Language Models",
            "abstract": "State-of-the-art language models are often accurate on many question-answering benchmarks with well-de\ufb01ned questions. Yet, in real settings questions are often unanswerable without asking the user for clarifying information. We show that current SotA models often do not ask the user for clari\ufb01cation when presented with imprecise questions and instead provide incorrect answers or \u2018hallucinate\u2019. To address this, we introduce CLAM, a framework that \ufb01rst uses the model to detect ambiguous questions, and if an ambiguous question is detected, prompts the model to ask the user for clari\ufb01cation. Furthermore, we show how to construct a scalable and cost-effective automatic evaluation protocol using an oracle language model with privileged information to provide clarifying information. We show that our method achieves a 20.15 percentage point accuracy improvement over SotA on a novel ambiguous question-answering answering data set derived from TriviaQA.",
            "year": 2022,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "CLAM is introduced, a framework that first uses the model to detect ambiguous questions, and if an ambiguous question is detected, prompts themodel to ask the user for clari\ufb01cation, and how to construct a scalable and cost-effective automatic evaluation protocol using an oracle language model with privileged information to provide clarifying information is shown."
            },
            "score": 8,
            "novelty_score": "The project proposal aims to improve the factuality of language model outputs by encouraging the model to ask clarifying questions when it is uncertain, using a method called Inquisitive Prompting (InqP). The paper proposes CLAM, a framework that detects ambiguous questions and prompts the model to ask the user for clarification to improve question-answering accuracy.\n\nBoth the project proposal and the paper address the problem of language models providing incorrect or overconfident answers when faced with ambiguous or unanswerable questions. They also share a similar approach of prompting the model to ask for clarification when needed.\n\nYes",
            "novelty_judgment": "yes"
        },
        {
            "id": "eae87af6e956532d6842718803179c9ab2386ea9",
            "paperId": "eae87af6e956532d6842718803179c9ab2386ea9",
            "title": "Large Language Models Should Ask Clarifying Questions to Increase Confidence in Generated Code",
            "abstract": "Large language models (LLMs) have significantly improved the ability to perform tasks in the field of code generation. However, there is still a gap between LLMs being capable coders and being top-tier software engineers. Based on the observation that toplevel software engineers often ask clarifying questions to reduce ambiguity in both requirements and coding solutions, I argue that the same should be applied to LLMs for code generation tasks. By asking probing questions in various topics before generating the final code, the challenges of programming with LLMs, such as unclear intent specification, lack of computational thinking, and undesired code quality, may be alleviated. This, in turn, increases confidence in the generated code. In this work, I explore how to leverage better communication skills to achieve greater confidence in generated code. I propose a communication-centered process that uses an LLM-generated communicator to identify issues with high ambiguity or low confidence in problem descriptions and generated code. I then ask clarifying questions to obtain responses from users for refining the code.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a communication-centered process that uses an LLM-generated communicator to identify issues with high ambiguity or low confidence in problem descriptions and generated code, and asks clarifying questions to obtain responses from users for refining the code."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is reducing hallucination and improving factuality in large language models by encouraging them to ask clarifying questions when uncertain. The approach is to use a prompting method called Inquisitive Prompting (InqP) that assesses the model's confidence and prompts it to generate clarifying questions if the confidence is below a threshold.\n\nThe research problem in the paper is improving confidence in code generated by large language models. The approach is to use a communication-centered process where an LLM-generated communicator identifies issues with high ambiguity or low confidence in problem descriptions and generated code, and asks clarifying questions to refine the code.\n\nWhile both works aim to improve the output quality of large language models, the proposal focuses on reducing hallucination in question-answering and open-ended generation tasks, while the paper focuses specifically on improving confidence in generated code. The approaches also differ, with the proposal using a prompting method to encourage clarifying questions, and the paper using an LLM-generated communicator to identify ambiguity and ask questions.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "0a51afdcd7cf4f33987d766082a7d3f174936c8a",
            "paperId": "0a51afdcd7cf4f33987d766082a7d3f174936c8a",
            "title": "Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models",
            "abstract": "In the face of uncertainty, the ability to seek information is of fundamental importance. In many practical applications, such as medical diagnosis and troubleshooting, the information needed to solve the task is not initially given, and has to be actively sought by asking follow-up questions (for example, a doctor asking a patient for more details about their symptoms). In this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment large language models with the ability to actively seek information by asking effective questions. UoT combines 1) an uncertainty-aware simulation approach which enables the model to simulate possible future scenarios and how likely they are to occur, 2) uncertainty-based rewards motivated by information gain which incentivizes the model to seek information, and 3) a reward propagation scheme to select the optimal question to ask in a way that maximizes the expected reward. In experiments on medical diagnosis, troubleshooting and the '20 Questions' game, UoT achieves an average performance improvement of 57.8% in the rate of successful task completion across multiple LLMs compared with direct prompting, and also improves efficiency (i.e., the number of questions needed to complete the task).",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Uncertainty of Thoughts is introduced, an algorithm to augment large language models with the ability to actively seek information by asking effective questions and achieves an average performance improvement of 57.8% in the rate of successful task completion across multiple LLMs compared with direct prompting."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to reduce hallucination in large language models by encouraging them to ask clarifying questions when uncertain, while the paper focuses on improving the ability of language models to actively seek information by asking effective questions in tasks such as medical diagnosis and troubleshooting.\n\nThe project proposes the Inquisitive Prompting (InqP) method, which prompts the model to generate clarifying questions when its confidence in answering a question is below a threshold. In contrast, the paper introduces the Uncertainty of Thoughts (UoT) algorithm, which combines uncertainty-aware simulation, uncertainty-based rewards, and reward propagation to select optimal questions.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "9386242a9ad5c1a15023043ef63d45e42ceac546",
            "paperId": "9386242a9ad5c1a15023043ef63d45e42ceac546",
            "title": "Selectively Answering Ambiguous Questions",
            "abstract": "Trustworthy language models should abstain from answering questions when they do not know the answer. However, the answer to a question can be unknown for a variety of reasons. Prior research has focused on the case in which the question is clear and the answer is unambiguous but possibly unknown, but the answer to a question can also be unclear due to uncertainty of the questioner's intent or context. We investigate question answering from this perspective, focusing on answering a subset of questions with a high degree of accuracy, from a set of questions in which many are inherently ambiguous. In this setting, we find that the most reliable approach to decide when to abstain involves quantifying repetition within sampled model outputs, rather than the model's likelihood or self-verification as used in prior work. We find this to be the case across different types of uncertainty and model scales,and with or without instruction tuning. Our results suggest that sampling-based confidence scores help calibrate answers to relatively unambiguous questions, with more dramatic improvements on ambiguous questions.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work finds that the most reliable approach to decide when to abstain from answering questions involves quantifying repetition within sampled model outputs, rather than the model's likelihood or self-verification as used in prior work."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is reducing hallucination and improving factuality in language model outputs by encouraging the model to ask clarifying questions when uncertain. The approach is to use a prompting method called Inquisitive Prompting (InqP) that assesses the model's confidence and prompts it to generate clarifying questions if the confidence is below a threshold.\n\nThe research problem in the paper is selectively answering questions from a set of questions where many are inherently ambiguous. The approach is to use sampling-based confidence scores to quantify repetition within sampled model outputs and decide when to abstain from answering.\n\nWhile both works aim to improve the reliability of question answering, the proposal focuses on reducing hallucination by asking clarifying questions, while the paper focuses on selectively answering questions from an ambiguous set. The approaches are also different, with the proposal using confidence-based prompting and the paper using sampling-based confidence scores.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "ab4ce5dda7ad4d9032995c9c049a89d65723c6aa",
            "paperId": "ab4ce5dda7ad4d9032995c9c049a89d65723c6aa",
            "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
            "abstract": "A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated. However, the most widely-used LMs are fine-tuned with reinforcement learning from human feedback (RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated. In light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50%.",
            "year": 2023,
            "citationCount": 96,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "For RLHF-LMs such as ChatGPT, GPT-4, and Claude, it is found that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50%."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to reduce hallucination in language models by encouraging them to ask clarifying questions when uncertain, while the paper focuses on improving the calibration of confidence scores in language models fine-tuned with human feedback.\n\nThe project proposes a new prompting method called Inquisitive Prompting (InqP) to generate clarifying questions when the model is uncertain, whereas the paper evaluates existing methods for extracting better-calibrated confidence scores from RLHF-LMs.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "c22bfecc684be370bc22611deb8737d65466a390",
            "paperId": "c22bfecc684be370bc22611deb8737d65466a390",
            "title": "Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models",
            "abstract": "This paper investigates the capabilities of Large Language Models (LLMs) in the context of understanding their own knowledge and measuring their uncertainty. We argue this is an important feature for mitigating hallucinations. Specifically, we focus on addressing \\textit{known-unknown} questions, characterized by high uncertainty due to the absence of definitive answers. To facilitate our study, we collect a dataset with new Known-Unknown Questions (KUQ) and propose a novel categorization scheme to elucidate the sources of uncertainty. Subsequently, we assess the LLMs' ability to differentiate between known and unknown questions and classify them accordingly. Moreover, we evaluate the quality of their answers in an Open-Ended QA setting. To quantify the uncertainty expressed in the answers, we create a semantic evaluation method that measures the model's accuracy in expressing uncertainty between known vs unknown questions.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper investigates the capabilities of Large Language Models in the context of understanding their own knowledge and measuring their uncertainty, and creates a semantic evaluation method that measures the model's accuracy in expressing uncertainty between known vs unknown questions."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is reducing hallucination in language models by encouraging them to ask clarifying questions when uncertain. The approach is to use a prompting method called Inquisitive Prompting (InqP) that assesses the model's confidence and prompts it to generate clarifying questions if the confidence is low.\n\nThe research problem in the paper is understanding the capabilities of language models in measuring their own uncertainty, specifically for known-unknown questions. The approach is to collect a dataset of known-unknown questions, categorize the sources of uncertainty, and evaluate the models' ability to differentiate between known and unknown questions and express uncertainty in their answers.\n\nWhile both works aim to address the issue of language model uncertainty, the proposal focuses on reducing hallucination by encouraging clarifying questions, while the paper explores the models' ability to understand and express their own uncertainty. The approaches and specific research questions differ.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "eb971944bccf9793ac463c3e2f4d4251d4e8e071",
            "paperId": "eb971944bccf9793ac463c3e2f4d4251d4e8e071",
            "title": "Do Large Language Models Know What They Don't Know?",
            "abstract": "Large language models (LLMs) have a wealth of knowledge that allows them to excel in various Natural Language Processing (NLP) tasks. Current research focuses on enhancing their performance within their existing knowledge. Despite their vast knowledge, LLMs are still limited by the amount of information they can accommodate and comprehend. Therefore, the ability to understand their own limitations on the unknows, referred to as self-knowledge, is of paramount importance. This study aims to evaluate LLMs' self-knowledge by assessing their ability to identify unanswerable or unknowable questions. We introduce an automated methodology to detect uncertainty in the responses of these models, providing a novel measure of their self-knowledge. We further introduce a unique dataset, SelfAware, consisting of unanswerable questions from five diverse categories and their answerable counterparts. Our extensive analysis, involving 20 LLMs including GPT-3, InstructGPT, and LLaMA, discovering an intrinsic capacity for self-knowledge within these models. Moreover, we demonstrate that in-context learning and instruction tuning can further enhance this self-knowledge. Despite this promising insight, our findings also highlight a considerable gap between the capabilities of these models and human proficiency in recognizing the limits of their knowledge.",
            "year": 2023,
            "citationCount": 58,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study aims to evaluate large language models' self-knowledge by assessing their ability to identify unanswerable or unknowable questions, and introduces an automated methodology to detect uncertainty in the responses of these models, providing a novel measure of their self- knowledge."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is reducing hallucination and improving factuality in large language models by encouraging them to ask clarifying questions when uncertain. The approach is to use a prompting method called Inquisitive Prompting (InqP) that assesses the model's confidence and prompts it to generate clarifying questions when needed.\n\nThe research problem in the paper is evaluating the self-knowledge of large language models, which is their ability to identify unanswerable or unknowable questions. The approach is to introduce an automated methodology to detect uncertainty in the responses of these models and create a dataset of unanswerable questions and their answerable counterparts.\n\nWhile both works focus on large language models, the research problems and approaches are different. The proposal aims to reduce hallucination by encouraging clarifying questions, while the paper evaluates the models' ability to recognize their own limitations in answering questions.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "a1e8a8842888c7cffecce53a87a800729e90c36d",
            "paperId": "a1e8a8842888c7cffecce53a87a800729e90c36d",
            "title": "R-Tuning: Teaching Large Language Models to Refuse Unknown Questions",
            "abstract": "Large language models (LLMs) have revolutionized numerous domains with their impressive performance but still face their challenges. A predominant issue is the propensity for these models to generate non-existent facts, a concern termed hallucination. Our research is motivated by the observation that previous instruction tuning methods force the model to complete a sentence no matter whether the model knows the knowledge or not. When the question is out of the parametric knowledge, it will try to make up something and fail to indicate when it lacks knowledge. In this paper, we present a new approach called Refusal-Aware Instruction Tuning (R-Tuning). This approach is formalized by first identifying the knowledge gap between parametric knowledge and the instruction tuning data. Then, we construct the refusal-aware data based on the knowledge intersection, to tune LLMs to refrain from responding to questions beyond its parametric knowledge. Experimental results demonstrate this new instruction tuning approach effectively improves a model's ability to answer known questions and refrain from answering unknown questions. Furthermore, when tested on out-of-domain datasets, the refusal ability was found to be a meta-skill that could be generalized to other tasks. Further analysis surprisingly finds that learning the uncertainty during training displays a better ability to estimate uncertainty than uncertainty-based testing. Our code will be released at https://github.com/shizhediao/R-Tuning.",
            "year": 2023,
            "citationCount": 20,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results demonstrate this new instruction tuning approach effectively improves a model's ability to answer known questions and refrain from answering unknown questions, and surprisingly finds that learning the uncertainty during training displays a better ability to estimate uncertainty than uncertainty-based testing."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is reducing hallucination and improving factuality in large language models by encouraging them to ask clarifying questions when uncertain. The approach is to use a prompting method called Inquisitive Prompting (InqP) that assesses the model's confidence and prompts it to generate clarifying questions when confidence is low.\n\nThe research problem in the paper is improving large language models' ability to refuse to answer questions that are beyond their knowledge. The approach is to use a method called Refusal-Aware Instruction Tuning (R-Tuning) that identifies the knowledge gap between the model's parametric knowledge and the instruction tuning data, and constructs refusal-aware data to tune the model to refrain from answering unknown questions.\n\nWhile both works aim to address the issue of hallucination in large language models, the proposal focuses on encouraging the model to ask for clarification, while the paper focuses on teaching the model to refuse to answer unknown questions. The approaches are different: the proposal uses a prompting method, while the paper uses a tuning method.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "bdbbbde626296772961cf9791cf4f1d89009fa04",
            "paperId": "bdbbbde626296772961cf9791cf4f1d89009fa04",
            "title": "Leveraging Event Schema to Ask Clarifying Questions for Conversational Legal Case Retrieval",
            "abstract": "Legal case retrieval is a special IR task aiming to retrieve supporting cases for a given query case. Existing works have shown that conversational search paradigm can improve users' search experience in legal case retrieval. One of the keys to a practical conversational search system is how to ask high-quality clarifying questions to initiate conversations with users and understand their search intents. Recently, Large Language Models, such as ChatGPT and GPT-4, have shown superior ability in both open-domain QA and conversations with human. Thus it is natural to believe that they could be applied to legal conversational search as well. However, our preliminary study has shown that generating clarifying questions in legal conversational search with SOTA LLMs (e.g., GPT-4) often suffers from several problems such as duplication and low-utility contents. To address these problems, we propose LeClari, which leverages legal event schema as external knowledge to instruct LLMs to generate effective clarifying questions for legal conversational search. LeClari is constructed with a prompt module and a novel legal event selection module. The former defines a prompt with legal events for clarifying question generation and the latter selects potential event types by modeling the relationships of legal event types, conversational context, and candidate cases. We also propose ranking-oriented rewards and employ the reward augmented maximum likelihood (RAML) method to optimize LeClari directly based on the final retrieval performance of the conversational legal search system. Empirical results over two widely adopted legal case retrieval datasets demonstrate the effectiveness of our approach as compared with the state-of-the-art baselines.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LeClari is proposed, which leverages legal event schema as external knowledge to instruct LLMs to generate effective clarifying questions for legal conversational search and proposes ranking-oriented rewards and the reward augmented maximum likelihood (RAML) method to optimize LeClari directly based on the final retrieval performance of the conversational legal search system."
            },
            "score": 6
        },
        {
            "id": "f9143d68be6475cac4d5d2096edcf1691c8f0356",
            "paperId": "f9143d68be6475cac4d5d2096edcf1691c8f0356",
            "title": "\"I'm Not Sure, But...\": Examining the Impact of Large Language Models' Uncertainty Expression on User Reliance and Trust",
            "abstract": "Widely deployed large language models (LLMs) can produce convincing yet incorrect outputs, potentially misleading users who may rely on them as if they were correct. To reduce such overreliance, there have been calls for LLMs to communicate their uncertainty to end users. However, there has been little empirical work examining how users perceive and act upon LLMs' expressions of uncertainty. We explore this question through a large-scale, pre-registered, human-subject experiment (N=404) in which participants answer medical questions with or without access to responses from a fictional LLM-infused search engine. Using both behavioral and self-reported measures, we examine how different natural language expressions of uncertainty impact participants' reliance, trust, and overall task performance. We find that first-person expressions (e.g.,\"I'm not sure, but...\") decrease participants' confidence in the system and tendency to agree with the system's answers, while increasing participants' accuracy. An exploratory analysis suggests that this increase can be attributed to reduced (but not fully eliminated) overreliance on incorrect answers. While we observe similar effects for uncertainty expressed from a general perspective (e.g.,\"It's not clear, but...\"), these effects are weaker and not statistically significant. Our findings suggest that using natural language expressions of uncertainty may be an effective approach for reducing overreliance on LLMs, but that the precise language used matters. This highlights the importance of user testing before deploying LLMs at scale.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Using natural language expressions of uncertainty may be an effective approach for reducing overreliance on LLMs, but that the precise language used matters, which highlights the importance of user testing before deploying LLMs at scale."
            },
            "score": 6
        },
        {
            "id": "9a8d0f0ace05d3795a9a58f94675710b89006941",
            "paperId": "9a8d0f0ace05d3795a9a58f94675710b89006941",
            "title": "Relying on the Unreliable: The Impact of Language Models' Reluctance to Express Uncertainty",
            "abstract": "As natural language becomes the default interface for human-AI interaction, there is a critical need for LMs to appropriately communicate uncertainties in downstream applications. In this work, we investigate how LMs incorporate confidence about their responses via natural language and how downstream users behave in response to LM-articulated uncertainties. We examine publicly deployed models and find that LMs are unable to express uncertainties when answering questions even when they produce incorrect responses. LMs can be explicitly prompted to express confidences, but tend to be overconfident, resulting in high error rates (on average 47%) among confident responses. We test the risks of LM overconfidence by running human experiments and show that users rely heavily on LM generations, whether or not they are marked by certainty. Lastly, we investigate the preference-annotated datasets used in RLHF alignment and find that humans have a bias against texts with uncertainty. Our work highlights a new set of safety harms facing human-LM interactions and proposes design recommendations and mitigating strategies moving forward.",
            "year": 2024,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates how LMs incorporate confidence about their responses via natural language and how downstream users behave in response to LM-articulated uncertainties, and examines publicly deployed models to find that LMs are unable to express uncertainties when answering questions even when they produce incorrect responses."
            },
            "score": 6
        },
        {
            "id": "04365f0f1db4c659c3297cb8e70c39b38ed3b487",
            "paperId": "04365f0f1db4c659c3297cb8e70c39b38ed3b487",
            "title": "Self-Evaluation Improves Selective Generation in Large Language Models",
            "abstract": "Safe deployment of large language models (LLMs) may benefit from a reliable method for assessing their generated content to determine when to abstain or to selectively generate. While likelihood-based metrics such as perplexity are widely employed, recent research has demonstrated the limitations of using sequence-level probability estimates given by LLMs as reliable indicators of generation quality. Conversely, LLMs have demonstrated strong calibration at the token level, particularly when it comes to choosing correct answers in multiple-choice questions or evaluating true/false statements. In this work, we reformulate open-ended generation tasks into token-level prediction tasks, and leverage LLMs' superior calibration at the token level. We instruct an LLM to self-evaluate its answers, employing either a multi-way comparison or a point-wise evaluation approach, with the option to include a ``None of the above'' option to express the model's uncertainty explicitly. We benchmark a range of scoring methods based on self-evaluation and evaluate their performance in selective generation using TruthfulQA and TL;DR. Through experiments with PaLM-2 and GPT-3, we demonstrate that self-evaluation based scores not only improve accuracy, but also correlate better with the overall quality of generated content.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work reformulates open-ended generation tasks into token-level prediction tasks, and leverage LLMs' superior calibration at the token level, and demonstrates that self-evaluation based scores not only improve accuracy, but also correlate better with the overall quality of generated content."
            },
            "score": 6
        },
        {
            "id": "0aa5940fda7c994675d08c41eca2a6909eb6d205",
            "paperId": "0aa5940fda7c994675d08c41eca2a6909eb6d205",
            "title": "Improving the Reliability of Large Language Models by Leveraging Uncertainty-Aware In-Context Learning",
            "abstract": "In recent years, large-scale language models (LLMs) have gained attention for their impressive text generation capabilities. However, these models often face the challenge of\"hallucination,\"which undermines their reliability. In this study, we introduce an uncertainty-aware in-context learning framework to empower the model to enhance or reject its output in response to uncertainty. Human-defined methods for estimating uncertainty typically assume that\"uncertainty is lower when the model's response is correct compared to when it is incorrect.\"However, setting a precise threshold to distinguish correctness is challenging. Therefore, we introduce uncertainty information as an intermediary variable that implicitly influences the model's behavior. Our innovative uncertainty-aware in-context learning framework involves fine-tuning the LLM using a calibration dataset. Our aim is to improve the model's responses by filtering out answers with high uncertainty while considering the model's knowledge limitations. We evaluate the model's knowledge by examining multiple responses to the same question for the presence of a correct answer. When the model lacks relevant knowledge, the response should indicate that the question cannot be answered. Conversely, when the model has relevant knowledge, the response should provide the correct answer. Extensive experiments confirm the effectiveness of our framework, leading to two key findings. First, the logit output values of the LLM partly reflect inherent uncertainty. Second, our model autonomously recognizes uncertainty, resulting in improved responses.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study introduces an uncertainty-aware in-context learning framework to empower the model to enhance or reject its output in response to uncertainty, and introduces uncertainty information as an intermediary variable that implicitly influences the model's behavior."
            },
            "score": 6
        },
        {
            "id": "3fc3460c4554a28e489a0ea6ef067b79b7d301d9",
            "paperId": "3fc3460c4554a28e489a0ea6ef067b79b7d301d9",
            "title": "Active Prompting with Chain-of-Thought for Large Language Models",
            "abstract": "The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks requiring reasoning, such as arithmetic and commonsense reasoning. It is known that the effective design of task-specific prompts is critical for LLMs' ability to produce high-quality answers. In particular, an effective approach for complex question-and-answer tasks is example-based prompting with chain-of-thought (CoT) reasoning, which significantly improves the performance of LLMs. However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks. This paper proposes a new method, Active-Prompt, to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning). For this purpose, we propose a solution to the key problem of determining which questions are the most important and helpful ones to annotate from a pool of task-specific queries. By borrowing ideas from the related problem of uncertainty-based active learning, we introduce several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation. Experimental results demonstrate the superiority of our proposed method, achieving state-of-the-art on eight complex reasoning tasks. Further analyses of different uncertainty metrics, pool sizes, zero-shot learning, and accuracy-uncertainty relationship demonstrate the effectiveness of our method. Our code will be available at https://github.com/shizhediao/active-prompt.",
            "year": 2023,
            "citationCount": 58,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new method to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning), and introduces several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation."
            },
            "score": 6
        },
        {
            "id": "620a1a585a5e433a47103c112de17553a81fcbe6",
            "paperId": "620a1a585a5e433a47103c112de17553a81fcbe6",
            "title": "Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks",
            "abstract": "Although remarkable progress has been achieved in preventing large language model (LLM) hallucinations using instruction tuning and retrieval augmentation, it remains challenging to measure the reliability of LLMs using human-crafted evaluation data which is not available for many tasks and domains and could suffer from data leakage. Inspired by adversarial machine learning, this paper aims to develop a method of automatically generating evaluation data by appropriately modifying existing data on which LLMs behave faithfully. Specifically, this paper presents AutoDebug, an LLM-based framework to use prompting chaining to generate transferable adversarial attacks in the form of question-answering examples. We seek to understand the extent to which these examples trigger the hallucination behaviors of LLMs. We implement AutoDebug using ChatGPT and evaluate the resulting two variants of a popular open-domain question-answering dataset, Natural Questions (NQ), on a collection of open-source and proprietary LLMs under various prompting settings. Our generated evaluation data is human-readable and, as we show, humans can answer these modified questions well. Nevertheless, we observe pronounced accuracy drops across multiple LLMs including GPT-4. Our experimental results show that LLMs are likely to hallucinate in two categories of question-answering scenarios where (1) there are conflicts between knowledge given in the prompt and their parametric knowledge, or (2) the knowledge expressed in the prompt is complex. Finally, we find that the adversarial examples generated by our method are transferable across all considered LLMs. The examples generated by a small model can be used to debug a much larger model, making our approach cost-effective.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents AutoDebug, an LLM-based framework to use prompting chaining to generate transferable adversarial attacks in the form of question-answering examples, and finds that the adversarial examples generated by the method are transferable across all considered LLMs."
            },
            "score": 6
        },
        {
            "id": "33422275fbb9958f55419620697faf531482699b",
            "paperId": "33422275fbb9958f55419620697faf531482699b",
            "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering",
            "abstract": "Abstract Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question, \u201cHow can we know when language models know, with confidence, the answer to a particular query?\u201d We examine this question from the point of view of calibration, the property of a probabilistic model\u2019s predicted probabilities actually being well correlated with the probabilities of correctness. We examine three strong generative models\u2014T5, BART, and GPT-2\u2014and study whether their probabilities on QA tasks are well calibrated, finding the answer is a relatively emphatic no. We then examine methods to calibrate such models to make their confidence scores correlate better with the likelihood of correctness through fine-tuning, post-hoc probability modification, or adjustment of the predicted outputs or inputs. Experiments on a diverse range of datasets demonstrate the effectiveness of our methods. We also perform analysis to study the strengths and limitations of these methods, shedding light on further improvements that may be made in methods for calibrating LMs. We have released the code at https://github.com/jzbjyb/lm-calibration.",
            "year": 2020,
            "citationCount": 233,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper examines three strong generative models -- T5, BART, and GPT-2 -- and examines methods to calibrate such models to make their confidence scores correlate better with the likelihood of correctness through fine-tuning, post-hoc probability modification, or adjustment of the predicted outputs or inputs."
            },
            "score": 6
        },
        {
            "id": "67dab0fedfbdc403a2eb882b8d0efa7e16bc25da",
            "paperId": "67dab0fedfbdc403a2eb882b8d0efa7e16bc25da",
            "title": "Strength in Numbers: Estimating Confidence of Large Language Models by Prompt Agreement",
            "abstract": "Large language models have achieved impressive few-shot performance on a wide variety of tasks. However, in many settings, users require confidence estimates for model predictions. While traditional classifiers produce scores for each label, language models instead produce scores for the generation which may not be well calibrated. We compare generations across diverse prompts and show that these can be used to create confidence scores. By utilizing more prompts we can get more precise confidence estimates and use response diversity as a proxy for confidence. We evaluate this approach across ten multiple-choice question-answering datasets using three models: T0, FLAN-T5, and GPT-3. In addition to analyzing multiple human written prompts, we automatically generate more prompts using a language model in order to produce finer-grained confidence estimates. Our method produces more calibrated confidence estimates compared to the log probability of the answer to a single prompt. These improvements could benefit users who rely on prediction confidence for integration into a larger system or in decision-making processes.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work automatically generates more prompts using a language model in order to produce finer-grained confidence estimates and produces more calibrated confidence estimates compared to the log probability of the answer to a single prompt."
            },
            "score": 6
        },
        {
            "id": "71d517ea75fac9f9fc62aefca4ad02eaa7cc0c76",
            "paperId": "71d517ea75fac9f9fc62aefca4ad02eaa7cc0c76",
            "title": "Multicalibration for Confidence Scoring in LLMs",
            "abstract": "This paper proposes the use of\"multicalibration\"to yield interpretable and reliable confidence scores for outputs generated by large language models (LLMs). Multicalibration asks for calibration not just marginally, but simultaneously across various intersecting groupings of the data. We show how to form groupings for prompt/completion pairs that are correlated with the probability of correctness via two techniques: clustering within an embedding space, and\"self-annotation\"- querying the LLM by asking it various yes-or-no questions about the prompt. We also develop novel variants of multicalibration algorithms that offer performance improvements by reducing their tendency to overfit. Through systematic benchmarking across various question answering datasets and LLMs, we show how our techniques can yield confidence scores that provide substantial improvements in fine-grained measures of both calibration and accuracy compared to existing methods.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper shows how to form groupings for prompt/completion pairs that are correlated with the probability of correctness via two techniques: clustering within an embedding space, and querying the LLM by asking it various yes-or-no questions about the prompt."
            },
            "score": 6
        },
        {
            "id": "dfab0f3ee6f47e36cccee145794cd117773e6f73",
            "paperId": "dfab0f3ee6f47e36cccee145794cd117773e6f73",
            "title": "Towards LLM-based Fact Verification on News Claims with a Hierarchical Step-by-Step Prompting Method",
            "abstract": "While large pre-trained language models (LLMs) have shown their impressive capabilities in various NLP tasks, they are still under-explored in the misinformation domain. In this paper, we examine LLMs with in-context learning (ICL) for news claim verification, and find that only with 4-shot demonstration examples, the performance of several prompting methods can be comparable with previous supervised models. To further boost performance, we introduce a Hierarchical Step-by-Step (HiSS) prompting method which directs LLMs to separate a claim into several subclaims and then verify each of them via multiple questions-answering steps progressively. Experiment results on two public misinformation datasets show that HiSS prompting outperforms state-of-the-art fully-supervised approach and strong few-shot ICL-enabled baselines.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Hierarchical Step-by-Step (HiSS) prompting method is introduced which directs LLMs to separate a claim into several subclaims and then verify each of them via multiple questions-answering steps progressively."
            },
            "score": 6
        },
        {
            "id": "ad934a9344f68fcc0b9aa704102aa48c39c5b591",
            "paperId": "ad934a9344f68fcc0b9aa704102aa48c39c5b591",
            "title": "Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models",
            "abstract": "Large language models (LLMs) specializing in natural language generation (NLG) have recently started exhibiting promising capabilities across a variety of domains. However, gauging the trustworthiness of responses generated by LLMs remains an open challenge, with limited research on uncertainty quantification (UQ) for NLG. Furthermore, existing literature typically assumes white-box access to language models, which is becoming unrealistic either due to the closed-source nature of the latest LLMs or computational constraints. In this work, we investigate UQ in NLG for black-box LLMs. We first differentiate uncertainty vs confidence: the former refers to the\"dispersion\"of the potential predictions for a fixed input, and the latter refers to the confidence on a particular prediction/generation. We then propose and compare several confidence/uncertainty metrics, applying them to selective NLG where unreliable results could either be ignored or yielded for further assessment. Experiments were carried out with several popular LLMs on question-answering datasets (for evaluation purposes). Results reveal that a simple metric for the semantic dispersion can be a reliable predictor of the quality of LLM responses, providing valuable insights for practitioners on uncertainty management when adopting LLMs. The code to replicate our experiments is available at https://github.com/zlin7/UQ-NLG.",
            "year": 2023,
            "citationCount": 37,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Results reveal that a simple metric for the semantic dispersion can be a reliable predictor of the quality of LLM responses, providing valuable insights for practitioners on uncertainty management when adopting LLMs."
            },
            "score": 5
        },
        {
            "id": "3864b52902f8315f21385c4a6d3ce6c0193e1ab9",
            "paperId": "3864b52902f8315f21385c4a6d3ce6c0193e1ab9",
            "title": "Conformal Prediction with Large Language Models for Multi-Choice Question Answering",
            "abstract": "As large language models continue to be widely developed, robust uncertainty quantification techniques will become crucial for their safe deployment in high-stakes scenarios. In this work, we explore how conformal prediction can be used to provide uncertainty quantification in language models for the specific task of multiple-choice question-answering. We find that the uncertainty estimates from conformal prediction are tightly correlated with prediction accuracy. This observation can be useful for downstream applications such as selective classification and filtering out low-quality predictions. We also investigate the exchangeability assumption required by conformal prediction to out-of-subject questions, which may be a more realistic scenario for many practical applications. Our work contributes towards more trustworthy and reliable usage of large language models in safety-critical situations, where robust guarantees of error rate are required.",
            "year": 2023,
            "citationCount": 29,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores how conformal prediction can be used to provide uncertainty quantification in language models for the specific task of multiple-choice question-answering and finds that the uncertainty estimates from conformal Prediction are tightly correlated with prediction accuracy."
            },
            "score": 5
        },
        {
            "id": "5424e311319c58847b4c690d5c91090e3b6a4ac3",
            "paperId": "5424e311319c58847b4c690d5c91090e3b6a4ac3",
            "title": "Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models",
            "abstract": "While Large Language Models (LLMs) have demonstrated remarkable potential in natural language generation and instruction following, a persistent challenge lies in their susceptibility to\"hallucinations\", which erodes trust in their outputs. Although Uncertainty Quantification (UQ) presents a promising solution, its accurate implementation within the context of LLMs remains a significant hurdle. To address this critical roadblock, our research originates from a fundamental heuristic insight: tokens within auto-regressive LLM-generated text do not equally reflect the underlying meaning. Some tokens carry greater relevance and representativeness than others, owing to the phenomenon of\"linguistic redundancy\", wherein a select few keywords suffice to convey the essence of lengthy sentences. Regrettably, existing methodologies treat all tokens with equal importance when estimating uncertainty, disregarding these inherent generative inequalities. Our analysis reveals a significant issue with state-of-the-art: numerous tokens (and sentences) of limited semantic significance receive equal or even excessive weighting during uncertainty estimation. To rectify this bias, we propose to jointly Shifting Attention to more Relevant (SAR) components, at both the token- and the sentence-levels for accurate uncertainty estimation. We conduct extensive experiments involving a range of popular\"off-the-shelf\"LLMs, including instruction-tuned LLMs such as Vicuna, WizardLM, and LLaMA-2-chat, as well as pretrained LLMs like OPT and LLaMA, with model sizes extending up to 33B parameters. We carry out evaluation across various free-form question-answering tasks, encompassing domains such as reading comprehension, science Q&A, and medical Q&A. Our experimental results demonstrate the superior performance of SAR in addressing the challenges of uncertainty estimation within the realm of LLMs.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results demonstrate the superior performance of SAR in addressing the challenges of uncertainty estimation within the realm of LLMs, and propose to jointly Shifting Attention to more Relevant (SAR) components, at both the token- and the sentence-levels for accurate uncertainty estimation."
            },
            "score": 5
        },
        {
            "id": "1e672bf4d38a93c4c140ee208216425444368fa6",
            "paperId": "1e672bf4d38a93c4c140ee208216425444368fa6",
            "title": "LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models",
            "abstract": "Large language models (LLMs) provide excellent text-generation capabilities, but standard prompting and generation methods generally do not lead to intentional or goal-directed agents and might necessitate considerable prompt tuning. This becomes particularly apparent in multi-turn conversations: even the best current LLMs rarely ask clarifying questions, engage in explicit information gathering, or take actions now that lead to better decisions after multiple turns. Reinforcement learning has the potential to leverage the powerful modeling capabilities of LLMs, as well as their internal representation of textual interactions, to create capable goal-directed language agents. This can enable intentional and temporally extended interactions, such as with humans, through coordinated persuasion and carefully crafted questions, or in goal-directed play through text games to bring about desired final outcomes. However, enabling this requires the community to develop stable and reliable reinforcement learning algorithms that can effectively train LLMs. Developing such algorithms requires tasks that can gauge progress on algorithm design, provide accessible and reproducible evaluations for multi-turn interactions, and cover a range of task properties and challenges in improving reinforcement learning algorithms. Our paper introduces the LMRL-Gym benchmark for evaluating multi-turn RL for LLMs, together with an open-source research framework containing a basic toolkit for getting started on multi-turn RL with offline value-based and policy-based RL methods. Our benchmark consists of 8 different language tasks, which require multiple rounds of language interaction and cover a range of tasks in open-ended dialogue and text games.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The LMRL-Gym benchmark for evaluating multi-turn RL for LLMs is introduced, together with an open-source research framework containing a basic toolkit for getting started on multi- turn RL with offline value-based and policy-based RL methods."
            },
            "score": 5
        },
        {
            "id": "a3cc857a10898b2eedd04ff1dd33e8b6c9b1e04c",
            "paperId": "a3cc857a10898b2eedd04ff1dd33e8b6c9b1e04c",
            "title": "CUE: An Uncertainty Interpretation Framework for Text Classifiers Built on Pre-Trained Language Models",
            "abstract": "Text classifiers built on Pre-trained Language Models (PLMs) have achieved remarkable progress in various tasks including sentiment analysis, natural language inference, and question-answering. However, the occurrence of uncertain predictions by these classifiers poses a challenge to their reliability when deployed in practical applications. Much effort has been devoted to designing various probes in order to understand what PLMs capture. But few studies have delved into factors influencing PLM-based classifiers' predictive uncertainty. In this paper, we propose a novel framework, called CUE, which aims to interpret uncertainties inherent in the predictions of PLM-based models. In particular, we first map PLM-encoded representations to a latent space via a variational auto-encoder. We then generate text representations by perturbing the latent space which causes fluctuation in predictive uncertainty. By comparing the difference in predictive uncertainty between the perturbed and the original text representations, we are able to identify the latent dimensions responsible for uncertainty and subsequently trace back to the input features that contribute to such uncertainty. Our extensive experiments on four benchmark datasets encompassing linguistic acceptability classification, emotion classification, and natural language inference show the feasibility of our proposed framework. Our source code is available at: https://github.com/lijiazheng99/CUE.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel framework, called CUE, is proposed, which aims to interpret uncertainties inherent in the predictions of PLM-based models via a variational auto-encoder, and identifies the latent dimensions responsible for uncertainty and traces back to the input features that contribute to such uncertainty."
            },
            "score": 5
        },
        {
            "id": "217e436fd23fe4184828e02a2b143835d6fd3b28",
            "paperId": "217e436fd23fe4184828e02a2b143835d6fd3b28",
            "title": "Navigating the Grey Area: How Expressions of Uncertainty and Overconfidence Affect Language Models",
            "abstract": "The increased deployment of LMs for real-world tasks involving knowledge and facts makes it important to understand model epistemology: what LMs think they know, and how their attitudes toward that knowledge are affected by language use in their inputs. Here, we study an aspect of model epistemology: how epistemic markers of certainty, uncertainty, or evidentiality like\"I'm sure it's\",\"I think it's\", or\"Wikipedia says it's\"affect models, and whether they contribute to model failures. We develop a typology of epistemic markers and inject 50 markers into prompts for question answering. We find that LMs are highly sensitive to epistemic markers in prompts, with accuracies varying more than 80%. Surprisingly, we find that expressions of high certainty result in a 7% decrease in accuracy as compared to low certainty expressions; similarly, factive verbs hurt performance, while evidentials benefit performance. Our analysis of a popular pretraining dataset shows that these markers of uncertainty are associated with answers on question-answering websites, while markers of certainty are associated with questions. These associations may suggest that the behavior of LMs is based on mimicking observed language use, rather than truly reflecting epistemic uncertainty.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that LMs are highly sensitive to epistemic markers in prompts, with accuracies varying more than 80%, and expressions of high certainty result in a 7% decrease in accuracy as compared to low certainty expressions; similarly, factive verbs hurt performance, while evidentials benefit performance."
            },
            "score": 5
        },
        {
            "id": "7c1707db9aafd209aa93db3251e7ebd593d55876",
            "paperId": "7c1707db9aafd209aa93db3251e7ebd593d55876",
            "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
            "abstract": "Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose\"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods.",
            "year": 2023,
            "citationCount": 155,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes \"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database, and demonstrates that it can detect non-factual and factual sentences and rank passages in terms of factuality."
            },
            "score": 5
        },
        {
            "id": "37710e4b0dd851bd0d9bc4994c3b32cb10279151",
            "paperId": "37710e4b0dd851bd0d9bc4994c3b32cb10279151",
            "title": "Calibrating Structured Output Predictors for Natural Language Processing",
            "abstract": "We address the problem of calibrating prediction confidence for output entities of interest in natural language processing (NLP) applications. It is important that NLP applications such as named entity recognition and question answering produce calibrated confidence scores for their predictions, especially if the applications are to be deployed in a safety-critical domain such as healthcare. However the output space of such structured prediction models are often too large to directly adapt binary or multi-class calibration methods. In this study, we propose a general calibration scheme for output entities of interest in neural network based structured prediction models. Our proposed method can be used with any binary class calibration scheme and a neural network model. Additionally, we show that our calibration method can also be used as an uncertainty-aware, entity-specific decoding step to improve the performance of the underlying model at no additional training cost or data requirements. We show that our method outperforms current calibration techniques for Named Entity Recognition, Part-of-speech tagging and Question Answering systems. We also observe an improvement in model performance from our decoding step across several tasks and benchmark datasets. Our method improves the calibration and model performance on out-of-domain test scenarios as well.",
            "year": 2020,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study proposes a general calibration scheme for output entities of interest in neural network based structured prediction models and shows that this method outperforms current calibration techniques for Named Entity Recognition, Part-of-speech tagging and Question Answering systems."
            },
            "score": 5
        },
        {
            "id": "62176de125738e3b95850d1227bac81fd646b78e",
            "paperId": "62176de125738e3b95850d1227bac81fd646b78e",
            "title": "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models",
            "abstract": "Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, Few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual efforts, Zero-shot-CoT concatenates the target problem statement with \u201cLet\u2019s think step by step\u201d as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.",
            "year": 2023,
            "citationCount": 115,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results over GPT-3 show that the proposed zero-shot prompting consistently outperforms Zero- shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem."
            },
            "score": 5
        },
        {
            "id": "9ffefdf1fcd780cb71450b0a7a29247c66aa87be",
            "paperId": "9ffefdf1fcd780cb71450b0a7a29247c66aa87be",
            "title": "The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning",
            "abstract": "Does prompting a large language model (LLM) like GPT-3 with explanations improve in-context learning? We study this question on two NLP tasks that involve reasoning over text, namely question answering and natural language inference. We test the performance of four LLMs on three textual reasoning datasets using prompts that include explanations in multiple different styles. For these tasks, we find that including explanations in the prompts for OPT, GPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to moderate accuracy improvements over standard few-show learning. However, text-davinci-002 is able to benefit more substantially. We further show that explanations generated by the LLMs may not entail the models' predictions nor be factually grounded in the input, even on simple tasks with extractive explanations. However, these flawed explanations can still be useful as a way to verify LLMs' predictions post-hoc. Through analysis in our three settings, we show that explanations judged by humans to be good--logically consistent with the input and the prediction--more likely cooccur with accurate predictions. Following these observations, we train calibrators using automatically extracted scores that assess the reliability of explanations, allowing us to improve performance post-hoc across all of our datasets.",
            "year": 2022,
            "citationCount": 95,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work studies two NLP tasks that involve reasoning over text, namely question answering and natural language inference, and shows that explanations judged by humans to be good--logically consistent with the input and the prediction--more likely cooccur with accurate predictions."
            },
            "score": 5
        },
        {
            "id": "8a9ede4be8458629d54451d241a8d0ae318d471b",
            "paperId": "8a9ede4be8458629d54451d241a8d0ae318d471b",
            "title": "Prompting with Divide-and-Conquer Program Makes Large Language Models Discerning to Hallucination and Deception",
            "abstract": "Foundation models, such as Large language Models (LLMs), have attracted significant amount of interest due to their large number of applications. Existing works show that appropriate prompt design, such as Chain-of-Thoughts, can unlock LLM's powerful capacity in diverse areas. However, when handling tasks involving repetitive sub-tasks and/or deceptive contents, such as arithmetic calculation and article-level fake news detection, existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination. To make LLM more discerning to such intermediate errors, we propose to guide LLM with a Divide-and-Conquer program that simultaneously ensures superior expressive power and disentangles task decomposition, sub-task resolution, and resolution assembly process. Theoretic analysis reveals that our strategy can guide LLM to extend the expressive power of fixed-depth Transformer. Experiments indicate that our proposed method can achieve better performance than typical prompting strategies in tasks bothered by intermediate errors and deceptive contents, such as large integer multiplication, hallucination detection and misinformation detection.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes to guide LLM with a Divide-and-Conquer program that simultaneously ensures superior expressive power and disentangles task decomposition, sub-task resolution, and resolution assembly process andoretic analysis reveals that this strategy can guide LLM to extend the expressive power of fixed-depth Transformer."
            },
            "score": 4
        },
        {
            "id": "44d74b0d77b4056ddd4c6611a76711c8bab2e0a7",
            "paperId": "44d74b0d77b4056ddd4c6611a76711c8bab2e0a7",
            "title": "Dehallucinating Large Language Models Using Formal Methods Guided Iterative Prompting",
            "abstract": "Large language models (LLMs) such as ChatGPT have been trained to generate human-like responses to natural language prompts. LLMs use a vast corpus of text data for training, and can generate coherent and contextually relevant responses to a wide range of questions and statements. Despite this remarkable progress, LLMs are prone to hallucinations making their application to safety-critical applications such as autonomous systems difficult. The hallucinations in LLMs refer to instances where the model generates responses that are not factually accurate or contextually appropriate. These hallucinations can occur due to a variety of factors, such as the model\u2019s lack of real-world knowledge, the influence of biased or inaccurate training data, or the model\u2019s tendency to generate responses based on statistical patterns rather than a true understanding of the input. While these hallucinations are a nuisance in tasks such as text summarization and question-answering, they can be catastrophic when LLMs are used in autonomy-relevant applications such as planning. In this paper, we focus on the application of LLMs in autonomous systems and sketch a novel self-monitoring and iterative prompting architecture that uses formal methods to detect these errors in the LLM response automatically. We exploit the dialog capability of LLMs to iteratively steer them to responses that are consistent with our correctness specification. We report preliminary experiments that show the promise of the proposed approach on tasks such as automated planning.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper sketches a novel self-monitoring and iterative prompting architecture that uses formal methods to detect errors in the LLM response automatically and exploits the dialog capability of LLMs to iteratively steer them to responses that are consistent with the correctness specification."
            },
            "score": 4
        },
        {
            "id": "cfce709a65f90312d2bdc1a6cf0380c19becf694",
            "paperId": "cfce709a65f90312d2bdc1a6cf0380c19becf694",
            "title": "RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models",
            "abstract": "Retrieval-augmented generation (RAG) has become a main technique for alleviating hallucinations in large language models (LLMs). Despite the integration of RAG, LLMs may still present unsupported or contradictory claims to the retrieved contents. In order to develop effective hallucination prevention strategies under RAG, it is important to create benchmark datasets that can measure the extent of hallucination. This paper presents RAGTruth, a corpus tailored for analyzing word-level hallucinations in various domains and tasks within the standard RAG frameworks for LLM applications. RAGTruth comprises nearly 18,000 naturally generated responses from diverse LLMs using RAG. These responses have undergone meticulous manual annotations at both the individual cases and word levels, incorporating evaluations of hallucination intensity. We not only benchmark hallucination frequencies across different LLMs, but also critically assess the effectiveness of several existing hallucination detection methodologies. Furthermore, we show that using a high-quality dataset such as RAGTruth, it is possible to finetune a relatively small LLM and achieve a competitive level of performance in hallucination detection when compared to the existing prompt-based approaches using state-of-the-art large language models such as GPT-4.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "RAGTruth is presented, a corpus tailored for analyzing word-level hallucinations in various domains and tasks within the standard RAG frameworks for LLM applications and it is shown that using a high-quality dataset such as RAGTruth, it is possible to finetune a relatively small LLM and achieve a competitive level of performance in hallucination detection when compared to the existing prompt-based approaches using state-of-the-art large language models such as GPT-4."
            },
            "score": 4
        },
        {
            "id": "212e2272a662e1edcddb77462a0a8fdeff49c03b",
            "paperId": "212e2272a662e1edcddb77462a0a8fdeff49c03b",
            "title": "Will this Question be Answered? Question Filtering via Answer Model Distillation for Efficient Question Answering",
            "abstract": "In this paper we propose a novel approach towards improving the efficiency of Question Answering (QA) systems by filtering out questions that will not be answered by them. This is based on an interesting new finding: the answer confidence scores of state-of-the-art QA systems can be approximated well by models solely using the input question text. This enables preemptive filtering of questions that are not answered by the system due to their answer confidence scores being lower than the system threshold. Specifically, we learn Transformer-based question models by distilling Transformer-based answering models. Our experiments on three popular QA datasets and one industrial QA benchmark demonstrate the ability of our question models to approximate the Precision/Recall curves of the target QA system well. These question models, when used as filters, can effectively trade off lower computation cost of QA systems for lower Recall, e.g., reducing computation by ~60%, while only losing ~3-4% of Recall.",
            "year": 2021,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An interesting new finding is made: the answer confidence scores of state-of-the-art QA systems can be approximated well by models solely using the input question text, which enables preemptive filtering of questions that are not answered by the system due to theiranswer confidence scores being lower than the system threshold."
            },
            "score": 4
        },
        {
            "id": "e33e672fd04c2367a45b3271b36aac829451a468",
            "paperId": "e33e672fd04c2367a45b3271b36aac829451a468",
            "title": "Hallucination-minimized Data-to-answer Framework for Financial Decision-makers",
            "abstract": "Large Language Models (LLMs) have been applied to build several automation and personalized question-answering prototypes so far. However, scaling such prototypes to robust products with minimized hallucinations or fake responses still remains an open challenge, especially in niche data-table heavy domains such as financial decision making. In this work, we present a novel Langchain-based framework that transforms data tables into hierarchical textual \"data chunks\" to enable a wide variety of actionable question answering. First, the user-queries are classified by intention followed by automated retrieval of the most relevant data chunks to generate customized LLM prompts per query. Next, the custom prompts and their responses undergo multi-metric scoring to assess for hallucinations and response confidence. The proposed system is optimized with user-query intention classification, advanced prompting, data scaling capabilities and it achieves over $ 90\\%$ confidence scores for a variety of user-queries responses ranging from {What, Where, Why, How, predict, trend, anomalies, exceptions} that are crucial for financial decision making applications. The proposed data to answers framework can be extended to other analytical domains such as sales and payroll to ensure optimal hallucination control guardrails.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel Langchain-based framework that transforms data tables into hierarchical textual \"data chunks\" to enable a wide variety of actionable question answering to enable optimal hallucination control guardrails for financial decision making applications."
            },
            "score": 4
        },
        {
            "id": "622b1978aa46a19b58fa40faa7c8e60709287515",
            "paperId": "622b1978aa46a19b58fa40faa7c8e60709287515",
            "title": "Using contradictions to improve QA systems",
            "abstract": "Ensuring the safety of question answering (QA) systems is critical for deploying them in biomedical and scienti\ufb01c domains. One approach to improving these systems uses nat-ural language inference (NLI) to determine whether answers are supported, or entailed, by some background context. However, these systems are vulnerable to supporting an answer with a source that is wrong or misleading. Our work proposes a critical approach by selecting answers based on whether they have been contradicted by some background context. We evaluate this system on multiple choice and extractive QA and \ufb01nd that while the contradiction-based systems are competitive with and often better than entailment-only systems, models that incorporate contra-diction, entailment, and QA model con\ufb01dence scores together are the best. Based on this result, we explore unique opportunities for leveraging contradiction-based approaches such for improving interpretability and selecting better answers.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a critical approach by selecting answers by selecting answers based on whether they have been contradicted by some background context, and explores unique opportunities for leveraging contradiction-based approaches such for improving interpretability and selecting better answers."
            },
            "score": 4
        },
        {
            "id": "c70eb74e09c41e8fcc71dd59e3b4d631f657f7cd",
            "paperId": "c70eb74e09c41e8fcc71dd59e3b4d631f657f7cd",
            "title": "Internet-augmented language models through few-shot prompting for open-domain question answering",
            "abstract": "In this work, we aim to capitalize on the unique few-shot capabilities of large-scale language models (LSLMs) to overcome some of their challenges with respect to grounding to factual and up-to-date information. Motivated by semi-parametric language models (LMs), which ground their decisions in external retrieved evidence, we use few-shot prompting to learn to condition LMs on information returned from the web using Google Search, a broad and constantly updated knowledge source. Our approach does not involve fine-tuning or learning additional parameters, thus making it applicable to any LM, offering therefore a strong baseline. Indeed, we find that LMs conditioned on the web surpass performance of closed-book models of similar, or even larger, model sizes in open-domain question answering. Finally, we find that increasing the inference-time compute of models, achieved via using multiple retrieved evidences to generate multiple answers followed by a reranking stage that uses scores generated by the same LMs, leads to better performance and alleviates lower performance of smaller few-shot LMs. All in all, our findings suggest that it might be beneficial to slow down the race towards the biggest model and instead shift attention towards finding more effective ways to use models, including but not limited to, better prompting or increasing inference-time compute.",
            "year": 2022,
            "citationCount": 79,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Motivated by semi-parametric language models (LMs), few-shot prompting is used to learn to condition LMs on information returned from the web using Google Search, a broad and constantly updated knowledge source, making it applicable to any LM, offering therefore a strong baseline."
            },
            "score": 4
        },
        {
            "id": "12c826f4195da172b212a529f8fcf10cc79e35da",
            "paperId": "12c826f4195da172b212a529f8fcf10cc79e35da",
            "title": "Context-faithful Prompting for Large Language Models",
            "abstract": "Large language models (LLMs) encode parametric knowledge about world facts and have shown remarkable performance in knowledge-driven NLP tasks. However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks). In this paper, we seek to assess and enhance LLMs' contextual faithfulness in two aspects: knowledge conflict and prediction with abstention. We demonstrate that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies. In particular, we identify opinion-based prompts and counterfactual demonstrations as the most effective methods. Opinion-based prompts reframe the context as a narrator's statement and inquire about the narrator's opinions, while counterfactual demonstrations use instances containing false facts to improve faithfulness in knowledge conflict situations. Neither technique requires additional training. We conduct experiments on three datasets of two standard NLP tasks, machine reading comprehension and relation extraction, and the results demonstrate significant improvement in faithfulness to contexts. Code and data are released at https://github.com/wzhouad/context-faithful-llm.",
            "year": 2023,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies, and opinion-based prompts and counterfactual demonstrations are identified as the most effective methods."
            },
            "score": 4
        },
        {
            "id": "d5a6fc6aa139066e3b66ba63002e7d84c109aebc",
            "paperId": "d5a6fc6aa139066e3b66ba63002e7d84c109aebc",
            "title": "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing",
            "abstract": "Large language models (LLMs) have shown remarkable capabilities in Natural Language Processing (NLP), especially in domains where labeled data is scarce or expensive, such as clinical domain. However, to unlock the clinical knowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform specific clinical NLP tasks without any task-specific training data. This is known as in-context learning, which is an art and science that requires understanding the strengths and weaknesses of different LLMs and prompt engineering approaches. In this paper, we present a comprehensive and systematic experimental study on prompt engineering for five clinical NLP tasks: Clinical Sense Disambiguation, Biomedical Evidence Extraction, Coreference Resolution, Medication Status Extraction, and Medication Attribute Extraction. We assessed the prompts proposed in recent literature, including simple prefix, simple cloze, chain of thought, and anticipatory prompts, and introduced two new types of prompts, namely heuristic prompting and ensemble prompting. We evaluated the performance of these prompts on three state-of-the-art LLMs: GPT-3.5, BARD, and LLAMA2. We also contrasted zero-shot prompting with few-shot prompting, and provide novel insights and guidelines for prompt engineering for LLMs in clinical NLP. To the best of our knowledge, this is one of the first works on the empirical evaluation of different prompt engineering approaches for clinical NLP in this era of generative AI, and we hope that it will inspire and inform future research in this area.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents a comprehensive and systematic experimental study on prompt engineering for five clinical NLP tasks: Clinical Sense Disambiguation, Biomedical Evidence Extraction, Coreference Resolution, Medication Status Ext extraction, and Medication Attribute Extraction."
            },
            "score": 4
        },
        {
            "id": "8d806a91e5f2166ee6823eb7e6e8e56826b6776d",
            "paperId": "8d806a91e5f2166ee6823eb7e6e8e56826b6776d",
            "title": "NLPBench: Evaluating Large Language Models on Solving NLP Problems",
            "abstract": "Recent developments in large language models (LLMs) have shown promise in enhancing the capabilities of natural language processing (NLP). Despite these successes, there remains a dearth of research dedicated to the NLP problem-solving abilities of LLMs. To fill the gap in this area, we present a unique benchmarking dataset, NLPBench, comprising 378 college-level NLP questions spanning various NLP topics sourced from Yale University's prior final exams. NLPBench includes questions with context, in which multiple sub-questions share the same public information, and diverse question types, including multiple choice, short answer, and math. Our evaluation, centered on LLMs such as GPT-3.5/4, PaLM-2, and LLAMA-2, incorporates advanced prompting strategies like the chain-of-thought (CoT) and tree-of-thought (ToT). Our study reveals that the effectiveness of the advanced prompting strategies can be inconsistent, occasionally damaging LLM performance, especially in smaller models like the LLAMA-2 (13b). Furthermore, our manual assessment illuminated specific shortcomings in LLMs' scientific problem-solving skills, with weaknesses in logical decomposition and reasoning notably affecting results.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study presents a unique benchmarking dataset, NLPBench, comprising 378 college-level NLP questions spanning various NLP topics sourced from Yale University's prior final exams, and reveals that the effectiveness of the advanced prompting strategies can be inconsistent, occasionally damaging LLM performance."
            },
            "score": 4
        },
        {
            "id": "930e86d49477c9d3305cd1f9d01b93749f85bb8b",
            "paperId": "930e86d49477c9d3305cd1f9d01b93749f85bb8b",
            "title": "Universal Self-adaptive Prompting",
            "abstract": "A hallmark of modern large language models (LLMs) is their impressive general zero-shot and few-shot abilities, often elicited through in-context learning (ICL) via prompting. However, while highly coveted and being the most general, zero-shot performances in LLMs are still typically weaker due to the lack of guidance and the difficulty of applying existing automatic prompt design methods in general tasks when ground-truth labels are unavailable. In this study, we address this by presenting Universal Self-Adaptive Prompting (USP), an automatic prompt design approach specifically tailored for zero-shot learning (while compatible with few-shot). Requiring only a small amount of unlabeled data and an inference-only LLM, USP is highly versatile: to achieve universal prompting, USP categorizes a possible NLP task into one of the three possible task types and then uses a corresponding selector to select the most suitable queries and zero-shot model-generated responses as pseudo-demonstrations, thereby generalizing ICL to the zero-shot setup in a fully automated way. We evaluate USP with PaLM and PaLM 2 models and demonstrate performances that are considerably stronger than standard zero-shot baselines and often comparable to or even superior to few-shot baselines across more than 40 natural language understanding, natural language generation, and reasoning tasks.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study presents Universal Self-Adaptive Prompting (USP), an automatic prompt design approach specifically tailored for zero-shot learning (while compatible with few-shot) and evaluates USP with PaLM and PaLM 2 models and demonstrates performances that are considerably stronger than standard zero- shot baselines."
            },
            "score": 4
        },
        {
            "id": "ca261cb681b082e90ca6c7a9d325b4265ed1dc28",
            "paperId": "ca261cb681b082e90ca6c7a9d325b4265ed1dc28",
            "title": "MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models",
            "abstract": "Large language models (LLMs) have achieved remarkable performance in natural language understanding and generation tasks. However, they often suffer from limitations such as difficulty in incorporating new knowledge, generating hallucinations, and explaining their reasoning process. To address these challenges, we propose a novel prompting pipeline, named \\method, that leverages knowledge graphs (KGs) to enhance LLMs' inference and transparency. Our method enables LLMs to comprehend KG inputs and infer with a combination of implicit and external knowledge. Moreover, our method elicits the mind map of LLMs, which reveals their reasoning pathways based on the ontology of knowledge. We evaluate our method on diverse question \\&answering tasks, especially in medical domains, and show significant improvements over baselines. We also introduce a new hallucination evaluation benchmark and analyze the effects of different components of our method. Our results demonstrate the effectiveness and robustness of our method in merging knowledge from LLMs and KGs for combined inference. To reproduce our results and extend the framework further, we make our codebase available at https://github.com/wyl-willing/MindMap.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel prompting pipeline is proposed that leverages knowledge graphs (KGs) to enhance LLMs' inference and transparency and elicits the mind map of LLMs, which reveals their reasoning pathways based on the ontology of knowledge."
            },
            "score": 3
        },
        {
            "id": "99832586d55f540f603637e458a292406a0ed75d",
            "paperId": "99832586d55f540f603637e458a292406a0ed75d",
            "title": "LANGUAGE MODELS",
            "abstract": "While large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. Furthermore, on two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.",
            "year": 2023,
            "citationCount": 600,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces."
            },
            "score": 3
        },
        {
            "id": "5838b56f2c7ca3dd946428dae07bdc26a9265c67",
            "paperId": "5838b56f2c7ca3dd946428dae07bdc26a9265c67",
            "title": "Deficiency of Large Language Models in Finance: An Empirical Examination of Hallucination",
            "abstract": "The hallucination issue is recognized as a fundamental deficiency of large language models (LLMs), especially when applied to fields such as finance, education, and law. Despite the growing concerns, there has been a lack of empirical investigation. In this paper, we provide an empirical examination of LLMs' hallucination behaviors in financial tasks. First, we empirically investigate LLM model's ability of explaining financial concepts and terminologies. Second, we assess LLM models' capacity of querying historical stock prices. Third, to alleviate the hallucination issue, we evaluate the efficacy of four practical methods, including few-shot learning, Decoding by Contrasting Layers (DoLa), the Retrieval Augmentation Generation (RAG) method and the prompt-based tool learning method for a function to generate a query command. Finally, our major finding is that off-the-shelf LLMs experience serious hallucination behaviors in financial tasks. Therefore, there is an urgent need to call for research efforts in mitigating LLMs' hallucination.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The major finding is that off-the-shelf LLMs experience serious hallucination behaviors in financial tasks, and there is an urgent need to call for research efforts in mitigating LLMs' hallucination."
            },
            "score": 3
        },
        {
            "id": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
            "paperId": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
            "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such\"society of minds\"approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.",
            "year": 2023,
            "citationCount": 206,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer is presented, indicating that this approach significantly enhances mathematical and strategic reasoning across a number of tasks."
            },
            "score": 3
        },
        {
            "id": "6a637773923c39d172f31beca9afb3e086f57d55",
            "paperId": "6a637773923c39d172f31beca9afb3e086f57d55",
            "title": "Evaluation of Hallucination and Robustness for Large Language Models",
            "abstract": "As large language models (LLMs) rapidly advance, rigorous testing and evaluation of these models grows increasingly crucial. To address this need, we have developed three types of questions: Chinese contextual, English contextual, and language context-independent. Testing in both Chinese and English probes the LLMs' hallucination tendencies. We investigate the impact of language on hallucinations from two perspectives: the type of language used in the input prompt and the cultural context underlying the prompt's content. Additionally, 52 multi-domain single-choice questions from C-EVAL are presented in original and randomized order to assess robustness to perturbations. Among the five LLMs, the tests demonstrate GPT -4 has the strongest anti-hallucination and robustness capabilities, answering with greater accuracy, consistency, and reliability. ChatGLM ranks second and outperforms GPT -4 on Chinese context-dependent questions. Emergent testing phenomena are analyzed from the user's perspective. Hallucinated responses are categorized and potential causal factors leading to hallucination and fragility are examined. Based on these findings, viable avenues for improvement are proposed.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Three types of questions are developed: Chinese contextual, English contextual, and language context-independent, which demonstrate GPT -4 has the strongest anti-hallucination and robustness capabilities, answering with greater accuracy, consistency, and reliability."
            },
            "score": 3
        },
        {
            "id": "be177300487b6d0f25e6cade9a31900454b13281",
            "paperId": "be177300487b6d0f25e6cade9a31900454b13281",
            "title": "FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation",
            "abstract": "Most large language models (LLMs) are trained once and never updated; thus, they lack the ability to dynamically adapt to our ever-changing world. In this work, we perform a detailed study of the factuality of LLM-generated text in the context of answering questions that test current world knowledge. Specifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a diverse range of question and answer types, including questions that require fast-changing world knowledge as well as questions with false premises that need to be debunked. We benchmark a diverse array of both closed and open-source LLMs under a two-mode evaluation procedure that allows us to measure both correctness and hallucination. Through human evaluations involving more than 50K judgments, we shed light on limitations of these models and demonstrate significant room for improvement: for instance, all models (regardless of model size) struggle on questions that involve fast-changing knowledge and false premises. Motivated by these results, we present FreshPrompt, a simple few-shot prompting method that substantially boosts the performance of an LLM on FreshQA by incorporating relevant and up-to-date information retrieved from a search engine into the prompt. Our experiments show that FreshPrompt outperforms both competing search engine-augmented prompting methods such as Self-Ask (Press et al., 2022) as well as commercial systems such as Perplexity.AI. Further analysis of FreshPrompt reveals that both the number of retrieved evidences and their order play a key role in influencing the correctness of LLM-generated answers. Additionally, instructing the LLM to generate concise and direct answers helps reduce hallucination compared to encouraging more verbose answers. To facilitate future work, we release FreshQA at github.com/freshllms/freshqa and commit to updating it at regular intervals.",
            "year": 2023,
            "citationCount": 51,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A detailed study of the factuality of LLM-generated text in the context of answering questions that test current world knowledge and introduces FreshQA, a novel dynamic QA benchmark encompassing a diverse range of question and answer types, including questions that require fast-changing world knowledge as well as questions with false premises that need to be debunked."
            },
            "score": 3
        },
        {
            "id": "8e37dc1215681aa153a51c07078ba8befd6a6e01",
            "paperId": "8e37dc1215681aa153a51c07078ba8befd6a6e01",
            "title": "AdaPlanner: Adaptive Planning from Feedback with Language Models",
            "abstract": "Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively.",
            "year": 2023,
            "citationCount": 49,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A closed-loop approach, AdaPlanner, is proposed, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback, and develops a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities."
            },
            "score": 3
        },
        {
            "id": "434b9f9bc71c935e4a46a1aff36a8cc4c22d9afa",
            "paperId": "434b9f9bc71c935e4a46a1aff36a8cc4c22d9afa",
            "title": "Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration",
            "abstract": "Human intelligence thrives on cognitive synergy, where collaboration among different minds yield superior outcomes compared to isolated individuals. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist is an intelligent agent that collaboratively combines multiple minds' strengths and knowledge to enhance problem-solving in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. Our in-depth analysis shows that assigning multiple fine-grained personas in LLMs improves problem-solving abilities compared to using a single or fixed number of personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing, Codenames Collaborative, and Logic Grid Puzzle, encompassing both knowledge-intensive and reasoning-intensive types. Unlike previous works, such as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, experimental results demonstrate that SPP effectively reduces factual hallucination, and maintains strong reasoning capabilities. Additionally, comparative experiments show that cognitive synergy only emerges in GPT-4 and does not appear in less capable models, such as GPT-3.5-turbo and Llama2-13b-chat, which draws an interesting analogy to human development. Code, data, and prompts can be found at: https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.",
            "year": 2023,
            "citationCount": 42,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas by dynamically identifying and simulating different personas based on task inputs, which unleashes the potential of cognitive synergy in LLMs."
            },
            "score": 3
        },
        {
            "id": "56aa2578c3c1d67f5cc078ab7800085d0b413b58",
            "paperId": "56aa2578c3c1d67f5cc078ab7800085d0b413b58",
            "title": "Exploiting Multiple Semantic Resources for Answer Selection",
            "abstract": "This paper describes the utility of semantic resources such as the Web, WordNet and gazetteers in the answer selection process for a question-answering system. In contrast with previous work using individual semantic resources to support answer selection, our work combines multiple resources to boost the confidence scores assigned to correct answers and evaluates different combination strategies based on unweighted sums, weighted linear combinations, and logistic regression. We apply our approach to select answers from candidates produced by three different extraction techniques of varying quality, focusing on TREC questions whose answers represent locations or proper-names. Our experimental results demonstrate that the combination of semantic resources is more effective than individual resources for all three extraction techniques, improving answer selection accuracy by as much as 32.35% for location questions and 72% for proper-name questions. Of the combination strategies tested, logistic regression models produced the best results for both location and proper-name questions.",
            "year": 2006,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results demonstrate that the combination of semantic resources is more effective than individual resources for all three extraction techniques, improving answer selection accuracy by as much as 32.35% for location questions and 72% for proper-name questions."
            },
            "score": 3
        },
        {
            "id": "7ed0faa6720cd176d57badbc0455af31a03f080c",
            "paperId": "7ed0faa6720cd176d57badbc0455af31a03f080c",
            "title": "Towards Expert-Level Medical Question Answering with Large Language Models",
            "abstract": "Recent artificial intelligence (AI) systems have reached milestones in\"grand challenges\"ranging from Go to protein-folding. The capability to retrieve medical knowledge, reason over it, and answer medical questions comparably to physicians has long been viewed as one such grand challenge. Large language models (LLMs) have catalyzed significant progress in medical question answering; Med-PaLM was the first model to exceed a\"passing\"score in US Medical Licensing Examination (USMLE) style questions with a score of 67.2% on the MedQA dataset. However, this and other prior work suggested significant room for improvement, especially when models' answers were compared to clinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by leveraging a combination of base LLM improvements (PaLM 2), medical domain finetuning, and prompting strategies including a novel ensemble refinement approach. Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 19% and setting a new state-of-the-art. We also observed performance approaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU clinical topics datasets. We performed detailed human evaluations on long-form questions along multiple axes relevant to clinical applications. In pairwise comparative ranking of 1066 consumer medical questions, physicians preferred Med-PaLM 2 answers to those produced by physicians on eight of nine axes pertaining to clinical utility (p<0.001). We also observed significant improvements compared to Med-PaLM on every evaluation axis (p<0.001) on newly introduced datasets of 240 long-form\"adversarial\"questions to probe LLM limitations. While further studies are necessary to validate the efficacy of these models in real-world settings, these results highlight rapid progress towards physician-level performance in medical question answering.",
            "year": 2023,
            "citationCount": 233,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Results highlight rapid progress towards physician-level performance in medical question answering by leveraging a combination of base LLM improvements (PaLM 2), medical domain finetuning, and prompting strategies including a novel ensemble refinement approach."
            },
            "score": 3
        },
        {
            "id": "5d0fe5417b9d6f3a97f4b4fd470c99d8ee3bb550",
            "paperId": "5d0fe5417b9d6f3a97f4b4fd470c99d8ee3bb550",
            "title": "Exploring the landscape of large language models in medical question answering",
            "abstract": "With the rapid development of new large language models (LLMs), each claiming to surpass previous models, an overall picture of medical LLM research can be elusive. To address this challenge, we benchmark a range of top LLMs and identify consistent patterns which appear across models. We test $8$ well-known LLMs on $874$ newly collected questions from Polish medical licensing exams. For each question, we score each model on the top-1 accuracy and the distribution of probabilities assigned. We then compare with factors including question difficulty for humans, question length, and the scores of the other models. LLM accuracies were positively correlated pairwise ($0.29$ to $0.62$). Model performance was also correlated with human performance ($0.07$ to $0.16$), but negatively correlated to the difference between the question-level accuracy of top-scoring and bottom-scoring humans ($-0.16$ to $-0.23$). The top output probability and question length were positive and negative predictors of accuracy respectively (p $<0.05$). The top scoring LLM, GPT-4 Turbo, scored $82\\%$, followed by Med42, PaLM 2, Mixtral and GPT-3.5 around $63\\%$. We found evidence of similarities between models in which questions they answer correctly, as well as similarities with human test takers. Larger models typically performed better, but differences in training methods were also highly impactful. Model accuracy was positively correlated with confidence, but negatively correlated with question length. We expect that similar training methods will lead these patterns to persist across future models. These patterns can therefore aid medical experts in forming expectations about LLMs as a category to support application research.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A range of top LLMs are benchmarked and consistent patterns which appear across models are identified, finding evidence of similarities between models in which questions they answer correctly, as well as similarities with human test takers."
            },
            "score": 3
        },
        {
            "id": "4ab41d9780f1d1ac34d39fa7e527e73652507fcc",
            "paperId": "4ab41d9780f1d1ac34d39fa7e527e73652507fcc",
            "title": "GreaseLM: Graph REASoning Enhanced Language Models for Question Answering",
            "abstract": "Answering complex questions about textual narratives requires reasoning over both stated context and the world knowledge that underlies it. However, pretrained language models (LM), the foundation of most modern QA systems, do not robustly represent latent relationships between concepts, which is necessary for reasoning. While knowledge graphs (KG) are often used to augment LMs with structured representations of world knowledge, it remains an open question how to effectively fuse and reason over the KG representations and the language context, which provides situational constraints and nuances. In this work, we propose GreaseLM, a new model that fuses encoded representations from pretrained LMs and graph neural networks over multiple layers of modality interaction operations. Information from both modalities propagates to the other, allowing language context representations to be grounded by structured world knowledge, and allowing linguistic nuances (e.g., negation, hedging) in the context to inform the graph representations of knowledge. Our results on three benchmarks in the commonsense reasoning (i.e., CommonsenseQA, OpenbookQA) and medical question answering (i.e., MedQA-USMLE) domains demonstrate that GreaseLM can more reliably answer questions that require reasoning over both situational constraints and structured knowledge, even outperforming models 8x larger.",
            "year": 2022,
            "citationCount": 126,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "GreaseLM is a new model that fuses encoded representations from pretrained LMs and graph neural networks over multiple layers of modality interaction operations, allowing language context representations to be grounded by structured world knowledge, and allowing linguistic nuances in the context to inform the graph representations of knowledge."
            },
            "score": 3
        },
        {
            "id": "ed38c6b157c11476939c426ec6871c926f2f3524",
            "paperId": "ed38c6b157c11476939c426ec6871c926f2f3524",
            "title": "Leveraging Large Language Models for Multiple Choice Question Answering",
            "abstract": "While large language models (LLMs) like GPT-3 have achieved impressive results on multiple choice question answering (MCQA) tasks in the zero, one, and few-shot settings, they generally lag behind the MCQA state of the art (SOTA). MCQA tasks have traditionally been presented to LLMs like cloze tasks. An LLM is conditioned on a question (without the associated answer options) and its chosen option is the one assigned the highest probability after normalization (for length, etc.). A more natural prompting approach is to present the question and answer options to the LLM jointly and have it output the symbol (e.g.,\"A\") associated with its chosen answer option. This approach allows the model to explicitly compare answer options, reduces computational costs, and mitigates the effects of tokenization scheme and answer option representations on answer selection. For the natural approach to be effective, the LLM it is used with must be able to associate answer options with the symbols that represent them. The LLM needs what we term multiple choice symbol binding (MCSB) ability. This ability varies greatly by model. We show that a model with high MCSB ability performs much better with the natural approach than with the traditional approach across 20 diverse datasets and largely closes the gap with the SOTA, suggesting that the MCQA ability of LLMs has been previously underestimated.",
            "year": 2022,
            "citationCount": 88,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that a model with high MCSB ability performs much better with the natural approach than with the traditional approach across 20 diverse datasets and largely closes the gap with the SOTA, suggesting that the MCQA ability of LLMs has been previously underestimated."
            },
            "score": 3
        },
        {
            "id": "e10c2ed8de5568e6a41fb60c2adc4bf9c225da25",
            "paperId": "e10c2ed8de5568e6a41fb60c2adc4bf9c225da25",
            "title": "Is EVALITA Done? On the Impact of Prompting on the Italian NLP Evaluation Campaign",
            "abstract": "Prompt-based learning is a recent paradigm in NLP that leverages large pre-trained language models to perform a variety of tasks. With this technique, it is possible to build classifiers that do not need training data (zero-shot). In this paper, we assess the status of prompt-based learning applied to several text classification tasks in the Italian language. The results indicate that the performance gap towards current supervised methods is still relevant. However, the difference in performance between pre-trained models and the characteristic of the prompt-based classifier of operating in a zero-shot fashion open a discussion regarding the next generation of evaluation campaigns for NLP.",
            "year": 2022,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results indicate that the performance gap towards current supervised methods is still relevant, however, the difference in performance between pre-trained models and the characteristic of the prompt-based classifier of operating in a zero-shot fashion open a discussion regarding the next generation of evaluation campaigns for NLP."
            },
            "score": 3
        },
        {
            "id": "8bc313e04cbd39847eb50b22af0a698ff2971a35",
            "paperId": "8bc313e04cbd39847eb50b22af0a698ff2971a35",
            "title": "Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models: A Case Study on ChatGPT",
            "abstract": "Generative large language models (LLMs), e.g., ChatGPT, have demonstrated remarkable proficiency across several NLP tasks, such as machine translation, text summarization. Recent research (Kocmi and Federmann, 2023) has shown that utilizing LLMs for assessing the quality of machine translation (MT) achieves state-of-the-art performance at the system level but \\textit{performs poorly at the segment level}. To further improve the performance of LLMs on MT quality assessment, we investigate several prompting designs, and propose a new prompting method called \\textbf{\\texttt{Error Analysis Prompting}} (EAPrompt) by combining Chain-of-Thoughts (Wei et al., 2022) and Error Analysis (Lu et al., 2023). This technique emulates the commonly accepted human evaluation framework - Multidimensional Quality Metrics (MQM, Freitag et al. (2021)) and \\textit{produces explainable and reliable MT evaluations at both the system and segment level}. Experimental Results from the WMT22 metrics shared task validate the effectiveness of EAPrompt on various LLMs, with different structures. Further analysis confirms that EAPrompt effectively distinguishes major errors from minor ones, while also sharing a similar distribution of the number of errors with MQM. These findings highlight the potential of EAPrompt as a human-like evaluator prompting technique for MT evaluation.",
            "year": 2023,
            "citationCount": 58,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Findings highlight the potential of EAPrompt as a human-like evaluator prompting technique for MT evaluation, and investigate several prompting designs, and propose a new prompting method called EAPrompt by combining Chain-of-Thoughts and Error Analysis."
            },
            "score": 3
        },
        {
            "id": "9141480721653789597b6e537ee0eeab401f3e60",
            "paperId": "9141480721653789597b6e537ee0eeab401f3e60",
            "title": "PromptNER: Prompting For Named Entity Recognition",
            "abstract": "In a surprising turn, Large Language Models (LLMs) together with a growing arsenal of prompt-based heuristics now offer powerful off-the-shelf approaches providing few-shot solutions to myriad classic NLP problems. However, despite promising early results, these LLM-based few-shot methods remain far from the state of the art in Named Entity Recognition (NER), where prevailing methods include learning representations via end-to-end structural understanding and fine-tuning on standard labeled corpora. In this paper, we introduce PromptNER, a new state-of-the-art algorithm for few-Shot and cross-domain NER. To adapt to any new NER task PromptNER requires a set of entity definitions in addition to the standard few-shot examples. Given a sentence, PromptNER prompts an LLM to produce a list of potential entities along with corresponding explanations justifying their compatibility with the provided entity type definitions. Remarkably, PromptNER achieves state-of-the-art performance on few-shot NER, achieving a 4% (absolute) improvement in F1 score on the ConLL dataset, a 9% (absolute) improvement on the GENIA dataset, and a 4% (absolute) improvement on the FewNERD dataset. PromptNER also moves the state of the art on Cross Domain NER, outperforming prior methods (including those not limited to the few-shot setting), setting a new mark on 3/5 CrossNER target domains, with an average F1 gain of 3%, despite using less than 2% of the available data.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces PromptNER, a new state-of-the-art algorithm for few-Shot and cross-domain NER, and prompts an LLM to produce a list of potential entities along with corresponding explanations justifying their compatibility with the provided entity type definitions."
            },
            "score": 3
        },
        {
            "id": "c2408a3a8da4f12d3eb156fe359a96b428e5aff1",
            "paperId": "c2408a3a8da4f12d3eb156fe359a96b428e5aff1",
            "title": "Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models",
            "abstract": "In this paper, we take the initiative to investigate the performance of LLMs on complex planning tasks that require LLMs to understand a virtual spatial environment simulated via natural language and act correspondingly in text. We propose a benchmark named Natural Language Planning (NLP) composed of a set of novel tasks: Brick World, NLVR-based Manipulations, and Natural Language Navigation. We found that current popular LLMs such as ChatGPT still lack abilities in complex planning. This arises a question \u2013 do the LLMs have a good understanding of the environments described in natural language, or maybe other alternatives such as symbolic representations are neater and hence better to be understood by LLMs? To this end, we propose a novel method called C O S ( C hain- o f- S ymbol Prompting) that represents the complex environments with condensed symbolic spatial representations during the chained intermediate thinking steps. C O S is easy to use and does not need additional training on LLMs. Extensive experiments indicate that C O S clearly surpasses the performance of the Chain-of-Thought (CoT) Prompting in all three planning tasks with even fewer tokens used in the inputs compared with CoT on ChatGPT and InstructGPT. The performance gain is strong, by up to 60.8% accuracy (from 31.8% to 92.6%) on Brick World for ChatGPT. C O S also reduces the number of tokens in the prompt obviously, by up to 65.8% of the tokens (from 407 to 139) for the intermediate steps from demonstrations on Brick World.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel method called C O S is proposed that represents the complex environments with condensed symbolic spatial representations during the chained intermediate thinking steps that surpasses the performance of the Chain-of-Thought Prompting in all three planning tasks with even fewer tokens used in the inputs."
            },
            "score": 3
        },
        {
            "id": "3a22aad6c18a9559be3bbb197494b434b872a05a",
            "paperId": "3a22aad6c18a9559be3bbb197494b434b872a05a",
            "title": "Diffusion-NAT: Self-Prompting Discrete Diffusion for Non-Autoregressive Text Generation",
            "abstract": "Recently, continuous diffusion models (CDM) have been introduced into non-autoregressive (NAR) text-to-text generation. However, the discrete nature of text increases the difficulty of CDM to generate coherent and fluent texts, and also causes the incompatibility problem between CDM and advanced NLP techniques, especially the popular pre-trained language models (PLMs).To solve it, we propose Diffusion-NAT, which introduces discrete diffusion models (DDM) into NAR text-to-text generation and integrates BART to improve the performance.By revising the decoding process of BART and the typical settings of DDM, we unify the inference process of BART and the denoising process of DDM into the same NAR masked tokens recovering task.In this way, DDM can rely on BART to perform denoising, which can benefit from both the rich pre-learned knowledge of BART and the iterative refining paradigm of DDM.Besides, we also propose the iterative self-prompting strategy to further improve the generation quality.Experimental results on 7 datasets show that our approach can outperform competitive NAR methods, and even surpass autoregressive methods.Our code and data are released at https://github.com/RUCAIBox/DiffusionNAT.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Diffusion-NAT is proposed, which introduces discrete diffusion models (DDM) into NAR text-to-text generation and integrates BART to improve the performance and proposes the iterative self-prompting strategy to further improve the generation quality."
            },
            "score": 3
        },
        {
            "id": "2522410b1cac0c14fa656a0aaeaff08bacb358a9",
            "paperId": "2522410b1cac0c14fa656a0aaeaff08bacb358a9",
            "title": "InterroLang: Exploring NLP Models and Datasets through Dialogue-based Explanations",
            "abstract": "While recently developed NLP explainability methods let us open the black box in various ways (Madsen et al., 2022), a missing ingredient in this endeavor is an interactive tool offering a conversational interface. Such a dialogue system can help users explore datasets and models with explanations in a contextualized manner, e.g. via clarification or follow-up questions, and through a natural language interface. We adapt the conversational explanation framework TalkToModel (Slack et al., 2022) to the NLP domain, add new NLP-specific operations such as free-text rationalization, and illustrate its generalizability on three NLP tasks (dialogue act classification, question answering, hate speech detection). To recognize user queries for explanations, we evaluate fine-tuned and few-shot prompting models and implement a novel Adapter-based approach. We then conduct two user studies on (1) the perceived correctness and helpfulness of the dialogues, and (2) the simulatability, i.e. how objectively helpful dialogical explanations are for humans in figuring out the model's predicted label when it's not shown. We found rationalization and feature attribution were helpful in explaining the model behavior. Moreover, users could more reliably predict the model outcome based on an explanation dialogue rather than one-off explanations.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work adapts the conversational explanation framework TalkToModel to the NLP domain, adds new NLP-specific operations such as free-text rationalization and feature attribution, and illustrates its generalizability on three NLP tasks."
            },
            "score": 3
        },
        {
            "id": "e7c97e953849f1a8e5d85ceb4cfcc0a5d54d2365",
            "paperId": "e7c97e953849f1a8e5d85ceb4cfcc0a5d54d2365",
            "title": "Enabling Large Language Models to Generate Text with Citations",
            "abstract": "Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. In this work, our aim is to allow LLMs to generate text with citations, improving their factual correctness and verifiability. Existing work mainly relies on commercial search engines and human evaluation, making it challenging to reproduce and compare different modeling approaches. We propose ALCE, the first benchmark for Automatic LLMs' Citation Evaluation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations. We develop automatic metrics along three dimensions -- fluency, correctness, and citation quality -- and demonstrate their strong correlation with human judgements. Our experiments with state-of-the-art LLMs and novel prompting strategies show that current systems have considerable room for improvement -- For example, on the ELI5 dataset, even the best models lack complete citation support 50% of the time. Our analyses further highlight promising future directions, including developing better retrievers, advancing long-context LLMs, and improving the ability to synthesize information from multiple sources.",
            "year": 2023,
            "citationCount": 102,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes ALCE, the first benchmark for Automatic LLMs' Citation Evaluation, and develops automatic metrics along three dimensions -- fluency, correctness, and citation quality -- and demonstrates their strong correlation with human judgements."
            },
            "score": 2
        },
        {
            "id": "7c9f69848d28e0a7cbb00942ee83dab9773c23e4",
            "paperId": "7c9f69848d28e0a7cbb00942ee83dab9773c23e4",
            "title": "GPT-NER: Named Entity Recognition via Large Language Models",
            "abstract": "Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model. In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text\"Columbus is a city\"is transformed to generate the text sequence\"@@Columbus## is a city\", where special tokens @@## marks the entity to extract. To efficiently address the\"hallucination\"issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a self-verification strategy by prompting LLMs to ask itself whether the extracted entities belong to a labeled entity tag. We conduct experiments on five widely adopted NER datasets, and GPT-NER achieves comparable performances to fully supervised baselines, which is the first time as far as we are concerned. More importantly, we find that GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, GPT-NER performs significantly better than supervised models. This demonstrates the capabilities of GPT-NER in real-world NER applications where the number of labeled examples is limited.",
            "year": 2023,
            "citationCount": 58,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, and performs significantly better than supervised models, which demonstrates the capabilities of GPT-ner in real-world NER applications where the number of labeled examples is limited."
            },
            "score": 2
        },
        {
            "id": "b9df0d4631f9fab1432c152765e243ae4cd667f4",
            "paperId": "b9df0d4631f9fab1432c152765e243ae4cd667f4",
            "title": "Effective Prompt Extraction from Language Models",
            "abstract": "The text generated by large language models is commonly controlled by prompting, where a prompt prepended to a user's query guides the model's output. The prompts used by companies to guide their models are often treated as secrets, to be hidden from the user making the query. They have even been treated as commodities to be bought and sold. However, anecdotal reports have shown adversarial users employing prompt extraction attacks to recover these prompts. In this paper, we present a framework for systematically measuring the effectiveness of these attacks. In experiments with 3 different sources of prompts and 11 underlying large language models, we find that simple text-based attacks can in fact reveal prompts with high probability. Our framework determines with high precision whether an extracted prompt is the actual secret prompt, rather than a model hallucination. Prompt extraction experiments on real systems such as Bing Chat and ChatGPT suggest that system prompts can be revealed by an adversary despite existing defenses in place.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents a framework for systematically measuring the effectiveness of prompt extraction attacks and determines with high precision whether an extracted prompt is the actual secret prompt, rather than a model hallucination."
            },
            "score": 2
        },
        {
            "id": "4073610a5fbe02b5f40bf6253509c03bf1bb2c62",
            "paperId": "4073610a5fbe02b5f40bf6253509c03bf1bb2c62",
            "title": "E&V: Prompting Large Language Models to Perform Static Analysis by Pseudo-code Execution and Verification",
            "abstract": "Static analysis, the process of examining code without executing it, is crucial for identifying software issues. Yet, static analysis is hampered by its complexity and the need for customization for different targets. Traditional static analysis tools require extensive human effort and are often limited to specific target programs and programming languages. Recent advancements in Large Language Models (LLMs), such as GPT-4 and Llama, offer new capabilities for software engineering tasks. However, their application in static analysis, especially in understanding complex code structures, remains under-explored. This paper introduces a novel approach named E&V , which leverages LLMs to perform static analysis. Specifically, E&V employs LLMs to simulate the execution of pseudo-code, effectively conducting static analysis encoded in the pseudo-code with minimal human effort, thereby improving the accuracy of results. E&V includes a verification process for pseudo-code execution without needing an external oracle. This process allows E&V to mitigate hallucinations of LLMs and enhance the accuracy of static analysis results. We have implemented E&V in a prototype tool designed for triaging crashes through backward taint analysis. This prototype, paired with GPT-4-32k, has been applied to triage 170 recently fixed Linux kernel bugs across seven bug categories. Our experiments demonstrate that the prototype correctly identifies the blamed function in 81.2% of the cases. Additionally, we observe that our novel verification process significantly improves the accuracy, increasing it from 28.2% to 81.2%.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a novel approach named E&V, which employs LLMs to simulate the execution of pseudo-code, effectively conducting static analysis encoded in the pseudo-code with minimal human effort, thereby improving the accuracy of results."
            },
            "score": 2
        },
        {
            "id": "7f98fff4c5bf4b19321f5476fd76106ce32edcc4",
            "paperId": "7f98fff4c5bf4b19321f5476fd76106ce32edcc4",
            "title": "Enhanced Bert-Based Ranking Models for Spoken Document Retrieval",
            "abstract": "The Bidirectional Encoder Representations from Transformers (BERT) model has recently achieved record-breaking success on many natural language processing (NLP) tasks such as question answering and language understanding. However, relatively little work has been done on ad-hoc information retrieval (IR), especially for spoken document retrieval (SDR). This paper adopts and extends BERT for SDR, while its contributions are at least three-fold. First, we augment BERT with extra language features such as unigram and inverse document frequency (IDF) statistics to make it more applicable to SDR. Second, we also explore the incorporation of confidence scores into document representations to see if they could help alleviate the negative effects resulting from imperfect automatic speech recognition (ASR). Third, we conduct a comprehensive set of experiments to compare our BERT-based ranking methods with other state-of-the-art ones and investigate the synergy effect of them as well.",
            "year": 2019,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper adopts and extends BERT for SDR, while its contributions are at least three-fold, with extra language features such as unigram and inverse document frequency (IDF) statistics to make it more applicable to SDR."
            },
            "score": 2
        },
        {
            "id": "a970c8fadef8497576660b288c52c0ec8eebdc12",
            "paperId": "a970c8fadef8497576660b288c52c0ec8eebdc12",
            "title": "Zero-Shot Video Question Answering via Frozen Bidirectional Language Models",
            "abstract": "Video question answering (VideoQA) is a complex task that requires diverse multi-modal data for training. Manual annotation of question and answers for videos, however, is tedious and prohibits scalability. To tackle this problem, recent methods consider zero-shot settings with no manual annotation of visual question-answer. In particular, a promising approach adapts frozen autoregressive language models pretrained on Web-scale text-only data to multi-modal inputs. In contrast, we here build on frozen bidirectional language models (BiLM) and show that such an approach provides a stronger and cheaper alternative for zero-shot VideoQA. In particular, (i) we combine visual inputs with the frozen BiLM using light trainable modules, (ii) we train such modules using Web-scraped multi-modal data, and finally (iii) we perform zero-shot VideoQA inference through masked language modeling, where the masked text is the answer to a given question. Our proposed approach, FrozenBiLM, outperforms the state of the art in zero-shot VideoQA by a significant margin on a variety of datasets, including LSMDC-FiB, iVQA, MSRVTT-QA, MSVD-QA, ActivityNet-QA, TGIF-FrameQA, How2QA and TVQA. It also demonstrates competitive performance in the few-shot and fully-supervised setting. Our code and models are publicly available at https://github.com/antoyang/FrozenBiLM.",
            "year": 2022,
            "citationCount": 130,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work builds on frozen bidirectional language models (BiLM) and shows that such an approach provides a stronger and cheaper alternative for zero-shot VideoQA and demonstrates competitive performance in the few-shot and fully-supervised setting."
            },
            "score": 2
        },
        {
            "id": "bb4516ad6eb7adda97d81f09d4bb92b3ad056c42",
            "paperId": "bb4516ad6eb7adda97d81f09d4bb92b3ad056c42",
            "title": "Large Language Models are Temporal and Causal Reasoners for Video Question Answering",
            "abstract": "Large Language Models (LLMs) have shown remarkable performances on a wide range of natural language understanding and generation tasks. We observe that the LLMs provide effective priors in exploiting $\\textit{linguistic shortcuts}$ for temporal and causal reasoning in Video Question Answering (VideoQA). However, such priors often cause suboptimal results on VideoQA by leading the model to over-rely on questions, $\\textit{i.e.}$, $\\textit{linguistic bias}$, while ignoring visual content. This is also known as `ungrounded guesses' or `hallucinations'. To address this problem while leveraging LLMs' prior on VideoQA, we propose a novel framework, Flipped-VQA, encouraging the model to predict all the combinations of $\\langle$V, Q, A$\\rangle$ triplet by flipping the source pair and the target label to understand their complex relationships, $\\textit{i.e.}$, predict A, Q, and V given a VQ, VA, and QA pairs, respectively. In this paper, we develop LLaMA-VQA by applying Flipped-VQA to LLaMA, and it outperforms both LLMs-based and non-LLMs-based models on five challenging VideoQA benchmarks. Furthermore, our Flipped-VQA is a general framework that is applicable to various LLMs (OPT and GPT-J) and consistently improves their performances. We empirically demonstrate that Flipped-VQA not only enhances the exploitation of linguistic shortcuts but also mitigates the linguistic bias, which causes incorrect answers over-relying on the question. Code is available at https://github.com/mlvlab/Flipped-VQA.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LLaMA-VQA is developed by applying Flipped-V QA to LLaMA, and it outperforms both LLMs- based and non-LLMs-based models on five challenging VideoQA benchmarks."
            },
            "score": 2
        },
        {
            "id": "70feb009bc1e8b1cb8dff64bf9fd67789636438b",
            "paperId": "70feb009bc1e8b1cb8dff64bf9fd67789636438b",
            "title": "From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models",
            "abstract": "Large language models (LLMs) have demonstrated excellent zero-shot generalization to new language tasks. However, effective utilization of LLMs for zero-shot visual question-answering (VQA) remains challenging, primarily due to the modality disconnect and task disconnect between the LLM and VQA tasks. End-to-end training on multimodal data may bridge the disconnects, but is inflexible and computationally expensive. To address this issue, we propose Img2LLM, a plug-and-play module that provides LLM prompts to enable LLMs to perform zeroshot VQA tasks without end-to-end training. We develop LLM-agnostic models describe image content as exemplar question-answer pairs, which prove to be effective LLM prompts. Img2LLM offers the following benefits: 1) It achieves comparable or better performance than methods relying on end-to-end training. For example, we outperform Flamingo [3] by 5.6% on VQAv2. On the challenging A-OKVQA dataset, our method outperforms few-shot methods by as much as 20%. 2) It flexibly interfaces with a wide range of LLMs to perform VQA. 3) It eliminates the need to specialize LLMs using end-to-end finetuning and serve highly specialized LLMs to end users, thereby reducing cost. Code is available via the LAVIS [28] framework at https://github.com/salesforce/LAVIS/tree/main/projects/img2llm-vqa.",
            "year": 2022,
            "citationCount": 70,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Img2LLM is a plug-and-play module that provides LLM prompts to enable LLMs to perform zeroshot VQA tasks without end-to-end training and eliminates the need to specialize LLMs using end-to-end finetuning and serve highly specialized LLMs to end users, thereby reducing cost."
            },
            "score": 2
        },
        {
            "id": "12c200e731017f9851afb1a6fe3fc7f76e6439c6",
            "paperId": "12c200e731017f9851afb1a6fe3fc7f76e6439c6",
            "title": "Explicit Visual Prompting for Low-Level Structure Segmentations",
            "abstract": "We consider the generic problem of detecting low-level structures in images, which includes segmenting the manipulated parts, identifying out-of-focus pixels, separating shadow regions, and detecting concealed objects. Whereas each such topic has been typically addressed with a domain-specific solution, we show that a unified approach performs well across all of them. We take inspiration from the widely-used pre-training and then prompt tuning protocols in NLP and propose a new visual prompting model, named Explicit Visual Prompting (EVP). Different from the previous visual prompting which is typically a dataset-level implicit embedding, our key insight is to enforce the tunable parameters focusing on the explicit visual content from each individual image, i.e., the features from frozen patch embeddings and the input's high-frequency components. The proposed EVP significantly outperforms other parameter-efficient tuning protocols under the same amount of tunable parameters (5.7% extra trainable parameters of each task). EVP also achieves state-of-the-art performances on diverse low-level structure segmentation tasks compared to task-specific solutions. Our code is available at: https://github.com/NiFangBaAGe/Explicit-Visual-Prompt.",
            "year": 2023,
            "citationCount": 42,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work takes inspiration from the widely-used pre-training and then prompt tuning protocols in NLP and proposes a new visual prompting model, named Explicit Visual Prompting (EVP), which significantly outperforms other parameter-efficient tuning protocols under the same amount of tunable parameters."
            },
            "score": 2
        },
        {
            "id": "80c698688bb4488beaceaab5c64f701a946cb7ae",
            "paperId": "80c698688bb4488beaceaab5c64f701a946cb7ae",
            "title": "All in One: Multi-Task Prompting for Graph Neural Networks",
            "abstract": "Recently, \"pre-training and fine-tuning'' has been adopted as a standard workflow for many graph tasks since it can take general graph knowledge to relieve the lack of graph annotations from each application. However, graph tasks with node level, edge level, and graph level are far diversified, making the pre-training pretext often incompatible with these multiple tasks. This gap may even cause a \"negative transfer'' to the specific application, leading to poor results. Inspired by the prompt learning in natural language processing (NLP), which has presented significant effectiveness in leveraging prior knowledge for various NLP tasks, we study the prompting topic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models. Specifically, we first unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern. In this way, the prompting idea from NLP can be seamlessly introduced to the graph area. Then, to further narrow the gap between various graph tasks and state-of-the-art pre-training strategies, we further study the task space of various graph applications and reformulate downstream problems to the graph-level task. Afterward, we introduce meta-learning to efficiently learn a better initialization for the multi-task prompt of graphs so that our prompting framework can be more reliable and general for different tasks. We conduct extensive experiments, results from which demonstrate the superiority of our method.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel multi-task prompting method for graph models that unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern, and introduces meta-learning to efficiently learn a better initialization for the multi- task prompt of graphs so that the prompting framework can be more reliable and general for different tasks."
            },
            "score": 2
        },
        {
            "id": "98fc8faae839cc9b9dbfcf8e3ec3a7e2f7729db2",
            "paperId": "98fc8faae839cc9b9dbfcf8e3ec3a7e2f7729db2",
            "title": "NOTABLE: Transferable Backdoor Attacks Against Prompt-based NLP Models",
            "abstract": "Prompt-based learning is vulnerable to backdoor attacks. Existing backdoor attacks against prompt-based models consider injecting backdoors into the entire embedding layers or word embedding vectors. Such attacks can be easily affected by retraining on downstream tasks and with different prompting strategies, limiting the transferability of backdoor attacks. In this work, we propose transferable backdoor attacks against prompt-based models, called NOTABLE, which is independent of downstream tasks and prompting strategies. Specifically, NOTABLE injects backdoors into the encoders of PLMs by utilizing an adaptive verbalizer to bind triggers to specific words (i.e., anchors). It activates the backdoor by pasting input with triggers to reach adversary-desired anchors, achieving independence from downstream tasks and prompting strategies. We conduct experiments on six NLP tasks, three popular models, and three prompting strategies. Empirical results show that NOTABLE achieves superior attack performance (i.e., attack success rate over 90% on all the datasets), and outperforms two state-of-the-art baselines. Evaluations on three defenses show the robustness of NOTABLE. Our code can be found at https://github.com/RU-System-Software-and-Security/Notable.",
            "year": 2023,
            "citationCount": 19,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes transferable backdoor attacks against prompt-based models, called NOTABLE, which is independent of downstream tasks and prompting strategies, and achieves superior attack performance and outperforms two state-of-the-art baselines."
            },
            "score": 2
        },
        {
            "id": "f953dbea152b4160a4fc7463cc6dcd3214111117",
            "paperId": "f953dbea152b4160a4fc7463cc6dcd3214111117",
            "title": "Aligning with Whom? Large Language Models Have Gender and Racial Biases in Subjective NLP Tasks",
            "abstract": "Human perception of language depends on personal backgrounds like gender and ethnicity. While existing studies have shown that large language models (LLMs) hold values that are closer to certain societal groups, it is unclear whether their prediction behaviors on subjective NLP tasks also exhibit a similar bias. In this study, leveraging the POPQUORN dataset which contains annotations of diverse demographic backgrounds, we conduct a series of experiments on four popular LLMs to investigate their capability to understand group differences and potential biases in their predictions for politeness and offensiveness. We find that for both tasks, model predictions are closer to the labels from White and female participants. We further explore prompting with the target demographic labels and show that including the target demographic in the prompt actually worsens the model's performance. More specifically, when being prompted to respond from the perspective of\"Black\"and\"Asian\"individuals, models show lower performance in predicting both overall scores as well as the scores from corresponding groups. Our results suggest that LLMs hold gender and racial biases for subjective NLP tasks and that demographic-infused prompts alone may be insufficient to mitigate such effects. Code and data are available at https://github.com/Jiaxin-Pei/LLM-Group-Bias.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that for both tasks, model predictions are closer to the labels from White and female participants, and that demographic-infused prompts alone may be insufficient to mitigate such effects."
            },
            "score": 2
        },
        {
            "id": "0a25c137edc7c9752aa6d99ae4084683c3fe6b56",
            "paperId": "0a25c137edc7c9752aa6d99ae4084683c3fe6b56",
            "title": "Visual Prompting via Image Inpainting",
            "abstract": "How does one adapt a pre-trained visual model to novel downstream tasks without task-specific finetuning or any model modification? Inspired by prompting in NLP, this paper investigates visual prompting: given input-output image example(s) of a new task at test time and a new input image, the goal is to automatically produce the output image, consistent with the given examples. We show that posing this problem as simple image inpainting - literally just filling in a hole in a concatenated visual prompt image - turns out to be surprisingly effective, provided that the inpainting algorithm has been trained on the right data. We train masked auto-encoders on a new dataset that we curated - 88k unlabeled figures from academic papers sources on Arxiv. We apply visual prompting to these pretrained models and demonstrate results on various downstream image-to-image tasks, including foreground segmentation, single object detection, colorization, edge detection, etc.",
            "year": 2022,
            "citationCount": 109,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper investigates visual prompting: given input-output image example(s) of a new task at test time and a new input image, the goal is to automatically produce the output image, consistent with the given examples, and shows that posing this problem as simple image inpainting turns out to be surprisingly effective."
            },
            "score": 1
        },
        {
            "id": "28fb3c592174878889301d21442c3cf0795f151b",
            "paperId": "28fb3c592174878889301d21442c3cf0795f151b",
            "title": "Explicit Visual Prompting for Universal Foreground Segmentations",
            "abstract": "Foreground segmentation is a fundamental problem in computer vision, which includes salient object detection, forgery detection, defocus blur detection, shadow detection, and camouflage object detection. Previous works have typically relied on domain-specific solutions to address accuracy and robustness issues in those applications. In this paper, we present a unified framework for a number of foreground segmentation tasks without any task-specific designs. We take inspiration from the widely-used pre-training and then prompt tuning protocols in NLP and propose a new visual prompting model, named Explicit Visual Prompting (EVP). Different from the previous visual prompting which is typically a dataset-level implicit embedding, our key insight is to enforce the tunable parameters focusing on the explicit visual content from each individual image, i.e., the features from frozen patch embeddings and high-frequency components. Our method freezes a pre-trained model and then learns task-specific knowledge using a few extra parameters. Despite introducing only a small number of tunable parameters, EVP achieves superior performance than full fine-tuning and other parameter-efficient fine-tuning methods. Experiments in fourteen datasets across five tasks show the proposed method outperforms other task-specific methods while being considerably simple. The proposed method demonstrates the scalability in different architectures, pre-trained weights, and tasks. The code is available at: https://github.com/NiFangBaAGe/Explicit-Visual-Prompt.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper takes inspiration from the widely-used pre-training and then prompt tuning protocols in NLP and proposes a new visual prompting model, named Explicit Visual Prompting (EVP), which freezes a pre-trained model and then learns task-specific knowledge using a few extra parameters."
            },
            "score": 1
        }
    ],
    "novelty": "no"
}