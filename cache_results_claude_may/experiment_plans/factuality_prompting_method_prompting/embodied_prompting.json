{
    "topic_description": "novel prompting methods that can improve factuality and reduce hallucination of large language models",
    "idea_name": "Embodied Prompting",
    "raw_idea": {
        "Problem": "Large language models struggle to reason about physical and spatial properties of objects and environments. This leads to non-sensical or inconsistent generations for tasks like visual storytelling, embodied QA, or robotic instructions that require an understanding of the physical world.",
        "Existing Methods": "Most prior work focuses on using visual grounding like images or videos to improve models' physical understanding. However, this requires aligned vision-language training data. Other approaches use knowledge graphs or simulations, but these have limited coverage or are hard to interface with language.",
        "Motivation": "Studies of child development show that humans learn about the physical world through interactive, embodied experience - moving around, manipulating objects, testing out physics, etc. This embodied learning builds rich intuitive knowledge that is critical for language understanding and generation. We propose prompting models to mimic embodied learning by imagining themselves as agents exploring and interacting with a physical environment.",
        "Proposed Method": "We develop a prompt chain that walks the model through an imagined embodied experience relevant to the task. For example, for visual storytelling, we may prompt the model to imagine itself as a character moving through the scene of the image, describing what it sees and does at each step. For a physical reasoning task, we may prompt the model to imagine performing a series of actions and physical tests to determine an object's properties. Critically, the prompt encourages the model to ground its imagination in physical laws, spatial relationships, and temporal dynamics. We also include a few-shot prompt demonstrating this kind of embodied imagination and physical grounding.",
        "Experiment Plan": "We will evaluate Embodied Prompting on a range of tasks that require physical and spatial reasoning, including visual storytelling (e.g. VIST), embodied QA (e.g. EQA), physical commonsense reasoning (e.g. PIQA), and robotic instruction following (e.g. R2R). We will compare to baselines like direct prompting, Chain-of-Thought prompting, and vision-language models. Evaluation will use both automatic metrics like perplexity, BLEU, and execution success rate, and human judgment of criteria like physical plausibility, temporal coherence, and consistency with task constraints. We will also collect human ratings of how well the generated imaginations capture relevant physical and spatial details."
    },
    "full_experiment_plan": {
        "Title": "Embodied Prompting: Improving Physical Reasoning in Language Models through Imagined Interactions",
        "Problem Statement": "Large language models struggle to reason about physical and spatial properties of objects and environments, leading to inconsistent or nonsensical outputs for tasks that require an understanding of the physical world, such as visual storytelling, embodied question answering, or generating instructions for robots.",
        "Motivation": "Most prior work has focused on using visual grounding like images or videos to improve models' physical understanding. However, this requires aligned vision-language training data which can be difficult to obtain at scale. Other approaches use knowledge graphs or simulations, but these have limited coverage or are hard to interface with natural language. Studies of child development show that humans learn about the physical world through interactive, embodied experience - moving around, manipulating objects, testing out physics, etc. This embodied learning builds rich intuitive knowledge that is critical for language understanding and generation. We propose prompting models to mimic embodied learning by imagining themselves as agents exploring and interacting with a physical environment.",
        "Proposed Method": "We develop a prompt chain that walks the model through an imagined embodied experience relevant to the task. For example, for visual storytelling, we may prompt the model to imagine itself as a character moving through the scene of the image, describing what it sees and does at each step. For a physical reasoning task, we may prompt the model to imagine performing a series of actions and physical tests to determine an object's properties. Critically, the prompt encourages the model to ground its imagination in physical laws, spatial relationships, and temporal dynamics. We also include a few-shot prompt demonstrating this kind of embodied imagination and physical grounding.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We will evaluate Embodied Prompting on a range of tasks that require physical and spatial reasoning, including visual storytelling (VIST dataset), embodied question answering (EQA dataset), physical commonsense reasoning (PIQA dataset), and robotic instruction following (R2R dataset).",
            "Step 2: Construct Prompts": "For each task, we will design a set of Embodied Prompting templates that guide the model to imagine an embodied experience relevant to the task. The prompts will include instructions for the model to imagine itself as an agent in the environment, to describe its imagined sensory observations, actions, and physical interactions, and to ground its outputs in physical and spatial constraints. We will also include a few-shot example in each prompt to demonstrate the desired style of embodied imagination and physical grounding. As a baseline, we will use direct prompting where the model is simply given the task input and asked to generate an output.",
            "Step 3: Select Models": "We will evaluate Embodied Prompting on state-of-the-art large language models including GPT-3.5 (text-davinci-003), GPT-4, and PaLM. These models have shown strong performance on language tasks but still struggle with physical reasoning.",
            "Step 4: Collect Model Outputs": "For each task and dataset, we will prompt the models using both direct prompting and Embodied Prompting templates. We will generate outputs from the models on the full test sets of each dataset.",
            "Step 5: Evaluate Results": "We will evaluate the model-generated outputs using both automatic metrics and human judgments. Automatic metrics will include perplexity, BLEU, ROUGE, and CIDEr for language quality, and task-specific metrics like goal progress for EQA and navigation error for R2R. We will also have human raters assess the outputs on criteria like physical plausibility, temporal coherence, and consistency with task constraints. Additionally, we will have humans rate how well the generated imaginations in Embodied Prompting capture relevant physical and spatial details."
        },
        "Test Case Examples": {
            "Example 1": {
                "Task": "Visual Storytelling",
                "Image": "(Include URL or description of an example image from the VIST dataset)",
                "Baseline Prompt": "Write a story about the following image: (image)",
                "Baseline Output": "(Include an example baseline story that has some physical inconsistencies)",
                "Embodied Prompt": "Imagine you are a character in the following scene. Describe your embodied experience exploring the environment, moving around, and interacting with objects, making sure to ground your descriptions in physical details and spatial relationships. Use your imagined experience to tell a coherent story. Here is the scene: (image)",
                "Embodied Output": "(Include an example Embodied Prompting story that is more physically grounded and consistent)",
                "Explanation": "The Embodied Prompting output should demonstrate a more physically grounded narrative where the character's imagined interactions with the environment lead to a story that is more consistent with the spatial and physical constraints of the depicted scene."
            },
            "Example 2": {
                "Task": "Physical Commonsense Reasoning",
                "Question": "A book is placed on a table. A ruler is placed on top of the book. If someone quickly pulls the book out from under the ruler, what will happen to the ruler?",
                "Baseline Prompt": "Answer the following question: (question)",
                "Baseline Output": "The ruler will stay in place floating in the air.",
                "Embodied Prompt": "Imagine the following scenario in detail, grounding your imagination in physical laws. Visualize the objects, their materials, and their relative positions. Mentally simulate what would happen if the described action is performed, paying attention to physical dynamics. After you've imagined through the scenario, use your embodied reasoning to answer the question. Here is the scenario and question: (question)",
                "Embodied Output": "Based on my imagined mental simulation, when the book is quickly pulled out from under the ruler, the ruler will fall and clatter onto the table. This is because the ruler was initially supported by the book against gravity. When that support is suddenly removed, gravity will immediately pull the unsupported ruler straight down onto the table surface. The ruler is likely light enough that it won't have enough horizontal momentum to slide along the table - it will just drop in place. The specific sound it makes when hitting the table will depend on the ruler's material (e.g. wood, metal, plastic) but there will likely be an audible clatter or thud.",
                "Explanation": "The Embodied Prompting output should demonstrate a more accurate physical prediction, grounded in an imagined mental simulation of the scenario that takes into account objects, positions, materials, forces, and dynamics over time. The language should reflect this embodied reasoning rather than naive folk physics misconceptions."
            }
        },
        "Fallback Plan": "If the proposed Embodied Prompting method does not show significant improvements over baselines, we can analyze the failure cases to understand why. Potential issues could include: (1) The prompts are not effectively eliciting embodied imagination and physical grounding from the models. We could try iterating on the prompt design, e.g. by providing more detailed instructions, examples, or constraints. (2) The models lack the necessary physical knowledge to ground their imaginations effectively, even when prompted. We could try augmenting the prompts with relevant physical facts or rules. (3) The tasks or datasets require physical knowledge that is too specialized or complex to be captured by imagined interactions alone. We could try combining Embodied Prompting with other methods like knowledge retrieval or simulation. If Embodied Prompting still underperforms, we could pivot to an analysis of its limitations and the challenges of physical reasoning for language models. The project could aim to characterize the kinds of physical knowledge that are easy or hard to elicit from models through prompting, and how this compares to human physical reasoning. This could yield insights into the cognitive mechanisms underlying physical understanding and suggest new paths for imbuing models with such abilities."
    },
    "novelty_queries": [
        "KeywordQuery(\"embodied prompting language models\")",
        "KeywordQuery(\"physical reasoning language models\")",
        "KeywordQuery(\"imagined interactions language models\")",
        "KeywordQuery(\"embodied learning language models\")",
        "KeywordQuery(\"Embodied Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "41531594d7e0f3b2e138ae43e0a0f6e24a9b014c",
            "paperId": "41531594d7e0f3b2e138ae43e0a0f6e24a9b014c",
            "title": "Code as Policies: Language Model Programs for Embodied Control",
            "abstract": "Large language models (LLMs) trained on code-completion have been shown to be capable of synthesizing simple Python programs from docstrings [1]. We find that these code-writing LLMs can be re-purposed to write robot policy code, given natural language commands. Specifically, policy code can express functions or feedback loops that process perception outputs (e.g., from object detectors [2], [3]) and parameterize control primitive APIs. When provided as input several example language commands (formatted as comments) followed by corresponding policy code (via few-shot prompting), LLMs can take in new commands and autonomously re-compose API calls to generate new policy code respectively. By chaining classic logic structures and referencing third-party libraries (e.g., NumPy, Shapely) to perform arithmetic, LLMs used in this way can write robot policies that (i) exhibit spatial-geometric reasoning, (ii) generalize to new instructions, and (iii) prescribe precise values (e.g., velocities) to ambiguous descriptions (\u2018faster\u2019) depending on context (i.e., behavioral commonsense). This paper presents Code as Policies: a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms. Central to our approach is prompting hierarchical code-gen (recursively defining undefined functions), which can write more complex code and also improves state-of-the-art to solve 39.8% of problems on the HumanEval [1] benchmark. Code and videos are available at https://code-as-policies.github.io",
            "year": 2022,
            "citationCount": 386,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Code as Policies is presented, a robot-centric formulation of language model generated programs (LMPs) that can represent reactive policies (e.g., impedance controllers), as well as waypoint-based policies (vision-based pick and place, trajectory-based control), demonstrated across multiple real robot platforms."
            },
            "score": 8,
            "novelty_score": "The project proposal aims to improve physical reasoning in language models through imagined embodied interactions, while the paper focuses on using language models to generate robot policy code from natural language commands.\n\nProject proposal summary: Improving physical reasoning in language models through imagined embodied interactions.\nPaper summary: Using language models to generate robot policy code from natural language commands.\n\nThe two works tackle different problems (physical reasoning vs. code generation) and propose different approaches (imagined embodied interactions vs. code-writing language models).\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "6f821d75968bc8de070af3ce5aa7f57bc031fafb",
            "paperId": "6f821d75968bc8de070af3ce5aa7f57bc031fafb",
            "title": "Language Models Meet World Models: Embodied Experiences Enhance Language Models",
            "abstract": "While large language models (LMs) have shown remarkable capabilities across numerous tasks, they often struggle with simple reasoning and planning in physical environments, such as understanding object permanence or planning household activities. The limitation arises from the fact that LMs are trained only on written text and miss essential embodied knowledge and skills. In this paper, we propose a new paradigm of enhancing LMs by finetuning them with world models, to gain diverse embodied knowledge while retaining their general language capabilities. Our approach deploys an embodied agent in a world model, particularly a simulator of the physical world (VirtualHome), and acquires a diverse set of embodied experiences through both goal-oriented planning and random exploration. These experiences are then used to finetune LMs to teach diverse abilities of reasoning and acting in the physical world, e.g., planning and completing goals, object permanence and tracking, etc. Moreover, it is desirable to preserve the generality of LMs during finetuning, which facilitates generalizing the embodied knowledge across tasks rather than being tied to specific simulations. We thus further introduce the classical (EWC) for selective weight updates, combined with low-rank adapters (LoRA) for training efficiency. Extensive experiments show our approach substantially improves base LMs on 18 downstream tasks by 64.28% on average. In particular, the small LMs (1.3B, 6B, and 13B) enhanced by our approach match or even outperform much larger LMs (e.g., ChatGPT).",
            "year": 2023,
            "citationCount": 32,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new paradigm of enhancing LMs by finetuning them with world models, to gain diverse embodied knowledge while retaining their general language capabilities, and introduces the classical for selective weight updates, combined with low-rank adapters (LoRA) for training efficiency."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is improving physical reasoning abilities of language models for tasks that require understanding the physical world. The proposed approach is to prompt the models to imagine embodied interactions with the environment.\n\nThe research problem in the paper is also improving language models' physical reasoning and planning abilities. The proposed approach is to finetune language models on embodied experiences collected from a physical world simulator.\n\nBoth works aim to enhance language models with physical reasoning skills through embodied knowledge. However, the proposal focuses on prompting while the paper focuses on finetuning with collected experiences.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "f197bf0fc2f228483f6af3285000d54d8d97f9eb",
            "paperId": "f197bf0fc2f228483f6af3285000d54d8d97f9eb",
            "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models",
            "abstract": "We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",
            "year": 2023,
            "citationCount": 336,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving physical and spatial reasoning abilities of language models through imagined embodied interactions. The approach is to use prompts that guide the model to imagine an embodied experience relevant to the task, grounding the imagination in physical laws and spatial relationships.\n\nThe research problem in the paper is building an open-ended embodied agent in Minecraft that continuously explores, acquires skills, and makes discoveries. The approach is to use an automatic curriculum, a skill library, and an iterative prompting mechanism that incorporates environment feedback.\n\nThe proposal focuses on improving language models' physical reasoning via imagined interactions, while the paper focuses on building an embodied Minecraft agent that learns through actual interactions. The methods are quite different - prompting for imagined grounding vs. a system for real embodied learning.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "c1c663d8a7d78342d8eabb6ca144e5761b6a2443",
            "paperId": "c1c663d8a7d78342d8eabb6ca144e5761b6a2443",
            "title": "Distilling Internet-Scale Vision-Language Models into Embodied Agents",
            "abstract": "Instruction-following agents must ground language into their observation and action spaces. Learning to ground language is challenging, typically requiring domain-specific engineering or large quantities of human interaction data. To address this challenge, we propose using pretrained vision-language models (VLMs) to supervise embodied agents. We combine ideas from model distillation and hindsight experience replay (HER), using a VLM to retroactively generate language describing the agent's behavior. Simple prompting allows us to control the supervision signal, teaching an agent to interact with novel objects based on their names (e.g., planes) or their features (e.g., colors) in a 3D rendered environment. Fewshot prompting lets us teach abstract category membership, including pre-existing categories (food vs toys) and ad-hoc ones (arbitrary preferences over objects). Our work outlines a new and effective way to use internet-scale VLMs, repurposing the generic language grounding acquired by such models to teach task-relevant groundings to embodied agents.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work outlines a new and effective way to use internet-scale VLMs, repurposing the generic language grounding acquired by such models to teach task-relevant groundings to embodied agents."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve physical reasoning in language models through imagined embodied interactions, while the paper focuses on using vision-language models to supervise embodied agents to ground language into observations and actions.\n\nThe project proposes prompting language models to imagine embodied experiences to improve physical reasoning, whereas the paper proposes using vision-language models to generate language descriptions to teach embodied agents language grounding.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "c3925ef53f864e5c30189272f63801248ff1406f",
            "paperId": "c3925ef53f864e5c30189272f63801248ff1406f",
            "title": "March in Chat: Interactive Prompting for Remote Embodied Referring Expression",
            "abstract": "Many Vision-and-Language Navigation (VLN) tasks have been proposed in recent years, from room-based to object-based and indoor to outdoor. The REVERIE (Remote Embodied Referring Expression) is interesting since it only provides high-level instructions to the agent, which are closer to human commands in practice. Nevertheless, this poses more challenges than other VLN tasks since it requires agents to infer a navigation plan only based on a short instruction. Large Language Models (LLMs) show great potential in robot action planning by providing proper prompts. Still, this strategy has not been explored under the REVERIE settings. There are several new challenges. For example, the LLM should be environment-aware so that the navigation plan can be adjusted based on the current visual observation. Moreover, the LLM planned actions should be adaptable to the much larger and more complex REVERIE environment. This paper proposes a March-in-Chat (MiC) model that can talk to the LLM on the fly and plan dynamically based on a newly proposed Room-and-Object Aware Scene Perceiver (ROASP). Our MiC model outperforms the previous state-of-the-art by large margins by SPL and RGSPL metrics on the REVERIE benchmark. The source code is available at https://github.com/YanyuanQiao/MiC",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A March-in-Chat (MiC) model that can talk to the LLM on the fly and plan dynamically based on a newly proposed Room-and-Object Aware Scene Perceiver (ROASP) and outperforms the previous state-of-the-art by large margins by SPL and RGSPL metrics on the REVERIE benchmark."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve physical reasoning in language models through imagined embodied interactions, while the paper focuses on interactive prompting for remote embodied referring expression in vision-and-language navigation tasks.\n\nThe project proposes using prompt chains to guide language models to imagine embodied experiences and ground their outputs in physical laws and spatial relationships. In contrast, the paper proposes a March-in-Chat model that interacts with large language models on the fly and plans dynamically based on visual observations in the REVERIE environment.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "9bf9250624729a002830efe10afb16c015ff8510",
            "paperId": "9bf9250624729a002830efe10afb16c015ff8510",
            "title": "Can an Embodied Agent Find Your \u201cCat-shaped Mug\u201d? LLM-Guided Exploration for Zero-Shot Object Navigation",
            "abstract": "\u2014We present LGX (Language-guided Exploration), a novel algorithm for Language-Driven Zero-Shot Object Goal Navigation (L-ZSON), where an embodied agent navigates to an uniquely described target object in a previously unseen environment. Our approach makes use of Large Language Models (LLMs) for this task by leveraging the LLM\u2019s commonsense-reasoning capabilities for making sequential navigational decisions. Simultaneously, we perform generalized target object detection using a pre-trained Vision-Language grounding model. We achieve state-of-the-art zero-shot object navigation results on RoboTHOR with a success rate (SR) improvement of over 27% over the current baseline of the OWL-ViT CLIP on Wheels (OWL CoW). Furthermore, we study the usage of LLMs for robot navigation and present an analysis of various prompting strategies affecting the model output. Finally, we showcase the benefits of our approach via real-world experiments that indicate the superior performance of LGX in detecting and navigating to visually unique objects.",
            "year": null,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve physical reasoning in language models through imagined embodied interactions, while the paper focuses on using large language models to guide an embodied agent in navigating to a target object in an unseen environment.\n\nProject proposal: Improving physical reasoning in language models through imagined embodied interactions.\nPaper: Using large language models to guide an embodied agent in navigating to a target object.\n\nThe two works have different goals and approaches. The project proposal focuses on improving language models' physical understanding, while the paper uses language models to guide navigation in embodied agents.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "ae8aabebad0c3ecae165ec05c18a2072ed360d1e",
            "paperId": "ae8aabebad0c3ecae165ec05c18a2072ed360d1e",
            "title": "NEWTON: Are Large Language Models Capable of Physical Reasoning?",
            "abstract": "Large Language Models (LLMs), through their contextualized representations, have been empirically proven to encapsulate syntactic, semantic, word sense, and common-sense knowledge. However, there has been limited exploration of their physical reasoning abilities, specifically concerning the crucial attributes for comprehending everyday objects. To address this gap, we introduce NEWTON, a repository and benchmark for evaluating the physics reasoning skills of LLMs. Further, to enable domain-specific adaptation of this benchmark, we present a pipeline to enable researchers to generate a variant of this benchmark that has been customized to the objects and attributes relevant for their application. The NEWTON repository comprises a collection of 2800 object-attribute pairs, providing the foundation for generating infinite-scale assessment templates. The NEWTON benchmark consists of 160K QA questions, curated using the NEWTON repository to investigate the physical reasoning capabilities of several mainstream language models across foundational, explicit, and implicit reasoning tasks. Through extensive empirical analysis, our results highlight the capabilities of LLMs for physical reasoning. We find that LLMs like GPT-4 demonstrate strong reasoning capabilities in scenario-based tasks but exhibit less consistency in object-attribute reasoning compared to humans (50% vs. 84%). Furthermore, the NEWTON platform demonstrates its potential for evaluating and enhancing language models, paving the way for their integration into physically grounded settings, such as robotic manipulation. Project site: https://newtonreasoning.github.io",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The NEWTON platform demonstrates its potential for evaluating and enhancing language models, paving the way for their integration into physically grounded settings, such as robotic manipulation."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving physical reasoning abilities of language models, while the approach is using embodied prompting to guide the model to imagine physical interactions.\n\nThe research problem in the paper is also evaluating the physical reasoning abilities of language models, but the approach is creating a benchmark dataset NEWTON to test their performance on various reasoning tasks.\n\nAlthough both works aim to study physical reasoning in language models, the proposal focuses on a novel prompting method to improve the ability, while the paper focuses on evaluating the existing ability using a new benchmark. Therefore, they are not directly relevant in terms of the proposed approach.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "316f980cfd2e217234386166a46eb080bf027cdd",
            "paperId": "316f980cfd2e217234386166a46eb080bf027cdd",
            "title": "Physically Grounded Vision-Language Models for Robotic Manipulation",
            "abstract": "Recent advances in vision-language models (VLMs) have led to improved performance on tasks such as visual question answering and image captioning. Consequently, these models are now well-positioned to reason about the physical world, particularly within domains such as robotic manipulation. However, current VLMs are limited in their understanding of the physical concepts (e.g., material, fragility) of common objects, which restricts their usefulness for robotic manipulation tasks that involve interaction and physical reasoning about such objects. To address this limitation, we propose PhysObjects, an object-centric dataset of 39.6K crowd-sourced and 417K automated physical concept annotations of common household objects. We demonstrate that fine-tuning a VLM on PhysObjects improves its understanding of physical object concepts, including generalization to held-out concepts, by capturing human priors of these concepts from visual appearance. We incorporate this physically grounded VLM in an interactive framework with a large language model-based robotic planner, and show improved planning performance on tasks that require reasoning about physical object concepts, compared to baselines that do not leverage physically grounded VLMs. We additionally illustrate the benefits of our physically grounded VLM on a real robot, where it improves task success rates. We release our dataset and provide further details and visualizations of our results at https://iliad.stanford.edu/pg-vlm/.",
            "year": 2023,
            "citationCount": 30,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that fine-tuning a VLM on PhysObjects improves its understanding of physical object concepts, including generalization to held-out concepts, by capturing human priors of these concepts from visual appearance."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve physical reasoning in language models through imagined embodied interactions, while the paper focuses on improving robotic manipulation by incorporating physical concept knowledge into vision-language models.\n\nThe project proposes using prompts to guide language models to imagine embodied experiences and ground their outputs in physical constraints, whereas the paper proposes fine-tuning vision-language models on a dataset of physical concept annotations to capture human priors from visual appearance.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "f3cf71c51b882fe3111d71c4bf104297d38197f8",
            "paperId": "f3cf71c51b882fe3111d71c4bf104297d38197f8",
            "title": "Inner Monologue: Embodied Reasoning through Planning with Language Models",
            "abstract": "Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to understand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs planning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent's own choices. In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high-level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment in the real world.",
            "year": 2022,
            "citationCount": 483,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios, and finds that closed-loop language feedback significantly improves high-level instruction completion on three domains."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve physical reasoning in language models through imagined embodied interactions, while the paper focuses on using language models for embodied reasoning and planning in robotics tasks through natural language feedback.\n\nThe project proposes using prompts to guide language models to imagine embodied experiences and ground their outputs in physical constraints, whereas the paper leverages environment feedback through natural language to enable language models to reason and plan in robotic control scenarios.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "729fc01274cc26798654a318d1a95e73c61f99a3",
            "paperId": "729fc01274cc26798654a318d1a95e73c61f99a3",
            "title": "Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models",
            "abstract": "Pre-trained and frozen large language models (LLMs) can effectively map simple scene rearrangement instructions to programs over a robot's visuomotor functions through appropriate few-shot example prompting. To parse open-domain natural language and adapt to a user's idiosyncratic procedures, not known during prompt engineering time, fixed prompts fall short. In this paper, we introduce HELPER, an embodied agent equipped with an external memory of language-program pairs that parses free-form human-robot dialogue into action programs through retrieval-augmented LLM prompting: relevant memories are retrieved based on the current dialogue, instruction, correction, or VLM description, and used as in-context prompt examples for LLM querying. The memory is expanded during deployment to include pairs of user's language and action plans, to assist future inferences and personalize them to the user's language and routines. HELPER sets a new state-of-the-art in the TEACh benchmark in both Execution from Dialog History (EDH) and Trajectory from Dialogue (TfD), with a 1.7x improvement over the previous state-of-the-art for TfD. Our models, code, and video results can be found in our project's website: https://helper-agent-llm.github.io.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "HELPER is introduced, an embodied agent equipped with an external memory of language-program pairs that parses free-form human-robot dialogue into action programs through retrieval-augmented LLM prompting: relevant memories are retrieved based on the current dialogue, instruction, correction, or VLM description, and used as in-context prompt examples for LLM querying."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve physical reasoning in language models through imagined embodied interactions, while the paper focuses on using memory-augmented large language models to map open-domain instructions to robot actions.\n\nThe project proposes using prompts to guide language models to imagine embodied experiences and ground their outputs in physical constraints, whereas the paper retrieves relevant examples from an external memory to personalize the model to the user's language and routines.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "2d30d800e946d3699d9c41bb95c36a6db63676e7",
            "paperId": "2d30d800e946d3699d9c41bb95c36a6db63676e7",
            "title": "Prompter: Utilizing Large Language Model Prompting for a Data Efficient Embodied Instruction Following",
            "abstract": "Embodied Instruction Following (EIF) studies how autonomous mobile manipulation robots should be controlled to accomplish long-horizon tasks described by natural language instructions. While much research on EIF is conducted in simulators, the ultimate goal of the field is to deploy the agents in real life. This is one of the reasons why recent methods have moved away from training models end-to-end and take modular approaches, which do not need the costly expert operation data. However, as it is still in the early days of importing modular ideas to EIF, a search for modules effective in the EIF task is still far from a conclusion. In this paper, we propose to extend the modular design using knowledge obtained from two external sources. First, we show that embedding the physical constraints of the deployed robots into the module design is highly effective. Our design also allows the same modular system to work across robots of different configurations with minimal modifications. Second, we show that the landmark-based object search, previously implemented by a trained model requiring a dedicated set of data, can be replaced by an implementation that prompts pretrained large language models for landmark-object relationships, eliminating the need for collecting dedicated training data. Our proposed Prompter achieves 41.53\\% and 45.32\\% on the ALFRED benchmark with high-level instructions only and step-by-step instructions, respectively, significantly outperforming the previous state of the art by 5.46\\% and 9.91\\%.",
            "year": 2022,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper shows that embedding the physical constraints of the deployed robots into the module design is highly effective, and shows that the landmark-based object search can be replaced by an implementation that prompts pretrained large language models for landmark-object relationships, eliminating the need for collecting dedicated training data."
            },
            "score": 6
        },
        {
            "id": "32524aa3ae8522542753ed7e6f4cca3970e4acab",
            "paperId": "32524aa3ae8522542753ed7e6f4cca3970e4acab",
            "title": "Can an Embodied Agent Find Your \u201cCat-shaped Mug\u201d? LLM-Based Zero-Shot Object Navigation",
            "abstract": "We present language-guided exploration (LGX), a novel algorithm for Language-Driven Zero-Shot Object Goal Navigation (L-ZSON), where an embodied agent navigates to an uniquely described target object in a previously unseen environment. Our approach makes use of large language models (LLMs) for this task by leveraging the LLM's commonsense-reasoning capabilities for making sequential navigational decisions. Simultaneously, we perform generalized target object detection using a pre-trained Vision-Language grounding model. We achieve state-of-the-art zero-shot object navigation results on RoboTHOR with a success rate (SR) improvement of over 27% over the current baseline of the OWL-ViT CLIP on Wheels (OWL CoW). Furthermore, we study the usage of LLMs for robot navigation and present an analysis of various prompting strategies affecting the model output. Finally, we showcase the benefits of our approach via real-world experiments that indicate the superior performance of LGX in detecting and navigating to visually unique objects.",
            "year": 2023,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LGX is presented, a novel algorithm for Language-Driven Zero-Shot Object Goal Navigation (L-ZSON), where an embodied agent navigates to an uniquely described target object in a previously unseen environment by leveraging the LLM's commonsense-reasoning capabilities for making sequential navigational decisions."
            },
            "score": 6
        },
        {
            "id": "19d3099bf0fbe63372df304c35f2ebe2ff1af357",
            "paperId": "19d3099bf0fbe63372df304c35f2ebe2ff1af357",
            "title": "Explore until Confident: Efficient Exploration for Embodied Question Answering",
            "abstract": "We consider the problem of Embodied Question Answering (EQA), which refers to settings where an embodied agent such as a robot needs to actively explore an environment to gather information until it is confident about the answer to a question. In this work, we leverage the strong semantic reasoning capabilities of large vision-language models (VLMs) to efficiently explore and answer such questions. However, there are two main challenges when using VLMs in EQA: they do not have an internal memory for mapping the scene to be able to plan how to explore over time, and their confidence can be miscalibrated and can cause the robot to prematurely stop exploration or over-explore. We propose a method that first builds a semantic map of the scene based on depth information and via visual prompting of a VLM - leveraging its vast knowledge of relevant regions of the scene for exploration. Next, we use conformal prediction to calibrate the VLM's question answering confidence, allowing the robot to know when to stop exploration - leading to a more calibrated and efficient exploration strategy. To test our framework in simulation, we also contribute a new EQA dataset with diverse, realistic human-robot scenarios and scenes built upon the Habitat-Matterport 3D Research Dataset (HM3D). Both simulated and real robot experiments show our proposed approach improves the performance and efficiency over baselines that do no leverage VLM for exploration or do not calibrate its confidence. Webpage with experiment videos and code: https://explore-eqa.github.io/",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a method that first builds a semantic map of the scene based on depth information and via visual prompting of a VLM - leveraging its vast knowledge of relevant regions of the scene for exploration, allowing the robot to know when to stop exploration - leading to a more calibrated and efficient exploration strategy."
            },
            "score": 6
        },
        {
            "id": "9d21467c22b1709ca5a7f6c21cbbcbf5a5c4c9a9",
            "paperId": "9d21467c22b1709ca5a7f6c21cbbcbf5a5c4c9a9",
            "title": "POSQA: Probe the World Models of LLMs with Size Comparisons",
            "abstract": "Embodied language comprehension emphasizes that language understanding is not solely a matter of mental processing in the brain but also involves interactions with the physical and social environment. With the explosive growth of Large Language Models (LLMs) and their already ubiquitous presence in our daily lives, it is becoming increasingly necessary to verify their real-world understanding. Inspired by cognitive theories, we propose POSQA: a Physical Object Size Question Answering dataset with simple size comparison questions to examine the extremity and analyze the potential mechanisms of the embodied comprehension of the latest LLMs. We show that even the largest LLMs today perform poorly under the zero-shot setting. We then push their limits with advanced prompting techniques and external knowledge augmentation. Furthermore, we investigate whether their real-world comprehension primarily derives from contextual information or internal weights and analyse the impact of prompt formats and report bias of different objects. Our results show that real-world understanding that LLMs shaped from textual data can be vulnerable to deception and confusion by the surface form of prompts, which makes it less aligned with human behaviours.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results show that real-world understanding that Large Language Models shaped from textual data can be vulnerable to deception and confusion by the surface form of prompts, which makes it less aligned with human behaviours."
            },
            "score": 6
        },
        {
            "id": "74219927bbd51ce99cc4aa48ebdd25f9a2ba298e",
            "paperId": "74219927bbd51ce99cc4aa48ebdd25f9a2ba298e",
            "title": "SimLM: Can Language Models Infer Parameters of Physical Systems?",
            "abstract": "Several machine learning methods aim to learn or reason about complex physical systems. A common first-step towards reasoning is to infer system parameters from observations of its behavior. In this paper, we investigate the performance of Large Language Models (LLMs) at performing parameter inference in the context of physical systems. Our experiments suggest that they are not inherently suited to this task, even for simple systems. We propose a promising direction of exploration, which involves the use of physical simulators to augment the context of LLMs. We assess and compare the performance of different LLMs on a simple example with and without access to physical simulation.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The performance of Large Language Models at performing parameter inference in the context of physical systems is investigated, and it is suggested that they are not inherently suited to this task, even for simple systems."
            },
            "score": 6
        },
        {
            "id": "2b6291eb76e2ff885238e94704bb795046d7d530",
            "paperId": "2b6291eb76e2ff885238e94704bb795046d7d530",
            "title": "SafeText: A Benchmark for Exploring Physical Safety in Language Models",
            "abstract": "Understanding what constitutes safe text is an important issue in natural language processing and can often prevent the deployment of models deemed harmful and unsafe. One such type of safety that has been scarcely studied is commonsense physical safety, i.e. text that is not explicitly violent and requires additional commonsense knowledge to comprehend that it leads to physical harm. We create the first benchmark dataset, SafeText, comprising real-life scenarios with paired safe and physically unsafe pieces of advice. We utilize SafeText to empirically study commonsense physical safety across various models designed for text generation and commonsense reasoning tasks. We find that state-of-the-art large language models are susceptible to the generation of unsafe text and have difficulty rejecting unsafe advice. As a result, we argue for further studies of safety and the assessment of commonsense physical safety in models before release.",
            "year": 2022,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that state-of-the-art large language models are susceptible to the generation of unsafe text and have difficulty rejecting unsafe advice, and is argued for further studies of safety and the assessment of commonsense physical safety in models before release."
            },
            "score": 6
        },
        {
            "id": "7637ed79d30d0139901175ae4abedd822c217ab4",
            "paperId": "7637ed79d30d0139901175ae4abedd822c217ab4",
            "title": "3D-LLM: Injecting the 3D World into Large Language Models",
            "abstract": "Large language models (LLMs) and Vision-Language Models (VLMs) have been proven to excel at multiple tasks, such as commonsense reasoning. Powerful as these models can be, they are not grounded in the 3D physical world, which involves richer concepts such as spatial relationships, affordances, physics, layout, and so on. In this work, we propose to inject the 3D world into large language models and introduce a whole new family of 3D-LLMs. Speci\ufb01cally, 3D-LLMs can take 3D point clouds and their features as input and perform a diverse set of 3D-related tasks, including captioning, dense captioning, 3D question answering, task decomposition, 3D grounding, 3D-assisted dialog, navigation, and so on. Using three types of prompting mechanisms that we design, we are able to collect over 300k 3D-language data covering these tasks. To ef\ufb01ciently train 3D-LLMs, we \ufb01rst utilize a 3D feature extractor that obtains 3D features from rendered multi-view images. Then, we use 2D VLMs as our backbones to train our 3D-LLMs. By introducing a 3D localization mechanism, 3D-LLMs can better capture 3D spatial information. Experiments on ScanQA show that our model outperforms state-of-the-art baselines by a large margin ( e.g. , the BLEU-1 score surpasses state-of-the-art score by 9%). Furthermore, experiments on our held-in datasets for 3D captioning, task composition, and 3D-assisted dialogue show that our model outperforms 2D VLMs. Qualitative examples also show that our model could perform more tasks beyond the scope of existing LLMs and VLMs. Project Page: : https:",
            "year": 2023,
            "citationCount": 70,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes to inject the 3D world into large language models and introduce a whole new family of 3D-LLMs, and introduces a 3D localization mechanism, which can better capture 3D spatial information."
            },
            "score": 6
        },
        {
            "id": "e4be613cc875e61b8c1c6c60d958f1c20d12d6c0",
            "paperId": "e4be613cc875e61b8c1c6c60d958f1c20d12d6c0",
            "title": "Task and Motion Planning with Large Language Models for Object Rearrangement",
            "abstract": "Multi-object rearrangement is a crucial skill for service robots, and commonsense reasoning is frequently needed in this process. However, achieving commonsense arrangements requires knowledge about objects, which is hard to transfer to robots. Large language models (LLMs) are one potential source of this knowledge, but they do not naively capture information about plausible physical arrangements of the world. We propose LLM-GROP, which uses prompting to extract commonsense knowledge about semantically valid object configurations from an LLM and instantiates them with a task and motion planner in order to generalize to varying scene geometry. LLM-GROP allows us to go from natural-language commands to human-aligned object rearrangement in varied environments. Based on human evaluations, our approach achieves the highest rating while outperforming competitive baselines in terms of success rate while maintaining comparable cumulative action costs. Finally, we demonstrate a practical implementation of LLM-GROP on a mobile manipulator in real-world scenarios. Supplementary materials are available at: https://sites.google.com/view/llm-grop",
            "year": 2023,
            "citationCount": 80,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LLM-GROP is proposed, which uses prompting to extract commonsense knowledge about semantically valid object configurations from an LLM and instantiates them with a task and motion planner in order to generalize to varying scene geometry."
            },
            "score": 6
        },
        {
            "id": "5edf706467dc76cd09319592d18db0ad4e1fb64d",
            "paperId": "5edf706467dc76cd09319592d18db0ad4e1fb64d",
            "title": "LiDAR-LLM: Exploring the Potential of Large Language Models for 3D LiDAR Understanding",
            "abstract": "Recently, Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have shown promise in instruction following and 2D image understanding. While these models are powerful, they have not yet been developed to comprehend the more challenging 3D physical scenes, especially when it comes to the sparse outdoor LiDAR data. In this paper, we introduce LiDAR-LLM, which takes raw LiDAR data as input and harnesses the remarkable reasoning capabilities of LLMs to gain a comprehensive understanding of outdoor 3D scenes. The central insight of our LiDAR-LLM is the reformulation of 3D outdoor scene cognition as a language modeling problem, encompassing tasks such as 3D captioning, 3D grounding, 3D question answering, etc. Specifically, due to the scarcity of 3D LiDAR-text pairing data, we introduce a three-stage training strategy and generate relevant datasets, progressively aligning the 3D modality with the language embedding space of LLM. Furthermore, we design a View-Aware Transformer (VAT) to connect the 3D encoder with the LLM, which effectively bridges the modality gap and enhances the LLM's spatial orientation comprehension of visual features. Our experiments show that LiDAR-LLM possesses favorable capabilities to comprehend various instructions regarding 3D scenes and engage in complex spatial reasoning. LiDAR-LLM attains a 40.9 BLEU-1 on the 3D captioning task and achieves a 63.1\\% classification accuracy and a 14.3\\% BEV mIoU on the 3D grounding task. Web page: https://sites.google.com/view/lidar-llm",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LiDAR-LLM is introduced, which takes raw LiDAR data as input and harnesses the remarkable reasoning capabilities of LLMs to gain a comprehensive understanding of outdoor 3D scenes and possesses favorable capabilities to comprehend various instructions regarding 3D scenes and engage in complex spatial reasoning."
            },
            "score": 6
        },
        {
            "id": "a3ca77456142b78367dd5d53138b50dfac8086ca",
            "paperId": "a3ca77456142b78367dd5d53138b50dfac8086ca",
            "title": "SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities",
            "abstract": "Understanding and reasoning about spatial relationships is a fundamental capability for Visual Question Answering (VQA) and robotics. While Vision Language Models (VLM) have demonstrated remarkable performance in certain VQA benchmarks, they still lack capabilities in 3D spatial reasoning, such as recognizing quantitative relationships of physical objects like distances or size differences. We hypothesize that VLMs' limited spatial reasoning capability is due to the lack of 3D spatial knowledge in training data and aim to solve this problem by training VLMs with Internet-scale spatial reasoning data. To this end, we present a system to facilitate this approach. We first develop an automatic 3D spatial VQA data generation framework that scales up to 2 billion VQA examples on 10 million real-world images. We then investigate various factors in the training recipe, including data quality, training pipeline, and VLM architecture. Our work features the first internet-scale 3D spatial reasoning dataset in metric space. By training a VLM on such data, we significantly enhance its ability on both qualitative and quantitative spatial VQA. Finally, we demonstrate that this VLM unlocks novel downstream applications in chain-of-thought spatial reasoning and robotics due to its quantitative estimation capability. Project website: https://spatial-vlm.github.io/",
            "year": 2024,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An automatic 3D spatial VQA data generation framework that scales up to 2 billion VQA examples on 10 million real-world images is developed and it is demonstrated that this VLM unlocks novel downstream applications in chain-of-thought spatial reasoning and robotics due to its quantitative estimation capability."
            },
            "score": 6
        },
        {
            "id": "56267168c5f1cea5049c0f1724e39166988d32b7",
            "paperId": "56267168c5f1cea5049c0f1724e39166988d32b7",
            "title": "Probing Physical Reasoning with Counter-Commonsense Context",
            "abstract": "In this study, we create a CConS (Counter-commonsense Contextual Size comparison) dataset to investigate how physical commonsense affects the contextualized size comparison task; the proposed dataset consists of both contexts that fit physical commonsense and those that do not.This dataset tests the ability of language models to predict the size relationship between objects under various contexts generated from our curated noun list and templates.We measure the ability of several masked language models and encoder-decoder models.The results show that while large language models can use prepositions such as \u201cin\u201d and \u201cinto\u201d in the provided context to infer size relationships, they fail to use verbs and thus make incorrect judgments led by their prior physical commonsense.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results show that while large language models can use prepositions such as \u201cin\u201d and \u201cinto\u201d in the provided context to infer size relationships, they fail to use verbs and thus make incorrect judgments led by their prior physical commonsense."
            },
            "score": 6
        },
        {
            "id": "c4b7f1ceef3f91619e80a040dbc5a9fdbd32ab22",
            "paperId": "c4b7f1ceef3f91619e80a040dbc5a9fdbd32ab22",
            "title": "Physical Reasoning and Object Planning for Household Embodied Agents",
            "abstract": "In this study, we explore the sophisticated domain of task planning for robust household embodied agents, with a particular emphasis on the intricate task of selecting substitute objects. We introduce the CommonSense Object Affordance Task (COAT), a novel framework designed to analyze reasoning capabilities in commonsense scenarios. This approach is centered on understanding how these agents can effectively identify and utilize alternative objects when executing household tasks, thereby offering insights into the complexities of practical decision-making in real-world environments.Drawing inspiration from human decision-making, we explore how large language models tackle this challenge through three meticulously crafted commonsense question-and-answer datasets, featuring refined rules and human annotations. Our evaluation of state-of-the-art language models on these datasets sheds light on three pivotal considerations: 1) aligning an object's inherent utility with the task at hand, 2) navigating contextual dependencies (societal norms, safety, appropriateness, and efficiency), and 3) accounting for the current physical state of the object. To maintain accessibility, we introduce five abstract variables reflecting an object's physical condition, modulated by human insights to simulate diverse household scenarios. Our contributions include insightful Object-Utility mappings addressing the first consideration and two extensive QA datasets (15k and 130k questions) probing the intricacies of contextual dependencies and object states. The datasets, along with our findings, are accessible at: \\url{https://github.com/com-phy-affordance/COAT}. This research not only advances our understanding of physical commonsense reasoning in language models but also paves the way for future improvements in household agent intelligence.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The CommonSense Object Affordance Task (COAT), a novel framework designed to analyze reasoning capabilities in commonsense scenarios, is introduced, thereby offering insights into the complexities of practical decision-making in real-world environments."
            },
            "score": 6
        },
        {
            "id": "4ebf49a7c053bf1d22fcce17bc8c80db827e8f99",
            "paperId": "4ebf49a7c053bf1d22fcce17bc8c80db827e8f99",
            "title": "Leveraging Commonsense Knowledge from Large Language Models for Task and Motion Planning",
            "abstract": "\u2014Multi-object rearrangement is a crucial skill for service robots, and commonsense reasoning is frequently needed in this process. However, achieving commonsense arrangements requires knowledge about objects, which is hard to transfer to robots. Large language models (LLMs) are one potential source of this knowledge, but they do not naively capture information about plausible physical arrangements of the world. We propose LLM-GROP, which uses prompting to extract commonsense knowledge about semantically valid object configurations from an LLM and instantiates them with a task and motion planner in order to generalize to varying scene geometry. LLM-GROP allows us to go from natural-language commands to human-aligned object rearrangement in varied environments. Based on human evaluations, our approach achieves the highest rating while outperforming competitive baselines in terms of success rate while maintaining comparable cumulative action costs. Finally, we demonstrate a practical implementation of LLM-GROP on a mobile manipulator in real-world scenarios. Supplementary materials are available at: https://sites.google.com/view/llm-grop",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LLM-GROP is proposed, which uses prompting to extract commonsense knowledge about semantically valid object configurations from an LLM and instantiates them with a task and motion planner in order to generalize to varying scene geometry."
            },
            "score": 6
        },
        {
            "id": "92a8f7f09f3705cb5a6009a42220a6f01ea084e8",
            "paperId": "92a8f7f09f3705cb5a6009a42220a6f01ea084e8",
            "title": "Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents",
            "abstract": "Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g.\"make breakfast\"), to a chosen set of actionable steps (e.g.\"open fridge\"). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models. Website at https://huangwl18.github.io/language-planner",
            "year": 2022,
            "citationCount": 582,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper investigates the possibility of grounding high-level tasks, expressed in natural language, to a chosen set of actionable steps and proposes a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions."
            },
            "score": 6
        },
        {
            "id": "0f82929fcfc9958d442009a7d50e6794f024b7f1",
            "paperId": "0f82929fcfc9958d442009a7d50e6794f024b7f1",
            "title": "Large Language Models as Generalizable Policies for Embodied Tasks",
            "abstract": "We show that large language models (LLMs) can be adapted to be generalizable policies for embodied visual tasks. Our approach, called Large LAnguage model Reinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take as input text instructions and visual egocentric observations and output actions directly in the environment. Using reinforcement learning, we train LLaRP to see and act solely through environmental interactions. We show that LLaRP is robust to complex paraphrasings of task instructions and can generalize to new tasks that require novel optimal behavior. In particular, on 1,000 unseen tasks it achieves 42% success rate, 1.7x the success rate of other common learned baselines or zero-shot applications of LLMs. Finally, to aid the community in studying language conditioned, massively multi-task, embodied AI problems we release a novel benchmark, Language Rearrangement, consisting of 150,000 training and 1,000 testing tasks for language-conditioned rearrangement. Video examples of LLaRP in unseen Language Rearrangement instructions are at https://llm-rl.github.io.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The approach, called Large LAnguage model Reinforcement Learning Policy (LLaRP), adapts a pre-trained frozen LLM to take as input text instructions and visual egocentric observations and output actions directly in the environment and is trained to see and act solely through environmental interactions."
            },
            "score": 6
        },
        {
            "id": "4b1278b2266ce5009e70f2efe85ccff87350de9c",
            "paperId": "4b1278b2266ce5009e70f2efe85ccff87350de9c",
            "title": "Vision-Language Models Provide Promptable Representations for Reinforcement Learning",
            "abstract": "Humans can quickly learn new behaviors by leveraging background world knowledge. In contrast, agents trained with reinforcement learning (RL) typically learn behaviors from scratch. We thus propose a novel approach that uses the vast amounts of general and indexable world knowledge encoded in vision-language models (VLMs) pre-trained on Internet-scale data for embodied RL. We initialize policies with VLMs by using them as promptable representations: embeddings that are grounded in visual observations and encode semantic features based on the VLM's internal knowledge, as elicited through prompts that provide task context and auxiliary information. We evaluate our approach on visually-complex, long horizon RL tasks in Minecraft and robot navigation in Habitat. We find that our policies trained on embeddings extracted from general-purpose VLMs outperform equivalent policies trained on generic, non-promptable image embeddings. We also find our approach outperforms instruction-following methods and performs comparably to domain-specific embeddings.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel approach that uses the vast amounts of general and indexable world knowledge encoded in vision-language models (VLMs) pre-trained on Internet-scale data for embodied RL to outperform instruction-following methods and performs comparably to domain-specific embeddings."
            },
            "score": 6
        },
        {
            "id": "b6c8c1745a18d6e59c7a8a99f0df7aa4c18a1e73",
            "paperId": "b6c8c1745a18d6e59c7a8a99f0df7aa4c18a1e73",
            "title": "Octopus: Embodied Vision-Language Programmer from Environmental Feedback",
            "abstract": "Large vision-language models (VLMs) have achieved substantial progress in multimodal perception and reasoning. Furthermore, when seamlessly integrated into an embodied agent, it signifies a crucial stride towards the creation of autonomous and context-aware systems capable of formulating plans and executing commands with precision. In this paper, we introduce Octopus, a novel VLM designed to proficiently decipher an agent's vision and textual task objectives and to formulate intricate action sequences and generate executable code. Our design allows the agent to adeptly handle a wide spectrum of tasks, ranging from mundane daily chores in simulators to sophisticated interactions in complex video games. Octopus is trained by leveraging GPT-4 to control an explorative agent to generate training data, i.e., action blueprints and the corresponding executable code, within our experimental environment called OctoVerse. We also collect the feedback that allows the enhanced training scheme of Reinforcement Learning with Environmental Feedback (RLEF). Through a series of experiments, we illuminate Octopus's functionality and present compelling results, and the proposed RLEF turns out to refine the agent's decision-making. By open-sourcing our model architecture, simulator, and dataset, we aspire to ignite further innovation and foster collaborative applications within the broader embodied AI community.",
            "year": 2023,
            "citationCount": 19,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Octopus is a novel VLM designed to proficiently decipher an agent's vision and textual task objectives and to formulate intricate action sequences and generate executable code and the proposed RLEF turns out to refine the agent's decision-making."
            },
            "score": 6
        },
        {
            "id": "bb3d4b9f6991509194cffaa132ce4f1eb52141c6",
            "paperId": "bb3d4b9f6991509194cffaa132ce4f1eb52141c6",
            "title": "Room-Object Entity Prompting and Reasoning for Embodied Referring Expression",
            "abstract": "Given a high-level instruction, the task of Embodied Referring Expression (REVERIE) requires an embodied agent to localise a remote referred object via navigating in the unseen environment. Previous vision-language navigation methods utilise the provided fine-grained instruction as step-by-step navigation guidance to conduct strict instruction-following, while REVERIE aims to achieve efficient goal-oriented exploration according to the high-level command. In this work, we propose a Cross-modal Knowledge Reasoning (abbreviated as CKR+) framework, which incorporates the prior knowledge as decision guidance to learn the navigation scheme comprehensively. Specifically, we design a Room-Object Aware (ROA) mechanism to explicitly decouple the room- and object-related clues from instruction and visual observations. Moreover, we propose a Knowledge-enabled Entity Relation Reasoning (KERR+) module to leverage the structured knowledge from the knowledge graph explicitly and unstructured knowledge from pre-trained model implicitly, to learn the internal-external correlations among room- and object-entities for the agent to make proper decisions. We devise an Entity Prompter (EP) that embeds in the KERR+ module, which utilises the navigation history and visual entities as prompts to transfer knowledge from the pre-trained CLIP model. In addition, we develop a Reinforced End Decider (RED) to learn the stopping scheme specifically, which is achieved by a customised reinforcement learning strategy and knowledge enhanced matching. Two techniques are also introduced to improve navigation efficiency further. Extensive experiments conducted on the REVERIE benchmark demonstrate the effectiveness and superiority of our proposed methods, which boosts the key metrics, i.e., SPL and REVERIE-success rate, to 14.46% and 13.81% respectively.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a Cross-modal Knowledge Reasoning (abbreviated as CKR+) framework, which incorporates the prior knowledge as decision guidance to learn the navigation scheme comprehensively and designs a Room-Object Aware mechanism to explicitly decouple the room- and object-related clues from instruction and visual observations."
            },
            "score": 6
        },
        {
            "id": "12c826f4195da172b212a529f8fcf10cc79e35da",
            "paperId": "12c826f4195da172b212a529f8fcf10cc79e35da",
            "title": "Context-faithful Prompting for Large Language Models",
            "abstract": "Large language models (LLMs) encode parametric knowledge about world facts and have shown remarkable performance in knowledge-driven NLP tasks. However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks). In this paper, we seek to assess and enhance LLMs' contextual faithfulness in two aspects: knowledge conflict and prediction with abstention. We demonstrate that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies. In particular, we identify opinion-based prompts and counterfactual demonstrations as the most effective methods. Opinion-based prompts reframe the context as a narrator's statement and inquire about the narrator's opinions, while counterfactual demonstrations use instances containing false facts to improve faithfulness in knowledge conflict situations. Neither technique requires additional training. We conduct experiments on three datasets of two standard NLP tasks, machine reading comprehension and relation extraction, and the results demonstrate significant improvement in faithfulness to contexts. Code and data are released at https://github.com/wzhouad/context-faithful-llm.",
            "year": 2023,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies, and opinion-based prompts and counterfactual demonstrations are identified as the most effective methods."
            },
            "score": 6
        },
        {
            "id": "755853c6b30f5a186131e23a63c68a3f2737068e",
            "paperId": "755853c6b30f5a186131e23a63c68a3f2737068e",
            "title": "SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models",
            "abstract": "In this work, we introduce SMART-LLM, an innovative framework designed for embodied multi-robot task planning. SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models (LLMs), harnesses the power of LLMs to convert high-level task instructions provided as input into a multi-robot task plan. It accomplishes this by executing a series of stages, including task decomposition, coalition formation, and task allocation, all guided by programmatic LLM prompts within the few-shot prompting paradigm. We create a benchmark dataset designed for validating the multi-robot task planning problem, encompassing four distinct categories of high-level instructions that vary in task complexity. Our evaluation experiments span both simulation and real-world scenarios, demonstrating that the proposed model can achieve promising results for generating multi-robot task plans. The experimental videos, code, and datasets from the work can be found at https://sites.google.com/view/smart-llm/.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models (LLMs), harnesses the power of LLMs to convert high-level task instructions provided as input into a multi-robot task plan by executing a series of stages."
            },
            "score": 5
        },
        {
            "id": "c82d8d80ea68400adb7faebb2f1cff38dd83093a",
            "paperId": "c82d8d80ea68400adb7faebb2f1cff38dd83093a",
            "title": "CAPE: Corrective Actions from Precondition Errors using Large Language Models",
            "abstract": "Extracting commonsense knowledge from a large language model (LLM) offers a path to designing intelligent robots. Existing approaches that leverage LLMs for planning are unable to recover when an action fails and often resort to retrying failed actions, without resolving the error's underlying cause. We propose a novel approach (CAPE) that attempts to propose corrective actions to resolve precondition errors during planning. CAPE improves the quality of generated plans by leveraging few-shot reasoning from action preconditions. Our approach enables embodied agents to execute more tasks than baseline methods while ensuring semantic correctness and minimizing re-prompting. In VirtualHome, CAPE generates executable plans while improving a human-annotated plan correctness metric from 28.89% to 49.63% over SayCan. Our improvements transfer to a Boston Dynamics Spot robot initialized with a set of skills (specified in language) and associated preconditions, where CAPE improves the correctness metric of the executed task plans by 76.49% compared to SayCan. Our approach enables the robot to follow natural language commands and robustly recover from failures, which baseline approaches largely cannot resolve or address inefficiently.",
            "year": 2022,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel approach (CAPE) that attempts to propose corrective actions to resolve precondition errors during planning and improves the quality of generated plans by leveraging few-shot reasoning from action preconditions."
            },
            "score": 5
        },
        {
            "id": "b908824639d18f11883abcab21efeb22e315ab9c",
            "paperId": "b908824639d18f11883abcab21efeb22e315ab9c",
            "title": "Multimodal Procedural Planning via Dual Text-Image Prompting",
            "abstract": "Embodied agents have achieved prominent performance in following human instructions to complete tasks. However, the potential of providing instructions informed by texts and images to assist humans in completing tasks remains underexplored. To uncover this capability, we present the multimodal procedural planning (MPP) task, in which models are given a high-level goal and generate plans of paired text-image steps, providing more complementary and informative guidance than unimodal plans. The key challenges of MPP are to ensure the informativeness, temporal coherence,and accuracy of plans across modalities. To tackle this, we propose Text-Image Prompting (TIP), a dual-modality prompting method that jointly leverages zero-shot reasoning ability in large language models (LLMs) and compelling text-to-image generation ability from diffusion-based models. TIP improves the interaction in the dual modalities using Text-to-Image Bridge and Image-to-Text Bridge, allowing LLMs to guide the textual-grounded image plan generation and leveraging the descriptions of image plans to ground the textual plan reversely. To address the lack of relevant datasets, we collect WIKIPLAN and RECIPEPLAN as a testbed for MPP. Our results show compelling human preferences and automatic scores against unimodal and multimodal baselines on WIKIPLAN and RECIPEPLAN in terms of informativeness, temporal coherence, and plan accuracy. Our code and data: https://github.com/YujieLu10/MPP.",
            "year": 2023,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Text-Image Prompting (TIP) is proposed, a dual-modality prompting method that jointly leverages zero-shot reasoning ability in large language models (LLMs) and compelling text-to-image generation ability from diffusion-based models to tackle the key challenges of MPP."
            },
            "score": 5
        },
        {
            "id": "961a1772f3b90d9dffd2b571c6996007a1d0ccd1",
            "paperId": "961a1772f3b90d9dffd2b571c6996007a1d0ccd1",
            "title": "JARVIS: A Neuro-Symbolic Commonsense Reasoning Framework for Conversational Embodied Agents",
            "abstract": "Building a conversational embodied agent to execute real-life tasks has been a long-standing yet quite challenging research goal, as it requires effective human-agent communication, multi-modal understanding, long-range sequential decision making, etc. Traditional symbolic methods have scaling and generalization issues, while end-to-end deep learning models suffer from data scarcity and high task complexity, and are often hard to explain. To benefit from both worlds, we propose JARVIS, a neuro-symbolic commonsense reasoning framework for modular, generalizable, and interpretable conversational embodied agents. First, it acquires symbolic representations by prompting large language models (LLMs) for language understanding and sub-goal planning, and by constructing semantic maps from visual observations. Then the symbolic module reasons for sub-goal planning and action generation based on task- and action-level common sense. Extensive experiments on the TEACh dataset validate the efficacy and efficiency of our JARVIS framework, which achieves state-of-the-art (SOTA) results on all three dialog-based embodied tasks, including Execution from Dialog History (EDH), Trajectory from Dialog (TfD), and Two-Agent Task Completion (TATC) (e.g., our method boosts the unseen Success Rate on EDH from 6.1\\% to 15.8\\%). Moreover, we systematically analyze the essential factors that affect the task performance and also demonstrate the superiority of our method in few-shot settings. Our JARVIS model ranks first in the Alexa Prize SimBot Public Benchmark Challenge.",
            "year": 2022,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "JARVIS, a neuro-symbolic commonsense reasoning framework for modular, generalizable, and interpretable conversational embodied agents, is proposed, which achieves state-of-the-art (SOTA) results on all three dialog-based embodied tasks."
            },
            "score": 5
        },
        {
            "id": "1cd8373490efc2d74c2796f4b2aa27c7d4415ec9",
            "paperId": "1cd8373490efc2d74c2796f4b2aa27c7d4415ec9",
            "title": "VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models",
            "abstract": "Large language models (LLMs) are shown to possess a wealth of actionable knowledge that can be extracted for robot manipulation in the form of reasoning and planning. Despite the progress, most still rely on pre-defined motion primitives to carry out the physical interactions with the environment, which remains a major bottleneck. In this work, we aim to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open-set of instructions and an open-set of objects. We achieve this by first observing that LLMs excel at inferring affordances and constraints given a free-form language instruction. More importantly, by leveraging their code-writing capabilities, they can interact with a vision-language model (VLM) to compose 3D value maps to ground the knowledge into the observation space of the agent. The composed value maps are then used in a model-based planning framework to zero-shot synthesize closed-loop robot trajectories with robustness to dynamic perturbations. We further demonstrate how the proposed framework can benefit from online experiences by efficiently learning a dynamics model for scenes that involve contact-rich interactions. We present a large-scale study of the proposed method in both simulated and real-robot environments, showcasing the ability to perform a large variety of everyday manipulation tasks specified in free-form natural language. Videos and code at https://voxposer.github.io",
            "year": 2023,
            "citationCount": 156,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A large-scale study of the proposed framework to synthesize closed-loop robot trajectories with robustness to dynamic perturbations is presented, showcasing the ability to perform a large variety of everyday manipulation tasks specified in free-form natural language."
            },
            "score": 5
        },
        {
            "id": "748a2700ec11f51560a69ec05c67ca9f97014be7",
            "paperId": "748a2700ec11f51560a69ec05c67ca9f97014be7",
            "title": "EvEntS ReaLM: Event Reasoning of Entity States via Language Models",
            "abstract": "This paper investigates models of event implications. Specifically, how well models predict entity state-changes, by targeting their understanding of physical attributes. Nominally, Large Language models (LLM) have been exposed to procedural knowledge about how objects interact, yet our benchmarking shows they fail to reason about the world. Conversely, we also demonstrate that existing approaches often misrepresent the surprising abilities of LLMs via improper task encodings and that proper model prompting can dramatically improve performance of reported baseline results across multiple tasks. In particular, our results indicate that our prompting technique is especially useful for unseen attributes (out-of-domain) or when only limited data is available.",
            "year": 2022,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results indicate that the prompting technique is especially useful for unseen attributes (out-of-domain) or when only limited data is available and that proper model prompting can dramatically improve performance of reported baseline results across multiple tasks."
            },
            "score": 5
        },
        {
            "id": "5dc1611ce19a1f6a8562da7455a5370fde148d1b",
            "paperId": "5dc1611ce19a1f6a8562da7455a5370fde148d1b",
            "title": "Hybrid Reasoning Based on Large Language Models for Autonomous Car Driving",
            "abstract": "Large Language Models (LLMs) have garnered significant attention for their ability to understand text and images, generate human-like text, and perform complex reasoning tasks. However, their ability to generalize this advanced reasoning with a combination of natural language text for decision-making in dynamic situations requires further exploration. In this study, we investigate how well LLMs can adapt and apply a combination of arithmetic and common-sense reasoning, particularly in autonomous driving scenarios. We hypothesize that LLMs hybrid reasoning abilities can improve autonomous driving by enabling them to analyze detected object and sensor data, understand driving regulations and physical laws, and offer additional context. This addresses complex scenarios, like decisions in low visibility (due to weather conditions), where traditional methods might fall short. We evaluated Large Language Models (LLMs) based on accuracy by comparing their answers with human-generated ground truth inside CARLA. The results showed that when a combination of images (detected objects) and sensor data is fed into the LLM, it can offer precise information for brake and throttle control in autonomous vehicles across various weather conditions. This formulation and answers can assist in decision-making for auto-pilot systems.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results showed that when a combination of images and sensor data is fed into the LLM, it can offer precise information for brake and throttle control in autonomous vehicles across various weather conditions, and can assist in decision-making for auto-pilot systems."
            },
            "score": 5
        },
        {
            "id": "6da2b478ea9b6a4c193c2830cf9e99beb5d826a5",
            "paperId": "6da2b478ea9b6a4c193c2830cf9e99beb5d826a5",
            "title": "Vision-Language Model-based Physical Reasoning for Robot Liquid Perception",
            "abstract": "There is a growing interest in applying large language models (LLMs) in robotic tasks, due to their remarkable reasoning ability and extensive knowledge learned from vast training corpora. Grounding LLMs in the physical world remains an open challenge as they can only process textual input. Recent advancements in large vision-language models (LVLMs) have enabled a more comprehensive understanding of the physical world by incorporating visual input, which provides richer contextual information than language alone. In this work, we proposed a novel paradigm that leveraged GPT-4V(ision), the state-of-the-art LVLM by OpenAI, to enable embodied agents to perceive liquid objects via image-based environmental feedback. Specifically, we exploited the physical understanding of GPT-4V to interpret the visual representation (e.g., time-series plot) of non-visual feedback (e.g., F/T sensor data), indirectly enabling multimodal perception beyond vision and language using images as proxies. We evaluated our method using 10 common household liquids with containers of various geometry and material. Without any training or fine-tuning, we demonstrated that our method can enable the robot to indirectly perceive the physical response of liquids and estimate their viscosity. We also showed that by jointly reasoning over the visual and physical attributes learned through interactions, our method could recognize liquid objects in the absence of strong visual cues (e.g., container labels with legible text or symbols), increasing the accuracy from 69.0% -- achieved by the best-performing vision-only variant -- to 86.0%.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposed a novel paradigm that leveraged GPT-4V(ision), the state-of-the-art LVLM by OpenAI, to enable embodied agents to perceive liquid objects via image-based environmental feedback to enable multimodal perception beyond vision and language using images as proxies."
            },
            "score": 5
        },
        {
            "id": "102e4c860e39a2bfd7bf3f03b9ad69aac7bf3b5f",
            "paperId": "102e4c860e39a2bfd7bf3f03b9ad69aac7bf3b5f",
            "title": "Collaborating with language models for embodied reasoning",
            "abstract": "Reasoning in a complex and ambiguous environment is a key goal for Reinforcement Learning (RL) agents. While some sophisticated RL agents can successfully solve difficult tasks, they require a large amount of training data and often struggle to generalize to new unseen environments and new tasks. On the other hand, Large Scale Language Models (LSLMs) have exhibited strong reasoning ability and the ability to to adapt to new tasks through in-context learning. However, LSLMs do not inherently have the ability to interrogate or intervene on the environment. In this work, we investigate how to combine these complementary abilities in a single system consisting of three parts: a Planner, an Actor, and a Reporter. The Planner is a pre-trained language model that can issue commands to a simple embodied agent (the Actor), while the Reporter communicates with the Planner to inform its next command. We present a set of tasks that require reasoning, test this system's ability to generalize zero-shot and investigate failure cases, and demonstrate how components of this system can be trained with reinforcement-learning to improve performance.",
            "year": 2023,
            "citationCount": 40,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates how to combine complementary abilities in a single system consisting of a Planner, an Actor, and a Reporter, and presents a set of tasks that require reasoning, test this system's ability to generalize zero-shot and investigate failure cases, and demonstrates how components of this system can be trained with reinforcement-learning to improve performance."
            },
            "score": 5
        },
        {
            "id": "38fe8f324d2162e63a967a9ac6648974fc4c66f3",
            "paperId": "38fe8f324d2162e63a967a9ac6648974fc4c66f3",
            "title": "PaLM-E: An Embodied Multimodal Language Model",
            "abstract": "Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.",
            "year": 2023,
            "citationCount": 746,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts to enable general inference in the real world."
            },
            "score": 5
        },
        {
            "id": "eba383558d36e6cacb23b3f477a2125c4b7d5043",
            "paperId": "eba383558d36e6cacb23b3f477a2125c4b7d5043",
            "title": "From Words to Blocks: Building Objects by Grounding Language Models with Reinforcement Learning",
            "abstract": "Leveraging pre-trained language models to gen-001 erate action plans for embodied agents is an 002 emerging research direction. However, exe-003 cuting instructions in real or simulated envi-004 ronments necessitates verifying the feasibility 005 of actions and their relevance in achieving a 006 goal. We introduce a novel method that in-007 tegrates a language model and reinforcement 008 learning for constructing objects in a Minecraft-009 like environment, based on natural language 010 instructions. Our method generates a set of 011 consistently achievable sub-goals derived from 012 the instructions and subsequently completes the 013 associated sub-tasks using a pre-trained RL pol-014 icy. We employ the IGLU competition, which 015 is based on the Minecraft-like simulator, as our 016 test environment, and compare our approach 017 to the competition\u2019s top-performing solutions. 018 Our approach outperforms existing solutions in 019 terms of both the quality of the language model 020 and the quality of the structures built within the 021 IGLU environment. 022",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel method that tegrates a language model and reinforcement learning for constructing objects in a Minecraft-009 like environment, based on natural language 010 instructions and outperforms existing solutions in terms of both the quality of the language model 020 and the quality of the structures built within the IGLU environment."
            },
            "score": 5
        },
        {
            "id": "82da02137bae421a3f7a89c3bf2ab662037f4dfa",
            "paperId": "82da02137bae421a3f7a89c3bf2ab662037f4dfa",
            "title": "Embodied Executable Policy Learning with Language-based Scene Summarization",
            "abstract": "Large Language models (LLMs) have shown remarkable success in assisting robot learning tasks, i.e., complex household planning. However, the performance of pretrained LLMs heavily relies on domain-specific templated text data, which may be infeasible in real-world robot learning tasks with image-based observations. Moreover, existing LLMs with text inputs lack the capability to evolve with non-expert interactions with environments. In this work, we introduce a novel learning paradigm that generates robots' executable actions in the form of text, derived solely from visual observations, using language-based summarization of these observations as the connecting bridge between both domains. Our proposed paradigm stands apart from previous works, which utilized either language instructions or a combination of language and visual data as inputs. Moreover, our method does not require oracle text summarization of the scene, eliminating the need for human involvement in the learning loop, which makes it more practical for real-world robot learning tasks. Our proposed paradigm consists of two modules: the SUM module, which interprets the environment using visual observations and produces a text summary of the scene, and the APM module, which generates executable action policies based on the natural language descriptions provided by the SUM module. We demonstrate that our proposed method can employ two fine-tuning strategies, including imitation learning and reinforcement learning approaches, to adapt to the target test tasks effectively. We conduct extensive experiments involving various SUM/APM model selections, environments, and tasks across 7 house layouts in the VirtualHome environment. Our experimental results demonstrate that our method surpasses existing baselines, confirming the effectiveness of this novel learning paradigm.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a novel learning paradigm that generates robots' executable actions in the form of text, derived solely from visual observations, using language-based summarization of these observations as the connecting bridge between both domains."
            },
            "score": 5
        },
        {
            "id": "62176de125738e3b95850d1227bac81fd646b78e",
            "paperId": "62176de125738e3b95850d1227bac81fd646b78e",
            "title": "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models",
            "abstract": "Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, Few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual efforts, Zero-shot-CoT concatenates the target problem statement with \u201cLet\u2019s think step by step\u201d as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.",
            "year": 2023,
            "citationCount": 115,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results over GPT-3 show that the proposed zero-shot prompting consistently outperforms Zero- shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem."
            },
            "score": 5
        },
        {
            "id": "dfab0f3ee6f47e36cccee145794cd117773e6f73",
            "paperId": "dfab0f3ee6f47e36cccee145794cd117773e6f73",
            "title": "Towards LLM-based Fact Verification on News Claims with a Hierarchical Step-by-Step Prompting Method",
            "abstract": "While large pre-trained language models (LLMs) have shown their impressive capabilities in various NLP tasks, they are still under-explored in the misinformation domain. In this paper, we examine LLMs with in-context learning (ICL) for news claim verification, and find that only with 4-shot demonstration examples, the performance of several prompting methods can be comparable with previous supervised models. To further boost performance, we introduce a Hierarchical Step-by-Step (HiSS) prompting method which directs LLMs to separate a claim into several subclaims and then verify each of them via multiple questions-answering steps progressively. Experiment results on two public misinformation datasets show that HiSS prompting outperforms state-of-the-art fully-supervised approach and strong few-shot ICL-enabled baselines.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Hierarchical Step-by-Step (HiSS) prompting method is introduced which directs LLMs to separate a claim into several subclaims and then verify each of them via multiple questions-answering steps progressively."
            },
            "score": 5
        },
        {
            "id": "1caa2a29d3ca38d0e5111f4f9ae140727bb7d567",
            "paperId": "1caa2a29d3ca38d0e5111f4f9ae140727bb7d567",
            "title": "Can Language Models Understand Physical Concepts?",
            "abstract": "Language models~(LMs) gradually become general-purpose interfaces in the interactive and embodied world, where the understanding of physical concepts is an essential prerequisite. However, it is not yet clear whether LMs can understand physical concepts in the human world. To investigate this, we design a benchmark VEC that covers the tasks of (i) Visual concepts, such as the shape and material of objects, and (ii) Embodied Concepts, learned from the interaction with the world such as the temperature of objects. Our zero (few)-shot prompting results show that the understanding of certain visual concepts emerges as scaling up LMs, but there are still basic concepts to which the scaling law does not apply. For example, OPT-175B performs close to humans with a zero-shot accuracy of 85\\% on the material concept, yet behaves like random guessing on the mass concept. Instead, vision-augmented LMs such as CLIP and BLIP achieve a human-level understanding of embodied concepts. Analysis indicates that the rich semantics in visual representation can serve as a valuable source of embodied knowledge. Inspired by this, we propose a distillation method to transfer embodied knowledge from VLMs to LMs, achieving performance gain comparable with that by scaling up the parameters of LMs 134x. Our dataset is available at \\url{https://github.com/TobiasLee/VEC}",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A distillation method is proposed to transfer embodied knowledge from VLMs to LMs, achieving performance gain comparable with that by scaling up the parameters of LMs 134x, and indicates that the rich semantics in visual representation can serve as a valuable source of embodied knowledge."
            },
            "score": 4
        },
        {
            "id": "412fe1f135cb20c952962133ca1e534a71bfd27f",
            "paperId": "412fe1f135cb20c952962133ca1e534a71bfd27f",
            "title": "When Do Program-of-Thoughts Work for Reasoning?",
            "abstract": "In the realm of embodied artificial intelligence, the reasoning capabilities of Large Language Models (LLMs) play a pivotal role. Although there are effective methods like program-of-thought prompting for LLMs which uses programming language to tackle complex reasoning tasks, the specific impact of code data on the improvement of reasoning capabilities remains under-explored. To address this gap, we propose complexity-impacted reasoning score CIRS, which combines structural and logical attributes, to measure the correlation between code and reasoning abilities. Specifically, we use the abstract syntax tree to encode the structural information and calculate logical complexity by considering the difficulty and the cyclomatic complexity. Through an empirical analysis, we find not all code data of complexity can be learned or understood by LLMs. Optimal level of complexity is critical to the improvement of reasoning abilities by program-aided prompting. Then we design an auto-synthesizing and stratifying algorithm, and apply it to instruction generation for mathematical reasoning and code data filtering for code generation tasks. Extensive results demonstrates the effectiveness of our proposed approach.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes complexity-impacted reasoning score CIRS, which combines structural and logical attributes, to measure the correlation between code and reasoning abilities, and designs an auto-synthesizing and stratifying algorithm that applies to instruction generation for mathematical reasoning and code data filtering for code generation tasks."
            },
            "score": 4
        },
        {
            "id": "0f01b5e98b60b1f47a88e7f0e06735fab466cff5",
            "paperId": "0f01b5e98b60b1f47a88e7f0e06735fab466cff5",
            "title": "How People Prompt to Create Interactive VR Scenes",
            "abstract": "Generative AI tools can provide people with the ability to create virtual environments and scenes with natural language prompts. Yet, how people will formulate such prompts is unclear -- particularly when they inhabit the environment that they are designing. For instance, it is likely that a person might say,\"Put a chair here\", while pointing at a location. If such linguistic features are common to people's prompts, we need to tune models to accommodate them. In this work, we present a wizard-of-oz elicitation study with 22 participants, where we studied people's implicit expectations when verbally prompting such programming agents to create interactive VR scenes. Our findings show that people prompt with several implicit expectations: (1) that agents have an embodied knowledge of the environment; (2) that agents understand embodied prompts by users; (3) that the agents can recall previous states of the scene and the conversation, and that (4) agents have a commonsense understanding of objects in the scene. Further, we found that participants prompt differently when they are prompting in situ (i.e. within the VR environment) versus ex situ (i.e. viewing the VR environment from the outside). To explore how our could be applied, we designed and built Oastaad, a conversational programming agent that allows non-programmers to design interactive VR experiences that they inhabit. Based on these explorations, we outline new opportunities and challenges for conversational programming agents that create VR environments.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Oastaad, a conversational programming agent that allows non-programmers to design interactive VR experiences that they inhabit, is designed and built and outlined new opportunities and challenges for conversational programming agents that create VR environments."
            },
            "score": 4
        },
        {
            "id": "41a41c75ba336dec98d58c563605f261019e5df0",
            "paperId": "41a41c75ba336dec98d58c563605f261019e5df0",
            "title": "\u201cAccording to . . . \u201d: Prompting Language Models Improves Quoting from Pre-Training Data",
            "abstract": "Large Language Models (LLMs) may hallucinate and generate fake information, despite pre-training on factual data. Inspired by the journalistic device of \u201caccording to sources\u201d, we propose according-to prompting: directing LLMs to ground responses against previously observed text. To quantify this grounding, we propose a novel evaluation metric (QUIP-Score) that measures the extent to which model-produced answers are directly found in underlying text corpora. We illustrate with experiments on three corpora (Wikipedia, PubMed, and the U.S. legal tax code) that these prompts improve grounding under our metrics, with the additional benefit of often improving end-task performance. Furthermore, prompts that ask the model to decrease grounding (or to ground to other corpora) indeed decrease QUIP-Score, indicating the ability of LLMs to increase or decrease grounded generations on request.",
            "year": 2023,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "According-to prompting is proposed: directing LLMs to ground responses against previously observed text, to quantify this grounding, and proposes a novel evaluation metric (QUIP-Score) that measures the extent to which model-produced answers are directly found in underlying text corpora."
            },
            "score": 4
        },
        {
            "id": "0a1e0c479e13e7553ac79ddd6780fbf8e8e82165",
            "paperId": "0a1e0c479e13e7553ac79ddd6780fbf8e8e82165",
            "title": "Benchmarking Sequential Visual Input Reasoning and Prediction in Multimodal Large Language Models",
            "abstract": "Multimodal large language models (MLLMs) have shown great potential in perception and interpretation tasks, but their capabilities in predictive reasoning remain under-explored. To address this gap, we introduce a novel benchmark that assesses the predictive reasoning capabilities of MLLMs across diverse scenarios. Our benchmark targets three important domains: abstract pattern reasoning, human activity prediction, and physical interaction prediction. We further develop three evaluation methods powered by large language model to robustly quantify a model's performance in predicting and reasoning the future based on multi-visual context. Empirical experiments confirm the soundness of the proposed benchmark and evaluation methods via rigorous testing and reveal pros and cons of current popular MLLMs in the task of predictive reasoning. Lastly, our proposed benchmark provides a standardized evaluation framework for MLLMs and can facilitate the development of more advanced models that can reason and predict over complex long sequence of multimodal input.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel benchmark that assesses the predictive reasoning capabilities of MLLMs across diverse scenarios and develops three evaluation methods to robustly quantify a model's performance in predicting and reasoning the future based on multi-visual context."
            },
            "score": 4
        },
        {
            "id": "455866ca838f356b53a7e3e5b344834f9e93dbbc",
            "paperId": "455866ca838f356b53a7e3e5b344834f9e93dbbc",
            "title": "ToolAlpaca: Generalized Tool Learning for Language Models with 3000 Simulated Cases",
            "abstract": "Enabling large language models to utilize real-world tools effectively is crucial for achieving embodied intelligence. Existing approaches to tool learning have either primarily relied on extremely large language models, such as GPT-4, to attain generalized tool-use abilities in a zero-shot manner, or utilized supervised learning to train limited scopes of tools on compact models. However, it remains uncertain whether smaller language models can achieve generalized tool-use abilities without tool-specific training. To address this question, this paper introduces ToolAlpaca, a novel framework designed to automatically generate a diverse tool-use corpus and learn generalized tool-use abilities on compact language models with minimal human intervention. Specifically, ToolAlpaca first automatically creates a highly diversified tool-use corpus by building a multi-agent simulation environment. The corpus contains 3938 tool-use instances from more than 400 real-world tool APIs spanning 50 distinct categories. Subsequently, the constructed corpus is employed to fine-tune compact language models, resulting in two models, namely ToolAlpaca-7B and ToolAlpaca-13B, respectively. Finally, we evaluate the ability of these models to utilize previously unseen tools without specific training. Experimental results demonstrate that ToolAlpaca achieves effective generalized tool-use capabilities comparable to those of extremely large language models like GPT-3.5, demonstrating that learning generalized tool-use ability is feasible for compact language models.",
            "year": 2023,
            "citationCount": 66,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results demonstrate that ToolAlpaca achieves effective generalized tool-use capabilities comparable to those of extremely large language models like GPT-3.5, demonstrating that learning generalizedtool-use ability is feasible for compact language models."
            },
            "score": 4
        },
        {
            "id": "04f87baf7d1b3eb303a52a8a66c8189f396dd114",
            "paperId": "04f87baf7d1b3eb303a52a8a66c8189f396dd114",
            "title": "Application of Pretrained Large Language Models in Embodied Artificial Intelligence",
            "abstract": null,
            "year": 2022,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A detailed review of models based on the ability of large language models trained with a large amount of unlabeled data to store common sense knowledge as applied to embodied artificial intelligence tasks is provided."
            },
            "score": 4
        },
        {
            "id": "93dca907ff738c4eb5df8d53ee5b8414e84a9048",
            "paperId": "93dca907ff738c4eb5df8d53ee5b8414e84a9048",
            "title": "Language-Informed Transfer Learning for Embodied Household Activities",
            "abstract": "For service robots to become general-purpose in everyday household environments, they need not only a large library of primitive skills, but also the ability to quickly learn novel tasks specified by users. Fine-tuning neural networks on a variety of downstream tasks has been successful in many vision and language domains, but research is still limited on transfer learning between diverse long-horizon tasks. We propose that, compared to reinforcement learning for a new household activity from scratch, home robots can benefit from transferring the value and policy networks trained for similar tasks. We evaluate this idea in the BEHAVIOR simulation benchmark which includes a large number of household activities and a set of action primitives. For easy mapping between state spaces of different tasks, we provide a text-based representation and leverage language models to produce a common embedding space. The results show that the selection of similar source activities can be informed by the semantic similarity of state and goal descriptions with the target task. We further analyze the results and discuss ways to overcome the problem of catastrophic forgetting.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes that, compared to reinforcement learning for a new household activity from scratch, home robots can benefit from transferring the value and policy networks trained for similar tasks, and provides a text-based representation and language models to produce a common embedding space."
            },
            "score": 4
        },
        {
            "id": "3266d207174bf833f43ffbdef677604def7162ac",
            "paperId": "3266d207174bf833f43ffbdef677604def7162ac",
            "title": "LLM A*: Human in the Loop Large Language Models Enabled A* Search for Robotics",
            "abstract": "This research focuses on how Large Language Models (LLMs) can help with path planning for mobile embodied agents such as robots, in a human-in-the-loop and interactive manner. A novel framework named LLM A*, aims to leverage the commonsense of LLMs, and the utility-optimal A* is proposed to facilitate few-shot near-optimal path planning. Prompts are used to 1) provide LLMs with essential information like environment, cost, heuristics, etc.; 2) communicate human feedback to LLMs on intermediate planning results. This makes the whole path planning process a `white box' and human feedback guides LLM A* to converge quickly compared to other data-driven methods such as reinforcement learning-based (RL) path planning. In addition, it makes code-free path planning practical, henceforth promoting the inclusiveness of artificial intelligence techniques. Comparative analysis against A* and RL shows that LLM A* is more efficient in terms of search space and achieves an on-a-par path with A* and a better path than RL. The interactive nature of LLM A* also makes it a promising tool for deployment in collaborative human-robot tasks.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Comparative analysis against A* and RL shows that LLM A* is more efficient in terms of search space and achieves an on-a-par path with A*, and a better path than RL, and makes code-free path planning practical, henceforth promoting the inclusiveness of artificial intelligence techniques."
            },
            "score": 4
        },
        {
            "id": "e10c2ed8de5568e6a41fb60c2adc4bf9c225da25",
            "paperId": "e10c2ed8de5568e6a41fb60c2adc4bf9c225da25",
            "title": "Is EVALITA Done? On the Impact of Prompting on the Italian NLP Evaluation Campaign",
            "abstract": "Prompt-based learning is a recent paradigm in NLP that leverages large pre-trained language models to perform a variety of tasks. With this technique, it is possible to build classifiers that do not need training data (zero-shot). In this paper, we assess the status of prompt-based learning applied to several text classification tasks in the Italian language. The results indicate that the performance gap towards current supervised methods is still relevant. However, the difference in performance between pre-trained models and the characteristic of the prompt-based classifier of operating in a zero-shot fashion open a discussion regarding the next generation of evaluation campaigns for NLP.",
            "year": 2022,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results indicate that the performance gap towards current supervised methods is still relevant, however, the difference in performance between pre-trained models and the characteristic of the prompt-based classifier of operating in a zero-shot fashion open a discussion regarding the next generation of evaluation campaigns for NLP."
            },
            "score": 4
        },
        {
            "id": "8bc313e04cbd39847eb50b22af0a698ff2971a35",
            "paperId": "8bc313e04cbd39847eb50b22af0a698ff2971a35",
            "title": "Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models: A Case Study on ChatGPT",
            "abstract": "Generative large language models (LLMs), e.g., ChatGPT, have demonstrated remarkable proficiency across several NLP tasks, such as machine translation, text summarization. Recent research (Kocmi and Federmann, 2023) has shown that utilizing LLMs for assessing the quality of machine translation (MT) achieves state-of-the-art performance at the system level but \\textit{performs poorly at the segment level}. To further improve the performance of LLMs on MT quality assessment, we investigate several prompting designs, and propose a new prompting method called \\textbf{\\texttt{Error Analysis Prompting}} (EAPrompt) by combining Chain-of-Thoughts (Wei et al., 2022) and Error Analysis (Lu et al., 2023). This technique emulates the commonly accepted human evaluation framework - Multidimensional Quality Metrics (MQM, Freitag et al. (2021)) and \\textit{produces explainable and reliable MT evaluations at both the system and segment level}. Experimental Results from the WMT22 metrics shared task validate the effectiveness of EAPrompt on various LLMs, with different structures. Further analysis confirms that EAPrompt effectively distinguishes major errors from minor ones, while also sharing a similar distribution of the number of errors with MQM. These findings highlight the potential of EAPrompt as a human-like evaluator prompting technique for MT evaluation.",
            "year": 2023,
            "citationCount": 59,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Findings highlight the potential of EAPrompt as a human-like evaluator prompting technique for MT evaluation, and investigate several prompting designs, and propose a new prompting method called EAPrompt by combining Chain-of-Thoughts and Error Analysis."
            },
            "score": 4
        },
        {
            "id": "80c698688bb4488beaceaab5c64f701a946cb7ae",
            "paperId": "80c698688bb4488beaceaab5c64f701a946cb7ae",
            "title": "All in One: Multi-Task Prompting for Graph Neural Networks",
            "abstract": "Recently, \"pre-training and fine-tuning'' has been adopted as a standard workflow for many graph tasks since it can take general graph knowledge to relieve the lack of graph annotations from each application. However, graph tasks with node level, edge level, and graph level are far diversified, making the pre-training pretext often incompatible with these multiple tasks. This gap may even cause a \"negative transfer'' to the specific application, leading to poor results. Inspired by the prompt learning in natural language processing (NLP), which has presented significant effectiveness in leveraging prior knowledge for various NLP tasks, we study the prompting topic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models. Specifically, we first unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern. In this way, the prompting idea from NLP can be seamlessly introduced to the graph area. Then, to further narrow the gap between various graph tasks and state-of-the-art pre-training strategies, we further study the task space of various graph applications and reformulate downstream problems to the graph-level task. Afterward, we introduce meta-learning to efficiently learn a better initialization for the multi-task prompt of graphs so that our prompting framework can be more reliable and general for different tasks. We conduct extensive experiments, results from which demonstrate the superiority of our method.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel multi-task prompting method for graph models that unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern, and introduces meta-learning to efficiently learn a better initialization for the multi- task prompt of graphs so that the prompting framework can be more reliable and general for different tasks."
            },
            "score": 4
        },
        {
            "id": "9141480721653789597b6e537ee0eeab401f3e60",
            "paperId": "9141480721653789597b6e537ee0eeab401f3e60",
            "title": "PromptNER: Prompting For Named Entity Recognition",
            "abstract": "In a surprising turn, Large Language Models (LLMs) together with a growing arsenal of prompt-based heuristics now offer powerful off-the-shelf approaches providing few-shot solutions to myriad classic NLP problems. However, despite promising early results, these LLM-based few-shot methods remain far from the state of the art in Named Entity Recognition (NER), where prevailing methods include learning representations via end-to-end structural understanding and fine-tuning on standard labeled corpora. In this paper, we introduce PromptNER, a new state-of-the-art algorithm for few-Shot and cross-domain NER. To adapt to any new NER task PromptNER requires a set of entity definitions in addition to the standard few-shot examples. Given a sentence, PromptNER prompts an LLM to produce a list of potential entities along with corresponding explanations justifying their compatibility with the provided entity type definitions. Remarkably, PromptNER achieves state-of-the-art performance on few-shot NER, achieving a 4% (absolute) improvement in F1 score on the ConLL dataset, a 9% (absolute) improvement on the GENIA dataset, and a 4% (absolute) improvement on the FewNERD dataset. PromptNER also moves the state of the art on Cross Domain NER, outperforming prior methods (including those not limited to the few-shot setting), setting a new mark on 3/5 CrossNER target domains, with an average F1 gain of 3%, despite using less than 2% of the available data.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces PromptNER, a new state-of-the-art algorithm for few-Shot and cross-domain NER, and prompts an LLM to produce a list of potential entities along with corresponding explanations justifying their compatibility with the provided entity type definitions."
            },
            "score": 4
        },
        {
            "id": "9ffefdf1fcd780cb71450b0a7a29247c66aa87be",
            "paperId": "9ffefdf1fcd780cb71450b0a7a29247c66aa87be",
            "title": "The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning",
            "abstract": "Does prompting a large language model (LLM) like GPT-3 with explanations improve in-context learning? We study this question on two NLP tasks that involve reasoning over text, namely question answering and natural language inference. We test the performance of four LLMs on three textual reasoning datasets using prompts that include explanations in multiple different styles. For these tasks, we find that including explanations in the prompts for OPT, GPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to moderate accuracy improvements over standard few-show learning. However, text-davinci-002 is able to benefit more substantially. We further show that explanations generated by the LLMs may not entail the models' predictions nor be factually grounded in the input, even on simple tasks with extractive explanations. However, these flawed explanations can still be useful as a way to verify LLMs' predictions post-hoc. Through analysis in our three settings, we show that explanations judged by humans to be good--logically consistent with the input and the prediction--more likely cooccur with accurate predictions. Following these observations, we train calibrators using automatically extracted scores that assess the reliability of explanations, allowing us to improve performance post-hoc across all of our datasets.",
            "year": 2022,
            "citationCount": 95,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work studies two NLP tasks that involve reasoning over text, namely question answering and natural language inference, and shows that explanations judged by humans to be good--logically consistent with the input and the prediction--more likely cooccur with accurate predictions."
            },
            "score": 4
        },
        {
            "id": "5ff9cd8fcb959ca6b458c11e780d61c3f2bf7691",
            "paperId": "5ff9cd8fcb959ca6b458c11e780d61c3f2bf7691",
            "title": "PLACES: Prompting Language Models for Social Conversation Synthesis",
            "abstract": "Collecting high quality conversational data can be very expensive for most applications and infeasible for others due to privacy, ethical, or similar concerns. A promising direction to tackle this problem is to generate synthetic dialogues by prompting large language models. In this work, we use a small set of expert-written conversations as in-context examples to synthesize a social conversation dataset using prompting. We perform several thorough evaluations of our synthetic conversations compared to human-collected conversations. This includes various dimensions of conversation quality with human evaluation directly on the synthesized conversations, and interactive human evaluation of chatbots fine-tuned on the synthetically generated dataset. We additionally demonstrate that this prompting approach is generalizable to multi-party conversations, providing potential to create new synthetic data for multi-party tasks. Our synthetic multi-party conversations were rated more favorably across all measured dimensions compared to conversation excerpts sampled from a human-collected multi-party dataset.",
            "year": 2023,
            "citationCount": 37,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work uses a small set of expert-written conversations as in-context examples to synthesize a social conversation dataset using prompting, and demonstrates that this prompting approach is generalizable to multi-party conversations, providing potential to create new synthetic data for multi- party tasks."
            },
            "score": 3
        },
        {
            "id": "fb49e88c6bd676516898e911e42b4f8479e6f1bf",
            "paperId": "fb49e88c6bd676516898e911e42b4f8479e6f1bf",
            "title": "Ask Me Anything: A simple strategy for prompting language models",
            "abstract": "Large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt that demonstrates how to perform the task and no additional training. Prompting is a brittle process wherein small modifications to the prompt can cause large variations in the model predictions, and therefore significant effort is dedicated towards designing a painstakingly\"perfect prompt\"for a task. To mitigate the high degree of effort involved in prompt-design, we instead ask whether producing multiple effective, yet imperfect, prompts and aggregating them can lead to a high quality prompting strategy. Our observations motivate our proposed prompting method, ASK ME ANYTHING (AMA). We first develop an understanding of the effective prompt formats, finding that question-answering (QA) prompts, which encourage open-ended generation (\"Who went to the park?\") tend to outperform those that restrict the model outputs (\"John went to the park. Output True or False.\"). Our approach recursively uses the LLM itself to transform task inputs to the effective QA format. We apply the collected prompts to obtain several noisy votes for the input's true label. We find that the prompts can have very different accuracies and complex dependencies and thus propose to use weak supervision, a procedure for combining the noisy predictions, to produce the final predictions for the inputs. We evaluate AMA across open-source model families (e.g., EleutherAI, BLOOM, OPT, and T0) and model sizes (125M-175B parameters), demonstrating an average performance lift of 10.2% over the few-shot baseline. This simple strategy enables the open-source GPT-J-6B model to match and exceed the performance of few-shot GPT3-175B on 15 of 20 popular benchmarks. Averaged across these tasks, the GPT-J-6B model outperforms few-shot GPT3-175B. We release our code here: https://github.com/HazyResearch/ama_prompting",
            "year": 2022,
            "citationCount": 117,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work develops an understanding of the effective prompt formats and proposes to use weak supervision, a procedure for combining the noisy predictions, to produce the final predictions for the inputs of a large language model."
            },
            "score": 3
        },
        {
            "id": "ad4e02784491f9794f6abb76b8982c980f51a6ee",
            "paperId": "ad4e02784491f9794f6abb76b8982c980f51a6ee",
            "title": "OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models",
            "abstract": "Neural Theory-of-Mind (N-ToM), machine's ability to understand and keep track of the mental states of others, is pivotal in developing socially intelligent agents. However, prevalent N-ToM benchmarks have several shortcomings, including the presence of ambiguous and artificial narratives, absence of personality traits and preferences, a lack of questions addressing characters' psychological mental states, and limited diversity in the questions posed. In response to these issues, we construct OpenToM, a new benchmark for assessing N-ToM with (1) longer and clearer narrative stories, (2) characters with explicit personality traits, (3) actions that are triggered by character intentions, and (4) questions designed to challenge LLMs' capabilities of modeling characters' mental states of both the physical and psychological world. Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling certain aspects of mental states in the physical world but fall short when tracking characters' mental states in the psychological world.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "OpenToM is constructed, a new benchmark for assessing N-ToM with longer and clearer narrative stories, characters with explicit personality traits, and questions designed to challenge LLMs' capabilities of modeling characters' mental states of both the physical and psychological world."
            },
            "score": 3
        },
        {
            "id": "468fc94845b52c6e96ba1f3c3884d0653d5421b4",
            "paperId": "468fc94845b52c6e96ba1f3c3884d0653d5421b4",
            "title": "GeoLM: Empowering Language Models for Geospatially Grounded Language Understanding",
            "abstract": "Humans subconsciously engage in geospatial reasoning when reading articles. We recognize place names and their spatial relations in text and mentally associate them with their physical locations on Earth. Although pretrained language models can mimic this cognitive process using linguistic context, they do not utilize valuable geospatial information in large, widely available geographical databases, e.g., OpenStreetMap. This paper introduces GeoLM, a geospatially grounded language model that enhances the understanding of geo-entities in natural language. GeoLM leverages geo-entity mentions as anchors to connect linguistic information in text corpora with geospatial information extracted from geographical databases. GeoLM connects the two types of context through contrastive learning and masked language modeling. It also incorporates a spatial coordinate embedding mechanism to encode distance and direction relations to capture geospatial context. In the experiment, we demonstrate that GeoLM exhibits promising capabilities in supporting toponym recognition, toponym linking, relation extraction, and geo-entity typing, which bridge the gap between natural language processing and geospatial sciences. The code is publicly available at https://github.com/knowledge-computing/geolm.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "GeoLM is introduced, a geospatially grounded language model that enhances the understanding of geo-entities in natural language and exhibits promising capabilities in supporting toponym recognition, toponym linking, relation extraction, and geo-entity typing, which bridge the gap between natural language processing and geospatial sciences."
            },
            "score": 3
        },
        {
            "id": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
            "paperId": "90abbc2cf38462b954ae1b772fac9532e2ccd8b0",
            "title": "Language Models are Few-Shot Learners",
            "abstract": "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.",
            "year": 2020,
            "citationCount": 24240,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic."
            },
            "score": 3
        },
        {
            "id": "cf744d77face5f22f6800411cc4060b07c0ef6ff",
            "paperId": "cf744d77face5f22f6800411cc4060b07c0ef6ff",
            "title": "Concept-Oriented Deep Learning with Large Language Models",
            "abstract": "Large Language Models (LLMs) have been successfully used in many natural-language tasks and applications including text generation and AI chatbots. They also are a promising new technology for concept-oriented deep learning (CODL). However, the prerequisite is that LLMs understand concepts and ensure conceptual consistency. We discuss these in this paper, as well as major uses of LLMs for CODL including concept extraction from text, concept graph extraction from text, and concept learning. Human knowledge consists of both symbolic (conceptual) knowledge and embodied (sensory) knowledge. Text-only LLMs, however, can represent only symbolic (conceptual) knowledge. Multimodal LLMs, on the other hand, are capable of representing the full range (conceptual and sensory) of human knowledge. We discuss conceptual understanding in visual-language LLMs, the most important multimodal LLMs, and major uses of them for CODL including concept extraction from image, concept graph extraction from image, and concept learning. While uses of LLMs for CODL are valuable standalone, they are particularly valuable as part of LLM applications such as AI chatbots.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Concept understanding in visual-language LLMs, the most important multimodal LLM, and major uses of them for CODL including concept extraction from image, concept graph extraction fromimage, and concept learning are discussed."
            },
            "score": 3
        },
        {
            "id": "0e0a513624426e017a125d0498235b8f32ce9aa5",
            "paperId": "0e0a513624426e017a125d0498235b8f32ce9aa5",
            "title": "SwarmBrain: Embodied agent for real-time strategy game StarCraft II via large language models",
            "abstract": "Large language models (LLMs) have recently garnered significant accomplishments in various exploratory tasks, even surpassing the performance of traditional reinforcement learning-based methods that have historically dominated the agent-based field. The purpose of this paper is to investigate the efficacy of LLMs in executing real-time strategy war tasks within the StarCraft II gaming environment. In this paper, we introduce SwarmBrain, an embodied agent leveraging LLM for real-time strategy implementation in the StarCraft II game environment. The SwarmBrain comprises two key components: 1) a Overmind Intelligence Matrix, powered by state-of-the-art LLMs, is designed to orchestrate macro-level strategies from a high-level perspective. This matrix emulates the overarching consciousness of the Zerg intelligence brain, synthesizing strategic foresight with the aim of allocating resources, directing expansion, and coordinating multi-pronged assaults. 2) a Swarm ReflexNet, which is agile counterpart to the calculated deliberation of the Overmind Intelligence Matrix. Due to the inherent latency in LLM reasoning, the Swarm ReflexNet employs a condition-response state machine framework, enabling expedited tactical responses for fundamental Zerg unit maneuvers. In the experimental setup, SwarmBrain is in control of the Zerg race in confrontation with an Computer-controlled Terran adversary. Experimental results show the capacity of SwarmBrain to conduct economic augmentation, territorial expansion, and tactical formulation, and it shows the SwarmBrain is capable of achieving victory against Computer players set at different difficulty levels.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results show the capacity of SwarmBrain to conduct economic augmentation, territorial expansion, and tactical formulation, and it shows the SwarmBrain is capable of achieving victory against Computer players set at different difficulty levels."
            },
            "score": 3
        },
        {
            "id": "bdaca984c81e54fe6773b2e7ba2d0992c7265c56",
            "paperId": "bdaca984c81e54fe6773b2e7ba2d0992c7265c56",
            "title": "Simple Embodied Language Learning as a Byproduct of Meta-Reinforcement Learning",
            "abstract": "Whereas machine learning models typically learn language by directly training on language tasks (e.g., next-word prediction), language emerges in human children as a byproduct of solving non-language tasks (e.g., acquiring food). Motivated by this observation, we ask: can embodied reinforcement learning (RL) agents also indirectly learn language from non-language tasks? Learning to associate language with its meaning requires a dynamic environment with varied language. Therefore, we investigate this question in a multi-task environment with language that varies across the different tasks. Specifically, we design an office navigation environment, where the agent's goal is to find a particular office, and office locations differ in different buildings (i.e., tasks). Each building includes a floor plan with a simple language description of the goal office's location, which can be visually read as an RGB image when visited. We find RL agents indeed are able to indirectly learn language. Agents trained with current meta-RL algorithms successfully generalize to reading floor plans with held-out layouts and language phrases, and quickly navigate to the correct office, despite receiving no direct language supervision.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work designs an office navigation environment, and finds RL agents trained with current meta-RL algorithms successfully generalize to reading floor plans with held-out layouts and language phrases, and quickly navigate to the correct office, despite receiving no direct language supervision."
            },
            "score": 3
        },
        {
            "id": "12c200e731017f9851afb1a6fe3fc7f76e6439c6",
            "paperId": "12c200e731017f9851afb1a6fe3fc7f76e6439c6",
            "title": "Explicit Visual Prompting for Low-Level Structure Segmentations",
            "abstract": "We consider the generic problem of detecting low-level structures in images, which includes segmenting the manipulated parts, identifying out-of-focus pixels, separating shadow regions, and detecting concealed objects. Whereas each such topic has been typically addressed with a domain-specific solution, we show that a unified approach performs well across all of them. We take inspiration from the widely-used pre-training and then prompt tuning protocols in NLP and propose a new visual prompting model, named Explicit Visual Prompting (EVP). Different from the previous visual prompting which is typically a dataset-level implicit embedding, our key insight is to enforce the tunable parameters focusing on the explicit visual content from each individual image, i.e., the features from frozen patch embeddings and the input's high-frequency components. The proposed EVP significantly outperforms other parameter-efficient tuning protocols under the same amount of tunable parameters (5.7% extra trainable parameters of each task). EVP also achieves state-of-the-art performances on diverse low-level structure segmentation tasks compared to task-specific solutions. Our code is available at: https://github.com/NiFangBaAGe/Explicit-Visual-Prompt.",
            "year": 2023,
            "citationCount": 42,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work takes inspiration from the widely-used pre-training and then prompt tuning protocols in NLP and proposes a new visual prompting model, named Explicit Visual Prompting (EVP), which significantly outperforms other parameter-efficient tuning protocols under the same amount of tunable parameters."
            },
            "score": 3
        },
        {
            "id": "98fc8faae839cc9b9dbfcf8e3ec3a7e2f7729db2",
            "paperId": "98fc8faae839cc9b9dbfcf8e3ec3a7e2f7729db2",
            "title": "NOTABLE: Transferable Backdoor Attacks Against Prompt-based NLP Models",
            "abstract": "Prompt-based learning is vulnerable to backdoor attacks. Existing backdoor attacks against prompt-based models consider injecting backdoors into the entire embedding layers or word embedding vectors. Such attacks can be easily affected by retraining on downstream tasks and with different prompting strategies, limiting the transferability of backdoor attacks. In this work, we propose transferable backdoor attacks against prompt-based models, called NOTABLE, which is independent of downstream tasks and prompting strategies. Specifically, NOTABLE injects backdoors into the encoders of PLMs by utilizing an adaptive verbalizer to bind triggers to specific words (i.e., anchors). It activates the backdoor by pasting input with triggers to reach adversary-desired anchors, achieving independence from downstream tasks and prompting strategies. We conduct experiments on six NLP tasks, three popular models, and three prompting strategies. Empirical results show that NOTABLE achieves superior attack performance (i.e., attack success rate over 90% on all the datasets), and outperforms two state-of-the-art baselines. Evaluations on three defenses show the robustness of NOTABLE. Our code can be found at https://github.com/RU-System-Software-and-Security/Notable.",
            "year": 2023,
            "citationCount": 19,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes transferable backdoor attacks against prompt-based models, called NOTABLE, which is independent of downstream tasks and prompting strategies, and achieves superior attack performance and outperforms two state-of-the-art baselines."
            },
            "score": 3
        },
        {
            "id": "0a25c137edc7c9752aa6d99ae4084683c3fe6b56",
            "paperId": "0a25c137edc7c9752aa6d99ae4084683c3fe6b56",
            "title": "Visual Prompting via Image Inpainting",
            "abstract": "How does one adapt a pre-trained visual model to novel downstream tasks without task-specific finetuning or any model modification? Inspired by prompting in NLP, this paper investigates visual prompting: given input-output image example(s) of a new task at test time and a new input image, the goal is to automatically produce the output image, consistent with the given examples. We show that posing this problem as simple image inpainting - literally just filling in a hole in a concatenated visual prompt image - turns out to be surprisingly effective, provided that the inpainting algorithm has been trained on the right data. We train masked auto-encoders on a new dataset that we curated - 88k unlabeled figures from academic papers sources on Arxiv. We apply visual prompting to these pretrained models and demonstrate results on various downstream image-to-image tasks, including foreground segmentation, single object detection, colorization, edge detection, etc.",
            "year": 2022,
            "citationCount": 109,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper investigates visual prompting: given input-output image example(s) of a new task at test time and a new input image, the goal is to automatically produce the output image, consistent with the given examples, and shows that posing this problem as simple image inpainting turns out to be surprisingly effective."
            },
            "score": 3
        },
        {
            "id": "a6a0963fcf21ed47a2616ca3980f8f4f21e6d5ad",
            "paperId": "a6a0963fcf21ed47a2616ca3980f8f4f21e6d5ad",
            "title": "Large language models as tax attorneys: a case study in legal capabilities emergence",
            "abstract": "Better understanding of Large Language Models' (LLMs) legal analysis abilities can contribute to improving the efficiency of legal services, governing artificial intelligence and leveraging LLMs to identify inconsistencies in law. This paper explores LLM capabilities in applying tax law. We choose this area of law because it has a structure that allows us to set up automated validation pipelines across thousands of examples, requires logical reasoning and maths skills, and enables us to test LLM capabilities in a manner relevant to real-world economic lives of citizens and companies. Our experiments demonstrate emerging legal understanding capabilities, with improved performance in each subsequent OpenAI model release. We experiment with retrieving and using the relevant legal authority to assess the impact of providing additional legal context to LLMs. Few-shot prompting, presenting examples of question\u2013answer pairs, is also found to significantly enhance the performance of the most advanced model, GPT-4. The findings indicate that LLMs, particularly when combined with prompting enhancements and the correct legal texts, can perform at high levels of accuracy but not yet at expert tax lawyer levels. As LLMs continue to advance, their ability to reason about law autonomously could have significant implications for the legal profession and AI governance. This article is part of the theme issue \u2018A complexity science approach to law and governance\u2019.",
            "year": 2023,
            "citationCount": 20,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The findings indicate that LLMs, particularly when combined with prompting enhancements and the correct legal texts, can perform at high levels of accuracy but not yet at expert tax lawyer levels."
            },
            "score": 2
        },
        {
            "id": "a122863d239643453195424c04067e89406246e1",
            "paperId": "a122863d239643453195424c04067e89406246e1",
            "title": "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations",
            "abstract": "Fine-tuning on instruction data has been widely validated as an effective practice for implementing chat language models like ChatGPT. Scaling the diversity and quality of such data, although straightforward, stands a great chance of leading to improved performance. This paper aims to improve the upper bound of open-source models further. We first provide a systematically designed, diverse, informative, large-scale dataset of instructional conversations, UltraChat, which does not involve human queries. Our objective is to capture the breadth of interactions that a human might have with an AI assistant and employs a comprehensive framework to generate multi-turn conversation iteratively. UltraChat contains 1.5 million high-quality multi-turn dialogues and covers a wide range of topics and instructions. Our statistical analysis of UltraChat reveals its superiority in various key metrics, including scale, average length, diversity, coherence, etc., solidifying its position as a leading open-source dataset. Building upon UltraChat, we fine-tune a LLaMA model to create a powerful conversational model, UltraLLaMA. Our evaluations indicate that UltraLLaMA consistently outperforms other open-source models, including Vicuna, the previously recognized state-of-the-art open-source model. The dataset and the model will be publicly released\\footnote{\\url{https://github.com/thunlp/UltraChat}}.",
            "year": 2023,
            "citationCount": 141,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper provides a systematically designed, diverse, informative, large-scale dataset of instructional conversations, UltraChat, and fine-tune a LLaMA model to create a powerful conversational model, UltraLLaMA, which consistently outperforms other open-source models, including Vicuna."
            },
            "score": 2
        },
        {
            "id": "e314d182fd9d35a05870b38a56ee38eb3149b47d",
            "paperId": "e314d182fd9d35a05870b38a56ee38eb3149b47d",
            "title": "Attack Prompt Generation for Red Teaming and Defending Large Language Models",
            "abstract": "Large language models (LLMs) are susceptible to red teaming attacks, which can induce LLMs to generate harmful content. Previous research constructs attack prompts via manual or automatic methods, which have their own limitations on construction cost and quality. To address these issues, we propose an integrated approach that combines manual and automatic methods to economically generate high-quality attack prompts. Specifically, considering the impressive capabilities of newly emerged LLMs, we propose an attack framework to instruct LLMs to mimic human-generated prompts through in-context learning. Furthermore, we propose a defense framework that fine-tunes victim LLMs through iterative interactions with the attack framework to enhance their safety against red teaming attacks. Extensive experiments on different LLMs validate the effectiveness of our proposed attack and defense frameworks. Additionally, we release a series of attack prompts datasets named SAP with varying sizes, facilitating the safety evaluation and enhancement of more LLMs. Our code and dataset is available on https://github.com/Aatrox103/SAP .",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes an attack framework to instruct LLMs to mimic human-generated prompts through in-context learning to enhance their safety against red teaming attacks, and proposes a defense framework that fine-tunes victim LLMs through iterative interactions with the attack framework."
            },
            "score": 2
        },
        {
            "id": "925ff363f781570afbceb361c9ae1cf503172b26",
            "paperId": "925ff363f781570afbceb361c9ae1cf503172b26",
            "title": "Structural Embeddings of Tools for Large Language Models",
            "abstract": "It is evident that the current state of Large Language Models (LLMs) necessitates the incorporation of external tools. The lack of straightforward algebraic and logical reasoning is well documented and prompted researchers to develop frameworks which allow LLMs to operate via external tools. The ontological nature of tool utilization for a specific task can be well formulated with a Directed Acyclic Graph (DAG). The central aim of the paper is to highlight the importance of graph based approaches to LLM-tool interaction in near future. We propose an exemplary framework to guide the orchestration of exponentially increasing numbers of external tools with LLMs,where objectives and functionalities of tools are graph encoded hierarchically. Assuming that textual segments of a Chain-of-Thought (CoT) can be imagined as a tool as defined here, the graph based framework can pave new avenues in that particular direction as well.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An exemplary framework to guide the orchestration of exponentially increasing numbers of external tools with LLMs is proposed, where objectives and functionalities of tools are graph encoded hierarchically."
            },
            "score": 2
        },
        {
            "id": "1866cf8e89f5f3375ca6abc019bdf963bd486d56",
            "paperId": "1866cf8e89f5f3375ca6abc019bdf963bd486d56",
            "title": "ReasoningLM: Enabling Structural Subgraph Reasoning in Pre-trained Language Models for Question Answering over Knowledge Graph",
            "abstract": "Question Answering over Knowledge Graph (KGQA) aims to seek answer entities for the natural language question from a large-scale Knowledge Graph~(KG). To better perform reasoning on KG, recent work typically adopts a pre-trained language model~(PLM) to model the question, and a graph neural network~(GNN) based module to perform multi-hop reasoning on the KG. Despite the effectiveness, due to the divergence in model architecture, the PLM and GNN are not closely integrated, limiting the knowledge sharing and fine-grained feature interactions. To solve it, we aim to simplify the above two-module approach, and develop a more capable PLM that can directly support subgraph reasoning for KGQA, namely ReasoningLM. In our approach, we propose a subgraph-aware self-attention mechanism to imitate the GNN for performing structured reasoning, and also adopt an adaptation tuning strategy to adapt the model parameters with 20,000 subgraphs with synthesized questions. After adaptation, the PLM can be parameter-efficient fine-tuned on downstream tasks. Experiments show that ReasoningLM surpasses state-of-the-art models by a large margin, even with fewer updated parameters and less training data. Our codes and data are publicly available at~\\url{https://github.com/RUCAIBox/ReasoningLM}.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "In this work, a subgraph-aware self-attention mechanism to imitate the GNN for performing structured reasoning for KGQA is proposed, and an adaptation tuning strategy to adapt the model parameters with 20,000 subgraphs with synthesized questions is adopted."
            },
            "score": 2
        },
        {
            "id": "43bab902791fc844d80b005e2c54bbbbe9e64f26",
            "paperId": "43bab902791fc844d80b005e2c54bbbbe9e64f26",
            "title": "Structural Adapters in Pretrained Language Models for AMR-to-Text Generation",
            "abstract": "Pretrained language models (PLM) have recently advanced graph-to-text generation, where the input graph is linearized into a sequence and fed into the PLM to obtain its representation. However, efficiently encoding the graph structure in PLMs is challenging because such models were pretrained on natural language, and modeling structured data may lead to catastrophic forgetting of distributional knowledge. In this paper, we propose StructAdapt, an adapter method to encode graph structure into PLMs. Contrary to prior work, StructAdapt effectively models interactions among the nodes based on the graph connectivity, only training graph structure-aware adapter parameters. In this way, we incorporate task-specific knowledge while maintaining the topological structure of the graph. We empirically show the benefits of explicitly encoding graph structure into PLMs using StructAdapt, outperforming the state of the art on two AMR-to-text datasets, training only 5.1% of the PLM parameters.",
            "year": 2021,
            "citationCount": 56,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The benefits of explicitly encoding graph structure into PLMs using StructAdapt are empirically shown, outperforming the state of the art on two AMR-to-text datasets, training only 5.1% of the PLM parameters."
            },
            "score": 2
        },
        {
            "id": "159b9c65260beea062cebdc023d9e802869768f3",
            "paperId": "159b9c65260beea062cebdc023d9e802869768f3",
            "title": "Learning from Mistakes via Cooperative Study Assistant for Large Language Models",
            "abstract": "Large language models (LLMs) have demonstrated their potential to refine their generation based on their own feedback. However, the feedback from LLM itself is often inaccurate, thereby limiting its benefits. In this paper, we propose Study Assistant for Large LAnguage Model (SALAM), a novel framework with an auxiliary agent to assist the main LLM in learning from mistakes through interactive cooperation. In the gathering phase, the student assistant agent probes the main LLM, analyzes its errors, and collects the interaction in a mistake memory. During the examination phase, the study assistant provides guidelines by retrieving relevant cases to help the main LLM anticipate and avoid similar errors. We first investigate the effectiveness of a general study assistant and then customize it to provide LLM-specific guidance through imitation learning from successful guidance experiences. Our experiments on three LLMs using two challenging frameworks demonstrate that SALAM can significantly boost LLMs by an accuracy margin of up to 6.6 on BBH and 12.6 on BBQ.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes Study Assistant for Large LAnguage Model (SALAM), a novel framework with an auxiliary agent to assist the main LLM in learning from mistakes through interactive cooperation, and demonstrates that SALAM can significantly boost LLMs by an accuracy margin of up to 6.6 on BBH and 12."
            },
            "score": 2
        },
        {
            "id": "8806ba65f2a477fe685c250c1536d2e8ef450958",
            "paperId": "8806ba65f2a477fe685c250c1536d2e8ef450958",
            "title": "Empowering Language Models with Knowledge Graph Reasoning for Open-Domain Question Answering",
            "abstract": "Answering open-domain questions requires world knowledge about in-context entities. As pre-trained Language Models (LMs) lack the power to store all required knowledge, external knowledge sources, such as knowledge graphs, are often used to augment LMs. In this work, we propose knOwledge REasOning empowered Language Model(OREO-LM), which consists of a novel Knowledge Interaction Layer that can be flexibly plugged into existing Transformer-based LMs to interact with a differentiable Knowledge Graph Reasoning module collaboratively. In this way, LM guides KG to walk towards the desired answer, while the retrieved knowledge improves LM.By adopting OREO-LM to RoBERTa and T5, we show significant performance gain, achieving state-of-art results in the Closed-Book setting. The performance enhancement is mainly from the KG reasoning\u2019s capacity to infer missing relational facts. In addition, OREO-LM provides reasoning paths as rationales to interpret the model\u2019s decision.",
            "year": 2022,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 2
        },
        {
            "id": "a8d02ff6d075c3cc48f0b97801cc52765c8f9ac9",
            "paperId": "a8d02ff6d075c3cc48f0b97801cc52765c8f9ac9",
            "title": "LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models",
            "abstract": "Large Vision-Language Models (LVLMs) have recently played a dominant role in multimodal vision-language learning. Despite the great success, it lacks a holistic evaluation of their efficacy. This paper presents a comprehensive evaluation of publicly available large multimodal models by building a LVLM evaluation Hub (LVLM-eHub). Our LVLM-eHub consists of $8$ representative LVLMs such as InstructBLIP and MiniGPT-4, which are thoroughly evaluated by a quantitative capability evaluation and an online arena platform. The former evaluates $6$ categories of multimodal capabilities of LVLMs such as visual question answering and embodied artificial intelligence on $47$ standard text-related visual benchmarks, while the latter provides the user-level evaluation of LVLMs in an open-world question-answering scenario. The study reveals several innovative findings. First, instruction-tuned LVLM with massive in-domain data such as InstructBLIP heavily overfits many existing tasks, generalizing poorly in the open-world scenario. Second, instruction-tuned LVLM with moderate instruction-following data may result in object hallucination issues (i.e., generate objects that are inconsistent with target images in the descriptions). It either makes the current evaluation metric such as CIDEr for image captioning ineffective or generates wrong answers. Third, employing a multi-turn reasoning evaluation framework can mitigate the issue of object hallucination, shedding light on developing an effective pipeline for LVLM evaluation. The findings provide a foundational framework for the conception and assessment of innovative strategies aimed at enhancing zero-shot multimodal techniques. Our LVLM-eHub will be available at https://github.com/OpenGVLab/Multi-Modality-Arena",
            "year": 2023,
            "citationCount": 65,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A comprehensive evaluation of publicly available large multimodal models by building a LVLM evaluation Hub (LVLM-eHub), which provides a foundational framework for the conception and assessment of innovative strategies aimed at enhancing zero-shot multimodals techniques."
            },
            "score": 2
        },
        {
            "id": "dfbfa21a93c3164ae8a033398c8de42b03b1b84d",
            "paperId": "dfbfa21a93c3164ae8a033398c8de42b03b1b84d",
            "title": "ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning",
            "abstract": "Over the last few years, large language models (LLMs) have emerged as the most important breakthroughs in natural language processing (NLP) that fundamentally transform research and developments in the field. ChatGPT represents one of the most exciting LLM systems developed recently to showcase impressive skills for language generation and highly attract public attention. Among various exciting applications discovered for ChatGPT in English, the model can process and generate texts for multiple languages due to its multilingual training data. Given the broad adoption of ChatGPT for English in different problems and areas, a natural question is whether ChatGPT can also be applied effectively for other languages or it is necessary to develop more language-specific technologies. The answer to this question requires a thorough evaluation of ChatGPT over multiple tasks with diverse languages and large datasets (i.e., beyond reported anecdotes), which is still missing or limited in current research. Our work aims to fill this gap for the evaluation of ChatGPT and similar LLMs to provide more comprehensive information for multilingual NLP applications. While this work will be an ongoing effort to include additional experiments in the future, our current paper evaluates ChatGPT on 7 different tasks, covering 37 diverse languages with high, medium, low, and extremely low resources. We also focus on the zero-shot learning setting for ChatGPT to improve reproducibility and better simulate the interactions of general users. Compared to the performance of previous models, our extensive experimental results demonstrate a worse performance of ChatGPT for different NLP tasks and languages, calling for further research to develop better models and understanding for multilingual learning.",
            "year": 2023,
            "citationCount": 137,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Compared to the performance of previous models, extensive experimental results demonstrate a worse performance of ChatGPT for different NLP tasks and languages, calling for further research to develop better models and understanding for multilingual learning."
            },
            "score": 1
        },
        {
            "id": "da4deaf81232d94e2f38a9d23c6b04ae1d79fbfc",
            "paperId": "da4deaf81232d94e2f38a9d23c6b04ae1d79fbfc",
            "title": "CLAIR: Evaluating Image Captions with Large Language Models",
            "abstract": "The evaluation of machine-generated image captions poses an interesting yet persistent challenge. Effective evaluation measures must consider numerous dimensions of similarity, including semantic relevance, visual structure, object interactions, caption diversity, and specificity. Existing highly-engineered measures attempt to capture specific aspects, but fall short in providing a holistic score that aligns closely with human judgments. Here, we propose CLAIR, a novel method that leverages the zero-shot language modeling capabilities of large language models (LLMs) to evaluate candidate captions. In our evaluations, CLAIR demonstrates a stronger correlation with human judgments of caption quality compared to existing measures. Notably, on Flickr8K-Expert, CLAIR achieves relative correlation improvements over SPICE of 39.6% and over image-augmented methods such as RefCLIP-S of 18.3%. Moreover, CLAIR provides noisily interpretable results by allowing the language model to identify the underlying reasoning behind its assigned score. Code is available at https://davidmchan.github.io/clair/",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "In evaluations, CLAIR demonstrates a stronger correlation with human judgments of caption quality compared to existing measures, and provides noisily interpretable results by allowing the language model to identify the underlying reasoning behind its assigned score."
            },
            "score": 1
        },
        {
            "id": "c476c1587beda904133f97592a39965be418c8bf",
            "paperId": "c476c1587beda904133f97592a39965be418c8bf",
            "title": "Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models",
            "abstract": "The recent success of large language models (LLMs) has shown great potential to develop more powerful conversational recommender systems (CRSs), which rely on natural language conversations to satisfy user needs. In this paper, we embark on an investigation into the utilization of ChatGPT for conversational recommendation, revealing the inadequacy of the existing evaluation protocol. It might over-emphasize the matching with the ground-truth items or utterances generated by human annotators, while neglecting the interactive nature of being a capable CRS. To overcome the limitation, we further propose an interactive Evaluation approach based on LLMs named iEvaLM that harnesses LLM-based user simulators. Our evaluation approach can simulate various interaction scenarios between users and systems. Through the experiments on two publicly available CRS datasets, we demonstrate notable improvements compared to the prevailing evaluation protocol. Furthermore, we emphasize the evaluation of explainability, and ChatGPT showcases persuasive explanation generation for its recommendations. Our study contributes to a deeper comprehension of the untapped potential of LLMs for CRSs and provides a more flexible and easy-to-use evaluation framework for future research endeavors. The codes and data are publicly available at https://github.com/RUCAIBox/iEvaLM-CRS.",
            "year": 2023,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An investigation into the utilization of ChatGPT for conversational recommendation is embarked on, revealing the inadequacy of the existing evaluation protocol and proposing an interactive Evaluation approach based on LLMs named iEvaLM that harnesses LLM-based user simulators that can simulate various interaction scenarios between users and systems."
            },
            "score": 1
        },
        {
            "id": "f52af5abe78ca2af836f70ce193f0161bc2e6264",
            "paperId": "f52af5abe78ca2af836f70ce193f0161bc2e6264",
            "title": "clembench: Using Game Play to Evaluate Chat-Optimized Language Models as Conversational Agents",
            "abstract": "Recent work has proposed a methodology for the systematic evaluation of\"Situated Language Understanding Agents\"-agents that operate in rich linguistic and non-linguistic contexts-through testing them in carefully constructed interactive settings. Other recent work has argued that Large Language Models (LLMs), if suitably set up, can be understood as (simulators of) such agents. A connection suggests itself, which this paper explores: Can LLMs be evaluated meaningfully by exposing them to constrained game-like settings that are built to challenge specific capabilities? As a proof of concept, this paper investigates five interaction settings, showing that current chat-optimised LLMs are, to an extent, capable to follow game-play instructions. Both this capability and the quality of the game play, measured by how well the objectives of the different games are met, follows the development cycle, with newer models performing better. The metrics even for the comparatively simple example games are far from being saturated, suggesting that the proposed instrument will remain to have diagnostic value. Our general framework for implementing and evaluating games with LLMs is available at https://github.com/clembench .",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper investigates five interaction settings, showing that current chat-optimised LLMs are, to an extent, capable to follow game-play instructions."
            },
            "score": 1
        }
    ],
    "novelty": "yes"
}