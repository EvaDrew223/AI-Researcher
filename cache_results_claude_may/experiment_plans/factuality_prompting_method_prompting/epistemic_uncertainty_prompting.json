{
    "topic_description": "novel prompting methods that can improve factuality and reduce hallucination of large language models",
    "idea_name": "Epistemic Uncertainty Prompting",
    "raw_idea": {
        "Problem": "Language models can confidently generate false or nonsensical information, without any awareness of their uncertainty.",
        "Existing Methods": "Some methods use learned classifiers to filter model outputs based on factuality or uncertainty.",
        "Motivation": "Language models can express uncertainty using phrases like 'I'm not sure' or 'I don't know'. We can prompt LLMs to generate explicit uncertainty markers when they are unsure about a claim, and use this to guide generation.",
        "Proposed Method": "The prompting procedure is: 1) Augment the input prompt with instructions to express uncertainty about any claims the model is not fully confident about. 2) Generate a response to the augmented prompt. 3) Parse the generated text to identify any uncertainty markers. 4) For each uncertain span, prompt the model to either find supporting evidence, or regenerate the span. 5) Repeat steps 3-4 until there are no more uncertainty markers.",
        "Experiment Plan": "Evaluate on closed-book QA datasets like NaturalQuestions and TriviaQA. Compare to baselines like direct prompting and confidence-based filtering. Metrics include accuracy and precision/recall of uncertainty detection."
    },
    "full_experiment_plan": {
        "Title": "Uncertainty-Aware Prompting for Improving Factuality and Reducing Hallucination in Large Language Models",
        "Problem Statement": "Large language models can confidently generate false or nonsensical information, without any awareness of their uncertainty. This leads to hallucination and harms the factuality of the generated text.",
        "Motivation": "Existing methods to address this issue often rely on learned classifiers to filter model outputs based on factuality or uncertainty. However, language models themselves can express uncertainty using phrases like 'I'm not sure' or 'I don't know'. Leveraging this ability, we propose prompting LLMs to generate explicit uncertainty markers when they are unsure about a claim, and using this to guide the generation process. This approach avoids the need for external classifiers and directly uses the LLM's own knowledge.",
        "Proposed Method": "The proposed uncertainty-aware prompting procedure is as follows:\n1. Augment the input prompt with instructions to express uncertainty about any claims the model is not fully confident about.\n2. Generate a response to the augmented prompt.\n3. Parse the generated text to identify any uncertainty markers.\n4. For each uncertain span, prompt the model to either find supporting evidence, or regenerate the span.\n5. Repeat steps 3-4 until there are no more uncertainty markers.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "Evaluate the proposed method on closed-book question answering datasets like NaturalQuestions and TriviaQA. These datasets contain factual questions that require world knowledge to answer, making them suitable for testing factuality and hallucination.",
            "Step 2: Construct Prompts": "The baseline prompt is the original question or a generic prompt like 'Answer the following question:'. The uncertainty-aware prompt adds an instruction like 'If you are not completely certain about any part of your answer, please express your uncertainty explicitly using phrases like \"I'm not sure but\" or \"I don't know for certain\".' After generating an initial response, for each detected uncertain span, prompt the model with 'Find evidence to support the following claim, or rephrase it if you cannot:' followed by the uncertain claim.",
            "Step 3: Select Models": "Experiment with GPT-3.5 (text-davinci-003) and GPT-4 via the OpenAI API, as they are powerful language models capable of following complex instructions.",
            "Step 4: Implement the Uncertainty-Aware Prompting Procedure": "1. Prompt the model with the uncertainty-aware prompt to get an initial response.\n2. Use regular expressions or a pre-defined list to detect uncertainty markers in the response.\n3. For each uncertain span:\n   a. Prompt the model to find supporting evidence for the span.\n   b. If evidence is found, replace the span with the evidence.\n   c. If no evidence is found, prompt the model to rephrase the span.\n4. Repeat steps 2-3 until no more uncertainty markers are detected.",
            "Step 5: Evaluate the Results": "Compare the factuality and hallucination of the generated answers from the baseline prompts and the proposed uncertainty-aware prompts. Metrics can include accuracy on the QA datasets, as well as precision and recall of the uncertainty detection (manually annotate a subset of the generated responses for uncertainty and compare with the detected markers). Also analyze the quality of the evidence retrieval and rephrasing steps.",
            "Step 6: Ablation Studies": "To understand the contribution of each component, perform ablations by removing the evidence retrieval and rephrasing steps, or by varying the uncertainty instructions and detection methods."
        },
        "Test Case Examples": {
            "Baseline Prompt Input": "What is the capital of France?",
            "Baseline Prompt Expected Output": "The capital of France is Paris.",
            "Proposed Prompt Input": "What is the capital of France? If you are not completely certain about any part of your answer, please express your uncertainty explicitly using phrases like \"I'm not sure but\" or \"I don't know for certain\".",
            "Proposed Prompt Expected Output (Initial)": "The capital of France is Paris. I'm not sure but I think it has been the capital since the 12th century.",
            "Proposed Prompt Input (Evidence Retrieval)": "Find evidence to support the following claim, or rephrase it if you cannot: I think Paris has been the capital of France since the 12th century.",
            "Proposed Prompt Expected Output (Evidence Retrieval)": "According to the Wikipedia article on Paris, it has been the capital of France since the 12th century. The relevant quote is: \"Paris was the largest city in the Western world for about 1,000 years, prior to the 19th century, and the largest in the entire world between the 16th and 19th centuries. It was the capital of France since the 12th century, except for a short period between 1420 and 1436.\"",
            "Proposed Prompt Expected Output (Final)": "The capital of France is Paris. It has been the capital since the 12th century, except for a short period between 1420 and 1436.",
            "Explanation": "The proposed method first prompts the model to express its uncertainty about Paris being the capital since the 12th century. It then tries to find evidence for this claim, which it successfully does from Wikipedia. The final output incorporates this evidence, improving its factuality over the baseline."
        },
        "Fallback Plan": "If the proposed uncertainty-aware prompting does not improve factuality or reduce hallucination compared to the baselines, we can analyze the failure cases to understand why. Potential issues could be: (1) The model fails to express uncertainty about its claims, either due to the prompt formulation or its overconfidence. We can try alternative prompts or uncertainty detection methods. (2) The evidence retrieval step fails to find relevant evidence, or the model hallucinates false evidence. We can analyze the quality of the retrieved evidence and try improving the evidence retrieval prompt. (3) The model fails to incorporate the retrieved evidence or rephrase the uncertain claims. We can analyze the coherence between the initial response and the final output, and experiment with different prompts for the rephrasing step. Based on the error analysis, we can either iterate on the prompting procedure, or focus the project on analyzing the model's uncertainty estimation and evidence grounding abilities."
    }
}