{
    "topic_description": "novel prompting methods that can improve factuality and reduce hallucination of large language models",
    "idea_name": "Entailment Tree Prompting",
    "raw_idea": {
        "Problem": "LLMs can generate false statements that are not logically entailed by the given context or not consistent with each other. Existing methods do not explicitly model the logical structure of the generated text.",
        "Existing Methods": "Datasets like EntailmentBank and CLUTRR evaluate logical entailment in generated text. Baselines include standard language modeling and inconsistency detection methods.",
        "Motivation": "We can prompt LLMs to generate texts that form a valid entailment tree, where each sentence is entailed by the conjunction of its parent sentences and the initial context. This ensures the logical consistency of the generated text.",
        "Proposed Method": "We propose entailment tree prompting, where starting from the initial context as the root, we prompt the LLM to generate child sentences that are entailed by the parent sentences. For each generated sentence, we prompt the LLM to score its entailment likelihood given the parent sentences. If the score is low, the sentence is discarded. We recursively prompt the LLM to generate entailed sentences until reaching a maximum depth or no more sentences can be generated. The final generated text is the conjunction of all sentences in the entailment tree. The entailment scores can be used to quantify the logical validity of the generated text.",
        "Experiment Plan": "Evaluate on logical entailment datasets like EntailmentBank and CLUTRR. Compare with baselines like standard language modeling and inconsistency detection methods. Metrics include entailment accuracy and human evaluation of logical coherence."
    },
    "full_experiment_plan": {
        "Title": "Entailment Tree Prompting: Improving Logical Consistency in Language Model Generation",
        "Problem Statement": "Large Language Models (LLMs) can generate false statements that are not logically entailed by the given context or not consistent with each other. Existing methods do not explicitly model the logical structure of the generated text.",
        "Motivation": "Datasets like EntailmentBank and CLUTRR evaluate logical entailment in generated text, but current language modeling and inconsistency detection methods do not perform well on these tasks. We can leverage the reasoning capabilities of LLMs themselves to generate texts that form a valid entailment tree, where each sentence is entailed by the conjunction of its parent sentences and the initial context. This ensures the logical consistency of the generated text.",
        "Proposed Method": "We propose entailment tree prompting, where starting from the initial context as the root, we prompt the LLM to generate child sentences that are entailed by the parent sentences. For each generated sentence, we prompt the LLM to score its entailment likelihood given the parent sentences. If the score is low, the sentence is discarded. We recursively prompt the LLM to generate entailed sentences until reaching a maximum depth or no more sentences can be generated. The final generated text is the conjunction of all sentences in the entailment tree. The entailment scores can be used to quantify the logical validity of the generated text.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "Evaluate on logical entailment datasets like EntailmentBank and CLUTRR. EntailmentBank contains entailment trees for reasoning about scientific facts. CLUTRR contains entailment trees for family relationships. Use the test sets for evaluation.",
            "Step 2: Construct Prompts": "For the baseline, use standard left-to-right language modeling prompts, e.g., 'Given the context, generate the next sentence:'. For the proposed method:\n1. Root Prompt: Concatenate the initial context with 'Generate a sentence that can be logically entailed from the above context:'\n2. Recursive Prompt: Concatenate the parent sentences with 'Generate a sentence that can be logically entailed from the above sentences:'\n3. Entailment Scoring Prompt: Concatenate the parent sentences and the candidate child sentence with 'On a scale of 1 to 5, where 1 is not at all entailed and 5 is strongly entailed, score how much the last sentence can be logically entailed from the previous sentences:'",
            "Step 3: Select Models": "Use GPT-3.5 (text-davinci-003) and GPT-4 via the OpenAI API.",
            "Step 4: Get Results": "For each example in the test set:\n1. Use the root prompt to generate the first level child sentences. Keep sentences with entailment score >= 4.\n2. Recursively use the recursive prompt to generate the next level child sentences for each kept sentence from the previous level. Keep sentences with entailment score >= 4.\n3. Repeat step 2 until reaching a maximum depth (e.g., 5) or no more sentences are generated.\n4. Concatenate all the kept sentences to form the final generated text.\n5. Compute the logical entailment accuracy by checking if the generated text forms a valid entailment tree according to the gold entailment labels in the dataset.",
            "Step 5: Analyze Results": "1. Compare the logical entailment accuracy of the proposed method with the baselines.\n2. Analyze the quality of the generated entailment trees, e.g., the average depth, the number of generated sentences, the average entailment score, etc.\n3. Perform human evaluation on a subset of examples to assess the logical coherence and factual correctness of the generated texts."
        },
        "Test Case Examples": {
            "Baseline Prompt Input": "Context: Alice is Bob's sister. Bob is Charlie's father. Charlie is David's brother. David is Emily's son.\nGenerate a continuation of the context:",
            "Baseline Prompt Expected Output": "Alice is Emily's aunt. Emily is Charlie's niece. Bob is Emily's grandfather. Alice and Charlie are not directly related.",
            "Proposed Prompt Input (Root)": "Context: Alice is Bob's sister. Bob is Charlie's father. Charlie is David's brother. David is Emily's son.\nGenerate a sentence that can be logically entailed from the above context:",
            "Proposed Prompt Expected Output (Root)": "Alice is Charlie's aunt.",
            "Proposed Prompt Input (Entailment Scoring)": "Sentences: Alice is Bob's sister. Bob is Charlie's father. Charlie is David's brother. David is Emily's son. Alice is Charlie's aunt. Alice is Emily's great aunt.\nCandidate: Bob is Emily's grandfather.\nOn a scale of 1 to 5, where 1 is not at all entailed and 5 is strongly entailed, score how much the last sentence can be logically entailed from the previous sentences:",
            "Proposed Prompt Expected Output (Entailment Scoring)": "5",
            "Proposed Prompt Input (Recursive)": "Sentences: Alice is Bob's sister. Bob is Charlie's father. Charlie is David's brother. David is Emily's son. Alice is Charlie's aunt. Alice is Emily's great aunt.\nGenerate a sentence that can be logically entailed from the above sentences:",
            "Proposed Prompt Expected Output (Recursive)": "Bob is Emily's grandfather.",
            "Proposed Prompt Input (Final Output)": "Context: Alice is Bob's sister. Bob is Charlie's father. Charlie is David's brother. David is Emily's son.\nGenerated Entailment Tree:\n- Alice is Charlie's aunt.\n- Alice is Emily's great aunt.\n- Bob is Emily's grandfather.",
            "Explanation": "The baseline output contains sentences that are not entailed by the context (e.g., 'Alice and Charlie are not directly related'). The proposed method generates an entailment tree where each sentence is entailed by the conjunction of its parent sentences and the initial context, leading to a more logically consistent output."
        },
        "Fallback Plan": "If the proposed entailment tree prompting method does not outperform the baselines, we can perform the following analyses:\n1. Check the quality of the generated sentences at each level of the tree. Are they actually entailed by the parent sentences? Are they relevant to the context? Are they factually correct?\n2. Analyze the entailment scores. Are they accurate in measuring the logical entailment? Can we use a different entailment scoring method?\n3. Vary the hyperparameters such as the maximum depth, the entailment score threshold, the prompt templates, etc. Do they affect the performance?\n4. Try different ways to combine the sentences into the final output. Instead of concatenation, can we use a tree-structured decoder to generate the final output?\nBased on the analysis, we can propose improvements to the method, such as better prompting strategies, alternative entailment scoring methods, or different decoding algorithms. If the improved methods still do not work well, we can turn the project into an analysis paper that investigates the challenges and limitations of using entailment tree prompting for language model generation, and propose future directions to address these challenges."
    },
    "novelty_queries": [
        "KeywordQuery(\"language models entailment tree prompting\")",
        "KeywordQuery(\"language models logical consistency entailment\")",
        "KeywordQuery(\"language models entailment tree generation\")",
        "KeywordQuery(\"Entailment Tree Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "484f8c2a19c0dbe4223cc7f77275cc4ca97b8ef8",
            "paperId": "484f8c2a19c0dbe4223cc7f77275cc4ca97b8ef8",
            "title": "Entailment Tree Explanations via Iterative Retrieval-Generation Reasoner",
            "abstract": "Large language models have achieved high performance on various question answering (QA) benchmarks, but the explainability of their output remains elusive. Structured explanations, called entailment trees, were recently suggested as a way to explain and inspect a QA system's answer. In order to better generate such entailment trees, we propose an architecture called Iterative Retrieval-Generation Reasoner (IRGR). Our model is able to explain a given hypothesis by systematically generating a step-by-step explanation from textual premises. The IRGR model iteratively searches for suitable premises, constructing a single entailment step at a time. Contrary to previous approaches, our method combines generation steps and retrieval of premises, allowing the model to leverage intermediate conclusions, and mitigating the input size limit of baseline encoder-decoder models. We conduct experiments using the EntailmentBank dataset, where we outperform existing benchmarks on premise retrieval and entailment tree generation, with around 300% gain in overall correctness.",
            "year": 2022,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes an architecture called Iterative Retrieval-Generation Reasoner (IRGR), able to explain a given hypothesis by systematically generating a step-by-step explanation from textual premises, allowing the model to leverage intermediate conclusions, and mitigating the input size limit of baseline encoder-decoder models."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the logical consistency of language model generation by using entailment tree prompting, where the model generates sentences that form a valid entailment tree. The paper focuses on generating entailment trees to explain a question answering system's output by iteratively retrieving premises and generating entailment steps.\n\nWhile both the project proposal and the paper involve generating entailment trees, the project proposal uses entailment trees to improve the logical consistency of general language model generation, whereas the paper uses entailment trees to explain the output of a specific question answering system. The approaches also differ, with the project proposal using prompting and the paper using an iterative retrieval-generation architecture.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "8ddeaa0bee603cae32422f47855b32349664f4d5",
            "paperId": "8ddeaa0bee603cae32422f47855b32349664f4d5",
            "title": "Do Natural Language Explanations Represent Valid Logical Arguments? Verifying Entailment in Explainable NLI Gold Standards",
            "abstract": "An emerging line of research in Explainable NLP is the creation of datasets enriched with human-annotated explanations and rationales, used to build and evaluate models with step-wise inference and explanation generation capabilities. While human-annotated explanations are used as ground-truth for the inference, there is a lack of systematic assessment of their consistency and rigour. In an attempt to provide a critical quality assessment of Explanation Gold Standards (XGSs) for NLI, we propose a systematic annotation methodology, named Explanation Entailment Verification (EEV), to quantify the logical validity of human-annotated explanations. The application of EEV on three mainstream datasets reveals the surprising conclusion that a majority of the explanations, while appearing coherent on the surface, represent logically invalid arguments, ranging from being incomplete to containing clearly identifiable logical errors. This conclusion confirms that the inferential properties of explanations are still poorly formalised and understood, and that additional work on this line of research is necessary to improve the way Explanation Gold Standards are constructed.",
            "year": 2021,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A systematic annotation methodology, named Explanation Entailment Verification (EEV), is proposed, to quantify the logical validity of human-annotated explanations, and confirms that the inferential properties of explanations are still poorly formalised and understood."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the logical consistency of language model generation by using entailment tree prompting, where the model generates sentences that form a valid entailment tree. The paper abstract, on the other hand, proposes a methodology to assess the logical validity of human-annotated explanations in existing NLI datasets, revealing that many explanations are logically invalid despite appearing coherent.\n\nProject Proposal: Improving logical consistency in language model generation using entailment tree prompting.\nPaper Abstract: Assessing the logical validity of human-annotated explanations in NLI datasets using Explanation Entailment Verification (EEV).\n\nThe project proposal focuses on generating logically consistent text, while the paper abstract focuses on evaluating the logical validity of existing explanations. They address different aspects of logical consistency in NLP tasks.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "e1943cbf4817605a1f988fe5fd785f6b707ca233",
            "paperId": "e1943cbf4817605a1f988fe5fd785f6b707ca233",
            "title": "METGEN: A Module-Based Entailment Tree Generation Framework for Answer Explanation",
            "abstract": "Knowing the reasoning chains from knowledge to the predicted answers can help construct an explainable question answering (QA) system. Advances on QA explanation propose to explain the answers with entailment trees composed of multiple entailment steps. While current work proposes to generate entailment trees with end-to-end generative models, the steps in the generated trees are not constrained and could be unreliable. In this paper, we propose METGEN, a Module-based Entailment Tree GENeration framework that has multiple modules and a reasoning controller. Given a question and several supporting knowledge, METGEN can iteratively generate the entailment tree by conducting single-step entailment with separate modules and selecting the reasoning flow with the controller. As each module is guided to perform a specific type of entailment reasoning, the steps generated by METGEN are more reliable and valid. Experiment results on the standard benchmark show that METGEN can outperform previous state-of-the-art models with only 9% of the parameters.",
            "year": 2022,
            "citationCount": 21,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "METGEN is proposed, a Module-based Entailment Tree GENeration framework that has multiple modules and a reasoning controller that can iteratively generate the entailment tree by conducting single-step entailment with separate modules and selecting the reasoning flow with the controller."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the logical consistency of language model generation by using entailment tree prompting, where the model generates sentences that form a valid entailment tree. The paper abstract proposes METGEN, a module-based entailment tree generation framework that uses multiple modules and a reasoning controller to iteratively generate entailment trees for answer explanation in question answering.\n\nWhile both the project proposal and the paper abstract focus on generating entailment trees, their research problems and approaches are different. The project proposal targets improving logical consistency in general language model generation, while the paper abstract specifically aims to explain answers in question answering. The project proposal uses a single language model with prompting, whereas the paper abstract employs a module-based framework with separate modules and a reasoning controller.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
            "paperId": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
            "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
            "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",
            "year": 2023,
            "citationCount": 587,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the logical consistency of language model generation by explicitly modeling the logical structure of the generated text. The proposed approach is entailment tree prompting, where the language model generates sentences that form a valid entailment tree.\n\nThe research problem in the paper is enhancing language models' problem-solving abilities on tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. The proposed approach is Tree of Thoughts (ToT), which enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving.\n\nWhile both works aim to improve language model generation, the proposal focuses specifically on logical consistency, while the paper addresses problem-solving abilities in general. The proposed methods are also different: entailment tree prompting vs. Tree of Thoughts.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "aade40af0d85b0b4fe15c97f6222d5c2e4d6d9b3",
            "paperId": "aade40af0d85b0b4fe15c97f6222d5c2e4d6d9b3",
            "title": "Graph of Thoughts: Solving Elaborate Problems with Large Language Models",
            "abstract": "We introduce Graph of Thoughts (GoT): a framework that\nadvances prompting capabilities in large language models\n(LLMs) beyond those offered by paradigms such as \nChain-of-Thought or Tree of Thoughts (ToT). The key idea and \nprimary advantage of GoT is the ability to model the information \ngenerated by an LLM as an arbitrary graph, where units of \ninformation (\"LLM thoughts\") are vertices, and edges correspond\nto dependencies between these vertices. This approach enables \ncombining arbitrary LLM thoughts into synergistic outcomes, \ndistilling the essence of whole networks of thoughts,\nor enhancing thoughts using feedback loops. We illustrate\nthat GoT offers advantages over state of the art on different\ntasks, for example increasing the quality of sorting by 62%\nover ToT, while simultaneously reducing costs by >31%.\nWe ensure that GoT is extensible with new thought \ntransformations and thus can be used to spearhead new prompting\nschemes. This work brings the LLM reasoning closer to human \nthinking or brain mechanisms such as recurrence, both\nof which form complex networks",
            "year": 2023,
            "citationCount": 172,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Graph of Thoughts is introduced: a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts, and is ensured that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the logical consistency of text generated by language models using entailment trees. The approach is to prompt the language model to generate sentences that are entailed by the previous context and sentences, forming an entailment tree.\n\nThe research problem in the paper is solving complex problems using language models by modeling the generated information as a graph. The approach is to represent units of information as vertices and dependencies between them as edges, allowing for combining and distilling networks of thoughts.\n\nThe proposal focuses on logical entailment in text generation, while the paper focuses on general problem-solving using graph representations. The methods are different: entailment tree prompting vs. graph-based reasoning.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "4ea9ee6ff16e4c7da58d10f8a2322e6a5aaccdf5",
            "paperId": "4ea9ee6ff16e4c7da58d10f8a2322e6a5aaccdf5",
            "title": "Autonomous Tree-search Ability of Large Language Models",
            "abstract": "Large Language Models have excelled in remarkable reasoning capabilities with advanced prompting techniques, but they fall short on tasks that require exploration, strategic foresight, and sequential decision-making. Recent works propose to utilize external programs to define search logic, such that LLMs can perform passive tree search to solve more challenging reasoning tasks. Though impressive results have been achieved, there are several fundamental limitations of these approaches. First, passive tree searches are not efficient as they usually require multiple rounds of LLM API calls to solve one single problem. Moreover, passive search methods are not flexible since they need task-specific program designs. Then a natural question arises: can we maintain the tree-search capability of LLMs without the aid of external programs, and can still generate responses that clearly demonstrate the process of a tree-structure search? To this end, we propose a new concept called autonomous tree-search ability of LLM, which can automatically generate a response containing search trajectories for the correct answer. Concretely, we perform search trajectories using capable LLM API via a fixed system prompt, allowing them to perform autonomous tree-search (ATS) right out of the box. Experiments on 4 puzzle games demonstrate our method can achieve huge improvements. The ATS-BFS method outperforms the Chain of Thought approach by achieving an average accuracy improvement of 33%. Compared to Tree of Thoughts, it requires 65.6% or 47.7% less GPT-api cost to attain a comparable level of accuracy. Moreover, we have collected data using the ATS prompt method and fine-tuned LLaMA. This approach yield a greater improvement compared to the ones fine-tuned on CoT data. Specifically, it outperforms CoT-tuned LLaMAs by an average of 40.6% and 38.5% for LLaMA2-7B and LLaMA2-13B, respectively.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a new concept called autonomous tree-search ability of LLM, which can automatically generate a response containing search trajectories for the correct answer, allowing them to perform autonomousTree-search (ATS) right out of the box."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve the logical consistency of language model generation by using entailment tree prompting, where the model generates sentences that form a valid entailment tree with the initial context and previously generated sentences.\n\nThe paper proposes to enable large language models to perform autonomous tree search without external programs, such that the model can automatically generate a response containing search trajectories for the correct answer in tasks like puzzle games.\n\nThe two works have different research problems and approaches. The project focuses on logical entailment in language model generation, while the paper studies tree search abilities of language models for reasoning tasks.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "ba4aa83248a1d08b521392eb971e47d10b7c74e1",
            "paperId": "ba4aa83248a1d08b521392eb971e47d10b7c74e1",
            "title": "Boosting Logical Reasoning in Large Language Models through a New Framework: The Graph of Thought",
            "abstract": "Recent advancements in large-scale models, such as GPT-4, have showcased remarkable capabilities in addressing standard queries. However, when facing complex problems that require multi-step logical reasoning, their accuracy dramatically decreases. Current research has explored the realm of \\textit{prompting engineering} to bolster the inferential capacities of these models. Our paper unveils a pioneering prompting technique, dubbed \\textit{Graph of Thoughts (GoT)}. Through testing on a trio of escalating challenges: the 24-point game, resolution of high-degree polynomial equations, and derivation of formulas for recursive sequences, our method outperformed GPT-4, achieving accuracy improvements of $89.7\\%$, $86\\%$, and $56\\%$ for each respective task. Moreover, when juxtaposed with the state-of-the-art (SOTA) prompting method, \\textit{Tree of Thought (ToT)}, our approach registered an average accuracy boost of $23\\%$, $24\\%$, and $15\\%$.",
            "year": 2023,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper unveils a pioneering prompting technique, dubbed \\textit{Graph of Thoughts (GoT), which outperformed GPT-4 and when juxtaposed with the state-of-the-art (SOTA) prompting method, the Tree of Thought (ToT), registered an average accuracy boost."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the logical consistency of text generated by language models, and the proposed approach is to use entailment tree prompting to explicitly model the logical structure of the generated text.\n\nThe research problem in the paper is improving the logical reasoning abilities of large language models on complex multi-step problems, and the proposed approach is a new prompting technique called Graph of Thoughts (GoT).\n\nWhile both works aim to improve the reasoning capabilities of language models, the specific research problems and approaches are different. The proposal focuses on generating logically consistent text, while the paper focuses on solving complex multi-step problems. The proposal uses entailment tree prompting, while the paper uses a new Graph of Thoughts prompting technique.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "99ad11764421bfff6261c9df8526c8b5af82d794",
            "paperId": "99ad11764421bfff6261c9df8526c8b5af82d794",
            "title": "Improving Logical Consistency in Pre-Trained Language Models using Natural Language Inference",
            "abstract": "Current state-of-the-art pre-trained language models (PTLMs) contain rich and vast amounts of world knowledge, demonstrating an ability to extrapolate information from contextual texts and to accurately answer questions [1]. However, the latent factual understanding captured by PTLMs can be irrational and inconsistent, causing PTLMs to be prone to generating contradictory statements [2]. We demonstrate that natural language inference (NLI) can provide additional signal about contradictory statements output by a PTLM. We explore several approaches for aggregating the entailment and contradiction probabilities acquired through NLI on a batch of PTLM predicted answers and define a scoring heuristic that balances between the NLI output and the PTLM\u2019s confidence in its answers. Predictions whose scores are below a tuned threshold are revised before outputting final answers. In addition, we investigate methods for using these NLI probabilities to define a MaxSAT problem that, when optimized, yields corrected predictions. Our results demonstrate that a system that uses either of our approaches to revise PTLM answers has better accuracy and logical consistency than a vanilla PTLM.",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that natural language inference (NLI) can provide additional signal about contradictory statements output by a PTLM and that a system that uses either of these approaches to revise PTLM answers has better accuracy and logical consistency than a vanilla PTLM."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the logical consistency of text generated by language models, and the proposed approach is to use entailment tree prompting to generate sentences that are logically entailed by the context and previous sentences.\n\nThe research problem in the paper is also improving the logical consistency of text generated by language models, but the proposed approach is to use natural language inference (NLI) to detect and revise contradictory statements.\n\nWhile both works aim to improve logical consistency, the proposal focuses on generating logically entailed sentences, while the paper focuses on revising contradictory statements using NLI. The methods are quite different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "d48b29889241551e1ee6622fa78c3fa4159255dd",
            "paperId": "d48b29889241551e1ee6622fa78c3fa4159255dd",
            "title": "Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning",
            "abstract": "Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.",
            "year": 2022,
            "citationCount": 212,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Selection-Inference (SI) framework is proposed that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the logical consistency of language model generation, and the proposed approach is to use entailment tree prompting to explicitly model the logical structure of the generated text.\n\nThe research problem in the paper is improving the performance of language models on multi-step logical reasoning problems, and the proposed approach is to use a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules and alternates between selection and inference to generate interpretable, causal reasoning steps.\n\nWhile both works aim to improve the logical reasoning capabilities of language models, the specific research problems and approaches are different. The proposal focuses on generating logically consistent text, while the paper focuses on solving multi-step logical reasoning problems. The proposal uses entailment tree prompting, while the paper uses a Selection-Inference framework.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "529e997e0d9730c25ad4347502da7e5a753274b8",
            "paperId": "529e997e0d9730c25ad4347502da7e5a753274b8",
            "title": "Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference",
            "abstract": "While large pre-trained language models are powerful, their predictions often lack logical consistency across test inputs. For example, a state-of-the-art Macaw question-answering (QA) model answers Yes to Is a sparrow a bird? and Does a bird have feet? but answers No to Does a sparrow have feet?. To address this failure mode, we propose a framework, Consistency Correction through Relation Detection, or ConCoRD, for boosting the consistency and accuracy of pre-trained NLP models using pre-trained natural language inference (NLI) models without fine-tuning or re-training. Given a batch of test inputs, ConCoRD samples several candidate outputs for each input and instantiates a factor graph that accounts for both the model\u2019s belief about the likelihood of each answer choice in isolation and the NLI model\u2019s beliefs about pair-wise answer choice compatibility. We show that a weighted MaxSAT solver can efficiently compute high-quality answer choices under this factor graph, improving over the raw model\u2019s predictions. Our experiments demonstrate that ConCoRD consistently boosts accuracy and consistency of off-the-shelf closed-book QA and VQA models using off-the-shelf NLI models, notably increasing accuracy of LXMERT on ConVQA by 5% absolute. See the project website (https://ericmitchell.ai/emnlp-2022-concord/) for code and data.",
            "year": 2022,
            "citationCount": 19,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a framework, Consistency Correction through Relation Detection, or ConCoRD, for boosting the consistency and accuracy of pre- trained NLP models using pre-trained natural language inference (NLI) models without fine-tuning or re-training."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve the logical consistency of language model generation by using entailment tree prompting, where the model generates sentences that form a valid entailment tree. The paper tries to enhance the self-consistency and performance of pre-trained language models by using pre-trained natural language inference models to correct inconsistencies in the model's predictions.\n\nWhile both works aim to improve the logical consistency of language models, the project proposal focuses on generating logically consistent text, while the paper focuses on correcting inconsistencies in the model's predictions on existing text.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "fadc0a6bcf968ed2ac71f567a48cd302dd62adde",
            "paperId": "fadc0a6bcf968ed2ac71f567a48cd302dd62adde",
            "title": "RLET: A Reinforcement Learning Based Approach for Explainable QA with Entailment Trees",
            "abstract": "Interpreting the reasoning process from questions to answers poses a challenge in approaching explainable QA. A recently proposed structured reasoning format, entailment tree, manages to offer explicit logical deductions with entailment steps in a tree structure. To generate entailment trees, prior single pass sequence-to-sequence models lack visible internal decision probability, while stepwise approaches are supervised with extracted single step data and cannot model the tree as a whole. In this work, we propose RLET, a Reinforcement Learning based Entailment Tree generation framework, which is trained utilising the cumulative signals across the whole tree. RLET iteratively performs single step reasoning with sentence selection and deduction generation modules, from which the training signal is accumulated across the tree with elaborately designed aligned reward function that is consistent with the evaluation. To the best of our knowledge, we are the first to introduce RL into the entailment tree generation task. Experiments on three settings of the EntailmentBank dataset demonstrate the strength of using RL framework.",
            "year": 2022,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "RLET is proposed, a Reinforcement Learning based Entailment Tree generation framework, which is trained utilising the cumulative signals across the whole tree, and is the first to introduce RL into the entailment tree generation task."
            },
            "score": 6
        },
        {
            "id": "d5d23834e7f25a5c5ef7a74ce01792281c1521fc",
            "paperId": "d5d23834e7f25a5c5ef7a74ce01792281c1521fc",
            "title": "A Logical Pattern Memory Pre-trained Model for Entailment Tree Generation",
            "abstract": "Generating coherent and credible explanations remains a significant challenge in the field of AI. In recent years, researchers have delved into the utilization of entailment trees to depict explanations, which exhibit a reasoning process of how a hypothesis is deduced from the supporting facts. However, existing models often overlook the importance of generating intermediate conclusions with logical consistency from the given facts, leading to inaccurate conclusions and undermining the overall credibility of entailment trees. To address this limitation, we propose the logical pattern memory pre-trained model (LMPM). LMPM incorporates an external memory structure to learn and store the latent representations of logical patterns, which aids in generating logically consistent conclusions. Furthermore, to mitigate the influence of logically irrelevant domain knowledge in the Wikipedia-based data, we introduce an entity abstraction approach to construct the dataset for pre-training LMPM. The experimental results highlight the effectiveness of our approach in improving the quality of entailment tree generation. By leveraging logical entailment patterns, our model produces more coherent and reasonable conclusions that closely align with the underlying premises. Code and Data are released at https://github.com/YuanLi95/T5-LMPM",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The logical pattern memory pre-trained model (LMPM) incorporates an external memory structure to learn and store the latent representations of logical patterns, which aids in generating logically consistent conclusions."
            },
            "score": 6
        },
        {
            "id": "c49fd6cac5382cdbc2bc31be195e42bc28dc615d",
            "paperId": "c49fd6cac5382cdbc2bc31be195e42bc28dc615d",
            "title": "Tree of Clarifications: Answering Ambiguous Questions with Retrieval-Augmented Large Language Models",
            "abstract": "Questions in open-domain question answering are often ambiguous, allowing multiple interpretations. One approach to handling them is to identify all possible interpretations of the ambiguous question (AQ) and to generate a long-form answer addressing them all, as suggested by Stelmakh et al., (2022). While it provides a comprehensive response without bothering the user for clarification, considering multiple dimensions of ambiguity and gathering corresponding knowledge remains a challenge. To cope with the challenge, we propose a novel framework, Tree of Clarifications (ToC): It recursively constructs a tree of disambiguations for the AQ -- via few-shot prompting leveraging external knowledge -- and uses it to generate a long-form answer. ToC outperforms existing baselines on ASQA in a few-shot setup across the metrics, while surpassing fully-supervised baselines trained on the whole training set in terms of Disambig-F1 and Disambig-ROUGE. Code is available at https://github.com/gankim/tree-of-clarifications.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel framework, Tree of Clarifications (ToC), recursively constructs a tree of disambiguations for the AQ -- via few-shot prompting leveraging external knowledge -- and uses it to generate a long-form answer."
            },
            "score": 5
        },
        {
            "id": "1e98a9532d4e1fcf947d5b215e2cfabbf6cc41e0",
            "paperId": "1e98a9532d4e1fcf947d5b215e2cfabbf6cc41e0",
            "title": "Tree-Planner: Efficient Close-loop Task Planning with Large Language Models",
            "abstract": "This paper studies close-loop task planning, which refers to the process of generating a sequence of skills (a plan) to accomplish a specific goal while adapting the plan based on real-time observations. Recently, prompting Large Language Models (LLMs) to generate actions iteratively has become a prevalent paradigm due to its superior performance and user-friendliness. However, this paradigm is plagued by two inefficiencies: high token consumption and redundant error correction, both of which hinder its scalability for large-scale testing and applications. To address these issues, we propose Tree-Planner, which reframes task planning with LLMs into three distinct phases: plan sampling, action tree construction, and grounded deciding. Tree-Planner starts by using an LLM to sample a set of potential plans before execution, followed by the aggregation of them to form an action tree. Finally, the LLM performs a top-down decision-making process on the tree, taking into account real-time environmental information. Experiments show that Tree-Planner achieves state-of-the-art performance while maintaining high efficiency. By decomposing LLM queries into a single plan-sampling call and multiple grounded-deciding calls, a considerable part of the prompt are less likely to be repeatedly consumed. As a result, token consumption is reduced by 92.2% compared to the previously best-performing model. Additionally, by enabling backtracking on the action tree as needed, the correction process becomes more flexible, leading to a 40.5% decrease in error corrections. Project page: https://tree-planner.github.io/",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Tree-Planner is proposed, which reframes task planning with Large Language Models into three distinct phases: plan sampling, action tree construction, and grounded deciding, which achieves state-of-the-art performance while maintaining high efficiency."
            },
            "score": 5
        },
        {
            "id": "a706cd2053923114038142cbc62ddf4ec91a0293",
            "paperId": "a706cd2053923114038142cbc62ddf4ec91a0293",
            "title": "Tree Prompting: Efficient Task Adaptation without Fine-Tuning",
            "abstract": "Prompting language models (LMs) is the main interface for applying them to new tasks. However, for smaller LMs, prompting provides low accuracy compared to gradient-based finetuning. Tree Prompting is an approach to prompting which builds a decision tree of prompts, linking multiple LM calls together to solve a task. At inference time, each call to the LM is determined by efficiently routing the outcome of the previous call using the tree. Experiments on classification datasets show that Tree Prompting improves accuracy over competing methods and is competitive with fine-tuning. We also show that variants of Tree Prompting allow inspection of a model's decision-making process.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Tree Prompting improves accuracy over competing methods and is competitive with fine-tuning, and variants of Tree Prompting allow inspection of a model's decision-making process."
            },
            "score": 5
        },
        {
            "id": "2379d69c98548d8c291a53a9a932b5ea7911fbe5",
            "paperId": "2379d69c98548d8c291a53a9a932b5ea7911fbe5",
            "title": "Probabilistic coherence, logical consistency, and Bayesian learning: Neural language models as epistemic agents",
            "abstract": "It is argued that suitably trained neural language models exhibit key properties of epistemic agency: they hold probabilistically coherent and logically consistent degrees of belief, which they can rationally revise in the face of novel evidence. To this purpose, we conduct computational experiments with rankers: T5 models [Raffel et al. 2020] that are pretrained on carefully designed synthetic corpora. Moreover, we introduce a procedure for eliciting a model\u2019s degrees of belief, and define numerical metrics that measure the extent to which given degrees of belief violate (probabilistic, logical, and Bayesian) rationality constraints. While pretrained rankers are found to suffer from global inconsistency (in agreement with, e.g., [Jang et al. 2021]), we observe that subsequent self-training on auto-generated texts allows rankers to gradually obtain a probabilistically coherent belief system that is aligned with logical constraints. In addition, such self-training is found to have a pivotal role in rational evidential learning, too, for it seems to enable rankers to propagate a novel evidence item through their belief systems, successively re-adjusting individual degrees of belief. All this, we conclude, confirms the Rationality Hypothesis, i.e., the claim that suitable trained NLMs may exhibit advanced rational skills. We suggest that this hypothesis has empirical, yet also normative and conceptual ramifications far beyond the practical linguistic problems NLMs have originally been designed to solve.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Rationality Hypothesis, i.e., the claim that suitable trained NLMs may exhibit advanced rational skills, is suggested that this hypothesis has empirical, yet also normative and conceptual ramifications far beyond the practical linguistic problems NLMs have originally been designed to solve."
            },
            "score": 5
        },
        {
            "id": "f727f928e7e179307d8d4a1da2387393f2bd7915",
            "paperId": "f727f928e7e179307d8d4a1da2387393f2bd7915",
            "title": "Methods for Measuring, Updating, and Visualizing Factual Beliefs in Language Models",
            "abstract": "Language models can memorize a considerable amount of factual information during pretraining that can be elicited through prompting or finetuning models on tasks like question answering. In this paper, we discuss approaches to measuring model factual beliefs, updating incorrect factual beliefs in models, and visualizing graphical relationships between factual beliefs. Our main contributions include: (1) new metrics for evaluating belief-updating methods focusing on the logical consistency of beliefs, (2) a training objective for Sequential, Local, and Generalizing updates (SLAG) that improves the performance of existing hypernetwork approaches, and (3) the introduction of the belief graph, a new form of visualization for language models that shows relationships between stored model beliefs. Our experiments suggest that models show only limited consistency between factual beliefs, but update methods can both fix incorrect model beliefs and greatly improve their consistency. Although off-the-shelf optimizers are surprisingly strong belief-updating baselines, our learned optimizers can outperform them in more difficult settings than have been considered in past work.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experiments suggest that models show only limited consistency between factual beliefs, but update methods can both fix incorrect model beliefs and greatly improve their consistency, and off-the-shelf optimizers can outperform them in more difficult settings than have been considered in past work."
            },
            "score": 5
        },
        {
            "id": "89c3bd70ad33c4f8832f00ab98872b77861ee0ec",
            "paperId": "89c3bd70ad33c4f8832f00ab98872b77861ee0ec",
            "title": "Discovering Latent Knowledge in Language Models Without Supervision",
            "abstract": "Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms zero-shot accuracy by 4\\% on average. We also find that it cuts prompt sensitivity in half and continues to maintain high accuracy even when models are prompted to generate incorrect answers. Our results provide an initial step toward discovering what language models know, distinct from what they say, even when we don't have access to explicit ground truth labels.",
            "year": 2022,
            "citationCount": 131,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a method for accurately answering yes-no questions given only unlabeled model activations, and shows that despite using no supervision and no model outputs, the method can recover diverse knowledge represented in large language models."
            },
            "score": 5
        },
        {
            "id": "cd2e04598909158494e556823d9de8baa692cee2",
            "paperId": "cd2e04598909158494e556823d9de8baa692cee2",
            "title": "Towards Mitigating Hallucination in Large Language Models via Self-Reflection",
            "abstract": "Large language models (LLMs) have shown promise for generative and knowledge-intensive tasks including question-answering (QA) tasks. However, the practical deployment still faces challenges, notably the issue of\"hallucination\", where models generate plausible-sounding but unfaithful or nonsensical information. This issue becomes particularly critical in the medical domain due to the uncommon professional concepts and potential social risks involved. This paper analyses the phenomenon of hallucination in medical generative QA systems using widely adopted LLMs and datasets. Our investigation centers on the identification and comprehension of common problematic answers, with a specific emphasis on hallucination. To tackle this challenge, we present an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation. Through this feedback process, our approach steadily enhances the factuality, consistency, and entailment of the generated answers. Consequently, we harness the interactivity and multitasking ability of LLMs and produce progressively more precise and accurate answers. Experimental results on both automatic and human evaluation demonstrate the superiority of our approach in hallucination reduction compared to baselines.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper analyses the phenomenon of hallucination in medical generative QA systems using widely adopted LLMs and datasets and presents an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation that steadily enhances the factuality, consistency, and entailment of the generated answers."
            },
            "score": 5
        },
        {
            "id": "6f9a3b2d6dedca25ac2612dc5dddde29b636e6bd",
            "paperId": "6f9a3b2d6dedca25ac2612dc5dddde29b636e6bd",
            "title": "Active entailment encoding for explanation tree construction using parsimonious generation of hard negatives",
            "abstract": "Entailment trees have been proposed to simulate the human reasoning process of explanation generation in the context of open--domain textual question answering. However, in practice, manually constructing these explanation trees proves a laborious process that requires active human involvement. Given the complexity of capturing the line of reasoning from question to the answer or from claim to premises, the issue arises of how to assist the user in efficiently constructing multi--level entailment trees given a large set of available facts. In this paper, we frame the construction of entailment trees as a sequence of active premise selection steps, i.e., for each intermediate node in an explanation tree, the expert needs to annotate positive and negative examples of premise facts from a large candidate list. We then iteratively fine--tune pre--trained Transformer models with the resulting positive and tightly controlled negative samples and aim to balance the encoding of semantic relationships and explanatory entailment relationships. Experimental evaluation confirms the measurable efficiency gains of the proposed active fine--tuning method in facilitating entailment trees construction: up to 20\\% improvement in explanatory premise selection when compared against several alternatives.",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper frames the construction of entailment trees as a sequence of active premise selection steps, i.e., for each intermediate node in an explanation tree, the expert needs to annotate positive and negative examples of premise facts from a large candidate list, and iteratively fine--tune pre--trained Transformer models with the resulting positive and tightly controlled negative samples."
            },
            "score": 5
        },
        {
            "id": "eff9d7ed06f30f121d30ee13802a11f172ef66f4",
            "paperId": "eff9d7ed06f30f121d30ee13802a11f172ef66f4",
            "title": "Demystifying Chains, Trees, and Graphs of Thoughts",
            "abstract": "The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models' (LLM) performance through innovative prompting techniques. Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph. As illustrated with numerous examples, this paradigm significantly enhances the LLM's capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing. To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes. For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts. We then build the first taxonomy of structure-enhanced LLM reasoning schemes. We focus on identifying fundamental classes of harnessed structures, and we analyze the representations of these structures, algorithms executed with these structures, and many others. We refer to these structures as reasoning topologies, because their representation becomes to a degree spatial, as they are contained within the LLM context. Our study compares existing prompting schemes using the proposed taxonomy, discussing how certain design choices lead to different patterns in performance and cost. We also outline theoretical underpinnings, relationships between prompting and other parts of the LLM ecosystem such as knowledge bases, and the associated research challenges. Our work will help to advance future prompt engineering techniques.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work conducts an in-depth analysis of the prompt execution pipeline, and builds the first taxonomy of structure-enhanced LLM reasoning schemes, referring to these structures as reasoning topologies, because their representation becomes to a degree spatial, as they are contained within the LLM context."
            },
            "score": 5
        },
        {
            "id": "f197bf0fc2f228483f6af3285000d54d8d97f9eb",
            "paperId": "f197bf0fc2f228483f6af3285000d54d8d97f9eb",
            "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models",
            "abstract": "We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",
            "year": 2023,
            "citationCount": 336,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 4
        },
        {
            "id": "e670b3a7e4e8af79b8cd69a0b303d89917750765",
            "paperId": "e670b3a7e4e8af79b8cd69a0b303d89917750765",
            "title": "How Interpretable are Reasoning Explanations from Prompting Large Language Models?",
            "abstract": "Prompt Engineering has garnered significant attention for enhancing the performance of large language models across a multitude of tasks. Techniques such as the Chain-of-Thought not only bolster task performance but also delineate a clear trajectory of reasoning steps, offering a tangible form of explanation for the audience. Prior works on interpretability assess the reasoning chains yielded by Chain-of-Thought solely along a singular axis, namely faithfulness. We present a comprehensive and multifaceted evaluation of interpretability, examining not only faithfulness but also robustness and utility across multiple commonsense reasoning benchmarks. Likewise, our investigation is not confined to a single prompting technique; it expansively covers a multitude of prevalent prompting techniques employed in large language models, thereby ensuring a wide-ranging and exhaustive evaluation. In addition, we introduce a simple interpretability alignment technique, termed Self-Entailment-Alignment Chain-of-thought, that yields more than 70\\% improvements across multiple dimensions of interpretability. Code is available at https://github.com/SenticNet/CoT_interpretability",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A simple interpretability alignment technique is introduced, termed Self-Entailment-Alignment Chain-of-thought, that yields more than 70\\% improvements across multiple dimensions of interpretability."
            },
            "score": 4
        },
        {
            "id": "7aaf3d224bf7366567f0fc76cc82e02745de80d2",
            "paperId": "7aaf3d224bf7366567f0fc76cc82e02745de80d2",
            "title": "Tree-Based Hard Attention with Self-Motivation for Large Language Models",
            "abstract": "While large language models (LLMs) excel at understanding and generating plain text, they are not specifically tailored to handle hierarchical text structures. Extracting the task-desired property from their natural language responses typically necessitates additional processing steps. In fact, selectively comprehending the hierarchical structure of large-scale text is pivotal to understanding its substance. Aligning LLMs more closely with the classification or regression values of specific task through prompting also remains challenging. To this end, we propose a novel framework called Tree-Based Hard Attention with Self-Motivation for Large Language Models (TEAROOM). TEAROOM incorporates a tree-based hard attention mechanism for LLMs to process hierarchically structured text inputs. By leveraging prompting, it enables a frozen LLM to selectively focus on relevant leaves in relation to the root, generating a tailored symbolic representation of their relationship. Moreover, TEAROOM comprises a self-motivation strategy for another LLM equipped with a trainable adapter and a linear layer. The selected symbolic outcomes are integrated into another prompt, along with the predictive value of the task. We iteratively feed output values back into the prompt, enabling the trainable LLM to progressively approximate the golden truth. TEAROOM outperforms existing state-of-the-art methods in experimental evaluations across three benchmark datasets, showing its effectiveness in estimating task-specific properties. Through comprehensive experiments and analysis, we have validated the ability of TEAROOM to gradually approach the underlying golden truth through multiple inferences.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "TEAROOM outperforms existing state-of-the-art methods in experimental evaluations across three benchmark datasets, showing its effectiveness in estimating task-specific properties and the ability of TEAROOM to gradually approach the underlying golden truth through multiple inferences."
            },
            "score": 4
        },
        {
            "id": "875d71bae61a66f7e65a2b6d363b7a0a27a6ed25",
            "paperId": "875d71bae61a66f7e65a2b6d363b7a0a27a6ed25",
            "title": "Tree of Uncertain Thoughts Reasoning for Large Language Models",
            "abstract": "While the recently introduced Tree of Thoughts (ToT) has heralded advancements in allowing Large Language Models (LLMs) to reason through foresight and backtracking for global decision-making, it has overlooked the inherent local uncertainties in intermediate decision points or\"thoughts\". These local uncertainties, intrinsic to LLMs given their potential for diverse responses, remain a significant concern in the reasoning process. Addressing this pivotal gap, we introduce the Tree of Uncertain Thoughts (TouT) - a reasoning framework tailored for LLMs. Our TouT effectively leverages Monte Carlo Dropout to quantify uncertainty scores associated with LLMs' diverse local responses at these intermediate steps. By marrying this local uncertainty quantification with global search algorithms, TouT enhances the model's precision in response generation. We substantiate our approach with rigorous experiments on two demanding planning tasks: Game of 24 and Mini Crosswords. The empirical evidence underscores TouT's superiority over both ToT and chain-of-thought prompting methods.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Tree of Uncertain Thoughts (TouT) is introduced - a reasoning framework tailored for LLMs that effectively leverages Monte Carlo Dropout to quantify uncertainty scores associated with LLMs' diverse local responses at these intermediate steps and enhances the model's precision in response generation."
            },
            "score": 4
        },
        {
            "id": "a5246f262bc0919bbfbc4b0fa66e85be41d92712",
            "paperId": "a5246f262bc0919bbfbc4b0fa66e85be41d92712",
            "title": "Improving Language Models Meaning Understanding and Consistency by Learning Conceptual Roles from Dictionary",
            "abstract": "The non-humanlike behaviour of contemporary pre-trained language models (PLMs) is a leading cause undermining their trustworthiness. A striking phenomenon of such faulty behaviours is the generation of inconsistent predictions, which produces logically contradictory results, such as generating different predictions for texts delivering the same meaning or violating logical properties. Previous studies exploited data augmentation or implemented specialised loss functions to alleviate the issue. However, their usage is limited, because they consume expensive training resources for large-sized PLMs and can only handle a certain consistency type. To this end, we propose a practical approach that alleviates the inconsistent behaviour issue by fundamentally improving PLMs' meaning awareness. Based on the conceptual role theory, our method allows PLMs to capture accurate meaning by learning precise interrelationships between concepts from word-definition pairs in a dictionary. Next, we propose an efficient parameter integration technique that updates only a few additional parameters to combine the learned interrelationship with PLMs' pre-trained knowledge. Our experimental results reveal that the approach can concurrently improve multiple types of consistency, enables efficient knowledge integration, and easily applies to other languages.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The method allows PLMs to capture accurate meaning by learning precise interrelationships between concepts from word-definition pairs in a dictionary, and proposes an efficient parameter integration technique that updates only a few additional parameters to combine the learned interrelationship with PLMs' pre-trained knowledge."
            },
            "score": 4
        },
        {
            "id": "806b5882c983bd156a8c10bcd34fe285d8a0593b",
            "paperId": "806b5882c983bd156a8c10bcd34fe285d8a0593b",
            "title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
            "abstract": "Recently, large language models (LLMs), including notable models such as GPT-4 and burgeoning community models, have showcased significant general language understanding abilities. However, there has been a scarcity of attempts to assess the logical reasoning capacities of these LLMs, an essential facet of natural language understanding. To encourage further investigation in this area, we introduce GLoRE, a meticulously assembled General Logical Reasoning Evaluation benchmark comprised of 12 datasets that span three different types of tasks. Our experimental results show that compared to the performance of human and supervised fine-tuning, the logical reasoning capabilities of open LLM models necessitate additional improvement; ChatGPT and GPT-4 show a strong capability of logical reasoning, with GPT-4 surpassing ChatGPT by a large margin. We propose a self-consistency probing method to enhance the accuracy of ChatGPT and a fine-tuned method to boost the performance of an open LLM. We release the datasets and evaluation programs to facilitate future research.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "GLoRE is introduced, a meticulously assembled General Logical Reasoning Evaluation benchmark comprised of 12 datasets that span three different types of tasks and proposes a self-consistency probing method to enhance the accuracy of ChatGPT and a fine-tuned method to boost the performance of an open LLM."
            },
            "score": 4
        },
        {
            "id": "67590dc371a89bef960b7bd547110f43cbe7196e",
            "paperId": "67590dc371a89bef960b7bd547110f43cbe7196e",
            "title": "APOLLO: A Simple Approach for Adaptive Pretraining of Language Models for Logical Reasoning",
            "abstract": "Logical reasoning over text is an important ability that requires understanding the semantics of the text and reasoning through them to arrive at correct inferences. Prior works on pretraining language models to improve the logical reasoning ability require complex processing of training data (e.g., aligning symbolic knowledge to text), yielding task-specific data augmentation that is not easy to adapt to any general text corpus. In this work, we propose APOLLO, a simple adaptive pretraining approach to improve the logical reasoning skills of language models. We select a subset of Wikipedia for adaptive pretraining using a set of logical inference keywords as filter words. Further, we propose two self-supervised loss functions for training. First, we modify the masked language modeling loss only to mask specific parts-of-speech words that likely require higher-order reasoning to predict them. Second, we propose a sentence-level classification loss that teaches the model to distinguish between entailment and contradiction types of sentences. The proposed pretraining paradigm is both simple and independent of task formats. We demonstrate the effectiveness of APOLLO by comparing it with prior baselines on two logical reasoning datasets. APOLLO performs comparably on ReClor and outperforms baselines on LogiQA.",
            "year": 2022,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes APOLLO, a simple adaptive Pretraining approach to improve the logical reasoning skills of language models using a subset of Wikipedia for adaptive pretraining using a set of logical inference keywords as filter words."
            },
            "score": 4
        },
        {
            "id": "ec55e16e934e4a7c4527a1f54b39a680a88b392c",
            "paperId": "ec55e16e934e4a7c4527a1f54b39a680a88b392c",
            "title": "Limits for learning with language models",
            "abstract": "With the advent of large language models (LLMs), the trend in NLP has been to train LLMs on vast amounts of data to solve diverse language understanding and generation tasks. The list of LLM successes is long and varied. Nevertheless, several recent papers provide empirical evidence that LLMs fail to capture important aspects of linguistic meaning. Focusing on universal quantification, we provide a theoretical foundation for these empirical findings by proving that LLMs cannot learn certain fundamental semantic properties including semantic entailment and consistency as they are defined in formal semantics. More generally, we show that LLMs are unable to learn concepts beyond the first level of the Borel Hierarchy, which imposes severe limits on the ability of LMs, both large and small, to capture many aspects of linguistic meaning. This means that LLMs will operate without formal guarantees on tasks that require entailments and deep linguistic understanding.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that LLMs are unable to learn concepts beyond the first level of the Borel Hierarchy, which imposes severe limits on the ability of LMs, both large and small, to capture many aspects of linguistic meaning."
            },
            "score": 4
        },
        {
            "id": "10633748251b0a3763240337caa466da37772463",
            "paperId": "10633748251b0a3763240337caa466da37772463",
            "title": "When Truth Matters - Addressing Pragmatic Categories in Natural Language Inference (NLI) by Large Language Models (LLMs)",
            "abstract": "In this paper, we focus on the ability of large language models (LLMs) to accommodate different pragmatic sentence types, such as questions, commands, as well as sentence fragments for natural language inference (NLI). On the commonly used notion of logical inference, nothing can be inferred from a question, an order, or an incomprehensible sentence fragment. We find MNLI, arguably the most important NLI dataset, and hence models fine-tuned on this dataset, insensitive to this fact. Using a symbolic semantic parser, we develop and make publicly available, fine-tuning datasets designed specifically to address this issue, with promising results. We also make a first exploration of ChatGPT\u2019s concept of entailment.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper develops and makes publicly available, fine-tuning datasets designed specifically to address the ability of large language models to accommodate different pragmatic sentence types, such as questions, commands, as well as sentence fragments for natural language inference (NLI)."
            },
            "score": 4
        },
        {
            "id": "e79a1bfe9a5828b458b4d6715b3b9bf2e48fd6fe",
            "paperId": "e79a1bfe9a5828b458b4d6715b3b9bf2e48fd6fe",
            "title": "Logic-Based Inference With Phrase Abduction Using Vision-and-Language Models",
            "abstract": "Recognizing Textual Entailment (RTE) is among the most fundamental tasks in natural language processing applications, such as question answering and machine translation. One of the main challenges in logic-based approaches to this task is the lack of background knowledge. This study proposes a logical inference system with phrasal knowledge by comparing their visual representations based on the intuition that visual representations enable people to judge entailment relations. First, we obtain candidate phrase pairs for phrasal knowledge from logical inference. Second, using a vision-and-language model, we acquire the visual representations of these phrases in the form of images or embedding vectors. Finally, we compare these obtained visual representations to determine whether to inject the knowledge corresponding to the candidate. In addition to simple similarity between phrases, we also consider asymmetric relations when comparing visual representations. Our logical inference system improved accuracy on the SICK dataset compared with a previous logical inference system, SPSA (Selector of Predicates with Shared Arguments). Moreover, our asymmetric evaluation functions using vision-and-language models are effective at capturing the entailment relations of word pairs in HyperLex.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study proposes a logical inference system with phrasal knowledge by comparing their visual representations based on the intuition that visual representations enable people to judge entailment relations, and improves accuracy on the SICK dataset compared with a previous logical inference systems."
            },
            "score": 4
        },
        {
            "id": "149d93a9945cdbcd42ef66e693a2baeddaa96a76",
            "paperId": "149d93a9945cdbcd42ef66e693a2baeddaa96a76",
            "title": "It is a Bird Therefore it is a Robin: On BERT's Internal Consistency Between Hypernym Knowledge and Logical Words",
            "abstract": "The lexical knowledge of NLP systems should be tested (i) for their internal consistency (avoiding groundedness issues) and (ii) both for content words and logical words. In this paper we propose a new method to test the understanding of the hypernymy relationship by measuring its antisymmetry according to the models. Previous studies often rely only on the direct question (e.g., A robin is a ... ), where we argue a correct answer could only rely on col-locational cues, rather than hierarchical cues. We show how to control for this, and how it is important. We develop a method to ask similar questions about logical words that encode an entailment-like relation (e.g., because or therefore ). Our results show important weaknesses of BERT-like models on these semantic tasks.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new method to test the understanding of the hypernymy relationship by measuring its antisymmetry according to the models, and develops a method to ask similar questions about logical words that encode an entailment-like relation."
            },
            "score": 4
        },
        {
            "id": "622b064bd56d5be022f6dae9f7656fa8b658e0cf",
            "paperId": "622b064bd56d5be022f6dae9f7656fa8b658e0cf",
            "title": "From Complex to Simple: Unraveling the Cognitive Tree for Reasoning with Small Language Models",
            "abstract": "Reasoning is a distinctive human capacity, enabling us to address complex problems by breaking them down into a series of manageable cognitive steps. Yet, complex logical reasoning is still cumbersome for language models. Based on the dual process theory in cognitive science, we are the first to unravel the cognitive reasoning abilities of language models. Our framework employs an iterative methodology to construct a Cognitive Tree (CogTree). The root node of this tree represents the initial query, while the leaf nodes consist of straightforward questions that can be answered directly. This construction involves two main components: the implicit extraction module (referred to as the intuitive system) and the explicit reasoning module (referred to as the reflective system). The intuitive system rapidly generates multiple responses by utilizing in-context examples, while the reflective system scores these responses using comparative learning. The scores guide the intuitive system in its subsequent generation step. Our experimental results on two popular and challenging reasoning tasks indicate that it is possible to achieve a performance level comparable to that of GPT-3.5 (with 175B parameters), using a significantly smaller language model that contains fewer parameters (<=7B) than 5% of GPT-3.5.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work is the first to unravel the cognitive reasoning abilities of language models using an iterative methodology to construct a Cognitive Tree (CogTree), and indicates that it is possible to achieve a performance level comparable to that of GPT-3.5."
            },
            "score": 4
        },
        {
            "id": "fca92fe287c44c9ec79ca1f2762b0bf2e5e8df2b",
            "paperId": "fca92fe287c44c9ec79ca1f2762b0bf2e5e8df2b",
            "title": "Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models",
            "abstract": "Current literature, aiming to surpass the\"Chain-of-Thought\"approach, often resorts to an external modus operandi involving halting, modifying, and then resuming the generation process to boost Large Language Models' (LLMs) reasoning capacities. This mode escalates the number of query requests, leading to increased costs, memory, and computational overheads. Addressing this, we propose the Algorithm of Thoughts -- a novel strategy that propels LLMs through algorithmic reasoning pathways, pioneering a new mode of in-context learning. By employing algorithmic examples, we exploit the innate recurrence dynamics of LLMs, expanding their idea exploration with merely one or a few queries. Our technique outperforms earlier single-query methods and stands on par with a recent multi-query strategy that employs an extensive tree search algorithm. Intriguingly, our results suggest that instructing an LLM using an algorithm can lead to performance surpassing that of the algorithm itself, hinting at LLM's inherent ability to weave its intuition into optimized searches. We probe into the underpinnings of our method's efficacy and its nuances in application.",
            "year": 2023,
            "citationCount": 32,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Algorithm of Thoughts is proposed -- a novel strategy that propels LLMs through algorithmic reasoning pathways, pioneering a new mode of in-context learning and suggesting that instructing an LLM using an algorithm can lead to performance surpassing that of the algorithm itself."
            },
            "score": 4
        },
        {
            "id": "1eb1a8c7f88de27af224153f43ecdd41774600f2",
            "paperId": "1eb1a8c7f88de27af224153f43ecdd41774600f2",
            "title": "PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization",
            "abstract": "Highly effective, task-specific prompts are often heavily engineered by experts to integrate detailed instructions and domain insights based on a deep understanding of both instincts of large language models (LLMs) and the intricacies of the target task. However, automating the generation of such expert-level prompts remains elusive. Existing prompt optimization methods tend to overlook the depth of domain knowledge and struggle to efficiently explore the vast space of expert-level prompts. Addressing this, we present PromptAgent, an optimization method that autonomously crafts prompts equivalent in quality to those handcrafted by experts. At its core, PromptAgent views prompt optimization as a strategic planning problem and employs a principled planning algorithm, rooted in Monte Carlo tree search, to strategically navigate the expert-level prompt space. Inspired by human-like trial-and-error exploration, PromptAgent induces precise expert-level insights and in-depth instructions by reflecting on model errors and generating constructive error feedback. Such a novel framework allows the agent to iteratively examine intermediate prompts (states), refine them based on error feedbacks (actions), simulate future rewards, and search for high-reward paths leading to expert prompts. We apply PromptAgent to 12 tasks spanning three practical domains: BIG-Bench Hard (BBH), as well as domain-specific and general NLP tasks, showing it significantly outperforms strong Chain-of-Thought and recent prompt optimization baselines. Extensive analyses emphasize its capability to craft expert-level, detailed, and domain-insightful prompts with great efficiency and generalizability.",
            "year": 2023,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents PromptAgent, an optimization method that autonomously crafts prompts equivalent in quality to those handcrafted by experts, and applies it to 12 tasks spanning three practical domains, showing it significantly outperforms strong Chain-of-Thought and recent prompt optimization baselines."
            },
            "score": 4
        },
        {
            "id": "81bd1081df12c554f5b577677eb1a3975f728476",
            "paperId": "81bd1081df12c554f5b577677eb1a3975f728476",
            "title": "Recognizing Entailment and Contradiction by Tree-based Convolution",
            "abstract": "Recognizing entailment and contradiction between two sentences has wide applications in NLP. Traditional methods include featurerich classifiers or formal reasoning. However, they are usually limited in terms of accuracy and scope. Recently, the renewed prosperity of neural networks has made many improvements in a variety of NLP tasks. In our previous work, the tree-based convolutional neural network (TBCNN) has achieved high performance in several sentence-level classification tasks. But whether TBCNN is applicable to the recognition of entailment and contradiction between two sentences remains unknown. In this paper, we propose TBCNN-pair model to recognize entailment/contradiction. Experimental results on a large dataset verify the rationale of using TBCNN as the sentencelevel model; leveraging additional heuristics like element-wise product/difference further improves the accuracy. Our model outperforms previously published results by a large margin.",
            "year": 2015,
            "citationCount": 32,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results on a large dataset verify the rationale of using TBCNN as the sentencelevel model; leveraging additional heuristics like element-wise product/difference further improves the accuracy."
            },
            "score": 4
        },
        {
            "id": "9ffefdf1fcd780cb71450b0a7a29247c66aa87be",
            "paperId": "9ffefdf1fcd780cb71450b0a7a29247c66aa87be",
            "title": "The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning",
            "abstract": "Does prompting a large language model (LLM) like GPT-3 with explanations improve in-context learning? We study this question on two NLP tasks that involve reasoning over text, namely question answering and natural language inference. We test the performance of four LLMs on three textual reasoning datasets using prompts that include explanations in multiple different styles. For these tasks, we find that including explanations in the prompts for OPT, GPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to moderate accuracy improvements over standard few-show learning. However, text-davinci-002 is able to benefit more substantially. We further show that explanations generated by the LLMs may not entail the models' predictions nor be factually grounded in the input, even on simple tasks with extractive explanations. However, these flawed explanations can still be useful as a way to verify LLMs' predictions post-hoc. Through analysis in our three settings, we show that explanations judged by humans to be good--logically consistent with the input and the prediction--more likely cooccur with accurate predictions. Following these observations, we train calibrators using automatically extracted scores that assess the reliability of explanations, allowing us to improve performance post-hoc across all of our datasets.",
            "year": 2022,
            "citationCount": 95,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work studies two NLP tasks that involve reasoning over text, namely question answering and natural language inference, and shows that explanations judged by humans to be good--logically consistent with the input and the prediction--more likely cooccur with accurate predictions."
            },
            "score": 4
        },
        {
            "id": "87db51e8211fca4b8ded047a8dadf0dd051ef2ce",
            "paperId": "87db51e8211fca4b8ded047a8dadf0dd051ef2ce",
            "title": "Recognizing Textual Entailment by Soft Dependency Tree Matching",
            "abstract": "We present a rule-based method for recognizing entailment relation between a pair of text fragments by comparing their dependency tree structures. We used a dependency parser to generate the dependency triples of the text-hypothesis pairs. A dependency triple is an arc in the dependency parse tree. Each triple in the hypothesis is checked against all the triples in the text to find a matching pair. We have developed a number of matching rules after a detailed analysis of the PETE dataset, which we used for the experiments. A successful match satisfying any of these rules assigns a matching score of 1 to the child node of that particular arc in the hypothesis dependency tree. Then the dependency parse tree is traversed in post- order way to obtain the final entailment score at the root node. The scores of the leaf nodes are propagated from the bottom of the tree to the non-leaf nodes, up to the root node. The entailment score of the root node is compared against a predefined threshold value to make the entailment decision. Experimental results on the PETE dataset show an accuracy of 87.69% on the development set and 73.75% on the test set, which outperforms the state-of-the-art results reported on this dataset so far. We did not use any other NLP tools or knowledge sources, to emphasize the role of dependency parsing in recognizing textual entailment.",
            "year": 2015,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A rule-based method for recognizing entailment relation between a pair of text fragments by comparing their dependency tree structures and developed a number of matching rules after a detailed analysis of the PETE dataset, which was used for the experiments."
            },
            "score": 4
        },
        {
            "id": "fccf373d964e2494725ca5868a3d90e7fbd0e438",
            "paperId": "fccf373d964e2494725ca5868a3d90e7fbd0e438",
            "title": "Dependency tree matching with extended Tree edit distance with subtrees for textual entailment",
            "abstract": "A lot of natural language processing (NLP) applications require the computation of similarities between pairs of syntactic or semantic trees. Tree edit distance (TED), in this context, is considered to be one of the most effective techniques. However, its main drawback is that it deals with single node operations only. We therefore extended TED to deal with subtree transformation operations as well as single nodes. This makes the extended TED with subtree operations more effective and flexible than the standard TED, especially for applications that pay attention to relations among nodes (e.g. in linguistic trees, deleting a modifier subtree should be cheaper than the sum of deleting its components individually). The preliminary results of extended TED with subtree operations were encouraging compared with the standard one when tested on different examples of dependency trees.",
            "year": 2012,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work extended TED to deal with subtree transformation operations as well as single node operations only, which makes the extended TED with subtrees operations more effective and flexible than the standard TED."
            },
            "score": 4
        },
        {
            "id": "d0beedcf5e94420bb90c7149817b75d5f0945f8f",
            "paperId": "d0beedcf5e94420bb90c7149817b75d5f0945f8f",
            "title": "Efficiently Programming Large Language Models using SGLang",
            "abstract": "Large language models (LLMs) are increasingly used for complex tasks requiring multiple chained generation calls, advanced prompting techniques, control flow, and interaction with external environments. However, efficient systems for programming and executing these applications are lacking. To bridge this gap, we introduce SGLang, a Structured Generation Language for LLMs. SGLang is designed for the efficient programming of LLMs and incorporates primitives for common LLM programming patterns. We have implemented SGLang as a domain-specific language embedded in Python, and we developed an interpreter, a compiler, and a high-performance runtime for SGLang. These components work together to enable optimizations such as parallelism, batching, caching, sharing, and other compilation techniques. Additionally, we propose RadixAttention, a novel technique that maintains a Least Recently Used (LRU) cache of the Key-Value (KV) cache for all requests in a radix tree, enabling automatic KV cache reuse across multiple generation calls at runtime. SGLang simplifies the writing of LLM programs and boosts execution efficiency. Our experiments demonstrate that SGLang can speed up common LLM tasks by up to 5x, while reducing code complexity and enhancing control.",
            "year": 2023,
            "citationCount": 19,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "SGLang simplifies the writing of LLM programs and boosts execution efficiency, while reducing code complexity and enhancing control, and this work proposes RadixAttention, a novel technique that maintains a Least Recently Used cache of the Key-Value cache for all requests in a radix tree."
            },
            "score": 3
        },
        {
            "id": "8d806a91e5f2166ee6823eb7e6e8e56826b6776d",
            "paperId": "8d806a91e5f2166ee6823eb7e6e8e56826b6776d",
            "title": "NLPBench: Evaluating Large Language Models on Solving NLP Problems",
            "abstract": "Recent developments in large language models (LLMs) have shown promise in enhancing the capabilities of natural language processing (NLP). Despite these successes, there remains a dearth of research dedicated to the NLP problem-solving abilities of LLMs. To fill the gap in this area, we present a unique benchmarking dataset, NLPBench, comprising 378 college-level NLP questions spanning various NLP topics sourced from Yale University's prior final exams. NLPBench includes questions with context, in which multiple sub-questions share the same public information, and diverse question types, including multiple choice, short answer, and math. Our evaluation, centered on LLMs such as GPT-3.5/4, PaLM-2, and LLAMA-2, incorporates advanced prompting strategies like the chain-of-thought (CoT) and tree-of-thought (ToT). Our study reveals that the effectiveness of the advanced prompting strategies can be inconsistent, occasionally damaging LLM performance, especially in smaller models like the LLAMA-2 (13b). Furthermore, our manual assessment illuminated specific shortcomings in LLMs' scientific problem-solving skills, with weaknesses in logical decomposition and reasoning notably affecting results.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study presents a unique benchmarking dataset, NLPBench, comprising 378 college-level NLP questions spanning various NLP topics sourced from Yale University's prior final exams, and reveals that the effectiveness of the advanced prompting strategies can be inconsistent, occasionally damaging LLM performance."
            },
            "score": 3
        },
        {
            "id": "c218cd1772999517b137bbbc9872c4f67e540b7f",
            "paperId": "c218cd1772999517b137bbbc9872c4f67e540b7f",
            "title": "OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models",
            "abstract": "We conduct a thorough investigation into the reasoning capabilities of Large Language Models (LLMs), focusing specifically on the Open Pretrained Transformers (OPT) models as a representative of such models. Our study entails finetuning three different sizes of OPT on a carefully curated reasoning corpus, resulting in two sets of finetuned models: OPT-R, finetuned without explanations, and OPT-RE, finetuned with explanations. We then evaluate all models on 57 out-of-domain tasks drawn from the Super-NaturalInstructions benchmark, covering 26 distinct reasoning skills, utilizing three prompting techniques. Through a comprehensive grid of 27 configurations and 6,156 test evaluations, we investigate the dimensions of finetuning, prompting, and scale to understand the role of explanations on different reasoning skills. Our findings reveal that having explanations in the fewshot exemplar has no significant impact on the model\u2019s performance when the model is finetuned, while positively affecting the non-finetuned counterpart. Moreover, we observe a slight yet consistent increase in classification accuracy as we incorporate explanations during prompting and finetuning, respectively. Finally, we offer insights on which reasoning skills benefit the most from incorporating explanations during finetuning and prompting, such as Numerical (+20.4%) and Analogical (+13.9%) reasoning, as well as skills that exhibit negligible or negative effects.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is revealed that having explanations in the fewshot exemplar has no significant impact on the model\u2019s performance when the model is finetuned, while positively affecting the non-finetuned counterpart, and a slight yet consistent increase in classification accuracy as the authors incorporate explanations during prompting and finetuning."
            },
            "score": 3
        },
        {
            "id": "8dce801e67f2f8923dc1f5b3059f70b462f4e4cf",
            "paperId": "8dce801e67f2f8923dc1f5b3059f70b462f4e4cf",
            "title": "DetectGPT-SC: Improving Detection of Text Generated by Large Language Models through Self-Consistency with Masked Predictions",
            "abstract": "General large language models (LLMs) such as ChatGPT have shown remarkable success, but it has also raised concerns among people about the misuse of AI-generated texts. Therefore, an important question is how to detect whether the texts are generated by ChatGPT or by humans. Existing detectors are built on the assumption that there is a distribution gap between human-generated and AI-generated texts. These gaps are typically identified using statistical information or classifiers. In contrast to prior research methods, we find that large language models such as ChatGPT exhibit strong self-consistency in text generation and continuation. Self-consistency capitalizes on the intuition that AI-generated texts can still be reasoned with by large language models using the same logical reasoning when portions of the texts are masked, which differs from human-generated texts. Using this observation, we subsequently proposed a new method for AI-generated texts detection based on self-consistency with masked predictions to determine whether a text is generated by LLMs. This method, which we call DetectGPT-SC. We conducted a series of experiments to evaluate the performance of DetectGPT-SC. In these experiments, we employed various mask scheme, zero-shot, and simple prompt for completing masked texts and self-consistency predictions. The results indicate that DetectGPT-SC outperforms the current state-of-the-art across different tasks.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that large language models such as ChatGPT exhibit strong self-consistency in text generation and continuation, and this method, which is called DetectGPT-SC, outperforms the current state-of-the-art across different tasks."
            },
            "score": 3
        },
        {
            "id": "0b395ed1c8b284e551172b728e83cf257e33729a",
            "paperId": "0b395ed1c8b284e551172b728e83cf257e33729a",
            "title": "HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models",
            "abstract": "We introduce HallusionBench, a comprehensive benchmark designed for the evaluation of image-context reasoning. This benchmark presents significant challenges to advanced large visual-language models (LVLMs), such as GPT-4V(Vision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, by emphasizing nuanced understanding and interpretation of visual data. The benchmark comprises 346 images paired with 1129 questions, all meticulously crafted by human experts. We introduce a novel structure for these visual questions designed to establish control groups. This structure enables us to conduct a quantitative analysis of the models' response tendencies, logical consistency, and various failure modes. In our evaluation on HallusionBench, we benchmarked 15 different models, highlighting a 31.42% question-pair accuracy achieved by the state-of-the-art GPT-4V. Notably, all other evaluated models achieve accuracy below 16%. Moreover, our analysis not only highlights the observed failure modes, including language hallucination and visual illusion, but also deepens an understanding of these pitfalls. Our comprehensive case studies within HallusionBench shed light on the challenges of hallucination and illusion in LVLMs. Based on these insights, we suggest potential pathways for their future improvement. The benchmark and codebase can be accessed at https://github.com/tianyi-lab/HallusionBench.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This benchmark presents significant challenges to advanced large visual-language models (LVLMs), such as GPT-4V(Vision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, by emphasizing nuanced understanding and interpretation of visual data."
            },
            "score": 3
        },
        {
            "id": "80248c8c7cbb5bb1d2a508001108f3f15bb60430",
            "paperId": "80248c8c7cbb5bb1d2a508001108f3f15bb60430",
            "title": "Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models",
            "abstract": "Object hallucination has been an Achilles' heel which hinders the broader applications of large vision-language models (LVLMs). Object hallucination refers to the phenomenon that the LVLMs claim non-existent objects in the image. To mitigate the object hallucinations, instruction tuning and external model-based detection methods have been proposed, which either require large-scare computational resources or depend on the detection result of external models. However, there remains an under-explored field to utilize the LVLM itself to alleviate object hallucinations. In this work, we adopt the intuition that the LVLM tends to respond logically consistently for existent objects but inconsistently for hallucinated objects. Therefore, we propose a Logical Closed Loop-based framework for Object Hallucination Detection and Mitigation, namely LogicCheckGPT. In specific, we devise logical consistency probing to raise questions with logical correlations, inquiring about attributes from objects and vice versa. Whether their responses can form a logical closed loop serves as an indicator of object hallucination. As a plug-and-play method, it can be seamlessly applied to all existing LVLMs. Comprehensive experiments conducted on three benchmarks across four LVLMs have demonstrated significant improvements brought by our method, indicating its effectiveness and generality.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a Logical Closed Loop-based framework for Object Hallucination Detection and Mitigation, namely LogicCheckGPT, which is a plug-and-play method that can be seamlessly applied to all existing LVLMs."
            },
            "score": 3
        },
        {
            "id": "fbcbba5c5bd921e009c615046df562247b45a32a",
            "paperId": "fbcbba5c5bd921e009c615046df562247b45a32a",
            "title": "Fact-checking benchmark for the Russian Large Language Models",
            "abstract": "Modern text-generative language models are rapidly developing. They produce text of high quality and are used in many real-world applications. However, they still have several limitations, for instance, the length of the context, degeneration processes, lack of logical structure, and facts consistency. In this work, we focus on the fact-checking problem applied to the output of the generative models on classical downstream tasks, such as paraphrasing, summarization, text style transfer, etc. We define the task of internal fact-checking, set the criteria for factual consistency, and present the novel dataset for this task for the Russian language. The benchmark for internal fact-checking and several baselines are also provided. We research data augmentation approaches to extend the training set and compare classification methods on different augmented data sets.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work defines the task of internal fact-checking, set the criteria for factual consistency, and presents the novel dataset for this task for the Russian language, along with the benchmark for internal fact-checking and several baselines."
            },
            "score": 3
        },
        {
            "id": "0042b9380f7da8335be040a3516e4f6765320834",
            "paperId": "0042b9380f7da8335be040a3516e4f6765320834",
            "title": "TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning",
            "abstract": "It is challenging to perform question-answering over complex, multimodal content such as television clips. This is in part because current video-language models rely on single-modality reasoning, have lowered performance on long inputs, and lack interpetability. We propose TV-TREES, the first multimodal entailment tree generator. TV-TREES serves as an approach to video understanding that promotes interpretable joint-modality reasoning by producing trees of entailment relationships between simple premises directly entailed by the videos and higher-level conclusions. We then introduce the task of multimodal entailment tree generation to evaluate the reasoning quality of such methods. Our method's experimental results on the challenging TVQA dataset demonstrate intepretable, state-of-the-art zero-shot performance on full video clips, illustrating a best-of-both-worlds contrast to black-box methods.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "TV-TREES serves as an approach to video understanding that promotes interpretable joint-modality reasoning by producing trees of entailment relationships between simple premises directly entailed by the videos and higher-level conclusions."
            },
            "score": 3
        },
        {
            "id": "6514abaf6bdd1c31ac5dc4ad40760ff8422c6a4e",
            "paperId": "6514abaf6bdd1c31ac5dc4ad40760ff8422c6a4e",
            "title": "A Study on Training and Developing Large Language Models for Behavior Tree Generation",
            "abstract": "This paper presents an innovative exploration of the application potential of large language models (LLM) in addressing the challenging task of automatically generating behavior trees (BTs) for complex tasks. The conventional manual BT generation method is inefficient and heavily reliant on domain expertise. On the other hand, existing automatic BT generation technologies encounter bottlenecks related to task complexity, model adaptability, and reliability. In order to overcome these challenges, we propose a novel methodology that leverages the robust representation and reasoning abilities of LLMs. The core contribution of this paper lies in the design of a BT generation framework based on LLM, which encompasses the entire process, from data synthesis and model training to application developing and data verification. Synthetic data is introduced to train the BT generation model (BTGen model), enhancing its understanding and adaptability to various complex tasks, thereby significantly improving its overall performance. In order to ensure the effectiveness and executability of the generated BTs, we emphasize the importance of data verification and introduce a multilevel verification strategy. Additionally, we explore a range of agent design and development schemes with LLM as the central element. We hope that the work in this paper may provide a reference for the researchers who are interested in BT generation based on LLMs.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An innovative exploration of the application potential of large language models (LLM) in addressing the challenging task of automatically generating behavior trees (BTs) for complex tasks with a novel methodology that leverages the robust representation and reasoning abilities of LLMs."
            },
            "score": 3
        },
        {
            "id": "1ddeb500dd88d4b860b32bec1e2a85f8a53910d6",
            "paperId": "1ddeb500dd88d4b860b32bec1e2a85f8a53910d6",
            "title": "How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?",
            "abstract": "Scaling language models have revolutionized widespread NLP tasks, yet little comprehensively explored few-shot relation extraction with large language models. In this paper, we investigate principal methodologies, in-context learning and data generation, for few-shot relation extraction via GPT-3.5 through exhaustive experiments. To enhance few-shot performance, we further propose task-related instructions and schema-constrained data generation. We observe that in-context learning can achieve performance on par with previous prompt learning approaches, and data generation with the large language model can boost previous solutions to obtain new state-of-the-art few-shot results on four widely-studied relation extraction datasets. We hope our work can inspire future research for the capabilities of large language models in few-shot relation extraction. Code is available in https://github.com/zjunlp/DeepKE/tree/main/example/llm.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is observed that in-context learning can achieve performance on par with previous prompt learning approaches, and data generation with the large language model can boost previous solutions to obtain new state-of-the-art few-shot results on four widely-studied relation extraction datasets."
            },
            "score": 3
        },
        {
            "id": "300b01dc726fe8acbededd805501811d427920bd",
            "paperId": "300b01dc726fe8acbededd805501811d427920bd",
            "title": "X-PARADE: Cross-Lingual Textual Entailment and Information Divergence across Paragraphs",
            "abstract": "Understanding when two pieces of text convey the same information is a goal touching many subproblems in NLP, including textual entailment and fact-checking. This problem becomes more complex when those two pieces of text are in different languages. Here, we introduce X-PARADE (Cross-lingual Paragraph-level Analysis of Divergences and Entailments), the first cross-lingual dataset of paragraph-level information divergences. Annotators label a paragraph in a target language at the span level and evaluate it with respect to a corresponding paragraph in a source language, indicating whether a given piece of information is the same, new, or new but can be inferred. This last notion establishes a link with cross-language NLI. Aligned paragraphs are sourced from Wikipedia pages in different languages, reflecting real information divergences observed in the wild. Armed with our dataset, we investigate a diverse set of approaches for this problem, including token alignment from machine translation, textual entailment methods that localize their decisions, and prompting LLMs. Our results show that these methods vary in their capability to handle inferable information, but they all fall short of human performance.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "X-PARADE (Cross-lingual Paragraph-level Analysis of Divergences and Entailments), the first cross-lingual dataset of paragraph-level information divergences, is introduced and a diverse set of approaches for this problem are investigated, including token alignment from machine translation, textual entailment methods that localize their decisions, and prompting LLMs."
            },
            "score": 3
        },
        {
            "id": "d6315056d1ee5be30ae9fd424a28c28be2d1fd34",
            "paperId": "d6315056d1ee5be30ae9fd424a28c28be2d1fd34",
            "title": "Chinese Textual Entailment Recognition Based on Syntactic Tree Clipping",
            "abstract": null,
            "year": 2014,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results show that the feature on changing structure of syntactic tree is effective and efficient in Chinese textual entailment, and the method firstly clips the syntactic trees into minimum information trees and then computes syntactic matching similarity on them."
            },
            "score": 3
        },
        {
            "id": "120d35dc73689ad41e7f23ac57643794df5b658b",
            "paperId": "120d35dc73689ad41e7f23ac57643794df5b658b",
            "title": "Feature based Entailment Recognition for Malayalam Language Texts",
            "abstract": "Textual entailment is a relationship between two text fragments, namely, text/premise and hypothesis. It has applications in question answering systems, multi-document summarization, information retrieval systems, and social network analysis. In the era of the digital world, recognizing semantic variability is important in understanding inferences in texts. The texts are either in the form of sentences, posts, tweets, or user experiences. Hence understanding inferences from customer experiences helps companies in customer segmentation. The availability of digital information is ever-growing with textual data in almost all languages, including low resource languages. This work deals with various machine learning approaches applied to textual entailment recognition or natural language inference for Malayalam, a South Indian low resource language. A performance-based analysis using machine learning classification techniques such as Logistic Regression, Decision Tree, Support Vector Machine, Random Forest, AdaBoost, and Naive Bayes is done for the MaNLI (Malayalam Natural Language Inference) dataset. Different lexical and surface-level features are used for this binary and multiclass classification. With the increasing size of the dataset, there is a drop in the performance of feature-based classification. A comparison of feature-based models with deep learning approaches highlights this inference. The main focus here is the feature-based analysis with 14 different features and its comparison, essential to any NLP classification problem. Keywords\u2014Textual entailment; natural language inference; Malayalam language; machine learning; deep learning",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work deals with various machine learning approaches applied to textual entailment recognition or natural language inference for Malayalam, a South Indian low resource language and a comparison of feature-based models with deep learning approaches highlights this inference."
            },
            "score": 3
        },
        {
            "id": "99da2f941c0a606278463f1e5e60a2bdbaf8eb99",
            "paperId": "99da2f941c0a606278463f1e5e60a2bdbaf8eb99",
            "title": "Enabling Synergistic Knowledge Sharing and Reasoning in Large Language Models with Collaborative Multi-Agents",
            "abstract": "Despite the significant advancements in the field of Natural Language Processing (NLP), Large Language Models (LLMs) have shown limitations in performing complex tasks that require arithmetic, commonsense, and symbolic reasoning. Reasoning frameworks like ReAct, Chain-of-thought (CoT), Tree-of-thoughts (ToT), etc. have shown success but with limitations in solving long-form complex tasks. To address this, we pro-pose a knowledge-sharing and collaborative multi-agent assisted framework on LLMs that leverages the capabilities of existing reasoning frameworks and the collaborative skills of multi-agent systems (MASs). The objectives of the proposed framework are to overcome the limitations of LLMs, enhance their reasoning capabilities, and improve their performance in complex tasks. It involves generating natural language rationales and in-context few-shot learning via prompting, and integrates the reasoning techniques with efficient knowledge-sharing and communication-driven agent networks. The potential benefits of the proposed framework include saving time and money, improved efficiency for computationally intensive reasoning, and the ability to incor-porate multiple collaboration strategies for dynamically changing environments.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A knowledge-sharing and collaborative multi-agent assisted framework that leverages the capabilities of existing reasoning frameworks and the collaborative skills of multi-agent systems (MASs) to overcome the limitations of LLMs, enhance their reasoning capabilities, and improve their performance in complex tasks."
            },
            "score": 3
        },
        {
            "id": "9d3cbe8d25c8f093714f6e5cfaf5a9e8f2f938e9",
            "paperId": "9d3cbe8d25c8f093714f6e5cfaf5a9e8f2f938e9",
            "title": "Robot Behavior-Tree-Based Task Generation with Large Language Models",
            "abstract": "Nowadays, the behavior tree is gaining popularity as a representation for robot tasks due to its modularity and reusability. Designing behavior-tree tasks manually is time-consuming for robot end-users, thus there is a need for investigating automatic behavior-tree-based task generation. Prior behavior-tree-based task generation approaches focus on fixed primitive tasks and lack generalizability to new task domains. To cope with this issue, we propose a novel behavior-tree-based task generation approach that utilizes state-of-the-art large language models. We propose a Phase-Step prompt design that enables a hierarchical-structured robot task generation and further integrate it with behavior-tree-embedding-based search to set up the appropriate prompt. In this way, we enable an automatic and cross-domain behavior-tree task generation. Our behavior-tree-based task generation approach does not require a set of pre-defined primitive tasks. End-users only need to describe an abstract desired task and our proposed approach can swiftly generate the corresponding behavior tree. A full-process case study is provided to demonstrate our proposed approach. An ablation study is conducted to evaluate the effectiveness of our Phase-Step prompts. Assessment on Phase-Step prompts and the limitation of large language models are presented and discussed.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Phase-Step prompt design is proposed that enables a hierarchical-structured robot task generation and further integrate it with behavior-tree-embedding-based search to set up the appropriate prompt and enable an automatic and cross-domain behavior- tree task generation."
            },
            "score": 2
        },
        {
            "id": "26690e79bf8750d3fdd7c77bb77f5dca3d5de0ba",
            "paperId": "26690e79bf8750d3fdd7c77bb77f5dca3d5de0ba",
            "title": "What do Language Models know about word senses? Zero-Shot WSD with Language Models and Domain Inventories",
            "abstract": "Language Models are the core for almost any Natural Language Processing system nowadays. One of their particularities is their contextualized representations, a game changer feature when a disambiguation between word senses is necessary. In this paper we aim to explore to what extent language models are capable of discerning among senses at inference time. We performed this analysis by prompting commonly used Languages Models such as BERT or RoBERTa to perform the task of Word Sense Disambiguation (WSD). We leverage the relation between word senses and domains, and cast WSD as a textual entailment problem, where the different hypothesis refer to the domains of the word senses. Our results show that this approach is indeed effective, close to supervised systems.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This analysis leverages the relation between word senses and domains, and cast WSD as a textual entailment problem, where the different hypothesis refer to the domains of the word senses."
            },
            "score": 2
        },
        {
            "id": "352bbd9950778530c5a6f7127a42ec361583a940",
            "paperId": "352bbd9950778530c5a6f7127a42ec361583a940",
            "title": "KnowComp at SemEval-2023 Task 7: Fine-tuning Pre-trained Language Models for Clinical Trial Entailment Identification",
            "abstract": "In this paper, we present our system for the textual entailment identification task as a subtask of the SemEval-2023 Task 7: Multi-evidence Natural Language Inference for Clinical Trial Data.The entailment identification task aims to determine whether a medical statement affirms a valid entailment given a clinical trial premise or forms a contradiction with it.Since the task is inherently a text classification task, we propose a system that performs binary classification given a statement and its associated clinical trial.Our proposed system leverages a human-defined prompt to aggregate the information contained in the statement, section name, and clinical trials.Pre-trained language models are then finetuned on the prompted input sentences to learn to discriminate the inference relation between the statement and clinical trial.To validate our system, we conduct extensive experiments with a wide variety of pre-trained language models.Our best system is built on DeBERTa-v3-large, which achieves an F1 score of 0.764 and secures the fifth rank in the official leaderboard.Further analysis indicates that leveraging our designed prompt is effective, and our model suffers from a low recall.Our code and pre-trained models are available at [https://github.com/HKUST-KnowComp/NLI4CT](https://github.com/HKUST-KnowComp/NLI4CT).",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a system that performs binary classification given a statement and its associated clinical trial and leverages a human-defined prompt to aggregate the information contained in the statement, section name, and clinical trials."
            },
            "score": 2
        },
        {
            "id": "78252edd0e5fb80491f75290af99d6c3f7c6a6ea",
            "paperId": "78252edd0e5fb80491f75290af99d6c3f7c6a6ea",
            "title": "Efficient Toxic Content Detection by Bootstrapping and Distilling Large Language Models",
            "abstract": "Toxic content detection is crucial for online services to remove inappropriate content that violates community standards. To automate the detection process, prior works have proposed varieties of machine learning (ML) approaches to train Language Models (LMs) for toxic content detection. However, both their accuracy and transferability across datasets are limited. Recently, Large Language Models (LLMs) have shown promise in toxic content detection due to their superior zero-shot and few-shot in-context learning ability as well as broad transferability on ML tasks.\nHowever, efficiently designing prompts for LLMs remains challenging. Moreover, the high run-time cost of LLMs may hinder their deployments in production. To address these challenges, in this work, we propose BD-LLM, a novel and efficient approach to bootstrapping and distilling LLMs for toxic content detection. \nSpecifically, we design a novel prompting method named Decision-Tree-of-Thought (DToT) to bootstrap LLMs' detection performance and extract high-quality rationales. DToT can automatically select more fine-grained context to re-prompt LLMs when their responses lack confidence. Additionally, we use the rationales extracted via DToT to fine-tune student LMs. Our experimental results on various datasets demonstrate that DToT can improve the accuracy of LLMs by up to 4.6%. Furthermore, student LMs fine-tuned with rationales extracted via DToT outperform baselines on all datasets with up to 16.9% accuracy improvement, while being more than 60x smaller than conventional LLMs. Finally, we observe that student LMs fine-tuned with rationales exhibit better cross-dataset transferability.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work designs a novel prompting method named Decision-Tree-of-Thought (DToT) to bootstrap LLMs' detection performance and extract high-quality rationales, and observes that student LMs fine-tuned with rationales exhibit better cross-dataset transferability."
            },
            "score": 2
        },
        {
            "id": "74bbaacd31283c2ae5aeaef378133acefad229b1",
            "paperId": "74bbaacd31283c2ae5aeaef378133acefad229b1",
            "title": "Evaluating the Potential of Leading Large Language Models in Reasoning Biology Questions",
            "abstract": "Recent advances in Large Language Models (LLMs) have presented new opportunities for integrating Artificial General Intelligence (AGI) into biological research and education. This study evaluated the capabilities of leading LLMs, including GPT-4, GPT-3.5, PaLM2, Claude2, and SenseNova, in answering conceptual biology questions. The models were tested on a 108-question multiple-choice exam covering biology topics in molecular biology, biological techniques, metabolic engineering, and synthetic biology. Among the models, GPT-4 achieved the highest average score of 90 and demonstrated the greatest consistency across trials with different prompts. The results indicated GPT-4's proficiency in logical reasoning and its potential to aid biology research through capabilities like data analysis, hypothesis generation, and knowledge integration. However, further development and validation are still required before the promise of LLMs in accelerating biological discovery can be realized.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Evaluated LLMs indicated GPT-4's proficiency in logical reasoning and its potential to aid biology research through capabilities like data analysis, hypothesis generation, and knowledge integration, but further development and validation are still required."
            },
            "score": 2
        },
        {
            "id": "a2c66274c9f876d8af615129599866380525c725",
            "paperId": "a2c66274c9f876d8af615129599866380525c725",
            "title": "Generic Methodology for Formal Verification of UML Models",
            "abstract": "This paper discusses a Unified Modelling Language (UML) based formal verification methodology for early error detection in the model-based software development cycle. Our approach proposes a UML-based formal verification process utilising functional and behavioural modelling artifacts of UML. It reinforces these artifacts with formal model transition and property verification. The main contribution is a UML to Labelled Transition System (LTS) Translator application that automatically converts UML Statecharts to formal models. Property specifications are derived from system requirements and corresponding Computational Tree Logic (CTL)/Linear Temporal Logic (LTL) model checking procedure verifies property entailment in LTS. With its ability to verify CTL and LTL specifications, the methodology becomes generic for verifying all types of embedded system behaviours. The steep learning curve associated with formal methods is avoided through the automatic formal model generation and thus reduces the reluctance of using formal methods in software development projects. A case study of an embedded controller used in military applications validates the methodology. It establishes how the methodology finds its use in verifying the correctness and consistency of UML models before implementation.",
            "year": 2022,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This approach proposes a UML-based formal verification process utilising functional and behavioural modelling artifacts of UML that reinforces these artifacts with formal model transition and property verification and becomes generic for verifying all types of embedded system behaviours."
            },
            "score": 2
        },
        {
            "id": "cb648d482dbd1e6ad0b0f4da43aca71c06538d4f",
            "paperId": "cb648d482dbd1e6ad0b0f4da43aca71c06538d4f",
            "title": "Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise",
            "abstract": "In this paper, we introduce a novel dIffusion language modEl pre-training framework for text generation, which we call GENIE. GENIE is a large-scale pretrained diffusion language model that consists of an encoder and a diffusion-based decoder, which can generate text by gradually transforming a random noise sequence into a coherent text sequence. To pre-train GENIE on a large-scale language corpus, we design a new continuous paragraph denoise objective, which encourages the diffusion-decoder to reconstruct a clean text paragraph from a corrupted version, while preserving the semantic and syntactic coherence. We evaluate GENIE on four downstream text generation benchmarks, namely XSum, CNN/DailyMail, Gigaword, and CommonGen. Our experimental results show that GENIE achieves comparable performance with the state-of-the-art autoregressive models on these benchmarks, and generates more diverse text samples. The code and models of GENIE are available at https://github.com/microsoft/ProphetNet/tree/master/GENIE.",
            "year": 2022,
            "citationCount": 25,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "To pre-train GENIE on a large-scale language corpus, a new continuous paragraph denoise objective is designed, which encourages the diffusion-decoder to reconstruct a clean text paragraph from a corrupted version, while preserving the semantic and syntactic coherence."
            },
            "score": 2
        },
        {
            "id": "eb9b60db6832b00b9932584663bec104d6d415dd",
            "paperId": "eb9b60db6832b00b9932584663bec104d6d415dd",
            "title": "Tree-Based Representation and Generation of Natural and Mathematical Language",
            "abstract": "Mathematical language in scientific communications and educational scenarios is important yet relatively understudied compared to natural languages. Recent works on mathematical language focus either on representing stand-alone mathematical expressions, especially in their natural tree format, or mathematical reasoning in pre-trained natural language models. Existing works on jointly modeling and generating natural and mathematical languages simply treat mathematical expressions as text, without accounting for the rigid structural properties of mathematical expressions. In this paper, we propose a series of modifications to existing language models to jointly represent and generate text and math: representing mathematical expressions as sequences of node tokens in their operator tree format, using math symbol and tree position embeddings to preserve the semantic and structural properties of mathematical expressions, and using a constrained decoding method to generate mathematically valid expressions. We ground our modifications in GPT-2, resulting in a model MathGPT, and demonstrate that it outperforms baselines on mathematical expression generation tasks.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A series of modifications to existing language models are proposed to jointly represent and generate text and math: representing mathematical expressions as sequences of node tokens in their operator tree format, using math symbol and tree position embeddings to preserve the semantic and structural properties of mathematical expressions, and using a constrained decoding method to generate mathematically valid expressions."
            },
            "score": 2
        },
        {
            "id": "0bfc804e31eecfd77f45e4ee7f4d629fffdcd628",
            "paperId": "0bfc804e31eecfd77f45e4ee7f4d629fffdcd628",
            "title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs",
            "abstract": "Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.",
            "year": 2023,
            "citationCount": 206,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ToolLLM is introduced, a general tool-use framework encompassing data construction, model training, and evaluation, and a novel depth-first search-based decision tree algorithm that enables LLMs to evaluate multiple reasoning traces and expand the search space."
            },
            "score": 2
        },
        {
            "id": "0d070b78fed9c485959c20012f089b9f390b8504",
            "paperId": "0d070b78fed9c485959c20012f089b9f390b8504",
            "title": "Emotion Prompting for Speech Emotion Recognition",
            "abstract": "Speech Emotion Recognition (SER) classi\ufb01es speech into emotion categories such as: Happy, Angry. Most prior works for SER focused on how to mine compelling features to improve performance. However, these methods ignore the in\ufb02uence of emotional label information on SER. Recent studies have attempted to prompt pre-trained language models and yield good performance for NLP tasks. Nevertheless, few works have attempted to prompt pre-trained speech models (PSM) on speech tasks. In light of these, we propose a simple but effective prompt-based method that prompts PSM for SER. Firstly, we reframe SER as an entailment task. Next, we generate speech prompts and combine them with the raw audio to form the input for PSM. Finally, we build a multi-task learning framework to extract more compelling features by simultaneously performing automatic speech recognition (ASR) and SER. Experiments on the IEMOCAP benchmark show that our method outperforms state-of-the-art baselines on the SER task.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a simple but effective prompt-based method that prompts pre-trained speech models (PSM) on speech tasks and builds a multi-task learning framework to extract more compelling features by simultaneously performing automatic speech recognition (ASR) and SER."
            },
            "score": 2
        },
        {
            "id": "38dccb7d87bdc61204984a689c915411c04494a2",
            "paperId": "38dccb7d87bdc61204984a689c915411c04494a2",
            "title": "Prompt-based Text Entailment for Low-Resource Named Entity Recognition",
            "abstract": "Pre-trained Language Models (PLMs) have been applied in NLP tasks and achieve promising results. Nevertheless, the fine-tuning procedure needs labeled data of the target domain, making it difficult to learn in low-resource and non-trivial labeled scenarios. To address these challenges, we propose Prompt-based Text Entailment (PTE) for low-resource named entity recognition, which better leverages knowledge in the PLMs. We first reformulate named entity recognition as the text entailment task. The original sentence with entity type-specific prompts is fed into PLMs to get entailment scores for each candidate. The entity type with the top score is then selected as final label. Then, we inject tagging labels into prompts and treat words as basic units instead of n-gram spans to reduce time complexity in generating candidates by n-grams enumeration. Experimental results demonstrate that the proposed method PTE achieves competitive performance on the CoNLL03 dataset, and better than fine-tuned counterparts on the MIT Movie and Few-NERD dataset in low-resource settings.",
            "year": 2022,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 2
        },
        {
            "id": "b7c44cc6631f7a7b3c4b58cb9675457d57db996f",
            "paperId": "b7c44cc6631f7a7b3c4b58cb9675457d57db996f",
            "title": "Hypergraph Enhanced Knowledge Tree Prompt Learning for Next-Basket Recommendation",
            "abstract": "Next-basket recommendation (NBR) aims to infer the items in the next basket given the corresponding basket sequence. Existing NBR methods are mainly based on either message passing in a plain graph or transition modelling in a basket sequence. However, these methods only consider point-to-point binary item relations while item dependencies in real world scenarios are often in higher order. Additionally, the importance of the same item to different users varies due to variation of user preferences, and the relations between items usually involve various aspects. As pretrained language models (PLMs) excel in multiple tasks in natural language processing (NLP) and computer vision (CV), many researchers have made great efforts in utilizing PLMs to boost recommendation. However, existing PLM-based recommendation methods degrade when encountering Out-Of-Vocabulary (OOV) items. OOV items are those whose IDs are out of PLM's vocabulary and thus unintelligible to PLM. To settle the above challenges, we propose a novel method HEKP4NBR, which transforms the knowledge graph (KG) into prompts, namely Knowledge Tree Prompt (KTP), to help PLM encode the OOV item IDs in the user's basket sequence. A hypergraph convolutional module is designed to build a hypergraph based on item similarities measured by an MoE model from multiple aspects and then employ convolution on the hypergraph to model correlations among multiple items. Extensive experiments are conducted on HEKP4NBR on two datasets based on real company data and validate its effectiveness against multiple state-of-the-art methods.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel method HEKP4NBR is proposed, which transforms the knowledge graph (KG) into prompts, namely Knowledge Tree Prompt (KTP), to help PLM encode the OOV item IDs in the user's basket sequence."
            },
            "score": 2
        },
        {
            "id": "caf8c9e52658af46ee6651bf004d545bd379f087",
            "paperId": "caf8c9e52658af46ee6651bf004d545bd379f087",
            "title": "Enhancing Text-to-SQL Translation for Financial System Design",
            "abstract": "Text-to-SQL, the task of translating natural language questions into SQL queries, is part of various business processes. Its automation, which is an emerging challenge, will empower software practitioners to seamlessly interact with relational databases using natural language, thereby bridging the gap between business needs and software capabilities. In this paper, we consider Large Language Models (LLMs), which have achieved state of the art for various NLP tasks. Specifically, we benchmark Text-to-SQL performance, the evaluation methodologies, as well as input optimization (e.g., prompting). In light of the empirical observations that we have made, we propose two novel metrics that were designed to adequately measure the similarity between SQL queries. Overall, we share with the community various findings, notably on how to select the right LLM on Text-to-SQL tasks. We further demonstrate that a tree-based edit distance constitutes a reliable metric for assessing the similarity between generated SQL queries and the oracle for benchmarking Text2SQL approaches. This metric is important as it relieves researchers from the need to perform computationally expensive experiments such as executing generated queries as done in prior works. Our work implements financial domain use cases and, therefore contributes to the advancement of Text2SQL systems and their practical adoption in this domain.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that a tree-based edit distance constitutes a reliable metric for assessing the similarity between generated SQL queries and the oracle for benchmarking Text2SQL approaches, and contributes to the advancement of Text2SQL systems and their practical adoption in this domain."
            },
            "score": 2
        },
        {
            "id": "97782a67971c4ff1a74bf07e82fe20b2c4bf86c4",
            "paperId": "97782a67971c4ff1a74bf07e82fe20b2c4bf86c4",
            "title": "Revisiting Relation Extraction in the era of Large Language Models",
            "abstract": "Relation extraction (RE) is the core NLP task of inferring semantic relationships between entities from text. Standard supervised RE techniques entail training modules to tag tokens comprising entity spans and then predict the relationship between them. Recent work has instead treated the problem as a sequence-to-sequence task, linearizing relations between entities as target strings to be generated conditioned on the input. Here we push the limits of this approach, using larger language models (GPT-3 and Flan-T5 large) than considered in prior work and evaluating their performance on standard RE tasks under varying levels of supervision. We address issues inherent to evaluating generative approaches to RE by doing human evaluations, in lieu of relying on exact matching. Under this refined evaluation, we find that: (1) Few-shot prompting with GPT-3 achieves near SOTA performance, i.e., roughly equivalent to existing fully supervised models; (2) Flan-T5 is not as capable in the few-shot setting, but supervising and fine-tuning it with Chain-of-Thought (CoT) style explanations (generated via GPT-3) yields SOTA results. We release this model as a new baseline for RE tasks.",
            "year": 2023,
            "citationCount": 47,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work addresses issues inherent to evaluating generative approaches to RE by doing human evaluations, in lieu of relying on exact matching, and releases this model as a new baseline for RE tasks."
            },
            "score": 2
        },
        {
            "id": "24cc83df913c752bd7e38a5acffeb7be6c17f657",
            "paperId": "24cc83df913c752bd7e38a5acffeb7be6c17f657",
            "title": "Pre-trained Language Models Can be Fully Zero-Shot Learners",
            "abstract": "How can we extend a pre-trained model to many language understanding tasks, without labeled or additional unlabeled data? Pre-trained language models (PLMs) have been effective for a wide range of NLP tasks. However, existing approaches either require fine-tuning on downstream labeled datasets or manually constructing proper prompts. In this paper, we propose nonparametric prompting PLM (NPPrompt) for fully zero-shot language understanding. Unlike previous methods, NPPrompt uses only pre-trained language models and does not require any labeled data or additional raw corpus for further fine-tuning, nor does it rely on humans to construct a comprehensive set of prompt label words. We evaluate NPPrompt against previous major few-shot and zero-shot learning methods on diverse NLP tasks: including text classification, text entailment, similar text retrieval, paraphrasing, and multiple-choice question answering. Experimental results demonstrate that our NPPrompt outperforms the previous best fully zero-shot method by big margins, with absolute gains of 12.8% in accuracy on text classification and 15.6% on the GLUE benchmark. Our source code is available at https://anonymous.4open. science/r/NPPrompt.",
            "year": 2022,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes nonparametric prompting PLM (NPPrompt) for fully zero-shot language understanding, which uses only pre-trained language models and does not require any labeled data or additional raw corpus for further fine-tuning, nor does it rely on humans to construct a comprehensive set of prompt label words."
            },
            "score": 2
        },
        {
            "id": "69825c38f3e9528002146a005f73f56b4d5698d9",
            "paperId": "69825c38f3e9528002146a005f73f56b4d5698d9",
            "title": "On the Origin of LLMs: An Evolutionary Tree and Graph for 15, 821 Large Language Models",
            "abstract": "Since late 2022, Large Language Models (LLMs) have become very prominent with LLMs like ChatGPT and Bard receiving millions of users. Hundreds of new LLMs are announced each week, many of which are deposited to Hugging Face, a repository of machine learning models and datasets. To date, nearly 16,000 Text Generation models have been uploaded to the site. Given the huge influx of LLMs, it is of interest to know which LLM backbones, settings, training methods, and families are popular or trending. However, there is no comprehensive index of LLMs available. We take advantage of the relatively systematic nomenclature of Hugging Face LLMs to perform hierarchical clustering and identify communities amongst LLMs using n-grams and term frequency-inverse document frequency. Our methods successfully identify families of LLMs and accurately cluster LLMs into meaningful subgroups. We present a public web application to navigate and explore Constellation, our atlas of 15,821 LLMs. Constellation rapidly generates a variety of visualizations, namely dendrograms, graphs, word clouds, and scatter plots. Constellation is available at the following link: https://constellation.sites.stanford.edu/.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work takes advantage of the relatively systematic nomenclature of Hugging Face LLMs to perform hierarchical clustering and identify communities amongst LLMs using n-grams and term frequency-inverse document frequency."
            },
            "score": 1
        }
    ],
    "novelty": "yes"
}