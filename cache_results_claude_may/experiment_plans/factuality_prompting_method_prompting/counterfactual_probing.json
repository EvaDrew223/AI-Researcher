{
    "topic_description": "novel prompting methods that can improve factuality and reduce hallucination of large language models",
    "idea_name": "Counterfactual Probing",
    "raw_idea": {
        "Problem": "Large language models can generate plausible but incorrect information, especially when the input query contains counterfactual or hypothetical scenarios.",
        "Existing Methods": "Existing methods for improving factuality often struggle with counterfactual reasoning, as they rely on direct comparisons with factual knowledge bases or generate self-consistency checks that may not capture the nuances of counterfactual scenarios.",
        "Motivation": "We can improve the factuality of generated responses to counterfactual queries by explicitly probing the model's understanding of the counterfactual scenario and its implications. By prompting the model to reason about the differences between the counterfactual and factual scenarios, we can guide it towards generating more accurate and consistent responses.",
        "Proposed Method": "We propose Counterfactual Probing (CP), a prompting method designed to improve factuality in counterfactual scenarios. Given a counterfactual query, CP first prompts the model to identify the key differences between the counterfactual and factual scenarios. Then, it prompts the model to reason about the implications of those differences on the query at hand. Finally, CP prompts the model to generate a response that is consistent with the counterfactual scenario and its implications. CP can be applied iteratively to refine the response based on further probing of the model's understanding.",
        "Experiment Plan": "We will evaluate CP on a range of counterfactual reasoning datasets, such as TimeTravel, NLVR2, and CounterQA. We will compare CP to baseline methods such as direct prompting and self-consistency prompting, as well as state-of-the-art methods that use external knowledge bases. We will measure factuality using both automatic metrics (e.g., accuracy, consistency) and human evaluation of the generated responses' plausibility and coherence."
    },
    "full_experiment_plan": {
        "Title": "Counterfactual Probing: Improving Factuality in Counterfactual Scenarios via Iterative Probing",
        "Problem Statement": "Large language models can generate plausible but incorrect information, especially when the input query contains counterfactual or hypothetical scenarios. Existing methods for improving factuality often struggle with counterfactual reasoning, as they rely on direct comparisons with factual knowledge bases or generate self-consistency checks that may not capture the nuances of counterfactual scenarios.",
        "Motivation": "We can improve the factuality of generated responses to counterfactual queries by explicitly probing the model's understanding of the counterfactual scenario and its implications. By prompting the model to reason about the differences between the counterfactual and factual scenarios, we can guide it towards generating more accurate and consistent responses. Counterfactual Probing is inspired by the human cognitive process of mentally simulating alternative scenarios and reasoning about their implications.",
        "Proposed Method": "We propose Counterfactual Probing (CP), a prompting method designed to improve factuality in counterfactual scenarios. Given a counterfactual query, CP first prompts the model to identify the key differences between the counterfactual and factual scenarios. Then, it prompts the model to reason about the implications of those differences on the query at hand. Finally, CP prompts the model to generate a response that is consistent with the counterfactual scenario and its implications. CP can be applied iteratively to refine the response based on further probing of the model's understanding.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We will evaluate CP on a range of counterfactual reasoning datasets, such as TimeTravel, NLVR2, and CounterQA. These datasets contain counterfactual queries that require the model to reason about alternative scenarios and their implications.",
            "Step 2: Construct Prompts": "We will compare CP to baseline methods such as direct prompting and self-consistency prompting. For direct prompting, we simply prompt the model with the counterfactual query. For self-consistency prompting, we prompt the model to generate multiple responses and select the most consistent one. For CP, we construct the following prompts:\n1. Identify differences: \"What are the key differences between the counterfactual scenario '[query]' and the corresponding factual scenario?\"\n2. Reason about implications: \"How do the differences between the counterfactual and factual scenarios affect the answer to the query '[query]'?\"\n3. Generate consistent response: \"Given the counterfactual scenario '[query]' and its implications, provide a response that is consistent with the scenario.\"",
            "Step 3: Select Models": "We will evaluate CP on state-of-the-art language models such as GPT-3.5 (text-davinci-002), GPT-4, and PaLM. These models have shown strong performance on a wide range of natural language tasks, including counterfactual reasoning.",
            "Step 4: Run Experiments": "For each dataset and model combination, we will run the following experiments:\n1. Direct prompting: Prompt the model with the counterfactual query and generate a response.\n2. Self-consistency prompting: Prompt the model to generate multiple responses to the counterfactual query and select the most consistent one.\n3. Counterfactual Probing: Apply the CP prompts iteratively, refining the response at each step based on the model's understanding of the counterfactual scenario and its implications.\nWe will generate responses for all test examples in each dataset using each method.",
            "Step 5: Evaluate Results": "We will measure the factuality of the generated responses using both automatic metrics and human evaluation. For automatic evaluation, we will use accuracy on datasets with ground-truth answers (TimeTravel, CounterQA) and consistency metrics such as self-BLEU on open-ended datasets (NLVR2). For human evaluation, we will have annotators rate the plausibility and coherence of the generated responses on a Likert scale. We will compare the performance of CP to the baseline methods and state-of-the-art models that use external knowledge bases."
        },
        "Test Case Examples": {
            "Test Case 1": {
                "Query": "What if the Roman Empire never fell?",
                "Direct Prompting": "If the Roman Empire never fell, the world would be a very different place today. Latin might still be a widely spoken language, and Christianity may not have become as widespread. The Roman system of government and law could still be in use, and many of the borders and countries we know today would not exist. However, it's impossible to say exactly how things would have turned out, as there are too many variables to consider.",
                "Self-Consistency Prompting": "If the Roman Empire never fell, a few key things might be different today:\n1. Latin would likely be more widely spoken and used as an official language in many countries.\n2. The Roman system of government, with a Senate and elected officials, could still be in use.\n3. Christianity may not have spread as widely, as the Roman Empire initially persecuted Christians.\n4. Many modern borders and countries would not exist, as the Roman Empire would have continued to control a large area.\n5. Roman art, architecture, and engineering would have continued to develop and influence the world.\nHowever, it's difficult to predict exactly how history would have unfolded, as there are many factors that could have affected the course of events.",
                "Counterfactual Probing": "Step 1: The key difference between the counterfactual scenario of the Roman Empire never falling and the factual history is that in the counterfactual, the Roman Empire remains a dominant power, while in reality, it declined and fell apart due to various factors such as invasions, economic troubles, and overextension.\nStep 2: If the Roman Empire had never fallen, it would have continued to shape the political, cultural, and religious landscape of Europe and the Mediterranean region. Latin would likely be more prominent, Roman systems of government and law could still be in use, and Christianity may not have spread as widely. The borders and countries we know today would also be very different.\nStep 3: In a world where the Roman Empire never fell, the Latin language would likely still be widely used, especially in official contexts. The Roman Empire's influence on government, law, and culture would be much more prominent, with many of its systems and ideas still in use today. Christianity, which grew in the vacuum left by the fall of the Empire, may not have become as dominant without that opportunity. The map of Europe and the Mediterranean would also look very different, with the Roman Empire continuing to control a large area. However, it's impossible to say exactly how things would have turned out, as the course of history is shaped by many complex factors.",
                "Explanation": "The Counterfactual Probing response is more detailed and nuanced than the direct prompting and self-consistency baselines. By explicitly identifying the key differences between the counterfactual and factual scenarios, reasoning about their implications, and generating a consistent response, CP produces a more coherent and plausible answer that acknowledges the complexity of the question."
            },
            "Test Case 2": {
                "Query": "What if dinosaurs never went extinct?",
                "Direct Prompting": "If dinosaurs never went extinct, they would likely still be the dominant land animals on Earth. Mammals, including humans, may not have had the opportunity to evolve and thrive as they did in the absence of dinosaurs. The world would be a very different place, with dinosaurs of all shapes and sizes roaming the continents. However, it's hard to predict exactly how things would have turned out, as there are many factors to consider, such as climate change and other extinction events that could have affected the course of history.",
                "Self-Consistency Prompting": "If dinosaurs never went extinct, a few key things might be different today:\n1. Dinosaurs would still be the dominant land animals, with many different species inhabiting various ecosystems.\n2. Mammals may not have had the chance to evolve and diversify as they did in the absence of dinosaurs, so humans and other mammalian species might not exist.\n3. The world's ecosystems and food chains would be very different, as dinosaurs would be the main consumers and predators.\n4. The course of evolution would have been altered, leading to different adaptations and species than we see today.\n5. Human civilization, if it existed at all, would have developed very differently, as we would have had to coexist with and potentially compete with dinosaurs.\nHowever, it's difficult to say exactly how things would have turned out, as there are many variables and potential extinction events that could have changed the course of history.",
                "Counterfactual Probing": "Step 1: The key difference between the counterfactual scenario of dinosaurs never going extinct and the factual history is that in the counterfactual, dinosaurs remain the dominant land animals, while in reality, they were wiped out by the Cretaceous-Paleogene extinction event about 66 million years ago, which allowed mammals to diversify and thrive.\nStep 2: If dinosaurs had never gone extinct, they would have continued to dominate terrestrial ecosystems, potentially preventing the evolution and diversification of mammals. This would have had major implications for the evolution of life on Earth, as many of the species we know today, including humans, might not have had the opportunity to evolve. The world's ecosystems and food chains would also be drastically different, with dinosaurs playing a central role.\nStep 3: In a world where dinosaurs never went extinct, the dominant land animals would likely be a diverse array of dinosaur species, adapted to various ecological niches. Mammals, which in our timeline evolved to fill many of these niches after the extinction of the dinosaurs, might have remained small and insignificant, or not evolved at all. This would mean that humans and other mammalian species we are familiar with would probably not exist. The course of evolution would have been very different, with dinosaurs continuing to evolve and adapt to changing conditions over millions of years. If human-like intelligence did evolve, it would have had to contend with the presence of dinosaurs, potentially leading to a very different kind of civilization. However, the exact outcome is impossible to predict, as there are many other factors, such as climate change and other extinction events, that could have altered the course of history in unpredictable ways.",
                "Explanation": "The Counterfactual Probing response provides a more in-depth analysis of the implications of dinosaurs never going extinct. By breaking down the key differences between the counterfactual and factual scenarios, and reasoning through their consequences step-by-step, CP generates a more nuanced and scientifically grounded response that acknowledges the complexity and uncertainty inherent in the question."
            }
        },
        "Fallback Plan": "If the proposed Counterfactual Probing method does not outperform the baseline methods, we can conduct additional analyses to understand why. Some potential avenues for investigation include:\n1. Analyzing the quality and relevance of the generated differences between the counterfactual and factual scenarios. If the model fails to identify key differences, it may struggle to reason about their implications.\n2. Evaluating the coherence and plausibility of the model's reasoning about the implications of the counterfactual scenario. If the model's reasoning is flawed or inconsistent, it may lead to less factual responses.\n3. Assessing the consistency and factuality of the final generated response. If the response fails to incorporate the insights from the probing steps, it may not be more factual than the baseline methods.\nBased on these analyses, we can refine the Counterfactual Probing prompts and procedure to better guide the model's reasoning. For example, we could provide more explicit instructions for identifying key differences, or break down the reasoning process into smaller sub-steps.\nIf refinements to the CP method still do not yield improvements, we could pivot to an analysis of why counterfactual reasoning remains challenging for language models, even with explicit probing. This could involve comparing the model's performance on different types of counterfactual scenarios (e.g., historical, scientific, social), or investigating the role of the model's training data and architecture in its ability to reason about alternative possibilities.\nUltimately, if Counterfactual Probing does not prove effective, the project could shift to a more general analysis of the limitations and challenges of counterfactual reasoning in language models, with the goal of informing future research directions in this area."
    },
    "novelty_queries": [
        "KeywordQuery(\"counterfactual probing language models\")",
        "KeywordQuery(\"improving factuality counterfactual scenarios language models\")",
        "KeywordQuery(\"iterative probing counterfactual reasoning language models\")",
        "KeywordQuery(\"Counterfactual Probing NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "f8e99be4f9a01761fab74bade2c3c18de9fc686b",
            "paperId": "f8e99be4f9a01761fab74bade2c3c18de9fc686b",
            "title": "Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks",
            "abstract": "The impressive performance of recent language models across a wide range of tasks suggests that they possess a degree of abstract reasoning skills. Are these skills general and transferable, or specialized to specific tasks seen during pretraining? To disentangle these effects, we propose an evaluation framework based on\"counterfactual\"task variants that deviate from the default assumptions underlying standard tasks. Across a suite of 11 tasks, we observe nontrivial performance on the counterfactual variants, but nevertheless find that performance substantially and consistently degrades compared to the default conditions. This suggests that while current LMs may possess abstract task-solving skills to an extent, they often also rely on narrow, non-transferable procedures for task-solving. These results motivate a more careful interpretation of language model performance that teases apart these aspects of behavior.",
            "year": 2023,
            "citationCount": 69,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An evaluation framework based on task variants that deviate from the default assumptions underlying standard tasks that suggests that while current LMs may possess abstract task-solving skills to an extent, they often also rely on narrow, non-transferable procedures for task-solving."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the factuality of language models in counterfactual scenarios, and the proposed approach is to use iterative probing to guide the model's reasoning. The research problem in the paper is evaluating the reasoning capabilities and limitations of language models using counterfactual task variants.\n\nWhile both works involve counterfactual scenarios, the proposal focuses on improving model performance through a specific probing method, while the paper aims to analyze and understand model behavior across various counterfactual tasks. The proposal does not directly address the paper's goal of disentangling general reasoning skills from task-specific procedures.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "eebdf7303256f081ab1f6a36ff0ea6126e4da484",
            "paperId": "eebdf7303256f081ab1f6a36ff0ea6126e4da484",
            "title": "CRASS: A Novel Data Set and Benchmark to Test Counterfactual Reasoning of Large Language Models",
            "abstract": "We introduce the CRASS (counterfactual reasoning assessment) data set and benchmark utilizing questionized counterfactual conditionals as a novel and powerful tool to evaluate large language models. We present the data set design and benchmark. We test six state-of-the-art models against our benchmark. Our results show that it poses a valid challenge for these models and opens up considerable room for their improvement.",
            "year": 2021,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The CRASS data set and benchmark utilizing questionized counterfactual conditionals is introduced as a novel and powerful tool to evaluate large language models and poses a valid challenge for these models and opens up considerable room for their improvement."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the factuality of language models in counterfactual scenarios, and the proposed approach is iterative probing to guide the model's reasoning. The research problem in the paper is evaluating the counterfactual reasoning abilities of language models, and the approach is introducing a new dataset and benchmark.\n\nThe proposal focuses on improving model performance on counterfactual queries, while the paper focuses on evaluating current models' counterfactual reasoning abilities. Although both deal with counterfactual reasoning, the proposal aims to develop a new method for improvement, while the paper introduces a new evaluation dataset and benchmark.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "b69ae70abf1e8519a565c40bed07ab499576a8f6",
            "paperId": "b69ae70abf1e8519a565c40bed07ab499576a8f6",
            "title": "Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models",
            "abstract": "Large language models (LLMs) have performed well on several reasoning benchmarks, including ones that test analogical reasoning abilities. However, it has been debated whether they are actually performing humanlike abstract reasoning or instead employing less general processes that rely on similarity to what has been seen in their training data. Here we investigate the generality of analogy-making abilities previously claimed for LLMs (Webb, Holyoak,&Lu, 2023). We take one set of analogy problems used to evaluate LLMs and create a set of\"counterfactual\"variants-versions that test the same abstract reasoning abilities but that are likely dissimilar from any pre-training data. We test humans and three GPT models on both the original and counterfactual problems, and show that, while the performance of humans remains high for all the problems, the GPT models' performance declines sharply on the counterfactual set. This work provides evidence that, despite previously reported successes of LLMs on analogical reasoning, these models lack the robustness and generality of human analogy-making.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work takes one set of analogy problems used to evaluate LLMs and creates a set of \"counterfactual\"variants-versions that test the same abstract reasoning abilities but that are likely dissimilar from any pre-training data, providing evidence that these models lack the robustness and generality of human analogy-making."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the factuality of language models in counterfactual scenarios, and the proposed approach is to use iterative probing to guide the model's reasoning. The research problem in the paper is evaluating the generality of analogical reasoning abilities in language models, and the approach is to test the models on counterfactual variants of analogy problems.\n\nThe proposal focuses on improving language models' factuality in counterfactual scenarios, while the paper aims to evaluate the generality of their analogical reasoning abilities. Although both involve counterfactuals, the proposal's goal is to enhance the models' performance, while the paper's goal is to test the robustness of their existing capabilities.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "e67dd9230cd4a8199ca6c9c9ebaa31ab29be23d2",
            "paperId": "e67dd9230cd4a8199ca6c9c9ebaa31ab29be23d2",
            "title": "Evidence from counterfactual tasks supports emergent analogical reasoning in large language models",
            "abstract": "We recently reported evidence that large language models are capable of solving a wide range of text-based analogy problems in a zero-shot manner, indicating the presence of an emergent capacity for analogical reasoning. Two recent commentaries have challenged these results, citing evidence from so-called `counterfactual' tasks in which the standard sequence of the alphabet is arbitrarily permuted so as to decrease similarity with materials that may have been present in the language model's training data. Here, we reply to these critiques, clarifying some misunderstandings about the test materials used in our original work, and presenting evidence that language models are also capable of generalizing to these new counterfactual task variants.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving factuality in counterfactual scenarios using iterative probing, while the paper is about emergent analogical reasoning in large language models and their ability to generalize to counterfactual tasks. Although both involve counterfactual reasoning, the proposal focuses on improving factuality through probing, while the paper studies the models' inherent analogical reasoning capabilities.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "eb44ce1f7e1f4deac10f6e7009e2073f1eb0b3e4",
            "paperId": "eb44ce1f7e1f4deac10f6e7009e2073f1eb0b3e4",
            "title": "The Linear Representation Hypothesis and the Geometry of Large Language Models",
            "abstract": "Informally, the 'linear representation hypothesis' is the idea that high-level concepts are represented linearly as directions in some representation space. In this paper, we address two closely related questions: What does\"linear representation\"actually mean? And, how do we make sense of geometric notions (e.g., cosine similarity or projection) in the representation space? To answer these, we use the language of counterfactuals to give two formalizations of\"linear representation\", one in the output (word) representation space, and one in the input (sentence) space. We then prove these connect to linear probing and model steering, respectively. To make sense of geometric notions, we use the formalization to identify a particular (non-Euclidean) inner product that respects language structure in a sense we make precise. Using this causal inner product, we show how to unify all notions of linear representation. In particular, this allows the construction of probes and steering vectors using counterfactual pairs. Experiments with LLaMA-2 demonstrate the existence of linear representations of concepts, the connection to interpretation and control, and the fundamental role of the choice of inner product.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper addresses two closely related questions: What does \"linear representation\"actually mean?"
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve the factuality of language models in counterfactual scenarios by iteratively probing the model's understanding and reasoning about the implications of the counterfactual scenario.\n\nThe paper abstract proposes formalizing the notion of \"linear representation\" in language models using counterfactuals and connecting it to linear probing and model steering.\n\nWhile both involve counterfactuals, the project focuses on improving factuality in counterfactual scenarios, while the paper focuses on understanding and formalizing linear representations in language models.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "7e7cc29b042793b27688beb765dc604dee65d536",
            "paperId": "7e7cc29b042793b27688beb765dc604dee65d536",
            "title": "Flexible text generation for counterfactual fairness probing",
            "abstract": "A common approach for testing fairness issues in text-based classifiers is through the use of counterfactuals: does the classifier output change if a sensitive attribute in the input is changed? Existing counterfactual generation methods typically rely on wordlists or templates, producing simple counterfactuals that fail to take into account grammar, context, or subtle sensitive attribute references, and could miss issues that the wordlist creators had not considered. In this paper, we introduce a task for generating counterfactuals that overcomes these shortcomings, and demonstrate how large language models (LLMs) can be leveraged to accomplish this task. We show that this LLM-based method can produce complex counterfactuals that existing methods cannot, comparing the performance of various counterfactual generation methods on the Civil Comments dataset and showing their value in evaluating a toxicity classifier.",
            "year": 2022,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that this LLM-based method can produce complex counterfactuals that existing methods cannot, comparing the performance of various counterfactUAL generation methods on the Civil Comments dataset and showing their value in evaluating a toxicity classifier."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving factuality in counterfactual scenarios via iterative probing, while the paper focuses on generating complex counterfactuals to test fairness issues in text classifiers. The proposal's approach is to use prompts to guide the model to identify differences, reason about implications, and generate consistent responses, whereas the paper leverages large language models to generate counterfactuals that take into account grammar, context, and subtle sensitive attribute references.\n\nThe proposal aims to improve the model's factuality in counterfactual reasoning, while the paper aims to generate more comprehensive counterfactuals for evaluating fairness in text classifiers. Although both involve counterfactuals, their research problems and approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "9028fd54ecbbd58da6f3d86629b48bb95405fff2",
            "paperId": "9028fd54ecbbd58da6f3d86629b48bb95405fff2",
            "title": "Counterfactual reasoning: Testing language models\u2019 understanding of hypothetical scenarios",
            "abstract": "Current pre-trained language models have enabled remarkable improvements in downstream tasks, but it remains difficult to distinguish effects of statistical correlation from more systematic logical reasoning grounded on the understanding of real world. We tease these factors apart by leveraging counterfactual conditionals, which force language models to predict unusual consequences based on hypothetical propositions. We introduce a set of tests from psycholinguistic experiments, as well as larger-scale controlled datasets, to probe counterfactual predictions from five pre-trained language models. We find that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge\u2014however, we also find that for most models this effect appears largely to be driven by simple lexical cues. When we mitigate effects of both world knowledge and lexical cues to test knowledge of linguistic nuances of counterfactuals, we find that only GPT-3 shows sensitivity to these nuances, though this sensitivity is also non-trivially impacted by lexical associative factors.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge\u2014however, it is also found that for most models this effect appears largely to be driven by simple lexical cues."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the factuality of language models in counterfactual scenarios, and the proposed approach is to use iterative probing to guide the model's reasoning. The research problem in the paper is testing language models' understanding of counterfactual scenarios, and the approach is to use psycholinguistic tests and controlled datasets to probe the models' predictions.\n\nWhile both works deal with counterfactual reasoning in language models, the proposal focuses on improving factuality through an iterative probing method, while the paper aims to test the models' understanding using existing tests and datasets. The proposal is more application-oriented, while the paper is more analysis-oriented.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "d3637f23d15e71bc1b16948a4c29f08c90b8c772",
            "paperId": "d3637f23d15e71bc1b16948a4c29f08c90b8c772",
            "title": "PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning",
            "abstract": "Procedural planning, which entails decomposing a high-level goal into a sequence of temporally ordered steps, is an important yet intricate task for machines. It involves integrating common-sense knowledge to reason about complex contextualized situations that are often counterfactual, e.g.\"scheduling a doctor's appointment without a phone\". While current approaches show encouraging results using large language models (LLMs), they are hindered by drawbacks such as costly API calls and reproducibility issues. In this paper, we advocate planning using smaller language models. We present PlaSma, a novel two-pronged approach to endow small language models with procedural knowledge and (counterfactual) planning capabilities. More concretely, we develop symbolic procedural knowledge distillation to enhance the implicit knowledge in small language models and an inference-time algorithm to facilitate more structured and accurate reasoning. In addition, we introduce a novel task, Counterfactual Planning, that requires a revision of a plan to cope with a counterfactual situation. In both the original and counterfactual setting, we show that orders-of-magnitude smaller models (770M-11B parameters) can compete and often surpass their larger teacher models' capabilities.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "PlaSma is presented, a novel two-pronged approach to endow small language models with procedural knowledge and (counterfactual) planning capabilities and symbolic procedural knowledge distillation is developed to enhance the implicit knowledge insmall language models and an inference-time algorithm to facilitate more structured and accurate reasoning."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving factuality in counterfactual scenarios using language models, and the proposed approach is iterative probing to guide the model's reasoning. The research problem in the paper is endowing small language models with procedural knowledge and counterfactual planning capabilities, and the proposed approach is symbolic procedural knowledge distillation and an inference-time algorithm.\n\nWhile both works involve counterfactual reasoning with language models, the specific research problems and approaches are different. The proposal focuses on improving factuality through iterative probing, while the paper aims to enhance small language models' planning capabilities through knowledge distillation and a novel inference algorithm.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "e5c72b92c48d68594b290c84a8904da7c8335554",
            "paperId": "e5c72b92c48d68594b290c84a8904da7c8335554",
            "title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback",
            "abstract": "Large language models (LLMs), such as ChatGPT, are able to generate human-like, fluent responses for many downstream tasks, e.g., task-oriented dialog and question answering. However, applying LLMs to real-world, mission-critical applications remains challenging mainly due to their tendency to generate hallucinations and their inability to use external knowledge. This paper proposes a LLM-Augmenter system, which augments a black-box LLM with a set of plug-and-play modules. Our system makes the LLM generate responses grounded in external knowledge, e.g., stored in task-specific databases. It also iteratively revises LLM prompts to improve model responses using feedback generated by utility functions, e.g., the factuality score of a LLM-generated response. The effectiveness of LLM-Augmenter is empirically validated on two types of scenarios, task-oriented dialog and open-domain question answering. LLM-Augmenter significantly reduces ChatGPT's hallucinations without sacrificing the fluency and informativeness of its responses. We make the source code and models publicly available.",
            "year": 2023,
            "citationCount": 222,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A LLM-Augmenter system, which augments a black-box LLM with a set of plug-and-play modules to significantly reduces ChatGPT's hallucinations without sacrificing the fluency and informativeness of its responses."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the factuality of language models in counterfactual scenarios, and the proposed approach is to use iterative probing to guide the model to reason about the differences and implications of the counterfactual scenario.\n\nThe research problem in the paper is improving the factuality and knowledge grounding of language models, and the proposed approach is to augment the language model with external knowledge and automated feedback.\n\nWhile both works aim to improve the factuality of language models, the proposal focuses specifically on counterfactual scenarios and uses iterative probing, while the paper focuses on general scenarios and uses external knowledge and automated feedback.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "0dc6e2476005d1aae30bede8ebbae92bdc831f6a",
            "paperId": "0dc6e2476005d1aae30bede8ebbae92bdc831f6a",
            "title": "Counterfactual reasoning: Do Language Models need world knowledge for causal inference?",
            "abstract": "Current pre-trained language models have enabled remarkable improvements in downstream tasks, but it remains difficult to distinguish effects of statistical correlation from more systematic logical reasoning grounded on understanding of the real world. In this paper we tease these factors apart by leveraging counterfactual conditionals , which force language models to predict unusual consequences based on hypothetical propositions. We introduce a set of tests drawn from psycholinguistic experiments, as well as larger-scale controlled datasets, to probe counterfactual predictions from a variety of popular pre-trained language models. We find that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge\u2014however, we also find that for most models this effect appears largely to be driven by simple lexical cues. When we mitigate effects of both world knowledge and lexical cues to test knowledge of linguistic nuances of counterfactuals, we find that only GPT-3 shows sensitivity to these nuances, though this sensitivity is also non-trivially impacted by lexical associative factors.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge\u2014however, it is also found that for most models this effect appears largely to be driven by simple lexical cues."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the factuality of language models in counterfactual scenarios, and the proposed approach is to use iterative probing to guide the model's reasoning. The research problem in the paper is evaluating the counterfactual reasoning abilities of language models, and the approach is to use psycholinguistic tests and controlled datasets to probe the models' predictions.\n\nWhile both works involve counterfactual reasoning in language models, the proposal focuses on improving factuality through a specific probing method, while the paper aims to evaluate and analyze the models' existing capabilities using various tests. The proposal is more application-oriented, while the paper is more exploratory and analytical.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "91a82593721c03ecffdef1c72ea55c6d87c42473",
            "paperId": "91a82593721c03ecffdef1c72ea55c6d87c42473",
            "title": "Counterfactual reasoning: Do language models need world knowledge for causal understanding?",
            "abstract": "Current pre-trained language models have enabled remarkable improvements in downstream tasks, but it remains difficult to distinguish effects of statistical correlation from more systematic logical reasoning grounded on understanding of the real world. In this paper we tease these factors apart by leveraging counterfactual conditionals, which force language models to predict unusual consequences based on hypothetical propositions. We introduce a set of tests drawn from psycholinguistic experiments, as well as larger-scale controlled datasets, to probe counterfactual predictions from a variety of popular pre-trained language models. We find that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge -- however, we also find that for most models this effect appears largely to be driven by simple lexical cues. When we mitigate effects of both world knowledge and lexical cues to test knowledge of linguistic nuances of counterfactuals, we find that only GPT-3 shows sensitivity to these nuances, though this sensitivity is also non-trivially impacted by lexical associative factors.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge -- however, it is also found that for most models this effect appears largely to be driven by simple lexical cues."
            },
            "score": 6
        },
        {
            "id": "20ae101289965d36dbd93e9b8c47ec9deab03ed0",
            "paperId": "20ae101289965d36dbd93e9b8c47ec9deab03ed0",
            "title": "What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models",
            "abstract": "Counterfactual reasoning ability is one of the core abilities of human intelligence. This reasoning process involves the processing of alternatives to observed states or past events, and this process can improve our ability for planning and decision-making. In this work, we focus on benchmarking the counterfactual reasoning ability of multimodal large language models. We take the question and answer pairs from the VQAv2 dataset and add one counterfactual presupposition to the questions, with the answer being modified accordingly. After generating counterfactual questions and answers using ChatGPT, we manually examine all generated questions and answers to ensure correctness. This results in over 2k counterfactual question and answer pairs. We evaluate recent vision language models on our newly collected test dataset and found that all models exhibit a large performance drop compared to the results tested on questions without counterfactual presupposition. This result indicates that there still exists space for developing vision language models. We hope our proposed benchmark can help the development of future systems.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work benchmarking the counterfactual reasoning ability of multimodal large language models found that all models exhibit a large performance drop compared to the results tested on questions without counterfactual presupposition, indicating that there still exists space for developing vision language models."
            },
            "score": 6
        },
        {
            "id": "415c594667bb94f086ea980f897c4f62f9a22d18",
            "paperId": "415c594667bb94f086ea980f897c4f62f9a22d18",
            "title": "Eyes Can Deceive: Benchmarking Counterfactual Reasoning Abilities of Multi-modal Large Language Models",
            "abstract": "Counterfactual reasoning, as a crucial manifestation of human intelligence, refers to making presuppositions based on established facts and extrapolating potential outcomes. Existing multimodal large language models (MLLMs) have exhibited impressive cognitive and reasoning capabilities, which have been examined across a wide range of Visual Question Answering (VQA) benchmarks. Nevertheless, how will existing MLLMs perform when faced with counterfactual questions? To answer this question, we first curate a novel \\textbf{C}ounter\\textbf{F}actual \\textbf{M}ulti\\textbf{M}odal reasoning benchmark, abbreviated as \\textbf{CFMM}, to systematically assess the counterfactual reasoning capabilities of MLLMs. Our CFMM comprises six challenging tasks, each including hundreds of carefully human-labeled counterfactual questions, to evaluate MLLM's counterfactual reasoning capabilities across diverse aspects. Through experiments, interestingly, we find that existing MLLMs prefer to believe what they see, but ignore the counterfactual presuppositions presented in the question, thereby leading to inaccurate responses. Furthermore, we evaluate a wide range of prevalent MLLMs on our proposed CFMM. The significant gap between their performance on our CFMM and that on several VQA benchmarks indicates that there is still considerable room for improvement in existing MLLMs toward approaching human-level intelligence. On the other hand, through boosting MLLMs performances on our CFMM in the future, potential avenues toward developing MLLMs with advanced intelligence can be explored.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that existing MLLMs prefer to believe what they see, but ignore the counterfactual presuppositions presented in the question, thereby leading to inaccurate responses on the proposed CFMM."
            },
            "score": 6
        },
        {
            "id": "894df6e5d0f34014e6a67b2d5f43e2bde91f63e3",
            "paperId": "894df6e5d0f34014e6a67b2d5f43e2bde91f63e3",
            "title": "Faithful Explanations of Black-box NLP Models Using LLM-generated Counterfactuals",
            "abstract": "Causal explanations of the predictions of NLP systems are essential to ensure safety and establish trust. Yet, existing methods often fall short of explaining model predictions effectively or efficiently and are often model-specific. In this paper, we address model-agnostic explanations, proposing two approaches for counterfactual (CF) approximation. The first approach is CF generation, where a large language model (LLM) is prompted to change a specific text concept while keeping confounding concepts unchanged. While this approach is demonstrated to be very effective, applying LLM at inference-time is costly. We hence present a second approach based on matching, and propose a method that is guided by an LLM at training-time and learns a dedicated embedding space. This space is faithful to a given causal graph and effectively serves to identify matches that approximate CFs. After showing theoretically that approximating CFs is required in order to construct faithful explanations, we benchmark our approaches and explain several models, including LLMs with billions of parameters. Our empirical results demonstrate the excellent performance of CF generation models as model-agnostic explainers. Moreover, our matching approach, which requires far less test-time resources, also provides effective explanations, surpassing many baselines. We also find that Top-K techniques universally improve every tested method. Finally, we showcase the potential of LLMs in constructing new benchmarks for model explanation and subsequently validate our conclusions. Our work illuminates new pathways for efficient and accurate approaches to interpreting NLP systems.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper addresses model-agnostic explanations, proposing two approaches for counterfactual (CF) approximation and presents a method that is guided by an LLM at training-time and learns a dedicated embedding space that effectively serves to identify matches that approximate CFs."
            },
            "score": 6
        },
        {
            "id": "03fe268a0bcf66104b648eb3f34cc729ede0948c",
            "paperId": "03fe268a0bcf66104b648eb3f34cc729ede0948c",
            "title": "Probing Intersectional Biases in Vision-Language Models with Counterfactual Examples",
            "abstract": "While vision-language models (VLMs) have achieved remarkable performance improvements recently, there is growing evidence that these models also posses harmful biases with respect to social attributes such as gender and race. Prior studies have primarily focused on probing such bias attributes individually while ignoring biases associated with intersections between social attributes. This could be due to the difficulty of collecting an exhaustive set of image-text pairs for various combinations of social attributes from existing datasets. To address this challenge, we employ text-to-image diffusion models to produce counterfactual examples for probing intserctional social biases at scale. Our approach utilizes Stable Diffusion with cross attention control to produce sets of counterfactual image-text pairs that are highly similar in their depiction of a subject (e.g., a given occupation) while differing only in their depiction of intersectional social attributes (e.g., race&gender). We conduct extensive experiments using our generated dataset which reveal the intersectional social biases present in state-of-the-art VLMs.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work employs text-to-image diffusion models to produce counterfactual examples for probing intserctional social biases at scale and reveals the intersectional social bias attributes present in state-of-the-art VLMs."
            },
            "score": 5
        },
        {
            "id": "79217b366b3659500de61f6aabd4aa4d3ff24e34",
            "paperId": "79217b366b3659500de61f6aabd4aa4d3ff24e34",
            "title": "SocialCounterfactuals: Probing and Mitigating Intersectional Social Biases in Vision-Language Models with Counterfactual Examples",
            "abstract": "While vision-language models (VLMs) have achieved remarkable performance improvements recently, there is growing evidence that these models also posses harmful biases with respect to social attributes such as gender and race. Prior studies have primarily focused on probing such bias attributes individually while ignoring biases associated with intersections between social attributes. This could be due to the difficulty of collecting an exhaustive set of image-text pairs for various combinations of social attributes. To address this challenge, we employ text-to-image diffusion models to produce counterfactual examples for probing intersectional social biases at scale. Our approach utilizes Stable Diffusion with cross attention control to produce sets of counterfactual image-text pairs that are highly similar in their depiction of a subject (e.g., a given occupation) while differing only in their depiction of intersectional social attributes (e.g., race&gender). Through our over-generate-then-filter methodology, we produce SocialCounterfactuals, a high-quality dataset containing 171k image-text pairs for probing intersectional biases related to gender, race, and physical characteristics. We conduct extensive experiments to demonstrate the usefulness of our generated dataset for probing and mitigating intersectional social biases in state-of-the-art VLMs.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Through the over-generate-then-filter methodology, this work produces SocialCounterfactuals, a high-quality dataset containing 171k image-text pairs for probing intersectional biases related to gender, race, and physical characteristics in state-of-the-art VLMs."
            },
            "score": 5
        },
        {
            "id": "8d45cf5e403f14d9f6498a97b314af03db49351e",
            "paperId": "8d45cf5e403f14d9f6498a97b314af03db49351e",
            "title": "Counterfactually Probing Language Identity in Multilingual Models",
            "abstract": "Techniques in causal analysis of language models illuminate how linguistic information is organized in LLMs. We use one such technique, AlterRep, a method of counterfactual probing, to explore the internal structure of multilingual models (mBERT and XLM-R). We train a linear classifier on a binary language identity task, to classify tokens between Language X and Language Y. Applying a counterfactual probing procedure, we use the classifier weights to project the embeddings into the null space and push the resulting embeddings either in the direction of Language X or Language Y. Then we evaluate on a masked language modeling task. We find that, given a template in Language X, pushing towards Language Y systematically increases the probability of Language Y words, above and beyond a third-party control language. But it does not specifically push the model towards translation-equivalent words in Language Y. Pushing towards Language X (the same direction as the template) has a minimal effect, but somewhat degrades these models. Overall, we take these results as further evidence of the rich structure of massive multilingual language models, which include both a language-specific and language-general component. And we show that counterfactual probing can be fruitfully applied to multilingual models.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "AlterRep, a method of counterfactual probing, is used to explore the internal structure of multilingual models (mBERT and XLM-R), which include both a language-specific and language-general component, and it is shown that counterfactually probing can be fruitfully applied to mult bilingual models."
            },
            "score": 5
        },
        {
            "id": "e886fe33b9cda6e0b0f523d5fbbb7287c23aba2b",
            "paperId": "e886fe33b9cda6e0b0f523d5fbbb7287c23aba2b",
            "title": "Causal Analysis of Syntactic Agreement Neurons in Multilingual Language Models",
            "abstract": "Structural probing work has found evidence for latent syntactic information in pre-trained language models. However, much of this analysis has focused on monolingual models, and analyses of multilingual models have employed correlational methods that are confounded by the choice of probing tasks. In this study, we causally probe multilingual language models (XGLM and multilingual BERT) as well as monolingual BERT-based models across various languages; we do this by performing counterfactual perturbations on neuron activations and observing the effect on models\u2019 subject-verb agreement probabilities. We observe where in the model and to what extent syntactic agreement is encoded in each language. We find significant neuron overlap across languages in autoregressive multilingual language models, but not masked language models. We also find two distinct layer-wise effect patterns and two distinct sets of neurons used for syntactic agreement, depending on whether the subject and verb are separated by other tokens. Finally, we find that behavioral analyses of language models are likely underestimating how sensitive masked language models are to syntactic information.",
            "year": 2022,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study causally probe multilingual language models as well as monolingual BERT-based models across various languages by performing counterfactual perturbations on neuron activations and observing the effect on models\u2019 subject-verb agreement probabilities, finding significant neuron overlap across languages in autoregressive mult bilingual language models, but not masked language models."
            },
            "score": 5
        },
        {
            "id": "3f2ea212135139eef4ff383bfcc9bf2b41e02616",
            "paperId": "3f2ea212135139eef4ff383bfcc9bf2b41e02616",
            "title": "Knowledge-Based Counterfactual Queries for Visual Question Answering",
            "abstract": "Visual Question Answering (VQA) has been a popular task that combines vision and language, with numerous relevant implementations in literature. Even though there are some attempts that approach explainability and robustness issues in VQA models, very few of them employ counterfactuals as a means of probing such challenges in a model-agnostic way. In this work, we propose a systematic method for explaining the behavior and investigating the robustness of VQA models through counterfactual perturbations. For this reason, we exploit structured knowledge bases to perform deterministic, optimal and controllable word-level replacements targeting the linguistic modality, and we then evaluate the model's response against such counterfactual inputs. Finally, we qualitatively extract local and global explanations based on counterfactual responses, which are ultimately proven insightful towards interpreting VQA model behaviors. By performing a variety of perturbation types, targeting different parts of speech of the input question, we gain insights to the reasoning of the model, through the comparison of its responses in different adversarial circumstances. Overall, we reveal possible biases in the decision-making process of the model, as well as expected and unexpected patterns, which impact its performance quantitatively and qualitatively, as indicated by our analysis.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a systematic method for explaining the behavior and investigating the robustness of VQA models through counterfactual perturbations, and reveals possible biases in the decision-making process of the model, as well as expected and unexpected patterns, which impact its performance quantitatively and qualitatively."
            },
            "score": 5
        },
        {
            "id": "69f2ba0f33a54e01de32c616b64e85d5d7194067",
            "paperId": "69f2ba0f33a54e01de32c616b64e85d5d7194067",
            "title": "Do Models Explain Themselves? Counterfactual Simulatability of Natural Language Explanations",
            "abstract": "Large language models (LLMs) are trained to imitate humans to explain human decisions. However, do LLMs explain themselves? Can they help humans build mental models of how LLMs process different inputs? To answer these questions, we propose to evaluate $\\textbf{counterfactual simulatability}$ of natural language explanations: whether an explanation can enable humans to precisely infer the model's outputs on diverse counterfactuals of the explained input. For example, if a model answers\"yes\"to the input question\"Can eagles fly?\"with the explanation\"all birds can fly\", then humans would infer from the explanation that it would also answer\"yes\"to the counterfactual input\"Can penguins fly?\". If the explanation is precise, then the model's answer should match humans' expectations. We implemented two metrics based on counterfactual simulatability: precision and generality. We generated diverse counterfactuals automatically using LLMs. We then used these metrics to evaluate state-of-the-art LLMs (e.g., GPT-4) on two tasks: multi-hop factual reasoning and reward modeling. We found that LLM's explanations have low precision and that precision does not correlate with plausibility. Therefore, naively optimizing human approvals (e.g., RLHF) may not be a sufficient solution.",
            "year": 2023,
            "citationCount": 19,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that LLM's explanations have low precision and that precision does not correlate with plausibility, therefore, naively optimizing human approvals (e.g., RLHF) may not be a sufficient solution."
            },
            "score": 5
        },
        {
            "id": "4115a24474ef5f184f5cbae3f43aca4d3bb07bea",
            "paperId": "4115a24474ef5f184f5cbae3f43aca4d3bb07bea",
            "title": "Improving Language Models via Plug-and-Play Retrieval Feedback",
            "abstract": "Large language models (LLMs) exhibit remarkable performance across various NLP tasks. However, they often generate incorrect or hallucinated information, which hinders their practical applicability in real-world scenarios. Human feedback has been shown to effectively enhance the factuality and quality of generated content, addressing some of these limitations. However, this approach is resource-intensive, involving manual input and supervision, which can be time-consuming and expensive. Moreover, it cannot be provided during inference, further limiting its practical utility in dynamic and interactive applications. In this paper, we introduce ReFeed, a novel pipeline designed to enhance LLMs by providing automatic retrieval feedback in a plug-and-play framework without the need for expensive fine-tuning. ReFeed first generates initial outputs, then utilizes a retrieval model to acquire relevant information from large document collections, and finally incorporates the retrieved information into the in-context demonstration for output refinement, thereby addressing the limitations of LLMs in a more efficient and cost-effective manner. Experiments on four knowledge-intensive benchmark datasets demonstrate our proposed ReFeed could improve over +6.0% under zero-shot setting and +2.5% under few-shot setting, compared to baselines without using retrieval feedback.",
            "year": 2023,
            "citationCount": 34,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces ReFeed, a novel pipeline designed to enhance LLMs by providing automatic retrieval feedback in a plug-and-play framework without the need for expensive fine-tuning, thereby addressing the limitations of LLMs in a more efficient and cost-effective manner."
            },
            "score": 5
        },
        {
            "id": "3a89e289e2dd29f5e52a2bf354a637762b661257",
            "paperId": "3a89e289e2dd29f5e52a2bf354a637762b661257",
            "title": "Fine-tuning Language Models for Factuality",
            "abstract": "The fluency and creativity of large pre-trained language models (LLMs) have led to their widespread use, sometimes even as a replacement for traditional search engines. Yet language models are prone to making convincing but factually inaccurate claims, often referred to as 'hallucinations.' These errors can inadvertently spread misinformation or harmfully perpetuate misconceptions. Further, manual fact-checking of model responses is a time-consuming process, making human factuality labels expensive to acquire. In this work, we fine-tune language models to be more factual, without human labeling and targeting more open-ended generation settings than past work. We leverage two key recent innovations in NLP to do so. First, several recent works have proposed methods for judging the factuality of open-ended text by measuring consistency with an external knowledge base or simply a large model's confidence scores. Second, the direct preference optimization algorithm enables straightforward fine-tuning of language models on objectives other than supervised imitation, using a preference ranking over possible model responses. We show that learning from automatically generated factuality preference rankings, generated either through existing retrieval systems or our novel retrieval-free approach, significantly improves the factuality (percent of generated claims that are correct) of Llama-2 on held-out topics compared with RLHF or decoding strategies targeted at factuality. At 7B scale, compared to Llama-2-chat, we observe 58% and 40% reduction in factual error rate when generating biographies and answering medical questions, respectively.",
            "year": 2023,
            "citationCount": 56,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that learning from automatically generated factuality preference rankings, generated either through existing retrieval systems or the novel retrieval-free approach, significantly improves the factuality of Llama-2 on held-out topics compared with RLHF or decoding strategies targeted at factuality."
            },
            "score": 5
        },
        {
            "id": "5ebd3965d55fbb91c2d45d3e7e6eb9e4257fa4cf",
            "paperId": "5ebd3965d55fbb91c2d45d3e7e6eb9e4257fa4cf",
            "title": "Probing Reasoning of Language Models with Inductive In-Context Learning",
            "abstract": "Despite their recent success, Language Models (LMs) have brought to question whether they statistically repeat data (\u2018stochastic parrots\u2019)[Bender et al. , 2021] or can learn the underlying generative process of the data. Current benchmarks used to probe the reasoning ability of LMs can be ambiguous as it is unclear whether the model has learned the benchmark or the generative process of the data. In this work we introduce a novel evaluation setting that we use with Inductive In-Context Learning (IIL) and a dataset, ReAnalogy , to probe the reasoning ability of LMs. ReAnalogy consists of sequences with positive examples, generated from regular expressions (regex), and contains quasi-natural language. We use regex to evaluate implicitly whether a LM can infer \u2018Rules\u2019 (regex) given limited sets of examples (\u2018Facts\u2019). We use the LM to generate additional Facts to evaluate whether the generated Facts abide by the Rules. We evaluate a GPT model in our setting and compare with the same model where a Rule is injected during training to replace a Fact. We use IIL during evaluation to probe the model to infer the Rule given Facts. We then use the inferred Rule to synthesize an additional Fact. IIL improves \u2018reason-ing\u2019 performance by as much as 33%. Our results suggest that LMs can learn more than statistical patterns in the data and we support our findings with ablation studies. We evaluate our dataset with existing benchmarks and baselines in inductive programming and find that current state-of-the-art symbolic or neuro-symbolic approaches fail to the complexity of our dataset; while the existing dataset and benchmark in the domain are inapplicable for LMs. Our probing method and dataset are complex enough for LMs and applicable for evaluating the inductive reasoning abilities of LMs, while IIL",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results suggest that LMs can learn more than statistical patterns in the data and are applicable for evaluating the inductive reasoning abilities of LMs, while the existing dataset and benchmark in the domain are inapplicable for LMs."
            },
            "score": 5
        },
        {
            "id": "10632e0a667cbc3c52cc8f11a46d8e8e9c7739e3",
            "paperId": "10632e0a667cbc3c52cc8f11a46d8e8e9c7739e3",
            "title": "Causal Reasoning and Large Language Models: Opening a New Frontier for Causality",
            "abstract": "The causal capabilities of large language models (LLMs) is a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We further our understanding of LLMs and their causal implications, considering the distinctions between different types of causal reasoning tasks, as well as the entangled threats of construct and measurement validity. LLM-based methods establish new state-of-the-art accuracies on multiple causal benchmarks. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain), and actual causality (86% accuracy in determining necessary and sufficient causes in vignettes). At the same time, LLMs exhibit unpredictable failure modes and we provide some techniques to interpret their robustness. Crucially, LLMs perform these causal tasks while relying on sources of knowledge and methods distinct from and complementary to non-LLM based approaches. Specifically, LLMs bring capabilities so far understood to be restricted to humans, such as using collected knowledge to generate causal graphs or identifying background causal context from natural language. We envision LLMs to be used alongside existing causal methods, as a proxy for human domain knowledge and to reduce human effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods. We also see existing causal methods as promising tools for LLMs to formalize, validate, and communicate their reasoning especially in high-stakes scenarios. In capturing common sense and domain knowledge about causal mechanisms and supporting translation between natural language and formal methods, LLMs open new frontiers for advancing the research, practice, and adoption of causality.",
            "year": 2023,
            "citationCount": 96,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LLMs are envisioned to be used alongside existing causal methods, as a proxy for human domain knowledge and to reduce human effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods."
            },
            "score": 5
        },
        {
            "id": "0885471c0215b3c0d31c82518066913f7f738128",
            "paperId": "0885471c0215b3c0d31c82518066913f7f738128",
            "title": "Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement",
            "abstract": "The ability to derive underlying principles from a handful of observations and then generalize to novel situations -- known as inductive reasoning -- is central to human intelligence. Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks. In this work, we conduct a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting. Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules. By examining the intermediate rules, we observe that LMs are phenomenal hypothesis proposers (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the proposed set of rules, this hybrid approach achieves strong results across inductive reasoning benchmarks that require inducing causal relations, language-like instructions, and symbolic concepts. However, they also behave as puzzling inductive reasoners, showing notable performance gaps between rule induction (i.e., identifying plausible rules) and rule application (i.e., applying proposed rules to instances), suggesting that LMs are proposing hypotheses without being able to actually apply the rules. Through empirical and human analyses, we further reveal several discrepancies between the inductive reasoning processes of LMs and humans, shedding light on both the potentials and limitations of using LMs in inductive reasoning tasks.",
            "year": 2023,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work conducts a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting."
            },
            "score": 5
        },
        {
            "id": "6ac627f57b26354ab537734d820da4a6a7dde2c6",
            "paperId": "6ac627f57b26354ab537734d820da4a6a7dde2c6",
            "title": "CLadder: Assessing Causal Reasoning in Language Models",
            "abstract": "The ability to perform causal reasoning is widely considered a core feature of intelligence. In this work, we investigate whether large language models (LLMs) can coherently reason about causality. Much of the existing work in natural language processing (NLP) focuses on evaluating commonsense causal reasoning in LLMs, thus failing to assess whether a model can perform causal inference in accordance with a set of well-defined formal rules. To address this, we propose a new NLP task, causal inference in natural language, inspired by the\"causal inference engine\"postulated by Judea Pearl et al. We compose a large dataset, CLadder, with 10K samples: based on a collection of causal graphs and queries (associational, interventional, and counterfactual), we obtain symbolic questions and ground-truth answers, through an oracle causal inference engine. These are then translated into natural language. We evaluate multiple LLMs on our dataset, and we introduce and evaluate a bespoke chain-of-thought prompting strategy, CausalCoT. We show that our task is highly challenging for LLMs, and we conduct an in-depth analysis to gain deeper insights into the causal reasoning abilities of LLMs. Our data is open-sourced at https://huggingface.co/datasets/causalNLP/cladder, and our code can be found at https://github.com/causalNLP/cladder.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates whether large language models (LLMs) can coherently reason about causality, and proposes a new NLP task, causal inference in natural language, inspired by the \"causal inference engine\" proposed by Judea Pearl et al."
            },
            "score": 5
        },
        {
            "id": "ead6e2873c776448d38c7596220a5f74b718b60c",
            "paperId": "ead6e2873c776448d38c7596220a5f74b718b60c",
            "title": "Counterfactual Editing for Search Result Explanation",
            "abstract": "Search Result Explanation (SeRE) aims to improve search sessions' effectiveness and efficiency by helping users interpret documents' relevance. Existing works mostly focus on factual explanation, i.e. to find/generate supporting evidence about documents' relevance to search queries. However, research in cognitive sciences has shown that human explanations are contrastive i.e. people explain an observed event using some counterfactual events; such explanations reduce cognitive load and provide actionable insights. Though already proven effective in machine learning and NLP communities, there lacks a strict formulation on how counterfactual explanations should be defined and structured, in the context of web search. In this paper, we first discuss the possible formulation of counterfactual explanations in the IR context. Next, we formulate a suite of desiderata for counterfactual explanation in SeRE task and corresponding automatic metrics. With this desiderata, we propose a method named \\textbf{C}ounter\\textbf{F}actual \\textbf{E}diting for Search Research \\textbf{E}xplanation (\\textbf{CFE2}). CFE2 provides pairwise counterfactual explanations for document pairs within a search engine result page. Our experiments on five public search datasets demonstrate that CFE2 can significantly outperform baselines in both automatic metrics and human evaluations.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "CFE2 provides pairwise counterfactual explanations for document pairs within a search engine result page and can significantly outperform baselines in both automatic metrics and human evaluations."
            },
            "score": 5
        },
        {
            "id": "804356bb7e8cc7b8735cb4a3cf37d5a4de315742",
            "paperId": "804356bb7e8cc7b8735cb4a3cf37d5a4de315742",
            "title": "COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs",
            "abstract": "Counterfactual examples have proven to be valuable in the field of natural language processing (NLP) for both evaluating and improving the robustness of language models to spurious correlations in datasets. Despite their demonstrated utility for NLP, multimodal counterfactual examples have been relatively unexplored due to the difficulty of creating paired image-text data with minimal counterfactual changes. To address this challenge, we introduce a scalable framework for automatic generation of counterfactual examples using text-to-image diffusion models. We use our framework to create COCO-Counterfactuals, a multimodal counterfactual dataset of paired image and text captions based on the MS-COCO dataset. We validate the quality of COCO-Counterfactuals through human evaluations and show that existing multimodal models are challenged by our counterfactual image-text pairs. Additionally, we demonstrate the usefulness of COCO-Counterfactuals for improving out-of-domain generalization of multimodal vision-language models via training data augmentation.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A scalable framework for automatic generation of counterfactual examples using text-to-image diffusion models and the usefulness of COCO-Counterfactuals for improving out-of-domain generalization of multimodal vision-language models via training data augmentation is demonstrated."
            },
            "score": 5
        },
        {
            "id": "301c09d74e46436bc75ea5f56d60acc549831961",
            "paperId": "301c09d74e46436bc75ea5f56d60acc549831961",
            "title": "CREST: A Joint Framework for Rationalization and Counterfactual Text Generation",
            "abstract": "Selective rationales and counterfactual examples have emerged as two effective, complementary classes of interpretability methods for analyzing and training NLP models. However, prior work has not explored how these methods can be integrated to combine their complementary advantages. We overcome this limitation by introducing CREST (ContRastive Edits with Sparse raTionalization), a joint framework for selective rationalization and counterfactual text generation, and show that this framework leads to improvements in counterfactual quality, model robustness, and interpretability. First, CREST generates valid counterfactuals that are more natural than those produced by previous methods, and subsequently can be used for data augmentation at scale, reducing the need for human-generated examples. Second, we introduce a new loss function that leverages CREST counterfactuals to regularize selective rationales and show that this regularization improves both model robustness and rationale quality, compared to methods that do not leverage CREST counterfactuals. Our results demonstrate that CREST successfully bridges the gap between selective rationales and counterfactual examples, addressing the limitations of existing methods and providing a more comprehensive view of a model\u2019s predictions.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results demonstrate that CREST successfully bridges the gap between selective rationales and counterfactual examples, addressing the limitations of existing methods and providing a more comprehensive view of a model\u2019s predictions."
            },
            "score": 5
        },
        {
            "id": "04f0d2b8873f0dc7f086ed82f93091782d59d19f",
            "paperId": "04f0d2b8873f0dc7f086ed82f93091782d59d19f",
            "title": "Counterfactual Probing for the influence of affect and specificity on Intergroup Bias",
            "abstract": "While existing work on studying bias in NLP focues on negative or pejorative language use, Govindarajan et al. (2023) offer a revised framing of bias in terms of intergroup social context, and its effects on language behavior. In this paper, we investigate if two pragmatic features (specificity and affect) systematically vary in different intergroup contexts -- thus connecting this new framing of bias to language output. Preliminary analysis finds modest correlations between specificity and affect of tweets with supervised intergroup relationship (IGR) labels. Counterfactual probing further reveals that while neural models finetuned for predicting IGR labels reliably use affect in classification, the model's usage of specificity is inconclusive. Code and data can be found at: https://github.com/venkatasg/intergroup-probing",
            "year": 2023,
            "citationCount": 0,
            "tldr": null,
            "score": 4
        },
        {
            "id": "59f7ba3a730f94bbba685cc79dcf961c1a477e9d",
            "paperId": "59f7ba3a730f94bbba685cc79dcf961c1a477e9d",
            "title": "Counterfactual Probing Intergroup Bias for Affect and Specificity",
            "abstract": "While existing work on studying bias in NLP focues on negative or pejorative language use, Govindarajan et al. (2023) offer a revised framing of bias in terms of intergroup social context, and its effects on language behavior. In this paper, we investigate if two pragmatic features (specificity and affect) systematically vary in different intergroup contexts \u2014 thus connecting this new framing of bias to language output. Preliminary analysis finds modest correlations between specificity and affect of tweets with supervised intergroup relationship labels. Coun-terfactual probing further reveals that while neural models finetuned for predicting IGR labels reliably use affect in classification, the model\u2019s usage of specificity is inconclusive.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper investigates if two pragmatic features (specificity and affect) systematically vary in different intergroup contexts \u2014 thus connecting this new framing of bias to language output."
            },
            "score": 4
        },
        {
            "id": "174a0e6da0dfb7f96d4a0a4076eed154c439e41a",
            "paperId": "174a0e6da0dfb7f96d4a0a4076eed154c439e41a",
            "title": "Probing Language Models for Understanding of Temporal Expressions",
            "abstract": "We present three Natural Language Inference (NLI) challenge sets that can evaluate NLI models on their understanding of temporal expressions. More specifically, we probe these models for three temporal properties: (a) the order between points in time, (b) the duration between two points in time, (c) the relation between the magnitude of times specified in different units. We find that although large language models fine-tuned on MNLI have some basic perception of the order between points in time, at large, these models do not have a thorough understanding of the relation between temporal expressions.",
            "year": 2021,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that although large language models fine-tuned on MNLI have some basic perception of the order between points in time, at large, these models do not have a thorough understanding of the relation between temporal expressions."
            },
            "score": 4
        },
        {
            "id": "c7091540c1fa77f1c6b27482f349330f8e559d6f",
            "paperId": "c7091540c1fa77f1c6b27482f349330f8e559d6f",
            "title": "Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks",
            "abstract": "We consider the questions of whether or not large language models (LLMs) have beliefs, and, if they do, how we might measure them. First, we evaluate two existing approaches, one due to Azaria and Mitchell (2023) and the other to Burns et al. (2022). We provide empirical results that show that these methods fail to generalize in very basic ways. We then argue that, even if LLMs have beliefs, these methods are unlikely to be successful for conceptual reasons. Thus, there is still no lie-detector for LLMs. After describing our empirical results we take a step back and consider whether or not we should expect LLMs to have something like beliefs in the first place. We consider some recent arguments aiming to show that LLMs cannot have beliefs. We show that these arguments are misguided. We provide a more productive framing of questions surrounding the status of beliefs in LLMs, and highlight the empirical nature of the problem. We conclude by suggesting some concrete paths for future work.",
            "year": 2023,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Empirical results show that, even if LLMs have beliefs, these methods are unlikely to be successful for conceptual reasons, and it is argued that there is still no lie-detector for LLMs."
            },
            "score": 4
        },
        {
            "id": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
            "paperId": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
            "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such\"society of minds\"approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.",
            "year": 2023,
            "citationCount": 206,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer is presented, indicating that this approach significantly enhances mathematical and strategic reasoning across a number of tasks."
            },
            "score": 4
        },
        {
            "id": "ed5020eeda1fbe8c29b1282d654b34abee22d90f",
            "paperId": "ed5020eeda1fbe8c29b1282d654b34abee22d90f",
            "title": "DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models",
            "abstract": "Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained LLMs that does not require conditioning on retrieved external knowledge nor additional fine-tuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an LLMs has generally been shown to be localized to particular transformer layers. We find that this Decoding by Contrasting Layers (DoLa) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA by 12-17% absolute points, demonstrating its potential in making LLMs reliably generate truthful facts.",
            "year": 2023,
            "citationCount": 35,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA by 12-17% absolute points, demonstrating its potential in making LLMs reliably generate truthful facts."
            },
            "score": 4
        },
        {
            "id": "d2d3960bd1e4e84cbb0051945ea1419005e75e07",
            "paperId": "d2d3960bd1e4e84cbb0051945ea1419005e75e07",
            "title": "Dial BeInfo for Faithfulness: Improving Factuality of Information-Seeking Dialogue via Behavioural Fine-Tuning",
            "abstract": "Factuality is a crucial requirement in information seeking dialogue: the system should respond to the user's queries so that the responses are meaningful and aligned with the knowledge provided to the system. However, most modern large language models suffer from hallucinations, that is, they generate responses not supported by or contradicting the knowledge source. To mitigate the issue and increase faithfulness of information-seeking dialogue systems, we introduce BeInfo, a simple yet effective method that applies behavioural tuning to aid information-seeking dialogue. Relying on three standard datasets, we show that models tuned with BeInfo} become considerably more faithful to the knowledge source both for datasets and domains seen during BeInfo-tuning, as well as on unseen domains, when applied in a zero-shot manner. In addition, we show that the models with 3B parameters (e.g., Flan-T5) tuned with BeInfo demonstrate strong performance on data from real `production' conversations and outperform GPT4 when tuned on a limited amount of such realistic in-domain dialogues.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces BeInfo, a simple yet effective method that applies behavioural tuning to aid information-seeking dialogue and shows that models tuned with BeInfo become considerably more faithful to the knowledge source both for datasets and domains seen during BeInfo-tuning, as well as on unseen domains, when applied in a zero-shot manner."
            },
            "score": 4
        },
        {
            "id": "a72975eb88eb31f193e9587e7415cb04e7bcdbee",
            "paperId": "a72975eb88eb31f193e9587e7415cb04e7bcdbee",
            "title": "Generating Benchmarks for Factuality Evaluation of Language Models",
            "abstract": "Before deploying a language model (LM) within a given domain, it is important to measure its tendency to generate factually incorrect information in that domain. Existing methods for factuality evaluation of LLM generation focus on facts sampled from the LM itself, and thus do not control the set of evaluated facts and might under-represent domain specific or rare facts. We propose FACTOR: Factual Assessment via Corpus TransfORmation, a scalable approach for evaluating LM factuality. FACTOR automatically transforms a factual corpus of interest into a benchmark evaluating an LM\u2019s propensity to generate true facts from the corpus vs. similar but incorrect statements. We use our framework to create three benchmarks: Wiki-FACTOR, News-FACTOR and Expert-FACTOR. We show that: (i) our benchmark scores increase with model size and improve when the LM is augmented with retrieval; (ii) benchmark score and perplexity do not always agree on model ranking; (iii) when perplexity and benchmark score disagree, the latter better reflects factuality in open-ended generation, as measured by human annotators.",
            "year": 2023,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes FACTOR: Factual Assessment via Corpus TransfORmation, a scalable approach for evaluating LM factuality that automatically transforms a factual corpus of interest into a benchmark evaluating an LM\u2019s propensity to generate true facts from the corpus vs. similar but incorrect statements."
            },
            "score": 4
        },
        {
            "id": "95ca67ba607d7859ee8eec457f4b59b115d69bf5",
            "paperId": "95ca67ba607d7859ee8eec457f4b59b115d69bf5",
            "title": "ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models",
            "abstract": "Although large language models (LLMs) have achieved excellent performance in a variety of evaluation benchmarks, they still struggle in complex reasoning tasks which require specific knowledge and multi-hop reasoning. To improve the reasoning abilities, we propose \\textbf{ChatCoT}, a tool-augmented chain-of-thought reasoning framework for chat-based LLMs. In ChatCoT, we model the chain-of-thought~(CoT) reasoning as multi-turn conversations, to utilize tools in a more natural way through chatting. At each turn, LLMs can either interact with tools or perform the reasoning. Our approach can effectively leverage the multi-turn conversation ability of chat-based LLMs, and integrate the thought chain following and tools manipulation in a unified way. Specially, we initialize the early turns of the conversation by the tools, tasks and reasoning format, and propose an iterative \\emph{tool-augmented reasoning} step to perform step-by-step tool-augmented reasoning. The experiment results on two complex reasoning datasets (MATH and HotpotQA) have shown the effectiveness of ChatCoT on complex reasoning tasks, achieving a 6.8\\% relative improvement over the state-of-the-art baseline. Our code and data are available at: \\url{https://github.com/RUCAIBOX/ChatCoT}.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes ChatCoT, a tool-augmented chain-of-thought reasoning framework for chat-based LLMs that can effectively leverage the multi-turn conversation ability of chat- based LLMs, and integrate the thought chain following and tools manipulation in a unified way."
            },
            "score": 4
        },
        {
            "id": "5aec6865043cb7c7f281699ae95652e0ff680f09",
            "paperId": "5aec6865043cb7c7f281699ae95652e0ff680f09",
            "title": "CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning",
            "abstract": "The sequential process of conceptualization and instantiation is essential to generalizable commonsense reasoning as it allows the application of existing knowledge to unfamiliar scenarios. However, existing works tend to undervalue the step of instantiation and heavily rely on pre-built concept taxonomies and human annotations to collect both types of knowledge, resulting in a lack of instantiated knowledge to complete reasoning, high cost, and limited scalability. To tackle these challenges, we introduce CANDLE, a distillation framework that iteratively performs contextualized conceptualization and instantiation over commonsense knowledge bases by instructing large language models to generate both types of knowledge with critic filtering. By applying CANDLE to ATOMIC, we construct a comprehensive knowledge base comprising six million conceptualizations and instantiated commonsense knowledge triples. Both types of knowledge are firmly rooted in the original ATOMIC dataset, and intrinsic evaluations demonstrate their exceptional quality and diversity. Empirical results indicate that distilling CANDLE on student models provides benefits across four downstream tasks. Our code, data, and models are publicly available at https://github.com/HKUST-KnowComp/CANDLE.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "CANDLE is introduced, a distillation framework that iteratively performs contextualized conceptualization and instantiation over commonsense knowledge bases by instructing large language models to generate both types of knowledge with critic filtering."
            },
            "score": 4
        },
        {
            "id": "498d1406fc4cddb05cd46477793f2e726a6fe238",
            "paperId": "498d1406fc4cddb05cd46477793f2e726a6fe238",
            "title": "The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code",
            "abstract": "Causal reasoning, the ability to identify cause-and-effect relationship, is crucial in human thinking. Although large language models (LLMs) succeed in many NLP tasks, it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning. Given the fact that programming code may express causal relations more often and explicitly with conditional statements like ``if``, we want to explore whether Code-LLMs acquire better causal reasoning abilities. Our experiments show that compared to text-only LLMs, Code-LLMs with code prompts are significantly better in causal reasoning. We further intervene on the prompts from different aspects, and discover that the programming structure is crucial in code prompt design, while Code-LLMs are robust towards format perturbations.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experiments show that compared to text-only LLMs, Code-LLMs with code prompts are significantly better in causal reasoning, and that the programming structure is crucial in code prompt design, while Code- LLMs are robust towards format perturbations."
            },
            "score": 4
        },
        {
            "id": "e239ea64487bab0d1b03259fede07cec9eb152e1",
            "paperId": "e239ea64487bab0d1b03259fede07cec9eb152e1",
            "title": "Counterfactual Matters: Intrinsic Probing For Dialogue State Tracking",
            "abstract": "A Dialogue State Tracker (DST) is a core component of modular task-oriented dialogue systems. Tremendous research progress has been made in past ten years to improve performance of DSTs especially on benchmark datasets. However, their generalization to novel and realistic scenarios beyond the held-out conversations is limited. In this paper, we design experimental studies to answer: 1) How does the distribution of dialogue data affect the performance of DSTs? 2) What are effective ways to probe counterfactual matter for DSTs? Our findings are: the performance variance of generative DSTs is not only due to the model structure itself, but can be attributed to the distribution of cross-domain values. Evaluating iconic generative DST models on MultiWOZ dataset with counterfactuals results in a significant performance drop of up to 34.64% (from 50.91% to 16.27%) in absolute joint goal accuracy. It is believed that our experimental results can guide the future work to better understand the intrinsic core of DST and rethink the suitable way for specific tasks given the application property.",
            "year": 2021,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The findings are: the performance variance of generative DSTs is not only due to the model structure itself, but can be attributed to the distribution of cross-domain values."
            },
            "score": 4
        },
        {
            "id": "bbffc1d5b4a8786d6a33080204e5a35590a08389",
            "paperId": "bbffc1d5b4a8786d6a33080204e5a35590a08389",
            "title": "BLCU-NLP at SemEval-2020 Task 5: Data Augmentation for Efficient Counterfactual Detecting",
            "abstract": "Counterfactuals describe events counter to facts and hence naturally involve common sense, knowledge, and reasoning. SemEval 2020 task 5 is focusing on this field. We participate in the subtask 1 and we use BERT as our system. Our Innovations are feature extraction and data augmentation. We extract and summarize features of counterfactual statements, augment counterfactual examples in training set with the help of these features, and two general methods of data augmentation is experimented in our work. We demonstrate the effectiveness of our approaches, which achieves 0.95 of subtask 1 in F1 while using only a subset of giving training set to fine-tune the BERT model, and our official submission achieves F1 0.802, which ranks us 16th in the competition.",
            "year": 2020,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The effectiveness of the approaches is demonstrated, which achieves 0.95 of subtask 1 in F1 while using only a subset of giving training set to fine-tune the BERT model, and the official submission achieves F1 0.802, which ranks us 16th in the competition."
            },
            "score": 4
        },
        {
            "id": "5060cae1ff3c07877950d5d44158762eb20b455f",
            "paperId": "5060cae1ff3c07877950d5d44158762eb20b455f",
            "title": "DoCoGen: Domain Counterfactual Generation for Low Resource Domain Adaptation",
            "abstract": "Natural language processing (NLP) algorithms have become very successful, but they still struggle when applied to out-of-distribution examples. In this paper we propose a controllable generation approach in order to deal with this domain adaptation (DA) challenge. Given an input text example, our DoCoGen algorithm generates a domain-counterfactual textual example (D-con) - that is similar to the original in all aspects, including the task label, but its domain is changed to a desired one. Importantly, DoCoGen is trained using only unlabeled examples from multiple domains - no NLP task labels or parallel pairs of textual examples and their domain-counterfactuals are required. We show that DoCoGen can generate coherent counterfactuals consisting of multiple sentences. We use the D-cons generated by DoCoGen to augment a sentiment classifier and a multi-label intent classifier in 20 and 78 DA setups, respectively, where source-domain labeled data is scarce. Our model outperforms strong baselines and improves the accuracy of a state-of-the-art unsupervised DA algorithm.",
            "year": 2022,
            "citationCount": 32,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The proposed DoCoGen model outperforms strong baselines and improves the accuracy of a state-of-the-art unsupervised DA algorithm and can generate coherent counterfactuals consisting of multiple sentences."
            },
            "score": 4
        },
        {
            "id": "775c439186b037c09cd9f95b9daf81d23ca21b54",
            "paperId": "775c439186b037c09cd9f95b9daf81d23ca21b54",
            "title": "WinoDict: Probing language models for in-context word acquisition",
            "abstract": "We introduce a new in-context learning paradigm to measure Large Language Models\u2019 (LLMs) ability to learn novel words during inference. In particular, we rewrite Winograd-style co-reference resolution problems by replacing the key concept word with a synthetic but plausible word that the model must understand to complete the task. Solving this task requires the model to make use of the dictionary definition of the new word given in the prompt. This benchmark addresses word acquisition, one important aspect of the diachronic degradation known to afflict LLMs. As LLMs are frozen in time at the moment they are trained, they are normally unable to reflect the way language changes over time. We show that the accuracy of LLMs compared to the original Winograd tasks decreases radically in our benchmark, thus identifying a limitation of current models and providing a benchmark to measure future improvements in LLMs ability to do in-context learning.",
            "year": 2022,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new in-context learning paradigm is introduced to measure Large Language Models\u2019 (LLMs) ability to learn novel words during inference by rewriting Winograd-style co-reference resolution problems by replacing the key concept word with a synthetic but plausible word that the model must understand to complete the task."
            },
            "score": 3
        },
        {
            "id": "c435ecd0321dcec1f25e458bf930311f9e1d04b6",
            "paperId": "c435ecd0321dcec1f25e458bf930311f9e1d04b6",
            "title": "Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality",
            "abstract": "We present a novel task and dataset for evaluating the ability of vision and language models to conduct visio-linguistic compositional reasoning, which we call Winoground. Given two images and two captions, the goal is to match them correctly-but crucially, both captions contain a completely identical set of words, only in a different order. The dataset was carefully hand-curated by expert annotators and is labeled with a rich set offine-grained tags to assist in analyzing model performance. We probe a diverse range of state-of-the-art vision and language models and find that, surprisingly, none of them do much better than chance. Evidently, these models are not as skilled at visio-linguistic compositional reasoning as we might have hoped. We perform an extensive analysis to obtain insights into how future work might try to mitigate these models' shortcomings. We aim for Winoground to serve as a useful evaluation set for advancing the state of the art and driving further progress in the field. The dataset is available at https://huggingface.co/datasets/facebook/winoground.",
            "year": 2022,
            "citationCount": 219,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel task and dataset for evaluating the ability of vision and language models to conduct visio-linguistic compositional reasoning, which is called Winoground, and it is found that, surprisingly, none of them do much better than chance."
            },
            "score": 3
        },
        {
            "id": "b2c68b708a9f98996b18c8d21b53a815a2c46a8b",
            "paperId": "b2c68b708a9f98996b18c8d21b53a815a2c46a8b",
            "title": "ProPILE: Probing Privacy Leakage in Large Language Models",
            "abstract": "The rapid advancement and widespread use of large language models (LLMs) have raised significant concerns regarding the potential leakage of personally identifiable information (PII). These models are often trained on vast quantities of web-collected data, which may inadvertently include sensitive personal data. This paper presents ProPILE, a novel probing tool designed to empower data subjects, or the owners of the PII, with awareness of potential PII leakage in LLM-based services. ProPILE lets data subjects formulate prompts based on their own PII to evaluate the level of privacy intrusion in LLMs. We demonstrate its application on the OPT-1.3B model trained on the publicly available Pile dataset. We show how hypothetical data subjects may assess the likelihood of their PII being included in the Pile dataset being revealed. ProPILE can also be leveraged by LLM service providers to effectively evaluate their own levels of PII leakage with more powerful prompts specifically tuned for their in-house models. This tool represents a pioneering step towards empowering the data subjects for their awareness and control over their own data on the web.",
            "year": 2023,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ProPILE lets data subjects formulate prompts based on their own PII to evaluate the level of privacy intrusion in LLMs, and can be leveraged by LLM service providers to effectively evaluate their own levels of PII leakage with more powerful prompts specifically tuned for their in-house models."
            },
            "score": 3
        },
        {
            "id": "0db108da7811200b25f979d659269812d0b52b61",
            "paperId": "0db108da7811200b25f979d659269812d0b52b61",
            "title": "Counterfactual Adversarial Training for Improving Robustness of Pre-trained Language Models",
            "abstract": "One of the approaches for improving the robustness of NLP models is adversarial training by adversarial examples. However, in previous work on adversarial training, the adversarial examples were not guaranteed to be minimally edited and to change the model\u2019s prediction. Our hypothesis is adversarial training could make models more robust if the adversarial examples were guaranteed to be minimally edited and to change the model\u2019s prediction. We propose Counterfactual Adversarial Training (CAT), which uses counterfactual explanations to improve the robustness of the model. Our experiments on Natural Language Inference and Sentiment Analysis show that CAT significantly enhances out-of-the-box pre-trained NLP models on 11 datasets, indicating that CAT is a promising approach to improve the robustness of the pre-trained language models.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experiments on Natural Language Inference and Sentiment Analysis show that CAT significantly enhances out-of-the-box pre-trained NLP models on 11 datasets, indicating that CAT is a promising approach to improve the robustness of the pre-trained language models."
            },
            "score": 3
        },
        {
            "id": "430aa6966c15c4a20a4fb2d8383e136b9cb6cde7",
            "paperId": "430aa6966c15c4a20a4fb2d8383e136b9cb6cde7",
            "title": "Almanac: Retrieval-Augmented Language Models for Clinical Medicine",
            "abstract": "Large-language models have recently demonstrated impressive zero-shot capabilities in a variety of natural language tasks such as summarization, dialogue generation, and question-answering. Despite many promising applications in clinical medicine, adoption of these models in real-world settings has been largely limited by their tendency to generate incorrect and sometimes even toxic statements. In this study, we develop Almanac, a large language model framework augmented with retrieval capabilities for medical guideline and treatment recommendations. Performance on a novel dataset of clinical scenarios (n= 130) evaluated by a panel of 5 board-certified and resident physicians demonstrates significant increases in factuality (mean of 18% at p-value < 0.05) across all specialties, with improvements in completeness and safety. Our results demonstrate the potential for large language models to be effective tools in the clinical decision-making process, while also emphasizing the importance of careful testing and deployment to mitigate their shortcomings.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Almanac, a large language model framework augmented with retrieval capabilities for medical guideline and treatment recommendations, demonstrates the potential for large language models to be effective tools in the clinical decision-making process, while also emphasizing the importance of careful testing and deployment to mitigate their shortcomings."
            },
            "score": 3
        },
        {
            "id": "6489640b1d30a8a3e7cb906bb6557f1ccd0d799d",
            "paperId": "6489640b1d30a8a3e7cb906bb6557f1ccd0d799d",
            "title": "Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models",
            "abstract": "Retrieval-augmented language models (RALMs) represent a substantial advancement in the capabilities of large language models, notably in reducing factual hallucination by leveraging external knowledge sources. However, the reliability of the retrieved information is not always guaranteed. The retrieval of irrelevant data can lead to misguided responses, and potentially causing the model to overlook its inherent knowledge, even when it possesses adequate information to address the query. Moreover, standard RALMs often struggle to assess whether they possess adequate knowledge, both intrinsic and retrieved, to provide an accurate answer. In situations where knowledge is lacking, these systems should ideally respond with\"unknown\"when the answer is unattainable. In response to these challenges, we introduces Chain-of-Noting (CoN), a novel approach aimed at improving the robustness of RALMs in facing noisy, irrelevant documents and in handling unknown scenarios. The core idea of CoN is to generate sequential reading notes for retrieved documents, enabling a thorough evaluation of their relevance to the given question and integrating this information to formulate the final answer. We employed ChatGPT to create training data for CoN, which was subsequently trained on an LLaMa-2 7B model. Our experiments across four open-domain QA benchmarks show that RALMs equipped with CoN significantly outperform standard RALMs. Notably, CoN achieves an average improvement of +7.9 in EM score given entirely noisy retrieved documents and +10.5 in rejection rates for real-time questions that fall outside the pre-training knowledge scope.",
            "year": 2023,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Chain-of-Noting (CoN) is introduced, a novel approach aimed at improving the robustness of RALMs in facing noisy, irrelevant documents and in handling unknown scenarios, and achieves an average improvement of +7.9 in EM score given entirely noisy retrieved documents and +10.5 in rejection rates for real-time questions that fall outside the pre-training knowledge scope."
            },
            "score": 3
        },
        {
            "id": "663d743272e9ab04f54d9105a3c3a3f6e22dd1dd",
            "paperId": "663d743272e9ab04f54d9105a3c3a3f6e22dd1dd",
            "title": "FactKB: Generalizable Factuality Evaluation using Language Models Enhanced with Factual Knowledge",
            "abstract": "Evaluating the factual consistency of automatically generated summaries is essential for the progress and adoption of reliable summarization systems. Despite recent advances, existing factuality evaluation models are not robust, being especially prone to entity and relation errors in new domains. We propose FactKB, a simple new approach to factuality evaluation that is generalizable across domains, in particular with respect to entities and relations. FactKB is based on language models pretrained using facts extracted from external knowledge bases. We introduce three types of complementary factuality pretraining objectives based on direct entity facts, facts grounded in auxiliary knowledge about entities, and facts constructed compositionally through knowledge base walks. The resulting factuality evaluation model achieves state-of-the-art performance on two in-domain news summarization benchmarks as well as on three out-of-domain scientific literature datasets. Further analysis of FactKB shows improved ability to detect erroneous entities and relations in summaries and is robust and generalizable across domains.",
            "year": 2023,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "FactKB is a simple new approach to factuality evaluation that is generalizable across domains, in particular with respect to entities and relations and shows improved ability to detect erroneous entities and relation in summaries."
            },
            "score": 3
        },
        {
            "id": "cb9f74be6f257ebcfad1fa0dbc9f9c00be1d6b68",
            "paperId": "cb9f74be6f257ebcfad1fa0dbc9f9c00be1d6b68",
            "title": "LogicNMR: Probing the Non-monotonic Reasoning Ability of Pre-trained Language Models",
            "abstract": ",",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 3
        },
        {
            "id": "0eeca7d863b9bbda81fa5f92bf020ca0432e3f09",
            "paperId": "0eeca7d863b9bbda81fa5f92bf020ca0432e3f09",
            "title": "Response-Based and Counterfactual Learning for Sequence-to-Sequence Tasks in NLP",
            "abstract": "Many applications nowadays rely on statistical machine-learnt models, such as a rising number of virtual personal assistants. To train statistical models, typically large amounts of labelled data are required which are expensive and difficult to obtain. In this thesis, we investigate two approaches that alleviate the need for labelled data by leveraging feedback to model outputs instead. Both scenarios are applied to two sequence-to-sequence tasks for Natural Language Processing (NLP): machine translation and semantic parsing for question-answering. Additionally, we define a new question-answering task based on the geographical database OpenStreetMap (OSM) and collect a corpus, NLmaps v2, with 28,609 question-parse pairs. With the corpus, we build semantic parsers for subsequent experiments. Furthermore, we are the first to design a natural language interface to OSM, for which we specifically tailor a parser. The first approach to learn from feedback given to model outputs, considers a scenario where weak supervision is available by grounding the model in a downstream task for which labelled data has been collected. Feedback obtained from the downstream task is used to improve the model in a response-based on-policy learning setup. We apply this approach to improve a machine translation system, which is grounded in a multilingual semantic parsing task, by employing ramp loss objectives. Next, we improve a neural semantic parser where only gold answers, but not gold parses, are available, by lifting ramp loss objectives to non-linear neural networks. In the second approach to learn from feedback, instead of collecting expensive labelled data, a model is deployed and user-model interactions are recorded in a log. This log is used to improve a model in a counterfactual off-policy learning setup. We first exemplify this approach on a domain adaptation task for machine translation. Here, we show that counterfactual learning can be applied to tasks with large output spaces and, in contrast to prevalent theory, deterministic logs can successfully be used on sequence-to-sequence tasks for NLP. Next, we demonstrate on a semantic parsing task that counterfactual learning can also be applied when the underlying model is a neural network and feedback is collected from human users. Applying both approaches to the same semantic parsing task, allows us to draw a direct comparison between them. Response-based on-policy learning outperforms counterfactual off-policy learning, but requires expensive labelled data for the downstream task, whereas interaction logs for counterfactual learning can be easier to obtain in various scenarios.",
            "year": 2019,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This thesis investigates two approaches that alleviate the need for labelled data by leveraging feedback to model outputs instead and shows that counterfactual learning can be applied to tasks with large output spaces and deterministic logs can successfully be used on sequence-to-sequence tasks for NLP."
            },
            "score": 3
        },
        {
            "id": "df2113c867c4836a12dad9c697d11654539ae35e",
            "paperId": "df2113c867c4836a12dad9c697d11654539ae35e",
            "title": "Metaphors in Pre-Trained Language Models: Probing and Generalization Across Datasets and Languages",
            "abstract": "Human languages are full of metaphorical expressions. Metaphors help people understand the world by connecting new concepts and domains to more familiar ones. Large pre-trained language models (PLMs) are therefore assumed to encode metaphorical knowledge useful for NLP systems. In this paper, we investigate this hypothesis for PLMs, by probing metaphoricity information in their encodings, and by measuring the cross-lingual and cross-dataset generalization of this information. We present studies in multiple metaphor detection datasets and in four languages (i.e., English, Spanish, Russian, and Farsi). Our extensive experiments suggest that contextual representations in PLMs do encode metaphorical knowledge, and mostly in their middle layers. The knowledge is transferable between languages and datasets, especially when the annotation is consistent across training and testing sets. Our findings give helpful insights for both cognitive and NLP scientists.",
            "year": 2022,
            "citationCount": 32,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The authors' extensive experiments suggest that contextual representations in PLMs do encode metaphorical knowledge, and mostly in their middle layers, and the knowledge is transferable between languages and datasets, especially when the annotation is consistent across training and testing sets."
            },
            "score": 2
        },
        {
            "id": "0f6fe87afd1a3571f77c790893b03717e5d0422a",
            "paperId": "0f6fe87afd1a3571f77c790893b03717e5d0422a",
            "title": "Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators",
            "abstract": "Large language models (LLMs) outperform information retrieval techniques for downstream knowledge-intensive tasks when being prompted to generate world knowledge. However, community concerns abound regarding the factuality and potential implications of using this uncensored knowledge. In light of this, we introduce CONNER, a COmpreheNsive kNowledge Evaluation fRamework, designed to systematically and automatically evaluate generated knowledge from six important perspectives -- Factuality, Relevance, Coherence, Informativeness, Helpfulness and Validity. We conduct an extensive empirical analysis of the generated knowledge from three different types of LLMs on two widely studied knowledge-intensive tasks, i.e., open-domain question answering and knowledge-grounded dialogue. Surprisingly, our study reveals that the factuality of generated knowledge, even if lower, does not significantly hinder downstream tasks. Instead, the relevance and coherence of the outputs are more important than small factual mistakes. Further, we show how to use CONNER to improve knowledge-intensive tasks by designing two strategies: Prompt Engineering and Knowledge Selection. Our evaluation code and LLM-generated knowledge with human annotations will be released to facilitate future research.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Surprisingly, the study reveals that the factuality of generated knowledge, even if lower, does not significantly hinder downstream tasks and the relevance and coherence of the outputs are more important than small factual mistakes."
            },
            "score": 2
        },
        {
            "id": "af44c205c648c21d06064b23613dd60ecbd4adf8",
            "paperId": "af44c205c648c21d06064b23613dd60ecbd4adf8",
            "title": "Improving Zero-shot Visual Question Answering via Large Language Models with Reasoning Question Prompts",
            "abstract": "Zero-shot Visual Question Answering (VQA) is a prominent vision-language task that examines both the visual and textual understanding capability of systems in the absence of training data. Recently, by converting the images into captions, information across multi-modalities is bridged and Large Language Models (LLMs) can apply their strong zero-shot generalization capability to unseen questions. To design ideal prompts for solving VQA via LLMs, several studies have explored different strategies to select or generate question-answer pairs as the exemplar prompts, which guide LLMs to answer the current questions effectively. However, they totally ignore the role of question prompts. The original questions in VQA tasks usually encounter ellipses and ambiguity which require intermediate reasoning. To this end, we present Reasoning Question Prompts for VQA tasks, which can further activate the potential of LLMs in zero-shot scenarios. Specifically, for each question, we first generate self-contained questions as reasoning question prompts via an unsupervised question edition module considering sentence fluency, semantic integrity and syntactic invariance. Each reasoning question prompt clearly indicates the intent of the original question. This results in a set of candidate answers. Then, the candidate answers associated with their confidence scores acting as answer heuristics are fed into LLMs and produce the final answer. We evaluate reasoning question prompts on three VQA challenges, experimental results demonstrate that they can significantly improve the results of LLMs on zero-shot setting and outperform existing state-of-the-art zero-shot methods on three out of four data sets. Our source code is publicly released at https://github.com/ECNU-DASE-NLP/RQP.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Reasoning Question Prompts for VQA tasks are presented, which can further activate the potential of Large Language Models in zero-shot scenarios and can significantly improve the results of LLMs on zero- shot setting and outperform existing state-of-the-art zero-Shot methods on three out of four data sets."
            },
            "score": 2
        },
        {
            "id": "ce6a912dddb8d03313f72549896eaa217c056b75",
            "paperId": "ce6a912dddb8d03313f72549896eaa217c056b75",
            "title": "Probing NLP Conceptual Relatedness Judgments Through the Word-Based Board Game Codenames",
            "abstract": "In this paper I evaluate the ability of different Natural Language Processing (NLP) techniques to make human-like word relatedness judgements in a variant of the wordbased board game Codenames. I analyze a variety of statistical and knowledge based approaches, combinations of these, and techniques for incorporating the wider game context into relatedness judgements. While no approach explored here reaches human performance, simple word embedding based approaches incorporate a surprising amount of the useful information captured by other techniques. I attempt to characterize the limitations of these approaches in relation to human game play, although differences are largely not systematic. Finally, I discuss these results in terms of future directions for the field of NLP.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper evaluates the ability of different Natural Language Processing techniques to make human-like word relatedness judgements in a variant of the wordbased board game Codenames, and attempts to characterize the limitations of these approaches in relation to human game play."
            },
            "score": 2
        },
        {
            "id": "78104024e8b3b296956c4e2cc207c820fe7fae55",
            "paperId": "78104024e8b3b296956c4e2cc207c820fe7fae55",
            "title": "Some Languages are More Equal than Others: Probing Deeper into the Linguistic Disparity in the NLP World",
            "abstract": "Linguistic disparity in the NLP world is a problem that has been widely acknowledged recently. However, different facets of this problem, or the reasons behind this disparity are seldom discussed within the NLP community. This paper provides a comprehensive analysis of the disparity that exists within the languages of the world. We show that simply categorising languages considering data availability may not be always correct. Using an existing language categorisation based on speaker population and vitality, we analyse the distribution of language data resources, amount of NLP/CL research, inclusion in multilingual web-based platforms and the inclusion in pre-trained multilingual models. We show that many languages do not get covered in these resources or platforms, and even within the languages belonging to the same language group, there is wide disparity. We analyse the impact of family, geographical location, GDP and the speaker population of languages and provide possible reasons for this disparity, along with some suggestions to overcome the same.",
            "year": 2022,
            "citationCount": 15,
            "tldr": null,
            "score": 2
        },
        {
            "id": "8762fafce5194c0973aca322125dfbd836b2e080",
            "paperId": "8762fafce5194c0973aca322125dfbd836b2e080",
            "title": "Probing Decision Boundaries in Cancer Data Using Noise Injection and Counterfactual Analysis",
            "abstract": "Advanced analyses and computations based on gene expressions are prone to errors as they depend on experimental design, chemical operations/measurements and data analysis",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 2
        },
        {
            "id": "0427110f0e79f41e69a8eb00a3ec8868bac26a4f",
            "paperId": "0427110f0e79f41e69a8eb00a3ec8868bac26a4f",
            "title": "Do NLP Models Know Numbers? Probing Numeracy in Embeddings",
            "abstract": "The ability to understand and work with numbers (numeracy) is critical for many complex reasoning tasks. Currently, most NLP models treat numbers in text in the same way as other tokens\u2014they embed them as distributed vectors. Is this enough to capture numeracy? We begin by investigating the numerical reasoning capabilities of a state-of-the-art question answering model on the DROP dataset. We find this model excels on questions that require numerical reasoning, i.e., it already captures numeracy. To understand how this capability emerges, we probe token embedding methods (e.g., BERT, GloVe) on synthetic list maximum, number decoding, and addition tasks. A surprising degree of numeracy is naturally present in standard embeddings. For example, GloVe and word2vec accurately encode magnitude for numbers up to 1,000. Furthermore, character-level embeddings are even more precise\u2014ELMo captures numeracy the best for all pre-trained methods\u2014but BERT, which uses sub-word units, is less exact.",
            "year": 2019,
            "citationCount": 223,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates the numerical reasoning capabilities of a state-of-the-art question answering model on the DROP dataset and finds this model excels on questions that require numerical reasoning, i.e., it already captures numeracy."
            },
            "score": 2
        },
        {
            "id": "1321419b4e093ebd5064cd9c44b61c0d8b6c361d",
            "paperId": "1321419b4e093ebd5064cd9c44b61c0d8b6c361d",
            "title": "Probing What Different NLP Tasks Teach Machines about Function Word Comprehension",
            "abstract": "We introduce a set of nine challenge tasks that test for the understanding of function words. These tasks are created by structurally mutating sentences from existing datasets to target the comprehension of specific types of function words (e.g., prepositions, wh-words). Using these probing tasks, we explore the effects of various pretraining objectives for sentence encoders (e.g., language modeling, CCG supertagging and natural language inference (NLI)) on the learned representations. Our results show that pretraining on CCG\u2014our most syntactic objective\u2014performs the best on average across our probing tasks, suggesting that syntactic knowledge helps function word comprehension. Language modeling also shows strong performance, supporting its widespread use for pretraining state-of-the-art NLP models. Overall, no pretraining objective dominates across the board, and our function word probing tasks highlight several intuitive differences between pretraining objectives, e.g., that NLI helps the comprehension of negation.",
            "year": 2019,
            "citationCount": 97,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results show that pretraining on CCG\u2014the authors' most syntactic objective\u2014performs the best on average across their probing tasks, suggesting that syntactic knowledge helps function word comprehension."
            },
            "score": 2
        },
        {
            "id": "3d48f1b4f69f7d22a47fa1599a1fc76277ba0c65",
            "paperId": "3d48f1b4f69f7d22a47fa1599a1fc76277ba0c65",
            "title": "Evaluating Large Language Models in Theory of Mind Tasks",
            "abstract": "Eleven Large Language Models (LLMs) were assessed using a custom-made battery of false-belief tasks, considered a gold standard in testing Theory of Mind (ToM) in humans. The battery included 640 prompts spread across 40 diverse tasks, each one including a false-belief scenario, three closely matched true-belief control scenarios, and the reversed versions of all four. To solve a single task, a model needed to correctly answer 16 prompts across all eight scenarios. Smaller and older models solved no tasks; GPT-3-davinci-003 (from November 2022) and ChatGPT-3.5-turbo (from March 2023) solved 20% of the tasks; ChatGPT-4 (from June 2023) solved 75% of the tasks, matching the performance of six-year-old children observed in past studies. We explore the potential interpretation of these findings, including the intriguing possibility that ToM, previously considered exclusive to humans, may have spontaneously emerged as a byproduct of LLMs' improving language skills.",
            "year": 2023,
            "citationCount": 37,
            "tldr": null,
            "score": 1
        },
        {
            "id": "ee0eb5ee1d6720cfa798658f19502b6e170cebf6",
            "paperId": "ee0eb5ee1d6720cfa798658f19502b6e170cebf6",
            "title": "Metamorphosis Knowledge Probing of Guild Data through Chat Bot Using NLP",
            "abstract": "Artificial intelligence conversational entities, are called as chat-bots. Chat-bots are computer programs capable to carry out conversation between human and computer. This application can run on local computer, but it has to be accessed through the internet. Chat Bots conversation system is an intelligent human machine interaction using natural language which will be built using voice recognition techniques and artificial intelligence algorithms that will analyze user's queries which was given by the user and understand user's input [13]. This system is an application which will provide answers to the queries of the students. The machine has been installed with learning to distinguish the sentences and settling on a choice itself as a reaction to respond to an inquiry. Natural language processing algorithms are used for Noise Removal, Term frequency, Word Embedding and Cosine similarity of the content of the complaint. The biggest advantage is that student does not have to go to college for the enquiry and anyone can access this chat bot either through voice or text. Students just have to put their query to the bot by voice or text. Questions asked to the bot, which is not understood is further processed using a third-party expert, Admin and the response is archived, improving the artificial brain capabilities for future generation of responses.",
            "year": 2019,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This system is an application which will provide answers to the queries of the students and the biggest advantage is that student does not have to go to college for the enquiry and anyone can access this chat bot either through voice or text."
            },
            "score": 1
        }
    ],
    "novelty": "yes"
}