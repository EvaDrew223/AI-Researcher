{
    "topic_description": "novel prompting methods that can improve factuality and reduce hallucination of large language models",
    "idea_name": "Knowledge Retrieval Prompting",
    "raw_idea": {
        "Problem": "Large language models often generate hallucinated content that is not grounded in factual knowledge, leading to inaccurate and unreliable outputs.",
        "Existing Methods": "Current methods for reducing hallucination include using retrieval-augmented generation or fine-tuning the model on factual datasets. However, these approaches require additional training or external knowledge bases.",
        "Motivation": "We hypothesize that large language models have already encoded a vast amount of factual knowledge during pre-training, and that this knowledge can be leveraged to improve the factuality of generated outputs. By prompting the model to retrieve relevant facts from its own knowledge before generating an answer, we can encourage it to rely on factual information rather than hallucinating.",
        "Proposed Method": "We propose Knowledge Retrieval Prompting (KRP), a two-stage prompting method that encourages the model to retrieve relevant facts before generating an answer. Given a question, we first prompt the model with \"Retrieve relevant facts from your knowledge to answer the following question: [question]\". The model generates a list of facts that it believes are relevant to the question. We then prompt the model with \"Based on the retrieved facts, answer the following question: [question]\nRetrieved facts: [generated facts]\", encouraging it to generate an answer that is grounded in the retrieved facts.",
        "Experiment Plan": "We will evaluate KRP on a range of factual question-answering datasets, such as Natural Questions and TriviaQA. We will compare the factuality and accuracy of answers generated by KRP to those generated by zero-shot prompting, few-shot prompting, and retrieval-augmented generation baselines. We will also conduct a human evaluation to assess the quality and factuality of the generated answers."
    },
    "full_experiment_plan": {
        "Title": "Knowledge Retrieval Prompting: Leveraging Pre-trained Knowledge for Factual Question Answering",
        "Problem Statement": "Large language models often generate hallucinated content that is not grounded in factual knowledge, leading to inaccurate and unreliable outputs. This is a significant issue for applications that require factual correctness, such as question answering systems.",
        "Motivation": "Current methods for reducing hallucination, such as retrieval-augmented generation or fine-tuning on factual datasets, require additional training or external knowledge bases. However, we hypothesize that large language models have already encoded a vast amount of factual knowledge during pre-training, and that this knowledge can be leveraged to improve the factuality of generated outputs without the need for additional training or external resources. By prompting the model to retrieve relevant facts from its own knowledge before generating an answer, we can encourage it to rely on factual information rather than hallucinating.",
        "Proposed Method": "We propose Knowledge Retrieval Prompting (KRP), a two-stage prompting method that encourages the model to retrieve relevant facts before generating an answer. Given a question, we first prompt the model with \"Retrieve relevant facts from your knowledge to answer the following question: [question]\". The model generates a list of facts that it believes are relevant to the question. We then prompt the model with \"Based on the retrieved facts, answer the following question: [question]\nRetrieved facts: [generated facts]\", encouraging it to generate an answer that is grounded in the retrieved facts.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We will evaluate KRP on a range of factual question-answering datasets, such as Natural Questions, TriviaQA, and WebQuestions. These datasets cover a wide range of domains and question types, making them suitable for evaluating the effectiveness of KRP in improving factual correctness. We will use the official train/dev/test splits provided by the dataset authors.",
            "Step 2: Construct Prompts": "For each dataset, we will construct the following prompts:\n1. Zero-shot: The question alone, without any additional prompting.\n2. Few-shot: The question preceded by 3-5 examples of question-answer pairs from the training set.\n3. KRP (fact retrieval): \"Retrieve relevant facts from your knowledge to answer the following question: [question]\"\n4. KRP (answer generation): \"Based on the retrieved facts, answer the following question: [question]\nRetrieved facts: [generated facts]\"\nWe will experiment with different variations of the KRP prompts, such as changing the wording or adding more instructions, to find the most effective formulation.",
            "Step 3: Select Models": "We will evaluate KRP on the following models:\n1. GPT-3 (davinci)\n2. GPT-3.5 (text-davinci-002)\n3. GPT-4\nThese models represent the state-of-the-art in language modeling and have been shown to have strong performance on a wide range of tasks.",
            "Step 4: Get Results": "For each combination of dataset, prompt, and model, we will generate answers for the test set questions. We will evaluate the generated answers using the following metrics:\n1. Exact match accuracy\n2. F1 score\n3. Precision\n4. Recall\nWe will compare the performance of KRP to the zero-shot and few-shot baselines to determine if retrieving relevant facts improves factual correctness.",
            "Step 5: Analyze Results": "We will perform a detailed analysis of the results to answer the following questions:\n1. Does KRP improve factual correctness compared to zero-shot and few-shot prompting?\n2. How does the effectiveness of KRP vary across different datasets and question types?\n3. What types of facts are most commonly retrieved by the model, and how do they relate to the question?\n4. How do the different variations of the KRP prompts affect performance?\n5. Are there any common failure modes or limitations of KRP?\nBased on this analysis, we will draw conclusions about the effectiveness of KRP and identify potential areas for future improvement."
        },
        "Test Case Examples": {
            "Test Case 1": {
                "Question": "What is the capital of France?",
                "Zero-shot Prompt": "What is the capital of France?",
                "Zero-shot Output": "The capital of France is Paris.",
                "Few-shot Prompt": "Q: What is the capital of Germany?\nA: The capital of Germany is Berlin.\n\nQ: What is the capital of Italy?\nA: The capital of Italy is Rome.\n\nQ: What is the capital of France?",
                "Few-shot Output": "The capital of France is Paris.",
                "KRP (fact retrieval) Prompt": "Retrieve relevant facts from your knowledge to answer the following question: What is the capital of France?",
                "KRP (fact retrieval) Output": "1. France is a country located in Western Europe.\n2. Paris is the capital and largest city of France.\n3. Paris is known for its art, fashion, cuisine, and architecture.\n4. Notable landmarks in Paris include the Eiffel Tower, the Louvre Museum, and the Notre-Dame Cathedral.",
                "KRP (answer generation) Prompt": "Based on the retrieved facts, answer the following question: What is the capital of France?\nRetrieved facts:\n1. France is a country located in Western Europe.\n2. Paris is the capital and largest city of France.\n3. Paris is known for its art, fashion, cuisine, and architecture.\n4. Notable landmarks in Paris include the Eiffel Tower, the Louvre Museum, and the Notre-Dame Cathedral.",
                "KRP (answer generation) Output": "Paris is the capital of France.",
                "Explanation": "In this example, all methods generate the correct answer. However, KRP provides additional context by retrieving relevant facts about France and Paris, which could be useful for more complex questions."
            },
            "Test Case 2": {
                "Question": "Who wrote the novel 'Pride and Prejudice'?",
                "Zero-shot Prompt": "Who wrote the novel 'Pride and Prejudice'?",
                "Zero-shot Output": "Pride and Prejudice was written by the English novelist Jane Austen.",
                "Few-shot Prompt": "Q: Who wrote the novel 'To Kill a Mockingbird'?\nA: To Kill a Mockingbird was written by Harper Lee.\n\nQ: Who wrote the novel '1984'?\nA: 1984 was written by George Orwell.\n\nQ: Who wrote the novel 'Pride and Prejudice'?",
                "Few-shot Output": "The novel Pride and Prejudice was written by Jane Austen.",
                "KRP (fact retrieval) Prompt": "Retrieve relevant facts from your knowledge to answer the following question: Who wrote the novel 'Pride and Prejudice'?",
                "KRP (fact retrieval) Output": "1. Pride and Prejudice is a romantic novel published in 1813.\n2. The novel was written by Jane Austen, an English novelist.\n3. Jane Austen was born on December 16, 1775, in Steventon, Hampshire, England.\n4. Austen's other notable works include Sense and Sensibility, Mansfield Park, and Emma.",
                "KRP (answer generation) Prompt": "Based on the retrieved facts, answer the following question: Who wrote the novel 'Pride and Prejudice'?\nRetrieved facts:\n1. Pride and Prejudice is a romantic novel published in 1813.\n2. The novel was written by Jane Austen, an English novelist.\n3. Jane Austen was born on December 16, 1775, in Steventon, Hampshire, England.\n4. Austen's other notable works include Sense and Sensibility, Mansfield Park, and Emma.",
                "KRP (answer generation) Output": "The novel 'Pride and Prejudice' was written by Jane Austen, an English novelist born in 1775. Austen published the romantic novel in 1813 and is also known for other works such as Sense and Sensibility, Mansfield Park, and Emma.",
                "Explanation": "In this example, KRP retrieves relevant facts about the author Jane Austen and the novel Pride and Prejudice. This additional context allows the model to generate a more detailed and informative answer compared to the zero-shot and few-shot baselines."
            }
        },
        "Fallback Plan": "If the proposed KRP method does not significantly improve factual correctness over the baselines, we will perform additional analysis to understand why. Some potential steps include:\n1. Analyzing the quality and relevance of the retrieved facts. If the retrieved facts are not relevant or accurate, this could explain why KRP does not improve performance. We can manually inspect a sample of the retrieved facts and assess their quality.\n2. Experimenting with different prompts and instructions for the fact retrieval and answer generation steps. The specific wording of the prompts could have a significant impact on the model's behavior. We can try different variations and see if they lead to better results.\n3. Investigating whether the model is actually using the retrieved facts to generate its answers. It's possible that the model is simply ignoring the retrieved facts and generating answers based on its general knowledge. We can compare the generated answers to the retrieved facts and see if there is any overlap or consistency.\n4. Exploring alternative methods for incorporating retrieved knowledge into the answer generation process. Instead of simply concatenating the retrieved facts to the prompt, we could try other approaches such as using them to guide the decoding process or conditioning the language model on them.\nIf none of these steps lead to improved results, we can still gain valuable insights into the limitations of using pre-trained knowledge for factual question answering. We can write up our findings as an analysis paper, discussing the challenges and potential future directions for this line of research."
    },
    "novelty_queries": [
        "KeywordQuery(\"knowledge retrieval prompting language models\")",
        "KeywordQuery(\"factual question answering language models\")",
        "KeywordQuery(\"leveraging pre-trained knowledge language models\")",
        "KeywordQuery(\"reducing hallucination language models\")",
        "KeywordQuery(\"Knowledge Retrieval Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "1e122149779c644855d1cccca5d96135db0482cb",
            "paperId": "1e122149779c644855d1cccca5d96135db0482cb",
            "title": "Self-Prompting Large Language Models for Open-Domain QA",
            "abstract": "Open-Domain Question Answering (ODQA) requires models to answer factoid questions with no context given. The common way for this task is to train models on a large-scale annotated dataset to retrieve related documents and generate answers based on these documents. In this paper, we show that the ODQA architecture can be dramatically simpli\ufb01ed by treating Large Language Models (LLMs) as a knowledge corpus and pro-pose a Self-Prompting framework for LLMs to perform ODQA so as to eliminate the need for training data and external knowledge corpus. Concretely, we \ufb01rstly generate multiple pseudo QA pairs with background passages and one-sentence explanations for these QAs by prompting LLMs step by step and then leverage the generated QA pairs for in-context learning. Experimental results show our method surpasses previous state-of-the-art methods by +8.8 EM averagely on three widely-used ODQA datasets, and even achieves comparable performance with several retrieval-augmented \ufb01ne-tuned models.",
            "year": 2022,
            "citationCount": 33,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper shows that the ODQA architecture can be dramatically simplified by treating Large Language Models (LLMs) as a knowledge corpus and pro-pose a Self-Prompting framework for LLMs to perform ODQA so as to eliminate the need for training data and external knowledge corpus."
            },
            "score": 8,
            "novelty_score": "The research problem in the project proposal is improving the factual correctness of large language models in question answering by leveraging their pre-trained knowledge. The proposed approach is Knowledge Retrieval Prompting (KRP), a two-stage prompting method that encourages the model to retrieve relevant facts before generating an answer.\n\nThe research problem in the paper is simplifying the Open-Domain Question Answering (ODQA) architecture by treating Large Language Models (LLMs) as a knowledge corpus. The proposed approach is a Self-Prompting framework for LLMs to perform ODQA without the need for training data and external knowledge corpus.\n\nWhile both the project proposal and the paper aim to improve question answering using large language models, their specific research problems and approaches differ. The project proposal focuses on improving factual correctness by prompting the model to retrieve relevant facts, while the paper aims to simplify the ODQA architecture by using LLMs as a knowledge corpus and generating pseudo QA pairs for in-context learning.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "9cd329e3b86e6869e73a91c467459b1947655b07",
            "paperId": "9cd329e3b86e6869e73a91c467459b1947655b07",
            "title": "Self-Prompting Large Language Models for Zero-Shot Open-Domain QA",
            "abstract": "Open-Domain Question Answering (ODQA) aims to answer questions without explicitly providing specific background documents. This task becomes notably challenging in a zero-shot setting where no data is available to train tailored retrieval-reader models. While recent Large Language Models (LLMs) like GPT-3 have demonstrated their effectiveness in zero-shot ODQA using direct prompting methods, these methods still fall short of fully harnessing the potential of LLMs when implicitly invoked. In this paper, we propose a Self-Prompting framework to explicitly utilize the massive knowledge encoded in the parameters of LLMs and their strong instruction understanding abilities. Concretely, we prompt LLMs step by step to generate multiple pseudo QA pairs with background passages and explanations entirely from scratch. These generated elements are then utilized for in-context learning. Experimental results show that our method significantly surpasses previous state-of-the-art zero-shot methods on three widely-used ODQA datasets and even achieves comparable performance with various customized fine-tuned models on full training data. Our code is available at https://github.com/lockon-n/self-prompting.",
            "year": 2022,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Self-Prompting framework to explicitly utilize the massive knowledge encoded in the parameters of LLMs and their strong instruction understanding abilities and even achieves comparable performance with various customized fine-tuned models on full training data."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is improving the factual correctness of large language models in question answering by leveraging their pre-trained knowledge. The approach is to use a two-stage prompting method called Knowledge Retrieval Prompting (KRP) that encourages the model to retrieve relevant facts before generating an answer.\n\nThe research problem in the paper is zero-shot open-domain question answering using large language models. The approach is a self-prompting framework that prompts the model to generate pseudo QA pairs with background passages and explanations, which are then used for in-context learning.\n\nWhile both the proposal and the paper aim to improve question answering using large language models, their specific research problems and approaches differ. The proposal focuses on factual correctness and uses a two-stage prompting method, while the paper focuses on zero-shot performance and uses a self-prompting framework to generate pseudo QA pairs.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "490d8006851b1562cfd9ec1f057471f2868289d1",
            "paperId": "490d8006851b1562cfd9ec1f057471f2868289d1",
            "title": "Rethinking with Retrieval: Faithful Large Language Model Inference",
            "abstract": "Despite the success of large language models (LLMs) in various natural language processing (NLP) tasks, the stored knowledge in these models may inevitably be incomplete, out-of-date, or incorrect. This motivates the need to utilize external knowledge to assist LLMs. Unfortunately, current methods for incorporating external knowledge often require additional training or fine-tuning, which can be costly and may not be feasible for LLMs. To address this issue, we propose a novel post-processing approach, rethinking with retrieval (RR), which retrieves relevant external knowledge based on the decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting. This lightweight approach does not require additional training or fine-tuning and is not limited by the input length of LLMs. We evaluate the effectiveness of RR through extensive experiments with GPT-3 on three complex reasoning tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our results show that RR can produce more faithful explanations and improve the performance of LLMs.",
            "year": 2022,
            "citationCount": 101,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel post-processing approach, rethinking with retrieval (RR), which retrieves relevant external knowledge based on the decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting, which can produce more faithful explanations and improve the performance of LLMs."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the factual correctness of large language models in question answering without additional training or external resources. The proposed approach is Knowledge Retrieval Prompting (KRP), a two-stage prompting method that encourages the model to retrieve relevant facts before generating an answer.\n\nThe research problem in the paper is utilizing external knowledge to assist large language models without additional training or fine-tuning. The proposed approach is Rethinking with Retrieval (RR), a post-processing method that retrieves relevant external knowledge based on the decomposed reasoning steps obtained from chain-of-thought prompting.\n\nWhile both the proposal and the paper aim to improve the performance of large language models by incorporating external knowledge, their specific research problems and proposed approaches differ. The proposal focuses on factual correctness in question answering and uses a two-stage prompting method, while the paper addresses complex reasoning tasks and uses a post-processing approach based on chain-of-thought prompting.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "240b0caabb415578bdea4da7d0a32bdff2e8163f",
            "paperId": "240b0caabb415578bdea4da7d0a32bdff2e8163f",
            "title": "Editing Factual Knowledge in Language Models",
            "abstract": "The factual knowledge acquired during pre-training and stored in the parameters of Language Models (LMs) can be useful in downstream tasks (e.g., question answering or textual inference). However, some facts can be incorrectly induced or become obsolete over time. We present KnowledgeEditor, a method which can be used to edit this knowledge and, thus, fix \u2018bugs\u2019 or unexpected predictions without the need for expensive re-training or fine-tuning. Besides being computationally efficient, KnowledgeEditordoes not require any modifications in LM pre-training (e.g., the use of meta-learning). In our approach, we train a hyper-network with constrained optimization to modify a fact without affecting the rest of the knowledge; the trained hyper-network is then used to predict the weight update at test time. We show KnowledgeEditor\u2019s efficacy with two popular architectures and knowledge-intensive tasks: i) a BERT model fine-tuned for fact-checking, and ii) a sequence-to-sequence BART model for question answering. With our method, changing a prediction on the specific wording of a query tends to result in a consistent change in predictions also for its paraphrases. We show that this can be further encouraged by exploiting (e.g., automatically-generated) paraphrases during training. Interestingly, our hyper-network can be regarded as a \u2018probe\u2019 revealing which components need to be changed to manipulate factual knowledge; our analysis shows that the updates tend to be concentrated on a small subset of components. Source code available at https://github.com/nicola-decao/KnowledgeEditor",
            "year": 2021,
            "citationCount": 279,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents KnowledgeEditor, a method which can be used to edit factual knowledge and, thus, fix \u2018bugs\u2019 or unexpected predictions without the need for expensive re-training or fine-tuning."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the factual correctness of language models in question answering by leveraging pre-trained knowledge, while the approach is to use a two-stage prompting method that first retrieves relevant facts and then generates an answer based on those facts.\n\nThe research problem in the paper is fixing incorrect or outdated factual knowledge in language models, while the approach is to train a hyper-network to modify specific facts without affecting the rest of the knowledge.\n\nThe proposal focuses on improving factual correctness in question answering using a prompting method, while the paper focuses on editing specific facts in language models using a hyper-network. The research problems and approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "2986b2b06173e065c94bae49c7a9a3718dad486c",
            "paperId": "2986b2b06173e065c94bae49c7a9a3718dad486c",
            "title": "Reducing hallucination in structured outputs via Retrieval-Augmented Generation",
            "abstract": "A common and fundamental limitation of Generative AI (GenAI) is its propensity to hallucinate. While large language models (LLM) have taken the world by storm, without eliminating or at least reducing hallucinations, real-world GenAI systems may face challenges in user adoption. In the process of deploying an enterprise application that produces workflows based on natural language requirements, we devised a system leveraging Retrieval Augmented Generation (RAG) to greatly improve the quality of the structured output that represents such workflows. Thanks to our implementation of RAG, our proposed system significantly reduces hallucinations in the output and improves the generalization of our LLM in out-of-domain settings. In addition, we show that using a small, well-trained retriever encoder can reduce the size of the accompanying LLM, thereby making deployments of LLM-based systems less resource-intensive.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This proposed system significantly reduces hallucinations in the output and improves the generalization of the LLM in out-of-domain settings, and it is shown that using a small, well-trained retriever encoder can reduce the size of the accompanying LLM, thereby making deployments of LLM-based systems less resource-intensive."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is reducing hallucination in factual question answering using large language models, and the proposed approach is to use knowledge retrieval prompting to leverage the model's pre-trained knowledge. The research problem in the paper is reducing hallucination in structured outputs, and the proposed approach is to use retrieval-augmented generation.\n\nWhile both works aim to reduce hallucination, the proposal focuses specifically on factual question answering, while the paper tackles the more general problem of structured outputs. The methods are also different: the proposal uses knowledge retrieval prompting, while the paper uses retrieval-augmented generation.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "03532123ccffae8d411264320e8a5ae2b6eddea0",
            "paperId": "03532123ccffae8d411264320e8a5ae2b6eddea0",
            "title": "Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP",
            "abstract": "Retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). Existing work has combined these in simple\"retrieve-then-read\"pipelines in which the RM retrieves passages that are inserted into the LM prompt. To begin to fully realize the potential of frozen LMs and RMs, we propose Demonstrate-Search-Predict (DSP), a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LM and RM can handle more reliably. We have written novel DSP programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art in-context learning results and delivering 37-120%, 8-39%, and 80-290% relative gains against the vanilla LM (GPT-3.5), a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline, respectively. We release DSP at https://github.com/stanfordnlp/dsp",
            "year": 2022,
            "citationCount": 133,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Demonstrate-Search-Predict (DSP) is proposed, a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM and can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the factual correctness of language model outputs for question answering by leveraging pre-trained knowledge. The proposed approach is Knowledge Retrieval Prompting (KRP), a two-stage prompting method that encourages the model to retrieve relevant facts before generating an answer.\n\nThe research problem in the paper is addressing knowledge-intensive tasks using frozen language models and retrieval models. The proposed approach is Demonstrate-Search-Predict (DSP), a framework that passes natural language texts between a language model and a retrieval model in sophisticated pipelines.\n\nWhile both works involve using retrieval to improve language model performance on knowledge-intensive tasks, the proposal focuses specifically on factual question answering and leveraging pre-trained knowledge, while the paper proposes a more general framework for composing retrieval and language models in various settings.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "a26fbe4e08cc37f48661c2adb538111248e42f71",
            "paperId": "a26fbe4e08cc37f48661c2adb538111248e42f71",
            "title": "ChatENT: Augmented Large Language Models for Expert Knowledge Retrieval in Otolaryngology - Head and Neck Surgery",
            "abstract": "The recent surge in popularity of Large Language Models (LLMs), such as ChatGPT, has showcased their proficiency in medical examinations and potential contributions to medical education. However, LLMs possess inherent limitations, including inconsistent accuracy, specific prompting requirements, and the risk of generating harmful hallucinations. A domain-specific, fine-tuned model would address these limitations effectively. OHNS-relevant data was systematically gathered from open-access internet sources and indexed into a database. We leveraged Retrieval-Augmented Language Modeling (RALM) to recall this information and used it for pre-training, which was then integrated into ChatGPT 4.0, creating a OHNS specific knowledge Q&A platform known as ChatENT. ChatENT showed enhanced performance in the analysis and interpretation of OHNS information, outperforming ChatGPT 4.0 in both the Canadian Royal College OHNS sample examination questions challenge and the US board practice questions challenge, with a 58.4% and 26.0% error reduction, respectively. ChatENT generated fewer hallucinations and demonstrated greater consistency.To the best of our knowledge, ChatENT is the first specialty-specific LLM in the medical field. It appears to have considerable promise in areas such as medical education, patient education, and clinical decision support. The fine-tuned model has demonstrated the capacity to overcome the limitations of existing LLMs, thereby signaling a future of more precise, safe, and user-friendly applications in the realm of OHNS.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The fine-tuned model has demonstrated the capacity to overcome the limitations of existing LLMs, thereby signaling a future of more precise, safe, and user-friendly applications in the realm of OHNS."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the factual correctness of large language models in question answering by leveraging their pre-trained knowledge. The approach is to use a two-stage prompting method called Knowledge Retrieval Prompting (KRP) that first retrieves relevant facts from the model's knowledge and then generates an answer based on those facts.\n\nThe research problem in the paper is enhancing the performance of large language models in the domain of otolaryngology - head and neck surgery (OHNS) by fine-tuning them on domain-specific data. The approach is to gather OHNS-relevant data, use it for pre-training, and integrate it into ChatGPT 4.0 using Retrieval-Augmented Language Modeling (RALM) to create a specialty-specific model called ChatENT.\n\nWhile both the proposal and the paper aim to improve the performance of large language models, they focus on different aspects. The proposal targets factual correctness in general question answering, while the paper focuses on domain-specific knowledge in OHNS. Additionally, the approaches differ: the proposal uses a novel prompting method, while the paper employs fine-tuning and RALM.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "49408c5e1ac75854f1580e561384df2be870d559",
            "paperId": "49408c5e1ac75854f1580e561384df2be870d559",
            "title": "KnowledGPT: Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases",
            "abstract": "Large language models (LLMs) have demonstrated impressive impact in the field of natural language processing, but they still struggle with several issues regarding, such as completeness, timeliness, faithfulness and adaptability. While recent efforts have focuses on connecting LLMs with external knowledge sources, the integration of knowledge bases (KBs) remains understudied and faces several challenges. In this paper, we introduce KnowledGPT, a comprehensive framework to bridge LLMs with various knowledge bases, facilitating both the retrieval and storage of knowledge. The retrieval process employs the program of thought prompting, which generates search language for KBs in code format with pre-defined functions for KB operations. Besides retrieval, KnowledGPT offers the capability to store knowledge in a personalized KB, catering to individual user demands. With extensive experiments, we show that by integrating LLMs with KBs, KnowledGPT properly answers a broader range of questions requiring world knowledge compared with vanilla LLMs, utilizing both knowledge existing in widely-known KBs and extracted into personalized KBs.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "By integrating LLMs with KBs, KnowledGPT properly answers a broader range of questions requiring world knowledge compared with vanilla LLMs, utilizing both knowledge existing in widely-known KBs and extracted into personalized KBs."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the factual correctness of large language models in question answering by leveraging their pre-trained knowledge. The proposed approach is Knowledge Retrieval Prompting (KRP), a two-stage prompting method that encourages the model to retrieve relevant facts before generating an answer.\n\nThe research problem in the paper is enhancing large language models with retrieval and storage access on knowledge bases to address issues like completeness, timeliness, faithfulness, and adaptability. The proposed approach is KnowledGPT, a framework that bridges LLMs with various knowledge bases, facilitating both the retrieval and storage of knowledge.\n\nWhile both the proposal and the paper aim to improve the performance of large language models by integrating them with external knowledge, the specific research problems and approaches are different. The proposal focuses on factual correctness in question answering and uses prompting to retrieve knowledge from the model's pre-trained knowledge. In contrast, the paper addresses a broader range of issues and proposes a framework for retrieving and storing knowledge from external knowledge bases.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "c49fd6cac5382cdbc2bc31be195e42bc28dc615d",
            "paperId": "c49fd6cac5382cdbc2bc31be195e42bc28dc615d",
            "title": "Tree of Clarifications: Answering Ambiguous Questions with Retrieval-Augmented Large Language Models",
            "abstract": "Questions in open-domain question answering are often ambiguous, allowing multiple interpretations. One approach to handling them is to identify all possible interpretations of the ambiguous question (AQ) and to generate a long-form answer addressing them all, as suggested by Stelmakh et al., (2022). While it provides a comprehensive response without bothering the user for clarification, considering multiple dimensions of ambiguity and gathering corresponding knowledge remains a challenge. To cope with the challenge, we propose a novel framework, Tree of Clarifications (ToC): It recursively constructs a tree of disambiguations for the AQ -- via few-shot prompting leveraging external knowledge -- and uses it to generate a long-form answer. ToC outperforms existing baselines on ASQA in a few-shot setup across the metrics, while surpassing fully-supervised baselines trained on the whole training set in terms of Disambig-F1 and Disambig-ROUGE. Code is available at https://github.com/gankim/tree-of-clarifications.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel framework, Tree of Clarifications (ToC), recursively constructs a tree of disambiguations for the AQ -- via few-shot prompting leveraging external knowledge -- and uses it to generate a long-form answer."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve the factual correctness of large language models in question answering by prompting them to retrieve relevant facts from their own knowledge before generating an answer. The paper focuses on answering ambiguous questions by recursively constructing a tree of disambiguations and using it to generate a long-form answer.\n\nThe project proposal addresses the problem of hallucination in large language models and proposes a prompting method to leverage pre-trained knowledge for factual question answering. The paper, on the other hand, tackles the issue of ambiguity in open-domain questions and introduces a framework to generate comprehensive answers by disambiguating the question.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "ccc772d88c231275f24c4fac9b28bbe0942e1107",
            "paperId": "ccc772d88c231275f24c4fac9b28bbe0942e1107",
            "title": "Query2doc: Query Expansion with Large Language Models",
            "abstract": "This paper introduces a simple yet effective query expansion approach, denoted as query2doc, to improve both sparse and dense retrieval systems. The proposed method first generates pseudo-documents by few-shot prompting large language models (LLMs), and then expands the query with generated pseudo-documents. LLMs are trained on web-scale text corpora and are adept at knowledge memorization. The pseudo-documents from LLMs often contain highly relevant information that can aid in query disambiguation and guide the retrievers. Experimental results demonstrate that query2doc boosts the performance of BM25 by 3% to 15% on ad-hoc IR datasets, such as MS-MARCO and TREC DL, without any model fine-tuning. Furthermore, our method also benefits state-of-the-art dense retrievers in terms of both in-domain and out-of-domain results.",
            "year": 2023,
            "citationCount": 30,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a simple yet effective query expansion approach, denoted as query2doc, to improve both sparse and dense retrieval systems, and benefits state-of-the-art dense retrievers in terms of both in-domain and out- of-domain results."
            },
            "score": 6,
            "novelty_score": "The research problem in the project proposal is improving the factual correctness of large language models in question answering by leveraging their pre-trained knowledge. The proposed approach is Knowledge Retrieval Prompting (KRP), a two-stage prompting method that encourages the model to retrieve relevant facts before generating an answer.\n\nThe research problem in the paper is improving the performance of sparse and dense retrieval systems. The proposed approach is query2doc, which generates pseudo-documents using large language models and expands the query with the generated pseudo-documents.\n\nWhile both the project proposal and the paper aim to leverage large language models, their research problems and approaches are different. The project proposal focuses on improving factual correctness in question answering, while the paper focuses on improving retrieval systems. The project proposal uses prompting to retrieve facts from the model's pre-trained knowledge, while the paper generates pseudo-documents to expand queries.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "c70eb74e09c41e8fcc71dd59e3b4d631f657f7cd",
            "paperId": "c70eb74e09c41e8fcc71dd59e3b4d631f657f7cd",
            "title": "Internet-augmented language models through few-shot prompting for open-domain question answering",
            "abstract": "In this work, we aim to capitalize on the unique few-shot capabilities of large-scale language models (LSLMs) to overcome some of their challenges with respect to grounding to factual and up-to-date information. Motivated by semi-parametric language models (LMs), which ground their decisions in external retrieved evidence, we use few-shot prompting to learn to condition LMs on information returned from the web using Google Search, a broad and constantly updated knowledge source. Our approach does not involve fine-tuning or learning additional parameters, thus making it applicable to any LM, offering therefore a strong baseline. Indeed, we find that LMs conditioned on the web surpass performance of closed-book models of similar, or even larger, model sizes in open-domain question answering. Finally, we find that increasing the inference-time compute of models, achieved via using multiple retrieved evidences to generate multiple answers followed by a reranking stage that uses scores generated by the same LMs, leads to better performance and alleviates lower performance of smaller few-shot LMs. All in all, our findings suggest that it might be beneficial to slow down the race towards the biggest model and instead shift attention towards finding more effective ways to use models, including but not limited to, better prompting or increasing inference-time compute.",
            "year": 2022,
            "citationCount": 79,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Motivated by semi-parametric language models (LMs), few-shot prompting is used to learn to condition LMs on information returned from the web using Google Search, a broad and constantly updated knowledge source, making it applicable to any LM, offering therefore a strong baseline."
            },
            "score": 6
        },
        {
            "id": "48a91be5a11140010fe49a912ca834dc3feb50b2",
            "paperId": "48a91be5a11140010fe49a912ca834dc3feb50b2",
            "title": "On Early Detection of Hallucinations in Factual Question Answering",
            "abstract": "While large language models (LLMs) have taken great strides towards helping humans with a plethora of tasks like search and summarization, hallucinations remain a major impediment towards gaining user trust. The fluency and coherence of model generations even when hallucinating makes it difficult to detect whether or not a model is hallucinating. In this work, we explore if the artifacts associated with the model generations can provide hints that the generation will contain hallucinations. Specifically, we probe LLMs at 1) the inputs via Integrated Gradients based token attribution, 2) the outputs via the Softmax probabilities, and 3) the internal state via self-attention and fully-connected layer activations for signs of hallucinations on open-ended question answering tasks. Our results show that the distributions of these artifacts differ between hallucinated and non-hallucinated generations. Building on this insight, we train binary classifiers that use these artifacts as input features to classify model generations into hallucinations and non-hallucinations. These hallucination classifiers achieve up to 0.80 AUROC. We further show that tokens preceding a hallucination can predict the subsequent hallucination before it occurs.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Binary classifiers that use artifacts associated with the model generations as input features to classify model generations into hallucinations and non-hallucinations are trained and it is shown that tokens preceding a hallucination can predict the subsequent hallucination before it occurs."
            },
            "score": 6
        },
        {
            "id": "84b77180228051040286423cec82b62c323a8fda",
            "paperId": "84b77180228051040286423cec82b62c323a8fda",
            "title": "Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation",
            "abstract": "Knowledge-intensive tasks (e.g., open-domain question answering (QA)) require a substantial amount of factual knowledge and often rely on external information for assistance. Recently, large language models (LLMs) (e.g., ChatGPT), have demonstrated impressive prowess in solving a wide range of tasks with world knowledge, including knowledge-intensive tasks. However, it remains unclear how well LLMs are able to perceive their factual knowledge boundaries, particularly how they behave when incorporating retrieval augmentation. In this study, we present an initial analysis of the factual knowledge boundaries of LLMs and how retrieval augmentation affects LLMs on open-domain QA. Specially, we focus on three primary research questions and analyze them by examining QA performance, priori judgement and posteriori judgement of LLMs. We show evidence that LLMs possess unwavering confidence in their capabilities to respond to questions and the accuracy of their responses. Furthermore, retrieval augmentation proves to be an effective approach in enhancing LLMs' awareness of knowledge boundaries, thereby improving their judgemental abilities. Additionally, we also find that LLMs have a propensity to rely on the provided retrieval results when formulating answers, while the quality of these results significantly impacts their reliance. The code to reproduce this work is available at https://github.com/RUCAIBox/LLM-Knowledge-Boundary.",
            "year": 2023,
            "citationCount": 48,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study presents an initial analysis of the factual knowledge boundaries of LLMs and how retrieval augmentation affects LLMs on open-domain QA and finds that LLMs have a propensity to rely on the provided retrieval results when formulating answers, while the quality of these results significantly impacts their reliance."
            },
            "score": 6
        },
        {
            "id": "f727f928e7e179307d8d4a1da2387393f2bd7915",
            "paperId": "f727f928e7e179307d8d4a1da2387393f2bd7915",
            "title": "Methods for Measuring, Updating, and Visualizing Factual Beliefs in Language Models",
            "abstract": "Language models can memorize a considerable amount of factual information during pretraining that can be elicited through prompting or finetuning models on tasks like question answering. In this paper, we discuss approaches to measuring model factual beliefs, updating incorrect factual beliefs in models, and visualizing graphical relationships between factual beliefs. Our main contributions include: (1) new metrics for evaluating belief-updating methods focusing on the logical consistency of beliefs, (2) a training objective for Sequential, Local, and Generalizing updates (SLAG) that improves the performance of existing hypernetwork approaches, and (3) the introduction of the belief graph, a new form of visualization for language models that shows relationships between stored model beliefs. Our experiments suggest that models show only limited consistency between factual beliefs, but update methods can both fix incorrect model beliefs and greatly improve their consistency. Although off-the-shelf optimizers are surprisingly strong belief-updating baselines, our learned optimizers can outperform them in more difficult settings than have been considered in past work.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experiments suggest that models show only limited consistency between factual beliefs, but update methods can both fix incorrect model beliefs and greatly improve their consistency, and off-the-shelf optimizers can outperform them in more difficult settings than have been considered in past work."
            },
            "score": 6
        },
        {
            "id": "7471cb40a33e9d971a922b5dff5ca9b4a73ca609",
            "paperId": "7471cb40a33e9d971a922b5dff5ca9b4a73ca609",
            "title": "Calibrating Factual Knowledge in Pretrained Language Models",
            "abstract": "Previous literature has proved that Pretrained Language Models (PLMs) can store factual knowledge. However, we find that facts stored in the PLMs are not always correct. It motivates us to explore a fundamental question: How do we calibrate factual knowledge in PLMs without re-training from scratch? In this work, we propose a simple and lightweight method CaliNet to achieve this goal. To be specific, we first detect whether PLMs can learn the right facts via a contrastive score between right and fake facts. If not, we then use a lightweight method to add and adapt new parameters to specific factual texts. Experiments on the knowledge probing task show the calibration effectiveness and efficiency. In addition, through closed-book question answering, we find that the calibrated PLM possesses knowledge generalization ability after fine-tuning. Beyond the calibration performance, we further investigate and visualize the knowledge calibration mechanism.",
            "year": 2022,
            "citationCount": 42,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a simple and lightweight method CaliNet to calibrate factual knowledge in PLMs without re-training from scratch, and finds that the calibrated PLM possesses knowledge generalization ability after fine-tuning."
            },
            "score": 6
        },
        {
            "id": "3aee33831e0bdea1a1eaae21c7586e4f7c0396d6",
            "paperId": "3aee33831e0bdea1a1eaae21c7586e4f7c0396d6",
            "title": "Making Retrieval-Augmented Language Models Robust to Irrelevant Context",
            "abstract": "Retrieval-augmented language models (RALMs) hold promise to produce language understanding systems that are are factual, efficient, and up-to-date. An important desideratum of RALMs, is that retrieved information helps model performance when it is relevant, and does not harm performance when it is not. This is particularly important in multi-hop reasoning scenarios, where misuse of irrelevant evidence can lead to cascading errors. However, recent work has shown that retrieval augmentation can sometimes have a negative effect on performance. In this work, we present a thorough analysis on five open-domain question answering benchmarks, characterizing cases when retrieval reduces accuracy. We then propose two methods to mitigate this issue. First, a simple baseline that filters out retrieved passages that do not entail question-answer pairs according to a natural language inference (NLI) model. This is effective in preventing performance reduction, but at a cost of also discarding relevant passages. Thus, we propose a method for automatically generating data to fine-tune the language model to properly leverage retrieved passages, using a mix of relevant and irrelevant contexts at training time. We empirically show that even 1,000 examples suffice to train the model to be robust to irrelevant contexts while maintaining high performance on examples with relevant ones.",
            "year": 2023,
            "citationCount": 40,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work empirically shows that even 1,000 examples suffice to train the model to be robust to irrelevant contexts while maintaining high performance on examples with relevant ones, and proposes a method for automatically generating data to fine-tune the language model to properly leverage retrieved passages."
            },
            "score": 6
        },
        {
            "id": "f41977c497c96c1da2e9e945315e9be6d6ad472e",
            "paperId": "f41977c497c96c1da2e9e945315e9be6d6ad472e",
            "title": "Towards reducing hallucination in extracting information from financial reports using Large Language Models",
            "abstract": "For a financial analyst, the question and answer (Q\\&A) segment of the company financial report is a crucial piece of information for various analysis and investment decisions. However, extracting valuable insights from the Q\\&A section has posed considerable challenges as the conventional methods such as detailed reading and note-taking lack scalability and are susceptible to human errors, and Optical Character Recognition (OCR) and similar techniques encounter difficulties in accurately processing unstructured transcript text, often missing subtle linguistic nuances that drive investor decisions. Here, we demonstrate the utilization of Large Language Models (LLMs) to efficiently and rapidly extract information from earnings report transcripts while ensuring high accuracy transforming the extraction process as well as reducing hallucination by combining retrieval-augmented generation technique as well as metadata. We evaluate the outcomes of various LLMs with and without using our proposed approach based on various objective metrics for evaluating Q\\&A systems, and empirically demonstrate superiority of our method.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work demonstrates the utilization of Large Language Models (LLMs) to efficiently and rapidly extract information from earnings report transcripts while ensuring high accuracy transforming the extraction process as well as reducing hallucination by combining retrieval-augmented generation technique aswell as metadata."
            },
            "score": 6
        },
        {
            "id": "798963674902f741c3ea9298403eb8384c099a42",
            "paperId": "798963674902f741c3ea9298403eb8384c099a42",
            "title": "Factored Verification: Detecting and Reducing Hallucination in Summaries of Academic Papers",
            "abstract": "Hallucination plagues even frontier LLMs--but how bad is it really for summarizing academic papers? We evaluate Factored Verification, a simple automated method for detecting hallucinations in abstractive summaries. This method sets a new SotA on hallucination detection in the summarization task of the HaluEval benchmark, achieving 76.2% accuracy. We then use this method to estimate how often language models hallucinate when summarizing across multiple academic papers and find 0.62 hallucinations in the average ChatGPT (16k) summary, 0.84 for GPT-4, and 1.55 for Claude 2. We ask models to self-correct using Factored Critiques and find that this lowers the number of hallucinations to 0.49 for ChatGPT, 0.46 for GPT-4, and 0.95 for Claude 2. The hallucinations we find are often subtle, so we advise caution when using models to synthesize academic papers.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Factored Verification, a simple automated method for detecting hallucinations in abstractive summaries, is evaluated and sets a new SotA on hallucination detection in the summarization task of the HaluEval benchmark, achieving 76.2% accuracy."
            },
            "score": 6
        },
        {
            "id": "6489640b1d30a8a3e7cb906bb6557f1ccd0d799d",
            "paperId": "6489640b1d30a8a3e7cb906bb6557f1ccd0d799d",
            "title": "Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models",
            "abstract": "Retrieval-augmented language models (RALMs) represent a substantial advancement in the capabilities of large language models, notably in reducing factual hallucination by leveraging external knowledge sources. However, the reliability of the retrieved information is not always guaranteed. The retrieval of irrelevant data can lead to misguided responses, and potentially causing the model to overlook its inherent knowledge, even when it possesses adequate information to address the query. Moreover, standard RALMs often struggle to assess whether they possess adequate knowledge, both intrinsic and retrieved, to provide an accurate answer. In situations where knowledge is lacking, these systems should ideally respond with\"unknown\"when the answer is unattainable. In response to these challenges, we introduces Chain-of-Noting (CoN), a novel approach aimed at improving the robustness of RALMs in facing noisy, irrelevant documents and in handling unknown scenarios. The core idea of CoN is to generate sequential reading notes for retrieved documents, enabling a thorough evaluation of their relevance to the given question and integrating this information to formulate the final answer. We employed ChatGPT to create training data for CoN, which was subsequently trained on an LLaMa-2 7B model. Our experiments across four open-domain QA benchmarks show that RALMs equipped with CoN significantly outperform standard RALMs. Notably, CoN achieves an average improvement of +7.9 in EM score given entirely noisy retrieved documents and +10.5 in rejection rates for real-time questions that fall outside the pre-training knowledge scope.",
            "year": 2023,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Chain-of-Noting (CoN) is introduced, a novel approach aimed at improving the robustness of RALMs in facing noisy, irrelevant documents and in handling unknown scenarios, and achieves an average improvement of +7.9 in EM score given entirely noisy retrieved documents and +10.5 in rejection rates for real-time questions that fall outside the pre-training knowledge scope."
            },
            "score": 6
        },
        {
            "id": "889feabe31ba0d24c093ac94d54a06eecb87e3f4",
            "paperId": "889feabe31ba0d24c093ac94d54a06eecb87e3f4",
            "title": "Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding",
            "abstract": "Dialogue systems powered by large pre-trained language models exhibit an innate ability to deliver fluent and natural-sounding responses. Despite their impressive performance, these models are fitful and can often generate factually incorrect statements impeding their widespread adoption. In this paper, we focus on the task of improving faithfulness and reducing hallucination of neural dialogue systems to known facts supplied by a Knowledge Graph (KG). We propose Neural Path Hunter which follows a generate-then-refine strategy whereby a generated response is amended using the KG. Neural Path Hunter leverages a separate token-level fact critic to identify plausible sources of hallucination followed by a refinement stage that retrieves correct entities by crafting a query signal that is propagated over a k-hop subgraph. We empirically validate our proposed approach on the OpenDialKG dataset (Moon et al., 2019) against a suite of metrics and report a relative improvement of faithfulness over dialogue responses by 20.35% based on FeQA (Durmus et al., 2020). The code is available at https://github.com/nouhadziri/Neural-Path-Hunter.",
            "year": 2021,
            "citationCount": 80,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes Neural Path Hunter which follows a generate-then-refine strategy whereby a generated response is amended using the KG, and leverages a separate token-level fact critic to identify plausible sources of hallucination and retrieves correct entities by crafting a query signal that is propagated over a k-hop subgraph."
            },
            "score": 6
        },
        {
            "id": "79429814fd4d967b9277af2805c53f370e52ebb5",
            "paperId": "79429814fd4d967b9277af2805c53f370e52ebb5",
            "title": "Chain of Natural Language Inference for Reducing Large Language Model Ungrounded Hallucinations",
            "abstract": "Large language models (LLMs) can generate fluent natural language texts when given relevant documents as background context. This ability has attracted considerable interest in developing industry applications of LLMs. However, LLMs are prone to generate hallucinations that are not supported by the provided sources. In this paper, we propose a hierarchical framework to detect and mitigate such ungrounded hallucination. Our framework uses Chain of Natural Language Inference (CoNLI) for hallucination detection and hallucination reduction via post-editing. Our approach achieves state-of-the-art performance on hallucination detection and enhances text quality through rewrite, using LLMs without any fine-tuning or domain-specific prompt engineering. We show that this simple plug-and-play framework can serve as an effective choice for hallucination detection and reduction, achieving competitive performance across various contexts.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a hierarchical framework to detect and mitigate ungrounded hallucination, using Chain of Natural Language Inference (CoNLI) for hallucination detection and hallucination reduction via post-editing and shows that this simple plug-and-play framework can serve as an effective choice for hallucinations detection and reduction, achieving competitive performance across various contexts."
            },
            "score": 6
        },
        {
            "id": "fcee1c19e12f3b7e3595aeba702416d055bdbc3f",
            "paperId": "fcee1c19e12f3b7e3595aeba702416d055bdbc3f",
            "title": "Knowledge Verification to Nip Hallucination in the Bud",
            "abstract": "While large language models (LLMs) have demonstrated exceptional performance across various tasks following human alignment, they may still generate responses that sound plausible but contradict factual knowledge, a phenomenon known as \\emph{hallucination}. In this paper, we demonstrate the feasibility of mitigating hallucinations by verifying and minimizing the inconsistency between external knowledge present in the alignment data and the intrinsic knowledge embedded within foundation LLMs. Specifically, we propose a novel approach called Knowledge Consistent Alignment (KCA), which employs a well-aligned LLM to automatically formulate assessments based on external knowledge to evaluate the knowledge boundaries of foundation LLMs. To address knowledge inconsistencies in the alignment data, KCA implements several specific strategies to deal with these data instances. We demonstrate the superior efficacy of KCA in reducing hallucinations across six benchmarks, utilizing foundation LLMs of varying backbones and scales. This confirms the effectiveness of mitigating hallucinations by reducing knowledge inconsistency. Our code, model weights, and data are openly accessible at \\url{https://github.com/fanqiwan/KCA}.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The superior efficacy of KCA is demonstrated in reducing hallucinations across six benchmarks, utilizing foundation LLMs of varying backbones and scales, which confirms the effectiveness of mitigating hallucinations by reducing knowledge inconsistency."
            },
            "score": 6
        },
        {
            "id": "959de2864ac2268c45bc1762cfb9df45ae630a1b",
            "paperId": "959de2864ac2268c45bc1762cfb9df45ae630a1b",
            "title": "Knowledge Discovery in the Age of LLMs",
            "abstract": "Large Language Models (LLMs) like GPT-3 are quickly changing the way NLP is done, and hence also how NLP is done for the purpose of knowledge discovery in the academic literature. Tasks that have traditionally been done by specialized NLP models, like entity extraction, summarization, and question answering, can now all be prototyped, usually with high accuracy, using zero-shot or few-shot prompting of LLMs. For example, this allows us to extract highly accurate meta-data, such as up-to-date author and affiliation, for scientific impact analysis from recent scientific papers 1 . The combination of LLMs and Information Retrieval systems has recently evolved into the paradigm of Retrieval Augmented Generation, which is one of the most promising approaches to use LLMs for Question Answering and to reduce hallucination, especially for data sets that were not accessible to LLMs during training, such as private data sources or very recent documents. Retrieval Augmented Generation can even be expanded to large document collections, like full conference proceedings, to quickly create conference summaries, blog-posts or tabular digests. At Zeta Alpha we are building a modern platform for scientific and enterprise knowledge discovery with neural search and generative language models at the core. In this talk, we show how we integrate LLMs with neural search to answer questions, explain documents while reading, summarize large document collections, and generate meta-data on the fly.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This talk shows how LLMs with neural search are integrated with neural search to answer questions, explain documents while reading, summarize large document collections, and generate meta-data on the fly."
            },
            "score": 6
        },
        {
            "id": "58ed1fbaabe027345f7bb3a6312d41c5aac63e22",
            "paperId": "58ed1fbaabe027345f7bb3a6312d41c5aac63e22",
            "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
            "abstract": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
            "year": 2020,
            "citationCount": 1842,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation, and finds that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline."
            },
            "score": 6
        },
        {
            "id": "9a4e4ab77c3d836bab35e0578de68e8ce79af1e8",
            "paperId": "9a4e4ab77c3d836bab35e0578de68e8ce79af1e8",
            "title": "Graph Neural Prompting with Large Language Models",
            "abstract": "Large language models (LLMs) have shown remarkable generalization capability with exceptional performance in various language modeling tasks. However, they still exhibit inherent limitations in precisely capturing and returning grounded knowledge. While existing work has explored utilizing knowledge graphs (KGs) to enhance language modeling via joint training and customized model architectures, applying this to LLMs is problematic owing to their large number of parameters and high computational cost. Therefore, how to enhance pre-trained LLMs using grounded knowledge, e.g., retrieval-augmented generation, remains an open question. In this work, we propose Graph Neural Prompting (GNP), a novel plug-and-play method to assist pre-trained LLMs in learning beneficial knowledge from KGs. GNP encompasses various designs, including a standard graph neural network encoder, a cross-modality pooling module, a domain projector, and a self-supervised link prediction objective. Extensive experiments on multiple datasets demonstrate the superiority of GNP on both commonsense and biomedical reasoning tasks across different LLM sizes and settings. Code is available at https://github.com/meettyj/GNP.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Graph Neural Prompting (GNP) is proposed, a novel plug-and-play method to assist pre-trained LLMs in learning beneficial knowledge from KGs that demonstrates the superiority of GNP on both commonsense and biomedical reasoning tasks across different LLM sizes and settings."
            },
            "score": 5
        },
        {
            "id": "6806ecad90a778aaa7f6a3cd3a539582d823066c",
            "paperId": "6806ecad90a778aaa7f6a3cd3a539582d823066c",
            "title": "DISC-LawLLM: Fine-tuning Large Language Models for Intelligent Legal Services",
            "abstract": "We propose DISC-LawLLM, an intelligent legal system utilizing large language models (LLMs) to provide a wide range of legal services. We adopt legal syllogism prompting strategies to construct supervised fine-tuning datasets in the Chinese Judicial domain and fine-tune LLMs with legal reasoning capability. We augment LLMs with a retrieval module to enhance models' ability to access and utilize external legal knowledge. A comprehensive legal benchmark, DISC-Law-Eval, is presented to evaluate intelligent legal systems from both objective and subjective dimensions. Quantitative and qualitative results on DISC-Law-Eval demonstrate the effectiveness of our system in serving various users across diverse legal scenarios. The detailed resources are available at https://github.com/FudanDISC/DISC-LawLLM.",
            "year": 2023,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Quantitative and qualitative results on DISC-Law-Eval demonstrate the effectiveness of the system in serving various users across diverse legal scenarios, and enhances models' ability to access and utilize external legal knowledge."
            },
            "score": 5
        },
        {
            "id": "a79330d1069a86b7f8bf6f9fe0ef01d7b1379a91",
            "paperId": "a79330d1069a86b7f8bf6f9fe0ef01d7b1379a91",
            "title": "Chain of Thought Prompting Elicits Knowledge Augmentation",
            "abstract": "The knowledge-augmented deep learning paradigm refers to a paradigm in which domain knowledge is identified and integrated into deep models. Conventional methods typically employ task-specific approaches to gather external knowledge from various sources. In contrast, large language models are extensively pre-trained and can serve as a comprehensive source of external knowledge. In this paper, we propose CoT-KA, a Chain-of-Thought-based method that augments knowledge for deep learning. CoT-KA avoids the need for additional knowledge retrieval or knowledge reasoning models, as required in conventional augmentation methods. Our results demonstrate that CoT-KA outperforms both pure CoT-based methods and the non-augmented method across the majority of eleven publicly available benchmarks for various reasoning tasks.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "CoT-KA is proposed, a Chain-of-Thought-based method that augments knowledge for deep learning that avoids the need for additional knowledge retrieval or knowledge reasoning models, as required in conventional augmentation methods."
            },
            "score": 5
        },
        {
            "id": "59395cf4f9346ef4ccb37499a3a7e52c2978fc61",
            "paperId": "59395cf4f9346ef4ccb37499a3a7e52c2978fc61",
            "title": "Right for Right Reasons: Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering",
            "abstract": "Knowledge Graph Question Answering (KGQA) methods seek to answer Natural Language questions using the relational information stored in Knowledge Graphs (KGs). With the recent advancements of Large Language Models (LLMs) and their remarkable reasoning abilities, there is a growing trend to leverage them for KGQA. However, existing methodologies have only focused on answering factual questions, e.g.,\"In which city was Silvio Berlusconi's first wife born?\", leaving questions involving commonsense reasoning that real-world users may pose more often, e.g.,\"Do I need separate visas to see the Venus of Willendorf and attend the Olympics this summer?\"unaddressed. In this work, we first observe that existing LLM-based methods for KGQA struggle with hallucination on such questions, especially on queries targeting long-tail entities (e.g., non-mainstream and recent entities), thus hindering their applicability in real-world applications especially since their reasoning processes are not easily verifiable. In response, we propose Right for Right Reasons (R3), a commonsense KGQA methodology that allows for a verifiable reasoning procedure by axiomatically surfacing intrinsic commonsense knowledge of LLMs and grounding every factual reasoning step on KG triples. Through experimental evaluations across three different tasks--question answering, claim verification, and preference matching--our findings showcase R3 as a superior approach, outperforming existing methodologies and notably reducing instances of hallucination and reasoning errors.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes Right for Right Reasons (R3), a commonsense KGQA methodology that allows for a verifiable reasoning procedure by axiomatically surfacing intrinsic commonsense knowledge of LLMs and grounding every factual reasoning step on KG triples."
            },
            "score": 5
        },
        {
            "id": "c54675e2fbec984c18f7e61de83bca919aa811d6",
            "paperId": "c54675e2fbec984c18f7e61de83bca919aa811d6",
            "title": "Enhancing Large Language Models with Pseudo- and Multisource- Knowledge Graphs for Open-ended Question Answering",
            "abstract": "Mitigating the hallucinations of Large Language Models (LLMs) and enhancing them is a crucial task. Although some existing methods employ model self-enhancement techniques, they fall short of effectively addressing unknown factual hallucinations. Using Knowledge Graph (KG) enhancement approaches fails to address the generalization across different KG sources and the enhancement of open-ended answer questions simultaneously. To tackle these limitations, there is a framework that combines Pseudo-Graph Generation and Atomic Knowledge Verification proposed. The enhancement of LLM using KG in an open-ended question-answering setting is implemented by leveraging the Pseudo-Graph Generation. Atomic Knowledge Verification utilizes atomic-level knowledge querying and verification to achieve generalizability under different KG sources. Compared to the baseline, this approach yields a minimum improvement of 11.5 in the ROUGE-L score for open-ended questions. For precise questions, we observe a minimum accuracy improvement of 7.5. Moreover, there is also demonstration that this framework exhibits generalizability across different KG sources. In summary, our results pave the way for enhancing LLMs by incorporating Pseudo- and Multisource-KGs, particularly in the context of open-ended questions.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results pave the way for enhancing LLMs by incorporating Pseudo- and Multisource-KGs, particularly in the context of open-ended questions, particularly in the context of open-ended questions."
            },
            "score": 5
        },
        {
            "id": "b42eb9e2981417fe617022dffe07e099a7cc02e1",
            "paperId": "b42eb9e2981417fe617022dffe07e099a7cc02e1",
            "title": "Enhancing Information Retrieval in the Drilling Domain: Zero-Shot Learning with Large Language Models for Question-Answering",
            "abstract": "\n Finding information across multiple databases, formats, and documents remains a manual job in the drilling industry. Large Language Models (LLMs) have proven effective in data-aggregation tasks, including answering questions. However, using LLMs for domain-specific factual responses poses a nontrivial challenge. The expert labor cost for training domain-specific LLMs prohibits niche industries from developing custom question-answering bots. This paper tests several commercial LLMs for information retrieval tasks for drilling data using zero-shot in-context learning. In addition, we studied the model\u2019s calibration using a few-shot multiple-choice drilling questionnaire.\n To create an LLM benchmark for drilling, we collated the text data from publicly available databases: the Norwegian Petroleum Directorate (NPD), company annual reports, and petroleum glossary. We used a zero-shot learning technique that relies on an LLM\u2019s ability to generate responses for tasks outside its training. We implemented a controlled zero-shot learning \"in-context\" procedure that sends a user\u2019s query augmented with text data to the LLM as inputs. This implementation encourages the LLM to take the answer from the data while leveraging its pre-trained contextual-learning capability.\n We evaluated several state-of-the-art generic LLMs available through an API, including G4, G3.5-TI, J2-ultra model, and L2 series. The paper documents the pre-trained LLMs\u2019 ability to provide correct answers and identify petroleum industry jargon from the collated dataset. Our zero-shot in-context learning implementation helps vanilla LLMs provide relevant factual responses for the drilling domain. While each LLM\u2019s performance varies, we have identified models suitable for a drilling chatbot application. In particular, G4 outperformed on all the tasks. This finding suggests that training expensive domain-specific LLMs is not necessary for question-answering tasks in the context of drilling data.\n We demonstrate the utility of zero-shot in-context learning using pre-trained LLMs for question-answering tasks relevant to the drilling industry. Additionally, we prepared and publicly released the collated datasets from the NPD database and companies\u2019 annual reports to enable results reproducibility and to foster acceleration of language model adoption and development for the subsurface and drilling industries. The petroleum industry may find our solution beneficial for enhancing personnel training and career development. It also offers a method for conducting data analytics and overcoming challenges in retrieving historical well data.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The utility of zero-shot in-context learning using pre-trained LLMs for question-answering tasks relevant to the drilling industry is demonstrated and G4 outperformed on all the tasks, suggesting that training expensive domain-specific LLMs is not necessary for question-answering tasks in the context of drilling data."
            },
            "score": 5
        },
        {
            "id": "e070ff286709db28312e08b52b05539debe88146",
            "paperId": "e070ff286709db28312e08b52b05539debe88146",
            "title": "Measuring and Narrowing the Compositionality Gap in Language Models",
            "abstract": "We investigate the ability of language models to perform compositional reasoning tasks where the overall solution depends on correctly composing the answers to sub-problems. We measure how often models can correctly answer all sub-problems but not generate the overall solution, a ratio we call the compositionality gap. We evaluate this ratio by asking multi-hop questions with answers that require composing multiple facts unlikely to have been observed together during pretraining. In the GPT-3 family of models, as model size increases we show that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning. We then demonstrate how elicitive prompting (such as chain of thought) narrows the compositionality gap by reasoning explicitly. We present a new method, self-ask, that further improves on chain of thought. In our method, the model explicitly asks itself (and answers) follow-up questions before answering the initial question. We finally show that self-ask's structured prompting lets us easily plug in a search engine to answer the follow-up questions, which additionally improves accuracy.",
            "year": 2022,
            "citationCount": 296,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease, and while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform compositional reasoning."
            },
            "score": 5
        },
        {
            "id": "6028780bfb3728292d37c07951e3f463fae0981e",
            "paperId": "6028780bfb3728292d37c07951e3f463fae0981e",
            "title": "Unsupervised Improvement of Factual Knowledge in Language Models",
            "abstract": "Masked language modeling (MLM) plays a key role in pretraining large language models. But the MLM objective is often dominated by high-frequency words that are sub-optimal for learning factual knowledge. In this work, we propose an approach for influencing MLM pretraining in a way that can improve language model performance on a variety of knowledge-intensive tasks. We force the language model to prioritize informative words in a fully unsupervised way. Experiments demonstrate that the proposed approach can significantly improve the performance of pretrained language models on tasks such as factual recall, question answering, sentiment analysis, and natural language inference in a closed-book setting.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes an approach for influencing MLM pretraining in a way that can improve language model performance on a variety of knowledge-intensive tasks and forces the language model to prioritize informative words in a fully unsupervised way."
            },
            "score": 5
        },
        {
            "id": "daf53a5b532fe32dea9d77b0375421ad0ee72ae5",
            "paperId": "daf53a5b532fe32dea9d77b0375421ad0ee72ae5",
            "title": "Systematic Assessment of Factual Knowledge in Large Language Models",
            "abstract": "Previous studies have relied on existing question-answering benchmarks to evaluate the knowledge stored in large language models (LLMs). However, this approach has limitations regarding factual knowledge coverage, as it mostly focuses on generic domains which may overlap with the pretraining data. This paper proposes a framework to systematically assess the factual knowledge of LLMs by leveraging knowledge graphs (KGs). Our framework automatically generates a set of questions and expected answers from the facts stored in a given KG, and then evaluates the accuracy of LLMs in answering these questions. We systematically evaluate the state-of-the-art LLMs with KGs in generic and specific domains. The experiment shows that ChatGPT is consistently the top performer across all domains. We also find that LLMs performance depends on the instruction finetuning, domain and question complexity and is prone to adversarial context.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a framework to systematically assess the factual knowledge of LLMs by leveraging knowledge graphs (KGs) and finds that LLMs performance depends on the instruction finetuning, domain and question complexity and is prone to adversarial context."
            },
            "score": 5
        },
        {
            "id": "d0086b86103a620a86bc918746df0aa642e2a8a3",
            "paperId": "d0086b86103a620a86bc918746df0aa642e2a8a3",
            "title": "Language Models as Knowledge Bases?",
            "abstract": "Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as \u201cfill-in-the-blank\u201d cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/LAMA.",
            "year": 2019,
            "citationCount": 1971,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models finds that BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge."
            },
            "score": 5
        },
        {
            "id": "6af460d34bfc8e955e43fbe15cedcf329b48bc19",
            "paperId": "6af460d34bfc8e955e43fbe15cedcf329b48bc19",
            "title": "SAC3: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency",
            "abstract": "Hallucination detection is a critical step toward understanding the trustworthiness of modern language models (LMs). To achieve this goal, we re-examine existing detection approaches based on the self-consistency of LMs and uncover two types of hallucinations resulting from 1) question-level and 2) model-level, which cannot be effectively identified through self-consistency check alone. Building upon this discovery, we propose a novel sampling-based method, i.e., semantic-aware cross-check consistency (SAC3) that expands on the principle of self-consistency checking. Our SAC3 approach incorporates additional mechanisms to detect both question-level and model-level hallucinations by leveraging advances including semantically equivalent question perturbation and cross-model response consistency checking. Through extensive and systematic empirical analysis, we demonstrate that SAC3 outperforms the state of the art in detecting both non-factual and factual statements across multiple question-answering and open-domain generation benchmarks.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel sampling-based method, i.e., semantic-aware cross-check consistency (SAC3) that expands on the principle of self-consistency checking and demonstrates that SAC3 outperforms the state of the art in detecting both non-factual and factual statements across multiple question-answering and open-domain generation benchmarks."
            },
            "score": 5
        },
        {
            "id": "515168d8791c393619c7153895312f1c6190dddc",
            "paperId": "515168d8791c393619c7153895312f1c6190dddc",
            "title": "Knowledge Prompt Makes Composed Pre-Trained Models Zero-Shot News Captioner",
            "abstract": "News image captioning aims to generate descriptions containing concrete named entities for news images by leveraging relevant news articles. However, existing approaches suffer from two shortcomings: 1) lack of commonsense knowledge required to understand named entities, and 2) limited multimodal context modeling capabilities. In this paper, we propose to migrate the ability of large-scale pre-trained models for news image captioning. To acquire factual knowledge for describing named entities, we induce a pre-trained language model for commonsense knowledge reasoning using context-aware knowledge prompts. To compose a new multimodal context modeling capability, we coordinate pre-trained models by a unified language representation and constrain joint multimodal context reasoning with cross-modal consistency objective. Experimental results on GoodNews and NYTimes datasets show that our proposed method exhibits considerable captioning capabilities even without training on news data.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "To acquire factual knowledge for describing named entities, a pre-trained language model for commonsense knowledge reasoning is induced using context-aware knowledge prompts and a new multimodal context modeling capability is composed."
            },
            "score": 5
        },
        {
            "id": "4e53b481beabba42aac027e5a8c69fed26ab4062",
            "paperId": "4e53b481beabba42aac027e5a8c69fed26ab4062",
            "title": "RHO ($\u03c1$): Reducing Hallucination in Open-domain Dialogues with Knowledge Grounding",
            "abstract": "Dialogue systems can leverage large pre-trained language models and knowledge to generate fluent and informative responses. However, these models are still prone to produce hallucinated responses not supported by the input source, which greatly hinders their application. The heterogeneity between external knowledge and dialogue context challenges representation learning and source integration, and further contributes to unfaithfulness. To handle this challenge and generate more faithful responses, this paper presents RHO ($\\rho$) utilizing the representations of linked entities and relation predicates from a knowledge graph (KG). We propose (1) local knowledge grounding to combine textual embeddings with the corresponding KG embeddings; and (2) global knowledge grounding to equip RHO with multi-hop reasoning abilities via the attention mechanism. In addition, we devise a response re-ranking technique based on walks over KG sub-graphs for better conversational reasoning. Experimental results on OpenDialKG show that our approach significantly outperforms state-of-the-art methods on both automatic and human evaluation by a large margin, especially in hallucination reduction (17.54% in FeQA).",
            "year": 2022,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "RHO is presented utilizing the representations of linked entities and relation predicates from a knowledge graph (KG) to equip RHO with multi-hop reasoning abilities via the attention mechanism and devise a response re-ranking technique based on walks over KG sub-graphs for better conversational reasoning."
            },
            "score": 5
        },
        {
            "id": "03764434729b83d4f04a8bd02f99f2500cd5bbae",
            "paperId": "03764434729b83d4f04a8bd02f99f2500cd5bbae",
            "title": "Teaching Language Models to Hallucinate Less with Synthetic Tasks",
            "abstract": "Large language models (LLMs) frequently hallucinate on abstractive summarization tasks such as document-based question-answering, meeting summarization, and clinical report generation, even though all necessary information is included in context. However, optimizing LLMs to hallucinate less on these tasks is challenging, as hallucination is hard to efficiently evaluate at each optimization step. In this work, we show that reducing hallucination on a synthetic task can also reduce hallucination on real-world downstream tasks. Our method, SynTra, first designs a synthetic task where hallucinations are easy to elicit and measure. It next optimizes the LLM's system message via prefix-tuning on the synthetic task, and finally transfers the system message to realistic, hard-to-optimize tasks. Across three realistic abstractive summarization tasks, SynTra reduces hallucination for two 13B-parameter LLMs using only a synthetic retrieval task for supervision. We also find that optimizing the system message rather than the model weights can be critical; fine-tuning the entire model on the synthetic task can counterintuitively increase hallucination. Overall, SynTra demonstrates that the extra flexibility of working with synthetic data can help mitigate undesired behaviors in practice.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Across three realistic abstractive summarization tasks, SynTra reduces hallucination for two 13B-parameter LLMs using only a synthetic retrieval task for supervision, demonstrating that the extra flexibility of working with synthetic data can help mitigate undesired behaviors in practice."
            },
            "score": 5
        },
        {
            "id": "696bc5ba0d023822bbee6b878a71ea2e4a4b0e5a",
            "paperId": "696bc5ba0d023822bbee6b878a71ea2e4a4b0e5a",
            "title": "N-Critics: Self-Refinement of Large Language Models with Ensemble of Critics",
            "abstract": "We propose a self-correction mechanism for Large Language Models (LLMs) to mitigate issues such as toxicity and fact hallucination. This method involves refining model outputs through an ensemble of critics and the model's own feedback. Drawing inspiration from human behavior, we explore whether LLMs can emulate the self-correction process observed in humans who often engage in self-reflection and seek input from others to refine their understanding of complex topics. Our approach is model-agnostic and can be applied across various domains to enhance trustworthiness by addressing fairness, bias, and robustness concerns. We consistently observe performance improvements in LLMs for reducing toxicity and correcting factual errors.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a self-correction mechanism for Large Language Models (LLMs) to mitigate issues such as toxicity and fact hallucination and consistently observe performance improvements in LLMs for reducing toxicity and correcting factual errors."
            },
            "score": 5
        },
        {
            "id": "60988a0ebad89af503f17de977785814fb864635",
            "paperId": "60988a0ebad89af503f17de977785814fb864635",
            "title": "Correction with Backtracking Reduces Hallucination in Summarization",
            "abstract": "Abstractive summarization aims at generating natural language summaries of a source document that are succinct while preserving the important elements. Despite recent advances, neural text summarization models are known to be susceptible to hallucinating (or more correctly confabulating), that is to produce summaries with details that are not grounded in the source document. In this paper, we introduce a simple yet efficient technique, CoBa, to reduce hallucination in abstractive summarization. The approach is based on two steps: hallucination detection and mitigation. We show that the former can be achieved through measuring simple statistics about conditional word probabilities and distance to context words. Further, we demonstrate that straight-forward backtracking is surprisingly effective at mitigation. We thoroughly evaluate the proposed method with prior art on three benchmark datasets for text summarization. The results show that CoBa is effective and efficient in reducing hallucination, and offers great adaptability and flexibility.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a simple yet efficient technique, CoBa, to reduce hallucination in abstractive summarization, and shows that the former can be achieved through measuring simple statistics about conditional word probabilities and distance to context words."
            },
            "score": 5
        },
        {
            "id": "afdeef9585232642d18e7c6a7942b2395e94ede1",
            "paperId": "afdeef9585232642d18e7c6a7942b2395e94ede1",
            "title": "A Cause-Effect Look at Alleviating Hallucination of Knowledge-grounded Dialogue Generation",
            "abstract": "Empowered by the large-scale pretrained language models, existing dialogue systems have demonstrated impressive performance conducting fluent and natural-sounding conversations. However, they are still plagued by the hallucination problem, causing unpredictable factual errors in the generated responses. Recently, knowledge-grounded dialogue generation models, that intentionally invoke external knowledge resources to more informative responses, are also proven to be effective in reducing hallucination. Following the idea of getting high-quality knowledge, a few efforts have achieved pretty good performance on this issue. As some inevitable knowledge noises may also lead to hallucinations, it is emergent to investigate the reason and future directions for building noise-tolerant methods in KGD tasks. In this paper, we analyze the causal story behind this problem with counterfactual reasoning methods. Based on the causal effect analysis, we propose a possible solution for alleviating the hallucination in KGD by exploiting the dialogue-knowledge interaction. Experimental results of our example implementation show that this method can reduce hallucination without disrupting other dialogue performance, while keeping adaptive to different generation models. We hope our efforts can support and call for more attention to developing lightweight techniques towards robust and trusty dialogue systems.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Based on the causal effect analysis, a possible solution for alleviating the hallucination in KGD by exploiting the dialogue-knowledge interaction is proposed and results show this method can reduce hallucination without disrupting other dialogue performance, while keeping adaptive to different generation models."
            },
            "score": 5
        },
        {
            "id": "ef8d2d1bcd7a91807fad8d028efde0a71b14c3b7",
            "paperId": "ef8d2d1bcd7a91807fad8d028efde0a71b14c3b7",
            "title": "ProS: Prompting-to-simulate Generalized knowledge for Universal Cross-Domain Retrieval",
            "abstract": "The goal of Universal Cross-Domain Retrieval (UCDR) is to achieve robust performance in generalized test scenarios, wherein data may belong to strictly unknown domains and categories during training. Recently, pre-trained models with prompt tuning have shown strong generalization capabilities and attained noteworthy achievements in various downstream tasks, such as few-shot learning and video-text retrieval. However, applying them directly to UCDR may not sufficiently to handle both domain shift (i.e., adapting to unfamiliar domains) and semantic shift (i.e., transferring to unknown categories). To this end, we propose \\textbf{Pro}mpting-to-\\textbf{S}imulate (ProS), the first method to apply prompt tuning for UCDR. ProS employs a two-step process to simulate Content-aware Dynamic Prompts (CaDP) which can impact models to produce generalized features for UCDR. Concretely, in Prompt Units Learning stage, we introduce two Prompt Units to individually capture domain and semantic knowledge in a mask-and-align way. Then, in Context-aware Simulator Learning stage, we train a Content-aware Prompt Simulator under a simulated test scenarios to produce the corresponding CaDP. Extensive experiments conducted on three benchmark datasets show that our method achieves new state-of-the-art performance without bringing excessive parameters. Our method is publicly available at https://github.com/fangkaipeng/ProS.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ProS is proposed, the first method to apply prompt tuning for UCDR, which employs a two-step process to simulate Content-aware Dynamic Prompts which can impact models to produce generalized features for UCDR."
            },
            "score": 5
        },
        {
            "id": "04407b388432e957031cebd1859e868c96006522",
            "paperId": "04407b388432e957031cebd1859e868c96006522",
            "title": "Artefact Retrieval: Overview of NLP Models with Knowledge Base Access",
            "abstract": "Many NLP models gain performance by having access to a knowledge base. A lot of research has been devoted to devising and improving the way the knowledge base is accessed and incorporated into the model, resulting in a number of mechanisms and pipelines. Despite the diversity of proposed mechanisms, there are patterns in the designs of such systems. In this paper, we systematically describe the typology of artefacts (items retrieved from a knowledge base), retrieval mechanisms and the way these artefacts are fused into the model. This further allows us to uncover combinations of design decisions that had not yet been tried. Most of the focus is given to language models, though we also show how question answering, fact-checking and knowledgable dialogue models fit into this system as well. Having an abstract model which can describe the architecture of specific models also helps with transferring these architectures between multiple NLP tasks.",
            "year": 2022,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper systematically describes the typology of artefacts, retrieval mechanisms and the way these artefacts are fused into the model to uncover combinations of design decisions that had not yet been tried in NLP systems."
            },
            "score": 5
        },
        {
            "id": "61bbdbf481a6d3519c22513ebe8d6c3cd381851e",
            "paperId": "61bbdbf481a6d3519c22513ebe8d6c3cd381851e",
            "title": "Language Models as Knowledge Bases for Visual Word Sense Disambiguation",
            "abstract": "Visual Word Sense Disambiguation (VWSD) is a novel challenging task that lies between linguistic sense disambiguation and fine-grained multimodal retrieval. The recent advancements in the development of visiolinguistic (VL) transformers suggest some off-the-self implementations with encouraging results, which however we argue that can be further improved. To this end, we propose some knowledge-enhancement techniques towards improving the retrieval performance of VL transformers via the usage of Large Language Models (LLMs) as Knowledge Bases. More specifically, knowledge stored in LLMs is retrieved with the help of appropriate prompts in a zero-shot manner, achieving performance advancements. Moreover, we convert VWSD to a purely textual question-answering (QA) problem by considering generated image captions as multiple-choice candidate answers. Zero-shot and few-shot prompting strategies are leveraged to explore the potential of such a transformation, while Chain-of-Thought (CoT) prompting in the zero-shot setting is able to reveal the internal reasoning steps an LLM follows to select the appropriate candidate. In total, our presented approach is the first one to analyze the merits of exploiting knowledge stored in LLMs in different ways to solve WVSD.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The presented approach is the first to analyze the merits of exploiting knowledge stored in LLMs in different ways to solve WVSD, and converts VWSD to a purely textual question-answering (QA) problem by considering generated image captions as multiple-choice candidate answers."
            },
            "score": 4
        },
        {
            "id": "e814810e32183529ba7b99feb364ecfd187a67d2",
            "paperId": "e814810e32183529ba7b99feb364ecfd187a67d2",
            "title": "Astronomical Knowledge Entity Extraction in Astrophysics Journal Articles via Large Language Models",
            "abstract": "Astronomical knowledge entities, such as celestial object identifiers, are crucial for literature retrieval and knowledge graph construction, and other research and applications in the field of astronomy. Traditional methods of extracting knowledge entities from texts face challenges like high manual effort, poor generalization, and costly maintenance. Consequently, there is a pressing need for improved methods to efficiently extract them. This study explores the potential of pre-trained Large Language Models (LLMs) to perform astronomical knowledge entity extraction (KEE) task from astrophysical journal articles using prompts. We propose a prompting strategy called Prompt-KEE, which includes five prompt elements, and design eight combination prompts based on them. Celestial object identifier and telescope name, two most typical astronomical knowledge entities, are selected to be experimental object. And we introduce four currently representative LLMs, namely Llama-2-70B, GPT-3.5, GPT-4, and Claude 2. To accommodate their token limitations, we construct two datasets: the full texts and paragraph collections of 30 articles. Leveraging the eight prompts, we test on full texts with GPT-4 and Claude 2, on paragraph collections with all LLMs. The experimental results demonstrated that pre-trained LLMs have the significant potential to perform KEE tasks in astrophysics journal articles, but there are differences in their performance. Furthermore, we analyze some important factors that influence the performance of LLMs in entity extraction and provide insights for future KEE tasks in astrophysical articles using LLMs.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study explores the potential of pre-trained Large Language Models (LLMs) to perform astronomical knowledge entity extraction (KEE) task from astrophysical journal articles using prompts, and proposes a prompting strategy called Prompt-KEE, which includes five prompt elements and design eight combination prompts based on them."
            },
            "score": 4
        },
        {
            "id": "a49e936075970e8ee574dfe73b9679b34354245e",
            "paperId": "a49e936075970e8ee574dfe73b9679b34354245e",
            "title": "TextGraphs-16 Natural Language Premise Selection Task: Zero-Shot Premise Selection with Prompting Generative Language Models",
            "abstract": "Automated theorem proving can benefit a lot from methods employed in natural language processing, knowledge graphs and information retrieval: this non-trivial task combines formal languages understanding, reasoning, similarity search. We tackle this task by enhancing semantic similarity ranking with prompt engineering, which has become a new paradigm in natural language understanding. None of our approaches requires additional training. Despite encouraging results reported by prompt engineering approaches for a range of NLP tasks, for the premise selection task vanilla re-ranking by prompting GPT-3 doesn\u2019t outperform semantic similarity ranking with SBERT, but merging of the both rankings shows better results.",
            "year": 2022,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work focuses on enhancing semantic similarity ranking with prompt engineering, which has become a new paradigm in natural language understanding, and introduces a new approach to premise selection task."
            },
            "score": 4
        },
        {
            "id": "36b8948a6d15a3c7622ea1567a054845a66a91cb",
            "paperId": "36b8948a6d15a3c7622ea1567a054845a66a91cb",
            "title": "Evaluating Large Language Models in Semantic Parsing for Conversational Question Answering over Knowledge Graphs",
            "abstract": "Conversational question answering systems often rely on semantic parsing to enable interactive information retrieval, which involves the generation of structured database queries from a natural language input. For information-seeking conversations about facts stored within a knowledge graph, dialogue utterances are transformed into graph queries in a process that is called knowledge-based conversational question answering. This paper evaluates the performance of large language models that have not been explicitly pre-trained on this task. Through a series of experiments on an extensive benchmark dataset, we compare models of varying sizes with different prompting techniques and identify common issue types in the generated output. Our results demonstrate that large language models are capable of generating graph queries from dialogues, with significant improvements achievable through few-shot prompting and fine-tuning techniques, especially for smaller models that exhibit lower zero-shot performance.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that large language models are capable of generating graph queries from dialogues, with significant improvements achievable through few-shot prompting and fine-tuning techniques, especially for smaller models that exhibit lower zero-shot performance."
            },
            "score": 4
        },
        {
            "id": "3bbd9a5a0fccf2e18041db9118fff7807501876c",
            "paperId": "3bbd9a5a0fccf2e18041db9118fff7807501876c",
            "title": "Do Language Models Learn about Legal Entity Types during Pretraining?",
            "abstract": "Language Models (LMs) have proven their ability to acquire diverse linguistic knowledge during the pretraining phase, potentially serving as a valuable source of incidental supervision for downstream tasks. However, there has been limited research conducted on the retrieval of domain-specific knowledge, and specifically legal knowledge. We propose to explore the task of Entity Typing, serving as a proxy for evaluating legal knowledge as an essential aspect of text comprehension, and a foundational task to numerous downstream legal NLP applications. Through systematic evaluation and analysis and two types of prompting (cloze sentences and QA-based templates) and to clarify the nature of these acquired cues, we compare diverse types and lengths of entities both general and domain-specific entities, semantics or syntax signals, and different LM pretraining corpus (generic and legal-oriented) and architectures (encoder BERT-based and decoder-only with Llama2). We show that (1) Llama2 performs well on certain entities and exhibits potential for substantial improvement with optimized prompt templates, (2) law-oriented LMs show inconsistent performance, possibly due to variations in their training corpus, (3) LMs demonstrate the ability to type entities even in the case of multi-token entities, (4) all models struggle with entities belonging to sub-domains of the law (5) Llama2 appears to frequently overlook syntactic cues, a shortcoming less present in BERT-based architectures.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The task of Entity Typing is proposed to explore, serving as a proxy for evaluating legal knowledge as an essential aspect of text comprehension, and a foundational task to numerous downstream legal NLP applications."
            },
            "score": 4
        },
        {
            "id": "a88018cbbaa547d14c19ec7c5abdea70c1e61211",
            "paperId": "a88018cbbaa547d14c19ec7c5abdea70c1e61211",
            "title": "HiPrompt: Few-Shot Biomedical Knowledge Fusion via Hierarchy-Oriented Prompting",
            "abstract": "Medical decision-making processes can be enhanced by comprehensive biomedical knowledge bases, which require fusing knowledge graphs constructed from different sources via a uniform index system. The index system often organizes biomedical terms in a hierarchy to provide the aligned entities with fine-grained granularity. To address the challenge of scarce supervision in the biomedical knowledge fusion (BKF) task, researchers have proposed various unsupervised methods. However, these methods heavily rely on ad-hoc lexical and structural matching algorithms, which fail to capture the rich semantics conveyed by biomedical entities and terms. Recently, neural embedding models have proved effective in semantic-rich tasks, but they rely on sufficient labeled data to be adequately trained. To bridge the gap between the scarce-labeled BKF and neural embedding models, we propose HiPrompt, a supervision-efficient knowledge fusion framework that elicits the few-shot reasoning ability of large language models through hierarchy-oriented prompts. Empirical results on the collected KG-Hi-BKF benchmark datasets demonstrate the effectiveness of HiPrompt.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "HiPrompt is proposed, a supervision-efficient knowledge fusion framework that elicits the few-shot reasoning ability of large language models through hierarchy-oriented prompts and demonstrates the effectiveness of HiPrompt on the collected KG-Hi-BKF benchmark datasets."
            },
            "score": 4
        },
        {
            "id": "3867d999b550b57e6762f9d4b0114ee7551b2e2f",
            "paperId": "3867d999b550b57e6762f9d4b0114ee7551b2e2f",
            "title": "Predicting Question-Answering Performance of Large Language Models through Semantic Consistency",
            "abstract": "Semantic consistency of a language model is broadly defined as the model\u2019s ability to produce semantically-equivalent outputs, given semantically-equivalent inputs. We address the task of assessing question-answering (QA) semantic consistency of contemporary large language models (LLMs) by manually creating a benchmark dataset with high-quality paraphrases for factual questions, and release the dataset to the community.We further combine the semantic consistency metric with additional measurements suggested in prior work as correlating with LLM QA accuracy, for building and evaluating a framework for factual QA reference-less performance prediction \u2013 predicting the likelihood of a language model to accurately answer a question. Evaluating the framework on five contemporary LLMs, we demonstrate encouraging, significantly outperforming baselines, results.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work addresses the task of assessing question-answering (QA) semantic consistency of contemporary large language models (LLMs) by manually creating a benchmark dataset with high-quality paraphrases for factual questions, and releases the dataset to the community."
            },
            "score": 4
        },
        {
            "id": "3eb808e0e2df1ab55a11289c16e9692fa50af154",
            "paperId": "3eb808e0e2df1ab55a11289c16e9692fa50af154",
            "title": "A General Approach to Website Question Answering with Large Language Models",
            "abstract": "Language Models (LMs), in their most basic form, perform just like any other machine learning model - they produce interpolations and extrapolations based on their training distribution. Although recent models such as OpenAI's GPT-4 have demonstrated unprecedented capabilities in absorbing the copious volumes of information in their training data, their ability to consistently reproduce factual information still remains unproven. Additionally, LMs on their own lack the ability to keep up to date with real life data without frequent fine-tuning. These drawbacks effectively render base LMs unserviceable in Question Answering scenarios where they must respond to queries regarding volatile information. Retrieval Augmented Generation (RAG) and Tool Learning [1] were proposed as solutions to these problems, and with the development and usage of associated libraries, the aforementioned problems can be greatly mitigated. In this paper, we ponder a general approach to website Question Answering that integrates the zero-shot decision-making capabilities of LMs with the RAG capabilities of LangChain and is able to be kept up to date with dynamic information without the need for constant fine-tuning.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A general approach to website Question Answering is ponder that integrates the zero-shot decision-making capabilities of LMs with the RAG capabilities of LangChain and is able to be kept up to date with dynamic information without the need for constant fine-tuning."
            },
            "score": 4
        },
        {
            "id": "2f33488d1e4d405f84b1f15021c0870aec8bf680",
            "paperId": "2f33488d1e4d405f84b1f15021c0870aec8bf680",
            "title": "Exploring the pitfalls of large language models: Inconsistency and inaccuracy in answering pathology board examination\u2010style questions",
            "abstract": "In the rapidly advancing field of artificial intelligence, large language models (LLMs) such as ChatGPT and Google Bard are making significant progress, with applications extending across various fields, including medicine. This study explores their potential utility and pitfalls by assessing the performance of these LLMs in answering 150 multiple-choice questions sourced from the PathologyOutlines.com Question Bank, a well-established resource for pathology examination preparation. The assessment, encompassing 15 subspecialties in pathology, evaluated the accuracy and consistency of responses by these LLMs. Overall, ChatGPT outperformed Google Bard, scoring 122 out of 150, while Google Bard achieved a score of 70. In addition to accuracy, we explored the consistency of these LLMs by applying a test-retest approach over a two-week interval. ChatGPT showed a consistency rate of 85%, while Google Bard exhibited a lower consistency rate of 61%. In-depth analysis of incorrect responses identified potential factual inaccuracies and interpretive errors, underscoring the need for ongoing model refinement and human oversight. In conclusion, while LLMs have potential to enhance medical education and assist clinical decision-making, their current limitations underscore the need for continued development and the critical role of human expertise in the application of such models.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "While LLMs have potential to enhance medical education and assist clinical decision-making, their current limitations underscore the need for continued development and the critical role of human expertise in the application of such models."
            },
            "score": 4
        },
        {
            "id": "ebc502a4d173f6550a8cd6384cb06f2c43c7c1a3",
            "paperId": "ebc502a4d173f6550a8cd6384cb06f2c43c7c1a3",
            "title": "ClinicalGPT: Large Language Models Finetuned with Diverse Medical Data and Comprehensive Evaluation",
            "abstract": "Large language models have exhibited exceptional performance on various Natural Language Processing (NLP) tasks, leveraging techniques such as the pre-training, and instruction fine-tuning. Despite these advances, their effectiveness in medical applications is limited, due to challenges such as factual inaccuracies, reasoning abilities, and lack grounding in real-world experience. In this study, we present ClinicalGPT, a language model explicitly designed and optimized for clinical scenarios. By incorporating extensive and diverse real-world data, such as medical records, domain-specific knowledge, and multi-round dialogue consultations in the training process, ClinicalGPT is better prepared to handle multiple clinical task. Furthermore, we introduce a comprehensive evaluation framework that includes medical knowledge question-answering, medical exams, patient consultations, and diagnostic analysis of medical records. Our results demonstrate that ClinicalGPT significantly outperforms other models in these tasks, highlighting the effectiveness of our approach in adapting large language models to the critical domain of healthcare.",
            "year": 2023,
            "citationCount": 34,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ClinicalGPT is presented, a language model explicitly designed and optimized for clinical scenarios that significantly outperforms other models in these tasks, highlighting the effectiveness of the approach in adapting large language models to the critical domain of healthcare."
            },
            "score": 4
        },
        {
            "id": "0f6fe87afd1a3571f77c790893b03717e5d0422a",
            "paperId": "0f6fe87afd1a3571f77c790893b03717e5d0422a",
            "title": "Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators",
            "abstract": "Large language models (LLMs) outperform information retrieval techniques for downstream knowledge-intensive tasks when being prompted to generate world knowledge. However, community concerns abound regarding the factuality and potential implications of using this uncensored knowledge. In light of this, we introduce CONNER, a COmpreheNsive kNowledge Evaluation fRamework, designed to systematically and automatically evaluate generated knowledge from six important perspectives -- Factuality, Relevance, Coherence, Informativeness, Helpfulness and Validity. We conduct an extensive empirical analysis of the generated knowledge from three different types of LLMs on two widely studied knowledge-intensive tasks, i.e., open-domain question answering and knowledge-grounded dialogue. Surprisingly, our study reveals that the factuality of generated knowledge, even if lower, does not significantly hinder downstream tasks. Instead, the relevance and coherence of the outputs are more important than small factual mistakes. Further, we show how to use CONNER to improve knowledge-intensive tasks by designing two strategies: Prompt Engineering and Knowledge Selection. Our evaluation code and LLM-generated knowledge with human annotations will be released to facilitate future research.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Surprisingly, the study reveals that the factuality of generated knowledge, even if lower, does not significantly hinder downstream tasks and the relevance and coherence of the outputs are more important than small factual mistakes."
            },
            "score": 4
        },
        {
            "id": "6a8ef5e410da32958d0cba56e8d4c3f6302f02a4",
            "paperId": "6a8ef5e410da32958d0cba56e8d4c3f6302f02a4",
            "title": "Leveraging Large Language Models for Pre-trained Recommender Systems",
            "abstract": "Recent advancements in recommendation systems have shifted towards more comprehensive and personalized recommendations by utilizing large language models (LLM). However, effectively integrating LLM's commonsense knowledge and reasoning abilities into recommendation systems remains a challenging problem. In this paper, we propose RecSysLLM, a novel pre-trained recommendation model based on LLMs. RecSysLLM retains LLM reasoning and knowledge while integrating recommendation domain knowledge through unique designs of data, training, and inference. This allows RecSysLLM to leverage LLMs' capabilities for recommendation tasks in an efficient, unified framework. We demonstrate the effectiveness of RecSysLLM on benchmarks and real-world scenarios. RecSysLLM provides a promising approach to developing unified recommendation systems by fully exploiting the power of pre-trained language models.",
            "year": 2023,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "RecSysLLM provides a promising approach to developing unified recommendation systems by fully exploiting the power of pre-trained language models by exploiting LLM's commonsense knowledge and reasoning abilities."
            },
            "score": 4
        },
        {
            "id": "37d4171225928ea11b552f33f16de0385c5160fc",
            "paperId": "37d4171225928ea11b552f33f16de0385c5160fc",
            "title": "CapEnrich: Enriching Caption Semantics for Web Images via Cross-modal Pre-trained Knowledge",
            "abstract": "Automatically generating textual descriptions for massive unlabeled images on the web can greatly benefit realistic web applications, e.g. multimodal retrieval and recommendation. However, existing models suffer from the problem of generating \u201cover-generic\u201d descriptions, such as their tendency to generate repetitive sentences with common concepts for different images. These generic descriptions fail to provide sufficient textual semantics for ever-changing web images. Inspired by the recent success of Vision-Language Pre-training (VLP) models that learn diverse image-text concept alignment during pretraining, we explore leveraging their cross-modal pre-trained knowledge to automatically enrich the textual semantics of image descriptions. With no need for additional human annotations, we propose a plug-and-play framework, i.e CapEnrich, to complement the generic image descriptions with more semantic details. Specifically, we first propose an automatic data-building strategy to get desired training sentences, based on which we then adopt prompting strategies, i.e. learnable and template prompts, to incentivize VLP models to generate more textual details. For learnable templates, we fix the whole VLP model and only tune the prompt vectors, which leads to two advantages: 1) the pre-training knowledge of VLP models can be reserved as much as possible to describe diverse visual concepts; 2) only lightweight trainable parameters are required, so it is friendly to low data resources. Extensive experiments show that our method significantly improves the descriptiveness and diversity of generated sentences for web images. The code is available at https://github.com/yaolinli/CapEnrich.",
            "year": 2022,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Inspired by the recent success of Vision-Language Pre-training (VLP) models that learn diverse image-text concept alignment during pretraining, this work explores leveraging their cross-modal pre-trained knowledge to automatically enrich the textual semantics of image descriptions."
            },
            "score": 4
        },
        {
            "id": "a16a4a369baea4a17b5579f3206e8c1515550697",
            "paperId": "a16a4a369baea4a17b5579f3206e8c1515550697",
            "title": "Self-conditioning pre-trained language models",
            "abstract": "We study the presence of expert units in pre-trained Transformer-based Language Models (TLMs), and how they can be used to condition text generation to contain speci\ufb01c concepts. We de\ufb01ne expert units to be neurons that are able to detect a concept in the input with a given average precision. A concept is represented with a set of sentences that either do or do not contain the concept. Leveraging the OneSec dataset [25], we compile a dataset of 1344 concepts that allows diverse expert units in TLMs to be discovered. Our experiments demonstrate that off-the-shelf pre-trained TLMs can be conditioned on their own knowledge (self-conditioning) to generate text that contains a given concept. To this end, we intervene on the top expert units by \ufb01xing their output during inference, and we show experimentally that this is an effective method to condition TLMs. Our method does not require \ufb01ne-tuning the model or using additional parameters, which allows conditioning large TLM with minimal compute resources. Furthermore, by intervening on a small number of experts in GPT2, we can achieve parity with respect to two concepts at generation time. The speci\ufb01c case of gender bias is explored, and we show that, for given contexts, gender parity is achieved while maintaining the model\u2019s perplexity.",
            "year": 2021,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The speci\ufb01c case of gender bias is explored, and it is shown that, for given contexts, gender parity is achieved while maintaining the model\u2019s perplexity."
            },
            "score": 4
        },
        {
            "id": "43b275c728794c66da14f9a5b937831375d79b88",
            "paperId": "43b275c728794c66da14f9a5b937831375d79b88",
            "title": "HORNET: Enriching Pre-trained Language Representations with Heterogeneous Knowledge Sources",
            "abstract": "Knowledge-Enhanced Pre-trained Language Models (KEPLMs) improve the language understanding abilities of deep language models by leveraging the rich semantic knowledge from knowledge graphs, other than plain pre-training texts. However, previous efforts mostly use homogeneous knowledge (especially structured relation triples in knowledge graphs) to enhance the context-aware representations of entity mentions, whose performance may be limited by the coverage of knowledge graphs. Also, it is unclear whether these KEPLMs truly understand the injected semantic knowledge due to the \"black-box'' training mechanism. In this paper, we propose a novel KEPLM named HORNET, which integrates Heterogeneous knowledge from various structured and unstructured sources into the Roberta NETwork and hence takes full advantage of both linguistic and factual knowledge simultaneously. Specifically, we design a hybrid attention heterogeneous graph convolution network (HaHGCN) to learn heterogeneous knowledge representations based on the structured relation triplets from knowledge graphs and the unstructured entity description texts. Meanwhile, we propose the explicit dual knowledge understanding tasks to help induce a more effective infusion of the heterogeneous knowledge, promoting our model for learning the complicated mappings from the knowledge graph embedding space to the deep context-aware embedding space and vice versa. Experiments show that our HORNET model outperforms various KEPLM baselines on knowledge-aware tasks including knowledge probing, entity typing and relation extraction. Our model also achieves substantial improvement over several GLUE benchmark datasets, compared to other KEPLMs.",
            "year": 2021,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel KEPLM named HORNET is proposed, which integrates Heterogeneous knowledge from various structured and unstructured sources into the Roberta NETwork and hence takes full advantage of both linguistic and factual knowledge simultaneously, and design a hybrid attention heterogeneous graph convolution network (HaHGCN)."
            },
            "score": 4
        },
        {
            "id": "e7046bf945ad6326537a1ac78a96fd2f45acc900",
            "paperId": "e7046bf945ad6326537a1ac78a96fd2f45acc900",
            "title": "Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension",
            "abstract": "Machine reading comprehension (MRC) is a crucial and challenging task in NLP. Recently, pre-trained language models (LMs), especially BERT, have achieved remarkable success, presenting new state-of-the-art results in MRC. In this work, we investigate the potential of leveraging external knowledge bases (KBs) to further improve BERT for MRC. We introduce KT-NET, which employs an attention mechanism to adaptively select desired knowledge from KBs, and then fuses selected knowledge with BERT to enable context- and knowledge-aware predictions. We believe this would combine the merits of both deep LMs and curated KBs towards better MRC. Experimental results indicate that KT-NET offers significant and consistent improvements over BERT, outperforming competitive baselines on ReCoRD and SQuAD1.1 benchmarks. Notably, it ranks the 1st place on the ReCoRD leaderboard, and is also the best single model on the SQuAD1.1 leaderboard at the time of submission (March 4th, 2019).",
            "year": 2019,
            "citationCount": 127,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces KT-NET, which employs an attention mechanism to adaptively select desired knowledge from KBs, and then fuses selected knowledge with BERT to enable context- and knowledge-aware predictions."
            },
            "score": 4
        },
        {
            "id": "06b2ac5153e3d8d05c13c82f93d7f4e13eee6d0f",
            "paperId": "06b2ac5153e3d8d05c13c82f93d7f4e13eee6d0f",
            "title": "Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites",
            "abstract": "Large language models (LLMs) have shown remarkable performance in natural language processing (NLP) tasks. To comprehend and execute diverse human instructions over image data, instruction-tuned large vision-language models (LVLMs) have been introduced. However, LVLMs may suffer from different types of object hallucinations. Nevertheless, LVLMs are evaluated for coarse-grained object hallucinations only (i.e., generated objects non-existent in the input image). The fine-grained object attributes and behaviors non-existent in the image may still be generated but not measured by the current evaluation methods. In this paper, we thus focus on reducing fine-grained hallucinations of LVLMs. We propose \\textit{ReCaption}, a framework that consists of two components: rewriting captions using ChatGPT and fine-tuning the instruction-tuned LVLMs on the rewritten captions. We also propose a fine-grained probing-based evaluation method named \\textit{Fine-Grained Object Hallucination Evaluation} (\\textit{FGHE}). Our experiment results demonstrate that ReCaption effectively reduces fine-grained object hallucination for different LVLM options and improves their text generation quality. The code can be found at https://github.com/Anonymousanoy/FOHE.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ReCaption is proposed, a framework that consists of two components: rewriting captions using ChatGPT and fine-tuning the instruction-tuned LVLMs on the rewritten captions, and a fine-grained probing-based evaluation method named \\textit{Fine-Grained Object Hallucination Evaluation} (FGHE)."
            },
            "score": 4
        },
        {
            "id": "f99116659c7522941c2353f23bddd07251adaccc",
            "paperId": "f99116659c7522941c2353f23bddd07251adaccc",
            "title": "BTR: Binary Token Representations for Efficient Retrieval Augmented Language Models",
            "abstract": "Retrieval augmentation addresses many critical problems in large language models such as hallucination, staleness, and privacy leaks. However, running retrieval-augmented language models (LMs) is slow and difficult to scale due to processing large amounts of retrieved text. We introduce binary token representations (BTR), which use 1-bit vectors to precompute every token in passages, significantly reducing computation during inference. Despite the potential loss of accuracy, our new calibration techniques and training objectives restore performance. Combined with offline and runtime compression, this only requires 127GB of disk space for encoding 3 billion tokens in Wikipedia. Our experiments show that on five knowledge-intensive NLP tasks, BTR accelerates state-of-the-art inference by up to 4x and reduces storage by over 100x while maintaining over 95% task performance.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "BTR is introduced, which use 1-bit vectors to precompute every token in passages, significantly reducing computation during inference, and accelerates state-of-the-art inference by up to 4x and reduces storage by over 100x while maintaining over 95% task performance."
            },
            "score": 4
        },
        {
            "id": "7cfbd36c0043098589cbaf18dca2b41d8dc24abe",
            "paperId": "7cfbd36c0043098589cbaf18dca2b41d8dc24abe",
            "title": "Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre-training",
            "abstract": "Large-scale vision-language pre-trained (VLP) models are prone to hallucinate non-existent visual objects when generating text based on visual information. In this paper, we systematically study the object hallucination problem from three aspects. First, we examine recent state-of-the-art VLP models, showing that they still hallucinate frequently and models achieving better scores on standard metrics (e.g., CIDEr) could be more unfaithful. Second, we investigate how different types of image encoding in VLP influence hallucination, including region-based, grid-based, and patch-based. Surprisingly, we find that patch-based features perform the best and smaller patch resolution yields a non-trivial reduction in object hallucination. Third, we decouple various VLP objectives and demonstrate that token-level image-text alignment and controlled generation are crucial to reducing hallucination. Based on that, we propose a simple yet effective VLP loss named ObjMLM to further mitigate object hallucination. Results show that it reduces object hallucination by up to 17.4% when tested on two benchmarks (COCO Caption for in-domain and NoCaps for out-of-domain evaluation).",
            "year": 2022,
            "citationCount": 35,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a simple yet effective VLP loss named ObjMLM to further mitigate object hallucination and decouple various VLP objectives and demonstrates that token-level image-text alignment and controlled generation are crucial to reducing hallucination."
            },
            "score": 4
        },
        {
            "id": "6df5ba162b38d2853cc8431ff6f878d085c03693",
            "paperId": "6df5ba162b38d2853cc8431ff6f878d085c03693",
            "title": "SERPENT-VLM : Self-Refining Radiology Report Generation Using Vision Language Models",
            "abstract": "Radiology Report Generation (R2Gen) demonstrates how Multi-modal Large Language Models (MLLMs) can automate the creation of accurate and coherent radiological reports. Existing methods often hallucinate details in text-based reports that don't accurately reflect the image content. To mitigate this, we introduce a novel strategy, SERPENT-VLM (SElf Refining Radiology RePort GENeraTion using Vision Language Models), which improves the R2Gen task by integrating a self-refining mechanism into the MLLM framework. We employ a unique self-supervised loss that leverages similarity between pooled image representations and the contextual representations of the generated radiological text, alongside the standard Causal Language Modeling objective, to refine image-text representations. This allows the model to scrutinize and align the generated text through dynamic interaction between a given image and the generated text, therefore reducing hallucination and continuously enhancing nuanced report generation. SERPENT-VLM outperforms existing baselines such as LLaVA-Med, BiomedGPT, etc., achieving SoTA performance on the IU X-ray and Radiology Objects in COntext (ROCO) datasets, and also proves to be robust against noisy images. A qualitative case study emphasizes the significant advancements towards more sophisticated MLLM frameworks for R2Gen, opening paths for further research into self-supervised refinement in the medical imaging domain.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel strategy, SERPENT-VLM (SElf Refining Radiology RePort GENeraTion using Vision Language Models), which improves the R2Gen task by integrating a self-refining mechanism into the MLLM framework, and proves to be robust against noisy images."
            },
            "score": 4
        },
        {
            "id": "7751f6cdec0f4473c1733eec91699744a7d5176f",
            "paperId": "7751f6cdec0f4473c1733eec91699744a7d5176f",
            "title": "HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding",
            "abstract": "While large vision-language models (LVLMs) have demonstrated impressive capabilities in interpreting multi-modal contexts, they invariably suffer from object hallucinations (OH). We introduce HALC, a novel decoding algorithm designed to mitigate OH in LVLMs. HALC leverages distinct fine-grained optimal visual information in vision-language tasks and operates on both local and global contexts simultaneously. Specifically, HALC integrates a robust auto-focal grounding mechanism (locally) to correct hallucinated tokens on the fly, and a specialized beam search algorithm (globally) to significantly reduce OH while preserving text generation quality. Additionally, HALC can be integrated into any LVLMs as a plug-and-play module without extra training. Extensive experimental studies demonstrate the effectiveness of HALC in reducing OH, outperforming state-of-the-arts across four benchmarks.",
            "year": 2024,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "HALC, a novel decoding algorithm designed to mitigate OH in LVLMs, is introduced, which leverages distinct fine-grained optimal visual information in vision-language tasks and operates on both local and global contexts simultaneously."
            },
            "score": 4
        },
        {
            "id": "f767f0a883c8fc70de03fb8b65ed87e1fef5f415",
            "paperId": "f767f0a883c8fc70de03fb8b65ed87e1fef5f415",
            "title": "Hallucination Diversity-Aware Active Learning for Text Summarization",
            "abstract": "Large Language Models (LLMs) have shown propensity to generate hallucinated outputs, i.e., texts that are factually incorrect or unsupported. Existing methods for alleviating hallucinations typically require costly human annotations to identify and correct hallucinations in LLM outputs. Moreover, most of these methods focus on a specific type of hallucination, e.g., entity or token errors, which limits their effectiveness in addressing various types of hallucinations exhibited in LLM outputs. To our best knowledge, in this paper we propose the first active learning framework to alleviate LLM hallucinations, reducing costly human annotations of hallucination needed. By measuring fine-grained hallucinations from errors in semantic frame, discourse and content verifiability in text summarization, we propose HAllucination Diversity-Aware Sampling (HADAS) to select diverse hallucinations for annotations in active learning for LLM finetuning. Extensive experiments on three datasets and different backbone models demonstrate advantages of our method in effectively and efficiently mitigating LLM hallucinations.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes the first active learning framework to alleviate LLM hallucinations, reducing costly human annotations of hallucination needed, and proposes HAllucination Diversity-Aware Sampling (HADAS) to select diverse hallucinations for annotations in active learning for LLM finetuning."
            },
            "score": 4
        },
        {
            "id": "c78d1cc1f9cba23235c8e67bac5c896f8e4708b5",
            "paperId": "c78d1cc1f9cba23235c8e67bac5c896f8e4708b5",
            "title": "LLMs in Biomedicine: A study on clinical Named Entity Recognition",
            "abstract": "Large Language Models (LLMs) demonstrate remarkable versatility in various NLP tasks but encounter distinct challenges in biomedicine due to medical language complexities and data scarcity. This paper investigates the application of LLMs in the medical domain by exploring strategies to enhance their performance for the Named-Entity Recognition (NER) task. Specifically, our study reveals the importance of meticulously designed prompts in biomedicine. Strategic selection of in-context examples yields a notable improvement, showcasing ~15-20\\% increase in F1 score across all benchmark datasets for few-shot clinical NER. Additionally, our findings suggest that integrating external resources through prompting strategies can bridge the gap between general-purpose LLM proficiency and the specialized demands of medical NER. Leveraging a medical knowledge base, our proposed method inspired by Retrieval-Augmented Generation (RAG) can boost the F1 score of LLMs for zero-shot clinical NER. We will release the code upon publication.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study reveals the importance of meticulously designed prompts in biomedicine and suggests that integrating external resources through prompting strategies can bridge the gap between general-purpose LLM proficiency and the specialized demands of medical NER."
            },
            "score": 4
        },
        {
            "id": "d9dd59421ec47fd74b08bafdd7e8a7a39b27b12c",
            "paperId": "d9dd59421ec47fd74b08bafdd7e8a7a39b27b12c",
            "title": "Edinburgh Research Explorer Do Language Models Learn about Legal Entity Types during Pretraining?",
            "abstract": "Language Models (LMs) have proven their ability to acquire diverse linguistic knowledge during the pretraining phase, potentially serving as a valuable source of incidental supervision for downstream tasks. However, there has been limited research conducted on the retrieval of domain-specific knowledge, and specifically legal knowledge. We propose to explore the task of Entity Typing, serving as a proxy for evaluating legal knowledge as an essential aspect of text comprehension, and a foundational task to numerous downstream legal NLP applications. Through systematic evaluation and analysis and two types of prompting (cloze sentences and QA-based templates) and to clarify the nature of these acquired cues, we compare diverse types and lengths of entities both general and domain-specific entities, semantics or syntax signals, and different LM pretraining corpus (generic and legal-oriented) and architectures (encoder BERT-based and decoder-only with Llama2). We show that (1) Llama2 performs well on certain entities and exhibits potential for substantial improvement with optimized prompt templates, (2) law-oriented LMs show inconsistent performance, possibly due to variations in their training corpus, (3) LMs demonstrate the ability to type entities even in the case of multi-token entities, (4) all models struggle with entities belonging to sub-domains of the law (5) Llama2 appears to frequently overlook syntactic cues, a shortcoming less present in BERT-based architectures. The code of the experiments is available at https://github.com/clairebarale/ probing_legal",
            "year": null,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The task of Entity Typing is proposed to explore, serving as a proxy for evaluating legal knowledge as an essential aspect of text comprehension, and a foundational task to numerous downstream legal NLP applications."
            },
            "score": 4
        },
        {
            "id": "165a7e7ff7452aa57c58b9ba90f2b32cb3cf8f1a",
            "paperId": "165a7e7ff7452aa57c58b9ba90f2b32cb3cf8f1a",
            "title": "Abstract 5421: Constructing the largest-scale knowledge graph using all PubMed abstracts and its application for highly specific and accurate knowledge retrieval",
            "abstract": "\n Most of the biomedical knowledge the research community has acquired during the past few decades has been deposited in scientific literature as unstructured text. Converting the unstructured text into the structured form will enable novel methodologies and applications for scientific discovery that can fully harness the power of the existing knowledge. To this end, two fundamental questions need to be addressed: named entity recognition (NER) and relation extraction (RE). NER deals with identifying the concepts or entities in texts, such as diseases, genes/proteins, chemical compounds, etc. while RE aims to extract the relations among these entities. Together, the extracted information forms a knowledge graph (KG) where the nodes are entities in the texts and the edges represent their relationships. KGs can link concepts within existing research to allow researchers to find connections that may have been difficult to discover without them. The LitCoin Natural Language Processing (NLP) Challenge was recently organized by NCATS of NIH and NASA to spur innovation by rewarding the most creative and high-impact uses of biomedical text to create KGs. Our team participated in the challenge and ranked first place. We have applied the methods we developed for the LitCoin NLP challenge to all PubMed abstracts and constructed the largest-scale biomedical KG to date. We show that powerful and versatile query functions can be implemented on top of the KG to enable highly specific and accurate knowledge retrieval and inference of causal and indirect relationships.\n Citation Format: Xin Sui, Yuan Zhang, Feng Pan, Donghu Sun, Menghan Chung, Jinfeng Zhang. Constructing the largest-scale knowledge graph using all PubMed abstracts and its application for highly specific and accurate knowledge retrieval. [abstract]. In: Proceedings of the American Association for Cancer Research Annual Meeting 2023; Part 1 (Regular and Invited Abstracts); 2023 Apr 14-19; Orlando, FL. Philadelphia (PA): AACR; Cancer Res 2023;83(7_Suppl):Abstract nr 5421.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that powerful and versatile query functions can be implemented on top of the KG to enable highly specific and accurate knowledge retrieval and inference of causal and indirect relationships."
            },
            "score": 4
        },
        {
            "id": "358e20d80d6758cf0f6ce9f16f8c21a197095e4b",
            "paperId": "358e20d80d6758cf0f6ce9f16f8c21a197095e4b",
            "title": "DAMO-NLP at SemEval-2023 Task 2: A Unified Retrieval-augmented System for Multilingual Named Entity Recognition",
            "abstract": "The MultiCoNER II shared task aims to tackle multilingual named entity recognition (NER) in fine-grained and noisy scenarios, and it inherits the semantic ambiguity and low-context setting of the MultiCoNER I task. To cope with these problems, the previous top systems in the MultiCoNER I either incorporate the knowledge bases or gazetteers. However, they still suffer from insufficient knowledge, limited context length, single retrieval strategy. In this paper, our team DAMO-NLP proposes a unified retrieval-augmented system (U-RaNER) for fine-grained multilingual NER. We perform error analysis on the previous top systems and reveal that their performance bottleneck lies in insufficient knowledge. Also, we discover that the limited context length causes the retrieval knowledge to be invisible to the model. To enhance the retrieval context, we incorporate the entity-centric Wikidata knowledge base, while utilizing the infusion approach to broaden the contextual scope of the model. Also, we explore various search strategies and refine the quality of retrieval knowledge. Our system wins 9 out of 13 tracks in the MultiCoNER II shared task. Additionally, we compared our system with ChatGPT, one of the large language models which have unlocked strong capabilities on many tasks. The results show that there is still much room for improvement for ChatGPT on the extraction task.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A unified retrieval-augmented system (U-RaNER) for fine-grained multilingual NER, which incorporates the entity-centric Wikidata knowledge base, while utilizing the infusion approach to broaden the contextual scope of the model."
            },
            "score": 4
        },
        {
            "id": "efbc94690cf2ebd935ef7a13cd2e378758aedb12",
            "paperId": "efbc94690cf2ebd935ef7a13cd2e378758aedb12",
            "title": "Auto-MLM: Improved Contrastive Learning for Self-supervised Multi-lingual Knowledge Retrieval",
            "abstract": "Contrastive learning (CL) has become a ubiquitous approach for several natural language processing (NLP) downstream tasks, especially for question answering (QA). However, the major challenge, how to efficiently train the knowledge retrieval model in an unsupervised manner, is still unresolved. Recently the commonly used methods are composed of CL and masked language model (MLM). Unexpectedly, MLM ignores the sentence-level training, and CL also neglects extraction of the internal info from the query. To optimize the CL hardly obtain internal information from the original query, we introduce a joint training method by combining CL and Auto-MLM for self-supervised multi-lingual knowledge retrieval. First, we acquire the fixed dimensional sentence vector. Then, mask some words among the original sentences with random strategy. Finally, we generate a new token representation for predicting the masked tokens. Experimental results show that our proposed approach consistently outperforms all the previous SOTA methods on both AliExpress $\\&$ LAZADA service corpus and openly available corpora in 8 languages.",
            "year": 2022,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a joint training method by combining CL and Auto-MLM for self-supervised multi-lingual knowledge retrieval, which consistently outperforms all the previous SOTA methods on both AliExpress $\\&$ LAZADA service corpus and openly available corpora in 8 languages."
            },
            "score": 4
        },
        {
            "id": "b2a0f97d9135f91f28196e1f8adea9ac79b230c2",
            "paperId": "b2a0f97d9135f91f28196e1f8adea9ac79b230c2",
            "title": "Large Language Models and Multimodal Retrieval for Visual Word Sense Disambiguation",
            "abstract": "Visual Word Sense Disambiguation (VWSD) is a novel challenging task with the goal of retrieving an image among a set of candidates, which better represents the meaning of an ambiguous word within a given context. In this paper, we make a substantial step towards unveiling this interesting task by applying a varying set of approaches. Since VWSD is primarily a text-image retrieval task, we explore the latest transformer-based methods for multimodal retrieval. Additionally, we utilize Large Language Models (LLMs) as knowledge bases to enhance the given phrases and resolve ambiguity related to the target word. We also study VWSD as a unimodal problem by converting to text-to-text and image-to-image retrieval, as well as question-answering (QA), to fully explore the capabilities of relevant models. To tap into the implicit knowledge of LLMs, we experiment with Chain-of-Thought (CoT) prompting to guide explainable answer generation. On top of all, we train a learn to rank (LTR) model in order to combine our different modules, achieving competitive ranking results. Extensive experiments on VWSD demonstrate valuable insights to effectively drive future directions.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper trains a learn to rank (LTR) model in order to combine the different modules of VWSD by converting to text-to-text and image- to-image retrieval, as well as question-answering (QA), to fully explore the capabilities of relevant models."
            },
            "score": 3
        },
        {
            "id": "177bf0086a714ff305e45b720cda82e992c9fc7c",
            "paperId": "177bf0086a714ff305e45b720cda82e992c9fc7c",
            "title": "PiTL: Cross-modal Retrieval with Weakly-supervised Vision-language Pre-training via Prompting",
            "abstract": "Vision-language (VL) Pre-training (VLP) has shown to well generalize VL models over a wide range of VL downstream tasks, especially for cross-modal retrieval. However, it hinges on a huge amount of image-text pairs, which requires tedious and costly curation. On the contrary,weakly-supervised VLP (W-VLP) explores means with object tags generated by a pre-trained object detector (OD) from images. Yet, they still require paired information, i.e. images and object-level annotations, as supervision to train an OD. To further reduce the amount of supervision, we propose Prompts-in-The-Loop (PiTL) that prompts knowledge from large language models (LLMs) to describe images. Concretely, given a category label of an image, e.g.refinery, the knowledge, e.g.a refinery could be seen with large storage tanks, pipework, and ..., extracted by LLMs is used as the language counterpart. The knowledge supplements, e.g. the common relations among entities most likely appearing in a scene. We create IN14K, a new VL dataset of 9M images and 1M descriptions of 14K categories from ImageNet21K with PiTL. Empirically, the VL models pre-trained with PiTL-generated pairs are strongly favored over other W-VLP works on image-to-text (I2T) and text-to-image (T2I) retrieval tasks, with less supervision. The results reveal the effectiveness of PiTL-generated pairs for VLP.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Empirically, the VL models pre-trained with PiTL-generated pairs are strongly favored over other W-VLP works on image-to-text (I2T) and text- to-image (T2I) retrieval tasks, with less supervision."
            },
            "score": 3
        },
        {
            "id": "e075acb6ca9dc698e3ac05f97ae55d7a58ee9e9f",
            "paperId": "e075acb6ca9dc698e3ac05f97ae55d7a58ee9e9f",
            "title": "Leveraging Pre-trained Language Models for Time Interval Prediction in Text-Enhanced Temporal Knowledge Graphs",
            "abstract": "Most knowledge graph completion (KGC) methods learn latent representations of entities and relations of a given graph by mapping them into a vector space. Although the majority of these methods focus on static knowledge graphs, a large number of publicly available KGs contain temporal information stating the time instant/period over which a certain fact has been true. Such graphs are often known as temporal knowledge graphs. Furthermore, knowledge graphs may also contain textual descriptions of entities and relations. Both temporal information and textual descriptions are not taken into account during representation learning by static KGC methods, and only structural information of the graph is leveraged. Recently, some studies have used temporal information to improve link prediction, yet they do not exploit textual descriptions and do not support inductive inference (prediction on entities that have not been seen in training). We propose a novel framework called TEMT that exploits the power of pre-trained language models (PLMs) for text-enhanced temporal knowledge graph completion. The knowledge stored in the parameters of a PLM allows TEMT to produce rich semantic representations of facts and to generalize on previously unseen entities. TEMT leverages textual and temporal information available in a KG, treats them separately, and fuses them to get plausibility scores of facts. Unlike previous approaches, TEMT effectively captures dependencies across different time points and enables predictions on unseen entities. To assess the performance of TEMT, we carried out several experiments including time interval prediction, both in transductive and inductive settings, and triple classification. The experimental results show that TEMT is competitive with the state-of-the-art.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel framework called TEMT is proposed that exploits the power of pre-trained language models (PLMs) for text-enhanced temporal knowledge graph completion and effectively captures dependencies across different time points and enables predictions on unseen entities."
            },
            "score": 3
        },
        {
            "id": "57eb41c7b5cbffb134cdcf67e455c9c852024cbd",
            "paperId": "57eb41c7b5cbffb134cdcf67e455c9c852024cbd",
            "title": "Knowledge Transfer from Pre-trained Language Models to Cif-based Speech Recognizers via Hierarchical Distillation",
            "abstract": "Large-scale pre-trained language models (PLMs) have shown great potential in natural language processing tasks. Leveraging the capabilities of PLMs to enhance automatic speech recognition (ASR) systems has also emerged as a promising research direction. However, previous works may be limited by the inflexible structures of PLMs and the insufficient utilization of PLMs. To alleviate these problems, we propose the hierarchical knowledge distillation (HKD) on the continuous integrate-and-fire (CIF) based ASR models. To transfer knowledge from PLMs to the ASR models, HKD employs cross-modal knowledge distillation with contrastive loss at the acoustic level and knowledge distillation with regression loss at the linguistic level. Compared with the original CIF-based model, our method achieves 15% and 9% relative error rate reduction on the AISHELL-1 and LibriSpeech datasets, respectively.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The hierarchical knowledge distillation (HKD) on the continuous integrate-and-fire (CIF) based ASR models is proposed, which achieves 15% and 9% relative error rate reduction on the AISHELL-1 and LibriSpeech datasets, respectively."
            },
            "score": 3
        },
        {
            "id": "72a0736627b2a8448dc653c0d1e4c80960c929b5",
            "paperId": "72a0736627b2a8448dc653c0d1e4c80960c929b5",
            "title": "Leveraging Pre-trained BERT for Audio Captioning",
            "abstract": "Audio captioning aims at using language to describe the content of an audio clip. Existing audio captioning systems are generally based on an encoder-decoder architecture, in which acoustic information is extracted by an audio encoder and then a language decoder is used to generate the captions. Training an audio captioning system often encounters the problem of data scarcity. Transferring knowledge from pre-trained audio models such as Pre-trained Audio Neural Networks (PANNs) have recently emerged as a useful method to mitigate this issue. However, there is less attention on exploiting pre-trained language models for the decoder, compared with the encoder. BERT is a pre-trained language model that has been extensively used in natural language processing tasks. Nevertheless, the potential of using BERT as the language decoder for audio captioning has not been investigated. In this study, we demonstrate the efficacy of the pre-trained BERT model for audio captioning. Specifically, we apply PANNs as the encoder and initialize the decoder from the publicly available pre-trained BERT models. We conduct an empirical study on the use of these BERT models for the decoder in the audio captioning model. Our models achieve competitive results with the existing audio captioning methods on the AudioCaps dataset.",
            "year": 2022,
            "citationCount": 25,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study applies PANNs as the encoder and initialize the decoder from the publicly available pre-trained BERT models for audio captioning, and achieves competitive results with the existingaudio captioning methods on the AudioCaps dataset."
            },
            "score": 3
        },
        {
            "id": "b2c426ae8aff6fd32c850c5fb24266b47417d96e",
            "paperId": "b2c426ae8aff6fd32c850c5fb24266b47417d96e",
            "title": "An Integration of Pre-Trained Speech and Language Models for End-to-End Speech Recognition",
            "abstract": "Advances in machine learning have made it possible to perform various text and speech processing tasks, including automatic speech recognition (ASR), in an end-to-end (E2E) manner. Since typical E2E approaches require large amounts of training data and resources, leveraging pre-trained foundation models instead of training from scratch is gaining attention. Although there have been attempts to use pre-trained speech and language models in ASR, most of them are limited to using either. This paper explores the potential of integrating a pre-trained speech representation model with a large language model (LLM) for E2E ASR. The proposed model enables E2E ASR by generating text tokens in an autoregressive manner via speech representations as speech prompts, taking advantage of the vast knowledge provided by the LLM. Furthermore, the proposed model can incorporate remarkable developments for LLM utilization, such as inference optimization and parameter-efficient domain adaptation. Experimental results show that the proposed model achieves performance comparable to modern E2E ASR models.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The potential of integrating a pre-trained speech representation model with a large language model (LLM) for E2E ASR is explored and the proposed model can incorporate remarkable developments for LLM utilization, such as inference optimization and parameter-efficient domain adaptation."
            },
            "score": 3
        },
        {
            "id": "2ac1267d2a29420b9e10c4b078e5143e1b407b31",
            "paperId": "2ac1267d2a29420b9e10c4b078e5143e1b407b31",
            "title": "Breaking Down Word Semantics from Pre-trained Language Models through Layer-wise Dimension Selection",
            "abstract": "Contextual word embeddings obtained from pre-trained language model (PLM) have proven effective for various natural language processing tasks at the word level. However, interpreting the hidden aspects within embeddings, such as syntax and semantics, remains challenging. Disentangled representation learning has emerged as a promising approach, which separates specific aspects into distinct embeddings. Furthermore, different linguistic knowledge is believed to be stored in different layers of PLM. This paper aims to disentangle semantic sense from BERT by applying a binary mask to middle outputs across the layers, without updating pre-trained parameters. The disentangled embeddings are evaluated through binary classification to determine if the target word in two different sentences has the same meaning. Experiments with cased BERT$_{\\texttt{base}}$ show that leveraging layer-wise information is effective and disentangling semantic sense further improve performance.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper aims to disentangle semantic sense from BERT by applying a binary mask to middle outputs across the layers, without updating pre-trained parameters."
            },
            "score": 3
        },
        {
            "id": "6844c5992e6697a8f6ab473fb8eda1db12359a7b",
            "paperId": "6844c5992e6697a8f6ab473fb8eda1db12359a7b",
            "title": "Enhancing Real-Time Semantic Segmentation with Textual Knowledge of Pre-Trained Vision-Language Model: A Lightweight Approach",
            "abstract": "In this paper, we present a lightweight method for real-time semantic segmentation models by leveraging the power of pre-trained vision-language models. Our approach incorporates the CLIP text encoder, which provides rich semantic embeddings for text labels, and effectively distills its rich textual knowledge to the segmentation model. The proposed framework integrates the image and text embeddings, enabling visual and textual information alignment. Besides, we introduce learnable prompt embeddings to capture class-specific information and enhance the semantic understanding of the model. To ensure efficient learning, we devise a two-stage training procedure that allows the segmentation backbone to learn from fixed text embeddings in the first stage and optimize the prompt embeddings in the second stage. Extensive experiments and ablation studies demonstrate the effectiveness of our method in significantly improving the performance of the real-time semantic segmentation model.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A lightweight method for real-time semantic segmentation models by leveraging the power of pre-trained vision-language models and introducing learnable prompt embeddings to capture class-specific information and enhance the semantic understanding of the model."
            },
            "score": 3
        },
        {
            "id": "e47abbbdc628ab00f71664b7304ad31390df8fad",
            "paperId": "e47abbbdc628ab00f71664b7304ad31390df8fad",
            "title": "Non-Autoregressive ASR Modeling Using Pre-Trained Language Models for Chinese Speech Recognition",
            "abstract": "Transformer-based models have led to significant innovation in various classic and practical subjects, including speech processing, natural language processing, and computer vision. On top of the Transformer, attention-based end-to-end automatic speech recognition (ASR) models have become a popular fashion in recent years. Specifically, an emergent research topic is non-autoregressive modeling, which can achieve fast inference speed and obtain competitive performance when compared with conventional autoregressive methods. In addition, in the context of natural language processing, the bidirectional encoder representations from Transformers (BERT) model and its variants have received widespread attention, partially due to their ability to infer contextualized word representations and obtain superior performances of downstream tasks through simple fine-tuning. However, to our knowledge, leveraging the synergistic power of non-autoregressive modeling and pre-trained language model for ASR remains relatively underexplored. In this regard, this study presents a novel pre-trained language model-based non-autoregressive ASR framework. A series of experiments were conducted on two publicly available Chinese datasets, AISHELL-1 and AISHELL-2, to demonstrate competitive or superior results of the proposed ASR models when compared with well-practiced baseline systems. In addition, a set of comparative experiments is likewise carried out with different settings to analyze the performance of the proposed framework.",
            "year": 2021,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study presents a novel pre-trained language model-based non-autoregressive ASR framework and conducts a series of experiments to demonstrate competitive or superior results of the proposed ASR models when compared with well-practiced baseline systems."
            },
            "score": 3
        },
        {
            "id": "ac14aa5c8ba41037446ae04ace3fdcabf1ecd710",
            "paperId": "ac14aa5c8ba41037446ae04ace3fdcabf1ecd710",
            "title": "Pingan Smart Health and SJTU at COIN - Shared Task: utilizing Pre-trained Language Models and Common-sense Knowledge in Machine Reading Tasks",
            "abstract": "To solve the shared tasks of COIN: COmmonsense INference in Natural Language Processing) Workshop in , we need explore the impact of knowledge representation in modeling commonsense knowledge to boost performance of machine reading comprehension beyond simple text matching. There are two approaches to represent knowledge in the low-dimensional space. The first is to leverage large-scale unsupervised text corpus to train fixed or contextual language representations. The second approach is to explicitly express knowledge into a knowledge graph (KG), and then fit a model to represent the facts in the KG. We have experimented both (a) improving the fine-tuning of pre-trained language models on a task with a small dataset size, by leveraging datasets of similar tasks; and (b) incorporating the distributional representations of a KG onto the representations of pre-trained language models, via simply concatenation or multi-head attention. We find out that: (a) for task 1, first fine-tuning on larger datasets like RACE (Lai et al., 2017) and SWAG (Zellersetal.,2018), and then fine-tuning on the target task improve the performance significantly; (b) for task 2, we find out the incorporating a KG of commonsense knowledge, WordNet (Miller, 1995) into the Bert model (Devlin et al., 2018) is helpful, however, it will hurts the performace of XLNET (Yangetal.,2019), a more powerful pre-trained model. Our approaches achieve the state-of-the-art results on both shared task\u2019s official test data, outperforming all the other submissions.",
            "year": 2019,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Two approaches to explore the impact of knowledge representation in modeling commonsense knowledge to boost performance of machine reading comprehension beyond simple text matching and achieve the state-of-the-art results on both shared task\u2019s official test data are achieved."
            },
            "score": 3
        },
        {
            "id": "92f2000f29f1aed7f2cc4e3cb5f783e079ef553f",
            "paperId": "92f2000f29f1aed7f2cc4e3cb5f783e079ef553f",
            "title": "MergeDistill: Merging Language Models using Pre-trained Distillation",
            "abstract": "Pre-trained multilingual language models (LMs) have achieved state-of-the-art results in cross-lingual transfer, but they often lead to an inequitable representation of languages due to limited capacity, skewed pre-training data, and sub-optimal vocabularies. This has prompted the creation of an ever-growing pre-trained model universe, where each model is trained on large amounts of language or domain specific data with a carefully curated, linguistically informed vocabulary. However, doing so brings us back full circle and prevents one from leveraging the benefits of multilinguality. To address the gaps at both ends of the spectrum, we propose MergeDistill, a framework to merge pre-trained LMs in a way that can best leverage their assets with minimal dependencies, using task-agnostic knowledge distillation. We demonstrate the applicability of our framework in a practical setting by leveraging pre-existing teacher LMs and training student LMs that perform competitively with or even outperform teacher LMs trained on several orders of magnitude more data and with a fixed model capacity. We also highlight the importance of teacher selection and its impact on student model performance.",
            "year": 2021,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes MergeDistill, a framework to merge pre-trained LMs in a way that can best leverage their assets with minimal dependencies, using task-agnostic knowledge distillation, and demonstrates the applicability of the framework in a practical setting."
            },
            "score": 3
        },
        {
            "id": "c991b527a87309550597fe63b9573f31a3317677",
            "paperId": "c991b527a87309550597fe63b9573f31a3317677",
            "title": "Slot Induction via Pre-trained Language Model Probing and Multi-level Contrastive Learning",
            "abstract": "Recent advanced methods in Natural Language Understanding for Task-oriented Dialogue (TOD) Systems (e.g., intent detection and slot filling) require a large amount of annotated data to achieve competitive performance. In reality, token-level annotations (slot labels) are time-consuming and difficult to acquire. In this work, we study the Slot Induction (SI) task whose objective is to induce slot boundaries without explicit knowledge of token-level slot annotations. We propose leveraging Unsupervised Pre-trained Language Model (PLM) Probing and Contrastive Learning mechanism to exploit (1) unsupervised semantic knowledge extracted from PLM, and (2) additional sentence-level intent label signals available from TOD. Our approach is shown to be effective in SI task and capable of bridging the gaps with token-level supervised models on two NLU benchmark datasets. When generalized to emerging intents, our SI objectives also provide enhanced slot label representations, leading to improved performance on the Slot Filling tasks.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes leveraging Unsupervised Pre-trained Language Model (PLM) Probing and Contrastive Learning mechanism to exploit (1) unsupervised semantic knowledge extracted from PLM, and (2) additional sentence-level intent label signals available from TOD to exploit token-level slot annotations."
            },
            "score": 3
        },
        {
            "id": "8e0dbc206db278c29c4b70eae2060db2818f72dd",
            "paperId": "8e0dbc206db278c29c4b70eae2060db2818f72dd",
            "title": "Generalization and Hallucination of Large Vision-Language Models through a Camouflaged Lens",
            "abstract": "Large Vision-Language Model (LVLM) has seen burgeoning development and increasing attention recently. In this paper, we propose a novel framework, camo-perceptive vision-language framework (CPVLF), to explore whether LVLM can generalize to the challenging camouflaged object detection (COD) scenario in a training-free manner. During the process of generalization, we find that due to hallucination issues within LVLM, it can erroneously perceive objects in camouflaged scenes, producing counterfactual concepts. Moreover, as LVLM is not specifically trained for the precise localization of camouflaged objects, it exhibits a degree of uncertainty in accurately pinpointing these objects. Therefore, we propose chain of visual perception, which enhances LVLM's perception of camouflaged scenes from both linguistic and visual perspectives, reducing the hallucination issue and improving its capability in accurately locating camouflaged objects. We validate the effectiveness of CPVLF on three widely used COD datasets, and the experiments show the potential of LVLM in the COD task.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel framework, camo-perceptive vision-language framework (CPVLF), is proposed, which enhances LVLM's perception of camouflaged scenes from both linguistic and visual perspectives, reducing the hallucination issue and improving its capability in accurately locating camouflaged objects."
            },
            "score": 3
        },
        {
            "id": "aae2e3a53ec7cdcff9898b11a7f73859206b89a1",
            "paperId": "aae2e3a53ec7cdcff9898b11a7f73859206b89a1",
            "title": "Zero-Shot Construction of Chinese Medical Knowledge Graph with ChatGPT",
            "abstract": "Knowledge graphs have revolutionized the organization and retrieval of real-world knowledge, prompting inter-est in automatic NLP-based approaches for extracting medical knowledge from texts. However, the availability of high-quality Chinese medical knowledge remains limited, posing challenges for constructing Chinese medical knowledge graphs. As LLMs like ChatGPT show promise in zero-shot learning for many NLP downstream tasks, their potential on constructing Chinese medical knowledge graphs is still uncertain. In this study, we create a Chinese medical knowledge graph by manually annotating textual data and using ChatGPT to automatically generate the graph. We refine the results using filtering and mapping rules to align with our schema. The manually generated graph serves as the ground truth for evaluation, and we explore different methods to enhance its accuracy through knowledge graph completion techniques. As a result, we emphasize the potential of employing ChatGPT for automated knowledge graph construction within the Chinese medical domain. While ChatGPT successfully identifies a larger number of entities, further en-hancements are required to improve its performance in extracting more qualified relations.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study creates a Chinese medical knowledge graph by manually annotating textual data and using ChatGPT to automatically generate the graph, and explores different methods to enhance its accuracy through knowledge graph completion techniques."
            },
            "score": 3
        },
        {
            "id": "bfe88dc10e0acf548548ff3fe6e7f392a229ff7d",
            "paperId": "bfe88dc10e0acf548548ff3fe6e7f392a229ff7d",
            "title": "Zero-Shot Construction of Chinese Medical Knowledge Graph with GPT-3.5-turbo and GPT-4",
            "abstract": "Knowledge graphs have revolutionized the organization and retrieval of real-world knowledge, prompting interest in automatic NLP-based approaches for extracting medical knowledge from texts. However, the availability of high-quality Chinese medical knowledge remains limited, posing challenges for constructing Chinese medical knowledge graphs. As LLMs like ChatGPT show promise in zero-shot learning for many NLP downstream tasks, their potential on constructing Chinese medical knowledge graphs is still uncertain. In this study, we create a Chinese medical knowledge graph by manually annotating textual data and using ChatGPT to automatically generate the graph. We refine the results using filtering and mapping rules to align with our schema. The manually generated graph serves as the ground truth for evaluation, and we explore different methods to enhance its accuracy through knowledge graph completion techniques. As a result, we emphasize the potential of employing ChatGPT for automated knowledge graph construction within the Chinese medical domain. While ChatGPT successfully identifies a larger number of entities, further enhancements are required to improve its performance in extracting more qualified relations.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study creates a Chinese medical knowledge graph by manually annotating textual data and using ChatGPT to automatically generate the graph, and explores different methods to enhance its accuracy through knowledge graph completion techniques."
            },
            "score": 3
        },
        {
            "id": "47dc9942a83fee414c3944a0ca69987edc44795a",
            "paperId": "47dc9942a83fee414c3944a0ca69987edc44795a",
            "title": "Open-Set Fine-Grained Retrieval via Prompting Vision-Language Evaluator",
            "abstract": "Open-set fine-grained retrieval is an emerging challenge that requires an extra capability to retrieve unknown subcategories during evaluation. However, current works focus on close-set visual concepts, where all the subcategories are pre-defined, and make it hard to capture discriminative knowledge from unknown subcategories, consequently failing to handle unknown subcategories in open-world scenarios. In this work, we propose a novel Prompting vision-Language Evaluator (PLEor) framework based on the recently introduced contrastive language-image pretraining (CLIP) model, for open-set fine-grained retrieval. PLEor could leverage pre-trained CLIP model to infer the discrepancies encompassing both pre-defined and unknown subcategories, called category-specific discrepancies, and transfer them to the backbone network trained in the close-set scenarios. To make pre-trained CLIP model sensitive to category-specific discrepancies, we design a dual prompt scheme to learn a vision prompt specifying the categoryspecific discrepancies, and turn random vectors with category names in a text prompt into category-specific discrepancy descriptions. Moreover, a vision-language evaluator is proposed to semantically align the vision and text prompts based on CLIP model, and reinforce each other. In addition, we propose an open-set knowledge transfer to transfer the category-specific discrepancies into the backbone network using knowledge distillation mechanism. Quantitative and qualitative experiments show that our PLEor achieves promising performance on open-set fine-grained datasets.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel Prompting vision-Language Evaluator (PLEor) framework based on the recently introduced contrastive language-image pretraining (CLIP) model, and proposes an open-set knowledge transfer to transfer the category-specific discrepancies into the backbone network using knowledge distillation mechanism."
            },
            "score": 3
        },
        {
            "id": "197bea3a704d8a372dd46e1e1f5c6bff420d601f",
            "paperId": "197bea3a704d8a372dd46e1e1f5c6bff420d601f",
            "title": "DeepTrust: A Reliable Financial Knowledge Retrieval Framework For Explaining Extreme Pricing Anomalies",
            "abstract": "Extreme pricing anomalies may occur unexpectedly without a trivial cause, and equity traders typically experience a meticulous process to source disparate information and analyze its reliability before integrating it into the trusted knowledge base. We introduce DeepTrust, a reliable financial knowledge retrieval framework on Twitter to explain extreme price moves at speed, while ensuring data veracity using state-of-the-art NLP techniques. Our proposed framework consists of three modules, specialized for anomaly detection, information retrieval and reliability assessment. The workflow starts with identifying anomalous asset price changes using machine learning models trained with historical pricing data, and retrieving correlated unstructured data from Twitter using enhanced queries with dynamic search conditions. DeepTrust extrapolates information reliability from tweet features, traces of generative language model, argumentation structure, subjectivity and sentiment signals, and refine a concise collection of credible tweets for market insights. The framework is evaluated on two self-annotated financial anomalies, i.e., Twitter and Facebook stock price on 29 and 30 April 2021. The optimal setup outperforms the baseline classifier by 7.75% and 15.77% on F0.5-scores, and 10.55% and 18.88% on precision, respectively, proving its capability in screening unreliable information precisely. At the same time, information retrieval and reliability assessment modules are analyzed individually on their effectiveness and causes of limitations, with identified subjective and objective factors that influence the performance. As a collaborative project with Refinitiv, this framework paves a promising path towards building a scalable commercial solution that assists traders to reach investment decisions on pricing anomalies with authenticated knowledge from social media platforms in real-time.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "DeepTrust is introduced, a reliable financial knowledge retrieval framework on Twitter to explain extreme price moves at speed, while ensuring data veracity using state-of-the-art NLP techniques, and is evaluated on two self-annotated financial anomalies."
            },
            "score": 3
        },
        {
            "id": "d53945d4afb4528590d79e20de52883d29037e86",
            "paperId": "d53945d4afb4528590d79e20de52883d29037e86",
            "title": "FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo Embeddings",
            "abstract": "Logo embedding plays a crucial role in various e-commerce applications by facilitating image retrieval or recognition, such as intellectual property protection and product search. However, current methods treat logo embedding as a purely visual problem, which may limit their performance in real-world scenarios. A notable issue is that the textual knowledge embedded in logo images has not been adequately explored. Therefore, we propose a novel approach that leverages textual knowledge as an auxiliary to improve the robustness of logo embedding. The emerging Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in both visual and textual understanding and could become valuable visual assistants in understanding logo images. Inspired by this observation, our proposed method, FashionLOGO, aims to utilize MLLMs to enhance fashion logo embedding. We explore how MLLMs can improve logo embedding by prompting them to generate explicit textual knowledge through three types of prompts, including image OCR, brief captions, and detailed descriptions prompts, in a zero-shot setting. We adopt a cross-attention transformer to enable image embedding queries to learn supplementary knowledge from textual embeddings automatically. To reduce computational costs, we only use the image embedding model in the inference stage, similar to traditional inference pipelines. Our extensive experiments on three real-world datasets demonstrate that FashionLOGO learns generalized and robust logo embeddings, achieving state-of-the-art performance in all benchmark datasets. Furthermore, we conduct comprehensive ablation studies to demonstrate the performance improvements resulting from the introduction of MLLMs.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores how MLLMs can improve logo embedding by prompting them to generate explicit textual knowledge through three types of prompts, including image OCR, brief captions, and detailed descriptions prompts, in a zero-shot setting and adopt a cross-attention transformer to enable image embedding queries to learn supplementary knowledge from textual embeddings automatically."
            },
            "score": 2
        },
        {
            "id": "2b88c03324308dace8612307d5b64bb93b3b094d",
            "paperId": "2b88c03324308dace8612307d5b64bb93b3b094d",
            "title": "Enhancing smart contract security: Leveraging pre\u2010trained language models for advanced vulnerability detection",
            "abstract": "The burgeoning interest in decentralized applications (Dapps), spurred by advancements in blockchain technology, underscores the critical role of smart contracts. However, many Dapp users, often without deep knowledge of smart contracts, face financial risks due to hidden vulnerabilities. Traditional methods for detecting these vulnerabilities, including manual inspections and automated static analysis, are plagued by issues such as high rates of false positives and overlooked security flaws. To combat this, the article introduces an innovative approach using the bidirectional encoder representations from transformers (BERT)\u2010ATT\u2010BiLSTM model for identifying potential weaknesses in smart contracts. This method leverages the BERT pre\u2010trained model to discern semantic features from contract opcodes, which are then refined using a Bidirectional Long Short\u2010Term Memory Network (BiLSTM) and augmented by an attention mechanism that prioritizes critical features. The goal is to improve the model's generalization ability and enhance detection accuracy. Experiments on various publicly available smart contract datasets confirm the model's superior performance, outperforming previous methods in key metrics like accuracy, F1\u2010score, and recall. This research not only offers a powerful tool to bolster smart contract security, mitigating financial risks for average users, but also serves as a valuable reference for advancements in natural language processing and deep learning.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An innovative approach using the bidirectional encoder representations from transformers (BERT)\u2010ATT\u2010BiLSTM model for identifying potential weaknesses in smart contracts is introduced, outperforming previous methods in key metrics like accuracy, F1\u2010score, and recall."
            },
            "score": 2
        },
        {
            "id": "8fd72a8bc1ab8fa08259656fa617d1c861f27239",
            "paperId": "8fd72a8bc1ab8fa08259656fa617d1c861f27239",
            "title": "Interpreting Art by Leveraging Pre-Trained Models",
            "abstract": "In many domains, so-called foundation models were recently proposed. These models are trained on immense amounts of data resulting in impressive performances on various downstream tasks and benchmarks. Later works focus on leveraging this pre-trained knowledge by combining these models. To reduce data and compute requirements, we utilize and combine foundation models in two ways. First, we use language and vision models to extract and generate a challenging language vision task in the form of artwork interpretation pairs. Second, we combine and fine-tune CLIP as well as GPT-2 to reduce compute requirements for training interpretation models. We perform a qualitative and quantitative analysis of our data and conclude that generating artwork leads to improvements in visual-text alignment and, therefore, to more proficient interpretation models1. Our approach addresses how to leverage and combine pre-trained models to tackle tasks where existing data is scarce or difficult to obtain.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work uses language and vision models to extract and generate a challenging language vision task in the form of artwork interpretation pairs and combines and fine-tune CLIP as well as GPT-2 to reduce compute requirements for training interpretation models."
            },
            "score": 2
        },
        {
            "id": "94a96f64bd93ad91642fa04da09bb709a26ac277",
            "paperId": "94a96f64bd93ad91642fa04da09bb709a26ac277",
            "title": "P2P: Tuning Pre-trained Image Models for Point Cloud Analysis with Point-to-Pixel Prompting",
            "abstract": "Nowadays, pre-training big models on large-scale datasets has become a crucial topic in deep learning. The pre-trained models with high representation ability and transferability achieve a great success and dominate many downstream tasks in natural language processing and 2D vision. However, it is non-trivial to promote such a pretraining-tuning paradigm to the 3D vision, given the limited training data that are relatively inconvenient to collect. In this paper, we provide a new perspective of leveraging pre-trained 2D knowledge in 3D domain to tackle this problem, tuning pre-trained image models with the novel Point-to-Pixel prompting for point cloud analysis at a minor parameter cost. Following the principle of prompting engineering, we transform point clouds into colorful images with geometry-preserved projection and geometry-aware coloring to adapt to pre-trained image models, whose weights are kept frozen during the end-to-end optimization of point cloud analysis tasks. We conduct extensive experiments to demonstrate that cooperating with our proposed Point-to-Pixel Prompting, better pre-trained image model will lead to consistently better performance in 3D vision. Enjoying prosperous development from image pre-training field, our method attains 89.3% accuracy on the hardest setting of ScanObjectNN, surpassing conventional point cloud models with much fewer trainable parameters. Our framework also exhibits very competitive performance on ModelNet classification and ShapeNet Part Segmentation. Code is available at https://github.com/wangzy22/P2P.",
            "year": 2022,
            "citationCount": 49,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper tuning pre-trained image models with the novel Point-to-Pixel prompting for point cloud analysis at a minor parameter cost achieves 89.3% accuracy on the hardest setting of ScanObjectNN, surpassing conventional point cloud models with much fewer trainable parameters."
            },
            "score": 2
        },
        {
            "id": "d16f8d624ab16c8bb35dde676f522b66a771d271",
            "paperId": "d16f8d624ab16c8bb35dde676f522b66a771d271",
            "title": "Large Language Models are Null-Shot Learners",
            "abstract": "This paper presents null-shot prompting. Null-shot prompting exploits hallucination in large language models (LLMs) by instructing LLMs to utilize information from the\"Examples\"section that never exists within the provided context to perform a task. While reducing hallucination is crucial and non-negligible for daily and critical uses of LLMs, we propose that in the current landscape in which these LLMs still hallucinate, it is possible, in fact, to exploit hallucination to increase performance in performing tasks compared to standard zero-shot prompting. Experiments with eight LLMs show improvements in performance across the majority of eight datasets, including reading comprehension, arithmetic reasoning, and closed-book question answering. The observed inconsistency in increased relative performance across the LLMs also potentially indicates a different degree of inherent hallucination in each model. These differences show that it is possible to utilize null-shot prompting as a way to detect degrees of hallucination in LLMs using existing benchmarking datasets. We also perform ablation studies, including experimenting with a modified version of null-shot prompting that incorporates ideas from zero-shot chain-of-thought prompting, which shows different trends of results.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experiments with eight LLMs show improvements in performance across the majority of eight datasets, including reading comprehension, arithmetic reasoning, and closed-book question answering, and differences show that it is possible to utilize null-shot prompting as a way to detect degrees of hallucination in LLMs using existing benchmarking datasets."
            },
            "score": 2
        }
    ],
    "novelty": "yes"
}