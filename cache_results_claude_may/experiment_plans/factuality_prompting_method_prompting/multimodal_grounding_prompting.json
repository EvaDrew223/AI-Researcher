{
    "topic_description": "novel prompting methods that can improve factuality and reduce hallucination of large language models",
    "idea_name": "Multimodal Grounding Prompting",
    "raw_idea": {
        "Problem": "Language models can generate statements that are inconsistent with visual or multimodal evidence, e.g. image captioning models can hallucinate objects not actually present in an image.",
        "Existing Methods": "Some methods use visual retrieval or visual QA models to ground text generation in visual evidence.",
        "Motivation": "Language models can be prompted to reason about visual inputs, and to cross-reference textual claims against visual evidence. This can help ground generation and reduce hallucination.",
        "Proposed Method": "The procedure is: 1) Given a query and an image, prompt the model to generate a claim relevant to the image. 2) Prompt the model to assess whether the generated claim is consistent with the visual evidence, and to identify any inconsistencies. 3) If inconsistencies are found, prompt the model to revise the claim to be more consistent with the image. 4) Repeat steps 2-3 until the claim is fully consistent.",
        "Experiment Plan": "Evaluate on visual storytelling and visual QA datasets. Compare to baselines like direct prompting and retrieve-and-generate. Metrics include consistency scores between images and generated text, and human evaluation of factual alignment."
    },
    "full_experiment_plan": {
        "Title": "Grounding Language Models in Visual Evidence to Reduce Hallucination",
        "Problem Statement": "Language models can generate statements that are inconsistent with visual or multimodal evidence, e.g. image captioning models can hallucinate objects not actually present in an image. This leads to factually incorrect outputs that are not properly grounded in the given evidence.",
        "Motivation": "Existing methods attempt to ground text generation by using visual retrieval or visual question answering models to incorporate visual information. However, these methods rely on external models and datasets. We propose a simpler approach that directly prompts language models to reason about the consistency between textual claims and visual evidence. Large language models have shown the ability to reason about and incorporate instructions and task descriptions. By explicitly prompting them to cross-reference claims against visual evidence and revise inconsistent statements, we aim to improve the factual grounding of the generated text while relying solely on the language model itself.",
        "Proposed Method": "Our proposed procedure, termed visual grounding prompts, consists of the following steps:\n1. Given a query and an image, prompt the language model to generate a claim relevant to the image.\n2. Prompt the model to assess whether the generated claim is consistent with the visual evidence present in the image, and to identify any inconsistencies.\n3. If inconsistencies are found, prompt the model to revise the claim to be more consistent with the image.\n4. Repeat steps 2-3 until the claim is fully consistent with the visual evidence or a maximum number of iterations is reached.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "Evaluate the proposed method on two types of datasets:\n1. Visual storytelling datasets like VIST and VisualCOMET, where the goal is to generate a coherent story or description based on a sequence of images. These datasets allow testing the factual alignment between generated text and images.\n2. Visual question answering datasets like VQAv2 and GQA, where the goal is to answer questions about an image. These datasets allow evaluating the ability to generate accurate answers grounded in visual evidence.",
            "Step 2: Construct Prompts": "We compare the proposed method to two baselines:\n1. Direct prompting: Given an image and a query, directly prompt the model to generate a response without any additional instructions.\n2. Retrieve-and-generate: Use a separate image encoder to retrieve relevant images from a database, and condition the language model on both the query and the retrieved images to generate a response.\nFor the proposed visual grounding prompts, we construct the following prompts for each step:\n1. Generate claim: \"Given the image: [image] and the query: [query], generate a claim that is relevant to the image.\"\n2. Assess consistency: \"Given the image: [image] and the generated claim: [claim], assess whether the claim is consistent with the visual evidence in the image. Identify any inconsistencies.\"\n3. Revise claim: \"Given the image: [image], the original claim: [claim], and the identified inconsistencies: [inconsistencies], revise the claim to be more consistent with the image.\"",
            "Step 3: Select Models": "Evaluate the proposed method using GPT-3.5 (text-davinci-002) and GPT-4 models via the OpenAI API. For the retrieve-and-generate baseline, use a pre-trained CLIP model to encode the images.",
            "Step 4: Collect Results": "For each example in the evaluation datasets, generate a response using the baseline methods and the proposed visual grounding prompts. For the proposed method, record the initial claim, the identified inconsistencies at each iteration, and the final revised claim.",
            "Step 5: Analyze Results": "Compute the following metrics:\n1. Consistency score: Use a pre-trained visual-language model (e.g., CLIP) to compute the cosine similarity between the generated text and the image. Higher scores indicate better alignment between the text and visual evidence.\n2. Factual accuracy: For visual QA datasets, compute the accuracy of the generated answers using the provided ground-truth answers.\n3. Human evaluation: Conduct a human evaluation study where raters assess the factual alignment between the generated text and the corresponding images on a Likert scale (e.g., 1-5). Also, ask raters to identify any inconsistencies or hallucinated entities in the generated text.\nCompare the performance of the proposed visual grounding prompts to the baselines across these metrics. Analyze the types of inconsistencies identified by the model and how well it is able to revise the claims based on visual evidence."
        },
        "Test Case Examples": {
            "Test Case 1": {
                "Image": "An image of a person riding a bicycle on a city street.",
                "Query": "What is the person doing?",
                "Baseline Prompt (Direct)": "The person is riding a motorcycle on a city street.",
                "Baseline Prompt (Retrieve-and-Generate)": "The person is riding a bicycle on a busy city street.",
                "Visual Grounding Prompt (Step 1)": "A person is riding a motorcycle on a city street.",
                "Visual Grounding Prompt (Step 2)": "The claim mentions a motorcycle, but the image clearly shows a bicycle. This is inconsistent.",
                "Visual Grounding Prompt (Step 3)": "A person is riding a bicycle on a city street.",
                "Explanation": "The direct prompting baseline hallucinates a motorcycle which is not present in the image. The retrieve-and-generate baseline produces a more factual output by incorporating visual information from similar retrieved images. However, the proposed visual grounding prompts explicitly identify the inconsistency between the claim and the image, and revise the claim to mention the bicycle, which is more factually accurate."
            },
            "Test Case 2": {
                "Image": "An image of a group of people sitting around a table eating pizza.",
                "Query": "What are the people eating?",
                "Baseline Prompt (Direct)": "The people are eating hamburgers and fries.",
                "Baseline Prompt (Retrieve-and-Generate)": "The people are eating pizza.",
                "Visual Grounding Prompt (Step 1)": "The people are eating hamburgers.",
                "Visual Grounding Prompt (Step 2)": "The claim mentions hamburgers, but the image shows the people eating pizza. This is inconsistent.",
                "Visual Grounding Prompt (Step 3)": "The people are eating pizza.",
                "Explanation": "The direct prompting baseline hallucinates hamburgers and fries which are not present in the image. The retrieve-and-generate baseline produces the correct answer by leveraging visual information. The proposed visual grounding prompts identify the inconsistency between the claim and the image, and revise the claim to accurately mention pizza."
            }
        },
        "Fallback Plan": "If the proposed visual grounding prompts do not show significant improvements over the baselines, we can conduct the following additional analyses:\n1. Analyze the quality of the identified inconsistencies. Check if the model is able to accurately detect inconsistencies between the claims and the visual evidence. If the inconsistencies are not being properly identified, explore alternative prompts or techniques for assessing consistency.\n2. Analyze the quality of the revised claims. Check if the model is making meaningful revisions to the claims based on the identified inconsistencies. If the revisions are not improving factual alignment, explore alternative prompts or strategies for guiding the model to make more effective revisions.\n3. Conduct an error analysis to identify common failure modes of the proposed method. Categorize the types of errors (e.g., object hallucination, attribute hallucination, relation hallucination) and analyze their frequency. This can provide insights into the limitations of the method and guide future improvements.\n4. Explore variations of the proposed method, such as using different visual encoders, incorporating additional visual reasoning steps, or using alternative prompting strategies.\nIf the proposed method still does not yield promising results after these additional analyses, we can pivot the project to focus on understanding the limitations of language models in grounding text generation in visual evidence. The analyses mentioned above can form the basis for an insightful study on the challenges and opportunities in this area."
    },
    "novelty_queries": [
        "KeywordQuery(\"language models visual grounding\")",
        "KeywordQuery(\"language models consistency visual evidence\")",
        "KeywordQuery(\"language models hallucination visual question answering\")",
        "KeywordQuery(\"Multimodal Grounding Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "28fbbf98bac1bb941162df553ca034d600cb59a6",
            "paperId": "28fbbf98bac1bb941162df553ca034d600cb59a6",
            "title": "Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models",
            "abstract": "An increasing number of vision-language tasks can be handled with little to no training, i.e., in a zero and few-shot manner, by marrying large language models (LLMs) to vision encoders, resulting in large vision-language models (LVLMs). While this has huge upsides, such as not requiring training data or custom architectures, how an input is presented to an LVLM can have a major impact on zero-shot model performance. In particular, inputs phrased in an underspecified way can result in incorrect answers due to factors like missing visual information, complex implicit reasoning, or linguistic ambiguity. Therefore, adding visually-grounded information to the input as a preemptive clarification should improve model performance by reducing underspecification, e.g., by localizing objects and disambiguating references. Similarly, in the VQA setting, changing the way questions are framed can make them easier for models to answer. To this end, we present Rephrase, Augment and Reason (RepARe), a gradient-free framework that extracts salient details about the image using the underlying LVLM as a captioner and reasoner, in order to propose modifications to the original question. We then use the LVLM's confidence over a generated answer as an unsupervised scoring function to select the rephrased question most likely to improve zero-shot performance. Focusing on three visual question answering tasks, we show that RepARe can result in a 3.85% (absolute) increase in zero-shot accuracy on VQAv2, 6.41%, and 7.94% points increase on A-OKVQA, and VizWiz respectively. Additionally, we find that using gold answers for oracle question candidate selection achieves a substantial gain in VQA accuracy by up to 14.41%. Through extensive analysis, we demonstrate that outputs from RepARe increase syntactic complexity, and effectively utilize vision-language interaction and the frozen LLM.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Rephrase, Augment and Reason (RepARe), a gradient-free framework that extracts salient details about the image using the underlying LVLM as a captioner and reasoner, in order to propose modifications to the original question, is presented."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to reduce hallucination in language models by prompting them to ground their outputs in visual evidence and iteratively revise inconsistent claims. The paper proposes a framework called RepARe that extracts salient details about the image using a vision-language model to rephrase questions and improve zero-shot performance on visual question answering tasks.\n\nWhile both the project proposal and the paper focus on improving the factual accuracy of vision-language models, their approaches differ. The project proposal suggests using prompts to guide the language model to assess and revise its own outputs based on visual evidence, while the paper proposes using the vision-language model to rephrase input questions to make them easier to answer accurately.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "1cfb7fba7194860e8b8818eb5e87e0a8e14e518a",
            "paperId": "1cfb7fba7194860e8b8818eb5e87e0a8e14e518a",
            "title": "ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling",
            "abstract": "By combining natural language understanding, generation capabilities, and breadth of knowledge of large language models with image perception, recent large vision language models (LVLMs) have shown unprecedented visual reasoning capabilities. However, the generated text often suffers from inaccurate grounding in the visual input, resulting in errors such as hallucination of nonexistent scene elements, missing significant parts of the scene, and inferring incorrect attributes of and relationships between objects. To address these issues, we introduce a novel framework, ViGoR(Visual Grounding Through Fine-Grained Reward Modeling) that utilizes fine-grained reward modeling to significantly enhance the visual grounding of LVLMs over pre-trained baselines. This improvement is efficiently achieved using much cheaper human evaluations instead of full supervisions, as well as automated methods. We show the effectiveness of our approach through a variety of evaluation methods and benchmarks. Additionally, we plan to release our human annotation comprising approximately 16,000 images and generated text pairs with fine-grained evaluations to contribute to related research in the community.",
            "year": 2024,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel framework, ViGoR (Visual Grounding Through Fine-Grained Reward Modeling) that utilizes fine-grained reward modeling to significantly enhance the visual grounding of LVLMs over pre-trained baselines is introduced."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the factual grounding of language models in visual evidence by prompting them to assess the consistency between generated claims and images, and revise inconsistent claims. The paper abstract proposes ViGoR, a framework that uses fine-grained reward modeling to enhance the visual grounding of large vision language models and reduce errors such as hallucination and incorrect inferences.\n\nBoth the project proposal and the paper abstract focus on improving the visual grounding of language models to reduce hallucination and inconsistencies between generated text and visual evidence. However, the project proposal uses prompting to guide the language model to assess and revise claims, while the paper abstract uses fine-grained reward modeling.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "0be923cad65522c921dfaa04e517b051593121ce",
            "paperId": "0be923cad65522c921dfaa04e517b051593121ce",
            "title": "Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models",
            "abstract": "We introduce Groma, a Multimodal Large Language Model (MLLM) with grounded and fine-grained visual perception ability. Beyond holistic image understanding, Groma is adept at region-level tasks such as region captioning and visual grounding. Such capabilities are built upon a localized visual tokenization mechanism, where an image input is decomposed into regions of interest and subsequently encoded into region tokens. By integrating region tokens into user instructions and model responses, we seamlessly enable Groma to understand user-specified region inputs and ground its textual output to images. Besides, to enhance the grounded chat ability of Groma, we curate a visually grounded instruction dataset by leveraging the powerful GPT-4V and visual prompting techniques. Compared with MLLMs that rely on the language model or external module for localization, Groma consistently demonstrates superior performances in standard referring and grounding benchmarks, highlighting the advantages of embedding localization into image tokenization. Project page: https://groma-mllm.github.io/.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces Groma, a Multimodal Large Language Model (MLLM) with grounded and fine-grained visual perception ability, and curates a visually grounded instruction dataset by leveraging the powerful GPT-4V and visual prompting techniques."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the factual grounding of language models in visual evidence by prompting them to assess and revise claims based on the consistency with the given image. The paper proposes Groma, a multimodal language model that uses localized visual tokenization to enable region-level visual understanding and grounding.\n\nThe project focuses on reducing hallucination in language models by explicitly prompting them to reason about the consistency between text and visual evidence. In contrast, the paper introduces a new model architecture that incorporates region-level visual tokens to enhance visual grounding capabilities.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "6ae4705139494fcb6b790b6dd6c4225b40ee40f8",
            "paperId": "6ae4705139494fcb6b790b6dd6c4225b40ee40f8",
            "title": "GLaMM: Pixel Grounding Large Multimodal Model",
            "abstract": "Large Multimodal Models (LMMs) extend Large Language Models to the vision domain. Initial LMMs used holistic images and text prompts to generate ungrounded textual responses. Recently, region-level LMMs have been used to generate visually grounded responses. However, they are limited to only referring to a single object category at a time, require users to specify the regions, or cannot offer dense pixel-wise object grounding. In this work, we present Grounding LMM (GLaMM), the first model that can generate natural language responses seamlessly intertwined with corresponding object segmentation masks. GLaMM not only grounds objects appearing in the conversations but is flexible enough to accept both textual and optional visual prompts (region of interest) as input. This empowers users to interact with the model at various levels of granularity, both in textual and visual domains. Due to the lack of standard benchmarks for the novel setting of visually Grounded Conversation Generation (GCG), we introduce a comprehensive evaluation protocol with our curated grounded conversations. Our proposed GCG task requires densely grounded concepts in natural scenes at a large-scale. To this end, we propose a densely annotated Grounding-anything Dataset (GranD) using our proposed automated annotation pipeline that encompasses 7.5M unique concepts grounded in a total of 810M regions available with segmentation masks. Besides GCG, GLaMM also performs effectively on several downstream tasks, e.g., referring expression segmentation, image and region-level captioning and vision-language conversations.",
            "year": 2023,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents Grounding LMM (GLaMM), the first model that can generate natural language responses seamlessly intertwined with corresponding object segmentation masks and is flexible enough to accept both textual and optional visual prompts (region of interest) as input."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the factual grounding of language models in visual evidence by prompting them to assess and revise generated claims for consistency with the given image. The paper proposes GLaMM, a model that generates natural language responses intertwined with corresponding object segmentation masks, grounding objects appearing in the conversations.\n\nThe project focuses on reducing hallucination in language models by explicitly prompting them to reason about the consistency between textual claims and visual evidence. In contrast, the paper introduces a model that generates grounded conversations by incorporating object segmentation masks, allowing for dense pixel-wise object grounding.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "9f3bc73b27d83a15baf81a4221322c04c522ff2b",
            "paperId": "9f3bc73b27d83a15baf81a4221322c04c522ff2b",
            "title": "Plug-and-Play Grounding of Reasoning in Multimodal Large Language Models",
            "abstract": "The surge of Multimodal Large Language Models (MLLMs), given their prominent emergent capabilities in instruction following and reasoning, has greatly advanced the field of visual reasoning. However, constrained by their non-lossless image tokenization, most MLLMs fall short of comprehensively capturing details of text and objects, especially in high-resolution images. To address this, we propose P2G, a novel framework for plug-and-play grounding of reasoning in MLLMs. Specifically, P2G exploits the tool-usage potential of MLLMs to employ expert agents to achieve on-the-fly grounding to critical visual and textual objects of image, thus achieving deliberate reasoning via multimodal prompting. We further create P2GB, a benchmark aimed at assessing MLLMs' ability to understand inter-object relationships and text in challenging high-resolution images. Comprehensive experiments on visual reasoning tasks demonstrate the superiority of P2G. Noteworthy, P2G achieved comparable performance with GPT-4V on P2GB, with a 7B backbone. Our work highlights the potential of plug-and-play grounding of reasoning and opens up a promising alternative beyond model scaling.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "P2G exploits the tool-usage potential of MLLMs to employ expert agents to achieve on-the-fly grounding to critical visual and textual objects of image, thus achieving deliberate reasoning via multimodal prompting."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the factual grounding of language models in visual evidence by prompting them to assess and revise claims based on the consistency with the given image. The paper proposes a framework for plug-and-play grounding of reasoning in multimodal language models by employing expert agents to achieve on-the-fly grounding to visual and textual objects in high-resolution images.\n\nWhile both works focus on grounding language models in visual information, the project proposal emphasizes prompting language models to self-assess and revise generated claims for consistency with visual evidence, whereas the paper proposes using expert agents to ground reasoning in multimodal language models for high-resolution images.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "1c5d887ec6c4d57bfee29fa632a96a75129a50ce",
            "paperId": "1c5d887ec6c4d57bfee29fa632a96a75129a50ce",
            "title": "LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models",
            "abstract": "With the recent significant advancements in large multi-modal models (LMMs), the importance of their grounding capability in visual chat is increasingly recognized. Despite recent efforts to enable LMMs to support grounding, their capabilities for grounding and chat are usually separate, and their chat performance drops dramatically when asked to ground. The problem is the lack of a dataset for grounded visual chat (GVC). Existing grounding datasets only contain short captions. To address this issue, we have created GVC data that allows for the combination of grounding and chat capabilities. To better evaluate the GVC capabilities, we have introduced a benchmark called Grounding-Bench. Additionally, we have proposed a model design that can support GVC and various types of visual prompts by connecting segmentation models with language models. Experimental results demonstrate that our model outperforms other LMMs on Grounding-Bench. Furthermore, our model achieves competitive performance on classic grounding benchmarks like RefCOCO/+/g and Flickr30K Entities. Our code will be released at https://github.com/UX-Decoder/LLaVA-Grounding .",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A model design is proposed that can support GVC and various types of visual prompts by connecting segmentation models with language models and achieves competitive performance on classic grounding benchmarks like RefCOCO/+/g and Flickr30K Entities."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the factual grounding of language models in visual evidence by prompting them to assess and revise generated claims for consistency with the given image. The paper focuses on creating a grounded visual chat dataset and benchmark to evaluate the combined grounding and chat capabilities of large multimodal models.\n\nProject proposal: Improving factual grounding of language models in visual evidence through prompting.\nPaper: Creating a grounded visual chat dataset and benchmark to evaluate combined grounding and chat capabilities.\n\nThe project proposal and paper have different research problems and approaches. The proposal focuses on prompting language models to improve factual grounding, while the paper creates a new dataset and benchmark for evaluating grounded visual chat.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "3b6179c293df29e31d31cea46476f104ab6950f2",
            "paperId": "3b6179c293df29e31d31cea46476f104ab6950f2",
            "title": "Kosmos-2: Grounding Multimodal Large Language Models to the World",
            "abstract": "We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generation. This work lays out the foundation for the development of Embodiment AI and sheds light on the big convergence of language, multimodal perception, action, and world modeling, which is a key step toward artificial general intelligence. Code and pretrained models are available at https://aka.ms/kosmos-2.",
            "year": 2023,
            "citationCount": 271,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Kosmos-2, a Multimodal Large Language Model (MLLM), is introduced, enabling new capabilities of perceiving object descriptions and grounding text to the visual world and sheds light on the big convergence of language, multimodal perception, action, and world modeling."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to reduce hallucination in language models by prompting them to ground their outputs in visual evidence and iteratively revise inconsistent claims. The paper introduces Kosmos-2, a multimodal language model that can ground text to the visual world by perceiving object descriptions and bounding boxes.\n\nWhile both works involve grounding language models in visual information, the project proposal focuses on using prompts to guide language models to assess and revise claims for consistency with visual evidence, without modifying the model itself. In contrast, the paper proposes a new multimodal language model architecture that is trained on grounded image-text pairs to inherently incorporate visual grounding capabilities.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "280353fd7a7a3e49c415c443e1b7ccf7de9c2b4e",
            "paperId": "280353fd7a7a3e49c415c443e1b7ccf7de9c2b4e",
            "title": "Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models",
            "abstract": "Vision-language models (VLMs) have recently demonstrated strong efficacy as visual assistants that can parse natural queries about the visual content and generate human-like outputs. In this work, we explore the ability of these models to demonstrate human-like reasoning based on the perceived information. To address a crucial concern regarding the extent to which their reasoning capabilities are fully consistent and grounded, we also measure the reasoning consistency of these models. We achieve this by proposing a chain-of-thought (CoT) based consistency measure. However, such an evaluation requires a benchmark that encompasses both high-level inference and detailed reasoning chains, which is costly. We tackle this challenge by proposing a LLM-Human-in-the-Loop pipeline, which notably reduces cost while simultaneously ensuring the generation of a high-quality dataset. Based on this pipeline and the existing coarse-grained annotated dataset, we build the CURE benchmark to measure both the zero-shot reasoning performance and consistency of VLMs. We evaluate existing state-of-the-art VLMs, and find that even the best-performing model is unable to demonstrate strong visual reasoning capabilities and consistency, indicating that substantial efforts are required to enable VLMs to perform visual reasoning as systematically and consistently as humans. As an early step, we propose a two-stage training framework aimed at improving both the reasoning performance and consistency of VLMs. The first stage involves employing supervised fine-tuning of VLMs using step-by-step reasoning samples automatically generated by LLMs. In the second stage, we further augment the training process by incorporating feedback provided by LLMs to produce reasoning chains that are highly consistent and grounded. We empirically highlight the effectiveness of our framework in both reasoning performance and consistency.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a LLM-Human-in-the-Loop pipeline, which notably reduces cost while simultaneously ensuring the generation of a high-quality dataset and builds the CURE benchmark, which is used to measure both the zero-shot reasoning performance and consistency of VLMs."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve the factual grounding of language models in visual evidence by prompting them to assess and revise generated claims for consistency with the given image. The paper focuses on measuring and improving the reasoning capabilities and consistency of vision-language models using chain-of-thought prompting and a two-stage training framework.\n\nWhile both works involve prompting models to improve certain aspects of their outputs (factual grounding and reasoning consistency), the project proposal specifically targets reducing hallucination in language models using only the model itself, while the paper studies vision-language models and proposes a training framework using external feedback from language models.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "658cd67a91da86cf451e6f1b015f762b56015172",
            "paperId": "658cd67a91da86cf451e6f1b015f762b56015172",
            "title": "Detecting and Preventing Hallucinations in Large Vision Language Models",
            "abstract": "Instruction tuned Large Vision Language Models (LVLMs) have significantly advanced in generalizing across a diverse set of multi-modal tasks, especially for Visual Question Answering (VQA). However, generating detailed responses that are visually grounded is still a challenging task for these models. We find that even the current state-of-the-art LVLMs (InstructBLIP) still contain a staggering 30 percent of the hallucinatory text in the form of non-existent objects, unfaithful descriptions, and inaccurate relationships. To address this, we introduce M-HalDetect, a Multimodal Hallucination Detection Dataset that can be used to train and benchmark models for hallucination detection and prevention. M-HalDetect consists of 16k fine-grained annotations on VQA examples, making it the first comprehensive multi-modal hallucination detection dataset for detailed image descriptions. Unlike previous work that only consider object hallucination, we additionally annotate both entity descriptions and relationships that are unfaithful. To demonstrate the potential of this dataset for hallucination prevention, we optimize InstructBLIP through our novel Fine-grained Direct Preference Optimization (FDPO). We also train fine-grained multi-modal reward models from InstructBLIP and evaluate their effectiveness with best-of-n rejection sampling (RS). We perform human evaluation on both FDPO and rejection sampling, and find that they reduce hallucination rates in InstructBLIP by 41% and 55% respectively. We also find that our reward model generalizes to other multi-modal models, reducing hallucinations in LLaVA and mPLUG-OWL by 15% and 57% respectively, and has strong correlation with human evaluated accuracy scores. The dataset is available at https://github.com/hendryx-scale/mhal-detect.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "M-HalDetect is introduced, a Multimodal Hallucination Detection Dataset that can be used to train and benchmark models for hallucination detection and prevention, and is the first comprehensive multi-modal hallucination detection dataset for detailed image descriptions."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is reducing hallucination in language models when generating text based on visual evidence, and the proposed approach is to use prompts that explicitly ask the model to assess and revise generated claims for consistency with the given image.\n\nThe research problem in the paper is also detecting and reducing hallucination in vision-language models, but the approach is different. The paper proposes creating a dataset for hallucination detection and using it to train models or perform rejection sampling, rather than using prompts.\n\nIn summary, while both works aim to reduce hallucination in vision-language models, the proposed approaches are quite different (prompting vs. dataset creation and fine-tuning/rejection sampling).\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "032cbe48a88325c50ad9bc73e8d5f000fb84ffe7",
            "paperId": "032cbe48a88325c50ad9bc73e8d5f000fb84ffe7",
            "title": "Visual Hallucination: Definition, Quantification, and Prescriptive Remediations",
            "abstract": "The troubling rise of hallucination presents perhaps the most significant impediment to the advancement of responsible AI. In recent times, considerable research has focused on detecting and mitigating hallucination in Large Language Models (LLMs). However, it's worth noting that hallucination is also quite prevalent in Vision-Language models (VLMs). In this paper, we offer a fine-grained discourse on profiling VLM hallucination based on two tasks: i) image captioning, and ii) Visual Question Answering (VQA). We delineate eight fine-grained orientations of visual hallucination: i) Contextual Guessing, ii) Identity Incongruity, iii) Geographical Erratum, iv) Visual Illusion, v) Gender Anomaly, vi) VLM as Classifier, vii) Wrong Reading, and viii) Numeric Discrepancy. We curate Visual HallucInation eLiciTation (VHILT), a publicly available dataset comprising 2,000 samples generated using eight VLMs across two tasks of captioning and VQA along with human annotations for the categories as mentioned earlier.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper delineates eight fine-grained orientations of visual hallucination and offers a fine-grained discourse on profiling VLM hallucination based on two tasks: i) image captioning, and ii) Visual Question Answering (VQA)."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is reducing hallucination in language models when generating text based on visual evidence, and the proposed approach is to use prompts that explicitly ask the model to assess the consistency between generated claims and visual evidence, and revise the claims if inconsistencies are found.\n\nThe research problem in the paper is defining and quantifying visual hallucination in vision-language models, and the proposed approach is to create a dataset of samples generated by vision-language models on image captioning and visual question answering tasks, annotated with eight fine-grained categories of visual hallucination.\n\nThe proposal focuses on reducing hallucination in language models, while the paper focuses on defining and quantifying hallucination in vision-language models. The proposal's approach is to use prompts to improve consistency, while the paper's approach is to create an annotated dataset for analysis.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "e4bbd1cfa8e53e529b5e8310838a2e455985137d",
            "paperId": "e4bbd1cfa8e53e529b5e8310838a2e455985137d",
            "title": "Grounding-Prompter: Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long Videos",
            "abstract": "Temporal Sentence Grounding (TSG), which aims to localize moments from videos based on the given natural language queries, has attracted widespread attention. Existing works are mainly designed for short videos, failing to handle TSG in long videos, which poses two challenges: i) complicated contexts in long videos require temporal reasoning over longer moment sequences, and ii) multiple modalities including textual speech with rich information require special designs for content understanding in long videos. To tackle these challenges, in this work we propose a Grounding-Prompter method, which is capable of conducting TSG in long videos through prompting LLM with multimodal information. In detail, we first transform the TSG task and its multimodal inputs including speech and visual, into compressed task textualization. Furthermore, to enhance temporal reasoning under complicated contexts, a Boundary-Perceptive Prompting strategy is proposed, which contains three folds: i) we design a novel Multiscale Denoising Chain-of-Thought (CoT) to combine global and local semantics with noise filtering step by step, ii) we set up validity principles capable of constraining LLM to generate reasonable predictions following specific formats, and iii) we introduce one-shot In-Context-Learning (ICL) to boost reasoning through imitation, enhancing LLM in TSG task understanding. Experiments demonstrate the state-of-the-art performance of our Grounding-Prompter method, revealing the benefits of prompting LLM with multimodal information for TSG in long videos.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a Grounding-Prompter method, capable of conducting TSG in long videos through prompting LLM with multimodal information, and introduces one-shot In-Context-Learning to boost reasoning through imitation, enhancing LLM in TSG task understanding."
            },
            "score": 6
        },
        {
            "id": "96a6df2b4aa50cfbd8984933e9c66b0763fc08a6",
            "paperId": "96a6df2b4aa50cfbd8984933e9c66b0763fc08a6",
            "title": "Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V",
            "abstract": "We present Set-of-Mark (SoM) , a new visual prompting method, to unleash the visual grounding abilities of large multimodal models (LMMs), such as GPT-4V. As illustrated in Fig. 1 (right), we employ off-the-shelf interactive segmentation models, such as SAM, to partition an image into regions at different levels of granularity",
            "year": 2023,
            "citationCount": 34,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 6
        },
        {
            "id": "4716f448e9dccb07e8c2b983a957da80834549af",
            "paperId": "4716f448e9dccb07e8c2b983a957da80834549af",
            "title": "MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis",
            "abstract": "Chest X-ray images are commonly used for predicting acute and chronic cardiopulmonary conditions, but efforts to integrate them with structured clinical data face challenges due to incomplete electronic health records (EHR). This paper introduces MedPromptX, the first model to integrate multimodal large language models (MLLMs), few-shot prompting (FP) and visual grounding (VG) to combine imagery with EHR data for chest X-ray diagnosis. A pre-trained MLLM is utilized to complement the missing EHR information, providing a comprehensive understanding of patients' medical history. Additionally, FP reduces the necessity for extensive training of MLLMs while effectively tackling the issue of hallucination. Nevertheless, the process of determining the optimal number of few-shot examples and selecting high-quality candidates can be burdensome, yet it profoundly influences model performance. Hence, we propose a new technique that dynamically refines few-shot data for real-time adjustment to new patient scenarios. Moreover, VG aids in focusing the model's attention on relevant regions of interest in X-ray images, enhancing the identification of abnormalities. We release MedPromptX-VQA, a new in-context visual question answering dataset encompassing interleaved image and EHR data derived from MIMIC-IV and MIMIC-CXR databases. Results demonstrate the SOTA performance of MedPromptX, achieving an 11% improvement in F1-score compared to the baselines. Code and data are available at https://github.com/BioMedIA-MBZUAI/MedPromptX",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces MedPromptX, the first model to integrate multimodal large language models, few-shot prompting (FP) and visual grounding (VG) to combine imagery with EHR data for chest X-ray diagnosis and releases MedPromptX-VQA, a new in-context visual question answering dataset encompassing interleaved image and EHR data derived from MIMIC-IV and MIMIC-CXR databases."
            },
            "score": 6
        },
        {
            "id": "0d043b0bd4a981f7e6135a79dac6d71a809af8cb",
            "paperId": "0d043b0bd4a981f7e6135a79dac6d71a809af8cb",
            "title": "Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings",
            "abstract": "Recent advances in large language models elicit reasoning in a chain-of-thought that allows models to decompose problems in a human-like fashion. Though this paradigm improves multi-step reasoning ability in language models, it is limited by being unimodal and applied mainly to question-answering tasks. We claim that incorporating visual augmentation into reasoning is essential, especially for complex, imaginative tasks. Consequently, we introduce VCoT, a novel method that leverages chain-of-thought prompting with vision-language grounding to recursively bridge the logical gaps within sequential data. Our method uses visual guidance to generate synthetic multimodal infillings that add consistent and novel information to reduce the logical gaps for downstream tasks that can benefit from temporal reasoning, as well as provide interpretability into models' multi-step reasoning. We apply VCoT to the Visual Storytelling and WikiHow summarization datasets and demonstrate through human evaluation that VCoT offers novel and consistent synthetic data augmentation beating chain-of-thought baselines, which can be used to enhance downstream performance.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces VCoT, a novel method that leverages chain-of-thought prompting with vision-language grounding to recursively bridge the logical gaps within sequential data and applies it to the Visual Storytelling and WikiHow summarization datasets."
            },
            "score": 6
        },
        {
            "id": "7d0d587ca37ee09247886684a220bb1625e77910",
            "paperId": "7d0d587ca37ee09247886684a220bb1625e77910",
            "title": "List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs",
            "abstract": "Set-of-Mark (SoM) Prompting unleashes the visual grounding capability of GPT-4V, by enabling the model to associate visual objects with tags inserted on the image. These tags, marked with alphanumerics, can be indexed via text tokens for easy reference. Despite the extraordinary performance from GPT-4V, we observe that other Multimodal Large Language Models (MLLMs) struggle to understand these visual tags. To promote the learning of SoM prompting for open-source models, we propose a new learning paradigm:\"list items one by one,\"which asks the model to enumerate and describe all visual tags placed on the image following the alphanumeric orders of tags. By integrating our curated dataset with other visual instruction tuning datasets, we are able to equip existing MLLMs with the SoM prompting ability. Furthermore, we evaluate our finetuned SoM models on five MLLM benchmarks. We find that this new dataset, even in a relatively small size (10k-30k images with tags), significantly enhances visual reasoning capabilities and reduces hallucinations for MLLMs. Perhaps surprisingly, these improvements persist even when the visual tags are omitted from input images during inference. This suggests the potential of\"list items one by one\"as a new paradigm for training MLLMs, which strengthens the object-text alignment through the use of visual tags in the training stage. Finally, we conduct analyses by probing trained models to understand the working mechanism of SoM. Our code and data are available at \\url{https://github.com/zzxslp/SoM-LLaVA}.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a new learning paradigm:\"list items one by one,\" which asks the model to enumerate and describe all visual tags placed on the image following the alphanumerics orders of tags, and finds that this new dataset, even in a relatively small size, significantly enhances visual reasoning capabilities and reduces hallucinations for MLLMs."
            },
            "score": 6
        },
        {
            "id": "2b2d9ee18507aa89a7f1ecba0bf68844c7d9aa75",
            "paperId": "2b2d9ee18507aa89a7f1ecba0bf68844c7d9aa75",
            "title": "Grounding Language Models to Images for Multimodal Generation",
            "abstract": "We propose an ef\ufb01cient method to ground pre-trained text-only language models to the visual domain, enabling them to process and generate arbitrarily interleaved image-and-text data. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and \ufb01netune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text inter-leaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.",
            "year": 2023,
            "citationCount": 66,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 5
        },
        {
            "id": "6173520a1eb2814d067e8c5fd16212b7cbf6ee78",
            "paperId": "6173520a1eb2814d067e8c5fd16212b7cbf6ee78",
            "title": "Grounding Language Models to Images for Multimodal Inputs and Outputs",
            "abstract": "We propose an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.",
            "year": 2023,
            "citationCount": 45,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 5
        },
        {
            "id": "76a3f4a79ae9a00db2f2b5f6877021d8deb96ada",
            "paperId": "76a3f4a79ae9a00db2f2b5f6877021d8deb96ada",
            "title": "SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models",
            "abstract": "We present SPHINX, a versatile multi-modal large language model (MLLM) with a joint mixing of model weights, tuning tasks, and visual embeddings. First, for stronger vision-language alignment, we unfreeze the large language model (LLM) during pre-training, and introduce a weight mix strategy between LLMs trained by real-world and synthetic data. By directly integrating the weights from two domains, the mixed LLM can efficiently incorporate diverse semantics with favorable robustness. Then, to enable multi-purpose capabilities, we mix a variety of tasks for joint visual instruction tuning, and design task-specific instructions to avoid inter-task conflict. In addition to the basic visual question answering, we include more challenging tasks such as region-level understanding, caption grounding, document layout detection, and human pose estimation, contributing to mutual enhancement over different scenarios. Additionally, we propose to extract comprehensive visual embeddings from various network architectures, pre-training paradigms, and information granularity, providing language models with more robust image representations. Based on our proposed joint mixing, SPHINX exhibits superior multi-modal understanding capabilities on a wide range of applications. On top of this, we further propose an efficient strategy aiming to better capture fine-grained appearances of high-resolution images. With a mixing of different scales and high-resolution sub-images, SPHINX attains exceptional visual parsing and reasoning performance on existing evaluation benchmarks. We hope our work may cast a light on the exploration of joint mixing in future MLLM research. Code is released at https://github.com/Alpha-VLLM/LLaMA2-Accessory.",
            "year": 2023,
            "citationCount": 59,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "SPHINX, a versatile multi-modal large language model (MLLM) with a joint mixing of model weights, tuning tasks, and visual embeddings is presented, and an efficient strategy aiming to better capture fine-grained appearances of high-resolution images is proposed."
            },
            "score": 5
        },
        {
            "id": "7ff441587afcd88860971210450cc1df7c772ff6",
            "paperId": "7ff441587afcd88860971210450cc1df7c772ff6",
            "title": "Grounding Language Models for Visual Entity Recognition",
            "abstract": "We introduce AutoVER, an Autoregressive model for Visual Entity Recognition. Our model extends an autoregressive Multi-modal Large Language Model by employing retrieval augmented constrained generation. It mitigates low performance on out-of-domain entities while excelling in queries that require visually-situated reasoning. Our method learns to distinguish similar entities within a vast label space by contrastively training on hard negative pairs in parallel with a sequence-to-sequence objective without an external retriever. During inference, a list of retrieved candidate answers explicitly guides language generation by removing invalid decoding paths. The proposed method achieves significant improvements across different dataset splits in the recently proposed Oven-Wiki benchmark. Accuracy on the Entity seen split rises from 32.7% to 61.5%. It also demonstrates superior performance on the unseen and query splits by a substantial double-digit margin.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces AutoVER, an Autoregressive model for Visual Entity Recognition that extends an autoregressive Multi-modal Large Language Model by employing retrieval augmented constrained generation and demonstrates superior performance on the unseen and query splits by a substantial double-digit margin."
            },
            "score": 5
        },
        {
            "id": "aeb6120224c8fec3398b7f597debcda2c6ce02c9",
            "paperId": "aeb6120224c8fec3398b7f597debcda2c6ce02c9",
            "title": "From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language Models",
            "abstract": "Multi-modal Large Language Models (MLLMs) have made significant strides in expanding the capabilities of Large Language Models (LLMs) through the incorporation of visual perception interfaces. Despite the emergence of exciting applications and the availability of diverse instruction tuning data, existing approaches often rely on CLIP or its variants as the visual branch, and merely extract features from the deep layers. However, these methods lack a comprehensive analysis of the visual encoders in MLLMs. In this paper, we conduct an extensive investigation into the effectiveness of different vision encoders within MLLMs. Our findings reveal that the shallow layer features of CLIP offer particular advantages for fine-grained tasks such as grounding and region understanding. Surprisingly, the vision-only model DINO, which is not pretrained with text-image alignment, demonstrates promising performance as a visual branch within MLLMs. By simply equipping it with an MLP layer for alignment, DINO surpasses CLIP in fine-grained related perception tasks. Building upon these observations, we propose a simple yet effective feature merging strategy, named COMM, that integrates CLIP and DINO with Multi-level features Merging, to enhance the visual capabilities of MLLMs. We evaluate COMM through comprehensive experiments on a wide range of benchmarks, including image captioning, visual question answering, visual grounding, and object hallucination. Experimental results demonstrate the superior performance of COMM compared to existing methods, showcasing its enhanced visual capabilities within MLLMs.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An extensive investigation into the effectiveness of different vision encoders within MLLMs reveals that the shallow layer features of CLIP offer particular advantages for fine-grained tasks such as grounding and region understanding, and proposes a simple yet effective feature merging strategy, named COMM, to enhance the visual capabilities of MLLMs."
            },
            "score": 5
        },
        {
            "id": "87089dd53f6279b0c348b78d7cc19989349b48e7",
            "paperId": "87089dd53f6279b0c348b78d7cc19989349b48e7",
            "title": "GRASP: A novel benchmark for evaluating language GRounding And Situated Physics understanding in multimodal language models",
            "abstract": "This paper presents GRASP, a novel benchmark to evaluate the language grounding and physical understanding capabilities of video-based multimodal large language models (LLMs). This evaluation is accomplished via a two-tier approach leveraging Unity simulations. The first level tests for language grounding by assessing a model's ability to relate simple textual descriptions with visual information. The second level evaluates the model's understanding of\"Intuitive Physics\"principles, such as object permanence and continuity. In addition to releasing the benchmark, we use it to evaluate several state-of-the-art multimodal LLMs. Our evaluation reveals significant shortcomings in the language grounding and intuitive physics capabilities of these models. Although they exhibit at least some grounding capabilities, particularly for colors and shapes, these capabilities depend heavily on the prompting strategy. At the same time, all models perform below or at the chance level of 50% in the Intuitive Physics tests, while human subjects are on average 80% correct. These identified limitations underline the importance of using benchmarks like GRASP to monitor the progress of future models in developing these competencies.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "GRASP is presented, a novel benchmark to evaluate the language grounding and physical understanding capabilities of video-based multimodal large language models (LLMs) and reveals significant shortcomings in the language grounding and intuitive physics capabilities of these models."
            },
            "score": 5
        },
        {
            "id": "f920895447be7eef1e53e415d7f0b7d69a9e8551",
            "paperId": "f920895447be7eef1e53e415d7f0b7d69a9e8551",
            "title": "CLIP-VG: Self-paced Curriculum Adapting of CLIP via Exploiting Pseudo-Language Labels for Visual Grounding",
            "abstract": "Visual Grounding (VG) refers to locating a region described by expressions in a speci\ufb01c image, which is a critical topic in vision-language \ufb01elds. To alleviate the dependence on labeled data, existing unsupervised methods try to locate regions using task-unrelated pseudo-labels. However, a large proportion of pseudo-labels are noisy and diversity scarcity in language taxonomy. Inspired by the advances in V-L pretraining, we consider utilizing the VLP models to realize unsupervised transfer learning in down-stream grounding task. Thus, we propose CLIP-VG, a novel method that can conduct self-paced curriculum adapting of CLIP via exploiting pseudo-language labels to solve VG problem. By elaborating an ef\ufb01cient model structure, we \ufb01rst propose a single-source and multi-source curriculum adapting method for unsupervised VG to progressively sample more reliable cross-modal pseudo-labels to obtain the optimal model, thus achieving implicit knowledge exploiting and denoising. Our method outperforms the existing state-of-the-art unsupervised VG method Pseudo-Q in both single-source and multi-source scenarios with a large margin, i.e., 6",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes CLIP-VG, a novel method that can conduct self-paced curriculum adapting of CLIP via exploiting pseudo-language labels to solve VG problem, and outperforms the existing state-of-the-art unsupervised VG method Pseudo-Q in both single-source and multi-source scenarios with a large margin."
            },
            "score": 5
        },
        {
            "id": "0b395ed1c8b284e551172b728e83cf257e33729a",
            "paperId": "0b395ed1c8b284e551172b728e83cf257e33729a",
            "title": "HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models",
            "abstract": "We introduce HallusionBench, a comprehensive benchmark designed for the evaluation of image-context reasoning. This benchmark presents significant challenges to advanced large visual-language models (LVLMs), such as GPT-4V(Vision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, by emphasizing nuanced understanding and interpretation of visual data. The benchmark comprises 346 images paired with 1129 questions, all meticulously crafted by human experts. We introduce a novel structure for these visual questions designed to establish control groups. This structure enables us to conduct a quantitative analysis of the models' response tendencies, logical consistency, and various failure modes. In our evaluation on HallusionBench, we benchmarked 15 different models, highlighting a 31.42% question-pair accuracy achieved by the state-of-the-art GPT-4V. Notably, all other evaluated models achieve accuracy below 16%. Moreover, our analysis not only highlights the observed failure modes, including language hallucination and visual illusion, but also deepens an understanding of these pitfalls. Our comprehensive case studies within HallusionBench shed light on the challenges of hallucination and illusion in LVLMs. Based on these insights, we suggest potential pathways for their future improvement. The benchmark and codebase can be accessed at https://github.com/tianyi-lab/HallusionBench.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This benchmark presents significant challenges to advanced large visual-language models (LVLMs), such as GPT-4V(Vision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, by emphasizing nuanced understanding and interpretation of visual data."
            },
            "score": 5
        },
        {
            "id": "3cebb93c399db7e1434741338b0a24db19786b15",
            "paperId": "3cebb93c399db7e1434741338b0a24db19786b15",
            "title": "Prompt to be Consistent is Better than Self-Consistent? Few-Shot and Zero-Shot Fact Verification with Pre-trained Language Models",
            "abstract": "Few-shot or zero-shot fact verification only relies on a few or no labeled training examples. In this paper, we propose a novel method called ProToCo, to \\underline{Pro}mpt pre-trained language models (PLMs) \\underline{To} be \\underline{Co}nsistent, for improving the factuality assessment capability of PLMs in the few-shot and zero-shot settings. Given a claim-evidence pair, ProToCo generates multiple variants of the claim with different relations and frames a simple consistency mechanism as constraints for making compatible predictions across these variants. We update PLMs by using parameter-efficient fine-tuning (PEFT), leading to more accurate predictions in few-shot and zero-shot fact verification tasks. Our experiments on three public verification datasets show that ProToCo significantly outperforms state-of-the-art few-shot fact verification baselines. With a small number of unlabeled instances, ProToCo also outperforms the strong zero-shot learner T0 on zero-shot verification. Compared to large PLMs using in-context learning (ICL) method, ProToCo outperforms OPT-30B and the Self-Consistency-enabled OPT-6.7B model in both few- and zero-shot settings.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel method called ProToCo, to improve the factuality assessment capability of pre-trained language models (PLMs) by using parameter-efficient fine-tuning (PEFT), leading to more accurate predictions in few-shot and zero-shot fact verification tasks."
            },
            "score": 5
        },
        {
            "id": "c1aa4b54b9f6a8d4fe6169257c208998d3d7f54c",
            "paperId": "c1aa4b54b9f6a8d4fe6169257c208998d3d7f54c",
            "title": "Examining Consistency of Visual Commonsense Reasoning based on Person Grounding",
            "abstract": "Given an image depicting multiple individuals, humans are capable of inferring each individual\u2019s emotions, intentions, and social norms based on commonsense understanding. However, a machine\u2019s ability of commonsense reasoning about distinct individuals in images remains underexplored. In this study, we examine the consistency of visual commonsense reasoning based on person grounding. We introduce a novel test dataset called V isual C ommonsense R easoning-C ontrast S ets (VCR-CS) to evaluate whether models can reason about individual people in an image by changing the person tags in the questions and answers. We benchmark various vision-language models on VCR-CS and observe that they fail in consistent common-sense reasoning about different people in one image, showing a performance decrease of up to 31.5%. To mitigate such failures, we propose a multi-task learning framework called P erson-centric ground I ng e N hanced T uning (PINT). Our framework enhances a model\u2019s ability to perform person-grounded commonsense reasoning by leveraging two novel person-centric pretraining tasks: Image Person-based Text Matching and Person-Masked Language Modeling. The experimental results revealed the effectiveness of PINT by showing the lowest performance degradation on VCR-CS and the improvements in consistency and sensitivity metrics. Our dataset and code are publicly available 1 .",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A multi-task learning framework enhances a model\u2019s ability to perform person-grounded commonsense reasoning by leveraging two novel person-centric pretraining tasks: Image Person-based Text Matching and Person-Masked Language Modeling."
            },
            "score": 5
        },
        {
            "id": "c36d7bc6adeee7d16df1194c44cb66c48c9ae681",
            "paperId": "c36d7bc6adeee7d16df1194c44cb66c48c9ae681",
            "title": "Negative Object Presence Evaluation (NOPE) to Measure Object Hallucination in Vision-Language Models",
            "abstract": "Object hallucination poses a significant challenge in vision-language (VL) models, often leading to the generation of nonsensical or unfaithful responses with non-existent objects. However, the absence of a general measurement for evaluating object hallucination in VL models has hindered our understanding and ability to mitigate this issue. In this work, we present NOPE (Negative Object Presence Evaluation), a novel benchmark designed to assess object hallucination in VL models through visual question answering (VQA). We propose a cost-effective and scalable approach utilizing large language models to generate 29.5k synthetic negative pronoun (NegP) data of high quality for NOPE. We extensively investigate the performance of 10 state-of-the-art VL models in discerning the non-existence of objects in visual questions, where the ground truth answers are denoted as NegP (e.g.,\"none\"). Additionally, we evaluate their standard performance on visual questions on 9 other VQA datasets. Through our experiments, we demonstrate that no VL model is immune to the vulnerability of object hallucination, as all models achieve accuracy below 10\\% on NegP. Furthermore, we uncover that lexically diverse visual questions, question types with large scopes, and scene-relevant objects capitalize the risk of object hallucination in VL models.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents NOPE (Negative Object Presence Evaluation), a novel benchmark designed to assess object hallucination in VL models through visual question answering (VQA), and proposes a cost-effective and scalable approach utilizing large language models to generate 29.5k synthetic negative pronoun (NegP) data of high quality for NOPE."
            },
            "score": 5
        },
        {
            "id": "4d7c68ec1a86ef5d187e7edb2f0ad63adddc8ea2",
            "paperId": "4d7c68ec1a86ef5d187e7edb2f0ad63adddc8ea2",
            "title": "Visual Hallucinations of Multi-modal Large Language Models",
            "abstract": "Visual hallucination (VH) means that a multi-modal LLM (MLLM) imagines incorrect details about an image in visual question answering. Existing studies find VH instances only in existing image datasets, which results in biased understanding of MLLMs' performance under VH due to limited diversity of such VH instances. In this work, we propose a tool called VHTest to generate a diverse set of VH instances. Specifically, VHTest finds some initial VH instances in existing image datasets (e.g., COCO), generates a text description for each VH mode, and uses a text-to-image generative model (e.g., DALL-E-3) to generate VH images based on the text descriptions. We collect a benchmark dataset with 1,200 VH instances in 8 VH modes using VHTest. We find that existing MLLMs such as GPT-4V, LLaVA-1.5, and MiniGPT-v2 hallucinate for a large fraction of the instances in our benchmark. Moreover, we find that fine-tuning an MLLM using our benchmark dataset reduces its likelihood to hallucinate without sacrificing its performance on other benchmarks. Our benchmarks are publicly available: https://github.com/wenhuang2000/VHTest.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a tool called VHTest, which finds some initial VH instances in existing image datasets, generates a text description for each VH mode, and uses a text-to-image generative model to generate VH images based on the text descriptions."
            },
            "score": 5
        },
        {
            "id": "97d6b474eba2299632ca573d082dcbe21c62bbc3",
            "paperId": "97d6b474eba2299632ca573d082dcbe21c62bbc3",
            "title": "Exploring Grounding Potential of VQA-oriented GPT-4V for Zero-shot Anomaly Detection",
            "abstract": "Large Multimodal Model (LMM) GPT-4V(ision) endows GPT-4 with visual grounding capabilities, making it possible to handle certain tasks through the Visual Question Answering (VQA) paradigm. This paper explores the potential of VQA-oriented GPT-4V in the recently popular visual Anomaly Detection (AD) and is the first to conduct qualitative and quantitative evaluations on the popular MVTec AD and VisA datasets. Considering that this task requires both image-/pixel-level evaluations, the proposed GPT-4V-AD framework contains three components: \\textbf{\\textit{1)}} Granular Region Division, \\textbf{\\textit{2)}} Prompt Designing, \\textbf{\\textit{3)}} Text2Segmentation for easy quantitative evaluation, and have made some different attempts for comparative analysis. The results show that GPT-4V can achieve certain results in the zero-shot AD task through a VQA paradigm, such as achieving image-level 77.1/88.0 and pixel-level 68.0/76.6 AU-ROCs on MVTec AD and VisA datasets, respectively. However, its performance still has a certain gap compared to the state-of-the-art zero-shot method, \\eg, WinCLIP and CLIP-AD, and further researches are needed. This study provides a baseline reference for the research of VQA-oriented LMM in the zero-shot AD task, and we also post several possible future works. Code is available at \\url{https://github.com/zhangzjn/GPT-4V-AD}.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results show that GPT-4V can achieve certain results in the zero-shot AD task through a VQA paradigm, and its performance still has a certain gap compared to the state-of-the-art zero-shot method, WinCLIP and CLIP-AD, and further researches are needed."
            },
            "score": 5
        },
        {
            "id": "85b292a8e5c0c297ed5262c80b521b83aa0555ac",
            "paperId": "85b292a8e5c0c297ed5262c80b521b83aa0555ac",
            "title": "Grounding Visual Illusions in Language: Do Vision-Language Models Perceive Illusions Like Humans?",
            "abstract": "Vision-Language Models (VLMs) are trained on vast amounts of data captured by humans emulating our understanding of the world. However, known as visual illusions, human's perception of reality isn't always faithful to the physical world. This raises a key question: do VLMs have the similar kind of illusions as humans do, or do they faithfully learn to represent reality? To investigate this question, we build a dataset containing five types of visual illusions and formulate four tasks to examine visual illusions in state-of-the-art VLMs. Our findings have shown that although the overall alignment is low, larger models are closer to human perception and more susceptible to visual illusions. Our dataset and initial findings will promote a better understanding of visual illusions in humans and machines and provide a stepping stone for future computational models that can better align humans and machines in perceiving and communicating about the shared visual world. The code and data are available at https://github.com/vl-illusion/dataset.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A dataset containing five types of visual illusions is built and four tasks to examine visual illusions in state-of-the-art VLMs are formulated and it is shown that although the overall alignment is low, larger models are closer to human perception and more susceptible to visual illusions."
            },
            "score": 4
        },
        {
            "id": "ca50f76c6b9023ddcb75cda975779482e43cc972",
            "paperId": "ca50f76c6b9023ddcb75cda975779482e43cc972",
            "title": "RSVG: Exploring Data and Models for Visual Grounding on Remote Sensing Data",
            "abstract": "In this article, we introduce the task of visual grounding for remote sensing data (RSVG). RSVG aims to localize the referred objects in remote sensing (RS) images with the guidance of natural language. To retrieve rich information from RS imagery using natural language, many research tasks, such as RS image visual question answering, RS image captioning, and RS image\u2013text retrieval, have been investigated a lot. However, the object-level visual grounding on RS images is still underexplored. Thus, in this work, we propose to construct the dataset and explore deep learning models for the RSVG task. Specifically, our contributions can be summarized as follows. First, we build the new large-scale benchmark of RSVG based on detection in optical remote sensing (DIOR) dataset, termed DIOR-RSVG, to fully advance the research of RSVG. This new dataset includes image/expression/box triplets for training and evaluating visual grounding models. Second, we benchmark extensive state-of-the-art (SOTA) natural image visual grounding methods on the constructed DIOR-RSVG dataset, and some insightful analyses are provided based on the results. Third, a novel transformer-based multigranularity visual language fusion (MGVLF) module is proposed. Remotely sensed images are usually with large-scale variations and cluttered backgrounds. To deal with the scale-variation problem, the MGVLF module takes advantage of multiscale visual features and multigranularity textual embeddings to learn more discriminative representations. To cope with the cluttered background problem, MGVLF adaptively filters irrelevant noise and enhances salient features. In this way, our proposed model can incorporate more effective multilevel and multimodal features to boost performance. This work can provide useful insights for developing better RSVG models.",
            "year": 2022,
            "citationCount": 30,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work builds the new large-scale benchmark of RSVG based on detection in optical remote sensing (DIOR) dataset, termed DIOR-RSVG, and proposes a novel transformer-based multigranularity visual language fusion (MGVLF) module that can incorporate more effective multilevel and multimodal features to boost performance."
            },
            "score": 4
        },
        {
            "id": "76dba703cc6da0a902d4e728d3ac5720bb645cd4",
            "paperId": "76dba703cc6da0a902d4e728d3ac5720bb645cd4",
            "title": "GPT-4 Enhanced Multimodal Grounding for Autonomous Driving: Leveraging Cross-Modal Attention with Large Language Models",
            "abstract": "In the field of autonomous vehicles (AVs), accurately discerning commander intent and executing linguistic commands within a visual context presents a significant challenge. This paper introduces a sophisticated encoder-decoder framework, developed to address visual grounding in AVs.Our Context-Aware Visual Grounding (CAVG) model is an advanced system that integrates five core encoders-Text, Image, Context, and Cross-Modal-with a Multimodal decoder. This integration enables the CAVG model to adeptly capture contextual semantics and to learn human emotional features, augmented by state-of-the-art Large Language Models (LLMs) including GPT-4. The architecture of CAVG is reinforced by the implementation of multi-head cross-modal attention mechanisms and a Region-Specific Dynamic (RSD) layer for attention modulation. This architectural design enables the model to efficiently process and interpret a range of cross-modal inputs, yielding a comprehensive understanding of the correlation between verbal commands and corresponding visual scenes. Empirical evaluations on the Talk2Car dataset, a real-world benchmark, demonstrate that CAVG establishes new standards in prediction accuracy and operational efficiency. Notably, the model exhibits exceptional performance even with limited training data, ranging from 50% to 75% of the full dataset. This feature highlights its effectiveness and potential for deployment in practical AV applications. Moreover, CAVG has shown remarkable robustness and adaptability in challenging scenarios, including long-text command interpretation, low-light conditions, ambiguous command contexts, inclement weather conditions, and densely populated urban environments. The code for the proposed model is available at our Github.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A sophisticated encoder-decoder framework, developed to address visual grounding in AVs, which establishes new standards in prediction accuracy and operational efficiency and highlights its effectiveness and potential for deployment in practical AV applications."
            },
            "score": 4
        },
        {
            "id": "b5250f7f77ca1fba6bd53c337a6e0c926b880dc1",
            "paperId": "b5250f7f77ca1fba6bd53c337a6e0c926b880dc1",
            "title": "Dynamic Inference with Grounding Based Vision and Language Models",
            "abstract": "Transformers have been recently utilized for vision and language tasks successfully. For example, recent image and language models with more than 200M parameters have been proposed to learn visual grounding in the pre-training step and show impressive results on downstream vision and language tasks. On the other hand, there exists a large amount of computational redundancy in these large models which skips their run-time efficiency. To address this problem, we propose dynamic inference for grounding based vision and language models conditioned on the input image-text pair. We first design an approach to dynamically skip multihead self-attention and feed forward network layers across two backbones and multimodal network. Additionally, we propose dynamic token pruning and fusion for two backbones. In particular, we remove redundant tokens at different levels of the backbones and fuse the image tokens with the language tokens in an adaptive manner. To learn policies for dynamic inference, we train agents using reinforcement learning. In this direction, we replace the CNN backbone in a recent grounding-based vision and language model, MDETR, with a vision transformer and call it ViTMDETR. Then, we apply our dynamic inference method to ViTMDETR, called D-ViTDMETR, and perform experiments on image-language tasks. Our results show that we can improve the run-time efficiency of the state-of-the-art models MDETR and GLIP by up to ~ 50% on Referring Expression Comprehension and Segmentation, and VQA with only maximum ~ 0.3% accuracy drop.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results show that the dynamic inference method proposed can improve the run-time efficiency of the state-of-the-art models MDETR and GLIP by up to ~ 50% on Referring Expression Comprehension and Segmentation, and VQA with only maximum ~ 0.3% accuracy drop."
            },
            "score": 4
        },
        {
            "id": "1fa3eea8665c3935c4e640e7a28f21d957ebba62",
            "paperId": "1fa3eea8665c3935c4e640e7a28f21d957ebba62",
            "title": "GVCCI: Lifelong Learning of Visual Grounding for Language-Guided Robotic Manipulation",
            "abstract": "Language-Guided Robotic Manipulation (LGRM) is a challenging task as it requires a robot to understand human instructions to manipulate everyday objects. Recent approaches in LGRM rely on pre-trained Visual Grounding (VG) models to detect objects without adapting to manipulation environments. This results in a performance drop due to a substantial domain gap between the pre-training and real-world data. A straight-forward solution is to collect additional training data, but the cost of human-annotation is extortionate. In this paper, we propose Grounding Vision to Ceaselessly Created Instructions (GVCCI), a lifelong learning framework for LGRM, which continuously learns VG without human supervision. GVCCI iteratively generates synthetic instruction via object detection and trains the VG model with the generated data. We validate our framework in offline and online settings across diverse environments on different VG models. Experimental results show that accumulating synthetic data from GVCCI leads to a steady improvement in VG by up to 56.7% and improves resultant LGRM by up to 29.4%. Furthermore, the qualitative analysis shows that the unadapted VG model often fails to find correct objects due to a strong bias learned from the pre-training data. Finally, we introduce a novel VG dataset for LGRM, consisting of nearly 252k triplets of image-object-instruction from diverse manipulation environments.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes Grounding Vision to Ceaselessly Created Instructions (GVCCI), a lifelong learning framework for LGRM, which continuously learns Visual Grounding (VG) without human supervision, and introduces a novel VG dataset for L GRM."
            },
            "score": 4
        },
        {
            "id": "8cc394a1cc1ce4e2a32302294af7fffb7de2f46d",
            "paperId": "8cc394a1cc1ce4e2a32302294af7fffb7de2f46d",
            "title": "Consistency and Uncertainty: Identifying Unreliable Responses From Black-Box Vision-Language Models for Selective Visual Question Answering",
            "abstract": "The goal of selective prediction is to allow an a model to abstain when it may not be able to deliver a reliable prediction, which is important in safety-critical contexts. Existing approaches to selective prediction typically require access to the internals of a model, require retraining a model or study only unimodal models. However, the most powerful models (e.g. GPT-4) are typically only available as black boxes with inaccessible internals, are not retrainable by end-users, and are frequently used for multimodal tasks. We study the possibility of selective prediction for vision-language models in a realistic, black-box setting. We propose using the principle of \\textit{neighborhood consistency} to identify unreliable responses from a black-box vision-language model in question answering tasks. We hypothesize that given only a visual question and model response, the consistency of the model's responses over the neighborhood of a visual question will indicate reliability. It is impossible to directly sample neighbors in feature space in a black-box setting. Instead, we show that it is possible to use a smaller proxy model to approximately sample from the neighborhood. We find that neighborhood consistency can be used to identify model responses to visual questions that are likely unreliable, even in adversarial settings or settings that are out-of-distribution to the proxy model.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that neighborhood consistency can be used to identify model responses to visual questions that are likely unreliable, even in adversarial settings or settings that are out-of-distribution to the proxy model."
            },
            "score": 4
        },
        {
            "id": "0e0d72be9950fde9b5e8996e2147d1318f216ebb",
            "paperId": "0e0d72be9950fde9b5e8996e2147d1318f216ebb",
            "title": "Prompting is not a substitute for probability measurements in large language models",
            "abstract": "Prompting is now a dominant method for evaluating the linguistic knowledge of large language models (LLMs). While other methods directly read out models' probability distributions over strings, prompting requires models to access this internal information by processing linguistic input, thereby implicitly testing a new type of emergent ability: metalinguistic judgment. In this study, we compare metalinguistic prompting and direct probability measurements as ways of measuring models' linguistic knowledge. Broadly, we find that LLMs' metalinguistic judgments are inferior to quantities directly derived from representations. Furthermore, consistency gets worse as the prompt query diverges from direct measurements of next-word probabilities. Our findings suggest that negative results relying on metalinguistic prompts cannot be taken as conclusive evidence that an LLM lacks a particular linguistic generalization. Our results also highlight the value that is lost with the move to closed APIs where access to probability distributions is limited.",
            "year": 2023,
            "citationCount": 11,
            "tldr": null,
            "score": 4
        },
        {
            "id": "dee83ee947ea95f54b40a3c101552b4eb447bbf6",
            "paperId": "dee83ee947ea95f54b40a3c101552b4eb447bbf6",
            "title": "Self-Consistency of Large Language Models under Ambiguity",
            "abstract": "Large language models (LLMs) that do not give consistent answers across contexts are problematic when used for tasks with expectations of consistency\u2013e.g. question-answering, explanations, etc. Our work presents an evaluation benchmark for self-consistency in cases of under-specification where two or more answers can be correct. We conduct a series of behavioral experiments on the OpenAI model suite using an ambiguous integer sequence completion task. We find that average consistency ranges from 67% to 82%, far higher than would be predicted if a model\u2019s consistency was random, and increases as model capability improves. Furthermore, we show that models tend to maintain self-consistency across a series of robustness checks, including prompting speaker changes and sequence length changes. These results suggest that self-consistency arises as an emergent capability without specifically training for it. Despite this, we find that models are uncalibrated when judging their own consistency, with models displaying both over- and under-confidence. We also propose a nonparametric test for determining from token output distribution whether a model assigns non-trivial probability to alternative answers. Using this test, we find that despite increases in self-consistency, models usually place significant weight on alternative, inconsistent answers. This distribution of probability mass provides evidence that even highly self-consistent models internally compute multiple possible responses.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents an evaluation benchmark for self-consistency in cases of under-specification where two or more answers can be correct, and proposes a nonparametric test for determining from token output distribution whether a model assigns non-trivial probability to alternative answers."
            },
            "score": 4
        },
        {
            "id": "ec55e16e934e4a7c4527a1f54b39a680a88b392c",
            "paperId": "ec55e16e934e4a7c4527a1f54b39a680a88b392c",
            "title": "Limits for learning with language models",
            "abstract": "With the advent of large language models (LLMs), the trend in NLP has been to train LLMs on vast amounts of data to solve diverse language understanding and generation tasks. The list of LLM successes is long and varied. Nevertheless, several recent papers provide empirical evidence that LLMs fail to capture important aspects of linguistic meaning. Focusing on universal quantification, we provide a theoretical foundation for these empirical findings by proving that LLMs cannot learn certain fundamental semantic properties including semantic entailment and consistency as they are defined in formal semantics. More generally, we show that LLMs are unable to learn concepts beyond the first level of the Borel Hierarchy, which imposes severe limits on the ability of LMs, both large and small, to capture many aspects of linguistic meaning. This means that LLMs will operate without formal guarantees on tasks that require entailments and deep linguistic understanding.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that LLMs are unable to learn concepts beyond the first level of the Borel Hierarchy, which imposes severe limits on the ability of LMs, both large and small, to capture many aspects of linguistic meaning."
            },
            "score": 4
        },
        {
            "id": "a3dd35dd2e8b75bfbc3fa1d986e5d9d913f2a19e",
            "paperId": "a3dd35dd2e8b75bfbc3fa1d986e5d9d913f2a19e",
            "title": "Towards Grounded Visual Spatial Reasoning in Multi-Modal Vision Language Models",
            "abstract": "Large vision-and-language models (VLMs) trained to match images with text on large-scale datasets of image-text pairs have shown impressive generalization ability on several vision and language tasks. Several recent works, however, showed that these models lack fine-grained understanding, such as the ability to count and recognize verbs, attributes, or relationships. The focus of this work is to study the understanding of spatial relations. This has been tackled previously using image-text matching (e.g., Visual Spatial Reasoning benchmark) or visual question answering (e.g., GQA or VQAv2), both showing poor performance and a large gap compared to human performance. In this work, we show qualitatively (using explainability tools) and quantitatively (using object detectors) that the poor object localization\"grounding\"ability of the models is a contributing factor to the poor image-text matching performance. We propose an alternative fine-grained, compositional approach for recognizing and ranking spatial clauses that combines the evidence from grounding noun phrases corresponding to objects and their locations to compute the final rank of the spatial clause. We demonstrate the approach on representative VLMs (such as LXMERT, GPV, and MDETR) and compare and highlight their abilities to reason about spatial relationships.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes an alternative fine-grained, compositional approach for recognizing and ranking spatial clauses that combines the evidence from grounding noun phrases corresponding to objects and their locations to compute the final rank of the spatial clause."
            },
            "score": 4
        },
        {
            "id": "f32ea390686b1eee3ba5b53c7a85e9e9385d4b94",
            "paperId": "f32ea390686b1eee3ba5b53c7a85e9e9385d4b94",
            "title": "Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models",
            "abstract": "Solving complex visual tasks such as\"Who invented the musical instrument on the right?\"involves a composition of skills: understanding space, recognizing instruments, and also retrieving prior knowledge. Recent work shows promise by decomposing such tasks using a large language model (LLM) into an executable program that invokes specialized vision models. However, generated programs are error-prone: they omit necessary steps, include spurious ones, and are unable to recover when the specialized models give incorrect outputs. Moreover, they require loading multiple models, incurring high latency and computation costs. We propose Visual Program Distillation (VPD), an instruction tuning framework that produces a vision-language model (VLM) capable of solving complex visual tasks with a single forward pass. VPD distills the reasoning ability of LLMs by using them to sample multiple candidate programs, which are then executed and verified to identify a correct one. It translates each correct program into a language description of the reasoning steps, which are then distilled into a VLM. Extensive experiments show that VPD improves the VLM's ability to count, understand spatial relations, and reason compositionally. Our VPD-trained PaLI-X outperforms all prior VLMs, achieving state-of-the-art performance across complex vision tasks, including MMBench, OK-VQA, A-OKVQA, TallyQA, POPE, and Hateful Memes. An evaluation with human annotators also confirms that VPD improves model response factuality and consistency. Finally, experiments on content moderation demonstrate that VPD is also helpful for adaptation to real-world applications with limited data.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Visual Program Distillation (VPD) is proposed, an instruction tuning framework that produces a vision-language model (VLM) capable of solving complex visual tasks with a single forward pass and improves the VLM's ability to count, understand spatial relations, and reason compositionally."
            },
            "score": 4
        },
        {
            "id": "90889802534e4fcdba7c74f7293a3644e542129e",
            "paperId": "90889802534e4fcdba7c74f7293a3644e542129e",
            "title": "Hallucination Benchmark in Medical Visual Question Answering",
            "abstract": "The recent success of large language and vision models (LLVMs) on vision question answering (VQA), particularly their applications in medicine (Med-VQA), has shown a great potential of realizing effective visual assistants for healthcare. However, these models are not extensively tested on the hallucination phenomenon in clinical settings. Here, we created a hallucination benchmark of medical images paired with question-answer sets and conducted a comprehensive evaluation of the state-of-the-art models. The study provides an in-depth analysis of current models' limitations and reveals the effectiveness of various prompting strategies.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A hallucination benchmark of medical images paired with question-answer sets is created and a comprehensive evaluation of the state-of-the-art models' limitations and the effectiveness of various prompting strategies are revealed."
            },
            "score": 4
        },
        {
            "id": "f86f71cfd2e9682a56d7334736a7b8a0b1c70b45",
            "paperId": "f86f71cfd2e9682a56d7334736a7b8a0b1c70b45",
            "title": "Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models",
            "abstract": "Visually-conditioned language models (VLMs) have seen growing adoption in applications such as visual dialogue, scene understanding, and robotic task planning; adoption that has fueled a wealth of new models such as LLaVa, InstructBLIP, and PaLI-3. Despite the volume of new releases, key design decisions around image preprocessing, architecture, and optimization are under-explored, making it challenging to understand what factors account for model performance $-$ a challenge further complicated by the lack of objective, consistent evaluations. To address these gaps, we first compile a suite of standardized evaluations spanning visual question answering, object localization from language, and targeted challenge sets that probe properties such as hallucination; evaluations that provide calibrated, fine-grained insight into a VLM's capabilities. Second, we rigorously investigate VLMs along key design axes, including pretrained visual representations and quantifying the tradeoffs of using base vs. instruct-tuned language models, amongst others. We couple our analysis with three resource contributions: (1) a unified framework for evaluating VLMs, (2) optimized, flexible code for VLM training, and (3) checkpoints for all models, including a family of VLMs at the 7-13B scale that strictly outperform InstructBLIP and LLaVa v1.5, the state-of-the-art in open-source VLMs.",
            "year": 2024,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work first compile a suite of standardized evaluations spanning visual question answering, object localization from language, and targeted challenge sets that probe properties such as hallucination; evaluations that provide calibrated, fine-grained insight into a VLM's capabilities."
            },
            "score": 4
        },
        {
            "id": "0c7f6e2d8b4feed05affb9a3e9b2591e19fe461e",
            "paperId": "0c7f6e2d8b4feed05affb9a3e9b2591e19fe461e",
            "title": "Insights into Classifying and Mitigating LLMs' Hallucinations",
            "abstract": "The widespread adoption of large language models (LLMs) across diverse AI applications is proof of the outstanding achievements obtained in several tasks, such as text mining, text generation, and question answering. However, LLMs are not exempt from drawbacks. One of the most concerning aspects regards the emerging problematic phenomena known as\"Hallucinations\". They manifest in text generation systems, particularly in question-answering systems reliant on LLMs, potentially resulting in false or misleading information propagation. This paper delves into the underlying causes of AI hallucination and elucidates its significance in artificial intelligence. In particular, Hallucination classification is tackled over several tasks (Machine Translation, Question and Answer, Dialog Systems, Summarisation Systems, Knowledge Graph with LLMs, and Visual Question Answer). Additionally, we explore potential strategies to mitigate hallucinations, aiming to enhance the overall reliability of LLMs. Our research addresses this critical issue within the HeReFaNMi (Health-Related Fake News Mitigation) project, generously supported by NGI Search, dedicated to combating Health-Related Fake News dissemination on the Internet. This endeavour represents a concerted effort to safeguard the integrity of information dissemination in an age of evolving AI technologies.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper delves into the underlying causes of AI hallucination and elucidates its significance in artificial intelligence, and explores potential strategies to mitigate hallucinations, aiming to enhance the overall reliability of LLMs."
            },
            "score": 4
        },
        {
            "id": "7dddcbb769ce827cc7154af50cb72658fa847d88",
            "paperId": "7dddcbb769ce827cc7154af50cb72658fa847d88",
            "title": "Visually Dehallucinative Instruction Generation",
            "abstract": "In recent years, synthetic visual instructions by generative language model have demonstrated plausible text generation performance on the visual question-answering tasks. However, challenges persist in the hallucination of generative language models, i.e., the generated image-text data contains unintended contents. This paper presents a novel and scalable method for generating visually dehallucinative instructions, dubbed CAP2QA, that constrains the scope to only image contents. Our key contributions lie in introducing image-aligned instructive QA dataset CAP2QA-COCO and its scalable recipe. In our experiments, we compare synthetic visual instruction datasets that share the same source data by visual instruction tuning and conduct general visual recognition tasks. It shows that our proposed method significantly reduces visual hallucination while consistently improving visual recognition ability and expressiveness.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents a novel and scalable method for generating visually dehallucinative instructions, dubbed CAP2QA, that constrains the scope to only image contents, and significantly reduces visual hallucination while consistently improving visual recognition ability and expressiveness."
            },
            "score": 4
        },
        {
            "id": "70feb009bc1e8b1cb8dff64bf9fd67789636438b",
            "paperId": "70feb009bc1e8b1cb8dff64bf9fd67789636438b",
            "title": "From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models",
            "abstract": "Large language models (LLMs) have demonstrated excellent zero-shot generalization to new language tasks. However, effective utilization of LLMs for zero-shot visual question-answering (VQA) remains challenging, primarily due to the modality disconnect and task disconnect between the LLM and VQA tasks. End-to-end training on multimodal data may bridge the disconnects, but is inflexible and computationally expensive. To address this issue, we propose Img2LLM, a plug-and-play module that provides LLM prompts to enable LLMs to perform zeroshot VQA tasks without end-to-end training. We develop LLM-agnostic models describe image content as exemplar question-answer pairs, which prove to be effective LLM prompts. Img2LLM offers the following benefits: 1) It achieves comparable or better performance than methods relying on end-to-end training. For example, we outperform Flamingo [3] by 5.6% on VQAv2. On the challenging A-OKVQA dataset, our method outperforms few-shot methods by as much as 20%. 2) It flexibly interfaces with a wide range of LLMs to perform VQA. 3) It eliminates the need to specialize LLMs using end-to-end finetuning and serve highly specialized LLMs to end users, thereby reducing cost. Code is available via the LAVIS [28] framework at https://github.com/salesforce/LAVIS/tree/main/projects/img2llm-vqa.",
            "year": 2022,
            "citationCount": 70,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Img2LLM is a plug-and-play module that provides LLM prompts to enable LLMs to perform zeroshot VQA tasks without end-to-end training and eliminates the need to specialize LLMs using end-to-end finetuning and serve highly specialized LLMs to end users, thereby reducing cost."
            },
            "score": 4
        },
        {
            "id": "5af106fb1cdeda43e369016ab5ece11f450ba485",
            "paperId": "5af106fb1cdeda43e369016ab5ece11f450ba485",
            "title": "Multimodal Grounding for Embodied AI via Augmented Reality Headsets for Natural Language Driven Task Planning",
            "abstract": "Recent advances in generative modeling have spurred a resurgence in the field of Embodied Artificial Intelligence (EAI). EAI systems typically deploy large language models to physical systems capable of interacting with their environment. In our exploration of EAI for industrial domains, we successfully demonstrate the feasibility of co-located, human-robot teaming. Specifically, we construct an experiment where an Augmented Reality (AR) headset mediates information exchange between an EAI agent and human operator for a variety of inspection tasks. To our knowledge the use of an AR headset for multimodal grounding and the application of EAI to industrial tasks are novel contributions within Embodied AI research. In addition, we highlight potential pitfalls in EAI's construction by providing quantitative and qualitative analysis on prompt robustness.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "In the exploration of EAI for industrial domains, the feasibility of co-located, human-robot teaming is successfully demonstrated and potential pitfalls in EAI's construction are highlighted by providing quantitative and qualitative analysis on prompt robustness."
            },
            "score": 4
        },
        {
            "id": "bf8491bef353df126e2306ad2fe4b898697b906a",
            "paperId": "bf8491bef353df126e2306ad2fe4b898697b906a",
            "title": "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",
            "abstract": "This paper proposes a framework for quantitatively evaluating interactive LLMs such as ChatGPT using publicly available data sets. We carry out an extensive technical evaluation of ChatGPT using 23 data sets covering 8 different common NLP application tasks. We evaluate the multitask, multilingual and multi-modal aspects of ChatGPT based on these data sets and a newly designed multimodal dataset. We find that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks. We find that it is better at understanding non-Latin script languages than generating them. It is able to generate multimodal content from textual prompts, via an intermediate code generation step. Moreover, we find that ChatGPT is 63.41% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense reasoning, hence making it an unreliable reasoner. It is, for example, better at deductive than inductive reasoning. ChatGPT suffers from hallucination problems like other LLMs and it generates more extrinsic hallucinations from its parametric memory as it does not have access to an external knowledge base. Finally, the interactive feature of ChatGPT enables human collaboration with the underlying LLM to improve its performance, i.e, 8% ROUGE-1 on summarization and 2% ChrF++ on machine translation, in a multi-turn\"prompt engineering\"fashion. We also release codebase for evaluation set extraction.",
            "year": 2023,
            "citationCount": 754,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks and is better at understanding non-Latin script languages than generating them."
            },
            "score": 4
        },
        {
            "id": "1f5e1a036b24b9dd34c006ba3bb61119624f4fdb",
            "paperId": "1f5e1a036b24b9dd34c006ba3bb61119624f4fdb",
            "title": "A Comprehensive Study of GPT-4V's Multimodal Capabilities in Medical Imaging",
            "abstract": "This paper presents a comprehensive evaluation of GPT-4V's capabilities across diverse medical imaging tasks, including Radiology Report Generation, Medical Visual Question Answering (VQA), and Visual Grounding. While prior efforts have explored GPT-4V's performance in medical imaging, to the best of our knowledge, our study represents the first quantitative evaluation on publicly available benchmarks. Our findings highlight GPT-4V's potential in generating descriptive reports for chest X-ray images, particularly when guided by well-structured prompts. However, its performance on the MIMIC-CXR dataset benchmark reveals areas for improvement in certain evaluation metrics, such as CIDEr. In the domain of Medical VQA, GPT-4V demonstrates proficiency in distinguishing between question types but falls short of prevailing benchmarks in terms of accuracy. Furthermore, our analysis finds the limitations of conventional evaluation metrics like the BLEU score, advocating for the development of more semantically robust assessment methods. In the field of Visual Grounding, GPT-4V exhibits preliminary promise in recognizing bounding boxes, but its precision is lacking, especially in identifying specific medical organs and signs. Our evaluation underscores the significant potential of GPT-4V in the medical imaging domain, while also emphasizing the need for targeted refinements to fully unlock its capabilities.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents a comprehensive evaluation of GPT-4V's capabilities across diverse medical imaging tasks, including Radiology Report Generation, Medical Visual Question Answering (VQA), and Visual Grounding, and finds the limitations of conventional evaluation metrics like the BLEU score."
            },
            "score": 4
        },
        {
            "id": "5aa41b45e10bed3bfce088441ccfedff474f8df6",
            "paperId": "5aa41b45e10bed3bfce088441ccfedff474f8df6",
            "title": "Detecting Concrete Visual Tokens for Multimodal Machine Translation",
            "abstract": "The challenge of visual grounding and masking in multimodal machine translation (MMT) systems has encouraged varying approaches to the detection and selection of visually-grounded text tokens for masking. We introduce new methods for detection of visually and contextually relevant (concrete) tokens from source sentences, including detection with natural language processing (NLP), detection with object detection, and a joint detection-verification technique. We also introduce new methods for selection of detected tokens, including shortest $n$ tokens, longest $n$ tokens, and all detected concrete tokens. We utilize the GRAM MMT architecture to train models against synthetically collated multimodal datasets of source images with masked sentences, showing performance improvements and improved usage of visual context during translation tasks over the baseline model.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The GRAM MMT architecture is utilized to train models against synthetically collated multimodal datasets of source images with masked sentences, showing performance improvements and improved usage of visual context during translation tasks over the baseline model."
            },
            "score": 4
        },
        {
            "id": "64ab90619b617d3f7d76a2b4bf95997cd999a3c1",
            "paperId": "64ab90619b617d3f7d76a2b4bf95997cd999a3c1",
            "title": "Grounding Meaning Representation for Situated Reasoning",
            "abstract": "As natural language technology becomes ever-present in everyday life, people will expect artificial agents to understand language use as humans do. Nevertheless, most advanced neural AI systems fail at some types of interactions that are trivial for humans (e.g., ask a smart system \u201cWhat am I pointing at?\u201d). One critical aspect of human language understanding is situated reasoning, where inferences make reference to the local context, perceptual surroundings, and contextual groundings from the interaction. In this cutting-edge tutorial, we bring to the NLP/CL community a synthesis of multimodal grounding and meaning representation techniques with formal and computational models of embodied reasoning. We will discuss existing approaches to multimodal language grounding and meaning representations, discuss the kind of information each method captures and their relative suitability to situated reasoning tasks, and demon- strate how to construct agents that conduct situated reasoning by embodying a simulated environment. In doing so, these agents also represent their human interlocutor(s) within the simulation, and are represented through their virtual embodiment in the real world, enabling true bidirectional communication with a computer using multiple modalities.",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This cutting-edge tutorial brings to the NLP/CL community a synthesis of multimodal grounding and meaning representation techniques with formal and computational models of embodied reasoning."
            },
            "score": 4
        },
        {
            "id": "30d5a29a35eb7268ff9fe0779f1303a640c9159b",
            "paperId": "30d5a29a35eb7268ff9fe0779f1303a640c9159b",
            "title": "GPT-4V(ision) for Robotics: Multimodal Task Planning from Human Demonstration",
            "abstract": "We introduce a pipeline that enhances a general-purpose Vision Language Model, GPT-4V(ision), by integrating observations of human actions to facilitate robotic manipulation. This system analyzes videos of humans performing tasks and creates executable robot programs that incorporate affordance insights. The computation starts by analyzing the videos with GPT-4V to convert environmental and action details into text, followed by a GPT-4-empowered task planner. In the following analyses, vision systems reanalyze the video with the task plan. Object names are grounded using an open-vocabulary object detector, while focus on the hand-object relation helps to detect the moment of grasping and releasing. This spatiotemporal grounding allows the vision systems to further gather affordance data (e.g., grasp type, way points, and body postures). Experiments across various scenarios demonstrate this method's efficacy in achieving real robots' operations from human demonstrations in a zero-shot manner. The prompts of GPT-4V/GPT-4 are available at this project page: https://microsoft.github.io/GPT4Vision-Robot-Manipulation-Prompts/",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A pipeline that enhances a general-purpose Vision Language Model, GPT-4V(ision), by integrating observations of human actions to facilitate robotic manipulation is introduced, demonstrating this method's efficacy in achieving real robots' operations from human demonstrations in a zero-shot manner."
            },
            "score": 4
        },
        {
            "id": "0cc8a707bb0166a49fa116477dcacad9f43be13e",
            "paperId": "0cc8a707bb0166a49fa116477dcacad9f43be13e",
            "title": "Enhancing Visual Grounding and Generalization: A Multi-Task Cycle Training Approach for Vision-Language Models",
            "abstract": "Visual grounding (VG) occupies a pivotal position in multi-modality vision-language models. In this study, we propose ViLaM, a large multi-modality model, that supports multi-tasks of VG using the cycle training strategy, with abundant interaction instructions. The cycle training between referring expression generation (REG) and referring expression comprehension (REC) is introduced. It enhances the consistency between visual location and referring expressions, and addresses the need for high-quality, multi-tasks VG datasets. Moreover, multi-tasks of VG are promoted in our model, contributed by the cycle training strategy. The multi-tasks in REC encompass a range of granularities, from region-level to pixel-level, which include referring bbox detection, referring keypoints detection, and referring image segmentation. In REG, referring region classification determines the fine-grained category of the target, while referring region captioning generates a comprehensive description. Meanwhile, all tasks participate in the joint training, synergistically enhancing one another and collectively improving the overall performance of the model. Furthermore, leveraging the capabilities of large language models, ViLaM extends a wide range of instructions, thereby significantly enhancing its generalization and interaction potentials. Extensive public datasets corroborate the superior capabilities of our model in VG with muti-tasks. Additionally, validating its robust generalization, ViLaM is validated under open-set and few-shot scenarios. Especially in the medical field, our model demonstrates cross-domain robust generalization capabilities. Furthermore, we contribute a VG dataset, especially with multi-tasks. To support and encourage the community focused on VG, we have made both the dataset and our code public: https://github.com/AnonymGiant/ViLaM.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study proposes ViLaM, a large multi-modality model, that supports multi-tasks of VG using the cycle training strategy, with abundant interaction instructions, and validates its robust generalization capabilities under open-set and few-shot scenarios."
            },
            "score": 3
        },
        {
            "id": "af258688503b48c1028680c67eb12cb351e9830a",
            "paperId": "af258688503b48c1028680c67eb12cb351e9830a",
            "title": "Visual grounding of spatial relations in recurrent neural language models",
            "abstract": "The task of automatically describing an image with natural language requires techniques to associate visual representations with their corresponding linguistic units. In the state of the art techniques, most commonly, a pre-trained convolutional neural networks extracts visual features of an image, and then a neural language model with attention mechanism is trained as a decoder to generate descriptions. However, as argued in the previous work such networks are good at detecting objects but are not using and describing geometric relations between objects. In this paper we explore the possibility of using the location of objects as explicit feature in generating image captions. We improve the state of the art image captioning model with adaptive attention described in [13] with explicit object locations derived from the annotated bounding boxes and investigate to what degree each kind of visual representations is contributing to prediction of spatial relations between objects: (i) visual object features from a pre-trained CNN and (ii) explicit object locations.",
            "year": 2018,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper improves the state of the art image captioning model with adaptive attention with explicit object locations derived from the annotated bounding boxes and investigates to what degree each kind of visual representations is contributing to prediction of spatial relations between objects."
            },
            "score": 3
        },
        {
            "id": "0392d58335ce674a70f5e58ac8c438de296a0e6a",
            "paperId": "0392d58335ce674a70f5e58ac8c438de296a0e6a",
            "title": "Interactive and Visual Prompt Engineering for Ad-hoc Task Adaptation with Large Language Models",
            "abstract": "State-of-the-art neural language models can now be used to solve ad-hoc language tasks through zero-shot prompting without the need for supervised training. This approach has gained popularity in recent years, and researchers have demonstrated prompts that achieve strong accuracy on specific NLP tasks. However, finding a prompt for new tasks requires experimentation. Different prompt templates with different wording choices lead to significant accuracy differences. PromptIDE allows users to experiment with prompt variations, visualize prompt performance, and iteratively optimize prompts. We developed a workflow that allows users to first focus on model feedback using small data before moving on to a large data regime that allows empirical grounding of promising prompts using quantitative measures of the task. The tool then allows easy deployment of the newly created ad-hoc models. We demonstrate the utility of PromptIDE (demo: http://prompt.vizhub.ai) and our workflow using several real-world use cases.",
            "year": 2022,
            "citationCount": 74,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A workflow that allows users to first focus on model feedback using small data before moving on to a large data regime that allows empirical grounding of promising prompts using quantitative measures of the task, and then allows easy deployment of the newly created ad-hoc models."
            },
            "score": 3
        },
        {
            "id": "591e26e292626abb994522e82aa751b22cd2c18e",
            "paperId": "591e26e292626abb994522e82aa751b22cd2c18e",
            "title": "Self-Supervised Representation Learning for Speech Using Visual Grounding and Masked Language Modeling",
            "abstract": "In this paper, we describe our submissions to the ZeroSpeech 2021 Challenge and SUPERB benchmark. Our submissions are based on the recently proposed FaST-VGS model, which is a Transformer-based model that learns to associate raw speech waveforms with semantically related images, all without the use of any transcriptions of the speech. Additionally, we introduce a novel extension of this model, FaST-VGS+, which is learned in a multi-task fashion with a masked language modeling objective in addition to the visual grounding objective. On ZeroSpeech 2021, we show that our models perform competitively on the ABX task, outperform all other concurrent submissions on the Syntactic and Semantic tasks, and nearly match the best system on the Lexical task. On the SUPERB benchmark, we show that our models also achieve strong performance, in some cases even outperforming the popular wav2vec2.0 model.",
            "year": 2022,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper describes the submissions to the ZeroSpeech 2021 Challenge and SUPERB benchmark, and introduces a novel extension of this model, FaST-VGS+, which is learned in a multi-task fashion with a masked language modeling objective in addition to the visual grounding objective."
            },
            "score": 3
        },
        {
            "id": "2379d69c98548d8c291a53a9a932b5ea7911fbe5",
            "paperId": "2379d69c98548d8c291a53a9a932b5ea7911fbe5",
            "title": "Probabilistic coherence, logical consistency, and Bayesian learning: Neural language models as epistemic agents",
            "abstract": "It is argued that suitably trained neural language models exhibit key properties of epistemic agency: they hold probabilistically coherent and logically consistent degrees of belief, which they can rationally revise in the face of novel evidence. To this purpose, we conduct computational experiments with rankers: T5 models [Raffel et al. 2020] that are pretrained on carefully designed synthetic corpora. Moreover, we introduce a procedure for eliciting a model\u2019s degrees of belief, and define numerical metrics that measure the extent to which given degrees of belief violate (probabilistic, logical, and Bayesian) rationality constraints. While pretrained rankers are found to suffer from global inconsistency (in agreement with, e.g., [Jang et al. 2021]), we observe that subsequent self-training on auto-generated texts allows rankers to gradually obtain a probabilistically coherent belief system that is aligned with logical constraints. In addition, such self-training is found to have a pivotal role in rational evidential learning, too, for it seems to enable rankers to propagate a novel evidence item through their belief systems, successively re-adjusting individual degrees of belief. All this, we conclude, confirms the Rationality Hypothesis, i.e., the claim that suitable trained NLMs may exhibit advanced rational skills. We suggest that this hypothesis has empirical, yet also normative and conceptual ramifications far beyond the practical linguistic problems NLMs have originally been designed to solve.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Rationality Hypothesis, i.e., the claim that suitable trained NLMs may exhibit advanced rational skills, is suggested that this hypothesis has empirical, yet also normative and conceptual ramifications far beyond the practical linguistic problems NLMs have originally been designed to solve."
            },
            "score": 3
        },
        {
            "id": "5a3744d3c871730661ff0f32330b0810608a9ca8",
            "paperId": "5a3744d3c871730661ff0f32330b0810608a9ca8",
            "title": "LICO: Explainable Models with Language-Image Consistency",
            "abstract": "Interpreting the decisions of deep learning models has been actively studied since the explosion of deep neural networks. One of the most convincing interpretation approaches is salience-based visual interpretation, such as Grad-CAM, where the generation of attention maps depends merely on categorical labels. Although existing interpretation methods can provide explainable decision clues, they often yield partial correspondence between image and saliency maps due to the limited discriminative information from one-hot labels. This paper develops a Language-Image COnsistency model for explainable image classification, termed LICO, by correlating learnable linguistic prompts with corresponding visual features in a coarse-to-fine manner. Specifically, we first establish a coarse global manifold structure alignment by minimizing the distance between the distributions of image and language features. We then achieve fine-grained saliency maps by applying optimal transport (OT) theory to assign local feature maps with class-specific prompts. Extensive experimental results on eight benchmark datasets demonstrate that the proposed LICO achieves a significant improvement in generating more explainable attention maps in conjunction with existing interpretation methods such as Grad-CAM. Remarkably, LICO improves the classification performance of existing models without introducing any computational overhead during inference. Source code is made available at https://github.com/ymLeiFDU/LICO.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Language-Image COnsistency model for explainable image classification, termed LICO, is developed by correlating learnable linguistic prompts with corresponding visual features in a coarse-to-fine manner and improves the classification performance of existing models without introducing any computational overhead during inference."
            },
            "score": 3
        },
        {
            "id": "23bbd94f93e360f373f78ce20f61ec3486b1923d",
            "paperId": "23bbd94f93e360f373f78ce20f61ec3486b1923d",
            "title": "Exploring Large Language Models for Multi-Modal Out-of-Distribution Detection",
            "abstract": "Out-of-distribution (OOD) detection is essential for reliable and trustworthy machine learning. Recent multi-modal OOD detection leverages textual information from in-distribution (ID) class names for visual OOD detection, yet it currently neglects the rich contextual information of ID classes. Large language models (LLMs) encode a wealth of world knowledge and can be prompted to generate descriptive features for each class. Indiscriminately using such knowledge causes catastrophic damage to OOD detection due to LLMs' hallucinations, as is observed by our analysis. In this paper, we propose to apply world knowledge to enhance OOD detection performance through selective generation from LLMs. Specifically, we introduce a consistency-based uncertainty calibration method to estimate the confidence score of each generation. We further extract visual objects from each image to fully capitalize on the aforementioned world knowledge. Extensive experiments demonstrate that our method consistently outperforms the state-of-the-art.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a consistency-based uncertainty calibration method to estimate the confidence score of each generation of large language models and extracts visual objects from each image to fully capitalize on the aforementioned world knowledge."
            },
            "score": 3
        },
        {
            "id": "c02935cb1329996dc462177094edfa38792a4a92",
            "paperId": "c02935cb1329996dc462177094edfa38792a4a92",
            "title": "Assessing the Performance of Zero-Shot Visual Question Answering in Multimodal Large Language Models for 12-Lead ECG Image Interpretation",
            "abstract": "Large Language Models (LLM) are increasingly multimodal, and Zero-Shot Visual Question Answering (VQA) shows promise for image interpretation. If zero-shot VQA can be applied to a 12-lead electrocardiogram (ECG), a prevalent diagnostic tool in the medical field, the potential benefits to the field would be substantial. This study evaluated the diagnostic performance of zero-shot VQA with multimodal LLMs on 12-lead ECG images. The results revealed that multimodal LLM tended to make more errors in extracting and verbalizing image features than in describing preconditions and making logical inferences. Even when the answers were correct, erroneous descriptions of image features were common. These findings suggest a need for improved control over image hallucination and indicate that performance evaluation using the percentage of correct answers to multiple-choice questions may not be sufficient for performance assessment in VQA tasks.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A need for improved control over image hallucination is suggested and performance evaluation using the percentage of correct answers to multiple-choice questions may not be sufficient for performance assessment in VQA tasks."
            },
            "score": 3
        },
        {
            "id": "2f231367b55f30186467158c644a9890c498e4cf",
            "paperId": "2f231367b55f30186467158c644a9890c498e4cf",
            "title": "Gemini Goes to Med School: Exploring the Capabilities of Multimodal Large Language Models on Medical Challenge Problems & Hallucinations",
            "abstract": "Large language models have the potential to be valuable in the healthcare industry, but it's crucial to verify their safety and effectiveness through rigorous evaluation. For this purpose, we comprehensively evaluated both open-source LLMs and Google's new multimodal LLM called Gemini across Medical reasoning, hallucination detection, and Medical Visual Question Answering tasks. While Gemini showed competence, it lagged behind state-of-the-art models like MedPaLM 2 and GPT-4 in diagnostic accuracy. Additionally, Gemini achieved an accuracy of 61.45\\% on the medical VQA dataset, significantly lower than GPT-4V's score of 88\\%. Our analysis revealed that Gemini is highly susceptible to hallucinations, overconfidence, and knowledge gaps, which indicate risks if deployed uncritically. We also performed a detailed analysis by medical subject and test type, providing actionable feedback for developers and clinicians. To mitigate risks, we applied prompting strategies that improved performance. Additionally, we facilitated future research and development by releasing a Python module for medical LLM evaluation and establishing a dedicated leaderboard on Hugging Face for medical domain LLMs. Python module can be found at https://github.com/promptslab/RosettaEval",
            "year": 2024,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "While Gemini showed competence, it lagged behind state-of-the-art models like MedPaLM 2 and GPT-4 in diagnostic accuracy, and was highly susceptible to hallucinations, overconfidence, and knowledge gaps, which indicate risks if deployed uncritically."
            },
            "score": 3
        },
        {
            "id": "3c101920b0b2b1bd4411b6ab27d4784e26864022",
            "paperId": "3c101920b0b2b1bd4411b6ab27d4784e26864022",
            "title": "Learning to Localize Objects Improves Spatial Reasoning in Visual-LLMs",
            "abstract": "Integration of Large Language Models (LLMs) into visual domain tasks, resulting in visual-LLMs (V-LLMs), has enabled exceptional performance in vision-language tasks, particularly for visual question answering (VQA). However, existing V-LLMs (e.g. BLIP-2, LLaVA) demonstrate weak spatial reasoning and localization awareness. Despite generating highly descriptive and elaborate textual answers, these models fail at simple tasks like distinguishing a left vs right location. In this work, we explore how image-space coordinate based instruction fine-tuning objectives could inject spatial awareness into V-LLMs. We discover optimal coordinate representations, data-efficient instruction fine-tuning objectives, and pseudo-data generation strategies that lead to improved spatial awareness in V-LLMs. Additionally, our resulting model improves VQA across image and video domains, reduces undesired hallucination, and generates better contextual object descriptions. Experiments across 5 vision-language tasks involving 14 different datasets establish the clear performance improvements achieved by our proposed framework.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores how image-space coordinate based instruction fine-tuning objectives could inject spatial awareness into V-LLMs, and discovers optimal coordinate representations, data-efficient instruction fine-tuning objectives, and pseudo-data generation strategies that lead to improved spatial awareness in V-LLMs."
            },
            "score": 3
        },
        {
            "id": "b6b5ffb9a1faee7b488eb46f2d68f62beaecb8c7",
            "paperId": "b6b5ffb9a1faee7b488eb46f2d68f62beaecb8c7",
            "title": "Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering",
            "abstract": "Knowledge-based visual question answering (KVQA) has been extensively studied to answer visual questions with external knowledge, e.g., knowledge graphs (KGs). While several attempts have been proposed to leverage large language models (LLMs) as an implicit knowledge source, it remains challenging since LLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g., images, KGs and LLMs, cannot be readily aligned for complex scenarios. To tackle these, we present a novel modality-aware integration with LLMs for KVQA (MAIL). It carefully leverages multimodal knowledge for both image understanding and knowledge reasoning. Specifically, (i) we propose a two-stage prompting strategy with LLMs to densely embody the image into a scene graph with detailed visual features; (ii) We construct a coupled concept graph by linking the mentioned entities with external facts. (iii) A tailored pseudo-siamese graph medium fusion is designed for sufficient multimodal fusion. We utilize the shared mentioned entities in two graphs as mediums to bridge a tight inter-modal exchange, while maximally preserving insightful intra-modal learning by constraining the fusion within mediums. Extensive experiments on two benchmark datasets show the superiority of MAIL with 24x less resources.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel modality-aware integration with LLMs for KVQA (MAIL) carefully leverages multimodal knowledge for both image understanding and knowledge reasoning, while maximally preserving insightful intra-modal learning by constraining the fusion within mediums."
            },
            "score": 3
        },
        {
            "id": "0d230fe2af4fe4fd2257ba2a10cfeb91126e27a3",
            "paperId": "0d230fe2af4fe4fd2257ba2a10cfeb91126e27a3",
            "title": "VIP5: Towards Multimodal Foundation Models for Recommendation",
            "abstract": "Computer Vision (CV), Natural Language Processing (NLP), and Recommender Systems (RecSys) are three prominent AI applications that have traditionally developed independently, resulting in disparate modeling and engineering methodologies. This has impeded the ability for these fields to directly benefit from each other's advancements. With the recent development of foundation models, large language models have emerged as a potential general-purpose interface for unifying different modalities and problem formulations. In light of this, we propose the development of a multimodal foundation model (MFM) considering visual, textual, and personalization modalities under the P5 recommendation paradigm, thus named VIP5 (Visual P5), to unify various modalities and recommendation tasks. This will enable the processing of multiple modalities in a shared architecture for improved recommendations. To achieve this, we introduce multimodal personalized prompts to accommodate multiple modalities under a shared format. Additionally, we propose a parameter-efficient training method for foundation models, which involves freezing the P5 backbone and fine-tuning lightweight adapters, resulting in improved recommendation performance and increased efficiency in terms of training time and memory usage. Code and data of VIP5 are available at https://github.com/jeykigung/VIP5.",
            "year": 2023,
            "citationCount": 32,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes the development of a multimodal foundation model (MFM) considering visual, textual, and personalization modalities under the P5 recommendation paradigm, thus named VIP5 (Visual P5), to unify various modalities and recommendation tasks."
            },
            "score": 3
        },
        {
            "id": "aa88d4add72db17cfc313f60374992658c949d2a",
            "paperId": "aa88d4add72db17cfc313f60374992658c949d2a",
            "title": "Do Visual-Language Maps Capture Latent Semantics?",
            "abstract": "Visual-language models (VLMs) have recently been introduced in robotic mapping by using the latent representations, i.e., embeddings, of the VLMs to represent the natural language semantics in the map. The main benefit is moving beyond a small set of human-created labels toward open-vocabulary scene understanding. While there is anecdotal evidence that maps built this way support downstream tasks, such as navigation, rigorous analysis of the quality of the maps using these embeddings is lacking. We investigate two critical properties of map quality: queryability and consistency. The evaluation of queryability addresses the ability to retrieve information from the embeddings. We investigate two aspects of consistency: intra-map consistency and inter-map consistency. Intra-map consistency captures the ability of the embeddings to represent abstract semantic classes, and inter-map consistency captures the generalization properties of the representation. In this paper, we propose a way to analyze the quality of maps created using VLMs, which forms an open-source benchmark to be used when proposing new open-vocabulary map representations. We demonstrate the benchmark by evaluating the maps created by two state-of-the-art methods, VLMaps and OpenScene, using two encoders, LSeg and OpenSeg, using real-world data from the Matterport3D data set. We find that OpenScene outperforms VLMaps with both encoders, and LSeg outperforms OpenSeg with both methods.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A way to analyze the quality of maps created using VLMs is proposed, which forms an open-source benchmark to be used when proposing new open-vocabulary map representations and investigates two critical properties of map quality: queryability and consistency."
            },
            "score": 2
        },
        {
            "id": "6c60965ca23a8ae123e2ab4ac6ef08d996ab78d5",
            "paperId": "6c60965ca23a8ae123e2ab4ac6ef08d996ab78d5",
            "title": "Language-Guided Audio-Visual Source Separation via Trimodal Consistency",
            "abstract": "We propose a self-supervised approach for learning to perform audio source separation in videos based on natu-ral language queries, using only unlabeled video and au-dio pairs as training data. A key challenge in this task is learning to associate the linguistic description of a sound-emitting object to its visual features and the corresponding components of the audio waveform, all without access to annotations during training. To overcome this challenge, we adapt off-the-shelf vision-language foundation models to provide pseudo-target supervision via two novel loss functions and encourage a stronger alignment between the audio, visual and natural language modalities. During inference, our approach can separate sounds given text, video and audio input, or given text and audio input alone. We demonstrate the effectiveness of our self-supervised approach on three audio-visual separation datasets, including MUSIC, SOLOS and AudioSet, where we outperform state-of-the-art strongly supervised approaches despite not using object detectors or text labels during training. Our project page including publicly available code can be found at https://cs-people.bu.edu/rxtan/projectsNAST.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work adapts off-the-shelf vision-language foundation models to provide pseudo-target supervision via two novel loss functions and encourage a stronger alignment between the audio, visual and natural language modalities to perform audio source separation in videos based on natu-ral language queries."
            },
            "score": 2
        },
        {
            "id": "fb80a0b1ed781728941f778c7b86afe244b2f0ea",
            "paperId": "fb80a0b1ed781728941f778c7b86afe244b2f0ea",
            "title": "Integrating Supervised Extractive and Generative Language Models for Suicide Risk Evidence Summarization",
            "abstract": "We propose a method that integrates supervised extractive and generative language models for providing supporting evidence of suicide risk in the CLPsych 2024 shared task. Our approach comprises three steps. Initially, we construct a BERT-based model for estimating sentence-level suicide risk and negative sentiment. Next, we precisely identify high suicide risk sentences by emphasizing elevated probabilities of both suicide risk and negative sentiment. Finally, we integrate generative summaries using the MentaLLaMa framework and extractive summaries from identified high suicide risk sentences and a specialized dictionary of suicidal risk words. SophiaADS, our team, achieved 1st place for highlight extraction and ranked 10th for summary generation, both based on recall and consistency metrics, respectively.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A method that integrates supervised extractive and generative language models for providing supporting evidence of suicide risk in the CLPsych 2024 shared task and achieves 1st place for highlight extraction and ranked 10th for summary generation, both based on recall and consistency metrics."
            },
            "score": 2
        },
        {
            "id": "4b84cb46a704cc3a978758d8bf09fff25ed71a5a",
            "paperId": "4b84cb46a704cc3a978758d8bf09fff25ed71a5a",
            "title": "Inferring cancer disease response from radiology reports using large language models with data augmentation and prompting",
            "abstract": "Abstract Objective To assess large language models on their ability to accurately infer cancer disease response from free-text radiology reports. Materials and Methods We assembled 10 602 computed tomography reports from cancer patients seen at a single institution. All reports were classified into: no evidence of disease, partial response, stable disease, or progressive disease. We applied transformer models, a bidirectional long short-term memory model, a convolutional neural network model, and conventional machine learning methods to this task. Data augmentation using sentence permutation with consistency loss as well as prompt-based fine-tuning were used on the best-performing models. Models were validated on a hold-out test set and an external validation set based on Response Evaluation Criteria in Solid Tumors (RECIST) classifications. Results The best-performing model was the GatorTron transformer which achieved an accuracy of 0.8916 on the test set and 0.8919 on the RECIST validation set. Data augmentation further improved the accuracy to 0.8976. Prompt-based fine-tuning did not further improve accuracy but was able to reduce the number of training reports to 500 while still achieving good performance. Discussion These models could be used by researchers to derive progression-free survival in large datasets. It may also serve as a decision support tool by providing clinicians an automated second opinion of disease response. Conclusions Large clinical language models demonstrate potential to infer cancer disease response from radiology reports at scale. Data augmentation techniques are useful to further improve performance. Prompt-based fine-tuning can significantly reduce the size of the training dataset.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Large clinical language models demonstrate potential to infer cancer disease response from radiology reports at scale by applying transformer models, a bidirectional long short-term memory model, a convolutional neural network model, and conventional machine learning methods."
            },
            "score": 2
        },
        {
            "id": "80a5aec9d2a972a1c4b4ead082ff256dc1115efb",
            "paperId": "80a5aec9d2a972a1c4b4ead082ff256dc1115efb",
            "title": "RCAgent: Cloud Root Cause Analysis by Autonomous Agents with Tool-Augmented Large Language Models",
            "abstract": "Large language model (LLM) applications in cloud root cause analysis (RCA) have been actively explored recently. However, current methods are still reliant on manual workflow settings and do not unleash LLMs' decision-making and environment interaction capabilities. We present RCAgent, a tool-augmented LLM autonomous agent framework for practical and privacy-aware industrial RCA usage. Running on an internally deployed model rather than GPT families, RCAgent is capable of free-form data collection and comprehensive analysis with tools. Our framework combines a variety of enhancements, including a unique Self-Consistency for action trajectories, and a suite of methods for context management, stabilization, and importing domain knowledge. Our experiments show RCAgent's evident and consistent superiority over ReAct across all aspects of RCA -- predicting root causes, solutions, evidence, and responsibilities -- and tasks covered or uncovered by current rules, as validated by both automated metrics and human evaluations. Furthermore, RCAgent has already been integrated into the diagnosis and issue discovery workflow of the Real-time Compute Platform for Apache Flink of Alibaba Cloud.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "RCAgent is presented, a tool-augmented LLM autonomous agent framework for practical and privacy-aware industrial RCA usage and shows RCAgent's evident and consistent superiority over ReAct across all aspects of RCA."
            },
            "score": 2
        },
        {
            "id": "d1313748a791e7ac21e4310d9b0e0d2febb8ab56",
            "paperId": "d1313748a791e7ac21e4310d9b0e0d2febb8ab56",
            "title": "Instruction Makes a Difference",
            "abstract": "We introduce Instruction Document Visual Question Answering (iDocVQA) dataset and Large Language Document (LLaDoc) model, for training Language-Vision (LV) models for document analysis and predictions on document images, respectively. Usually, deep neural networks for the DocVQA task are trained on datasets lacking instructions. We show that using instruction-following datasets improves performance. We compare performance across document-related datasets using the recent state-of-the-art (SotA) Large Language and Vision Assistant (LLaVA)1.5 as the base model. We also evaluate the performance of the derived models for object hallucination using the Polling-based Object Probing Evaluation (POPE) dataset. The results show that instruction-tuning performance ranges from 11X to 32X of zero-shot performance and from 0.1% to 4.2% over non-instruction (traditional task) finetuning. Despite the gains, these still fall short of human performance (94.36%), implying there's much room for improvement.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Instruction Document Visual Question Answering dataset and Large Language Document model are introduced, for training Language-Vision models for document analysis and predictions on document images, respectively, and it is shown that using instruction-following datasets improves performance."
            },
            "score": 2
        },
        {
            "id": "785650a805851c7e945523e495c5a523c60f72a4",
            "paperId": "785650a805851c7e945523e495c5a523c60f72a4",
            "title": "Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models",
            "abstract": "Medical Visual Question Answering (VQA) is an important challenge, as it would lead to faster and more accurate diagnoses and treatment decisions. Most existing methods approach it as a multi-class classification problem, which restricts the outcome to a predefined closed-set of curated answers. We focus on open-ended VQA and motivated by the recent advances in language models consider it as a generative task. Leveraging pre-trained language models, we introduce a novel method particularly suited for small, domain-specific, medical datasets. To properly communicate the medical images to the language model, we develop a network that maps the extracted visual features to a set of learnable tokens. Then, alongside the question, these learnable tokens directly prompt the language model. We explore recent parameter-efficient fine-tuning strategies for language models, which allow for resource- and data-efficient fine-tuning. We evaluate our approach on the prime medical VQA benchmarks, namely, Slake, OVQA and PathVQA. The results demonstrate that our approach outperforms existing methods across various training settings while also being computationally efficient.",
            "year": 2023,
            "citationCount": 21,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work focuses on open-ended VQA and motivated by the recent advances in language models consider it as a generative task, and introduces a novel method particularly suited for small, domain-specific, medical datasets."
            },
            "score": 2
        },
        {
            "id": "a970c8fadef8497576660b288c52c0ec8eebdc12",
            "paperId": "a970c8fadef8497576660b288c52c0ec8eebdc12",
            "title": "Zero-Shot Video Question Answering via Frozen Bidirectional Language Models",
            "abstract": "Video question answering (VideoQA) is a complex task that requires diverse multi-modal data for training. Manual annotation of question and answers for videos, however, is tedious and prohibits scalability. To tackle this problem, recent methods consider zero-shot settings with no manual annotation of visual question-answer. In particular, a promising approach adapts frozen autoregressive language models pretrained on Web-scale text-only data to multi-modal inputs. In contrast, we here build on frozen bidirectional language models (BiLM) and show that such an approach provides a stronger and cheaper alternative for zero-shot VideoQA. In particular, (i) we combine visual inputs with the frozen BiLM using light trainable modules, (ii) we train such modules using Web-scraped multi-modal data, and finally (iii) we perform zero-shot VideoQA inference through masked language modeling, where the masked text is the answer to a given question. Our proposed approach, FrozenBiLM, outperforms the state of the art in zero-shot VideoQA by a significant margin on a variety of datasets, including LSMDC-FiB, iVQA, MSRVTT-QA, MSVD-QA, ActivityNet-QA, TGIF-FrameQA, How2QA and TVQA. It also demonstrates competitive performance in the few-shot and fully-supervised setting. Our code and models are publicly available at https://github.com/antoyang/FrozenBiLM.",
            "year": 2022,
            "citationCount": 130,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work builds on frozen bidirectional language models (BiLM) and shows that such an approach provides a stronger and cheaper alternative for zero-shot VideoQA and demonstrates competitive performance in the few-shot and fully-supervised setting."
            },
            "score": 2
        },
        {
            "id": "bb4516ad6eb7adda97d81f09d4bb92b3ad056c42",
            "paperId": "bb4516ad6eb7adda97d81f09d4bb92b3ad056c42",
            "title": "Large Language Models are Temporal and Causal Reasoners for Video Question Answering",
            "abstract": "Large Language Models (LLMs) have shown remarkable performances on a wide range of natural language understanding and generation tasks. We observe that the LLMs provide effective priors in exploiting $\\textit{linguistic shortcuts}$ for temporal and causal reasoning in Video Question Answering (VideoQA). However, such priors often cause suboptimal results on VideoQA by leading the model to over-rely on questions, $\\textit{i.e.}$, $\\textit{linguistic bias}$, while ignoring visual content. This is also known as `ungrounded guesses' or `hallucinations'. To address this problem while leveraging LLMs' prior on VideoQA, we propose a novel framework, Flipped-VQA, encouraging the model to predict all the combinations of $\\langle$V, Q, A$\\rangle$ triplet by flipping the source pair and the target label to understand their complex relationships, $\\textit{i.e.}$, predict A, Q, and V given a VQ, VA, and QA pairs, respectively. In this paper, we develop LLaMA-VQA by applying Flipped-VQA to LLaMA, and it outperforms both LLMs-based and non-LLMs-based models on five challenging VideoQA benchmarks. Furthermore, our Flipped-VQA is a general framework that is applicable to various LLMs (OPT and GPT-J) and consistently improves their performances. We empirically demonstrate that Flipped-VQA not only enhances the exploitation of linguistic shortcuts but also mitigates the linguistic bias, which causes incorrect answers over-relying on the question. Code is available at https://github.com/mlvlab/Flipped-VQA.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LLaMA-VQA is developed by applying Flipped-V QA to LLaMA, and it outperforms both LLMs- based and non-LLMs-based models on five challenging VideoQA benchmarks."
            },
            "score": 2
        },
        {
            "id": "3c8cc9a5ee373d51e0bf71621b6eb6901c762e8f",
            "paperId": "3c8cc9a5ee373d51e0bf71621b6eb6901c762e8f",
            "title": "DriveLM: Driving with Graph Visual Question Answering",
            "abstract": "We study how vision-language models (VLMs) trained on web-scale data can be integrated into end-to-end driving systems to boost generalization and enable interactivity with human users. While recent approaches adapt VLMs to driving via single-round visual question answering (VQA), human drivers reason about decisions in multiple steps. Starting from the localization of key objects, humans estimate object interactions before taking actions. The key insight is that with our proposed task, Graph VQA, where we model graph-structured reasoning through perception, prediction and planning question-answer pairs, we obtain a suitable proxy task to mimic the human reasoning process. We instantiate datasets (DriveLM-Data) built upon nuScenes and CARLA, and propose a VLM-based baseline approach (DriveLM-Agent) for jointly performing Graph VQA and end-to-end driving. The experiments demonstrate that Graph VQA provides a simple, principled framework for reasoning about a driving scene, and DriveLM-Data provides a challenging benchmark for this task. Our DriveLM-Agent baseline performs end-to-end autonomous driving competitively in comparison to state-of-the-art driving-specific architectures. Notably, its benefits are pronounced when it is evaluated zero-shot on unseen objects or sensor configurations. We hope this work can be the starting point to shed new light on how to apply VLMs for autonomous driving. To facilitate future research, all code, data, and models are available to the public.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work instantiate datasets built upon nuScenes and CARLA, and proposes a VLM-based baseline approach for jointly performing Graph VQA and end-to-end driving, demonstrating that Graph VQA provides a simple, principled framework for reasoning about a driving scene, and DriveLM-Data provides a challenging benchmark for this task."
            },
            "score": 2
        },
        {
            "id": "84f53210891bfbdce3336c855b5bd0c0381270cb",
            "paperId": "84f53210891bfbdce3336c855b5bd0c0381270cb",
            "title": "Kiki or Bouba? Sound Symbolism in Vision-and-Language Models",
            "abstract": "Although the mapping between sound and meaning in human language is assumed to be largely arbitrary, research in cognitive science has shown that there are non-trivial correlations between particular sounds and meanings across languages and demographic groups, a phenomenon known as sound symbolism. Among the many dimensions of meaning, sound symbolism is particularly salient and well-demonstrated with regards to cross-modal associations between language and the visual domain. In this work, we address the question of whether sound symbolism is reflected in vision-and-language models such as CLIP and Stable Diffusion. Using zero-shot knowledge probing to investigate the inherent knowledge of these models, we find strong evidence that they do show this pattern, paralleling the well-known kiki-bouba effect in psycholinguistics. Our work provides a novel method for demonstrating sound symbolism and understanding its nature using computational tools. Our code will be made publicly available.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work addresses the question of whether sound symbolism is reflected in vision-and-language models such as CLIP and Stable Diffusion and finds strong evidence that they do show this pattern, paralleling the well-known kiki-bouba effect in psycholinguistics."
            },
            "score": 1
        }
    ],
    "novelty": "yes"
}