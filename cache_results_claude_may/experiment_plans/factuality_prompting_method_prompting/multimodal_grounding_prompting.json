{
    "topic_description": "novel prompting methods that can improve factuality and reduce hallucination of large language models",
    "idea_name": "Multimodal Grounding Prompting",
    "raw_idea": {
        "Problem": "Language models can generate statements that are inconsistent with visual or multimodal evidence, e.g. image captioning models can hallucinate objects not actually present in an image.",
        "Existing Methods": "Some methods use visual retrieval or visual QA models to ground text generation in visual evidence.",
        "Motivation": "Language models can be prompted to reason about visual inputs, and to cross-reference textual claims against visual evidence. This can help ground generation and reduce hallucination.",
        "Proposed Method": "The procedure is: 1) Given a query and an image, prompt the model to generate a claim relevant to the image. 2) Prompt the model to assess whether the generated claim is consistent with the visual evidence, and to identify any inconsistencies. 3) If inconsistencies are found, prompt the model to revise the claim to be more consistent with the image. 4) Repeat steps 2-3 until the claim is fully consistent.",
        "Experiment Plan": "Evaluate on visual storytelling and visual QA datasets. Compare to baselines like direct prompting and retrieve-and-generate. Metrics include consistency scores between images and generated text, and human evaluation of factual alignment."
    },
    "full_experiment_plan": {
        "Title": "Grounding Language Models in Visual Evidence to Reduce Hallucination",
        "Problem Statement": "Language models can generate statements that are inconsistent with visual or multimodal evidence, e.g. image captioning models can hallucinate objects not actually present in an image. This leads to factually incorrect outputs that are not properly grounded in the given evidence.",
        "Motivation": "Existing methods attempt to ground text generation by using visual retrieval or visual question answering models to incorporate visual information. However, these methods rely on external models and datasets. We propose a simpler approach that directly prompts language models to reason about the consistency between textual claims and visual evidence. Large language models have shown the ability to reason about and incorporate instructions and task descriptions. By explicitly prompting them to cross-reference claims against visual evidence and revise inconsistent statements, we aim to improve the factual grounding of the generated text while relying solely on the language model itself.",
        "Proposed Method": "Our proposed procedure, termed visual grounding prompts, consists of the following steps:\n1. Given a query and an image, prompt the language model to generate a claim relevant to the image.\n2. Prompt the model to assess whether the generated claim is consistent with the visual evidence present in the image, and to identify any inconsistencies.\n3. If inconsistencies are found, prompt the model to revise the claim to be more consistent with the image.\n4. Repeat steps 2-3 until the claim is fully consistent with the visual evidence or a maximum number of iterations is reached.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "Evaluate the proposed method on two types of datasets:\n1. Visual storytelling datasets like VIST and VisualCOMET, where the goal is to generate a coherent story or description based on a sequence of images. These datasets allow testing the factual alignment between generated text and images.\n2. Visual question answering datasets like VQAv2 and GQA, where the goal is to answer questions about an image. These datasets allow evaluating the ability to generate accurate answers grounded in visual evidence.",
            "Step 2: Construct Prompts": "We compare the proposed method to two baselines:\n1. Direct prompting: Given an image and a query, directly prompt the model to generate a response without any additional instructions.\n2. Retrieve-and-generate: Use a separate image encoder to retrieve relevant images from a database, and condition the language model on both the query and the retrieved images to generate a response.\nFor the proposed visual grounding prompts, we construct the following prompts for each step:\n1. Generate claim: \"Given the image: [image] and the query: [query], generate a claim that is relevant to the image.\"\n2. Assess consistency: \"Given the image: [image] and the generated claim: [claim], assess whether the claim is consistent with the visual evidence in the image. Identify any inconsistencies.\"\n3. Revise claim: \"Given the image: [image], the original claim: [claim], and the identified inconsistencies: [inconsistencies], revise the claim to be more consistent with the image.\"",
            "Step 3: Select Models": "Evaluate the proposed method using GPT-3.5 (text-davinci-002) and GPT-4 models via the OpenAI API. For the retrieve-and-generate baseline, use a pre-trained CLIP model to encode the images.",
            "Step 4: Collect Results": "For each example in the evaluation datasets, generate a response using the baseline methods and the proposed visual grounding prompts. For the proposed method, record the initial claim, the identified inconsistencies at each iteration, and the final revised claim.",
            "Step 5: Analyze Results": "Compute the following metrics:\n1. Consistency score: Use a pre-trained visual-language model (e.g., CLIP) to compute the cosine similarity between the generated text and the image. Higher scores indicate better alignment between the text and visual evidence.\n2. Factual accuracy: For visual QA datasets, compute the accuracy of the generated answers using the provided ground-truth answers.\n3. Human evaluation: Conduct a human evaluation study where raters assess the factual alignment between the generated text and the corresponding images on a Likert scale (e.g., 1-5). Also, ask raters to identify any inconsistencies or hallucinated entities in the generated text.\nCompare the performance of the proposed visual grounding prompts to the baselines across these metrics. Analyze the types of inconsistencies identified by the model and how well it is able to revise the claims based on visual evidence."
        },
        "Test Case Examples": {
            "Test Case 1": {
                "Image": "An image of a person riding a bicycle on a city street.",
                "Query": "What is the person doing?",
                "Baseline Prompt (Direct)": "The person is riding a motorcycle on a city street.",
                "Baseline Prompt (Retrieve-and-Generate)": "The person is riding a bicycle on a busy city street.",
                "Visual Grounding Prompt (Step 1)": "A person is riding a motorcycle on a city street.",
                "Visual Grounding Prompt (Step 2)": "The claim mentions a motorcycle, but the image clearly shows a bicycle. This is inconsistent.",
                "Visual Grounding Prompt (Step 3)": "A person is riding a bicycle on a city street.",
                "Explanation": "The direct prompting baseline hallucinates a motorcycle which is not present in the image. The retrieve-and-generate baseline produces a more factual output by incorporating visual information from similar retrieved images. However, the proposed visual grounding prompts explicitly identify the inconsistency between the claim and the image, and revise the claim to mention the bicycle, which is more factually accurate."
            },
            "Test Case 2": {
                "Image": "An image of a group of people sitting around a table eating pizza.",
                "Query": "What are the people eating?",
                "Baseline Prompt (Direct)": "The people are eating hamburgers and fries.",
                "Baseline Prompt (Retrieve-and-Generate)": "The people are eating pizza.",
                "Visual Grounding Prompt (Step 1)": "The people are eating hamburgers.",
                "Visual Grounding Prompt (Step 2)": "The claim mentions hamburgers, but the image shows the people eating pizza. This is inconsistent.",
                "Visual Grounding Prompt (Step 3)": "The people are eating pizza.",
                "Explanation": "The direct prompting baseline hallucinates hamburgers and fries which are not present in the image. The retrieve-and-generate baseline produces the correct answer by leveraging visual information. The proposed visual grounding prompts identify the inconsistency between the claim and the image, and revise the claim to accurately mention pizza."
            }
        },
        "Fallback Plan": "If the proposed visual grounding prompts do not show significant improvements over the baselines, we can conduct the following additional analyses:\n1. Analyze the quality of the identified inconsistencies. Check if the model is able to accurately detect inconsistencies between the claims and the visual evidence. If the inconsistencies are not being properly identified, explore alternative prompts or techniques for assessing consistency.\n2. Analyze the quality of the revised claims. Check if the model is making meaningful revisions to the claims based on the identified inconsistencies. If the revisions are not improving factual alignment, explore alternative prompts or strategies for guiding the model to make more effective revisions.\n3. Conduct an error analysis to identify common failure modes of the proposed method. Categorize the types of errors (e.g., object hallucination, attribute hallucination, relation hallucination) and analyze their frequency. This can provide insights into the limitations of the method and guide future improvements.\n4. Explore variations of the proposed method, such as using different visual encoders, incorporating additional visual reasoning steps, or using alternative prompting strategies.\nIf the proposed method still does not yield promising results after these additional analyses, we can pivot the project to focus on understanding the limitations of language models in grounding text generation in visual evidence. The analyses mentioned above can form the basis for an insightful study on the challenges and opportunities in this area."
    }
}