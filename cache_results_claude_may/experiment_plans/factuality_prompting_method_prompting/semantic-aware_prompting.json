{
    "topic_description": "novel prompting methods that can improve factuality and reduce hallucination of large language models",
    "idea_name": "Semantic-Aware Prompting",
    "raw_idea": {
        "Problem": "Large language models often generate responses that are semantically inconsistent with the input prompt, leading to factual errors and hallucinations.",
        "Existing Methods": "Current methods for reducing hallucination include using retrieval-augmented generation and incorporating external knowledge bases.",
        "Motivation": "We hypothesize that by explicitly modeling the semantic relationships between the input prompt and the generated response, we can guide the model to generate more factually consistent responses.",
        "Proposed Method": "We propose Semantic-Aware Prompting (SAP), a novel prompting method that incorporates semantic information into the prompt to guide the model's generation process. SAP consists of three main steps: 1) Semantic Parsing: We first parse the input prompt into a structured semantic representation using a pre-trained semantic parser. 2) Semantic Augmentation: We then augment the original prompt with the extracted semantic information, such as named entities, relations, and events. 3) Semantic Consistency Checking: During the generation process, we periodically check the semantic consistency between the generated response and the augmented prompt, and guide the model to generate responses that are semantically aligned with the input.",
        "Experiment Plan": "We plan to evaluate SAP on a range of factual question answering and knowledge-intensive generation tasks, such as TruthfulQA, FEVER, and WikiBio. We will compare SAP with state-of-the-art baselines, including retrieval-augmented generation and knowledge-grounded generation methods. We will also conduct ablation studies to investigate the effectiveness of each component in SAP."
    },
    "full_experiment_plan": {
        "Title": "Semantic-Aware Prompting: Incorporating Semantic Information for Factual Consistency in Language Models",
        "Problem Statement": "Large language models often generate responses that are semantically inconsistent with the input prompt, leading to factual errors and hallucinations.",
        "Motivation": "Current methods for reducing hallucination, such as retrieval-augmented generation and incorporating external knowledge bases, still struggle with factual inconsistencies. We hypothesize that by explicitly modeling the semantic relationships between the input prompt and the generated response, we can guide the model to generate more factually consistent responses. Our approach is inspired by the success of semantic parsing in capturing the structured meaning of natural language, and the ability of large language models to follow instructions and incorporate additional information.",
        "Proposed Method": "We propose Semantic-Aware Prompting (SAP), a novel prompting method that incorporates semantic information into the prompt to guide the model's generation process. SAP consists of three main steps:\n1. Semantic Parsing: We first parse the input prompt into a structured semantic representation using a pre-trained semantic parser.\n2. Semantic Augmentation: We then augment the original prompt with the extracted semantic information, such as named entities, relations, and events.\n3. Semantic Consistency Checking: During the generation process, we periodically check the semantic consistency between the generated response and the augmented prompt, and guide the model to generate responses that are semantically aligned with the input.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We evaluate SAP on a range of factual question answering and knowledge-intensive generation tasks, including TruthfulQA, FEVER, and WikiBio. These datasets cover different aspects of factual consistency, such as verifying claims against evidence, generating factual summaries, and answering questions truthfully. We use standard metrics such as accuracy, BLEU, and ROUGE for evaluation.",
            "Step 2: Construct Prompts": "For each dataset, we construct the following prompts:\n1. Baseline: The original input prompt without any modifications.\n2. SAP: The input prompt augmented with semantic information extracted by the semantic parser. The specific format of the augmented prompt is as follows:\n<original_prompt>\nSemantic Information:\n<semantic_parse>\nGenerate a response that is semantically consistent with the above information.\nHere, <original_prompt> is the input prompt, and <semantic_parse> is the output of the semantic parser, which includes named entities, relations, and events extracted from the prompt.",
            "Step 3: Select Models": "We experiment with GPT-3.5 (text-davinci-003) and GPT-4 from the OpenAI API. These models have shown strong performance on a wide range of natural language tasks and are capable of following instructions and incorporating additional information.",
            "Step 4: Get Results": "For each dataset and model combination, we generate responses using both the baseline and SAP prompts. We then evaluate the generated responses using the corresponding metrics for each dataset (e.g., accuracy for TruthfulQA, BLEU and ROUGE for FEVER and WikiBio).",
            "Step 5: Analyze Results": "We compare the performance of SAP against the baseline to determine if incorporating semantic information improves factual consistency. We also conduct a qualitative analysis of the generated responses to identify specific cases where SAP helps to reduce factual errors and hallucinations. Additionally, we perform an error analysis to understand the limitations of SAP and identify potential areas for improvement."
        },
        "Test Case Examples": {
            "Baseline Prompt Input": "What is the capital of France?",
            "Baseline Prompt Expected Output": "The capital of France is Paris, but it is also known for its fashion, art, and cuisine.",
            "Proposed Prompt Input (SAP)": "What is the capital of France?\nSemantic Information:\n- France: country\n- capital: relation\nGenerate a response that is semantically consistent with the above information.",
            "Proposed Prompt Expected Output (SAP)": "The capital of France is Paris.",
            "Explanation": "The baseline prompt generates a response that, while factually correct, includes additional information that is not directly relevant to the question. By incorporating semantic information and explicitly instructing the model to generate a semantically consistent response, SAP helps to focus the model's output on the most relevant facts, reducing the potential for hallucination."
        },
        "Fallback Plan": "If SAP does not significantly improve factual consistency compared to the baseline, we can explore the following alternatives:\n1. Analyze the quality of the semantic parses to determine if errors in the semantic parsing step are limiting the effectiveness of SAP. We can experiment with different semantic parsing models or techniques to improve the accuracy of the extracted semantic information.\n2. Investigate alternative methods for incorporating semantic information into the prompts, such as using different prompt formats or providing more explicit instructions for maintaining semantic consistency.\n3. Conduct a more detailed error analysis to identify specific types of factual errors that SAP struggles with, and develop targeted strategies for addressing those errors.\n4. Explore the use of SAP in combination with other techniques for reducing hallucination, such as retrieval-augmented generation or incorporating external knowledge bases, to see if the combined approach yields better results.\nIf these alternative approaches still do not yield significant improvements, we can focus on analyzing the limitations of SAP and the specific challenges of maintaining factual consistency in language models. This analysis can provide valuable insights for future research on reducing hallucination and improving the factual accuracy of generated responses."
    },
    "novelty_queries": [
        "KeywordQuery(\"semantic parsing prompting language models\")",
        "KeywordQuery(\"semantic consistency language models\")",
        "KeywordQuery(\"factual consistency prompting language models\")",
        "KeywordQuery(\"semantic information prompting hallucination reduction\")",
        "KeywordQuery(\"Semantic-Aware Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "03d131aaf8a933c0c1fdc42530210449958b5036",
            "paperId": "03d131aaf8a933c0c1fdc42530210449958b5036",
            "title": "Semantic Consistency for Assuring Reliability of Large Language Models",
            "abstract": "Large Language Models (LLMs) exhibit remarkable fluency and competence across various natural language tasks. However, recent research has highlighted their sensitivity to variations in input prompts. To deploy LLMs in a safe and reliable manner, it is crucial for their outputs to be consistent when prompted with expressions that carry the same meaning or intent. While some existing work has explored how state-of-the-art LLMs address this issue, their evaluations have been confined to assessing lexical equality of single- or multi-word answers, overlooking the consistency of generative text sequences. For a more comprehensive understanding of the consistency of LLMs in open-ended text generation scenarios, we introduce a general measure of semantic consistency, and formulate multiple versions of this metric to evaluate the performance of various LLMs. Our proposal demonstrates significantly higher consistency and stronger correlation with human evaluations of output consistency than traditional metrics based on lexical consistency. Finally, we propose a novel prompting strategy, called Ask-to-Choose (A2C), to enhance semantic consistency. When evaluated for closed-book question answering based on answer variations from the TruthfulQA benchmark, A2C increases accuracy metrics for pretrained and finetuned LLMs by up to 47%, and semantic consistency metrics for instruction-tuned models by up to 7-fold.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A general measure of semantic consistency is introduced, and multiple versions of this metric are formulated to evaluate the performance of various LLMs, and a novel prompting strategy is proposed, called Ask-to-Choose (A2C), to enhance semantic consistency."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is improving factual consistency in language model outputs by incorporating semantic information into prompts. The approach is to parse the input prompt into a semantic representation, augment the prompt with the extracted semantic information, and check for semantic consistency during generation.\n\nThe research problem in the paper is improving the semantic consistency of language model outputs across prompts with the same meaning or intent. The approach is to introduce a general measure of semantic consistency and propose a novel prompting strategy called Ask-to-Choose (A2C) to enhance semantic consistency.\n\nWhile both works aim to improve the consistency of language model outputs, the proposal focuses specifically on factual consistency and incorporates semantic information into prompts, while the paper addresses semantic consistency across prompts with the same meaning and proposes a new prompting strategy and evaluation metric.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "9a5f6fc16f00270c0baca335a8105b48824e6d4a",
            "paperId": "9a5f6fc16f00270c0baca335a8105b48824e6d4a",
            "title": "Measuring Reliability of Large Language Models through Semantic Consistency",
            "abstract": "While large pretrained language models (PLMs) demonstrate incredible fluency and performance on many natural language tasks, recent work has shown that well-performing PLMs are very sensitive to what prompts are feed into them. Even when prompts are semantically identical, language models may give very different answers. When considering safe and trustworthy deployments of PLMs we would like their outputs to be consistent under prompts that mean the same thing or convey the same intent. While some work has looked into how state-of-the-art PLMs address this need, they have been limited to only evaluating lexical equality of single- or multi-word answers and do not address consistency of generative text sequences. In order to understand consistency of PLMs under text generation settings, we develop a measure of semantic consistency that allows the comparison of open-ended text outputs. We implement several versions of this consistency metric to evaluate the performance of a number of PLMs on paraphrased versions of questions in the TruthfulQA dataset, we find that our proposed metrics are considerably more consistent than traditional metrics embodying lexical consistency, and also correlate with human evaluation of output consistency to a higher degree.",
            "year": 2022,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A measure of semantic consistency that allows the comparison of open-ended text outputs is developed that is considerably more consistent than traditional metrics embodying lexical consistency, and also correlates with human evaluation of output consistency to a higher degree."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is improving factual consistency in language model outputs by incorporating semantic information into prompts. The approach is to parse the input prompt into a semantic representation, augment the prompt with the extracted semantic information, and check for semantic consistency during generation.\n\nThe research problem in the paper is measuring the semantic consistency of language model outputs under semantically identical prompts. The approach is to develop a measure of semantic consistency that compares open-ended text outputs and evaluate the performance of language models on paraphrased questions.\n\nWhile both works address the issue of consistency in language model outputs, the proposal focuses on improving factual consistency through semantic prompting, while the paper focuses on measuring semantic consistency under paraphrased prompts. The approaches are different: the proposal uses semantic parsing and augmentation, while the paper develops a semantic consistency metric.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "2cbdd4ef5a14dbf3446e1d5b0ecf4b222ded820d",
            "paperId": "2cbdd4ef5a14dbf3446e1d5b0ecf4b222ded820d",
            "title": "Semantic Parsing by Large Language Models for Intricate Updating Strategies of Zero-Shot Dialogue State Tracking",
            "abstract": "Zero-shot Dialogue State Tracking (DST) addresses the challenge of acquiring and annotating task-oriented dialogues, which can be time-consuming and costly. However, DST extends beyond simple slot-filling and requires effective updating strategies for tracking dialogue state as conversations progress. In this paper, we propose ParsingDST, a new In-Context Learning (ICL) method, to introduce additional intricate updating strategies in zero-shot DST. Our approach reformulates the DST task by leveraging powerful Large Language Models (LLMs) and translating the original dialogue text to JSON through semantic parsing as an intermediate state. We also design a novel framework that includes more modules to ensure the effectiveness of updating strategies in the text-to-JSON process. Experimental results demonstrate that our approach outperforms existing zero-shot DST methods on MultiWOZ, exhibiting significant improvements in Joint Goal Accuracy (JGA) and slot accuracy compared to existing ICL methods. Our code has been released.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ParsingDST, a new In-Context Learning (ICL) method, is proposed to introduce additional intricate updating strategies in zero-shot DST, reformulates the DST task by leveraging powerful Large Language Models (LLMs) and translating the original dialogue text to JSON through semantic parsing as an intermediate state."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving factual consistency in language model outputs by incorporating semantic information, while the approach is to use semantic parsing to augment the input prompts.\n\nThe research problem in the paper is improving zero-shot dialogue state tracking by introducing intricate updating strategies, and the approach is to reformulate the task by using large language models to translate dialogue text to JSON through semantic parsing.\n\nAlthough both works involve semantic parsing, the research problems and high-level approaches are different. The proposal focuses on factual consistency in general language model outputs, while the paper specifically targets zero-shot dialogue state tracking.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "3867d999b550b57e6762f9d4b0114ee7551b2e2f",
            "paperId": "3867d999b550b57e6762f9d4b0114ee7551b2e2f",
            "title": "Predicting Question-Answering Performance of Large Language Models through Semantic Consistency",
            "abstract": "Semantic consistency of a language model is broadly defined as the model\u2019s ability to produce semantically-equivalent outputs, given semantically-equivalent inputs. We address the task of assessing question-answering (QA) semantic consistency of contemporary large language models (LLMs) by manually creating a benchmark dataset with high-quality paraphrases for factual questions, and release the dataset to the community.We further combine the semantic consistency metric with additional measurements suggested in prior work as correlating with LLM QA accuracy, for building and evaluating a framework for factual QA reference-less performance prediction \u2013 predicting the likelihood of a language model to accurately answer a question. Evaluating the framework on five contemporary LLMs, we demonstrate encouraging, significantly outperforming baselines, results.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work addresses the task of assessing question-answering (QA) semantic consistency of contemporary large language models (LLMs) by manually creating a benchmark dataset with high-quality paraphrases for factual questions, and releases the dataset to the community."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the factual consistency of language model outputs by incorporating semantic information into the prompts, while the paper focuses on predicting the question-answering performance of language models using semantic consistency metrics.\n\nProject Proposal:\n- Problem: Language models generate semantically inconsistent responses, leading to factual errors.\n- Approach: Incorporate semantic information into prompts to guide the model's generation process.\n\nPaper:\n- Problem: Assessing question-answering semantic consistency of language models.\n- Approach: Create a benchmark dataset with paraphrases for factual questions and combine semantic consistency metrics with other measurements to predict QA accuracy.\n\nThe two works address different problems and propose different approaches. The project proposal focuses on improving factual consistency during generation, while the paper aims to predict QA performance using semantic consistency metrics.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "0c0ec45aab03428f732c2191315a2a3af4ea05b3",
            "paperId": "0c0ec45aab03428f732c2191315a2a3af4ea05b3",
            "title": "DCR-Consistency: Divide-Conquer-Reasoning for Consistency Evaluation and Improvement of Large Language Models",
            "abstract": "Evaluating the quality and variability of text generated by Large Language Models (LLMs) poses a significant, yet unresolved research challenge. Traditional evaluation methods, such as ROUGE and BERTScore, which measure token similarity, often fail to capture the holistic semantic equivalence. This results in a low correlation with human judgments and intuition, which is especially problematic in high-stakes applications like healthcare and finance where reliability, safety, and robust decision-making are highly critical. This work proposes DCR, an automated framework for evaluating and improving the consistency of LLM-generated texts using a divide-conquer-reasoning approach. Unlike existing LLM-based evaluators that operate at the paragraph level, our method employs a divide-and-conquer evaluator (DCE) that breaks down the paragraph-to-paragraph comparison between two generated responses into individual sentence-to-paragraph comparisons, each evaluated based on predefined criteria. To facilitate this approach, we introduce an automatic metric converter (AMC) that translates the output from DCE into an interpretable numeric score. Beyond the consistency evaluation, we further present a reason-assisted improver (RAI) that leverages the analytical reasons with explanations identified by DCE to generate new responses aimed at reducing these inconsistencies. Through comprehensive and systematic empirical analysis, we show that our approach outperforms state-of-the-art methods by a large margin (e.g., +19.3% and +24.3% on the SummEval dataset) in evaluating the consistency of LLM generation across multiple benchmarks in semantic, factual, and summarization consistency tasks. Our approach also substantially reduces nearly 90% of output inconsistencies, showing promise for effective hallucination mitigation.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes DCR, an automated framework for evaluating and improving the consistency of LLM-generated texts using a divide-conquer-reasoning approach that outperforms state-of-the-art methods by a large margin and substantially reduces nearly 90% of output inconsistencies, showing promise for effective hallucination mitigation."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving factual consistency in language model outputs by incorporating semantic information during prompting. The approach is to parse the input prompt, augment it with semantic information, and check for semantic consistency during generation.\n\nThe research problem in the paper is evaluating and improving the consistency of language model outputs using a divide-and-conquer reasoning approach. The approach is to break down the comparison between generated responses into sentence-level comparisons, evaluate them based on predefined criteria, and use the identified inconsistencies to generate improved responses.\n\nWhile both works aim to improve the consistency of language model outputs, the proposal focuses specifically on factual consistency and uses semantic parsing and augmentation during prompting, whereas the paper proposes a general framework for evaluating and improving consistency using divide-and-conquer reasoning and reason-assisted improvement.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "40047a74b707743157051d38f76061ba5ff9aab4",
            "paperId": "40047a74b707743157051d38f76061ba5ff9aab4",
            "title": "Compositional Semantic Parsing with Large Language Models",
            "abstract": "Humans can reason compositionally when presented with new tasks. Previous research shows that appropriate prompting techniques enable large language models (LLMs) to solve artificial compositional generalization tasks such as SCAN. In this work, we identify additional challenges in more realistic semantic parsing tasks with larger vocabulary and refine these prompting techniques to address them. Our best method is based on least-to-most prompting: it decomposes the problem using prompting-based syntactic parsing, then uses this decomposition to select appropriate exemplars and to sequentially generate the semantic parse. This method allows us to set a new state of the art for CFQ while requiring only 1% of the training data used by traditional approaches. Due to the general nature of our approach, we expect similar efforts will lead to new results in other tasks and domains, especially for knowledge-intensive applications.",
            "year": 2022,
            "citationCount": 70,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The best method is based on least-to-most prompting: it decomposes the problem using prompting-based syntactic parsing, then uses this decomposition to select appropriate exemplars and to sequentially generate the semantic parse."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving factual consistency in language model outputs by incorporating semantic information during prompting. The approach is to parse the input prompt into a semantic representation, augment the prompt with this information, and check for semantic consistency during generation.\n\nThe research problem in the paper is compositional semantic parsing using large language models. The approach is to use prompting techniques such as least-to-most prompting, which decomposes the problem using syntactic parsing and sequentially generates the semantic parse.\n\nWhile both works involve semantic parsing and prompting, the research problems and approaches are different. The proposal focuses on improving factual consistency in general language model outputs, while the paper specifically addresses compositional semantic parsing. The proposal incorporates semantic information into prompts, while the paper uses prompting techniques to decompose and solve semantic parsing tasks.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "f66fe1ef497ecdc10590c250cbc4390aeb2fac1e",
            "paperId": "f66fe1ef497ecdc10590c250cbc4390aeb2fac1e",
            "title": "Semantic Parsing for Knowledge Graph Question Answering with Large Language Models",
            "abstract": null,
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This thesis uses the text generation ability of pre-trained text-to-text language models to convert natural language questions to logical forms, and tries to make the same models generate additional information to aid the process of grounding of the logical forms to entities, relations and literals in the Knowledge Graph."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving factual consistency in language model outputs by incorporating semantic information, while the approach is to use semantic parsing to augment the input prompts.\n\nThe research problem in the paper is knowledge graph question answering using language models, and the approach is to use the language models to convert natural language questions to logical forms and generate additional information for grounding the logical forms.\n\nThe two works have different research problems (factual consistency vs. knowledge graph QA) and approaches (semantic parsing for prompt augmentation vs. using language models for semantic parsing and logical form grounding).\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "af705d648b5b16daa3dcc593bc593f2574d76c07",
            "paperId": "af705d648b5b16daa3dcc593bc593f2574d76c07",
            "title": "Grammar Prompting for Domain-Specific Language Generation with Large Language Models",
            "abstract": "Large language models (LLMs) can learn to perform a wide range of natural language tasks from just a handful of in-context examples. However, for generating strings from highly structured languages (e.g., semantic parsing to complex domain-specific languages), it is challenging for the LLM to generalize from just a few exemplars. We explore $\\textbf{grammar prompting}$ as a simple approach for enabling LLMs to use external knowledge and domain-specific constraints, expressed through a grammar expressed in Backus--Naur Form (BNF), during in-context learning. Grammar prompting augments each demonstration example with a specialized grammar that is minimally sufficient for generating the particular output example, where the specialized grammar is a subset of the full DSL grammar. For inference, the LLM first predicts a BNF grammar given a test input, and then generates the output according to the rules of the grammar. Experiments demonstrate that grammar prompting can enable LLMs to perform competitively on a diverse set of DSL generation tasks, including semantic parsing (SMCalFlow, Overnight, GeoQuery), PDDL planning, and even molecule generation (SMILES).",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experiments demonstrate that grammar prompting can enable LLMs to perform competitively on a diverse set of DSL generation tasks, including semantic parsing, Overnight, GeoQuery, PDDL planning, and even molecule generation (SMILES)."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve the factual consistency of language model outputs by incorporating semantic information into the prompts, while the paper focuses on using grammar prompting to enable large language models to generate strings from highly structured domain-specific languages.\n\nThe project proposal addresses the problem of semantic inconsistency and factual errors in language model outputs, proposing Semantic-Aware Prompting (SAP) as a solution. In contrast, the paper tackles the challenge of generating strings from complex domain-specific languages using large language models, exploring grammar prompting as an approach.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "12db3efff4cc9e16822dd64bb1cad66f3f034f3b",
            "paperId": "12db3efff4cc9e16822dd64bb1cad66f3f034f3b",
            "title": "L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models",
            "abstract": "Recently, large language models (LLMs), especially those that are pretrained on code, have demonstrated strong capabilities in generating programs from natural language inputs in a few-shot or even zero-shot manner. Despite promising results, there is a notable lack of a comprehensive evaluation of these models language-to-code generation capabilities. Existing studies often focus on specific tasks, model architectures, or learning paradigms, leading to a fragmented understanding of the overall landscape. In this work, we present L2CEval, a systematic evaluation of the language-to-code generation capabilities of LLMs on 7 tasks across the domain spectrum of semantic parsing, math reasoning and Python programming, analyzing the factors that potentially affect their performance, such as model size, pretraining data, instruction tuning, and different prompting methods. In addition to assessing model performance, we measure confidence calibration for the models and conduct human evaluations of the output programs. This enables us to identify and analyze the typical failure modes across various tasks and models. L2CEval offers a comprehensive understanding of the capabilities and limitations of LLMs in language-to-code generation. We also release the evaluation framework and all model outputs, hoping to lay the groundwork for further future research in this domain.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents L2CEval, a systematic evaluation of the language-to-code generation capabilities of LLMs on 7 tasks across the domain spectrum of semantic parsing, math reasoning and Python programming, analyzing the factors that potentially affect their performance."
            },
            "score": 6,
            "novelty_score": "The research problem in the project proposal is improving the factual consistency of language model outputs by incorporating semantic information into prompts. The proposed approach is to parse the input prompt into a semantic representation, augment the prompt with the extracted semantic information, and check the semantic consistency between the generated response and the augmented prompt during the generation process.\n\nThe research problem in the paper is evaluating the language-to-code generation capabilities of large language models across various tasks and analyzing the factors that affect their performance. The proposed approach is to conduct a systematic evaluation on 7 tasks, measure confidence calibration, and perform human evaluations to identify typical failure modes.\n\nThe project proposal focuses on improving factual consistency in natural language generation, while the paper focuses on evaluating code generation capabilities. The approaches also differ, with the proposal using semantic parsing and augmentation, and the paper conducting a comprehensive evaluation across tasks and models.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "40c9280d87059c0cc28f2a08d46a7045fa3e9736",
            "paperId": "40c9280d87059c0cc28f2a08d46a7045fa3e9736",
            "title": "Divide and Prompt: Chain of Thought Prompting for Text-to-SQL",
            "abstract": "Chain-of-thought (CoT) prompting combined with large language models (LLMs) have achieved encouraging results on complex reasoning tasks. Text-to-SQL is a critical semantic parsing task that converts natural language questions into SQL statements, involving a complex reasoning process. However, there is little work about using CoT prompting to activate LLM's reasoning capabilities on Text-to-SQL tasks. In this work, we propose a new paradigm for prompting Text-to-SQL tasks, called Divide-and-Prompt, which first divides the task into subtasks, and then approach each subtask through CoT. We present 3 prompting-based methods to enhance the Text-to-SQL ability of LLMs. Experiments show that these prompts guide LLMs to generate Text-to-SQL with higher execution accuracy.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new paradigm for prompting Text-to-SQL tasks is proposed, called Divide-and-Prompt, which first divides the task into subtasks, and then approach each subtask through CoT."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving factual consistency in language model outputs by incorporating semantic information during prompting. The approach is to parse the input prompt into a semantic representation, augment the prompt with the extracted information, and check for semantic consistency during generation.\n\nThe research problem in the paper is improving the accuracy of Text-to-SQL models using large language models. The approach is to divide the task into subtasks and use chain-of-thought prompting for each subtask.\n\nThe proposal focuses on factual consistency in general language model outputs, while the paper specifically targets Text-to-SQL tasks. The proposal incorporates semantic information into prompts, while the paper uses task decomposition and chain-of-thought prompting.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "b8d06dd769f89d08bdd9997d7bd363c89ede845b",
            "paperId": "b8d06dd769f89d08bdd9997d7bd363c89ede845b",
            "title": "ZEROTOP: Zero-Shot Task-Oriented Semantic Parsing using Large Language Models",
            "abstract": "We explore the use of large language models (LLMs) for zero-shot semantic parsing. Semantic parsing involves mapping natural language utterances to task-specific meaning representations. Language models are generally trained on the publicly available text and code and cannot be expected to directly generalize to domain-specific parsing tasks in a zero-shot setting. In this work, we propose ZEROTOP, a zero-shot task-oriented parsing method that decomposes a semantic parsing problem into a set of abstractive and extractive question-answering (QA) problems, enabling us to leverage the ability of LLMs to zero-shot answer reading comprehension questions. For each utterance, we prompt the LLM with questions corresponding to its top-level intent and a set of slots and use the LLM generations to construct the target meaning representation. We observe that current LLMs fail to detect unanswerable questions; and as a result, cannot handle questions corresponding to missing slots. To address this problem, we fine-tune a language model on public QA datasets using synthetic negative samples. Experimental results show that our QA-based decomposition paired with the fine-tuned LLM can correctly parse ~16% of utterances in the MTOP dataset without requiring any annotated data.",
            "year": 2022,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes ZEROTOP, a zero-shot task-oriented parsing method that decomposes a semantic parsing problem into a set of abstractive and extractive question-answering (QA) problems, enabling us to leverage the ability of LLMs to zero- shot answer reading comprehension questions."
            },
            "score": 6
        },
        {
            "id": "6e8f8e2d2c73c91d1c9198eb802f1c64b860ea4a",
            "paperId": "6e8f8e2d2c73c91d1c9198eb802f1c64b860ea4a",
            "title": "Few-Shot Semantic Parsing with Language Models Trained on Code",
            "abstract": "Large language models can perform semantic parsing with little training data, when prompted with in-context examples. It has been shown that this can be improved by formulating the problem as paraphrasing into canonical utterances, which casts the underlying meaning representation into a controlled natural language-like representation. Intuitively, such models can more easily output canonical utterances as they are closer to the natural language used for pre-training. Recently, models also pre-trained on code, like OpenAI Codex, have risen in prominence. For semantic parsing tasks where we map natural language into code, such models may prove more adept at it. In this paper, we test this hypothesis and find that Codex performs better on such tasks than equivalent GPT-3 models. We evaluate on Overnight and SMCalFlow and find that unlike GPT-3, Codex performs similarly when targeting meaning representations directly, perhaps because meaning representations are structured similar to code in these datasets.",
            "year": 2021,
            "citationCount": 54,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper evaluates OpenAI Codex on Overnight and SMCalFlow and finds that unlike GPT-3, Codex performs similarly when targeting meaning representations directly, perhaps because meaning representations are structured similar to code in these datasets."
            },
            "score": 6
        },
        {
            "id": "716f9d0f6e96f437e127de90c87f7b2f7a6c8f12",
            "paperId": "716f9d0f6e96f437e127de90c87f7b2f7a6c8f12",
            "title": "SeqZero: Few-shot Compositional Semantic Parsing with Sequential Prompts and Zero-shot Models",
            "abstract": "Recent research showed promising results on combining pretrained language models (LMs) with canonical utterance for few-shot semantic parsing. The canonical utterance is often lengthy and complex due to the compositional structure of formal languages. Learning to generate such canonical utterance requires significant amount of data to reach high performance. Fine-tuning with only few-shot samples, the LMs can easily forget pretrained knowledge, overfit spurious biases, and suffer from compositionally out-of-distribution generalization errors. To tackle these issues, we propose a novel few-shot semantic parsing method -- SeqZero. SeqZero decomposes the problem into a sequence of sub-problems, which correspond to the sub-clauses of the formal language. Based on the decomposition, the LMs only need to generate short answers using prompts for predicting sub-clauses. Thus, SeqZero avoids generating a long canonical utterance at once. Moreover, SeqZero employs not only a few-shot model but also a zero-shot model to alleviate the overfitting. In particular, SeqZero brings out the merits from both models via ensemble equipped with our proposed constrained rescaling. SeqZero achieves SOTA performance of BART-based models on GeoQuery and EcommerceQuery, which are two few-shot datasets with compositional data split.",
            "year": 2022,
            "citationCount": 39,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "SeqZero achieves SOTA performance of BART-based models on GeoQuery and EcommerceQuery, which are two few-shot datasets with compositional data split and brings out the merits from both models via ensemble equipped with the proposed constrained rescaling."
            },
            "score": 6
        },
        {
            "id": "6af460d34bfc8e955e43fbe15cedcf329b48bc19",
            "paperId": "6af460d34bfc8e955e43fbe15cedcf329b48bc19",
            "title": "SAC3: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency",
            "abstract": "Hallucination detection is a critical step toward understanding the trustworthiness of modern language models (LMs). To achieve this goal, we re-examine existing detection approaches based on the self-consistency of LMs and uncover two types of hallucinations resulting from 1) question-level and 2) model-level, which cannot be effectively identified through self-consistency check alone. Building upon this discovery, we propose a novel sampling-based method, i.e., semantic-aware cross-check consistency (SAC3) that expands on the principle of self-consistency checking. Our SAC3 approach incorporates additional mechanisms to detect both question-level and model-level hallucinations by leveraging advances including semantically equivalent question perturbation and cross-model response consistency checking. Through extensive and systematic empirical analysis, we demonstrate that SAC3 outperforms the state of the art in detecting both non-factual and factual statements across multiple question-answering and open-domain generation benchmarks.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel sampling-based method, i.e., semantic-aware cross-check consistency (SAC3) that expands on the principle of self-consistency checking and demonstrates that SAC3 outperforms the state of the art in detecting both non-factual and factual statements across multiple question-answering and open-domain generation benchmarks."
            },
            "score": 6
        },
        {
            "id": "91b470af2ce5a407df43fd9039cb0d09d102ed8f",
            "paperId": "91b470af2ce5a407df43fd9039cb0d09d102ed8f",
            "title": "From Form(s) to Meaning: Probing the Semantic Depths of Language Models Using Multisense Consistency",
            "abstract": "The staggering pace with which the capabilities of large language models (LLMs) are increasing, as measured by a range of commonly used natural language understanding (NLU) benchmarks, raises many questions regarding what\"understanding\"means for a language model and how it compares to human understanding. This is especially true since many LLMs are exclusively trained on text, casting doubt on whether their stellar benchmark performances are reflective of a true understanding of the problems represented by these benchmarks, or whether LLMs simply excel at uttering textual forms that correlate with what someone who understands the problem would say. In this philosophically inspired work, we aim to create some separation between form and meaning, with a series of tests that leverage the idea that world understanding should be consistent across presentational modes - inspired by Fregean senses - of the same meaning. Specifically, we focus on consistency across languages as well as paraphrases. Taking GPT-3.5 as our object of study, we evaluate multisense consistency across five different languages and various tasks. We start the evaluation in a controlled setting, asking the model for simple facts, and then proceed with an evaluation on four popular NLU benchmarks. We find that the model's multisense consistency is lacking and run several follow-up analyses to verify that this lack of consistency is due to a sense-dependent task understanding. We conclude that, in this aspect, the understanding of LLMs is still quite far from being consistent and human-like, and deliberate on how this impacts their utility in the context of learning about human language and understanding.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The understanding of LLMs is still quite far from being consistent and human-like, and the idea that world understanding should be consistent across presentational modes - inspired by Fregean senses - of the same meaning is leveraged."
            },
            "score": 6
        },
        {
            "id": "2df0f63f30780af9259d9c874f9bdc23d14671f4",
            "paperId": "2df0f63f30780af9259d9c874f9bdc23d14671f4",
            "title": "Evaluating Factual Consistency of Texts with Semantic Role Labeling",
            "abstract": "Automated evaluation of text generation systems has recently seen increasing attention, particularly checking whether generated text stays truthful to input sources.Existing methods frequently rely on an evaluation using task-specific language models, which in turn allows for little interpretability of generated scores.We introduce SRLScore, a reference-free evaluation metric designed with text summarization in mind. Our approach generates fact tuples constructed from Semantic Role Labels, applied to both input and summary texts.A final factuality score is computed by an adjustable scoring mechanism, which allows for easy adaption of the method across domains. Correlation with human judgments on English summarization datasets shows that SRLScore is competitive with state-of-the-art methods and exhibits stable generalization across datasets without requiring further training or hyperparameter tuning.We experiment with an optional co-reference resolution step, but find that the performance boost is mostly outweighed by the additional compute required.Our metric is available online at: https://github.com/heyjing/SRLScore",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Correlation with human judgments on English summarization datasets shows that SRLScore is competitive with state-of-the-art methods and exhibits stable generalization across datasets without requiring further training or hyperparameter tuning."
            },
            "score": 6
        },
        {
            "id": "152d9a231c00d4495c9bc4a466f42165ce2e2164",
            "paperId": "152d9a231c00d4495c9bc4a466f42165ce2e2164",
            "title": "Evaluating Factual Consistency of Summaries with Large Language Models",
            "abstract": "Detecting factual errors in summaries has been an important and challenging subject in summarization research. Inspired by the emergent ability of large language models (LLMs), we explore evaluating factual consistency of summaries by directly prompting LLMs. We present a comprehensive empirical study to assess the ability of LLMs as factual consistency evaluators, which consists of (1) analyzing different LLMs such as the GPT model series and Flan-T5; (2) investigating a variety of prompting methods including vanilla prompting, chain-of-thought prompting, and a sentence-by-sentence prompting method to tackle long summaries; and (3) evaluating on diverse summaries generated by multiple summarization systems, ranging from pre-transformer methods to SOTA pretrained models. Our experiments demonstrate that prompting LLMs is able to outperform the previous best factuality systems in all settings, by up to 12.2 absolute points in terms of the binary classification accuracy on inconsistency detection.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A comprehensive empirical study to assess the ability of LLMs as factual consistency evaluators, which consists of analyzing different LLMs such as the GPT model series and Flan-T5 and investigating a variety of prompting methods including vanilla prompting, chain-of-thought prompting, and a sentence-by-sentence prompting method to tackle long summaries."
            },
            "score": 6
        },
        {
            "id": "22d5459d1f47341b355feeb1becc37208d6ec365",
            "paperId": "22d5459d1f47341b355feeb1becc37208d6ec365",
            "title": "RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought",
            "abstract": "Large language Models (LLMs) have achieved promising performance on arithmetic reasoning tasks by incorporating step-by-step chain-of-thought (CoT) prompting. However, LLMs face challenges in maintaining factual consistency during reasoning, exhibiting tendencies to condition overlooking, question misinterpretation, and condition hallucination over given problems. Existing methods use coarse-grained feedback (e.g., whether the answer is correct) to improve factual consistency. In this work, we propose RCoT (Reversing Chain-of-Thought), a novel method to improve LLMs' reasoning abilities by automatically detecting and rectifying factual inconsistency in LLMs, generated solutions. To detect factual inconsistency, RCoT first asks LLMs to reconstruct the problem based on generated solutions. Then fine-grained comparisons between the original problem and the reconstructed problem expose the factual inconsistency in the original solutions. To rectify the solution, RCoT formulates detected factual inconsistency into fine-grained feedback to guide LLMs in revising solutions. Experimental results demonstrate improvements of RCoT over standard CoT, Self-Consistency and Self-Refine across seven arithmetic datasets. Moreover, we find that manually written fine-grained feedback can dramatically improve LLMs' reasoning abilities (e.g., ChatGPT reaches 94.6% accuracy on GSM8K), encouraging the community to further explore the fine-grained feedback generation methods.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes RCoT (Reversing Chain-of-Thought), a novel method to improve LLMs' reasoning abilities by automatically detecting and rectifying factual inconsistency in LLMs, generated solutions and finds that manually written fine-grained feedback can dramatically improve LLM's reasoning abilities."
            },
            "score": 6
        },
        {
            "id": "d2d16333a4b0dc7e3463b280b9945e5ee6c53396",
            "paperId": "d2d16333a4b0dc7e3463b280b9945e5ee6c53396",
            "title": "TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models",
            "abstract": "Factual consistency evaluation is often conducted using Natural Language Inference (NLI) models, yet these models exhibit limited success in evaluating summaries. Previous work improved such models with synthetic training data. However, the data is typically based on perturbed human-written summaries, which often differ in their characteristics from real model-generated summaries and have limited coverage of possible factual errors. Alternatively, large language models (LLMs) have recently shown promising results in directly evaluating generative tasks, but are too computationally expensive for practical use. Motivated by these limitations, we introduce TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM. Unlike prior work, TrueTeacher does not rely on human-written summaries, and is multilingual by nature. Experiments on the TRUE benchmark show that a student model trained using our data, substantially outperforms both the state-of-the-art model with similar capacity, and the LLM teacher. In a systematic study, we compare TrueTeacher to existing synthetic data generation methods and demonstrate its superiority and robustness to domain-shift. We also show that our method generalizes to multilingual scenarios. Lastly, we release our large scale synthetic dataset (1.4M examples), generated using TrueTeacher, and a checkpoint trained on this data.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM, which does not rely on human-written summaries, and is multilingual by nature."
            },
            "score": 6
        },
        {
            "id": "90a51ca64fc9bb3b84eb20a8c9d68ad78b49d4b7",
            "paperId": "90a51ca64fc9bb3b84eb20a8c9d68ad78b49d4b7",
            "title": "Trapping LLM Hallucinations Using Tagged Context Prompts",
            "abstract": "Recent advances in large language models (LLMs), such as ChatGPT, have led to highly sophisticated conversation agents. However, these models suffer from\"hallucinations,\"where the model generates false or fabricated information. Addressing this challenge is crucial, particularly with AI-driven platforms being adopted across various sectors. In this paper, we propose a novel method to recognize and flag instances when LLMs perform outside their domain knowledge, and ensuring users receive accurate information. We find that the use of context combined with embedded tags can successfully combat hallucinations within generative language models. To do this, we baseline hallucination frequency in no-context prompt-response pairs using generated URLs as easily-tested indicators of fabricated data. We observed a significant reduction in overall hallucination when context was supplied along with question prompts for tested generative engines. Lastly, we evaluated how placing tags within contexts impacted model responses and were able to eliminate hallucinations in responses with 98.88% effectiveness.",
            "year": 2023,
            "citationCount": 21,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that the use of context combined with embedded tags can successfully combat hallucinations within generative language models and is able to eliminate hallucinations in responses with 98.88% effectiveness."
            },
            "score": 6
        },
        {
            "id": "339a9ef4187c33e9cc7ee867c7ce98a1f6f43735",
            "paperId": "339a9ef4187c33e9cc7ee867c7ce98a1f6f43735",
            "title": "Workshop On Large Language Models' Interpretability and Trustworthiness (LLMIT)",
            "abstract": "Large language models (LLMs), when scaled from millions to billions of parameters, have been demonstrated to exhibit the so-called 'emergence' effect, in that they are not only able to produce semantically correct and coherent text, but are also able to adapt themselves surprisingly well with small changes in contexts supplied as inputs (commonly called prompts). Despite producing semantically coherent and potentially relevant text for a given context, LLMs are vulnerable to yield incorrect information. This misinformation generation, or the so-called hallucination problem of an LLM, gets worse when an adversary manipulates the prompts to their own advantage, e.g., generating false propaganda to disrupt communal harmony, generating false information to trap consumers with target consumables etc. Not only does the consumption of an LLM-generated hallucinated content by humans pose societal threats, such misinformation, when used as prompts, may lead to detrimental effects for in-context learning (also known as few-shot prompt learning). With reference to the above-mentioned problems of LLM usage, we argue that it is necessary to foster research on topics related to not only identifying misinformation from LLM-generated content, but also to mitigate the propagation effects of this generated misinformation on downstream predictive tasks thus leading to more robust and effective leveraging in-context learning.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is argued that it is necessary to foster research on topics related to not only identifying misinformation from LLM-generated content, but also to mitigate the propagation effects of this generated misinformation on downstream predictive tasks thus leading to more robust and effective leveraging in-context learning."
            },
            "score": 6
        },
        {
            "id": "36b8948a6d15a3c7622ea1567a054845a66a91cb",
            "paperId": "36b8948a6d15a3c7622ea1567a054845a66a91cb",
            "title": "Evaluating Large Language Models in Semantic Parsing for Conversational Question Answering over Knowledge Graphs",
            "abstract": "Conversational question answering systems often rely on semantic parsing to enable interactive information retrieval, which involves the generation of structured database queries from a natural language input. For information-seeking conversations about facts stored within a knowledge graph, dialogue utterances are transformed into graph queries in a process that is called knowledge-based conversational question answering. This paper evaluates the performance of large language models that have not been explicitly pre-trained on this task. Through a series of experiments on an extensive benchmark dataset, we compare models of varying sizes with different prompting techniques and identify common issue types in the generated output. Our results demonstrate that large language models are capable of generating graph queries from dialogues, with significant improvements achievable through few-shot prompting and fine-tuning techniques, especially for smaller models that exhibit lower zero-shot performance.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that large language models are capable of generating graph queries from dialogues, with significant improvements achievable through few-shot prompting and fine-tuning techniques, especially for smaller models that exhibit lower zero-shot performance."
            },
            "score": 5
        },
        {
            "id": "38e1a9c5599fc7597b7c5ffd37951ba5f528094c",
            "paperId": "38e1a9c5599fc7597b7c5ffd37951ba5f528094c",
            "title": "XRICL: Cross-lingual Retrieval-Augmented In-Context Learning for Cross-lingual Text-to-SQL Semantic Parsing",
            "abstract": "In-context learning using large language models has recently shown surprising results for semantic parsing tasks such as Text-to-SQL translation. Prompting GPT-3 or Codex using several examples of question-SQL pairs can produce excellent results, comparable to state-of-the-art finetuning-based models. However, existing work primarily focuses on English datasets, and it is unknown whether large language models can serve as competitive semantic parsers for other languages. To bridge this gap, our work focuses on cross-lingual Text-to-SQL semantic parsing for translating non-English utterances into SQL queries based on an English schema. We consider a zero-shot transfer learning setting with the assumption that we do not have any labeled examples in the target language (but have annotated examples in English). This work introduces the XRICL framework, which learns to retrieve relevant English exemplars for a given query to construct prompts. We also include global translation exemplars for a target language to facilitate the translation process for large language models. To systematically evaluate our model, we construct two new benchmark datasets, XSpider and XKaggle-dbqa, which include questions in Chinese, Vietnamese, Farsi, and Hindi. Our experiments show that XRICL effectively leverages large pre-trained language models to outperform existing baselines. Data and code are publicly available at https://github.com/Impavidity/XRICL.",
            "year": 2022,
            "citationCount": 19,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces the XRICL framework, which learns to retrieve relevant English exemplars for a given query to construct prompts and effectively leverages large pre-trained language models to outperform existing baselines."
            },
            "score": 5
        },
        {
            "id": "132751a80632e80a90d7c3d3cd8a361f48fdb9b4",
            "paperId": "132751a80632e80a90d7c3d3cd8a361f48fdb9b4",
            "title": "BenchCLAMP: A Benchmark for Evaluating Language Models on Semantic Parsing",
            "abstract": "We introduce BenchCLAMP , a Bench mark to evaluate C onstrained LA nguage M odel P arsing, which produces semantic outputs based on the analysis of input text through constrained decoding of a prompted or \ufb01ne-tuned language model. Developers of pretrained language models currently benchmark on classi-\ufb01cation, span extraction and free-text generation tasks. Semantic parsing is neglected in language model evaluation because of the complexity of handling task-speci\ufb01c architectures and representations. Recent work has shown that generation from a prompted or \ufb01ne-tuned language model can perform well at semantic parsing when the output is constrained to be a valid semantic representation. BenchCLAMP includes context-free grammars for six semantic parsing datasets with varied output meaning representations, as well as a constrained decoding interface to generate outputs covered by these grammars. We provide low, medium, and high resource splits for each dataset, allowing accurate comparison of various language models under different data regimes. Our benchmark supports both prompt-based learning as well as \ufb01ne-tuning, and provides an easy-to-use toolkit for language model developers to evaluate on semantic parsing.",
            "year": 2022,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 5
        },
        {
            "id": "95e2f656017f9ec5d9cd411b1f744b278131ce6c",
            "paperId": "95e2f656017f9ec5d9cd411b1f744b278131ce6c",
            "title": "BenchCLAMP: A Benchmark for Evaluating Language Models on Syntactic and Semantic Parsing",
            "abstract": "Recent work has shown that generation from a prompted or fine-tuned language model can perform well at semantic parsing when the output is constrained to be a valid semantic representation. We introduce BenchCLAMP, a Benchmark to evaluate Constrained LAnguage Model Parsing, that includes context-free grammars for seven semantic parsing datasets and two syntactic parsing datasets with varied output representations, as well as a constrained decoding interface to generate only valid outputs covered by these grammars. We provide low, medium, and high resource splits for each dataset, allowing accurate comparison of various language models under different data regimes. Our benchmark supports evaluation of language models using prompt-based learning as well as fine-tuning. We benchmark eight language models, including two GPT-3 variants available only through an API. Our experiments show that encoder-decoder pretrained language models can achieve similar performance or surpass state-of-the-art methods for syntactic and semantic parsing when the model output is constrained to be valid.",
            "year": 2022,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "BenchCLAMP is introduced, a Benchmark to evaluate Constrained LAnguage Model Parsing, that includes context-free grammars for seven semantic parsing datasets and two syntactic parsing datasets with varied output representations, as well as a constrained decoding interface to generate only valid outputs covered by these grammars."
            },
            "score": 5
        },
        {
            "id": "0098123efc851b67137c1028f7bac8d8bffbc8fd",
            "paperId": "0098123efc851b67137c1028f7bac8d8bffbc8fd",
            "title": "Awakening Latent Grounding from Pretrained Language Models for Semantic Parsing",
            "abstract": "Recent years pretrained language models (PLMs) hit a success on several downstream tasks, showing their power on modeling language. To better understand and leverage what PLMs have learned, several techniques have emerged to explore syntactic structures entailed by PLMs. However, few efforts have been made to explore grounding capabilities of PLMs, which are also essential. In this paper, we highlight the ability of PLMs to discover which token should be grounded to which concept, if combined with our proposed erasing-then-awakening approach. Empirical studies on four datasets demonstrate that our approach can awaken latent grounding which is understandable to human experts, even if it is not exposed to such labels during training. More importantly, our approach shows great potential to benefit downstream semantic parsing models. Taking text-to-SQL as a case study, we successfully couple our approach with two off-the-shelf parsers, obtaining an absolute improvement of up to 9.8%.",
            "year": 2021,
            "citationCount": 29,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Empirical studies demonstrate that the proposed erasing-then-awakening approach can awaken latent grounding which is understandable to human experts, even if it is not exposed to such labels during training, and shows great potential to benefit downstream semantic parsing models."
            },
            "score": 5
        },
        {
            "id": "316ae7a80564fe6afbd5fe99cab9eaf758d558a0",
            "paperId": "316ae7a80564fe6afbd5fe99cab9eaf758d558a0",
            "title": "SP-NLG: A Semantic-Parsing-Guided Natural Language Generation Framework",
            "abstract": "We propose SP-NLG: A semantic-parsing-guided natural language generation framework for logical content generation with high fidelity. Prior studies adopt large pretrained language models and coarse-to-fine decoding techniques to generate text with logic; while achieving considerable results on automatic evaluation metrics, they still face challenges in terms of logical fidelity based on human evaluation. Inspired by semantic parsing, which translates natural language utterances into executable logical forms, we propose to guide the generation process with a semantic parser. Different from semantic parsing and QA tasks, of which the logical correctness can be evaluated based on the execution result, the logic information of the generated content is implicit. To leverage a semantic parser for generation, we propose a slot-tied back-search algorithm. We follow the coarse-to-fine generation scheme, but instead of filling the slots with model predictions, which is less uncontrolled, the slot values are offline searched by the algorithm. The slot-tied back-search algorithm ties the parameters of a logic form with the slots of a template in one-to-one correspondence. We back-search the arguments that correctly execute the logic form and fill the arguments (as slot values) into the textual template to generate the final target, which ensures the logical correctness. Experiment results of a model built on SP-NLG demonstrates that our framework achieves high fidelity on logical text generation.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A slot-tied back-search algorithm that ties the parameters of a logic form with the slots of a template in one-to-one correspondence to leverage a semantic parser for generation, which achieves high fidelity on logical text generation."
            },
            "score": 5
        },
        {
            "id": "ec55e16e934e4a7c4527a1f54b39a680a88b392c",
            "paperId": "ec55e16e934e4a7c4527a1f54b39a680a88b392c",
            "title": "Limits for learning with language models",
            "abstract": "With the advent of large language models (LLMs), the trend in NLP has been to train LLMs on vast amounts of data to solve diverse language understanding and generation tasks. The list of LLM successes is long and varied. Nevertheless, several recent papers provide empirical evidence that LLMs fail to capture important aspects of linguistic meaning. Focusing on universal quantification, we provide a theoretical foundation for these empirical findings by proving that LLMs cannot learn certain fundamental semantic properties including semantic entailment and consistency as they are defined in formal semantics. More generally, we show that LLMs are unable to learn concepts beyond the first level of the Borel Hierarchy, which imposes severe limits on the ability of LMs, both large and small, to capture many aspects of linguistic meaning. This means that LLMs will operate without formal guarantees on tasks that require entailments and deep linguistic understanding.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that LLMs are unable to learn concepts beyond the first level of the Borel Hierarchy, which imposes severe limits on the ability of LMs, both large and small, to capture many aspects of linguistic meaning."
            },
            "score": 5
        },
        {
            "id": "5fef3a3e5e73d6ad44f0c8279374c141747d35a8",
            "paperId": "5fef3a3e5e73d6ad44f0c8279374c141747d35a8",
            "title": "Improving the Semantic Consistency of Textual Adversarial Attacks via Prompt",
            "abstract": "Adversarial examples can expose the vulnerabilities of neural networks. State-of-the-art textual adversarial attacks have demonstrated their effectiveness in triggering errors in the output of natural language processing models. However, these attacks are limited to ensuring the semantic consistency between the adversarial example and the original input, increasing the possibility of the attacks being detected by human judges. In this paper, we propose a novel textual adversarial attack, Prompt-Attack, which aims to generate the adversarial examples having consistent semantics with the original input. Specifically, Prompt-Attack enhances the input's semantics with the prompts that represent the semantics of the different target segments extracted from the original input, to predict the substitutions having consistent semantics with the target segments. Then, it crafts the adversarial examples by replacing the important segments with their substitutions that can most affect the victim model's output. Besides, Prompt-Attack proposes a span-level segment identification strategy to extract more target segments from the input and a novel masking strategy to ensure the grammatical correctness of the generated adversarial examples. Extensive experiments on public datasets illustrate that Prompt-Attack significantly improves the semantic consistency score of the baseline attacks by an average of 48%. Further, Prompt-Attack achieves the best attack success rate of 0.906, showing an average improvement of 40% to the baselines. Moreover, the experimental results demonstrate that Prompt-Attack can achieve good performance in attacking different language models and Prompt-Attack is not sensitive to different settings.",
            "year": 2022,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel textual adversarial attack, Prompt-Attack, which aims to generate the adversarial examples having consistent semantics with the original input and proposes a span-level segment identification strategy to extract more target segments from the input and a novel masking strategy to ensure the grammatical correctness of the generated adversarialExamples."
            },
            "score": 5
        },
        {
            "id": "87a0279c1d640b486dc5c9f1e0d3705ed87754cf",
            "paperId": "87a0279c1d640b486dc5c9f1e0d3705ed87754cf",
            "title": "Improving Generation and Evaluation of Visual Stories via Semantic Consistency",
            "abstract": "Story visualization is an underexplored task that falls at the intersection of many important research directions in both computer vision and natural language processing. In this task, given a series of natural language captions which compose a story, an agent must generate a sequence of images that correspond to the captions. Prior work has introduced recurrent generative models which outperform text-to-image synthesis models on this task. However, there is room for improvement of generated images in terms of visual quality, coherence and relevance. We present a number of improvements to prior modeling approaches, including (1) the addition of a dual learning framework that utilizes video captioning to reinforce the semantic alignment between the story and generated images, (2) a copy-transform mechanism for sequentially-consistent story visualization, and (3) MART-based transformers to model complex interactions between frames. We present ablation studies to demonstrate the effect of each of these techniques on the generative power of the model for both individual images as well as the entire narrative. Furthermore, due to the complexity and generative nature of the task, standard evaluation metrics do not accurately reflect performance. Therefore, we also provide an exploration of evaluation metrics for the model, focused on aspects of the generated frames such as the presence/quality of generated characters, the relevance to captions, and the diversity of the generated images. We also present correlation experiments of our proposed automated metrics with human evaluations.",
            "year": 2021,
            "citationCount": 34,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A number of improvements to prior modeling approaches are presented, including the addition of a dual learning framework that utilizes video captioning to reinforce the semantic alignment between the story and generated images, a copy-transform mechanism for sequentially-consistent story visualization, and MART-based transformers to model complex interactions between frames."
            },
            "score": 5
        },
        {
            "id": "f727f928e7e179307d8d4a1da2387393f2bd7915",
            "paperId": "f727f928e7e179307d8d4a1da2387393f2bd7915",
            "title": "Methods for Measuring, Updating, and Visualizing Factual Beliefs in Language Models",
            "abstract": "Language models can memorize a considerable amount of factual information during pretraining that can be elicited through prompting or finetuning models on tasks like question answering. In this paper, we discuss approaches to measuring model factual beliefs, updating incorrect factual beliefs in models, and visualizing graphical relationships between factual beliefs. Our main contributions include: (1) new metrics for evaluating belief-updating methods focusing on the logical consistency of beliefs, (2) a training objective for Sequential, Local, and Generalizing updates (SLAG) that improves the performance of existing hypernetwork approaches, and (3) the introduction of the belief graph, a new form of visualization for language models that shows relationships between stored model beliefs. Our experiments suggest that models show only limited consistency between factual beliefs, but update methods can both fix incorrect model beliefs and greatly improve their consistency. Although off-the-shelf optimizers are surprisingly strong belief-updating baselines, our learned optimizers can outperform them in more difficult settings than have been considered in past work.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experiments suggest that models show only limited consistency between factual beliefs, but update methods can both fix incorrect model beliefs and greatly improve their consistency, and off-the-shelf optimizers can outperform them in more difficult settings than have been considered in past work."
            },
            "score": 5
        },
        {
            "id": "3d2f9bd3f2ea1e1095e6d22e32ed8eb5fbb096f6",
            "paperId": "3d2f9bd3f2ea1e1095e6d22e32ed8eb5fbb096f6",
            "title": "Rationale Belief Aggregation for Self-Verified Reasoning",
            "abstract": "Large language models such as GPT-3 [1] have a great deal of information encoded within their parameters, however, our ability to access this information is bottlenecked by how we communicate or interface with these models, namely through prompting. Chain-of-thought prompting [2] demonstrates the value of producing step-by-step reasoning chains (rationales) before answering a question, and self-consistency [3] shows that sampling multiple rationales and aggregating their outputs can allow for more robust reasoning. In this work we postulate that rationales consist of multiple beliefs, or informational phrases that the model uses as context for its reasoning (which may or may not be factual), and some inference over these beliefs. Empirically, we observe that different rationales expose different beliefs and hypothesize that performing a principled aggregation over the beliefs surfaced by different rationales would allow us to reduce internal contradictions within a language model and produce more consistent rationales to reason over. We propose two such aggregation strategies, Belief Aggregation and Belief Majority Voting, and evaluate their performance over three challenging QA datasets [4]. Our method results in modest gains in accuracy over self-consistency and greedy decoding for chain-of-thought prompting, while providing strong gains in coverage (% of questions a model is able to answer without abstaining), thereby resulting in confident reasoning over a larger set of questions.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The method results in modest gains in accuracy over self-consistency and greedy decoding for chain-of-thought prompting, while providing strong gains in coverage (% of questions a model is able to answer without abstaining), thereby resulting in confident reasoning over a larger set of questions."
            },
            "score": 5
        },
        {
            "id": "41a41c75ba336dec98d58c563605f261019e5df0",
            "paperId": "41a41c75ba336dec98d58c563605f261019e5df0",
            "title": "\u201cAccording to . . . \u201d: Prompting Language Models Improves Quoting from Pre-Training Data",
            "abstract": "Large Language Models (LLMs) may hallucinate and generate fake information, despite pre-training on factual data. Inspired by the journalistic device of \u201caccording to sources\u201d, we propose according-to prompting: directing LLMs to ground responses against previously observed text. To quantify this grounding, we propose a novel evaluation metric (QUIP-Score) that measures the extent to which model-produced answers are directly found in underlying text corpora. We illustrate with experiments on three corpora (Wikipedia, PubMed, and the U.S. legal tax code) that these prompts improve grounding under our metrics, with the additional benefit of often improving end-task performance. Furthermore, prompts that ask the model to decrease grounding (or to ground to other corpora) indeed decrease QUIP-Score, indicating the ability of LLMs to increase or decrease grounded generations on request.",
            "year": 2023,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "According-to prompting is proposed: directing LLMs to ground responses against previously observed text, to quantify this grounding, and proposes a novel evaluation metric (QUIP-Score) that measures the extent to which model-produced answers are directly found in underlying text corpora."
            },
            "score": 5
        },
        {
            "id": "56373d3fd0f1354a61f9e577db039cdb187d8d43",
            "paperId": "56373d3fd0f1354a61f9e577db039cdb187d8d43",
            "title": "Evaluating the Factual Consistency of Large Language Models Through News Summarization",
            "abstract": "While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information. To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB(Factual Inconsistency Benchmark) that focuses on the task of summarization. Specifically, our benchmark involves comparing the scores an LLM assigns to a factually consistent versus a factually inconsistent summary for an input news article. For factually consistent summaries, we use human-written reference summaries that we manually verify as factually consistent. To generate summaries that are factually inconsistent, we generate summaries from a suite of summarization models that we have manually annotated as factually inconsistent. A model's factual consistency is then measured according to its accuracy, i.e.\\ the proportion of documents where it assigns a higher score to the factually consistent summary. To validate the usefulness of FIB, we evaluate 23 large language models ranging from 1B to 176B parameters from six different model families including BLOOM and OPT. We find that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries. However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries. We validate design choices in our benchmark including the scoring method and source of distractor summaries. Our code and benchmark data can be found at https://github.com/r-three/fib.",
            "year": 2022,
            "citationCount": 51,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new benchmark called FIB(Factual Inconsistency Benchmark) that focuses on the task of summarization, and finds that existing LLMs generally assign a higher score to factually consistent summaries than tofactually inconsistent summaries."
            },
            "score": 5
        },
        {
            "id": "f33d19614d78f4a6e107e768ccd0a2d4244fc89a",
            "paperId": "f33d19614d78f4a6e107e768ccd0a2d4244fc89a",
            "title": "Evaluating the Factual Consistency of Large Language Models Through Summarization",
            "abstract": "While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information. To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB ( F actual I nconsistency B enchmark) that focuses on the task of summarization. Specifically, our benchmark involves comparing the scores an LLM assigns to a factually consistent versus a factually inconsistent summary for an input news article. For factually consistent summaries, we use human-written reference summaries that we manually verify as fac-tually consistent. To generate summaries that are factually inconsistent, we generate summaries from a suite of summarization models that we have manually annotated as fac-tually inconsistent. A model\u2019s factual consistency is then measured according to its accuracy, i.e. the proportion of documents where it assigns a higher score to the factually consistent summary. To validate the usefulness of FIB , we evaluate 23 large language models ranging from 1B to 176B parameters from six different model families including BLOOM and OPT. We \ufb01nd that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries. However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries. We validate design choices in our benchmark including the scoring method and source of distractor summaries. Our code and",
            "year": 2022,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new benchmark called FIB is proposed that focuses on the task of summarization and shows that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries, and that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries."
            },
            "score": 5
        },
        {
            "id": "f3cd3a0e1a0a29193fd88178ef75946dfe756622",
            "paperId": "f3cd3a0e1a0a29193fd88178ef75946dfe756622",
            "title": "Factual Consistency Evaluation of Summarisation in the Era of Large Language Models",
            "abstract": "Factual inconsistency with source documents in automatically generated summaries can lead to misinformation or pose risks. Existing factual consistency(FC) metrics are constrained by their performance, efficiency, and explainability. Recent advances in Large language models (LLMs) have demonstrated remarkable potential in text evaluation but their effectiveness in assessing FC in summarisation remains underexplored. Prior research has mostly focused on proprietary LLMs, leaving essential factors that affect their assessment capabilities unexplored. Additionally, current FC evaluation benchmarks are restricted to news articles, casting doubt on the generality of the FC methods tested on them. In this paper, we first address the gap by introducing TreatFact a dataset of LLM-generated summaries of clinical texts, annotated for FC by domain experts. Moreover, we benchmark 11 LLMs for FC evaluation across news and clinical domains and analyse the impact of model size, prompts, pre-training and fine-tuning data. Our findings reveal that despite proprietary models prevailing on the task, open-source LLMs lag behind. Nevertheless, there is potential for enhancing the performance of open-source LLMs through increasing model size, expanding pre-training data, and developing well-curated fine-tuning data. Experiments on TreatFact suggest that both previous methods and LLM-based evaluators are unable to capture factual inconsistencies in clinical summaries, posing a new challenge for FC evaluation.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "TreatFact, a dataset of LLM-generated summaries of clinical texts, annotated for FC by domain experts, is introduced and it is revealed that despite proprietary models prevailing on the task, open-source LLMs lag behind."
            },
            "score": 5
        },
        {
            "id": "663d743272e9ab04f54d9105a3c3a3f6e22dd1dd",
            "paperId": "663d743272e9ab04f54d9105a3c3a3f6e22dd1dd",
            "title": "FactKB: Generalizable Factuality Evaluation using Language Models Enhanced with Factual Knowledge",
            "abstract": "Evaluating the factual consistency of automatically generated summaries is essential for the progress and adoption of reliable summarization systems. Despite recent advances, existing factuality evaluation models are not robust, being especially prone to entity and relation errors in new domains. We propose FactKB, a simple new approach to factuality evaluation that is generalizable across domains, in particular with respect to entities and relations. FactKB is based on language models pretrained using facts extracted from external knowledge bases. We introduce three types of complementary factuality pretraining objectives based on direct entity facts, facts grounded in auxiliary knowledge about entities, and facts constructed compositionally through knowledge base walks. The resulting factuality evaluation model achieves state-of-the-art performance on two in-domain news summarization benchmarks as well as on three out-of-domain scientific literature datasets. Further analysis of FactKB shows improved ability to detect erroneous entities and relations in summaries and is robust and generalizable across domains.",
            "year": 2023,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "FactKB is a simple new approach to factuality evaluation that is generalizable across domains, in particular with respect to entities and relations and shows improved ability to detect erroneous entities and relation in summaries."
            },
            "score": 5
        },
        {
            "id": "590802c314c34d5746503e5b3efa327845d0587d",
            "paperId": "590802c314c34d5746503e5b3efa327845d0587d",
            "title": "Experimenting with Large Language Models and vector embeddings in NASA SciX",
            "abstract": "Open-source Large Language Models enable projects such as NASA SciX (i.e., NASA ADS) to think out of the box and try alternative approaches for information retrieval and data augmentation, while respecting data copyright and users' privacy. However, when large language models are directly prompted with questions without any context, they are prone to hallucination. At NASA SciX we have developed an experiment where we created semantic vectors for our large collection of abstracts and full-text content, and we designed a prompt system to ask questions using contextual chunks from our system. Based on a non-systematic human evaluation, the experiment shows a lower degree of hallucination and better responses when using Retrieval Augmented Generation. Further exploration is required to design new features and data augmentation processes at NASA SciX that leverages this technology while respecting the high level of trust and quality that the project holds.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An experiment where semantic vectors are created for the authors' large collection of abstracts and full-text content, and a prompt system to ask questions using contextual chunks from their system shows a lower degree of hallucination and better responses when using Retrieval Augmented Generation."
            },
            "score": 5
        },
        {
            "id": "a26fbe4e08cc37f48661c2adb538111248e42f71",
            "paperId": "a26fbe4e08cc37f48661c2adb538111248e42f71",
            "title": "ChatENT: Augmented Large Language Models for Expert Knowledge Retrieval in Otolaryngology - Head and Neck Surgery",
            "abstract": "The recent surge in popularity of Large Language Models (LLMs), such as ChatGPT, has showcased their proficiency in medical examinations and potential contributions to medical education. However, LLMs possess inherent limitations, including inconsistent accuracy, specific prompting requirements, and the risk of generating harmful hallucinations. A domain-specific, fine-tuned model would address these limitations effectively. OHNS-relevant data was systematically gathered from open-access internet sources and indexed into a database. We leveraged Retrieval-Augmented Language Modeling (RALM) to recall this information and used it for pre-training, which was then integrated into ChatGPT 4.0, creating a OHNS specific knowledge Q&A platform known as ChatENT. ChatENT showed enhanced performance in the analysis and interpretation of OHNS information, outperforming ChatGPT 4.0 in both the Canadian Royal College OHNS sample examination questions challenge and the US board practice questions challenge, with a 58.4% and 26.0% error reduction, respectively. ChatENT generated fewer hallucinations and demonstrated greater consistency.To the best of our knowledge, ChatENT is the first specialty-specific LLM in the medical field. It appears to have considerable promise in areas such as medical education, patient education, and clinical decision support. The fine-tuned model has demonstrated the capacity to overcome the limitations of existing LLMs, thereby signaling a future of more precise, safe, and user-friendly applications in the realm of OHNS.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The fine-tuned model has demonstrated the capacity to overcome the limitations of existing LLMs, thereby signaling a future of more precise, safe, and user-friendly applications in the realm of OHNS."
            },
            "score": 5
        },
        {
            "id": "3ed1c94ec4fdd2a9235afeb2d929fde965b1d723",
            "paperId": "3ed1c94ec4fdd2a9235afeb2d929fde965b1d723",
            "title": "Characterizing Attribution and Fluency Tradeoffs for Retrieval-Augmented Large Language Models",
            "abstract": "Despite recent progress, it has been difficult to prevent semantic hallucinations in generative Large Language Models. One common solution to this is augmenting LLMs with a retrieval system and making sure that the generated output is attributable to the retrieved information. Given this new added constraint, it is plausible to expect that the overall quality of the output will be affected, for example, in terms of fluency. Can scaling language models help? Here we examine the relationship between fluency and attribution in LLMs prompted with retrieved evidence in knowledge-heavy dialog settings. Our experiments were implemented with a set of auto-metrics that are aligned with human preferences. They were used to evaluate a large set of generations, produced under varying parameters of LLMs and supplied context. We show that larger models tend to do much better in both fluency and attribution, and that (naively) using top-k retrieval versus top-1 retrieval improves attribution but hurts fluency. We next propose a recipe that could allow smaller models to both close the gap with larger models and preserve the benefits of top-k retrieval while avoiding its drawbacks.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The relationship between fluency and attribution in LLMs prompted with retrieved evidence in knowledge-heavy dialog settings is examined and a recipe is proposed that could allow smaller models to both close the gap with larger models and preserve the benefits of top-k retrieval while avoiding its drawbacks."
            },
            "score": 5
        },
        {
            "id": "dda0f7f086fc875d583604f8b0cf4a8678bc4de4",
            "paperId": "dda0f7f086fc875d583604f8b0cf4a8678bc4de4",
            "title": "Bootstrapping Multilingual Semantic Parsers using Large Language Models",
            "abstract": "Despite cross-lingual generalization demonstrated by pre-trained multilingual models, the translate-train paradigm of transferring English datasets across multiple languages remains to be a key mechanism for training task-specific multilingual models. However, for many low-resource languages, the availability of a reliable translation service entails significant amounts of costly human-annotated translation pairs. Further, translation services may continue to be brittle due to domain mismatch between task-specific input text and general-purpose text used for training translation models. For multilingual semantic parsing, we demonstrate the effectiveness and flexibility offered by large language models (LLMs) for translating English datasets into several languages via few-shot prompting. Through extensive comparisons on two public datasets, MTOP and MASSIVE, spanning 50 languages and several domains, we show that our method of translating data using LLMs outperforms a strong translate-train baseline on 41 out of 50 languages. We study the key design choices that enable more effective multilingual data translation via prompted LLMs.",
            "year": 2022,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The effectiveness and flexibility offered by large language models (LLMs) for translating English datasets into several languages via few-shot prompting are demonstrated and it is shown that the method of translating data using LLMs outperforms a strong translate-train baseline on 41 out of 50 languages."
            },
            "score": 4
        },
        {
            "id": "7fff3df60b8d9a5e57627f4eb6b3022b60f3a39d",
            "paperId": "7fff3df60b8d9a5e57627f4eb6b3022b60f3a39d",
            "title": "An Investigation of LLMs' Inefficacy in Understanding Converse Relations",
            "abstract": "Large Language Models (LLMs) have achieved remarkable success in many formal language oriented tasks, such as structural data-to-text and semantic parsing. However current benchmarks mostly follow the data distribution of the pre-training data of LLMs. Therefore, a natural question rises that do LLMs really understand the structured semantics of formal languages. In this paper, we investigate this problem on a special case, converse binary relation. We introduce a new benchmark ConvRe focusing on converse relations, which contains 17 relations and 1240 triples extracted from popular knowledge graph completion datasets. Our ConvRE features two tasks, Re2Text and Text2Re, which are formulated as multi-choice question answering to evaluate LLMs' ability to determine the matching between relations and associated text. For the evaluation protocol, apart from different prompting methods, we further introduce variants to the test text and few-shot example text. We conduct experiments on three popular LLM families and have observed various scaling trends. The results suggest that LLMs often resort to shortcut learning and still face challenges on our proposed benchmark.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new benchmark ConvRe focusing on converse relations, which contains 17 relations and 1240 triples extracted from popular knowledge graph completion datasets is introduced, which suggests that LLMs often resort to shortcut learning and still face challenges on this proposed benchmark."
            },
            "score": 4
        },
        {
            "id": "62176de125738e3b95850d1227bac81fd646b78e",
            "paperId": "62176de125738e3b95850d1227bac81fd646b78e",
            "title": "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models",
            "abstract": "Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, Few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual efforts, Zero-shot-CoT concatenates the target problem statement with \u201cLet\u2019s think step by step\u201d as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.",
            "year": 2023,
            "citationCount": 115,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results over GPT-3 show that the proposed zero-shot prompting consistently outperforms Zero- shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem."
            },
            "score": 4
        },
        {
            "id": "1c91b23d78944f7f237cb512029c2165972ae9d5",
            "paperId": "1c91b23d78944f7f237cb512029c2165972ae9d5",
            "title": "On Robustness of Prompt-based Semantic Parsing with Large Pre-trained Language Model: An Empirical Study on Codex",
            "abstract": "Semantic parsing is a technique aimed at constructing a structured representation of the meaning of a natural-language question. Recent advances in language models trained on code have shown superior performance in generating these representations compared to language models trained solely on natural language text. The existing fine-tuned neural semantic parsers are vulnerable to adversarial attacks on natural-language inputs. While it has been established that the robustness of smaller semantic parsers can be enhanced through adversarial training, this approach is not feasible for large language models in real-world scenarios, as it requires both substantial computational resources and expensive human annotation on in-domain semantic parsing data. This paper presents the first empirical study on the adversarial robustness of a prompt-based semantic parser based on CODEX, a stateof-the-art (SOTA) language model trained on code. Our results demonstrate that the large language model of code is vulnerable to carefully crafted adversarial examples. To overcome this challenge, we propose methods for enhancing robustness without requiring substantial amounts of labelled data or intensive computational resources.",
            "year": 2023,
            "citationCount": 30,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents the first empirical study on the adversarial robustness of a prompt-based semantic parser based on CODEX, a stateof-the-art (SOTA) language model trained on code."
            },
            "score": 4
        },
        {
            "id": "87265a62fc5d49d80037b7f62f90cf4757b37bc2",
            "paperId": "87265a62fc5d49d80037b7f62f90cf4757b37bc2",
            "title": "SaGE: Evaluating Moral Consistency in Large Language Models",
            "abstract": "Despite recent advancements showcasing the impressive capabilities of Large Language Models (LLMs) in conversational systems, we show that even state-of-the-art LLMs are morally inconsistent in their generations, questioning their reliability (and trustworthiness in general). Prior works in LLM evaluation focus on developing ground-truth data to measure accuracy on specific tasks. However, for moral scenarios that often lack universally agreed-upon answers, consistency in model responses becomes crucial for their reliability. To address this issue, we propose an information-theoretic measure called Semantic Graph Entropy (SaGE), grounded in the concept of\"Rules of Thumb\"(RoTs) to measure a model's moral consistency. RoTs are abstract principles learned by a model and can help explain their decision-making strategies effectively. To this extent, we construct the Moral Consistency Corpus (MCC), containing 50K moral questions, responses to them by LLMs, and the RoTs that these models followed. Furthermore, to illustrate the generalizability of SaGE, we use it to investigate LLM consistency on two popular datasets -- TruthfulQA and HellaSwag. Our results reveal that task-accuracy and consistency are independent problems, and there is a dire need to investigate these issues further.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An information-theoretic measure called Semantic Graph Entropy (SaGE), grounded in the concept of \"Rules of Thumb\"(RoTs) to measure a model's moral consistency is proposed, and is used to investigate LLM consistency on two popular datasets -- TruthfulQA and HellaSwag."
            },
            "score": 4
        },
        {
            "id": "ff3efb5bf333850f69282b15417a4606165e70ae",
            "paperId": "ff3efb5bf333850f69282b15417a4606165e70ae",
            "title": "Beyond Lexical Consistency: Preserving Semantic Consistency for Program Translation",
            "abstract": "Program translation aims to convert the input programs from one programming language to another. Automatic program translation is a prized target of software engineering research, which leverages the reusability of projects and improves the efficiency of development. Recently, thanks to the rapid development of deep learning model architectures and the availability of large-scale parallel corpus of programs, the performance of program translation has been greatly improved. However, the existing program translation models are still far from satisfactory, in terms of the quality of translated programs. In this paper, we argue that a major limitation of the current approaches is the lack of consideration of semantic consistency. Beyond lexical consistency, semantic consistency is also critical for the task. To make the program translation model more semantically aware, we propose a general framework named Preserving Semantic Consistency for Program Translation (PSCPT), which considers semantic consistency with regularization in the training objective of program translation and can be easily applied to all encoder-decoder methods with various neural networks (e.g., LSTM, Transformer) as the backbone. We conduct extensive experiments in 7 general programming languages. Experimental results show that with CodeBERT as the backbone, our approach outperforms not only the state-of-the-art open-source models but also the commercial closed large language models (e.g., textdavinci-002, text-davinci-003) on the program translation task. Our replication package (including code, data, etc.) is publicly available at https://github.com/duyali2000/PSCPT.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a general framework named Preserving Semantic Consistency for Program Translation (PSCPT), which considers semantic consistency with regularization in the training objective of program translation and can be easily applied to all encoder-decoder methods with various neural networks (e.g., LSTM, Transformer) as the backbone."
            },
            "score": 4
        },
        {
            "id": "3475f6bf35d010cff6c73fa8424e15525364137f",
            "paperId": "3475f6bf35d010cff6c73fa8424e15525364137f",
            "title": "Checking spatio-semantic consistency of building information models by means of a query language",
            "abstract": "One of the characteristic features of object-based Building Information Models is the close integration of geometric and semantic information into one model. This concept is thoroughly implemented by the Industry Foundation Classes (IFC), a comprehensive data model designed to provide a sound foundation for complex data exchange scenarios. Besides the provision of a large variety of data types for capturing the semantics of building elements and spaces, the IFC also makes it possible to define relationships between building elements and/or spaces, respectively. In particular, a spatial aggregation hierarchy can be modeled by successively applying the relationship IfcRelAggregates to space objects. However, no validation options currently exist to check whether the semantically defined aggregation hierarchy complies with the geometric setup of the individual spaces and building elements. This lack of consistency between the semantic and the geometric part of the BIM model may lead to serious data interpretation errors at the receiving end. To prevent this, we propose a new method for validating spatio-semantic consistency based on the usage of the Query Language for Building Information Models (QL4BIM) which on the one hand provides a means of accessing the IFC object model and on the other hand provides high-level spatial operators, such as Disjoint, Touching and Containing. The formulation of corresponding queries makes it possible to verify the spatio-semantic consistency of the IFC model. The paper discusses application scenarios and provides a number of relevant examples.",
            "year": 2013,
            "citationCount": 20,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new method for validating spatio-semantic consistency based on the usage of the Query Language for Building Information Models (QL4BIM) which on the one hand provides a means of accessing the IFC object model and on the other hand provides high-level spatial operators, such as Disjoint, Touching and Containing, which makes it possible to verify the spatio, semantic consistency of the I FC model."
            },
            "score": 4
        },
        {
            "id": "62ee2d23968b8ee97c958d75ae2b6ae13c84da1a",
            "paperId": "62ee2d23968b8ee97c958d75ae2b6ae13c84da1a",
            "title": "BRAINTEASER: Lateral Thinking Puzzles for Large Language Models",
            "abstract": "The success of language models has inspired the NLP community to attend to tasks that require implicit and complex reasoning, relying on human-like commonsense mechanisms. While such vertical thinking tasks have been relatively popular, lateral thinking puzzles have received little attention. To bridge this gap, we devise BRAINTEASER: a multiple-choice Question Answering task designed to test the model's ability to exhibit lateral thinking and defy default commonsense associations. We design a three-step procedure for creating the first lateral thinking benchmark, consisting of data collection, distractor generation, and generation of adversarial examples, leading to 1,100 puzzles with high-quality annotations. To assess the consistency of lateral reasoning by models, we enrich BRAINTEASER based on a semantic and contextual reconstruction of its questions. Our experiments with state-of-the-art instruction- and commonsense language models reveal a significant gap between human and model performance, which is further widened when consistency across adversarial formats is considered. We make all of our code and data available to stimulate work on developing and evaluating lateral thinking models.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A multiple-choice Question Answering task designed to test the model's ability to exhibit lateral thinking and defy default commonsense associations, and enrich BRAINTEASER based on a semantic and contextual reconstruction of its questions to assess the consistency of lateral reasoning by models."
            },
            "score": 4
        },
        {
            "id": "7e70391224b5dd8c2b13a3165b322858da17bcdf",
            "paperId": "7e70391224b5dd8c2b13a3165b322858da17bcdf",
            "title": "Cogito Ergo Summ: Abstractive Summarization of Biomedical Papers via Semantic Parsing Graphs and Consistency Rewards",
            "abstract": "The automatic synthesis of biomedical publications catalyzes a profound research interest elicited by literature congestion. Current sequence-to-sequence models mainly rely on the lexical surface and seldom consider the deep semantic interconnections between the entities mentioned in the source document. Such superficiality translates into fabricated, poorly informative, redundant, and near-extractive summaries that severely restrict their real-world application in biomedicine, where the specialized jargon and the convoluted facts further emphasize task complexity. To fill this gap, we argue that the summarizer should acquire semantic interpretation over input, exploiting structured and unambiguous representations to capture and conserve the most relevant parts of the text content. This paper presents CogitoErgoSumm, the first framework for biomedical abstractive summarization equipping large pre-trained language models with rich semantic graphs. Precisely, we infuse graphs from two complementary semantic parsing techniques with different goals and granularities\u2014Event Extraction and Abstract Meaning Representation, also designing a reward signal to maximize information content preservation through reinforcement learning. Extensive quantitative and qualitative evaluations on the CDSR dataset show that our solution achieves competitive performance according to multiple metrics, despite using 2.5x fewer parameters. Results and ablation studies indicate that our joint text-graph model generates more enlightening, readable, and consistent summaries. Code available at: https://github.com/disi-unibo-nlp/cogito-ergo-summ.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "CogitoErgoSumm is presented, the first framework for biomedical abstractive summarization equipping large pre-trained language models with rich semantic graphs, infuse graphs from two complementary semantic parsing techniques with different goals and granularities\u2014Event Extraction and Abstract Meaning Representation."
            },
            "score": 4
        },
        {
            "id": "551166fb4f76bb041ba7d556f60f65ce75e6def9",
            "paperId": "551166fb4f76bb041ba7d556f60f65ce75e6def9",
            "title": "Image-Text Retrieval With Cross-Modal Semantic Importance Consistency",
            "abstract": "Cross-modal image-text retrieval is an important area of Vision-and-Language task that models the similarity of image-text pairs by embedding features into a shared space for alignment. To bridge the heterogeneous gap between the two modalities, current approaches achieve inter-modal alignment and intra-modal semantic relationship modeling through complex weighted combinations between items. In the intra-modal association and inter-modal interaction processes, the higher-weight items have a higher contribution to the global semantics. However, the same item always produces different contributions in the two processes, since most traditional approaches only focus on the alignment. This usually results in semantic changes and misalignment. To address this issue, this paper proposes Cross-modal Semantic Importance Consistency (CSIC) which achieves invariance in the semantic of items during aligning. The proposed technique measures the semantic importance of items obtained from intra-modal and inter-modal self-attention and learns a more reasonable representation vector by inter-calibrating the importance distribution to improve performance. We conducted extensive experiments on the Flickr30K and MS COCO datasets. The results show that our approach can significantly improve retrieval performance, proving the proposed approach\u2019s superiority and rationality.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The proposed Cross-modal Semantic Importance Consistency (CSIC) which achieves invariance in the semantic of items during aligning and can significantly improve retrieval performance, proving the proposed approach\u2019s superiority and rationality."
            },
            "score": 4
        },
        {
            "id": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
            "paperId": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
            "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such\"society of minds\"approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.",
            "year": 2023,
            "citationCount": 206,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer is presented, indicating that this approach significantly enhances mathematical and strategic reasoning across a number of tasks."
            },
            "score": 4
        },
        {
            "id": "d998ff93b2e58c9b219a9103f8c2ad714a41e4b9",
            "paperId": "d998ff93b2e58c9b219a9103f8c2ad714a41e4b9",
            "title": "The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency of Language Models",
            "abstract": "Large Language Models (LLMs) make natural interfaces to factual knowledge, but their usefulness is limited by their tendency to deliver inconsistent answers to semantically equivalent questions. For example, a model might predict both\"Anne Redpath passed away in Edinburgh.\"and\"Anne Redpath's life ended in London.\"In this work, we identify potential causes of inconsistency and evaluate the effectiveness of two mitigation strategies: up-scaling and augmenting the LM with a retrieval corpus. Our results on the LLaMA and Atlas models show that both strategies reduce inconsistency while retrieval augmentation is considerably more efficient. We further consider and disentangle the consistency contributions of different components of Atlas. For all LMs evaluated we find that syntactical form and other evaluation task artifacts impact consistency. Taken together, our results provide a better understanding of the factors affecting the factual consistency of language models.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work identifies potential causes of inconsistency and evaluates the effectiveness of two mitigation strategies: up-scaling and augmenting the LM with a retrieval corpus, showing that both strategies reduce inconsistency while retrieval augmentation is considerably more efficient."
            },
            "score": 4
        },
        {
            "id": "e96348576b682e709b2ee06ea28cd81f8bfa102b",
            "paperId": "e96348576b682e709b2ee06ea28cd81f8bfa102b",
            "title": "Exploring the Factual Consistency in Dialogue Comprehension of Large Language Models",
            "abstract": "LLMs (Large Language Models) usually interact with users in the form of dialogue and generate responses following their instructions, which naturally require dialogue comprehension abilities. However, dialogue comprehension is a general language ability which is hard to be evaluated directly. In this work, we propose to perform the evaluation focusing on the factual consistency issue with the help of the dialogue summarization task. Besides evaluating and analyzing the dialogue summarization performance (DIAC-Sum) of different LLMs, we also derive factual questions from the generated summaries and use them as a more flexible measurement of dialogue comprehension (DIAC-QA). Our evaluation shows that, on average, 26.8% of the summaries generated by LLMs contain factual inconsistency. Even ChatGPT, the strongest model evaluated, has such errors in 16% of its summaries. For answering the factual questions, which is more challenging, the average error rate of all evaluated LLMs is 36.1%. Both results indicate serious deficiencies. Detailed analysis shows that the understanding of subject/object of the conversation is still challenging for LLMs. Furthermore, to stimulate and enhance the dialogue comprehension ability of LLMs, we propose a fine-tuning paradigm with auto-constructed multi-task data, which achieved a relative error rate reduction of 11% on DIAC-QA.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "To stimulate and enhance the dialogue comprehension ability of LLMs, a fine-tuning paradigm with auto-constructed multi-task data is proposed, which achieved a relative error rate reduction of 11% on DIAC-QA."
            },
            "score": 4
        },
        {
            "id": "468d1e2d75a23fecaf96fe65d8b01ff35ea5d0bd",
            "paperId": "468d1e2d75a23fecaf96fe65d8b01ff35ea5d0bd",
            "title": "Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models",
            "abstract": "Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts inserted in English reveal a clear pattern whereby the new piece of knowledge transfers only to languages with which English has a high RankC score.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy, and conducts an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level."
            },
            "score": 4
        },
        {
            "id": "36870ac7332b45edf59e5c111dc5bd24139dc1ce",
            "paperId": "36870ac7332b45edf59e5c111dc5bd24139dc1ce",
            "title": "Factual Consistency of Multilingual Pretrained Language Models",
            "abstract": "Pretrained language models can be queried for factual knowledge, with potential applications in knowledge base acquisition and tasks that require inference. However, for that, we need to know how reliable this knowledge is, and recent work has shown that monolingual English language models lack consistency when predicting factual knowledge, that is, they fill-in-the-blank differently for paraphrases describing the same fact. In this paper, we extend the analysis of consistency to a multilingual setting. We introduce a resource, mParaRel, and investigate (i) whether multilingual language models such as mBERT and XLM-R are more consistent than their monolingual counterparts;and (ii) if such models are equally consistent across languages.We find that mBERT is as inconsistent as English BERT in English paraphrases, but that both mBERT and XLM-R exhibit a high degree of inconsistency in English and even more so for all the other 45 languages.",
            "year": 2022,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "MBERT is as inconsistent as English BERT in English paraphrases, but that both mBERT and XLM-R exhibit a high degree of inconsistency in English and even more so for all the other 45 languages."
            },
            "score": 4
        },
        {
            "id": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
            "paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
            "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "abstract": "Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).",
            "year": 2022,
            "citationCount": 1396,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting that first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths."
            },
            "score": 4
        },
        {
            "id": "261549439aebdda72b648ecc462448fd24857ac1",
            "paperId": "261549439aebdda72b648ecc462448fd24857ac1",
            "title": "Progressive-Hint Prompting Improves Reasoning in Large Language Models",
            "abstract": "The performance of Large Language Models (LLMs) in reasoning tasks depends heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency being critical methods that enhance this ability. However, these methods do not fully exploit the answers generated by the LLM to guide subsequent responses. This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers. PHP is orthogonal to CoT and self-consistency, making it easy to combine with state-of-the-art techniques to further improve performance. We conducted extensive and comprehensive experiments on seven benchmarks. The results show that PHP significantly improves accuracy while remaining highly efficient. For instance, with text-davinci-003, we observed a 4.2% improvement on GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction in sample paths with self-consistency. With GPT-4 and PHP, we achieve state-of-the-art performances on SVAMP (89.1% ->91.9%), GSM8K (92% ->95.5%), AQuA (76.4% ->79.9%) and MATH (50.3% ->53.9%).",
            "year": 2023,
            "citationCount": 64,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers."
            },
            "score": 4
        },
        {
            "id": "4ea413e5a21a743d68c92e7f169535d0543f6051",
            "paperId": "4ea413e5a21a743d68c92e7f169535d0543f6051",
            "title": "On Improving Summarization Factual Consistency from Natural Language Feedback",
            "abstract": "Despite the recent progress in language generation models, their outputs may not always meet user expectations. In this work, we study whether informational feedback in natural language can be leveraged to improve generation quality and user preference alignment. To this end, we consider factual consistency in summarization, the quality that the summary should only contain information supported by the input documents, as the user-expected preference. We collect a high-quality dataset, DeFacto, containing human demonstrations and informational natural language feedback consisting of corrective instructions, edited summaries, and explanations with respect to the factual consistency of the summary. Using our dataset, we study three natural language generation tasks: (1) editing a summary by following the human feedback, (2) generating human feedback for editing the original summary, and (3) revising the initial summary to correct factual errors by generating both the human feedback and edited summary. We show that DeFacto can provide factually consistent human-edited summaries and further insights into summarization factual consistency thanks to its informational natural language feedback. We further demonstrate that fine-tuned language models can leverage our dataset to improve the summary factual consistency, while large language models lack the zero-shot learning ability in our proposed tasks that require controllable text generation.",
            "year": 2022,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "DeFacto can provide factually consistent human-edited summaries and further insights into summarization factual consistency thanks to its informational natural language feedback, and it is demonstrated that fine-tuned language models can leverage the dataset to improve the summary factual consistency."
            },
            "score": 4
        },
        {
            "id": "a1853500aa8ff46e440a6f2e76bfbb2a06b25378",
            "paperId": "a1853500aa8ff46e440a6f2e76bfbb2a06b25378",
            "title": "TrumorGPT: Query Optimization and Semantic Reasoning over Networks for Automated Fact-Checking",
            "abstract": "In the age of social media, the rapid spread of misinformation and rumors has led to the emergence of infodemics, where false information poses a significant threat to society. To combat this issue, we introduce TrumorGPT, a novel generative artificial intelligence solution designed for automated fact-checking. TrumorGPT aims to distinguish \"trumors\", which are rumors that turn out to be true, providing a crucial tool in differentiating between mere speculation and verified facts. This framework merges machine learning with natural language processing techniques, leveraging a large language model (LLM) with few-shot learning for knowledge graph construction and semantic reasoning. TrumorGPT addresses the \"hallucination\" issue common in LLMs and the limitations of static training data by incorporating retrieval-augmented generation. This approach involves accessing and utilizing information from regularly updated knowledge graphs that consist of the latest news and information, ensuring that fact-checking of TrumorGPT is based on the most recent data. Accessing updated knowledge graphs greatly enhances the proficiency of TrumorGPT in delivering accurate and reliable information promptly. Evaluating with extensive datasets, TrumorGPT demonstrates superior performance in automated fact-checking. Its ability to effectively conduct automated fact-checking across various platforms marks a critical step forward in the fight against misinformation, enhancing trust and accuracy in the digital information age.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "TrumorGPT aims to distinguish \"trumors\", which are rumors that turn out to be true, providing a crucial tool in differentiating between mere speculation and verified facts, in the fight against misinformation."
            },
            "score": 4
        },
        {
            "id": "8767dcddfb856db4bfa1e150470fc99f51f43835",
            "paperId": "8767dcddfb856db4bfa1e150470fc99f51f43835",
            "title": "KELLMRec: Knowledge-Enhanced Large Language Models for Recommendation",
            "abstract": "The utilization of semantic information is an important research problem in the field of recommender systems, which aims to complement the missing parts of mainstream ID-based approaches. With the rise of LLM, its ability to act as a knowledge base and its reasoning capability have opened up new possibilities for this research area, making LLM-based recommendation an emerging research direction. However, directly using LLM to process semantic information for recommendation scenarios is unreliable and sub-optimal due to several problems such as hallucination. A promising way to cope with this is to use external knowledge to aid LLM in generating truthful and usable text. Inspired by the above motivation, we propose a Knowledge-Enhanced LLMRec method. In addition to using external knowledge in prompts, the proposed method also includes a knowledge-based contrastive learning scheme for training. Experiments on public datasets and in-enterprise datasets validate the effectiveness of the proposed method.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a Knowledge-Enhanced LLMRec method, which uses external knowledge to aid LLM in generating truthful and usable text and includes a knowledge-based contrastive learning scheme for training."
            },
            "score": 4
        },
        {
            "id": "182921694e351be521fa895e6741287a1a1cbc78",
            "paperId": "182921694e351be521fa895e6741287a1a1cbc78",
            "title": "Knowledge Graphs as Context Sources for LLM-Based Explanations of Learning Recommendations",
            "abstract": "In the era of personalized education, the provision of comprehensible explanations for learning recommendations is of a great value to enhance the learner's understanding and engagement with the recommended learning content. Large language models (LLMs) and generative AI in general have recently opened new doors for generating human-like explanations, for and along learning recommendations. However, their precision is still far away from acceptable in a sensitive field like education. To harness the abilities of LLMs, while still ensuring a high level of precision towards the intent of the learners, this paper proposes an approach to utilize knowledge graphs (KG) as a source of factual context, for LLM prompts, reducing the risk of model hallucinations, and safeguarding against wrong or imprecise information, while maintaining an application-intended learning context. We utilize the semantic relations in the knowledge graph to offer curated knowledge about learning recommendations. With domain-experts in the loop, we design the explanation as a textual template, which is filled and completed by the LLM. Domain experts were integrated in the prompt engineering phase as part of a study, to ensure that explanations include information that is relevant to the learner. We evaluate our approach quantitatively using Rouge-N and Rouge-L measures, as well as qualitatively with experts and learners. Our results show an enhanced recall and precision of the generated explanations compared to those generated solely by the GPT model, with a greatly reduced risk of generating imprecise information in the final learning explanation.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes an approach to utilize knowledge graphs (KG) as a source of factual context, for LLM prompts, reducing the risk of model hallucinations, and safeguarding against wrong or imprecise information, while maintaining an application-intended learning context."
            },
            "score": 4
        },
        {
            "id": "390342245563699a88bd5b8f4093e2c0e199b0c2",
            "paperId": "390342245563699a88bd5b8f4093e2c0e199b0c2",
            "title": "Keyword Augmented Retrieval: Novel framework for Information Retrieval integrated with speech interface",
            "abstract": "Retrieving answers in a quick and low cost manner without hallucinations from a combination of structured and unstructured data using Language models is a major hurdle. This is what prevents employment of Language models in knowledge retrieval automation. This becomes accentuated when one wants to integrate a speech interface on top of a text based knowledge retrieval system. Besides, for commercial search and chat-bot applications, complete reliance on commercial large language models (LLMs) like GPT 3.5 etc. can be very costly. In the present study, the authors have addressed the aforementioned problem by first developing a keyword based search framework which augments discovery of the context from the document to be provided to the LLM. The keywords in turn are generated by a relatively smaller LLM and cached for comparison with keywords generated by the same smaller LLM against the query raised. This significantly reduces time and cost to find the context within documents. Once the context is set, a larger LLM uses that to provide answers based on a prompt tailored for Q\\&A. This research work demonstrates that use of keywords in context identification reduces the overall inference time and cost of information retrieval. Given this reduction in inference time and cost with the keyword augmented retrieval framework, a speech based interface for user input and response readout was integrated. This allowed a seamless interaction with the language model.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This research work demonstrates that use of keywords in context identification reduces the overall inference time and cost of information retrieval."
            },
            "score": 4
        },
        {
            "id": "d4fc988c6510420a5290dfe8d1a991ca4878d696",
            "paperId": "d4fc988c6510420a5290dfe8d1a991ca4878d696",
            "title": "Diversity Measures: Domain-Independent Proxies for Failure in Language Model Queries",
            "abstract": "Hallucinations and reasoning errors limit the ability of large language models (LLMs) to serve as a natural language interface for various prompts. Meanwhile, error prediction in large language models often relies on domain-specific information. In this paper, we present domain independent measures for quantification of error in the response of a large language model based on the diversity of responses to a given prompt specifically considering components of the response. This results in an approach that is well-suited for prompts where the response can be viewed as an answer set such as semantic prompts, a common natural language interface use-case. We describe how three such measures - based on entropy, Gini impurity, and centroid distance - can be employed. We perform a suite of experiments on multiple datasets and temperature settings to demonstrate that these measures strongly correlate with the probability of failure. Additionally, we present empirical results demonstrating how these measures can be applied to few-shot prompting, chain-of-thought reasoning, and error detection.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 4
        },
        {
            "id": "8c49a6be7f8c88dbca41b25dd929f4df2ffed7ef",
            "paperId": "8c49a6be7f8c88dbca41b25dd929f4df2ffed7ef",
            "title": "Token-Level Contrastive Learning with Modality-Aware Prompting for Multimodal Intent Recognition",
            "abstract": "Multimodal intent recognition aims to leverage diverse modalities such as expressions, body movements and tone of speech to comprehend user's intent, constituting a critical task for understanding human language and behavior in real-world multimodal scenarios. Nevertheless, the majority of existing methods ignore potential correlations among different modalities and own limitations in effectively learning semantic features from nonverbal modalities. In this paper, we introduce a token-level contrastive learning method with modality-aware prompting (TCL-MAP) to address the above challenges. To establish an optimal multimodal semantic environment for text modality, we develop a modality-aware prompting module (MAP), which effectively aligns and fuses features from text, video and audio modalities with similarity-based modality alignment and cross-modality attention mechanism. Based on the modality-aware prompt and ground truth labels, the proposed token-level contrastive learning framework (TCL) constructs augmented samples and employs NT-Xent loss on the label token. Specifically, TCL capitalizes on the optimal textual semantic insights derived from intent labels to guide the learning processes of other modalities in return. Extensive experiments show that our method achieves remarkable improvements compared to state-of-the-art methods. Additionally, ablation analyses demonstrate the superiority of the modality-aware prompt over the handcrafted prompt, which holds substantial significance for multimodal prompt learning. The codes are released at https://github.com/thuiar/TCL-MAP.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper develops a modality-aware prompting module (MAP), which effectively aligns and fuses features from text, video and audio modalities with similarity-based modality alignment and cross-modality attention mechanism, and introduces a token-level contrastive learning framework (TCL) to address the above challenges."
            },
            "score": 4
        },
        {
            "id": "4ee95eb13f6be5899a5be9f1940080ffb1b0cf8e",
            "paperId": "4ee95eb13f6be5899a5be9f1940080ffb1b0cf8e",
            "title": "CPSeg: Finer-grained Image Semantic Segmentation via Chain-of-Thought Language Prompting",
            "abstract": "Natural scene analysis and remote sensing imagery offer immense potential for advancements in large-scale language-guided context-aware data utilization. This potential is particularly significant for enhancing performance in downstream tasks such as object detection and segmentation with designed language prompting. In light of this, we introduce the CPSeg (Chain-of-Thought Language Prompting for Finer-grained Semantic Segmentation), an innovative framework designed to augment image segmentation performance by integrating a novel \"Chain-of-Thought\" process that harnesses textual information associated with images. This groundbreaking approach has been applied to a flood disaster scenario. CPSeg encodes prompt texts derived from various sentences to formulate a coherent chain-of-thought. We use a new vision-language dataset, FloodPrompt, which includes images, semantic masks, and corresponding text information. This not only strengthens the semantic understanding of the scenario but also aids in the key task of semantic segmentation through an interplay of pixel and text matching maps. Our qualitative and quantitative analyses validate the effectiveness of CPSeg.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The CPSeg (Chain-of-Thought Language Prompting for Finer-grained Semantic Segmentation), an innovative framework designed to augment image segmentation performance by integrating a novel \"Chain-of-Thought\" process that harnesses textual information associated with images, is introduced."
            },
            "score": 4
        },
        {
            "id": "d7d8c21bb385b701687f220466e519fc898502a7",
            "paperId": "d7d8c21bb385b701687f220466e519fc898502a7",
            "title": "Language-Aware Soft Prompting for Vision & Language Foundation Models",
            "abstract": "This paper is on soft prompt learning for Vision & Language (V&L) models. Similarly to their NLP counterparts, V&L models can be adapted to a downstream task by learning soft continuous prompts using a few training examples. Current methods learn the soft prompts by minimizing a cross-entropy loss using as class weights the features obtained by passing the prompts plus the class names through the text encoder. Such methods, however, signi\ufb01cantly over\ufb01t the training data suffering from large accuracy degradation when tested on unseen classes from the same domain. Our main contribution, in this paper, is a surprisingly simple approach to alleviate this problem: we use a second cross entropy loss to minimize the distance between the learned soft prompts and a set of hand-engineered manual prompts (obtained by prompt engineering). The proposed loss can be interpreted in multiple ways including as a regularizer, as a means for language-based augmentation, and as a way of learning more discriminative class centroids. Importantly, our formulation is inherently amenable to including, during training, virtual classes, i.e. class names for which no visual samples are available, further increasing the robustness of the learned prompts. Through extensive evaluations on 11 datasets, we show that our approach (a) signi\ufb01cantly outperforms all prior works on soft prompting, and (b) matches and surpasses, for the \ufb01rst time, the accuracy on novel classes obtained by hand-crafted prompts and CLIP for the majority of the test datasets. Code will be made available.",
            "year": 2022,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work uses a second cross entropy loss to minimize the distance between the learned soft prompts and a set of hand-engineered manual prompts (obtained by prompt engineering), and can be interpreted in multiple ways including as a regularizer, as a means for language-based augmentation, and as a way of learning more discriminating class centroids."
            },
            "score": 4
        },
        {
            "id": "2f604e6e42d79ab35fa4cf6a88540521d1a4cdc5",
            "paperId": "2f604e6e42d79ab35fa4cf6a88540521d1a4cdc5",
            "title": "Fixed Input Parameterization for Efficient Prompting",
            "abstract": "Recent works have shown that attaching prompts to the input is effective at conditioning Language Models (LM) to perform specific tasks. However, prompts are always included in the input text during inference, even when they are fixed, thus incurring substantial computational and memory overhead. Also, there is currently no straightforward method of utilizing prompts that are longer than the maximum input length of the LMs without incurring additional costs during inference. We formally define Fixed Input Parameterization (FIP) problem that focuses on injecting the fixed prompt into the parameters of an LM to be an efficient alternative to attaching fixed prompts to the input. We show that in scenarios with long fixed prompts, FIP can be up to 280 times more efficient in terms of total FLOPs than previous approaches. We further explore methodologies for FIP and show promising results in persona-dependent conversation, semantic parsing, and zero-shot learning with task instructions. Through these explorations, we show that FIP can be a promising direction for conditioning language models, in scenarios with long and fixed prompts 1 .",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that in scenarios with long fixed prompts, FIP can be up to 280 times more efficient in terms of total FLOPs than previous approaches and further explore methodologies for FIP and shows promising results in persona-dependent conversation, semantic parsing, and zero-shot learning with task instructions."
            },
            "score": 3
        },
        {
            "id": "4626890a6cbaa92f20d0bb181a499d23c2cf01a1",
            "paperId": "4626890a6cbaa92f20d0bb181a499d23c2cf01a1",
            "title": "Learning Open-Vocabulary Semantic Segmentation Models From Natural Language Supervision",
            "abstract": "This paper considers the problem of open-vocabulary semantic segmentation (OVS), that aims to segment objects of arbitrary classes beyond a pre-defined, closed-set categories. The main contributions are as follows: First, we propose a transformer-based model for OVS, termed as OVSegmentor, which only exploits web-crawled imagetext pairs for pre-training without using any mask annotations. OVSegmentor assembles the image pixels into a set of learnable group tokens via a slotattention based binding module, then aligns the group tokens to corresponding caption embeddings. Second, we propose two proxy tasks for training, namely masked entity completion and cross-image mask consistency. The former aims to infer all masked entities in the caption given group tokens, that enables the model to learn fine-grained alignment between visual groups and text entities. The latter enforces consistent mask predictions between images that contain shared entities, encouraging the model to learn visual invariance. Third, we construct CC4M dataset for pre-training by filtering CC12M with frequently appeared entities, which significantly improves training efficiency. Fourth, we perform zero-shot transfer on four benchmark datasets, PASCAL VOC, PASCAL Context, COCO Object, and ADE20K. OVSegmentor achieves superior results over state-of-the-art approaches on PASCAL VOC using only 3% data (4M vs 134M) for pre-training.",
            "year": 2023,
            "citationCount": 40,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A transformer-based model for OVS is proposed, termed as OVSegmentor, which only exploits web-crawled imagetext pairs for pre-training without using any mask annotations, and achieves superior results over state-of-the-art approaches on PASCAL VOC using only 3% data forPre-training."
            },
            "score": 3
        },
        {
            "id": "e33ff3cf8e209762ca27ca320cca248198194833",
            "paperId": "e33ff3cf8e209762ca27ca320cca248198194833",
            "title": "Red Teaming Game: A Game-Theoretic Framework for Red Teaming Language Models",
            "abstract": "Deployable Large Language Models (LLMs) must conform to the criterion of helpfulness and harmlessness, thereby achieving consistency between LLMs outputs and human values. Red-teaming techniques constitute a critical way towards this criterion. Existing work rely solely on manual red team designs and heuristic adversarial prompts for vulnerability detection and optimization. These approaches lack rigorous mathematical formulation, thus limiting the exploration of diverse attack strategy within quantifiable measure and optimization of LLMs under convergence guarantees. In this paper, we present Red-teaming Game (RTG), a general game-theoretic framework without manual annotation. RTG is designed for analyzing the multi-turn attack and defense interactions between Red-team language Models (RLMs) and Blue-team Language Model (BLM). Within the RTG, we propose Gamified Red-teaming Solver (GRTS) with diversity measure of the semantic space. GRTS is an automated red teaming technique to solve RTG towards Nash equilibrium through meta-game analysis, which corresponds to the theoretically guaranteed optimization direction of both RLMs and BLM. Empirical results in multi-turn attacks with RLMs show that GRTS autonomously discovered diverse attack strategies and effectively improved security of LLMs, outperforming existing heuristic red-team designs. Overall, RTG has established a foundational framework for red teaming tasks and constructed a new scalable oversight technique for alignment.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Red-teaming Game (RTG), a general game-theoretic framework without manual annotation, is designed for analyzing the multi-turn attack and defense interactions between Red-team language Models (RLMs) and Blue-team Language Model (BLM)."
            },
            "score": 3
        },
        {
            "id": "47f97856e4890b9d96daa5dfdf008eda148e0d70",
            "paperId": "47f97856e4890b9d96daa5dfdf008eda148e0d70",
            "title": "Generalized Category Discovery with Large Language Models in the Loop",
            "abstract": "Generalized Category Discovery (GCD) is a crucial task that aims to recognize both known and novel categories from a set of unlabeled data by utilizing a few labeled data with only known categories. Due to the lack of supervision and category information, current methods usually perform poorly on novel categories and struggle to reveal semantic meanings of the discovered clusters, which limits their applications in the real world. To mitigate above issues, we propose Loop, an end-to-end active-learning framework that introduces Large Language Models (LLMs) into the training loop, which can boost model performance and generate category names without relying on any human efforts. Specifically, we first propose Local Inconsistent Sampling (LIS) to select samples that have a higher probability of falling to wrong clusters, based on neighborhood prediction consistency and entropy of cluster assignment probabilities. Then we propose a Scalable Query strategy to allow LLMs to choose true neighbors of the selected samples from multiple candidate samples. Based on the feedback from LLMs, we perform Refined Neighborhood Contrastive Learning (RNCL) to pull samples and their neighbors closer to learn clustering-friendly representations. Finally, we select representative samples from clusters corresponding to novel categories to allow LLMs to generate category names for them. Extensive experiments on three benchmark datasets show that Loop outperforms SOTA models by a large margin and generates accurate category names for the discovered clusters. We will release our code and data after publication.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Loop is an end-to-end active-learning framework that introduces Large Language Models (LLMs) into the training loop, which can boost model performance and generate category names without relying on any human efforts."
            },
            "score": 3
        },
        {
            "id": "e6e3020251d354159885a7c603c0e6fa28144f2a",
            "paperId": "e6e3020251d354159885a7c603c0e6fa28144f2a",
            "title": "Reliability Check: An Analysis of GPT-3\u2019s Response to Sensitive Topics and Prompt Wording",
            "abstract": "Large language models (LLMs) have become mainstream technology with their versatile use cases and impressive performance. Despite the countless out-of-the-box applications, LLMs are still not reliable. A lot of work is being done to improve the factual accuracy, consistency, and ethical standards of these models through fine-tuning, prompting, and Reinforcement Learning with Human Feedback (RLHF), but no systematic analysis of the responses of these models to different categories of statements, or on their potential vulnerabilities to simple prompting changes is available. In this work, we analyze what confuses GPT-3: how the model responds to certain sensitive topics and what effects the prompt wording has on the model response. We find that GPT-3 correctly disagrees with obvious Conspiracies and Stereotypes but makes mistakes with common Misconceptions and Controversies. The model responses are inconsistent across prompts and settings, highlighting GPT-3\u2019s unreliability.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work analyzes what confuses GPT-3: how the model responds to certain sensitive topics and what effects the prompt wording has on the model response."
            },
            "score": 3
        },
        {
            "id": "3e7fd5f19f992d1086870eeb824edd51cdeb0682",
            "paperId": "3e7fd5f19f992d1086870eeb824edd51cdeb0682",
            "title": "Tumor segmentation on whole slide images: training or prompting?",
            "abstract": "Tumor segmentation stands as a pivotal task in cancer diagnosis. Given the immense dimensions of whole slide images (WSI) in histology, deep learning approaches for WSI classification mainly operate at patch-wise or superpixel-wise level. However, these solutions often struggle to capture global WSI information and cannot directly generate the binary mask. Downsampling the WSI and performing semantic segmentation is another possible approach. While this method offers computational efficiency, it necessitates a large amount of annotated data since resolution reduction may lead to information loss. Visual prompting is a novel paradigm that allows the model to perform new tasks by making subtle modifications to the input space, rather than adapting the model itself. Such approach has demonstrated promising results on many computer vision tasks. In this paper, we show the efficacy of visual prompting in the context of tumor segmentation for three distinct organs. In comparison to classical methods trained for this specific task, our findings reveal that, with appropriate prompt examples, visual prompting can achieve comparable or better performance without extensive fine-tuning.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The efficacy of visual prompting is shown in the context of tumor segmentation for three distinct organs and it is revealed that, with appropriate prompt examples, visual prompting can achieve comparable or better performance without extensive fine-tuning."
            },
            "score": 3
        },
        {
            "id": "9d56b27ae6b89142f9c967fbd499ef223655937c",
            "paperId": "9d56b27ae6b89142f9c967fbd499ef223655937c",
            "title": "Visual Hallucination Elevates Speech Recognition",
            "abstract": "Due to the detrimental impact of noise on the conventional audio speech recognition (ASR) task, audio-visual speech recognition~(AVSR) has been proposed by incorporating both audio and visual video signals. Although existing methods have demonstrated that the aligned visual input of lip movements can enhance the robustness of AVSR systems against noise, the paired videos are not always \navailable during inference, leading to the problem of \nthe missing visual modality, which restricts their practicality in real-world scenarios. \n\nTo tackle this problem, we propose a Discrete Feature based Visual Generative Model (DFVGM) which exploits semantic correspondences between the audio and visual modalities \nduring training, generating \nvisual hallucinations in lieu of\nreal videos during inference. To achieve that, the \nprimary challenge is to generate the visual hallucination \ngiven the noisy audio while preserving semantic correspondences with the clean speech. To \ntackle this challenge, we \nstart with training the audio encoder in the Audio-Only (AO) setting, which generates continuous semantic features closely associated with the linguistic information. Simultaneously, the visual encoder is trained in the Visual-Only (VO) setting, producing visual features that are phonetically related. Next, we employ K-means to \ndiscretize the continuous audio and visual feature spaces. The discretization step \nallows DFVGM to capture high-level semantic structures that are more resilient to noise and generate \nvisual hallucinations with high quality. \nTo evaluate the effectiveness and robustness of our approach, we conduct extensive experiments on two publicly available datasets. The results demonstrate that our method achieves a remarkable 53% relative reduction (30.5%->12.9%) in Word Error Rate (WER) on average compared to the current state-of-the-art Audio-Only (AO) baselines while maintaining comparable results (< 5% difference) under the Audio-Visual (AV) setting even without video as input.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Discrete Feature based Visual Generative Model (DFVGM) which exploits semantic correspondences between the audio and visual modalities during training, generating visual hallucinations in lieu of real videos during inference and achieves a remarkable 53% relative reduction in Word Error Rate."
            },
            "score": 3
        },
        {
            "id": "7f8cf90b6de9dbb57b80e2855680d76923550764",
            "paperId": "7f8cf90b6de9dbb57b80e2855680d76923550764",
            "title": "Are we describing the same sound? An analysis of word embedding spaces of expressive piano performance",
            "abstract": "Semantic embeddings play a crucial role in natural language-based information retrieval. Embedding models represent words and contexts as vectors whose spatial configuration is derived from the distribution of words in large text corpora. While such representations are generally very powerful, they might fail to account for fine-grained domain-specific nuances. In this article, we investigate this uncertainty for the domain of characterizations of expressive piano performance. Using a music research dataset of free text performance characterizations and a follow-up study sorting the annotations into clusters, we derive a ground truth for a domain-specific semantic similarity structure. We test five embedding models and their similarity structure for correspondence with the ground truth. We further assess the effects of contextualizing prompts, hubness reduction, cross-modal similarity, and k-means clustering. The quality of embedding models shows great variability with respect to this task; more general models perform better than domain-adapted ones and the best model configurations reach human-level agreement.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This article derives a ground truth for a domain-specific semantic similarity structure for expressive piano performance characterizations using a music research dataset of free text performance characterizations and a follow-up study sorting the annotations into clusters."
            },
            "score": 3
        },
        {
            "id": "21305ff0bdedc48eb46bf5f2ddfcd842927c0ac9",
            "paperId": "21305ff0bdedc48eb46bf5f2ddfcd842927c0ac9",
            "title": "Fine-grained Contract NER using instruction based model",
            "abstract": "Lately, instruction-based techniques have made significant strides in improving performance in few-shot learning scenarios. They achieve this by bridging the gap between pre-trained language models and fine-tuning for specific downstream tasks. Despite these advancements, the performance of Large Language Models (LLMs) in information extraction tasks like Named Entity Recognition (NER), using prompts or instructions, still falls short of supervised baselines. The reason for this performance gap can be attributed to the fundamental disparity between NER and LLMs. NER is inherently a sequence labeling task, where the model must assign entity-type labels to individual tokens within a sentence. In contrast, LLMs are designed as a text generation task. This distinction between semantic labeling and text generation leads to subpar performance. In this paper, we transform the NER task into a text-generation task that can be readily adapted by LLMs. This involves enhancing source sentences with task-specific instructions and answer choices, allowing for the identification of entities and their types within natural language. We harness the strength of LLMs by integrating supervised learning within them. The goal of this combined strategy is to boost the performance of LLMs in extraction tasks like NER while simultaneously addressing hallucination issues often observed in LLM-generated content. A novel corpus Contract NER comprising seven frequently observed contract categories, encompassing named entities associated with 18 distinct legal entity types is released along with our baseline models. Our models and dataset are available to the community for future research * .",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The NER task is transformed into a text-generation task that can be readily adapted by Large Language Models, and a novel corpus Contract NER comprising seven frequently observed contract categories, encompassing named entities associated with 18 distinct legal entity types is released."
            },
            "score": 3
        },
        {
            "id": "ed66b0d9e872aae097bb8772c6c9422b442620f9",
            "paperId": "ed66b0d9e872aae097bb8772c6c9422b442620f9",
            "title": "Fine-grained Contract NER using instruction based mode",
            "abstract": "Lately, instruction-based techniques have made significant strides in improving performance in few-shot learning scenarios. They achieve this by bridging the gap between pre-trained language models and fine-tuning for specific downstream tasks. Despite these advancements, the performance of Large Language Models (LLMs) in information extraction tasks like Named Entity Recognition (NER), using prompts or instructions, still falls short of supervised baselines. The reason for this performance gap can be attributed to the fundamental disparity between NER and LLMs. NER is inherently a sequence labeling task, where the model must assign entity-type labels to individual tokens within a sentence. In contrast, LLMs are designed as a text generation task. This distinction between semantic labeling and text generation leads to subpar performance. In this paper, we transform the NER task into a text-generation task that can be readily adapted by LLMs. This involves enhancing source sentences with task-specific instructions and answer choices, allowing for the identification of entities and their types within natural language. We harness the strength of LLMs by integrating supervised learning within them. The goal of this combined strategy is to boost the performance of LLMs in extraction tasks like NER while simultaneously addressing hallucination issues often observed in LLM-generated content. A novel corpus Contract NER comprising seven frequently observed contract categories, encompassing named entities associated with 18 distinct legal entity types is released along with our baseline models. Our models and dataset are available to the community for future research 1 .",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The NER task is transformed into a text-generation task that can be readily adapted by Large Language Models, and a novel corpus Contract NER comprising seven frequently observed contract categories, encompassing named entities associated with 18 distinct legal entity types is released."
            },
            "score": 3
        },
        {
            "id": "191e300e381d4128b749d16fe3d83c8643a3bd1f",
            "paperId": "191e300e381d4128b749d16fe3d83c8643a3bd1f",
            "title": "Retrieval-augmented GPT-3.5-based Text-to-SQL Framework with Sample-aware Prompting and Dynamic Revision Chain",
            "abstract": "Text-to-SQL aims at generating SQL queries for the given natural language questions and thus helping users to query databases. Prompt learning with large language models (LLMs) has emerged as a recent approach, which designs prompts to lead LLMs to understand the input question and generate the corresponding SQL. However, it faces challenges with strict SQL syntax requirements. Existing work prompts the LLMs with a list of demonstration examples (i.e. question-SQL pairs) to generate SQL, but the fixed prompts can hardly handle the scenario where the semantic gap between the retrieved demonstration and the input question is large. In this paper, we propose a retrieval-augmented prompting method for a LLM-based Text-to-SQL framework, involving sample-aware prompting and a dynamic revision chain. Our approach incorporates sample-aware demonstrations, which include the composition of SQL operators and fine-grained information related to the given question. To retrieve questions sharing similar intents with input questions, we propose two strategies for assisting retrieval. Firstly, we leverage LLMs to simplify the original questions, unifying the syntax and thereby clarifying the users' intentions. To generate executable and accurate SQLs without human intervention, we design a dynamic revision chain which iteratively adapts fine-grained feedback from the previously generated SQL. Experimental results on three Text-to-SQL benchmarks demonstrate the superiority of our method over strong baseline models.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a retrieval-augmented prompting method for a LLM-based Text-to-SQL framework, involving sample-aware prompting and a dynamic revision chain which iteratively adapts fine-grained feedback from the previously generated SQL."
            },
            "score": 3
        },
        {
            "id": "13db9968398abfbaf9d371dde44e6bbce8809933",
            "paperId": "13db9968398abfbaf9d371dde44e6bbce8809933",
            "title": "SAGE: Semantic-Aware Global Explanations for Named Entity Recognition",
            "abstract": "In the last decades, deep learning approaches achieved impressive results in many research fields, such as Computer Vision and Natural Language Processing (NLP). NLP in particular has greatly benefit from unsupervised methods that allow to learn distributed representation of language. On the race for better performances Language Models have reached hundred of billions parameters nowadays. Despite the remarkable results, deep models are still far from being fully exploited in real world applications. Indeed, these approaches are black-boxes, i.e. they are not interpretable by design nor explainable, which is often crucial to make decisions in business. Several task-agnostic methods have been proposed in literature to explain models' decisions. Most techniques rely on the \u201clocal\u201d assumption, i.e. explanations are made example-wise. In this paper instead, we present a post-hoc method to produce highly interpretable global rules to explain NLP classifiers. Rules are extracted with a data mining approach on a semantically enriched input representation, instead of using words/wordpieces solely. Semantic information yields more abstract and general rules that are both more explanatory and less complex, while being also better at reflecting the model behaviour. In the experiments we focus on Named Entity Recognition, an NLP task where explainability is under-investigated. We explain the predictions of BERT NER classifiers trained on two popular benchmarks, CoNLL03 and Ontonotes, and compare our model against LIME [1] and Decision Trees.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A post-hoc method to produce highly interpretable global rules to explain NLP classifiers and focuses on Named Entity Recognition, an NLP task where explainability is under-investigated."
            },
            "score": 3
        },
        {
            "id": "3f1ac59ed7ef6085c254a16d750b81550c9939d8",
            "paperId": "3f1ac59ed7ef6085c254a16d750b81550c9939d8",
            "title": "A Span-based Target-aware Relation Model for Frame-semantic Parsing",
            "abstract": "Frame-semantic Parsing (FSP) is a challenging and critical task in Natural Language Processing (NLP). Most of the existing studies decompose the FSP task into frame identification (FI) and frame semantic role labeling (FSRL) subtasks, and adopt a pipeline model architecture that clearly causes error propagation problem. However, recent jointly learning models aim to address the above problem and generally treat FSP as a span-level structured prediction task, which, unfortunately, leads to cascading error propagation problem between roles and less-efficient solutions due to huge search space of roles. To address these problems, we reformulate the FSRL task into a target-aware relation classification task and propose a novel and lightweight jointly learning framework that simultaneously processes three subtasks of FSP, including frame identification, argument identification, and role classification. The novel task formulation and jointly learning with interaction mechanisms among subtasks can help improve the overall system performance and reduce the search space and time complexity, compared with existing methods. Extensive experimental results demonstrate that our proposed model significantly outperforms 10 state-of-the-art models in terms of F1 score across two benchmark datasets.",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work reformulates the FSRL task into a target-aware relation classification task and proposes a novel and lightweight jointly learning framework that simultaneously processes three subtasks of FSP, including frame identification, argument identification, and role classification."
            },
            "score": 3
        },
        {
            "id": "195eb3b47e3096cbed67c22f1432fb95eebc62ea",
            "paperId": "195eb3b47e3096cbed67c22f1432fb95eebc62ea",
            "title": "SSCAE: A Novel Semantic, Syntactic, and Context-Aware Natural Language Adversarial Example Generator",
            "abstract": "Training a machine learning model with adver-001 sarial examples (AEs) improves its robustness 002 against adversarial attacks. Hence, it is crucial 003 to develop effective generative models to pro-004 duce high-quality AEs. Developing such mod-005 els has been much slower in natural language 006 processing (NLP). The current state-of-the-art 007 in NLP generates AEs that are somehow hu-008 man detectable and/or include semantic and lin-009 guistic defects. This paper introduces a novel, 010 practical, and efficient adversarial attack model 011 called SSCAE for S emantic, S yntactic, and 012 C ontext-aware natural language A dversarial 013 E xamples generator. SSCAE generates hu-014 manly imperceptible context-aware AEs that 015 preserve semantic consistency and source lan-016 guage\u2019s syntactical and grammatical require-017 ments. The effectiveness and superiority of 018 the proposed SSCAE model are illustrated over 019 eleven comparative experiments, extensive ab-020 lation studies, and human evaluations. 021",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel, practical, and efficient adversarial attack model called SSCAE for S emantic, S yntactic, and C ontext-aware natural language A dversarial A dversarial generator is introduced."
            },
            "score": 3
        },
        {
            "id": "0cc2a2636195735668bae9b293c0c44257c3410f",
            "paperId": "0cc2a2636195735668bae9b293c0c44257c3410f",
            "title": "SE4ExSum: An Integrated Semantic-aware Neural Approach with Graph Convolutional Network for Extractive Text Summarization",
            "abstract": "Recently, advanced techniques in deep learning such as recurrent neural network (GRU, LSTM and Bi-LSTM) and auto-encoding (attention-based transformer and BERT) have achieved great successes in multiple application domains including text summarization. Recent state-of-the-art encoding-based text summarization models such as BertSum, PreSum and DiscoBert have demonstrated significant improvements on extractive text summarization tasks. However, recent models still encounter common problems related to the language-specific dependency which requires the supports of the external NLP tools. Besides that, recent advanced text representation methods, such as BERT as the sentence-level textual encoder, also fail to fully capture the representation of a full-length document. To address these challenges, in this paper we proposed a novel semantic-ware embedding approach for extractive text summarization, called as: SE4ExSum. Our proposed SE4ExSum is an integration between the use of feature graph-of-words (FGOW) with BERT-based encoder for effectively learning the word/sentence-level representations of a given document. Then, the graph convolutional network (GCN) based encoder is applied to learn the global document's representation which is then used to facilitate the text summarization task. Extensive experiments on benchmark datasets show the effectiveness of our proposed model in comparing with recent state-of-the-art text summarization models.",
            "year": 2021,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel semantic-ware embedding approach for extractive text summarization, called as SE4ExSum, which is an integration between the use of feature graph-of-words with BERT-based encoder for effectively learning the word/sentence-level representations of a given document."
            },
            "score": 3
        },
        {
            "id": "8ff1dd6e15408581592f91434be870acc5f1bdd4",
            "paperId": "8ff1dd6e15408581592f91434be870acc5f1bdd4",
            "title": "REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative Large Language Models",
            "abstract": "We present REMARK-LLM, a novel efficient, and robust watermarking framework designed for texts generated by large language models (LLMs). Synthesizing human-like content using LLMs necessitates vast computational resources and extensive datasets, encapsulating critical intellectual property (IP). However, the generated content is prone to malicious exploitation, including spamming and plagiarism. To address the challenges, REMARK-LLM proposes three new components: (i) a learning-based message encoding module to infuse binary signatures into LLM-generated texts; (ii) a reparameterization module to transform the dense distributions from the message encoding to the sparse distribution of the watermarked textual tokens; (iii) a decoding module dedicated for signature extraction; Furthermore, we introduce an optimized beam search algorithm to guarantee the coherence and consistency of the generated content. REMARK-LLM is rigorously trained to encourage the preservation of semantic integrity in watermarked content, while ensuring effective watermark retrieval. Extensive evaluations on multiple unseen datasets highlight REMARK-LLM proficiency and transferability in inserting 2 times more signature bits into the same texts when compared to prior art, all while maintaining semantic integrity. Furthermore, REMARK-LLM exhibits better resilience against a spectrum of watermark detection and removal attacks.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "REMARK-LLM is rigorously trained to encourage the preservation of semantic integrity in watermarked content, while ensuring effective watermark retrieval, and exhibits better resilience against a spectrum of watermark detection and removal attacks."
            },
            "score": 2
        },
        {
            "id": "1a1fa2a275601758cd11cd859bffecf68f189015",
            "paperId": "1a1fa2a275601758cd11cd859bffecf68f189015",
            "title": "Dual Stage Stylization Modulation for Domain Generalized Semantic Segmentation",
            "abstract": "Obtaining sufficient labeled data for training deep models is often challenging in real-life applications. To address this issue, we propose a novel solution for single-source domain generalized semantic segmentation. Recent approaches have explored data diversity enhancement using hallucination techniques. However, excessive hallucination can degrade performance, particularly for imbalanced datasets. As shown in our experiments, minority classes are more susceptible to performance reduction due to hallucination compared to majority classes. To tackle this challenge, we introduce a dual-stage Feature Transform (dFT) layer within the Adversarial Semantic Hallucination+ (ASH+) framework. The ASH+ framework performs a dual-stage manipulation of hallucination strength. By leveraging semantic information for each pixel, our approach adaptively adjusts the pixel-wise hallucination strength, thus providing fine-grained control over hallucination. We validate the effectiveness of our proposed method through comprehensive experiments on publicly available semantic segmentation benchmark datasets (Cityscapes and SYNTHIA). Quantitative and qualitative comparisons demonstrate that our approach is competitive with state-of-the-art methods for the Cityscapes dataset and surpasses existing solutions for the SYNTHIA dataset. Code for our framework will be made readily available to the research community.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a dual-stage Feature Transform (dFT) layer within the Adversarial Semantic Hallucination+ (ASH+) framework, which adaptively adjusts the pixel-wise hallucination strength, thus providing fine-grained control over hallucination."
            },
            "score": 2
        },
        {
            "id": "9576df54881f73cb74b718705f5b423555f6ee05",
            "paperId": "9576df54881f73cb74b718705f5b423555f6ee05",
            "title": "SparseOcc: Rethinking Sparse Latent Representation for Vision-Based Semantic Occupancy Prediction",
            "abstract": "Vision-based perception for autonomous driving requires an explicit modeling of a 3D space, where 2D latent representations are mapped and subsequent 3D operators are applied. However, operating on dense latent spaces introduces a cubic time and space complexity, which limits scalability in terms of perception range or spatial resolution. Existing approaches compress the dense representation using projections like Bird's Eye View (BEV) or Tri-Perspective View (TPV). Although efficient, these projections result in information loss, especially for tasks like semantic occupancy prediction. To address this, we propose SparseOcc, an efficient occupancy network inspired by sparse point cloud processing. It utilizes a lossless sparse latent representation with three key innovations. Firstly, a 3D sparse diffuser performs latent completion using spatially decomposed 3D sparse convolutional kernels. Secondly, a feature pyramid and sparse interpolation enhance scales with information from others. Finally, the transformer head is redesigned as a sparse variant. SparseOcc achieves a remarkable 74.9% reduction on FLOPs over the dense baseline. Interestingly, it also improves accuracy, from 12.8% to 14.1% mIOU, which in part can be attributed to the sparse representation's ability to avoid hallucinations on empty voxels.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "SarseOcc is proposed, an efficient occupancy network inspired by sparse point cloud processing that achieves a remarkable 74.9% reduction on FLOPs over the dense baseline and improves accuracy, from 12.8% to 14.1% mIOU, which can be attributed to the sparse representation's ability to avoid hallucinations on empty voxels."
            },
            "score": 2
        },
        {
            "id": "6e91ee75ed37cb82cd79c360a99026c39da785cb",
            "paperId": "6e91ee75ed37cb82cd79c360a99026c39da785cb",
            "title": "Deep learning from latent spatiotemporal information of the heart: Identifying advanced bioimaging markers from echocardiograms.",
            "abstract": "A key strength of echocardiography lies in its integration of comprehensive spatiotemporal cardiac imaging data in real-time, to aid frontline or bedside patient risk stratification and management. Nonetheless, its acquisition, processing, and interpretation are known to all be subject to heterogeneity from its reliance on manual and subjective human tracings, which challenges workflow and protocol standardization and final interpretation accuracy. In the era of advanced computational power, utilization of machine learning algorithms for big data analytics in echocardiography promises reduction in cost, cognitive errors, and intra- and inter-observer variability. Novel spatiotemporal deep learning (DL) models allow the integration of temporal arm information based on unlabeled pixel echocardiographic data for convolution of an adaptive semantic spatiotemporal calibration to construct personalized 4D heart meshes, assess global and regional cardiac function, detect early valve pathology, and differentiate uncommon cardiovascular disorders. Meanwhile, data visualization on spatiotemporal DL prediction models helps extract latent temporal imaging features to develop advanced imaging biomarkers in early disease stages and advance our understanding of pathophysiology to support the development of personalized prevention or treatment strategies. Since portable echocardiograms have been increasingly used as point-of-care imaging tools to aid rural care delivery, the application of these new spatiotemporal DL techniques show the potentials in streamlining echocardiographic acquisition, processing, and data analysis to improve workflow standardization and efficiencies, and provide risk stratification and decision supporting tools in real-time, to prompt the building of new imaging diagnostic networks to enhance rural healthcare engagement.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Novel spatiotemporal deep learning models allow the integration of temporal arm information based on unlabeled pixel echocardiographic data for convolution of an adaptive semantic spatiotemporal calibration to construct personalized 4D heart meshes, assess global and regional cardiac function, detect early valve pathology, and differentiate uncommon cardiovascular disorders."
            },
            "score": 2
        },
        {
            "id": "b98f6725341e75df30f9d61e284972c6b11a7e78",
            "paperId": "b98f6725341e75df30f9d61e284972c6b11a7e78",
            "title": "Semantic-Aware Autoregressive Image Modeling for Visual Representation Learning",
            "abstract": "The development of autoregressive modeling (AM) in computer vision lags behind natural language processing (NLP) in self-supervised pre-training. This is mainly caused by the challenge that images are not sequential signals and lack a natural order when applying autoregressive modeling. In this study, inspired by human beings\u2019 way of grasping an image, i.e., focusing on the main object first, we present a semantic-aware autoregressive image modeling (SemAIM) method to tackle this challenge. The key insight of SemAIM is to autoregressively model images from the semantic patches to the less semantic patches. To this end, we first calculate a semantic-aware permutation of patches according to their feature similarities and then perform the autoregression procedure based on the permutation. In addition, considering that the raw pixels of patches are low-level signals and are not ideal prediction targets for learning high-level semantic representation, we also explore utilizing the patch features as the prediction targets. Extensive experiments are conducted on a broad range of downstream tasks, including image classification, object detection, and instance/semantic segmentation, to evaluate the performance of SemAIM. The results demonstrate SemAIM achieves state-of-the-art performance compared with other self-supervised methods. Specifically, with ViT-B, SemAIM achieves 84.1% top-1 accuracy for fine-tuning on ImageNet, 51.3% AP and 45.4% AP for object detection and instance segmentation on COCO, which outperforms the vanilla MAE by 0.5%, 1.0%, and 0.5%, respectively. Code is available at https://github.com/skyoux/SemAIM.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A semantic-aware autoregressive image modeling method to autoregressively model images from the semantic patches to the less semantic patches, which achieves state-of-the-art performance compared with other self-supervised methods."
            },
            "score": 2
        },
        {
            "id": "f4480ca069b5c1cb3d17ef1d869ae4ea5aa9873d",
            "paperId": "f4480ca069b5c1cb3d17ef1d869ae4ea5aa9873d",
            "title": "Research on Multi-knowledge Graph and Semantic-aware for Automatic Text Summarization",
            "abstract": "Automatic text summarization is one of the important tasks in NLP. Knowledge graph can improve the quality of summary by building the relationship between entities from source document. However, existing studies introducing knowledge graph have the problem of poor coordination between text content and knowledge bases. In our work, we raise a new model, ATMG, which combines feature matrix obtained from the external knowledge with original text classification label and context to allocate the attention weight. ATMG integrates external knowledge embedding into global semantic information, and chooses the appropriate knowledge graph to match different document backgrounds. In this way, the accuracy of lexical entity relationship construction can be improved so that the quality of summaries can be enhanced. A lot of experiments show that our model outperforms in different degrees compared to the traditional models, and the ablation experiments demonstrate the effectiveness of each module of ATMG.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new model, ATMG, is raised, which combines feature matrix obtained from the external knowledge with original text classification label and context to allocate the attention weight and chooses the appropriate knowledge graph to match different document backgrounds."
            },
            "score": 2
        },
        {
            "id": "5311db0b04b95fa43b886387fb1f484055638660",
            "paperId": "5311db0b04b95fa43b886387fb1f484055638660",
            "title": "Modal-aware Visual Prompting for Incomplete Multi-modal Brain Tumor Segmentation",
            "abstract": "In the realm of medical imaging, distinct magnetic resonance imaging (MRI) modalities can provide complementary medical insights. However, it is not uncommon for one or more modalities to be absent due to image corruption, artifacts, acquisition protocols, allergies to contrast agents, or cost constraints, posing a significant challenge for perceiving the modality-absent state in incomplete modality segmentation.In this work, we introduce a novel incomplete multi-modal segmentation framework called Modal-aware Visual Prompting (MAVP), which draws inspiration from the widely used pre-training and prompt adjustment protocol employed in natural language processing (NLP). In contrast to previous prompts that typically use textual network embeddings, we utilize embeddings as the prompts generated by a modality state classifier that focuses on the missing modality states. Additionally, we integrate modality state prompts into both the extraction stage of each modality and the modality fusion stage to facilitate intra/inter-modal adaptation. Our approach achieves state-of-the-art performance in various modality-incomplete scenarios compared to incomplete modality-specific solutions.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a novel incomplete multi-modal segmentation framework called Modal-aware Visual Prompting (MAVP), which draws inspiration from the widely used pre-training and prompt adjustment protocol employed in natural language processing (NLP), and utilizes embeddings as the prompts generated by a modality state classifier that focuses on the missing modality states."
            },
            "score": 2
        },
        {
            "id": "e4f230dc5fce17293a3f4cf39150a52e45808125",
            "paperId": "e4f230dc5fce17293a3f4cf39150a52e45808125",
            "title": "Semantic similarity measure of natural language text through machine learning and a keyword\u2010aware cross\u2010encoder\u2010ranking summarizer\u2014A case study using UCGIS GIS&T body of knowledge",
            "abstract": "Initiated by the University Consortium of Geographic Information Science (UCGIS), the GIS&T Body of Knowledge (BoK) is a community\u2010driven endeavor to define, develop, and document geospatial topics related to geographic information science and technologies (GIS&T). In recent years, GIS&T BoK has undergone rigorous development in terms of its topic re\u2010organization and content updating, resulting in a new digital version of the project. While the BoK topics provide useful materials for researchers and students to learn about GIS, the semantic relationships among the topics, such as semantic similarity, should also be identified so that a better and automated topic navigation can be achieved. Currently, the related topics are either defined manually by editors or authors, which may result in an incomplete assessment of topic relationships. To address this challenge, our research evaluates the effectiveness of multiple natural language processing (NLP) techniques in extracting semantics from text, including both deep neural networks and traditional machine learning approaches. Besides, a novel text summarization\u2014KACERS (Keyword\u2010Aware Cross\u2010Encoder\u2010Ranking Summarizer)\u2014is proposed to generate a semantic summary of scientific publications. By identifying the semantic linkages among key topics, this work guides the future development and content organization of the GIS&T BoK project. It also offers a new perspective on the use of machine learning techniques for analyzing scientific publications and demonstrates the potential of the KACERS summarizer in semantic understanding of long text documents.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work guides the future development and content organization of the GIS&T BoK project and offers a new perspective on the use of machine learning techniques for analyzing scientific publications and demonstrates the potential of the KACERS summarizer in semantic understanding of long text documents."
            },
            "score": 2
        },
        {
            "id": "83fc0537891bd40aaf382f9a07362b8d3873d077",
            "paperId": "83fc0537891bd40aaf382f9a07362b8d3873d077",
            "title": "AN APPROACH TO SEMANTIC EDUCATIONAL CONTENT MINING USING NATURAL LANGUAGE PROCESSING (NLP)",
            "abstract": "The COVID-19 pandemic had caused a significant disruption to the global education system. Many educational institutions faced sudden pressure to switch from face-to-face to online delivery of courses. The conventional classroom setting is no longer the primary means of delivery; instead, online education and resources have become the prominent approach. With the increasing demand for supplementary course materials to fulfill the needs of each area of study, students began to use search engines and online resources that contain discussions, practical demonstrations, and tutorial videos to aid students in their studies and course work. This study addresses the underlying challenges of retrieving relevant online educational materials by introducing an intelligent agent for semantic data mining. It works as middleware infrastructure that allow context-aware data processing and mining. YouTube was used to assess the consistency of the proposed model since it returns a large number of results in its search pool. The results showed that using the extraction of topics method, the similarities scores with the proposed model provided favorable results. Furthermore, an improvement in video ranking and sorting was realized. According to the findings, using this method provided users with a more productive and reliable study experience.",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study addresses the underlying challenges of retrieving relevant online educational materials by introducing an intelligent agent for semantic data mining that works as middleware infrastructure that allow context-aware data processing and mining."
            },
            "score": 2
        },
        {
            "id": "41283facd95e63315ea46a8ca71cf8874069f83e",
            "paperId": "41283facd95e63315ea46a8ca71cf8874069f83e",
            "title": "AN APPROACH TO SEMANTIC EDUCATIONAL CONTENT MINING USING NLP",
            "abstract": "The COVID-19 epidemic had caused one of the most significant disruptions to the global education system. Many educational institutions faced sudden pressure to switch from face-to-face to online delivery of courses. The conventional classes are no longer the primary means of delivery;instead, online education and resources have become the prominent approach. With the increasing demand for supplementary course materials to fulfill the needs of each area of study, students began to use search engines and online resources that contain discussions, practical demonstrations, and tutorial videos to aid students in their studies and course work. This study addresses the underlying challenges of retrieving relevant online educational materials by introducing an intelligent agent for semantic data mining. It works as middleware infrastructure that allow context-aware data processing and mining. YouTube was used to assess the consistency of the proposed model since it returns a large number of results in its search pool. The results showed that using the extraction of topics method, the similarities scores with the proposed model provided favorable results. Furthermore, an improvement in video ranking and sorting was realized. According to the findings, using this method provided users with a more productive and reliable study experience. \u00a9 Proceedings of the International Conference on E-Learning 2022, EL 2022 - Part of the Multi Conference on Computer Science and Information Systems 2022, MCCSIS 2022. All rights reserved.",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study addresses the underlying challenges of retrieving relevant online educational materials by introducing an intelligent agent for semantic data mining that works as middleware infrastructure that allow context-aware data processing and mining."
            },
            "score": 2
        },
        {
            "id": "c8a180288480099988b330b2558cebd45ca15a4b",
            "paperId": "c8a180288480099988b330b2558cebd45ca15a4b",
            "title": "LivelySpeaker: Towards Semantic-Aware Co-Speech Gesture Generation",
            "abstract": "Gestures are non-verbal but important behaviors accompanying people\u2019s speech. While previous methods are able to generate speech rhythm-synchronized gestures, the semantic context of the speech is generally lacking in the gesticulations. Although semantic gestures do not occur very regularly in human speech, they are indeed the key for the audience to understand the speech context in a more immersive environment. Hence, we introduce LivelySpeaker, a framework that realizes semantics-aware co-speech gesture generation and offers several control handles. In particular, our method decouples the task into two stages: script-based gesture generation and audio-guided rhythm refinement. Specifically, the script-based gesture generation leverages the pre-trained CLIP text embeddings as the guidance for generating gestures that are highly semantically aligned with the script. Then, we devise a simple but effective diffusion-based gesture generation backbone simply using pure MLPs, that is conditioned on only audio signals and learns to gesticulate with realistic motions. We utilize such powerful prior to rhyme the script-guided gestures with the audio signals, notably in a zero-shot setting. Our novel two-stage generation framework also enables several applications, such as changing the gesticulation style, editing the co-speech gestures via textual prompting, and controlling the semantic awareness and rhythm alignment with guided diffusion. Extensive experiments demonstrate the advantages of the proposed framework over competing methods. In addition, our core diffusion-based generative model also achieves state-of-the-art performance on two benchmarks. The code and model will be released to facilitate future research.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces LivelySpeaker, a framework that realizes semantics-aware co-speech gesture generation and offers several control handles, and devise a simple but effective diffusion-based gesture generation backbone simply using pure MLPs, that is conditioned on only audio signals and learns to gesticulate with realistic motions."
            },
            "score": 2
        },
        {
            "id": "17cf6e72b820eb7f1f0318fea99fc2cf77b39d6e",
            "paperId": "17cf6e72b820eb7f1f0318fea99fc2cf77b39d6e",
            "title": "Semantic-aware Image Similarity Search",
            "abstract": "\u2014This paper seeks improvement on image similarity search. In addition to support a query image as the only input parameter, we will also support additional query attributes as combined search criteria. These additional attributes include supplemental text attributes and image native attributes. Supplemental text attributes are image\u2019s title, caption or category, as a natural target of NLP. Image native attributes are attributes embedded in the image itself such as color. Combining text and image attributes injects semantic meaning from NLP to an image, enables composite search and semantic arithmetic operation on image data. Our result shows 84% accuracy of image search and 79% accuracy of image arithmetic operation.",
            "year": 2021,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper seeks improvement on image similarity search by supporting a query image as the only input parameter and additional query attributes as combined search criteria, including supplemental text attributes and image native attributes."
            },
            "score": 2
        },
        {
            "id": "61e78342047a413fdcc3bf66fc6d0241761a4204",
            "paperId": "61e78342047a413fdcc3bf66fc6d0241761a4204",
            "title": "Skip Gram Model Base Entropy Reduction using Vision based Page Segmentation for Web Mining",
            "abstract": "Lots of web applications, information recovery, for example, info extraction and then programmed page adjustment is able to profit by this particular structure. Extraction of web information from the full web page will be the intensive assignment to recover the substantial info because they\u2019re website programming language subordinate. This model prepared to identify both marked picture info and in addition semantic info gathered from an annotated content. Semantic learning enhances the hit rates of up to eighteen % crosswise over a good many novel marks never ever noticed through the visible design. Given a content corpus, SKIP GRAM goes for Prompting term portrayals which are good at foreseeing the environment words surrounding an objective word. The primary goal of this paper is analyzing the Vision Based Programming Language Independent Approach Trained with Skip Gram, to assess the VIPS algorithm to assess the Vision Based Page Segmentation algorithm for Web Mining and also To Implement a strong Web Data Extraction from Semi Structured Source.",
            "year": 2019,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The primary goal of this paper is analyzing the Vision Based Programming Language Independent Approach Trained with Skip Gram, to assess the VIPS algorithm to assessThe Vision Based Page Segmentation algorithm for Web Mining and also to Implement a strong Web Data Extraction from Semi Structured Source."
            },
            "score": 1
        },
        {
            "id": "29a3f2f2dff004f631c3357dc6ce6fd2c66ac477",
            "paperId": "29a3f2f2dff004f631c3357dc6ce6fd2c66ac477",
            "title": "Self-care nursing interventions: A qualitative study into electronic health records' contents.",
            "abstract": "AIMS\nThis study aims to (1) analyse all self-care-related interventions Portuguese nurses documented, (2) determine potential issues that may impair semantic interoperability and (3) propose a new set of interventions representing nursing actions regarding self-care that may integrate any HER application.\n\n\nBACKGROUND\nAs populations age and chronic diseases increase, self-care concerns rise. Individuals who seek healthcare, regardless of context, need prompt access to accurate health information. Healthcare professionals need to understand the information in all places\u00a0where care is provided, creating the need for semantic interoperability within electronic health records.\n\n\nMETHODS\nA qualitative descriptive and exploratory study was conducted in two phases: (1) a content analysis of nursing interventions e-documentation and (2) a focus group with fifteen registered nurses exploring latent criteria or insights gleaned from the findings of content analysis. The COREQ statement was used to guide research reporting.\n\n\nRESULTS\nWe extracted 1529 nursing intervention sentences from the electronic health records and created 209 intervention categories. We identified the main issues with semantic interoperability in nursing intervention identification.\n\n\nCONCLUSION\nAccording to the findings, nurses cooperate with clients, offering physical aid and encouraging them to overcome functional limitations to self-care tasks hampered by their conditions.\n\n\nIMPLICATIONS FOR NURSING POLICY AND HEALTH POLICY\nThis article provides evidence to warn policy makers against decisions to use locally customised electronic health records, as well as evidence on the importance of policy promoting the adoption of a nursing ontology for electronic health records. And, as a result, the harmonisation and effective provision of high-quality nursing care and the reduction of healthcare costs across nations.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "According to the findings, nurses cooperate with clients, offering physical aid and encouraging them to overcome functional limitations to self-care tasks hampered by their conditions, providing evidence to warn policy makers against decisions to use locally customised electronic health records, as well as evidence on the importance of policy promoting the adoption of a nursing ontology for Electronic health records."
            },
            "score": 1
        },
        {
            "id": "8281385c69bf827cfba714e6879847aeb9dc67e1",
            "paperId": "8281385c69bf827cfba714e6879847aeb9dc67e1",
            "title": "CLIP-Hand3D: Exploiting 3D Hand Pose Estimation via Context-Aware Prompting",
            "abstract": "Contrastive Language-Image Pre-training (CLIP) starts to emerge in many computer vision tasks and has achieved promising performance. However, it remains underexplored whether CLIP can be generalized to 3D hand pose estimation, as bridging text prompts with pose-aware features presents significant challenges due to the discrete nature of joint positions in 3D space. In this paper, we make one of the first attempts to propose a novel 3D hand pose estimator from monocular images, dubbed as CLIP-Hand3D, which successfully bridges the gap between text prompts and irregular detailed pose distribution. In particular, the distribution order of hand joints in various 3D space directions is derived from pose labels, forming corresponding text prompts that are subsequently encoded into text representations. Simultaneously, 21 hand joints in the 3D space are retrieved, and their spatial distribution (in x, y, and z axes) is encoded to form pose-aware features. Subsequently, we maximize semantic consistency for a pair of pose-text features following a CLIP-based contrastive learning paradigm. Furthermore, a coarse-to-fine mesh regressor is designed, which is capable of effectively querying joint-aware cues from the feature pyramid. Extensive experiments on several public hand benchmarks show that the proposed model attains a significantly faster inference speed while achieving state-of-the-art performance compared to methods utilizing the similar scale backbone. Code is available at: https://github.com/ShaoXiang23/CLIP_Hand_Demo.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "One of the first attempts to propose a novel 3D hand pose estimator from monocular images, dubbed as CLIP-Hand3D, which successfully bridges the gap between text prompts and irregular detailed pose distribution and maximize semantic consistency for a pair of pose-text features following a ClIP-based contrastive learning paradigm."
            },
            "score": 1
        },
        {
            "id": "b0ff6986f44b7dc90cb24e1f13345f81c8b4e9f7",
            "paperId": "b0ff6986f44b7dc90cb24e1f13345f81c8b4e9f7",
            "title": "NLP-EYE: Detecting Memory Corruptions via Semantic-Aware Memory Operation Function Identification",
            "abstract": "Memory corruption vulnerabilities are serious threats to software security, which is often triggered by improper use of memory operation functions. The detection of memory cor-ruptions relies on identifying memory operation functions and examining how it manipulates the memory. Distinguishing memory operation functions is challenging because they usually come in various forms in real-world software. In this paper, we propose NLP-EYE, an NLP-based memory corruption detection system. NLP-EYE is able to identify memory operation functions through a semantic-aware source code analysis automatically. It \ufb01rst creates a programming language friendly corpus in order to parse function prototypes. Based on the similarity comparison by utilizing both semantic and syntax information, NLP-EYE identi\ufb01es and labels both standard and customized memory operation functions. It uses symbolic execution at last to check whether a memory operation causes incorrect memory usage. Instead of analyzing data dependencies of the entire source code, NLP-EYE only focuses on memory operation parts. We evaluated the performance of NLP-EYE by using seven real-world libraries and programs, including Vim , Git , CPython , etc. NLP-EYE successfully identi\ufb01es 27 null pointer de-reference, two double-free and three use-after-free that are not discovered before",
            "year": 2019,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "NLP-EYE, an NLP-based memory corruption detection system that is able to identify memory operation functions through a semantic-aware source code analysis automatically and labels both standard and customized memory operation functions."
            },
            "score": 1
        }
    ],
    "novelty": "yes"
}