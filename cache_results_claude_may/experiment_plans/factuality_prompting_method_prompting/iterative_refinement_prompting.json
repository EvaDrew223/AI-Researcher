{
    "topic_description": "novel prompting methods that can improve factuality and reduce hallucination of large language models",
    "idea_name": "Iterative Refinement Prompting",
    "raw_idea": {
        "Problem": "Large language models often generate fluent but factually incorrect responses in one shot, without the ability to iteratively refine and correct their responses.",
        "Existing Methods": "Existing methods for iterative refinement mainly focus on using retrieval-augmented generation and incorporating human feedback.",
        "Motivation": "We observe that humans often generate initial drafts of responses and then iteratively refine them based on self-reflection and external feedback. We hypothesize that by explicitly modeling the iterative refinement process in the prompt, we can guide the model to generate more factually accurate and coherent responses.",
        "Proposed Method": "We propose Iterative Refinement Prompting (IRP), a novel prompting method that incorporates iterative refinement into the generation process. IRP consists of three main steps: 1) Initial Response Generation: Given an input prompt, we first ask the model to generate an initial response. 2) Refinement Prompt Generation: We then generate a set of refinement prompts based on the initial response, such as \"What are the potential factual errors in this response?\" and \"How can we make this response more coherent and logical?\" 3) Iterative Refinement: We iteratively refine the response by asking the model to revise its response based on the refinement prompts. We repeat this process for a fixed number of iterations or until the model generates a satisfactory response.",
        "Experiment Plan": "We plan to evaluate IRP on a range of tasks that require factual accuracy and coherence, such as question answering, summarization, and dialogue generation. We will compare IRP with state-of-the-art baselines, including retrieval-augmented generation and methods that incorporate human feedback. We will also conduct human evaluation to assess the factual accuracy and coherence of the generated responses, as well as the effectiveness of the iterative refinement process."
    },
    "full_experiment_plan": {
        "Title": "Iterative Refinement Prompting (IRP): Improving Factuality and Coherence of Large Language Models",
        "Problem Statement": "Large language models often generate fluent but factually incorrect responses in one shot, without the ability to iteratively refine and correct their responses. This leads to hallucination and incoherent outputs.",
        "Motivation": "Existing methods for iterative refinement mainly focus on using retrieval-augmented generation and incorporating human feedback. However, these approaches require additional resources and may not be feasible in all scenarios. We observe that humans often generate initial drafts of responses and then iteratively refine them based on self-reflection and external feedback. We hypothesize that by explicitly modeling the iterative refinement process in the prompt, we can guide the model to generate more factually accurate and coherent responses without relying on external resources.",
        "Proposed Method": "We propose Iterative Refinement Prompting (IRP), a novel prompting method that incorporates iterative refinement into the generation process. IRP consists of three main steps:\n1. Initial Response Generation: Given an input prompt, we first ask the model to generate an initial response.\n2. Refinement Prompt Generation: We then generate a set of refinement prompts based on the initial response, such as \"What are the potential factual errors in this response?\" and \"How can we make this response more coherent and logical?\"\n3. Iterative Refinement: We iteratively refine the response by asking the model to revise its response based on the refinement prompts. We repeat this process for a fixed number of iterations or until the model generates a satisfactory response.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We evaluate IRP on a range of tasks that require factual accuracy and coherence, including question answering (Natural Questions, TriviaQA), summarization (CNN/DailyMail, XSum), and dialogue generation (PersonaChat, Wizard of Wikipedia). For each dataset, we use the standard train/validation/test splits.",
            "Step 2: Construct Prompts": "For each task, we construct the following prompts:\n1. Baseline Prompt: The standard prompt for the task, e.g., \"Q: [question] A:\" for question answering.\n2. IRP Prompt: The baseline prompt followed by the iterative refinement steps. For example, for question answering:\n\"Q: [question]\nA: [initial response]\nRefinement Prompts:\n- What are the potential factual errors in this response?\n- How can we make this response more coherent and logical?\nRevised Response:\"\nWe experiment with different refinement prompts and number of iterations to find the optimal configuration for each task.",
            "Step 3: Select Models": "We evaluate IRP on state-of-the-art language models, including GPT-3.5 (text-davinci-002), GPT-4, and PaLM. We use the OpenAI API for GPT-3.5 and GPT-4, and the official implementation of PaLM.",
            "Step 4: Get Results": "For each task and model combination, we generate responses using both the baseline and IRP prompts. We evaluate the factual accuracy and coherence of the generated responses using both automatic metrics (e.g., BLEU, ROUGE, BERTScore) and human evaluation.",
            "Step 5: Analyze Results": "We compare the performance of IRP with the baseline prompts and state-of-the-art methods for each task. We also analyze the impact of different refinement prompts and number of iterations on the quality of the generated responses. Finally, we conduct error analysis to identify the strengths and weaknesses of IRP and potential areas for improvement."
        },
        "Test Case Examples": {
            "Example 1": {
                "Task": "Question Answering",
                "Dataset": "Natural Questions",
                "Test Case": "Q: What is the capital of France?",
                "Baseline Prompt": "Q: What is the capital of France?\nA:",
                "Baseline Output": "The capital of France is Paris, known for its iconic landmarks such as the Eiffel Tower and the Louvre museum. However, it is not the largest city in France, as that distinction belongs to Marseille.",
                "IRP Prompt": "Q: What is the capital of France?\nA: The capital of France is Paris, known for its iconic landmarks such as the Eiffel Tower and the Louvre museum. However, it is not the largest city in France, as that distinction belongs to Marseille.\nRefinement Prompts:\n- What are the potential factual errors in this response?\n- How can we make this response more coherent and logical?\nRevised Response:",
                "IRP Output": "Refinement Prompts:\n- The response incorrectly states that Marseille is the largest city in France. In fact, Paris is both the capital and the largest city in France.\n- The response is mostly coherent, but the second sentence about Marseille is not directly relevant to the question and may confuse the reader.\nRevised Response: The capital of France is Paris, known for its iconic landmarks such as the Eiffel Tower and the Louvre museum. Paris is also the largest city in France.",
                "Explanation": "The baseline output contains a factual error stating that Marseille is the largest city in France, which is corrected by IRP. IRP also improves the coherence of the response by removing the irrelevant information about Marseille."
            },
            "Example 2": {
                "Task": "Summarization",
                "Dataset": "CNN/DailyMail",
                "Test Case": "Article: (truncated for brevity) The U.S. is sending more than 100 additional troops to Iraq and is moving helicopters and other assets into the region in the wake of the attack on the U.S. consulate in Benghazi, Libya, that left the ambassador and three other Americans dead. The deployment, detailed by defense officials, comes as the United States is evacuating all its personnel from Benghazi to Tripoli and sending additional military security to protect them. The officials said a Marine anti-terrorism team is being sent to Libya to help secure U.S. facilities there in the aftermath of the attack. The officials spoke on condition of anonymity because they were not authorized to publicly discuss troop movements. The reinforcements are in addition to a 50-person Marine response team based in Spain that was dispatched to Tripoli on Wednesday in response to the attack. The officials said the team is expected to remain in Libya to help enhance security. The officials also said a Navy destroyer, the USS Laboon, is being moved to a position off the coast of Libya to provide additional security and is capable of responding to any mission ordered by the president. The officials said the ship, which carries Tomahawk missiles, does not have a specific mission but gives commanders flexibility to respond to any mission ordered by the president. The officials said the reinforcements are being sent as a precautionary measure and are not based on any specific intelligence indicating another attack is imminent.",
                "Baseline Prompt": "Summarize the article:",
                "Baseline Output": "The U.S. is sending additional troops and military assets to Iraq and Libya in response to the attack on the U.S. consulate in Benghazi that left four Americans dead, including the ambassador. A Marine anti-terrorism team is being deployed to Libya to secure U.S. facilities, while a Navy destroyer is being positioned off the coast to provide additional security. The reinforcements are a precautionary measure and are not based on specific intelligence indicating another imminent attack.",
                "IRP Prompt": "Summarize the article:\nThe U.S. is sending additional troops and military assets to Iraq and Libya in response to the attack on the U.S. consulate in Benghazi that left four Americans dead, including the ambassador. A Marine anti-terrorism team is being deployed to Libya to secure U.S. facilities, while a Navy destroyer is being positioned off the coast to provide additional security. The reinforcements are a precautionary measure and are not based on specific intelligence indicating another imminent attack.\nRefinement Prompts:\n- What are the potential factual errors in this summary?\n- How can we make this summary more coherent and logical?\nRevised Summary:",
                "IRP Output": "Refinement Prompts:\n- The summary incorrectly states that additional troops are being sent to Iraq. The article only mentions sending troops and assets to Libya.\n- The summary is mostly coherent and logical, but it could be improved by clarifying the timeline of events and the specific actions being taken.\nRevised Summary: In the aftermath of the attack on the U.S. consulate in Benghazi, Libya, which left the ambassador and three other Americans dead, the U.S. is taking several measures to enhance security in the region. The U.S. is evacuating all personnel from Benghazi to Tripoli and sending a Marine anti-terrorism team to help secure U.S. facilities in Libya. Additionally, a Navy destroyer, the USS Laboon, is being positioned off the coast of Libya to provide flexibility in responding to any mission ordered by the president. These reinforcements are being sent as a precautionary measure and are not based on specific intelligence indicating another imminent attack.",
                "Explanation": "The baseline summary contains a factual error stating that troops are being sent to Iraq, which is corrected by IRP. IRP also improves the coherence of the summary by clarifying the timeline of events and the specific actions being taken."
            }
        },
        "Fallback Plan": "If the proposed IRP method does not significantly improve the factual accuracy and coherence of the generated responses compared to the baselines, we can consider the following alternative approaches:\n1. Analyze the refinement prompts to understand if they are effective in guiding the model to identify and correct errors. We can experiment with different types of refinement prompts and see if they lead to better results.\n2. Investigate the impact of the number of refinement iterations on the quality of the generated responses. We can try increasing or decreasing the number of iterations to find the optimal balance between accuracy and efficiency.\n3. Explore alternative methods for incorporating iterative refinement into the generation process, such as using retrieval-augmented generation or reinforcement learning to guide the refinement process.\n4. Conduct a detailed error analysis to identify the types of errors that IRP fails to correct and the reasons behind these failures. This can help us design more targeted refinement prompts or develop new methods to address these specific challenges.\nIf none of these alternative approaches lead to significant improvements, we can still gain valuable insights into the limitations of iterative refinement for improving factuality and coherence in large language models. We can present our findings as an analysis paper, highlighting the challenges and opportunities for future research in this area."
    },
    "novelty_queries": [
        "KeywordQuery(\"iterative refinement prompting language models\")",
        "KeywordQuery(\"factual coherence language models\")",
        "KeywordQuery(\"self-reflection feedback language models\")",
        "KeywordQuery(\"Iterative Refinement Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "3aaf6a2cbad5850ad81ab5c163599cb3d523436f",
            "paperId": "3aaf6a2cbad5850ad81ab5c163599cb3d523436f",
            "title": "Self-Refine: Iterative Refinement with Self-Feedback",
            "abstract": "Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.",
            "year": 2023,
            "citationCount": 505,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Self-Refine is introduced, an approach for improving initial outputs from LLMs through iterative feedback and refinement that demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using this simple, standalone approach."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is improving the factuality and coherence of large language models, and the proposed approach is Iterative Refinement Prompting (IRP), which incorporates iterative refinement into the generation process.\n\nThe research problem in the paper is also improving the outputs from large language models, and the proposed approach is Self-Refine, which uses the same language model to provide feedback and iteratively refine its own outputs.\n\nBoth the proposal and the paper focus on improving the outputs of large language models through iterative refinement, without requiring additional training data or reinforcement learning.\n\nYes",
            "novelty_judgment": "yes"
        },
        {
            "id": "696bc5ba0d023822bbee6b878a71ea2e4a4b0e5a",
            "paperId": "696bc5ba0d023822bbee6b878a71ea2e4a4b0e5a",
            "title": "N-Critics: Self-Refinement of Large Language Models with Ensemble of Critics",
            "abstract": "We propose a self-correction mechanism for Large Language Models (LLMs) to mitigate issues such as toxicity and fact hallucination. This method involves refining model outputs through an ensemble of critics and the model's own feedback. Drawing inspiration from human behavior, we explore whether LLMs can emulate the self-correction process observed in humans who often engage in self-reflection and seek input from others to refine their understanding of complex topics. Our approach is model-agnostic and can be applied across various domains to enhance trustworthiness by addressing fairness, bias, and robustness concerns. We consistently observe performance improvements in LLMs for reducing toxicity and correcting factual errors.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a self-correction mechanism for Large Language Models (LLMs) to mitigate issues such as toxicity and fact hallucination and consistently observe performance improvements in LLMs for reducing toxicity and correcting factual errors."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is improving the factuality and coherence of large language model outputs through iterative refinement prompting. The approach involves generating an initial response, creating refinement prompts to identify potential errors and improvements, and iteratively revising the response based on the model's own feedback.\n\nThe research problem in the paper is mitigating issues such as toxicity and fact hallucination in large language models. The approach involves refining model outputs through an ensemble of critics and the model's own feedback.\n\nWhile both the proposal and the paper aim to improve the quality of large language model outputs, the specific problems they address (factuality/coherence vs. toxicity/fact hallucination) and the approaches they propose (iterative refinement prompting vs. ensemble of critics) are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "a173b47ee90fb18948d70126f4d40b5561b7d932",
            "paperId": "a173b47ee90fb18948d70126f4d40b5561b7d932",
            "title": "SELF: Self-Evolution with Language Feedback",
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable versatility across various domains. To further advance LLMs, we propose 'SELF' (Self-Evolution with Language Feedback), a novel approach that enables LLMs to self-improve through self-reflection, akin to human learning processes. SELF initiates with a meta-skill learning process that equips the LLMs with capabilities for self-feedback and self-refinement. Subsequently, the model undergoes an iterative process of self-evolution. In each iteration, it utilizes an unlabeled dataset of instructions to generate initial responses. These responses are enhanced through self-feedback and self-refinement. The model is then fine-tuned using this enhanced data. The model undergoes progressive improvement through this iterative self-evolution process. Moreover, the SELF framework enables the model to apply self-refinement during inference, which further improves response quality. Our experiments in mathematics and general tasks demonstrate that SELF can enhance the capabilities of LLMs without human intervention. The SELF framework indicates a promising direction for the autonomous evolution of LLMs, transitioning them from passive information receivers to active participants in their development.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The proposed 'SELF' (Self-Evolution with Language Feedback), a novel approach that enables LLMs to self-improve through self-reflection, akin to human learning processes, indicates a promising direction for the autonomous evolution of LLMs."
            },
            "score": 8,
            "novelty_score": "The research problem in the project proposal is improving the factuality and coherence of large language models through iterative refinement prompting. The proposed approach is to guide the model to generate an initial response, then iteratively refine it based on self-reflection prompts.\n\nThe research problem in the paper is enabling large language models to self-improve through self-reflection and self-evolution. The proposed approach is to equip the model with self-feedback and self-refinement capabilities, then iteratively generate responses, enhance them through self-feedback and self-refinement, and fine-tune the model using the enhanced data.\n\nWhile both the project proposal and the paper aim to improve the performance of large language models, their specific research problems and approaches differ. The project proposal focuses on factuality and coherence, while the paper focuses on general self-improvement. The project proposal uses iterative refinement prompting, while the paper uses self-evolution with language feedback.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "44d74b0d77b4056ddd4c6611a76711c8bab2e0a7",
            "paperId": "44d74b0d77b4056ddd4c6611a76711c8bab2e0a7",
            "title": "Dehallucinating Large Language Models Using Formal Methods Guided Iterative Prompting",
            "abstract": "Large language models (LLMs) such as ChatGPT have been trained to generate human-like responses to natural language prompts. LLMs use a vast corpus of text data for training, and can generate coherent and contextually relevant responses to a wide range of questions and statements. Despite this remarkable progress, LLMs are prone to hallucinations making their application to safety-critical applications such as autonomous systems difficult. The hallucinations in LLMs refer to instances where the model generates responses that are not factually accurate or contextually appropriate. These hallucinations can occur due to a variety of factors, such as the model\u2019s lack of real-world knowledge, the influence of biased or inaccurate training data, or the model\u2019s tendency to generate responses based on statistical patterns rather than a true understanding of the input. While these hallucinations are a nuisance in tasks such as text summarization and question-answering, they can be catastrophic when LLMs are used in autonomy-relevant applications such as planning. In this paper, we focus on the application of LLMs in autonomous systems and sketch a novel self-monitoring and iterative prompting architecture that uses formal methods to detect these errors in the LLM response automatically. We exploit the dialog capability of LLMs to iteratively steer them to responses that are consistent with our correctness specification. We report preliminary experiments that show the promise of the proposed approach on tasks such as automated planning.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper sketches a novel self-monitoring and iterative prompting architecture that uses formal methods to detect errors in the LLM response automatically and exploits the dialog capability of LLMs to iteratively steer them to responses that are consistent with the correctness specification."
            },
            "score": 7,
            "novelty_score": "The research problem in the project proposal is improving the factuality and coherence of large language models, while the approach is to use iterative refinement prompting to guide the model to generate more accurate and coherent responses.\n\nThe research problem in the paper is reducing hallucinations in large language models when applied to autonomous systems, and the approach is to use formal methods to detect errors in the model's response and iteratively steer the model to generate responses consistent with a correctness specification.\n\nAlthough both works aim to improve the factuality of large language models, the project proposal focuses on general factuality and coherence, while the paper specifically targets hallucinations in the context of autonomous systems. Additionally, the project proposal uses iterative refinement prompting, while the paper uses formal methods to guide the iterative prompting process.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "2aba5bba16dac5cd62683bab9de5d6faaaed0de1",
            "paperId": "2aba5bba16dac5cd62683bab9de5d6faaaed0de1",
            "title": "Shepherd Pre-trained Language Models to Develop a Train of Thought: An Iterative Prompting Approach",
            "abstract": "While Pre-trained Language Models (PLMs) 001 internalize a great amount of world knowledge, 002 they have been shown incapable of recalling 003 these knowledge to solve tasks requiring com-004 plex & multi-step inference procedures. Simi-005 lar to how humans develop a \u201ctrain of thought\u201d 006 for these tasks, how can we equip PLMs with 007 such abilities? In this work, we explore an iter-008 ative prompting framework, a new prompting 009 paradigm which progressively elicits relevant 010 knowledge from PLMs for multi-step inference 011 tasks. We identify key limitations of existing 012 prompting methods, namely they are either re-013 stricted to queries with a single identifiable re-014 lation/predicate, or being agnostic to input con-015 texts, which makes it difficult to capture vari-016 abilities across different inference steps. We 017 propose an iterative context-aware prompter, 018 which addresses these limitations by learning 019 to dynamically synthesize prompts conditioned 020 on the current step\u2019s contexts. Experiments on 021 three datasets involving multi-step inference 022 show the effectiveness of the iterative scheme 023 and the context-aware prompter design. 1 024",
            "year": 2022,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores an iter-008 ative prompting framework, a new prompting 009 paradigm which progressively elicits relevant 010 knowledge from PLMs for multi-step inference 011 tasks, and proposes an iterative context-aware prompter design which addresses key limitations of existing prompting methods."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the factuality and coherence of large language model outputs through iterative refinement prompting. The approach is to generate an initial response, then iteratively refine it by generating refinement prompts and asking the model to revise its response.\n\nThe research problem in the paper is enabling pre-trained language models to perform complex multi-step inference tasks. The approach is an iterative prompting framework that progressively elicits relevant knowledge from the model, using a context-aware prompter that dynamically synthesizes prompts conditioned on the current step's context.\n\nWhile both works involve iterative prompting, the research problems and approaches are different. The proposal focuses on improving factuality and coherence, while the paper aims to enable multi-step inference. The proposal's approach is based on generating refinement prompts, while the paper's approach is based on a context-aware prompter.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "9a9b1e2968302eb882870537d4af6e2c722dfd1a",
            "paperId": "9a9b1e2968302eb882870537d4af6e2c722dfd1a",
            "title": "Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement",
            "abstract": "To enhance the multi-step reasoning capabilities of large language models, researchers have extensively explored prompting methods, notably the Chain-of-Thought (CoT) method which explicitly elicits human-like rationales. However, they have inadvertently overlooked the potential of enhancing model reasoning performance by formulating higher-quality problems. In this work, we start from the problem side and propose Self-Polish (SP), a novel method that facilitates the model's reasoning by guiding it to progressively refine the given problems to be more comprehensible and solvable. We also explore several automatic prompting varients and propose the Self-Polish prompt bank for the community. SP is orthogonal to all other prompting methods of answer/reasoning side like CoT, allowing for seamless integration with state-of-the-art techniques for further improvement. Thorough experiments show that the proposed method attains notable and consistent effectiveness on five reasoning benchmarks across different models. Furthermore, our method also showcases impressive performance on robustness evaluation. Codes and prompts are available at https://github.com/WooooDyy/Self-Polish.",
            "year": 2023,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Self-Polish (SP) is proposed, a novel method that facilitates the model's reasoning by guiding it to progressively refine the given problems to be more comprehensible and solvable and seamless integration with state-of-the-art techniques for further improvement."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the factuality and coherence of large language model outputs through iterative refinement prompting. The approach is to guide the model to iteratively refine its own responses based on self-reflection prompts.\n\nThe research problem in the paper is enhancing the multi-step reasoning capabilities of large language models by formulating higher-quality problems. The approach is to guide the model to progressively refine the given problems to be more comprehensible and solvable.\n\nWhile both works aim to improve the performance of large language models, the proposal focuses on factuality and coherence of the model outputs, while the paper focuses on multi-step reasoning capabilities. The proposal uses iterative refinement on the model's responses, while the paper refines the input problems given to the model.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "cd2e04598909158494e556823d9de8baa692cee2",
            "paperId": "cd2e04598909158494e556823d9de8baa692cee2",
            "title": "Towards Mitigating Hallucination in Large Language Models via Self-Reflection",
            "abstract": "Large language models (LLMs) have shown promise for generative and knowledge-intensive tasks including question-answering (QA) tasks. However, the practical deployment still faces challenges, notably the issue of\"hallucination\", where models generate plausible-sounding but unfaithful or nonsensical information. This issue becomes particularly critical in the medical domain due to the uncommon professional concepts and potential social risks involved. This paper analyses the phenomenon of hallucination in medical generative QA systems using widely adopted LLMs and datasets. Our investigation centers on the identification and comprehension of common problematic answers, with a specific emphasis on hallucination. To tackle this challenge, we present an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation. Through this feedback process, our approach steadily enhances the factuality, consistency, and entailment of the generated answers. Consequently, we harness the interactivity and multitasking ability of LLMs and produce progressively more precise and accurate answers. Experimental results on both automatic and human evaluation demonstrate the superiority of our approach in hallucination reduction compared to baselines.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper analyses the phenomenon of hallucination in medical generative QA systems using widely adopted LLMs and datasets and presents an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation that steadily enhances the factuality, consistency, and entailment of the generated answers."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the factuality and coherence of large language model outputs, while the approach is iterative refinement prompting. The research problem in the paper is mitigating hallucination in large language models for medical question answering, and the approach is interactive self-reflection.\n\nAlthough both works aim to improve the factuality of large language model outputs, the proposal focuses on general language generation tasks while the paper specifically targets medical question answering. Moreover, the proposed approaches are different: iterative refinement prompting vs. interactive self-reflection.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "ab15a0fc78532b2cae1007e5315f838ae23d0a4a",
            "paperId": "ab15a0fc78532b2cae1007e5315f838ae23d0a4a",
            "title": "Mirror: A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning",
            "abstract": "While Large language models (LLMs) have the capability to iteratively reflect on their own outputs, recent studies have observed their struggles with knowledge-rich problems without access to external resources. In addition to the inefficiency of LLMs in self-assessment, we also observe that LLMs struggle to revisit their predictions despite receiving explicit negative feedback. Therefore, We propose Mirror, a Multiple-perspective self-reflection method for knowledge-rich reasoning, to avoid getting stuck at a particular reflection iteration. Mirror enables LLMs to reflect from multiple-perspective clues, achieved through a heuristic interaction between a Navigator and a Reasoner. It guides agents toward diverse yet plausibly reliable reasoning trajectory without access to ground truth by encouraging (1) diversity of directions generated by Navigator and (2) agreement among strategically induced perturbations in responses generated by the Reasoner. The experiments on five reasoning datasets demonstrate that Mirror's superiority over several contemporary self-reflection approaches. Additionally, the ablation study studies clearly indicate that our strategies alleviate the aforementioned challenges.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Mirror, a Multiple-perspective self-reflection method for knowledge-rich reasoning, is proposed to avoid getting stuck at a particular reflection iteration, to avoid getting stuck at a particular reflection iteration."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the factual accuracy and coherence of large language models through iterative refinement prompting. The approach is to guide the model to generate an initial response, then iteratively refine it based on self-generated refinement prompts.\n\nThe research problem in the paper is improving the performance of large language models on knowledge-rich reasoning tasks through a multiple-perspective self-reflection method. The approach is to use a Navigator-Reasoner interaction to generate diverse reflection directions and encourage agreement among strategically induced perturbations in responses.\n\nWhile both the proposal and the paper aim to improve the performance of large language models through iterative refinement, the specific research problems and approaches are different. The proposal focuses on factual accuracy and coherence, while the paper focuses on knowledge-rich reasoning. The proposal uses self-generated refinement prompts, while the paper uses a Navigator-Reasoner interaction with multiple-perspective clues.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "58a282c89864f35bff1741f5ab439222da6bb3ec",
            "paperId": "58a282c89864f35bff1741f5ab439222da6bb3ec",
            "title": "MLLM-DataEngine: An Iterative Refinement Approach for MLLM",
            "abstract": "Despite the great advance of Multimodal Large Language Models (MLLMs) in both instruction dataset building and benchmarking, the independence of training and evaluation makes current MLLMs hard to further improve their capability under the guidance of evaluation results with a relatively low human cost. In this paper, we propose MLLM-DataEngine, a novel closed-loop system that bridges data generation, model training, and evaluation. Within each loop iteration, the MLLM-DataEngine first analyze the weakness of the model based on the evaluation results, then generate a proper incremental dataset for the next training iteration and enhance the model capability iteratively. Compared with previous data collection methods which are separate from the benchmarking, the data generated by MLLM-DataEngine shows better targeting, quality, and correctness. For targeting, we propose an Adaptive Bad-case Sampling module, which adjusts the ratio of different types of data within each incremental dataset based on the benchmarking results. For quality, we resort to GPT-4 to generate high-quality data with each given data type. For correctness, prompt design is critical for the data generation results. Rather than previous hand-crafted prompt, we propose an Interactive Prompt Optimization strategy, which optimizes the prompt with the multi-round interaction between human and GPT, and improve the correctness of generated data greatly. Through extensive experiments, we find our MLLM-DataEngine could boost the MLLM capability in a targeted and automatic manner, with only a few human participation. We hope it could be a general solution for the following MLLMs building. The MLLM-DataEngine has been open-sourced and is now available at https://github.com/opendatalab/MLLM-DataEngine.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes MLLM-DataEngine, a novel closed-loop system that bridges data generation, model training, and evaluation, and proposes an Interactive Prompt Optimization strategy, which optimizes the prompt with the multi-round interaction between human and GPT, and improves the correctness of generated data greatly."
            },
            "score": 7,
            "novelty_score": "The research problem in the project proposal is improving the factuality and coherence of large language models through iterative refinement prompting. The approach is to generate an initial response, then iteratively refine it based on generated refinement prompts.\n\nThe research problem in the paper is improving the capability of multimodal large language models (MLLMs) through an iterative refinement approach guided by evaluation results. The approach is to analyze the model's weaknesses based on evaluation results, generate targeted incremental datasets, and iteratively train the model.\n\nWhile both the project proposal and the paper focus on iterative refinement, the project proposal targets factuality and coherence in text-only language models, while the paper focuses on improving the overall capability of multimodal language models. The methods also differ, with the project proposal using prompting and the paper using targeted data generation.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "0885471c0215b3c0d31c82518066913f7f738128",
            "paperId": "0885471c0215b3c0d31c82518066913f7f738128",
            "title": "Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement",
            "abstract": "The ability to derive underlying principles from a handful of observations and then generalize to novel situations -- known as inductive reasoning -- is central to human intelligence. Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks. In this work, we conduct a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting. Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules. By examining the intermediate rules, we observe that LMs are phenomenal hypothesis proposers (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the proposed set of rules, this hybrid approach achieves strong results across inductive reasoning benchmarks that require inducing causal relations, language-like instructions, and symbolic concepts. However, they also behave as puzzling inductive reasoners, showing notable performance gaps between rule induction (i.e., identifying plausible rules) and rule application (i.e., applying proposed rules to instances), suggesting that LMs are proposing hypotheses without being able to actually apply the rules. Through empirical and human analyses, we further reveal several discrepancies between the inductive reasoning processes of LMs and humans, shedding light on both the potentials and limitations of using LMs in inductive reasoning tasks.",
            "year": 2023,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work conducts a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the factuality and coherence of large language model outputs through iterative refinement prompting. The approach is to guide the model to generate an initial response, then iteratively refine it based on self-generated refinement prompts.\n\nThe research problem in the paper is studying the inductive reasoning capabilities of language models through iterative hypothesis refinement. The approach is to employ a three-step process of proposing, selecting, and refining hypotheses in the form of textual rules.\n\nWhile both works involve iterative refinement, the proposal focuses on improving factuality and coherence in general language model outputs, while the paper specifically studies inductive reasoning capabilities. The methods also differ, with the proposal using self-generated refinement prompts and the paper using a propose-select-refine process for hypotheses.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "f53a4f34757d1f237446b4d887d5323f2a17ed02",
            "paperId": "f53a4f34757d1f237446b4d887d5323f2a17ed02",
            "title": "PREFER: Prompt Ensemble Learning via Feedback-Reflect-Refine",
            "abstract": "As an effective tool for eliciting the power of Large Language Models (LLMs), prompting has recently demonstrated unprecedented abilities across a variety of complex tasks. To further improve the performance, prompt ensemble has attracted substantial interest for tackling the hallucination and instability of LLMs. However, existing methods usually adopt a two-stage paradigm, which requires a pre-prepared set of prompts with substantial manual effort, and is unable to perform directed optimization for different weak learners. In this paper, we propose a simple, universal, and automatic method named PREFER (Prompt Ensemble learning via Feedback-Reflect-Refine) to address the stated limitations. Specifically, given the fact that weak learners are supposed to focus on hard examples during boosting, PREFER builds a feedback mechanism for reflecting on the inadequacies of existing weak learners. Based on this, the LLM is required to automatically synthesize new prompts for iterative refinement. Moreover, to enhance stability of the prompt effect evaluation, we propose a novel prompt bagging method involving forward and backward thinking, which is superior to majority voting and is beneficial for both feedback and weight calculation in boosting. Extensive experiments demonstrate that our PREFER achieves state-of-the-art performance in multiple types of tasks by a significant margin. We have made our code publicly available.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a simple, universal, and automatic method named PREFER (Prompt Ensemble learning via Feedback-Reflect-Refine) to address the stated limitations of prompt ensemble and builds a feedback mechanism for reflecting on the inadequacies of existing weak learners."
            },
            "score": 6
        },
        {
            "id": "ac37accd7aedf1c25c3d54c7982579b297b3ff2b",
            "paperId": "ac37accd7aedf1c25c3d54c7982579b297b3ff2b",
            "title": "Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models",
            "abstract": "Large language models (LLMs) can achieve highly effective performance on various reasoning tasks by incorporating step-by-step chain-of-thought (CoT) prompting as demonstrations. However, the reasoning chains of demonstrations generated by LLMs are prone to errors, which can subsequently lead to incorrect reasoning during inference. Furthermore, inappropriate exemplars (overly simplistic or complex), can affect overall performance among varying levels of difficulty. We introduce Iter-CoT (Iterative bootstrapping in Chain-of-Thoughts Prompting), an iterative bootstrapping approach for selecting exemplars and generating reasoning chains. By utilizing iterative bootstrapping, our approach enables LLMs to autonomously rectify errors, resulting in more precise and comprehensive reasoning chains. Simultaneously, our approach selects challenging yet answerable questions accompanied by reasoning chains as exemplars with a moderate level of difficulty, which enhances the LLMs' generalizability across varying levels of difficulty. Experimental results indicate that Iter-CoT exhibits superiority, achieving competitive performance across three distinct reasoning tasks on ten datasets.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "By utilizing iterative bootstrapping, this approach enables LLMs to autonomously rectify errors, resulting in more precise and comprehensive reasoning chains, which enhances the LLMs' generalizability across varying levels of difficulty."
            },
            "score": 6
        },
        {
            "id": "e69b41e53ce897a576ab80a6e9d59a253cbd6c62",
            "paperId": "e69b41e53ce897a576ab80a6e9d59a253cbd6c62",
            "title": "Iterative Prompt Refinement for Radiation Oncology Symptom Extraction Using Teacher-Student Large Language Models",
            "abstract": "This study introduces a novel teacher-student architecture utilizing Large Language Models (LLMs) to improve prostate cancer radiotherapy symptom extraction from clinical notes. Mixtral, the student model, initially extracts symptoms, followed by GPT-4, the teacher model, which refines prompts based on Mixtral's performance. This iterative process involved 294 single symptom clinical notes across 12 symptoms, with up to 16 rounds of refinement per epoch. Results showed significant improvements in extracting symptoms from both single and multi-symptom notes. For 59 single symptom notes, accuracy increased from 0.51 to 0.71, precision from 0.52 to 0.82, recall from 0.52 to 0.72, and F1 score from 0.49 to 0.73. In 375 multi-symptom notes, accuracy rose from 0.24 to 0.43, precision from 0.6 to 0.76, recall from 0.24 to 0.43, and F1 score from 0.20 to 0.44. These results demonstrate the effectiveness of advanced prompt engineering in LLMs for radiation oncology use.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel teacher-student architecture utilizing Large Language Models (LLMs) to improve prostate cancer radiotherapy symptom extraction from clinical notes is introduced, demonstrating the effectiveness of advanced prompt engineering in LLMs for radiation oncology use."
            },
            "score": 6
        },
        {
            "id": "7ed0faa6720cd176d57badbc0455af31a03f080c",
            "paperId": "7ed0faa6720cd176d57badbc0455af31a03f080c",
            "title": "Towards Expert-Level Medical Question Answering with Large Language Models",
            "abstract": "Recent artificial intelligence (AI) systems have reached milestones in\"grand challenges\"ranging from Go to protein-folding. The capability to retrieve medical knowledge, reason over it, and answer medical questions comparably to physicians has long been viewed as one such grand challenge. Large language models (LLMs) have catalyzed significant progress in medical question answering; Med-PaLM was the first model to exceed a\"passing\"score in US Medical Licensing Examination (USMLE) style questions with a score of 67.2% on the MedQA dataset. However, this and other prior work suggested significant room for improvement, especially when models' answers were compared to clinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by leveraging a combination of base LLM improvements (PaLM 2), medical domain finetuning, and prompting strategies including a novel ensemble refinement approach. Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 19% and setting a new state-of-the-art. We also observed performance approaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU clinical topics datasets. We performed detailed human evaluations on long-form questions along multiple axes relevant to clinical applications. In pairwise comparative ranking of 1066 consumer medical questions, physicians preferred Med-PaLM 2 answers to those produced by physicians on eight of nine axes pertaining to clinical utility (p<0.001). We also observed significant improvements compared to Med-PaLM on every evaluation axis (p<0.001) on newly introduced datasets of 240 long-form\"adversarial\"questions to probe LLM limitations. While further studies are necessary to validate the efficacy of these models in real-world settings, these results highlight rapid progress towards physician-level performance in medical question answering.",
            "year": 2023,
            "citationCount": 233,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Results highlight rapid progress towards physician-level performance in medical question answering by leveraging a combination of base LLM improvements (PaLM 2), medical domain finetuning, and prompting strategies including a novel ensemble refinement approach."
            },
            "score": 6
        },
        {
            "id": "d2d16333a4b0dc7e3463b280b9945e5ee6c53396",
            "paperId": "d2d16333a4b0dc7e3463b280b9945e5ee6c53396",
            "title": "TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models",
            "abstract": "Factual consistency evaluation is often conducted using Natural Language Inference (NLI) models, yet these models exhibit limited success in evaluating summaries. Previous work improved such models with synthetic training data. However, the data is typically based on perturbed human-written summaries, which often differ in their characteristics from real model-generated summaries and have limited coverage of possible factual errors. Alternatively, large language models (LLMs) have recently shown promising results in directly evaluating generative tasks, but are too computationally expensive for practical use. Motivated by these limitations, we introduce TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM. Unlike prior work, TrueTeacher does not rely on human-written summaries, and is multilingual by nature. Experiments on the TRUE benchmark show that a student model trained using our data, substantially outperforms both the state-of-the-art model with similar capacity, and the LLM teacher. In a systematic study, we compare TrueTeacher to existing synthetic data generation methods and demonstrate its superiority and robustness to domain-shift. We also show that our method generalizes to multilingual scenarios. Lastly, we release our large scale synthetic dataset (1.4M examples), generated using TrueTeacher, and a checkpoint trained on this data.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM, which does not rely on human-written summaries, and is multilingual by nature."
            },
            "score": 6
        },
        {
            "id": "385c74957858e7d6856d48e72b5a902b4c1aa28c",
            "paperId": "385c74957858e7d6856d48e72b5a902b4c1aa28c",
            "title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
            "abstract": "Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of\"tit for tat\"and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counter-intuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of\"tit for tat\"state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents. Codes: https://github.com/Skytliang/Multi-Agents-Debate",
            "year": 2023,
            "citationCount": 125,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Multi-Agent Debate (MAD) framework is proposed, in which multiple agents express their arguments in the state of\"tit for tat\"and a judge manages the debate process to obtain a final solution."
            },
            "score": 6
        },
        {
            "id": "1ca3b6ff250b4f73486a89f6954edcc4ae21834e",
            "paperId": "1ca3b6ff250b4f73486a89f6954edcc4ae21834e",
            "title": "When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models",
            "abstract": "Recent studies suggest that self-reflective prompting can significantly enhance the reasoning capabilities of Large Language Models (LLMs). However, the use of external feedback as a stop criterion raises doubts about the true extent of LLMs' ability to emulate human-like self-reflection. In this paper, we set out to clarify these capabilities under a more stringent evaluation setting in which we disallow any kind of external feedback. Our findings under this setting show a split: while self-reflection enhances performance in TruthfulQA, it adversely affects results in HotpotQA. We conduct follow-up analyses to clarify the contributing factors in these patterns, and find that the influence of self-reflection is impacted both by reliability of accuracy in models' initial responses, and by overall question difficulty: specifically, self-reflection shows the most benefit when models are less likely to be correct initially, and when overall question difficulty is higher. We also find that self-reflection reduces tendency toward majority voting. Based on our findings, we propose guidelines for decisions on when to implement self-reflection. We release the codebase for reproducing our experiments at https://github.com/yanhong-lbh/LLM-SelfReflection-Eval.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The influence of self-reflection is impacted both by reliability of accuracy in models' initial responses, and by overall question difficulty: specifically, self-reflection shows the most benefit when models are less likely to be correct initially, and when overall question difficulty is higher."
            },
            "score": 6
        },
        {
            "id": "4a94be767309f9d461f4b90943002c5c54d9d867",
            "paperId": "4a94be767309f9d461f4b90943002c5c54d9d867",
            "title": "MATEval: A Multi-Agent Discussion Framework for Advancing Open-Ended Text Evaluation",
            "abstract": "Recent advancements in generative Large Language Models(LLMs) have been remarkable, however, the quality of the text generated by these models often reveals persistent issues. Evaluating the quality of text generated by these models, especially in open-ended text, has consistently presented a significant challenge. Addressing this, recent work has explored the possibility of using LLMs as evaluators. While using a single LLM as an evaluation agent shows potential, it is filled with significant uncertainty and instability. To address these issues, we propose the MATEval: A\"Multi-Agent Text Evaluation framework\"where all agents are played by LLMs like GPT-4. The MATEval framework emulates human collaborative discussion methods, integrating multiple agents' interactions to evaluate open-ended text. Our framework incorporates self-reflection and Chain-of-Thought (CoT) strategies, along with feedback mechanisms, enhancing the depth and breadth of the evaluation process and guiding discussions towards consensus, while the framework generates comprehensive evaluation reports, including error localization, error types and scoring. Experimental results show that our framework outperforms existing open-ended text evaluation methods and achieves the highest correlation with human evaluation, which confirms the effectiveness and advancement of our framework in addressing the uncertainties and instabilities in evaluating LLMs-generated text. Furthermore, our framework significantly improves the efficiency of text evaluation and model iteration in industrial scenarios.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results show that the MATEval framework outperforms existing open-ended text evaluation methods and achieves the highest correlation with human evaluation, which confirms the effectiveness and advancement of the framework in addressing the uncertainties and instabilities in evaluating LLMs-generated text."
            },
            "score": 6
        },
        {
            "id": "9e3c493fb09dcd61bb05e8c5659f23327b7b6340",
            "paperId": "9e3c493fb09dcd61bb05e8c5659f23327b7b6340",
            "title": "Teaching Large Language Models to Self-Debug",
            "abstract": "Large language models (LLMs) have achieved impressive performance on code generation. However, for complex programming tasks, generating the correct solution in one go becomes challenging, thus some prior works have designed program repair approaches to improve code generation performance. In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. In particular, we demonstrate that Self-Debugging can teach the large language model to perform rubber duck debugging; i.e., without any human feedback on the code correctness or error messages, the model is able to identify its mistakes by investigating the execution results and explaining the generated code in natural language. Self-Debugging achieves the state-of-the-art performance on several code generation benchmarks, including the Spider dataset for text-to-SQL generation, TransCoder for C++-to-Python translation, and MBPP for text-to-Python generation. On the Spider benchmark where there are no unit tests to verify the correctness of predictions, Self-Debugging with code explanation consistently improves the baseline by 2-3%, and improves the prediction accuracy on problems of the hardest level by 9%. On TransCoder and MBPP where unit tests are available, Self-Debugging improves the baseline accuracy by up to 12%. Meanwhile, by leveraging feedback messages and reusing failed predictions, Self-Debugging notably improves sample efficiency, and can match or outperform baseline models that generate more than 10x candidate programs.",
            "year": 2023,
            "citationCount": 258,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Self-Debugging is proposed, which teaches a large language model to debug its predicted program via few-shot demonstrations, and can match or outperform baseline models that generate more than 10x candidate programs."
            },
            "score": 6
        },
        {
            "id": "bcdaf6c98ddbd6809cf6241aa77200d7394db163",
            "paperId": "bcdaf6c98ddbd6809cf6241aa77200d7394db163",
            "title": "CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing",
            "abstract": "Recent developments in large language models (LLMs) have been impressive. However, these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content. Unlike these models, humans typically utilize external tools to cross-check and refine their initial content, like using a search engine for fact-checking, or a code interpreter for debugging. Inspired by this observation, we introduce a framework called CRITIC that allows LLMs, which are essentially\"black boxes\"to validate and progressively amend their own outputs in a manner similar to human interaction with tools. More specifically, starting with an initial output, CRITIC interacts with appropriate tools to evaluate certain aspects of the text, and then revises the output based on the feedback obtained during this validation process. Comprehensive evaluations involving free-form question answering, mathematical program synthesis, and toxicity reduction demonstrate that CRITIC consistently enhances the performance of LLMs. Meanwhile, our research highlights the crucial importance of external feedback in promoting the ongoing self-improvement of LLMs.",
            "year": 2023,
            "citationCount": 120,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A framework called CRITIC is introduced that allows LLMs, which are essentially\"black boxes\" to validate and progressively amend their own outputs in a manner similar to human interaction with tools."
            },
            "score": 6
        },
        {
            "id": "bb2ef694e8b5a99e1f7ceb014968b4d1dc2e122a",
            "paperId": "bb2ef694e8b5a99e1f7ceb014968b4d1dc2e122a",
            "title": "EIPE-text: Evaluation-Guided Iterative Plan Extraction for Long-Form Narrative Text Generation",
            "abstract": "Plan-and-Write is a common hierarchical approach in long-form narrative text generation, which first creates a plan to guide the narrative writing. Following this approach, several studies rely on simply prompting large language models for planning, which often yields suboptimal results. In this paper, we propose a new framework called Evaluation-guided Iterative Plan Extraction for long-form narrative text generation (EIPE-text), which extracts plans from the corpus of narratives and utilizes the extracted plans to construct a better planner. EIPE-text has three stages: plan extraction, learning, and inference. In the plan extraction stage, it iteratively extracts and improves plans from the narrative corpus and constructs a plan corpus. We propose a question answer (QA) based evaluation mechanism to automatically evaluate the plans and generate detailed plan refinement instructions to guide the iterative improvement. In the learning stage, we build a better planner by fine-tuning with the plan corpus or in-context learning with examples in the plan corpus. Finally, we leverage a hierarchical approach to generate long-form narratives. We evaluate the effectiveness of EIPE-text in the domains of novels and storytelling. Both GPT-4-based evaluations and human evaluations demonstrate that our method can generate more coherent and relevant long-form narratives. Our code will be released in the future.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new framework called Evaluation-guided Iterative Plan Extraction for long-form narrative text generation (EIPE-text), which extracts plans from the corpus of narratives and utilizes the extracted plans to construct a better planner."
            },
            "score": 5
        },
        {
            "id": "4637f79ddfaf923ce569996ffa5b6cda1996faa1",
            "paperId": "4637f79ddfaf923ce569996ffa5b6cda1996faa1",
            "title": "Jailbreaking Black Box Large Language Models in Twenty Queries",
            "abstract": "There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR -- which is inspired by social engineering attacks -- uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and PaLM-2.",
            "year": 2023,
            "citationCount": 119,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "PAIR is an algorithm that generates semantic jailbreaks with only black-box access to an LLM with competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and PaLM."
            },
            "score": 5
        },
        {
            "id": "48a91be5a11140010fe49a912ca834dc3feb50b2",
            "paperId": "48a91be5a11140010fe49a912ca834dc3feb50b2",
            "title": "On Early Detection of Hallucinations in Factual Question Answering",
            "abstract": "While large language models (LLMs) have taken great strides towards helping humans with a plethora of tasks like search and summarization, hallucinations remain a major impediment towards gaining user trust. The fluency and coherence of model generations even when hallucinating makes it difficult to detect whether or not a model is hallucinating. In this work, we explore if the artifacts associated with the model generations can provide hints that the generation will contain hallucinations. Specifically, we probe LLMs at 1) the inputs via Integrated Gradients based token attribution, 2) the outputs via the Softmax probabilities, and 3) the internal state via self-attention and fully-connected layer activations for signs of hallucinations on open-ended question answering tasks. Our results show that the distributions of these artifacts differ between hallucinated and non-hallucinated generations. Building on this insight, we train binary classifiers that use these artifacts as input features to classify model generations into hallucinations and non-hallucinations. These hallucination classifiers achieve up to 0.80 AUROC. We further show that tokens preceding a hallucination can predict the subsequent hallucination before it occurs.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Binary classifiers that use artifacts associated with the model generations as input features to classify model generations into hallucinations and non-hallucinations are trained and it is shown that tokens preceding a hallucination can predict the subsequent hallucination before it occurs."
            },
            "score": 5
        },
        {
            "id": "f727f928e7e179307d8d4a1da2387393f2bd7915",
            "paperId": "f727f928e7e179307d8d4a1da2387393f2bd7915",
            "title": "Methods for Measuring, Updating, and Visualizing Factual Beliefs in Language Models",
            "abstract": "Language models can memorize a considerable amount of factual information during pretraining that can be elicited through prompting or finetuning models on tasks like question answering. In this paper, we discuss approaches to measuring model factual beliefs, updating incorrect factual beliefs in models, and visualizing graphical relationships between factual beliefs. Our main contributions include: (1) new metrics for evaluating belief-updating methods focusing on the logical consistency of beliefs, (2) a training objective for Sequential, Local, and Generalizing updates (SLAG) that improves the performance of existing hypernetwork approaches, and (3) the introduction of the belief graph, a new form of visualization for language models that shows relationships between stored model beliefs. Our experiments suggest that models show only limited consistency between factual beliefs, but update methods can both fix incorrect model beliefs and greatly improve their consistency. Although off-the-shelf optimizers are surprisingly strong belief-updating baselines, our learned optimizers can outperform them in more difficult settings than have been considered in past work.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experiments suggest that models show only limited consistency between factual beliefs, but update methods can both fix incorrect model beliefs and greatly improve their consistency, and off-the-shelf optimizers can outperform them in more difficult settings than have been considered in past work."
            },
            "score": 5
        },
        {
            "id": "663d743272e9ab04f54d9105a3c3a3f6e22dd1dd",
            "paperId": "663d743272e9ab04f54d9105a3c3a3f6e22dd1dd",
            "title": "FactKB: Generalizable Factuality Evaluation using Language Models Enhanced with Factual Knowledge",
            "abstract": "Evaluating the factual consistency of automatically generated summaries is essential for the progress and adoption of reliable summarization systems. Despite recent advances, existing factuality evaluation models are not robust, being especially prone to entity and relation errors in new domains. We propose FactKB, a simple new approach to factuality evaluation that is generalizable across domains, in particular with respect to entities and relations. FactKB is based on language models pretrained using facts extracted from external knowledge bases. We introduce three types of complementary factuality pretraining objectives based on direct entity facts, facts grounded in auxiliary knowledge about entities, and facts constructed compositionally through knowledge base walks. The resulting factuality evaluation model achieves state-of-the-art performance on two in-domain news summarization benchmarks as well as on three out-of-domain scientific literature datasets. Further analysis of FactKB shows improved ability to detect erroneous entities and relations in summaries and is robust and generalizable across domains.",
            "year": 2023,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "FactKB is a simple new approach to factuality evaluation that is generalizable across domains, in particular with respect to entities and relations and shows improved ability to detect erroneous entities and relation in summaries."
            },
            "score": 5
        },
        {
            "id": "e01515c6138bc525f7aec30fc85f2adf028d4156",
            "paperId": "e01515c6138bc525f7aec30fc85f2adf028d4156",
            "title": "Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision",
            "abstract": "Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and guide the LLM through in-context learning from demonstrations (of principles application) to produce helpful, ethical, and reliable responses to user's queries; third, we fine-tune the original LLM with the high-quality self-aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally, we offer a refinement step to address the issues of overly-brief or indirect responses. Applying SELF-ALIGN to the LLaMA-65b base language model, we develop an AI assistant named Dromedary. With fewer than 300 lines of human annotations (including<200 seed prompts, 16 generic principles, and 5 exemplars for in-context learning). Dromedary significantly surpasses the performance of several state-of-the-art AI systems, including Text-Davinci-003 and Alpaca, on benchmark datasets with various settings.",
            "year": 2023,
            "citationCount": 137,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An AI assistant named Dromedary is developed, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision and significantly surpasses the performance of several state-of-the-art AI systems on benchmark datasets with various settings."
            },
            "score": 5
        },
        {
            "id": "6d4bacb69923e1e94fb4de468b939ce6db32fb51",
            "paperId": "6d4bacb69923e1e94fb4de468b939ce6db32fb51",
            "title": "Large Language Models Cannot Self-Correct Reasoning Yet",
            "abstract": "Large Language Models (LLMs) have emerged as a groundbreaking technology with their unparalleled text generation capabilities across various applications. Nevertheless, concerns persist regarding the accuracy and appropriateness of their generated content. A contemporary methodology, self-correction, has been proposed as a remedy to these issues. Building upon this premise, this paper critically examines the role and efficacy of self-correction within LLMs, shedding light on its true potential and limitations. Central to our investigation is the notion of intrinsic self-correction, whereby an LLM attempts to correct its initial responses based solely on its inherent capabilities, without the crutch of external feedback. In the context of reasoning, our research indicates that LLMs struggle to self-correct their responses without external feedback, and at times, their performance even degrades after self-correction. Drawing from these insights, we offer suggestions for future research and practical applications in this field.",
            "year": 2023,
            "citationCount": 122,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is indicated that LLMs struggle to self-correct their responses without external feedback, and at times, their performance even degrades after self-correction."
            },
            "score": 5
        },
        {
            "id": "9aa6a885754a27fe42a87e4dfaed87d618fd8518",
            "paperId": "9aa6a885754a27fe42a87e4dfaed87d618fd8518",
            "title": "Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback",
            "abstract": "Large language models (LLMs) have shown remarkable progress in automated code generation. Yet, incorporating LLM-based code generation into real-life software projects poses challenges, as the generated code may contain errors in API usage, class, data structure, or missing project-specific information. As much of this project-specific context cannot fit into the prompts of LLMs, we must find ways to allow the model to explore the project-level code context. To this end, this paper puts forward a novel approach, termed ProCoder, which iteratively refines the project-level code context for precise code generation, guided by the compiler feedback. In particular, ProCoder first leverages compiler techniques to identify a mismatch between the generated code and the project's context. It then iteratively aligns and fixes the identified errors using information extracted from the code repository. We integrate ProCoder with two representative LLMs, i.e., GPT-3.5-Turbo and Code Llama (13B), and apply it to Python code generation. Experimental results show that ProCoder significantly improves the vanilla LLMs by over 80% in generating code dependent on project context, and consistently outperforms the existing retrieval-based code generation baselines.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel approach, termed ProCoder, which iteratively refines the project-level code context for precise code generation, guided by the compiler feedback, and consistently outperforms the existing retrieval-based code generation baselines."
            },
            "score": 5
        },
        {
            "id": "3961fee54aae34f2cf18dbde6309a3bb447e382f",
            "paperId": "3961fee54aae34f2cf18dbde6309a3bb447e382f",
            "title": "Improving Iterative Text Revision by Learning Where to Edit from Other Revision Tasks",
            "abstract": "Iterative text revision improves text quality by fixing grammatical errors, rephrasing for better readability or contextual appropriateness, or reorganizing sentence structures throughout a document.Most recent research has focused on understanding and classifying different types of edits in the iterative revision process from human-written text instead of building accurate and robust systems for iterative text revision.In this work, we aim to build an end-to-end text revision system that can iteratively generate helpful edits by explicitly detecting editable spans (where-to-edit) with their corresponding edit intents and then instructing a revision model to revise the detected edit spans.Leveraging datasets from other related text editing NLP tasks, combined with the specification of editable spans, leads our system to more accurately model the process of iterative text refinement, as evidenced by empirical results and human evaluations.Our system significantly outperforms previous baselines on our text revision tasks and other standard text revision tasks, including grammatical error correction, text simplification, sentence fusion, and style transfer.Through extensive qualitative and quantitative analysis, we make vital connections between edit intentions and writing quality, and better computational modeling of iterative text revisions.",
            "year": 2022,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work aims to build an end-to-end text revision system that can iteratively generate helpful edits by explicitly detecting editable spans (where to-edit) with their corresponding edit intents and then instructing a revision model to revise the detected edit spans."
            },
            "score": 5
        },
        {
            "id": "afee8cdc51e95b50d7574ed1700a797874bf792c",
            "paperId": "afee8cdc51e95b50d7574ed1700a797874bf792c",
            "title": "Adversarial Fine-Tuning of Language Models: An Iterative Optimisation Approach for the Generation and Detection of Problematic Content",
            "abstract": "In this paper, we tackle the emerging challenge of unintended harmful content generation in Large Language Models (LLMs) with a novel dual-stage optimisation technique using adversarial fine-tuning. Our two-pronged approach employs an adversarial model, fine-tuned to generate potentially harmful prompts, and a judge model, iteratively optimised to discern these prompts. In this adversarial cycle, the two models seek to outperform each other in the prompting phase, generating a dataset of rich examples which are then used for fine-tuning. This iterative application of prompting and fine-tuning allows continuous refinement and improved performance. The performance of our approach is evaluated through classification accuracy on a dataset consisting of problematic prompts not detected by GPT-4, as well as a selection of contentious but unproblematic prompts. We show considerable increase in classification accuracy of the judge model on this challenging dataset as it undergoes the optimisation process. Furthermore, we show that a rudimentary model \\texttt{ada} can achieve 13\\% higher accuracy on the hold-out test set than GPT-4 after only a few rounds of this process, and that this fine-tuning improves performance in parallel tasks such as toxic comment identification.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper shows that a rudimentary model can achieve 13\\% higher accuracy on the hold-out test set than GPT-4 after only a few rounds of this process, and that this fine-tuning improves performance in parallel tasks such as toxic comment identification."
            },
            "score": 4
        },
        {
            "id": "bf4810017b54e50354cccffd8966121c7166cb17",
            "paperId": "bf4810017b54e50354cccffd8966121c7166cb17",
            "title": "Iterative Translation Refinement with Large Language Models",
            "abstract": "Large language models have shown surprising performances in understanding instructions and performing natural language tasks. In this paper, we propose iterative translation refinement to leverage the power of large language models for more natural translation and post-editing. We show that by simply involving a large language model in an iterative process, the output quality improves beyond mere translation. Extensive test scenarios with GPT-3.5 reveal that although iterations reduce string-based metric scores, neural metrics indicate comparable if not improved translation quality. Further, human evaluations demonstrate that our method effectively reduces translationese compared to initial GPT translations and even human references, especially for into-English directions. Ablation studies underscore the importance of anchoring the refinement process to the source input and a reasonable initial translation.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that by simply involving a large language model in an iterative process, the output quality improves beyond mere translation, and that although iterations reduce string-based metric scores, neural metrics indicate comparable if not improved translation quality."
            },
            "score": 4
        },
        {
            "id": "5fb3521f87d03899731b98718702927afd227f3a",
            "paperId": "5fb3521f87d03899731b98718702927afd227f3a",
            "title": "Enhancing Ethical Explanations of Large Language Models through Iterative Symbolic Refinement",
            "abstract": "An increasing amount of research in Natural Language Inference (NLI) focuses on the application and evaluation of Large Language Models (LLMs) and their reasoning capabilities. Despite their success, however, LLMs are still prone to factual errors and inconsistencies in their explanations, offering limited control and interpretability for inference in complex domains. In this paper, we focus on ethical NLI, investigating how hybrid neuro-symbolic techniques can enhance the logical validity and alignment of ethical explanations produced by LLMs. Specifically, we present an abductive-deductive framework named Logic-Explainer, which integrates LLMs with an external backward-chaining solver to refine step-wise natural language explanations and jointly verify their correctness, reduce incompleteness and minimise redundancy. An extensive empirical analysis demonstrates that Logic-Explainer can improve explanations generated via in-context learning methods and Chain-of-Thought (CoT) on challenging ethical NLI tasks, while, at the same time, producing formal proofs describing and supporting models\u2019 reasoning. As ethical NLI requires commonsense reasoning to identify underlying moral violations, our results suggest the effectiveness of neuro-symbolic methods for multi-step NLI more broadly, opening new opportunities to enhance the logical consistency, reliability, and alignment of LLMs.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An abductive-deductive framework named Logic-Explainer is presented, which integrates LLMs with an external backward-chaining solver to refine step-wise natural language explanations and jointly verify their correctness, reduce incompleteness and minimise redundancy."
            },
            "score": 4
        },
        {
            "id": "b73ea8adfef3cdc63167e3a7f1f2be3f4ef4219d",
            "paperId": "b73ea8adfef3cdc63167e3a7f1f2be3f4ef4219d",
            "title": "Knowledge Refinement via Interaction Between Search Engines and Large Language Models",
            "abstract": "Information retrieval (IR) plays a crucial role in locating relevant resources from vast amounts of data, and its applications have evolved from traditional knowledge bases to modern search engines (SEs). The emergence of large language models (LLMs) has further revolutionized the IR field by enabling users to interact with search systems in natural language. In this paper, we explore the advantages and disadvantages of LLMs and SEs, highlighting their respective strengths in understanding user-issued queries and retrieving up-to-date information. To leverage the benefits of both paradigms while circumventing their limitations, we propose InteR, a novel framework that facilitates knowledge refinement through interaction between SEs and LLMs. InteR allows SEs to expand knowledge in queries using LLM-generated knowledge collections and enables LLMs to enhance prompt formulation using SE-retrieved documents. This iterative refinement process augments the inputs of SEs and LLMs, leading to more accurate retrieval. Experiments on large-scale retrieval benchmarks involving web search and low-resource retrieval tasks demonstrate that InteR achieves overall superior zero-shot retrieval performance compared to state-of-the-art methods, even those using relevance judgment. Source code is available at https://github.com/Cyril-JZ/InteR.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "InteR is proposed, a novel framework that facilitates knowledge refinement through interaction between SEs and LLMs, which allows SEs to expand knowledge in queries using LLM-generated knowledge collections and enables LLMs to enhance prompt formulation using SE-retrieved documents."
            },
            "score": 4
        },
        {
            "id": "995b2f650f55de6077b87db6dadb01cecd86dbd7",
            "paperId": "995b2f650f55de6077b87db6dadb01cecd86dbd7",
            "title": "Advanced prompting as a catalyst: Empowering large language models in the management of gastrointestinal cancers",
            "abstract": "Large Language Models' (LLMs) performance in healthcare can be significantly impacted by prompt engineering. However, the area of study remains relatively uncharted in gastrointestinal oncology until now. Our research delves into this unexplored territory, investigating the efficacy of varied prompting strategies, including simple prompts, templated prompts, in-context learning (ICL), and multi-round iterative questioning, for optimizing the performance of LLMs within a medical setting. We develop a comprehensive evaluation system to assess the performance of LLMs across multiple dimensions. This robust evaluation system ensures a thorough assessment of the LLMs' capabilities in the field of medicine. Our findings suggest a positive relationship between the comprehensiveness of the prompts and the LLMs' performance. Notably, the multi-round strategy, which is characterized by iterative question-and-answer rounds, consistently yields the best results. ICL, a strategy that capitalizes on interrelated contextual learning, also displays significant promise, surpassing the outcomes achieved with simpler prompts. The research underscores the potential of advanced prompt engineering and iterative learning approaches for boosting the applicability of LLMs in healthcare. We recommend that additional research be conducted to refine these strategies and investigate their potential integration, to truly harness the full potential of LLMs in medical applications.\n",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The research investigates the efficacy of varied prompting strategies, including simple prompts, templated prompts, in-context learning (ICL), and multi-round iterative questioning, for optimizing the performance of LLMs within a medical setting, and develops a comprehensive evaluation system."
            },
            "score": 4
        },
        {
            "id": "0f6fe87afd1a3571f77c790893b03717e5d0422a",
            "paperId": "0f6fe87afd1a3571f77c790893b03717e5d0422a",
            "title": "Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators",
            "abstract": "Large language models (LLMs) outperform information retrieval techniques for downstream knowledge-intensive tasks when being prompted to generate world knowledge. However, community concerns abound regarding the factuality and potential implications of using this uncensored knowledge. In light of this, we introduce CONNER, a COmpreheNsive kNowledge Evaluation fRamework, designed to systematically and automatically evaluate generated knowledge from six important perspectives -- Factuality, Relevance, Coherence, Informativeness, Helpfulness and Validity. We conduct an extensive empirical analysis of the generated knowledge from three different types of LLMs on two widely studied knowledge-intensive tasks, i.e., open-domain question answering and knowledge-grounded dialogue. Surprisingly, our study reveals that the factuality of generated knowledge, even if lower, does not significantly hinder downstream tasks. Instead, the relevance and coherence of the outputs are more important than small factual mistakes. Further, we show how to use CONNER to improve knowledge-intensive tasks by designing two strategies: Prompt Engineering and Knowledge Selection. Our evaluation code and LLM-generated knowledge with human annotations will be released to facilitate future research.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Surprisingly, the study reveals that the factuality of generated knowledge, even if lower, does not significantly hinder downstream tasks and the relevance and coherence of the outputs are more important than small factual mistakes."
            },
            "score": 4
        },
        {
            "id": "f1a834ca955edcbab2c6247e4e08c3f9898d43d9",
            "paperId": "f1a834ca955edcbab2c6247e4e08c3f9898d43d9",
            "title": "GO FIGURE: A Meta Evaluation of Factuality in Summarization",
            "abstract": "While neural language models can generate text with remarkable fluency and coherence, controlling for factual correctness in generation remains an open research question. This major discrepancy between the surface-level fluency and the content-level correctness of neural generation has motivated a new line of research that seeks automatic metrics for evaluating the factuality of machine text. In this paper, we introduce GO FIGURE, a meta-evaluation framework for evaluating factuality evaluation metrics. We propose five necessary and intuitive conditions to evaluate factuality metrics on diagnostic factuality data across three different summarization tasks. Our benchmark analysis on ten factuality metrics reveals that our meta-evaluation framework provides a robust and efficient evaluation that is extensible to multiple types of factual consistency and standard generation metrics, including QA metrics. It also reveals that while QA metrics generally improve over standard metrics that measure factuality across domains, performance is highly dependent on the way in which questions are generated.",
            "year": 2020,
            "citationCount": 75,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "GO FIGURE, a meta-evaluation framework for evaluating factuality evaluation metrics, is introduced and it is revealed that while QA metrics generally improve over standard metrics that measure factuality across domains, performance is highly dependent on the way in which questions are generated."
            },
            "score": 4
        },
        {
            "id": "2b590573c920ddf166cc0b9f1c2d100858c9ac84",
            "paperId": "2b590573c920ddf166cc0b9f1c2d100858c9ac84",
            "title": "FFCI: A Framework for Interpretable Automatic Evaluation of Summarization",
            "abstract": "In this paper, we propose FFCI, a framework for fine-grained summarization evaluation that comprises four elements: faithfulness (degree of factual consistency with the source), focus (precision of summary content relative to the reference), coverage (recall of summary content relative to the reference), and inter-sentential coherence (document fluency between adjacent sentences). We construct a novel dataset for focus, coverage, and inter-sentential coherence, and develop automatic methods for evaluating each of the four dimensions of FFCI based on cross-comparison of evaluation metrics and model-based evaluation methods, including question answering (QA) approaches, semantic textual similarity (STS), next-sentence prediction (NSP), and scores derived from 19 pre-trained language models. We then apply the developed metrics in evaluating a broad range of summarization models across two datasets, with some surprising findings.",
            "year": 2020,
            "citationCount": 29,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper constructs a novel dataset for focus, coverage, and inter-sentential coherence, and develops automatic methods for evaluating each of the four dimensions of FFCI based on cross-comparison of evaluation metrics and model-based evaluation methods."
            },
            "score": 4
        },
        {
            "id": "84b77180228051040286423cec82b62c323a8fda",
            "paperId": "84b77180228051040286423cec82b62c323a8fda",
            "title": "Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation",
            "abstract": "Knowledge-intensive tasks (e.g., open-domain question answering (QA)) require a substantial amount of factual knowledge and often rely on external information for assistance. Recently, large language models (LLMs) (e.g., ChatGPT), have demonstrated impressive prowess in solving a wide range of tasks with world knowledge, including knowledge-intensive tasks. However, it remains unclear how well LLMs are able to perceive their factual knowledge boundaries, particularly how they behave when incorporating retrieval augmentation. In this study, we present an initial analysis of the factual knowledge boundaries of LLMs and how retrieval augmentation affects LLMs on open-domain QA. Specially, we focus on three primary research questions and analyze them by examining QA performance, priori judgement and posteriori judgement of LLMs. We show evidence that LLMs possess unwavering confidence in their capabilities to respond to questions and the accuracy of their responses. Furthermore, retrieval augmentation proves to be an effective approach in enhancing LLMs' awareness of knowledge boundaries, thereby improving their judgemental abilities. Additionally, we also find that LLMs have a propensity to rely on the provided retrieval results when formulating answers, while the quality of these results significantly impacts their reliance. The code to reproduce this work is available at https://github.com/RUCAIBox/LLM-Knowledge-Boundary.",
            "year": 2023,
            "citationCount": 48,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study presents an initial analysis of the factual knowledge boundaries of LLMs and how retrieval augmentation affects LLMs on open-domain QA and finds that LLMs have a propensity to rely on the provided retrieval results when formulating answers, while the quality of these results significantly impacts their reliance."
            },
            "score": 4
        },
        {
            "id": "fdc75b7b51ca98e555a4a8f0bc261e76bac6fb70",
            "paperId": "fdc75b7b51ca98e555a4a8f0bc261e76bac6fb70",
            "title": "Attention Satisfies: A Constraint-Satisfaction Lens on Factual Errors of Language Models",
            "abstract": "We investigate the internal behavior of Transformer-based Large Language Models (LLMs) when they generate factually incorrect text. We propose modeling factual queries as constraint satisfaction problems and use this framework to investigate how the LLM interacts internally with factual constraints. We find a strong positive relationship between the LLM's attention to constraint tokens and the factual accuracy of generations. We curate a suite of 10 datasets containing over 40,000 prompts to study the task of predicting factual errors with the Llama-2 family across all scales (7B, 13B, 70B). We propose SAT Probe, a method probing attention patterns, that can predict factual errors and fine-grained constraint satisfaction, and allow early error identification. The approach and findings take another step towards using the mechanistic understanding of LLMs to enhance their reliability.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A strong positive relationship is found between the LLM's attention to constraint tokens and the factual accuracy of generations, and SAT Probe, a method probing attention patterns, that can predict factual errors and fine-grained constraint satisfaction, and allow early error identification."
            },
            "score": 4
        },
        {
            "id": "5e096f65139e789fd3aa41de7e11bc9c04da79d5",
            "paperId": "5e096f65139e789fd3aa41de7e11bc9c04da79d5",
            "title": "Measuring and Modifying Factual Knowledge in Large Language Models",
            "abstract": "Large Language Models (LLMs) store an extensive amount of factual knowledge obtained from vast collections of text. To effectively utilize these models for downstream tasks, it is crucial to have reliable methods for measuring their knowledge. However, existing approaches for knowledge measurement have certain limitations, and despite recent efforts, they fail to provide accurate measurements and the necessary insights for modifying the knowledge within LLMs. In this work, we employ information theory-based measurements to provide a framework estimating the factual knowledge contained within large language models. More specifically, we measure knowledge by analyzing the LLM's prediction probability distribution before and after instilling the target knowledge, employing metrics such as entropy and KL-divergence. Introducing our metrics, we first assess their accuracy in comparison to previous ranking-based methods, surpassing them by around 30% in a synthetic experiment. Then, we explore two prominent methods of knowledge instillation, discovering that LLMs exhibit limitations in capturing new knowledge under specific circumstances for one of these methods. Lastly, we demonstrate the applicability of our methods in extracting unlearned and mislearned facts in LLMs through their application to in-context learning.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work employs information theory-based measurements to provide a framework estimating the factual knowledge contained within large language models by analyzing the LLM's prediction probability distribution before and after instilling the target knowledge, employing metrics such as entropy and KL-divergence."
            },
            "score": 4
        },
        {
            "id": "56373d3fd0f1354a61f9e577db039cdb187d8d43",
            "paperId": "56373d3fd0f1354a61f9e577db039cdb187d8d43",
            "title": "Evaluating the Factual Consistency of Large Language Models Through News Summarization",
            "abstract": "While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information. To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB(Factual Inconsistency Benchmark) that focuses on the task of summarization. Specifically, our benchmark involves comparing the scores an LLM assigns to a factually consistent versus a factually inconsistent summary for an input news article. For factually consistent summaries, we use human-written reference summaries that we manually verify as factually consistent. To generate summaries that are factually inconsistent, we generate summaries from a suite of summarization models that we have manually annotated as factually inconsistent. A model's factual consistency is then measured according to its accuracy, i.e.\\ the proportion of documents where it assigns a higher score to the factually consistent summary. To validate the usefulness of FIB, we evaluate 23 large language models ranging from 1B to 176B parameters from six different model families including BLOOM and OPT. We find that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries. However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries. We validate design choices in our benchmark including the scoring method and source of distractor summaries. Our code and benchmark data can be found at https://github.com/r-three/fib.",
            "year": 2022,
            "citationCount": 51,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new benchmark called FIB(Factual Inconsistency Benchmark) that focuses on the task of summarization, and finds that existing LLMs generally assign a higher score to factually consistent summaries than tofactually inconsistent summaries."
            },
            "score": 4
        },
        {
            "id": "7471cb40a33e9d971a922b5dff5ca9b4a73ca609",
            "paperId": "7471cb40a33e9d971a922b5dff5ca9b4a73ca609",
            "title": "Calibrating Factual Knowledge in Pretrained Language Models",
            "abstract": "Previous literature has proved that Pretrained Language Models (PLMs) can store factual knowledge. However, we find that facts stored in the PLMs are not always correct. It motivates us to explore a fundamental question: How do we calibrate factual knowledge in PLMs without re-training from scratch? In this work, we propose a simple and lightweight method CaliNet to achieve this goal. To be specific, we first detect whether PLMs can learn the right facts via a contrastive score between right and fake facts. If not, we then use a lightweight method to add and adapt new parameters to specific factual texts. Experiments on the knowledge probing task show the calibration effectiveness and efficiency. In addition, through closed-book question answering, we find that the calibrated PLM possesses knowledge generalization ability after fine-tuning. Beyond the calibration performance, we further investigate and visualize the knowledge calibration mechanism.",
            "year": 2022,
            "citationCount": 42,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a simple and lightweight method CaliNet to calibrate factual knowledge in PLMs without re-training from scratch, and finds that the calibrated PLM possesses knowledge generalization ability after fine-tuning."
            },
            "score": 4
        },
        {
            "id": "29e1ad90206023b2a9ddde36be019a04a166df60",
            "paperId": "29e1ad90206023b2a9ddde36be019a04a166df60",
            "title": "Self-Play Preference Optimization for Language Model Alignment",
            "abstract": "Traditional reinforcement learning from human feedback (RLHF) approaches relying on parametric models like the Bradley-Terry model fall short in capturing the intransitivity and irrationality in human preferences. Recent advancements suggest that directly working with preference probabilities can yield a more accurate reflection of human preferences, enabling more flexible and accurate language model alignment. In this paper, we propose a self-play-based method for language model alignment, which treats the problem as a constant-sum two-player game aimed at identifying the Nash equilibrium policy. Our approach, dubbed \\textit{Self-Play Preference Optimization} (SPPO), approximates the Nash equilibrium through iterative policy updates and enjoys theoretical convergence guarantee. Our method can effectively increase the log-likelihood of the chosen response and decrease that of the rejected response, which cannot be trivially achieved by symmetric pairwise loss such as Direct Preference Optimization (DPO) and Identity Preference Optimization (IPO). In our experiments, using only 60k prompts (without responses) from the UltraFeedback dataset and without any prompt augmentation, by leveraging a pre-trained preference model PairRM with only 0.4B parameters, SPPO can obtain a model from fine-tuning Mistral-7B-Instruct-v0.2 that achieves the state-of-the-art length-controlled win-rate of 28.53% against GPT-4-Turbo on AlpacaEval 2.0. It also outperforms the (iterative) DPO and IPO on MT-Bench and the Open LLM Leaderboard. Notably, the strong performance of SPPO is achieved without additional external supervision (e.g., responses, preferences, etc.) from GPT-4 or other stronger language models.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A self-play-based method for language model alignment, which approximates the Nash equilibrium through iterative policy updates and enjoys theoretical convergence guarantee, and can effectively increase the log-likelihood of the chosen response and decrease that of the rejected response."
            },
            "score": 4
        },
        {
            "id": "79902c0b7692c4ba122fbd15faf517a8d82308db",
            "paperId": "79902c0b7692c4ba122fbd15faf517a8d82308db",
            "title": "Harnessing Large Language Models to Enhance Self-Regulated Learning via Formative Feedback",
            "abstract": "Effectively supporting students in mastering all facets of self-regulated learning is a central aim of teachers and educational researchers. Prior research could demonstrate that formative feedback is an effective way to support students during self-regulated learning (SRL). However, for formative feedback to be effective, it needs to be tailored to the learners, requiring information about their learning progress. In this work, we introduce LEAP, a novel platform that utilizes advanced large language models (LLMs), such as ChatGPT, to provide formative feedback to students. LEAP empowers teachers with the ability to effectively pre-prompt and assign tasks to the LLM, thereby stimulating students' cognitive and metacognitive processes and promoting self-regulated learning. We demonstrate that a systematic prompt design based on theoretical principles can provide a wide range of types of scaffolds to students, including sense-making, elaboration, self-explanation, partial task-solution scaffolds, as well as metacognitive and motivational scaffolds. In this way, we emphasize the critical importance of synchronizing educational technological advances with empirical research and theoretical frameworks.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that a systematic prompt design based on theoretical principles can provide a wide range of types of scaffolds to students, including sense-making, elaboration, self-explanation, partial task-solution scaffolds, as well as metacognitive and motivational scaffolds."
            },
            "score": 4
        },
        {
            "id": "6fbf4e4c7872efdc03f7003d2d89b15ad8c4c552",
            "paperId": "6fbf4e4c7872efdc03f7003d2d89b15ad8c4c552",
            "title": "The Capacity for Moral Self-Correction in Large Language Models",
            "abstract": "We test the hypothesis that language models trained with reinforcement learning from human feedback (RLHF) have the capability to\"morally self-correct\"-- to avoid producing harmful outputs -- if instructed to do so. We find strong evidence in support of this hypothesis across three different experiments, each of which reveal different facets of moral self-correction. We find that the capability for moral self-correction emerges at 22B model parameters, and typically improves with increasing model size and RLHF training. We believe that at this level of scale, language models obtain two capabilities that they can use for moral self-correction: (1) they can follow instructions and (2) they can learn complex normative concepts of harm like stereotyping, bias, and discrimination. As such, they can follow instructions to avoid certain kinds of morally harmful outputs. We believe our results are cause for cautious optimism regarding the ability to train language models to abide by ethical principles.",
            "year": 2023,
            "citationCount": 101,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 4
        },
        {
            "id": "8e37dc1215681aa153a51c07078ba8befd6a6e01",
            "paperId": "8e37dc1215681aa153a51c07078ba8befd6a6e01",
            "title": "AdaPlanner: Adaptive Planning from Feedback with Language Models",
            "abstract": "Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively.",
            "year": 2023,
            "citationCount": 49,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A closed-loop approach, AdaPlanner, is proposed, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback, and develops a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities."
            },
            "score": 4
        },
        {
            "id": "1d14a708622917da4b9820ada6d32af24fc1651a",
            "paperId": "1d14a708622917da4b9820ada6d32af24fc1651a",
            "title": "Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic Image Design and Generation",
            "abstract": "We introduce ``Idea to Image,'' a system that enables multimodal iterative self-refinement with GPT-4V(ision) for automatic image design and generation. Humans can quickly identify the characteristics of different text-to-image (T2I) models via iterative explorations. This enables them to efficiently convert their high-level generation ideas into effective T2I prompts that can produce good images. We investigate if systems based on large multimodal models (LMMs) can develop analogous multimodal self-refinement abilities that enable exploring unknown models or environments via self-refining tries. Idea2Img cyclically generates revised T2I prompts to synthesize draft images, and provides directional feedback for prompt revision, both conditioned on its memory of the probed T2I model's characteristics. The iterative self-refinement brings Idea2Img various advantages over vanilla T2I models. Notably, Idea2Img can process input ideas with interleaved image-text sequences, follow ideas with design instructions, and generate images of better semantic and visual qualities. The user preference study validates the efficacy of multimodal iterative self-refinement on automatic image design and generation.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 4
        },
        {
            "id": "f97ebf8bad02544a42d9305849737160a3b7ad65",
            "paperId": "f97ebf8bad02544a42d9305849737160a3b7ad65",
            "title": "Improving Word Embeddings through Iterative Refinement of Word- and Character-level Models",
            "abstract": "Embedding of rare and out-of-vocabulary (OOV) words is an important open NLP problem. A popular solution is to train a character-level neural network to reproduce the embeddings from a standard word embedding model. The trained network is then used to assign vectors to any input string, including OOV and rare words. We enhance this approach and introduce an algorithm that iteratively refines and improves both word- and character-level models. We demonstrate that our method outperforms the existing algorithms on 5 word similarity data sets, and that it can be successfully applied to job title normalization, an important problem in the e-recruitment domain that suffers from the OOV problem.",
            "year": 2020,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that this method outperforms the existing algorithms on 5 word similarity data sets, and that it can be successfully applied to job title normalization, an important problem in the e-recruitment domain that suffers from the OOV problem."
            },
            "score": 4
        },
        {
            "id": "c6e2837894bb545fc62ef41d7dadcd6e1bb51dc5",
            "paperId": "c6e2837894bb545fc62ef41d7dadcd6e1bb51dc5",
            "title": "PromptCharm: Text-to-Image Generation through Multi-modal Prompting and Refinement",
            "abstract": "The recent advancements in Generative AI have significantly advanced the field of text-to-image generation. The state-of-the-art text-to-image model, Stable Diffusion, is now capable of synthesizing high-quality images with a strong sense of aesthetics. Crafting text prompts that align with the model's interpretation and the user's intent thus becomes crucial. However, prompting remains challenging for novice users due to the complexity of the stable diffusion model and the non-trivial efforts required for iteratively editing and refining the text prompts. To address these challenges, we propose PromptCharm, a mixed-initiative system that facilitates text-to-image creation through multi-modal prompt engineering and refinement. To assist novice users in prompting, PromptCharm first automatically refines and optimizes the user's initial prompt. Furthermore, PromptCharm supports the user in exploring and selecting different image styles within a large database. To assist users in effectively refining their prompts and images, PromptCharm renders model explanations by visualizing the model's attention values. If the user notices any unsatisfactory areas in the generated images, they can further refine the images through model attention adjustment or image inpainting within the rich feedback loop of PromptCharm. To evaluate the effectiveness and usability of PromptCharm, we conducted a controlled user study with 12 participants and an exploratory user study with another 12 participants. These two studies show that participants using PromptCharm were able to create images with higher quality and better aligned with the user's expectations compared with using two variants of PromptCharm that lacked interaction or visualization support.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "PromptCharm is a mixed-initiative system that facilitates text-to-image creation through multi-modal prompt engineering and refinement and helps novice users in prompting by automatically refines and optimizes the user's initial prompt."
            },
            "score": 4
        },
        {
            "id": "1db56be01aeb44ca0f3fcebb45180cab1e4cd82e",
            "paperId": "1db56be01aeb44ca0f3fcebb45180cab1e4cd82e",
            "title": "AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation",
            "abstract": "The advancement of natural language processing (NLP) has been significantly boosted by the development of transformer-based large language models (LLMs). These models have revolutionized NLP tasks, particularly in code generation, aiding developers in creating software with enhanced efficiency. Despite their advancements, challenges in balancing code snippet generation with effective test case generation and execution persist. To address these issues, this paper introduces Multi-Agent Assistant Code Generation (AgentCoder), a novel solution comprising a multi-agent framework with specialized agents: the programmer agent, the test designer agent, and the test executor agent. During the coding procedure, the programmer agent will focus on the code generation and refinement based on the test executor agent's feedback. The test designer agent will generate test cases for the generated code, and the test executor agent will run the code with the test cases and write the feedback to the programmer. This collaborative system ensures robust code generation, surpassing the limitations of single-agent models and traditional methodologies. Our extensive experiments on 9 code generation models and 12 enhancement approaches showcase AgentCoder's superior performance over existing code generation models and prompt engineering techniques across various benchmarks. For example, AgentCoder achieves 77.4% and 89.1% pass@1 in HumanEval-ET and MBPP-ET with GPT-3.5, while SOTA baselines obtain only 69.5% and 63.0%.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces Multi-Agent Assistant Code Generation (AgentCoder), a novel solution comprising a multi-agent framework with specialized agents: the programmer agent, the test designer agent, and the test executor agent that ensures robust code generation, surpassing the limitations of single-agent models and traditional methodologies"
            },
            "score": 4
        },
        {
            "id": "ea6b0b5904d6e8eccbccb609ac35911ae967cd2c",
            "paperId": "ea6b0b5904d6e8eccbccb609ac35911ae967cd2c",
            "title": "Creating Suspenseful Stories: Iterative Planning with Large Language Models",
            "abstract": "Automated story generation has been one of the long-standing challenges in NLP. Among all dimensions of stories, *suspense* is very common in human-written stories but relatively under-explored in AI-generated stories. While recent advances in large language models (LLMs) have greatly promoted language generation in general, state-of-the-art LLMs are still unreliable when it comes to suspenseful story generation. We propose a novel iterative-prompting-based planning method that is grounded in two theoretical foundations of story suspense from cognitive psychology and narratology. This theory-grounded method works in a fully zero-shot manner and does not rely on any supervised story corpora. To the best of our knowledge, this paper is the first attempt at suspenseful story generation with LLMs. Extensive human evaluations of the generated suspenseful stories demonstrate the effectiveness of our method.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel iterative-prompting-based planning method that is grounded in two theoretical foundations of story suspense from cognitive psychology and narratology and works in a fully zero-shot manner and does not rely on any supervised story corpora."
            },
            "score": 4
        },
        {
            "id": "b9d51cf2fde08765cbb60c8d1efa0c2aa883982c",
            "paperId": "b9d51cf2fde08765cbb60c8d1efa0c2aa883982c",
            "title": "Knowledge Pursuit Prompting for Zero-Shot Multimodal Synthesis",
            "abstract": "Hallucinations and unfaithful synthesis due to inaccurate prompts with insufficient semantic details are widely observed in multimodal generative models. A prevalent strategy to align multiple modalities is to fine-tune the generator with a large number of annotated text-image pairs. However, such a procedure is labor-consuming and resource-draining. The key question we ask is: can we enhance the quality and faithfulness of text-driven generative models beyond extensive text-image pair annotations? To address this question, we propose Knowledge Pursuit Prompting (KPP), a zero-shot framework that iteratively incorporates external knowledge to help generators produce reliable visual content. Instead of training generators to handle generic prompts, KPP employs a recursive knowledge query process to gather informative external facts from the knowledge base, instructs a language model to compress the acquired knowledge for prompt refinement, and utilizes text-driven generators for visual synthesis. The entire process is zero-shot, without accessing the architectures and parameters of generative models. We evaluate the framework across multiple text-driven generative tasks (image, 3D rendering, and video) on datasets of different domains. We further demonstrate the extensibility and adaptability of KPP through varying foundation model bases and instructions. Our results show that KPP is capable of generating faithful and semantically rich content across diverse visual domains, offering a promising solution to improve multimodal generative models.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Knowledge Pursuit Prompting (KPP), a zero-shot framework that iteratively incorporates external knowledge to help generators produce reliable visual content, is proposed, offering a promising solution to improve multimodal generative models."
            },
            "score": 4
        },
        {
            "id": "f197bf0fc2f228483f6af3285000d54d8d97f9eb",
            "paperId": "f197bf0fc2f228483f6af3285000d54d8d97f9eb",
            "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models",
            "abstract": "We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",
            "year": 2023,
            "citationCount": 336,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 3
        },
        {
            "id": "56e7bda25b83228f91962d3465fd587cfe8908e1",
            "paperId": "56e7bda25b83228f91962d3465fd587cfe8908e1",
            "title": "How Far Can We Extract Diverse Perspectives from Large Language Models? Criteria-Based Diversity Prompting!",
            "abstract": "Collecting diverse human opinions is costly and challenging. This leads to a recent trend in collaborative efforts between humans and Large Language Models (LLMs) for generating diverse data, offering potential scalable and efficient solutions. However, the extent of LLMs' capability to generate diverse perspectives on subjective topics remains an unexplored question. In this study, we investigate LLMs' capacity for generating diverse perspectives and rationales on subjective topics, such as social norms and argumentative texts. We formulate a new problem of maximum diversity extraction from LLMs. Motivated by how humans develop their opinions through their values, we propose a criteria-based prompting technique to ground diverse opinions. To see how far we can extract diverse perspectives from LLMs, or called diversity coverage, we employ a step-by-step recall prompting for generating more outputs from the model in an iterative manner. As we apply our methods to various tasks, indeed we find that LLMs can generate diverse opinions according to the degree of task subjectivity",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study investigates LLMs' capacity for generating diverse perspectives and rationales on subjective topics, such as social norms and argumentative texts, and proposes a criteria-based prompting technique to ground diverse opinions."
            },
            "score": 3
        },
        {
            "id": "3aa200f562346a5e312767e5e9c1333a4f2c951b",
            "paperId": "3aa200f562346a5e312767e5e9c1333a4f2c951b",
            "title": "Large Language Models, scientific knowledge and factuality: A systematic analysis in antibiotic discovery",
            "abstract": "Inferring over and extracting information from Large Language Models (LLMs) trained on a large corpus of scientific literature can potentially drive a new era in biomedical research, reducing the barriers for accessing existing medical evidence. This work examines the potential of LLMs for dialoguing with biomedical background knowledge, using the context of antibiotic discovery. The systematic analysis is applied to ten state-of-the-art models, from models specialised on biomedical scientific corpora to general models such as ChatGPT, GPT-4 and Llama 2 in two prompting-based tasks: chemical compound definition generation and chemical compound-fungus relation determination. The work provides a systematic assessment on the ability of LLMs to encode and express these relations, verifying for fluency, prompt-alignment, semantic coherence, factual knowledge and specificity of generated responses. Results show that while recent models have improved in fluency, factual accuracy is still low and models are biased towards over-represented entities. The ability of LLMs to serve as biomedical knowledge bases is questioned, and the need for additional systematic evaluation frameworks is highlighted. The best performing GPT-4 produced a factual definition for 70% of chemical compounds and 43.6% factual relations to fungi, whereas the best open source model BioGPT-large 30% of the compounds and 30% of the relations for the best-performing prompt. The results show that while LLMs are currently not fit for purpose to be used as biomedical factual knowledge bases, there is a promising emerging property in the direction of factuality as the models become domain specialised, scale-up in size and level of human feedback.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work examines the potential of LLMs for dialoguing with biomedical background knowledge, using the context of antibiotic discovery, and shows a promising emerging property in the direction of factuality as the models become domain specialised, scale-up in size and level of human feedback."
            },
            "score": 3
        },
        {
            "id": "4d74a5048b884e8bb3842240abf98915c619c8f8",
            "paperId": "4d74a5048b884e8bb3842240abf98915c619c8f8",
            "title": "Multi-Dimensional Evaluation of Text Summarization with In-Context Learning",
            "abstract": "Evaluation of natural language generation (NLG) is complex and multi-dimensional. Generated text can be evaluated for fluency, coherence, factuality, or any other dimensions of interest. Most frameworks that perform such multi-dimensional evaluation require training on large manually or synthetically generated datasets. In this paper, we study the efficacy of large language models as multi-dimensional evaluators using in-context learning, obviating the need for large training datasets. Our experiments show that in-context learning-based evaluators are competitive with learned evaluation frameworks for the task of text summarization, establishing state-of-the-art on dimensions such as relevance and factual consistency. We then analyze the effects of factors such as the selection and number of in-context examples on performance. Finally, we study the efficacy of in-context learning based evaluators in evaluating zero-shot summaries written by large language models such as GPT-3.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experiments show that in-context learning-based evaluators are competitive with learned evaluation frameworks for the task of text summarization, establishing state-of-the-art on dimensions such as relevance and factual consistency."
            },
            "score": 3
        },
        {
            "id": "133b97e40017a9bbbadd10bcd7f13088a97ca3cc",
            "paperId": "133b97e40017a9bbbadd10bcd7f13088a97ca3cc",
            "title": "Dissecting Recall of Factual Associations in Auto-Regressive Language Models",
            "abstract": "Transformer-based language models (LMs) are known to capture factual knowledge in their parameters. While previous work looked into where factual associations are stored, only little is known about how they are retrieved internally during inference. We investigate this question through the lens of information flow. Given a subject-relation query, we study how the model aggregates information about the subject and relation to predict the correct attribute. With interventions on attention edges, we first identify two critical points where information propagates to the prediction: one from the relation positions followed by another from the subject positions. Next, by analyzing the information at these points, we unveil a three-step internal mechanism for attribute extraction. First, the representation at the last-subject position goes through an enrichment process, driven by the early MLP sublayers, to encode many subject-related attributes. Second, information from the relation propagates to the prediction. Third, the prediction representation\"queries\"the enriched subject to extract the attribute. Perhaps surprisingly, this extraction is typically done via attention heads, which often encode subject-attribute mappings in their parameters. Overall, our findings introduce a comprehensive view of how factual associations are stored and extracted internally in LMs, facilitating future research on knowledge localization and editing.",
            "year": 2023,
            "citationCount": 89,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A comprehensive view of how factual associations are stored and extracted internally in LMs is introduced, facilitating future research on knowledge localization and editing."
            },
            "score": 3
        },
        {
            "id": "53d128ea815bcc0526856eb5a9c42cc977cb36a7",
            "paperId": "53d128ea815bcc0526856eb5a9c42cc977cb36a7",
            "title": "Toolformer: Language Models Can Teach Themselves to Use Tools",
            "abstract": "Language models (LMs) exhibit remarkable abilities to solve new tasks from just a few examples or textual instructions, especially at scale. They also, paradoxically, struggle with basic functionality, such as arithmetic or factual lookup, where much simpler and smaller models excel. In this paper, we show that LMs can teach themselves to use external tools via simple APIs and achieve the best of both worlds. We introduce Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction. This is done in a self-supervised way, requiring nothing more than a handful of demonstrations for each API. We incorporate a range of tools, including a calculator, a Q\\&A system, two different search engines, a translation system, and a calendar. Toolformer achieves substantially improved zero-shot performance across a variety of downstream tasks, often competitive with much larger models, without sacrificing its core language modeling abilities.",
            "year": 2023,
            "citationCount": 784,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces Toolformer, a model trained to decide which APIs to call, when to call them, what arguments to pass, and how to best incorporate the results into future token prediction, which achieves substantially improved zero-shot performance across a variety of downstream tasks."
            },
            "score": 3
        },
        {
            "id": "468d1e2d75a23fecaf96fe65d8b01ff35ea5d0bd",
            "paperId": "468d1e2d75a23fecaf96fe65d8b01ff35ea5d0bd",
            "title": "Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models",
            "abstract": "Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts inserted in English reveal a clear pattern whereby the new piece of knowledge transfers only to languages with which English has a high RankC score.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy, and conducts an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level."
            },
            "score": 3
        },
        {
            "id": "4dc023ce9ef579a797b21e5395df706130d3fefa",
            "paperId": "4dc023ce9ef579a797b21e5395df706130d3fefa",
            "title": "Welding Natural Language Queries to Analytics IRs with LLMs",
            "abstract": "From the recent momentum behind translating natural language to SQL (nl2sql), to commercial product offerings such as Co-Pilot for Microsoft Fabric, Large Language Models (LLMs) are poised to have a big impact on data analytics. In this paper, we show that LLMs can be used to convert natural language analytics queries directly to custom intermediate query representations (IRs) of modern data analytics systems. This has the direct benefit of making IRs more accessible to end-users, but interestingly, it can also result in improved translation accuracy and better end-to-end performance, especially when the query semantics is better captured in the IR rather than in SQL. We build an LLM-based pipeline (nl2weld) for one instance of this flow, to translate natural language queries to the Weld IR using gpt-4. nl2weld is carefully designed to harness self-reflection and instruction-following capabilities of gpt-4, providing it various forms of feedback such as domain specific instructions and feedback from the Weld compiler. We evaluate nl2weld on a subset of the Spider benchmark and compare it against the gold standard SQL and DIN-SQL , a state-of-the-art nl2sql system. We report a comparable accuracy of 77 . 4% on the dataset, and also demonstrate examples on which nl2weld produces code that is 1 . 2 \u2212 3 \u00d7 faster than the gold standard and DIN-SQL .",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LLMs can be used to convert natural language analytics queries directly to custom intermediate query representations (IRs) of modern data analytics systems, and can result in improved translation accuracy and better end-to-end performance, especially when the query semantics is better captured in the IR rather than in SQL."
            },
            "score": 3
        },
        {
            "id": "57865d9894e1bf199fd566e963c8f58057dc9042",
            "paperId": "57865d9894e1bf199fd566e963c8f58057dc9042",
            "title": "Words into Action: Learning Diverse Humanoid Robot Behaviors using Language Guided Iterative Motion Refinement",
            "abstract": "Humanoid robots are well suited for human habitats due to their morphological similarity, but developing controllers for them is a challenging task that involves multiple sub-problems, such as control, planning and perception. In this paper, we introduce a method to simplify controller design by enabling users to train and fine-tune robot control policies using natural language commands. We first learn a neural network policy that generates behaviors given a natural language command, such as\"walk forward\", by combining Large Language Models (LLMs), motion retargeting, and motion imitation. Based on the synthesized motion, we iteratively fine-tune by updating the text prompt and querying LLMs to find the best checkpoint associated with the closest motion in history. We validate our approach using a simulated Digit humanoid robot and demonstrate learning of diverse motions, such as walking, hopping, and kicking, without the burden of complex reward engineering. In addition, we show that our iterative refinement enables us to learn 3x times faster than a naive formulation that learns from scratch.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a method to simplify controller design by enabling users to train and fine-tune robot control policies using natural language commands, by combining Large Language Models, motion retargeting, and motion imitation."
            },
            "score": 3
        },
        {
            "id": "3a22aad6c18a9559be3bbb197494b434b872a05a",
            "paperId": "3a22aad6c18a9559be3bbb197494b434b872a05a",
            "title": "Diffusion-NAT: Self-Prompting Discrete Diffusion for Non-Autoregressive Text Generation",
            "abstract": "Recently, continuous diffusion models (CDM) have been introduced into non-autoregressive (NAR) text-to-text generation. However, the discrete nature of text increases the difficulty of CDM to generate coherent and fluent texts, and also causes the incompatibility problem between CDM and advanced NLP techniques, especially the popular pre-trained language models (PLMs).To solve it, we propose Diffusion-NAT, which introduces discrete diffusion models (DDM) into NAR text-to-text generation and integrates BART to improve the performance.By revising the decoding process of BART and the typical settings of DDM, we unify the inference process of BART and the denoising process of DDM into the same NAR masked tokens recovering task.In this way, DDM can rely on BART to perform denoising, which can benefit from both the rich pre-learned knowledge of BART and the iterative refining paradigm of DDM.Besides, we also propose the iterative self-prompting strategy to further improve the generation quality.Experimental results on 7 datasets show that our approach can outperform competitive NAR methods, and even surpass autoregressive methods.Our code and data are released at https://github.com/RUCAIBox/DiffusionNAT.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Diffusion-NAT is proposed, which introduces discrete diffusion models (DDM) into NAR text-to-text generation and integrates BART to improve the performance and proposes the iterative self-prompting strategy to further improve the generation quality."
            },
            "score": 3
        },
        {
            "id": "bb2049d437dbeaead770f4c36c4e541bc44a654f",
            "paperId": "bb2049d437dbeaead770f4c36c4e541bc44a654f",
            "title": "Process Modeling With Large Language Models",
            "abstract": "In the realm of Business Process Management (BPM), process modeling plays a crucial role in translating complex process dynamics into comprehensible visual representations, facilitating the understanding, analysis, improvement, and automation of organizational processes. Traditional process modeling methods often require extensive expertise and can be time-consuming. This paper explores the integration of Large Language Models (LLMs) into process modeling to enhance the accessibility of process modeling, offering a more intuitive entry point for non-experts while augmenting the efficiency of experts. We propose a framework that leverages LLMs for the automated generation and iterative refinement of process models starting from textual descriptions. Our framework involves innovative prompting strategies for effective LLM utilization, along with a secure model generation protocol and an error-handling mechanism. Moreover, we instantiate a concrete system extending our framework. This system provides robust quality guarantees on the models generated and supports exporting them in standard modeling notations, such as the Business Process Modeling Notation (BPMN) and Petri nets. Preliminary results demonstrate the framework's ability to streamline process modeling tasks, underscoring the transformative potential of generative AI in the BPM field.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a framework that leverages LLMs for the automated generation and iterative refinement of process models starting from textual descriptions, and instantiates a concrete system extending this framework that provides robust quality guarantees on the models generated and supports exporting them in standard modeling notations."
            },
            "score": 2
        },
        {
            "id": "d36bbbe2eb83981c4e714f1d4688334f2aae6369",
            "paperId": "d36bbbe2eb83981c4e714f1d4688334f2aae6369",
            "title": "GPT-4 as Evaluator: Evaluating Large Language Models on Pest Management in Agriculture",
            "abstract": "In the rapidly evolving field of artificial intelligence (AI), the application of large language models (LLMs) in agriculture, particularly in pest management, remains nascent. We aimed to prove the feasibility by evaluating the content of the pest management advice generated by LLMs, including the Generative Pre-trained Transformer (GPT) series from OpenAI and the FLAN series from Google. Considering the context-specific properties of agricultural advice, automatically measuring or quantifying the quality of text generated by LLMs becomes a significant challenge. We proposed an innovative approach, using GPT-4 as an evaluator, to score the generated content on Coherence, Logical Consistency, Fluency, Relevance, Comprehensibility, and Exhaustiveness. Additionally, we integrated an expert system based on crop threshold data as a baseline to obtain scores for Factual Accuracy on whether pests found in crop fields should take management action. Each model's score was weighted by percentage to obtain a final score. The results showed that GPT-3.4 and GPT-4 outperform the FLAN models in most evaluation categories. Furthermore, the use of instruction-based prompting containing domain-specific knowledge proved the feasibility of LLMs as an effective tool in agriculture, with an accuracy rate of 72%, demonstrating LLMs' effectiveness in providing pest management suggestions.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The use of instruction-based prompting containing domain-specific knowledge proved the feasibility of LLMs as an effective tool in agriculture, with an accuracy rate of 72%, demonstrating LLMs' effectiveness in providing pest management suggestions."
            },
            "score": 2
        },
        {
            "id": "165224b81f46fbda2140a4b656991287e96d3548",
            "paperId": "165224b81f46fbda2140a4b656991287e96d3548",
            "title": "Is This Abstract Generated by AI? A Research for the Gap between AI-generated Scientific Text and Human-written Scientific Text",
            "abstract": "BACKGROUND: Recent neural language models have taken a significant step forward in producing remarkably controllable, fluent, and grammatical text. Although some recent works have found that AI-generated text is not distinguishable from human-authored writing for crowd-sourcing workers, there still exist errors in AI-generated text which are even subtler and harder to spot. METHOD: In this paper, we investigate the gap between scientific content generated by AI and written by humans. Specifically, we first adopt several publicly available tools or models to investigate the performance for detecting GPT-generated scientific text. Then we utilize features from writing style to analyze the similarities and differences between the two types of content. Furthermore, more complex and deep perspectives, such as consistency, coherence, language redundancy, and factual errors, are also taken into consideration for in-depth analysis. RESULT: The results suggest that while AI has the potential to generate scientific content that is as accurate as human-written content, there is still a gap in terms of depth and overall quality. AI-generated scientific content is more likely to contain errors in language redundancy and factual issues. CONCLUSION: We find that there exists a \u201cwriting style\u201d gap between AI-generated scientific text and human-written scientific text. Moreover, based on the analysis result, we summarize a series of model-agnostic or distribution-agnostic features, which could be utilized to unknown or novel domain distribution and different generation methods. Future research should focus on not only improving the capabilities of AI models to produce high-quality content but also examining and addressing ethical and security concerns related to the generation and the use of AI-generated content.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "There exists a \u201cwriting style\u201d gap between AI-generated scientific text and human-written scientific text, which suggests that while AI has the potential to generate scientific content that is as accurate as human- written content, there is still a gap in terms of depth and overall quality."
            },
            "score": 2
        },
        {
            "id": "52b785b398e618ed13d9f1c0eb04f7e19894b595",
            "paperId": "52b785b398e618ed13d9f1c0eb04f7e19894b595",
            "title": "Et Machina: Exploring the Use of Conversational Agents Such as ChatGPT in Scientific Writing",
            "abstract": "Background: Advancements in natural language processing have led to the development of powerful language models like ChatGPT, capable of generating coherent text with minimal human input. While the potential for using ChatGPT in scientific paper writing is of growing interest, its full potential and usefulness remain unexplored. Method: We investigated the use of ChatGPT to write scientific papers on the topic of conversational agents in scientific writing. ChatGPT served as a tool to facilitate scientific writing, and we evaluated the quality, factuality, and coherence of the generated text. In this paper, we include information about the generation, processing, or adjustments made to the text in differently-colored blocks. \u2217 Position: We argue that large-scale trained conversational agents like ChatGPT can generate concise, grammatically correct, and well-structured text. However, occasional errors in factual accuracy, clarity, and scientific rigor were observed. We acknowledge the potential of ChatGPT and similar models as tools for generating initial drafts of scientific work. Conclusion: ChatGPT demonstrates promise as a scientific writing tool, but unsupervised use in this context is not yet recommended. Further research is necessary to enhance the accuracy and scientific validity of the generated text. Researchers should exercise caution and view ChatGPT as a complement to human writing rather than a replacement. We provide key recommendations for using conversational agents in scientific writing, including human oversight and training the model on scientific literature and unbiased data. By utilizing AI in the authorship process, we present an example of how AI involvement can be reported.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is argued that large-scale trained conversational agents like ChatGPT can generate concise, grammatically correct, and well-structured text, however, occasional errors in factual accuracy, clarity, and scientific rigor were observed."
            },
            "score": 2
        },
        {
            "id": "11cf91b5a3f39125cd2f6fd869e0dc2715a27bd4",
            "paperId": "11cf91b5a3f39125cd2f6fd869e0dc2715a27bd4",
            "title": "Feedback as a form of control in a technogenic educational environment",
            "abstract": "The article is devoted to control and feedback in foreign language teaching in a technogenic environment. The educational process is transformed in terms of the implementation and active use of digital technologies. ICT-rich environment provides new models of interaction between the teacher, students and digital tools. It also enriches the diversity of tasks and expands the range of possible forms of control and feedback. According to the authors, automated evaluation takes place both in out-of-classroom activities and directly in the classroom using Bring Your Own Device technology (BYOD). Automated control contributes to the intensity of the educational process. It provides all the participants with an opportunity to choose a convenient mode of work and get instant feedback, thereby allowing self-assessment and self-reflection of their own actions. When teaching foreign languages, special attention should be paid to chatbot technology. Chatbots imitate human actions and are able to perform standard repetitive tasks. The growing popularity of bots is explained by a wide range of usage spheres and the ability to integrate chatbots into social networks and mobile technologies. In the technogenic educational environment, ICT can be the basis for interaction, co-editing and peer assessment in collaborative projects. In this case, students receive feedback not only from the teacher, but also from other students, which increases the motivation for independent learning. Thus, automated control, self-assessment and peer assessment can both identify problem areas for each student and design an individual learning path, which increases the effectiveness of learning a foreign language.",
            "year": 2020,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Automated control, self-assessment and peer assessment can both identify problem areas for each student and design an individual learning path, which increases the effectiveness of learning a foreign language."
            },
            "score": 2
        },
        {
            "id": "3657d153f8e2e10b0a9b481bdfef3cdedd959fce",
            "paperId": "3657d153f8e2e10b0a9b481bdfef3cdedd959fce",
            "title": "A model of formative assessment for foreign language teaching in a secondary school",
            "abstract": "We develop and present a model of formative assessment. The analysis of official educational requirements and existing models is carried out. Further, we compare some of the existing models of formative assessment presented by domestic and foreign authors. Then we develop our own model of formative assessment for foreign language teaching in a secondary school on the base of Spotlight course. The model is developed as a supplementary tool for the assessment model designed by the authors of the course. The main purpose of the model is to involve students in the process of assessment and make them active agents of their own learning. The model consists of the following stages: learning aims delivery and sharing, students\u2019 self and peer assessment, formative feedback, given by a teacher and reflection. Each stage assumes use of certain assess-ment tools. They are: checklists, headings, Forms of comparative assessment, Map of differen-tiated assessment, Achievement Tree and a learning diary.",
            "year": 2021,
            "citationCount": 0,
            "tldr": null,
            "score": 2
        },
        {
            "id": "04fad23bbcafd647ead664cc2e112f033d52c2a0",
            "paperId": "04fad23bbcafd647ead664cc2e112f033d52c2a0",
            "title": "Generative Artificial Intelligence as a Tool for Teaching Communication in Nutrition and Dietetics Education\u2014A Novel Education Innovation",
            "abstract": "Although effective communication is fundamental to nutrition and dietetics practice, providing novice practitioners with efficacious training remains a challenge. Traditionally, human simulated patients have been utilised in health professions training, however their use and development can be cost and time prohibitive. Presented here is a platform the authors have created that allows students to interact with virtual simulated patients to practise and develop their communication skills. Leveraging the structured incorporation of large language models, it is designed by pedagogical content experts and comprises individual cases based on curricula and student needs. It is targeted towards the practice of rapport building, asking of difficult questions, paraphrasing and mistake making, all of which are essential to learning. Students appreciate the individualised and immediate feedback based on validated communication tools that encourage self-reflection and improvement. Early trials have shown students are enthusiastic about this platform, however further investigations are required to determine its impact as an experiential communication skills tool. This platform harnesses the power of artificial intelligence to bridge the gap between theory and practice in communication skills training, requiring significantly reduced costs and resources than traditional simulated patient encounters.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This platform harnesses the power of artificial intelligence to bridge the gap between theory and practice in communication skills training, requiring significantly reduced costs and resources than traditional simulated patient encounters."
            },
            "score": 2
        },
        {
            "id": "a7d81ee6c928e7c7c5f6fc816e8e8630038fa55d",
            "paperId": "a7d81ee6c928e7c7c5f6fc816e8e8630038fa55d",
            "title": "Evaluating the effectiveness of a preservice teacher technology training module incorporating SQD strategies",
            "abstract": null,
            "year": 2020,
            "citationCount": 30,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Among the six training strategies, reflection and instructional design had the highest positive impacts on these preservice language teachers\u2019 self-reported knowledge about and attitudes towards using ICT."
            },
            "score": 2
        },
        {
            "id": "7dbd677d7d79a5121a5209937984117653196931",
            "paperId": "7dbd677d7d79a5121a5209937984117653196931",
            "title": "Iterative Data Refinement for Self-Supervised Learning MR Image Reconstruction",
            "abstract": "Magnetic Resonance Imaging (MRI) is an important technique in the clinic. Fast MRI based on k-space undersampling and high-quality image reconstruction has been widely utilized, and many deep learning-based methods have been developed in recent years. Although promising results have been achieved, most existing methods require fully-sampled reference data. Unfortunately, fully-sampled MRI data are difficult to obtain in real-world applications. To address this issue, we propose a data refinement framework for self-supervised MR image reconstruction. Specifically, we first analyze the reason of the performance gap between self-supervised and supervised methods and identify that the bias in the training datasets is one major factor. Then, we design an effective self-supervised data refinement learning framework to reduce this data bias. With the data refinement, an enhanced self-supervised MR image reconstruction method is developed to prompt fast MR imaging. We evaluate our method on an in-vivo MRI dataset. Experimental results show that without utilizing any fully sampled MRI data, our self-supervised learning framework possesses strong capabilities in capturing image details and structures at high acceleration factors.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results show that without utilizing any fully sampled MRI data, the proposed self-supervised data refinement learning framework possesses strong capabilities in capturing image details and structures at high acceleration factors."
            },
            "score": 2
        },
        {
            "id": "2e250a1a593738361a1ff0aa7b6e16068467e53d",
            "paperId": "2e250a1a593738361a1ff0aa7b6e16068467e53d",
            "title": "Iterative Data Refinement for Self-Supervised MR Image Reconstruction",
            "abstract": "Magnetic Resonance Imaging (MRI) has become an important technique in the clinic for the visualization, detection, and diagnosis of various diseases. However, one bottleneck limitation of MRI is the relatively slow data acquisition process. Fast MRI based on k-space undersampling and high-quality image reconstruction has been widely utilized, and many deep learning-based methods have been developed in recent years. Although promising results have been achieved, most existing methods require fully-sampled reference data for training the deep learning models. Unfortunately, fully-sampled MRI data are difficult if not impossible to obtain in real-world applications. To address this issue, we propose a data refinement framework for self-supervised MR image reconstruction. Specifically, we first analyze the reason of the performance gap between self-supervised and supervised methods and identify that the bias in the training datasets between the two is one major factor. Then, we design an effective self-supervised training data refinement method to reduce this data bias. With the data refinement, an enhanced self-supervised MR image reconstruction framework is developed to prompt accurate MR imaging. We evaluate our method on an in-vivo MRI dataset. Experimental results show that without utilizing any fully sampled MRI data, our self-supervised framework possesses strong capabilities in capturing image details and structures at high acceleration factors.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results show that without utilizing any fully sampled MRI data, the proposed self-supervised framework possesses strong capabilities in capturing image details and structures at high acceleration factors."
            },
            "score": 2
        }
    ],
    "novelty": "no"
}