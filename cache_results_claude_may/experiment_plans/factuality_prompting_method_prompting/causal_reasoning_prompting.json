{
    "topic_description": "novel prompting methods that can improve factuality and reduce hallucination of large language models",
    "idea_name": "Causal Reasoning Prompting",
    "raw_idea": {
        "Problem": "LLMs often struggle to reason about causal relationships between events or entities, leading to hallucinations and inconsistencies when generating responses that require understanding cause-and-effect dynamics.",
        "Existing Methods": "Current approaches to improve causal reasoning in LLMs include fine-tuning on datasets with causal annotations or using prompts that encourage identifying causal relationships. However, these methods often fail to capture the complexity of real-world causal networks.",
        "Motivation": "Causal reasoning is a fundamental aspect of human cognition, enabling us to understand and predict the consequences of actions or events. By explicitly prompting LLMs to engage in causal reasoning, we can improve their ability to generate more factual and consistent responses in scenarios that involve complex causal relationships.",
        "Proposed Method": "We propose Causal Reasoning Prompting (CRP), a multi-step prompting approach that guides LLMs to reason about causal relationships in a given scenario. The steps include: 1) Identifying the key entities and events in the scenario; 2) Constructing a causal graph that captures the relationships between these entities and events; 3) Generating a response that takes into account the causal dependencies encoded in the graph; 4) Verifying the causal consistency of the generated response by comparing it with the causal graph; 5) Iteratively refining the response to resolve any causal inconsistencies.",
        "Experiment Plan": "Evaluate CRP on datasets that require causal reasoning, such as COPA and CausalQA. Compare performance with baselines such as zero-shot prompting and fine-tuning on causally annotated datasets. Assess the causal consistency of generated responses using metrics like precision and recall of causal relationships, as well as human evaluation."
    },
    "full_experiment_plan": {
        "Title": "Causal Reasoning Prompting: Improving Factuality and Consistency in Language Models",
        "Problem Statement": "Large Language Models (LLMs) often struggle to reason about causal relationships between events or entities, leading to hallucinations and inconsistencies when generating responses that require understanding cause-and-effect dynamics.",
        "Motivation": "Current approaches to improve causal reasoning in LLMs, such as fine-tuning on datasets with causal annotations or using prompts that encourage identifying causal relationships, often fail to capture the complexity of real-world causal networks. Causal reasoning is a fundamental aspect of human cognition, enabling us to understand and predict the consequences of actions or events. By explicitly prompting LLMs to engage in causal reasoning, we can improve their ability to generate more factual and consistent responses in scenarios that involve complex causal relationships.",
        "Proposed Method": "We propose Causal Reasoning Prompting (CRP), a multi-step prompting approach that guides LLMs to reason about causal relationships in a given scenario. The steps include: 1) Identifying the key entities and events in the scenario; 2) Constructing a causal graph that captures the relationships between these entities and events; 3) Generating a response that takes into account the causal dependencies encoded in the graph; 4) Verifying the causal consistency of the generated response by comparing it with the causal graph; 5) Iteratively refining the response to resolve any causal inconsistencies.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "Evaluate CRP on datasets that require causal reasoning, such as COPA (Choice of Plausible Alternatives), CausalQA, and selected tasks from BIG-bench that involve causal reasoning.",
            "Step 2: Construct Prompts": "Design prompts for each step of the CRP approach:\n1) Entity and Event Identification Prompt: Guide the model to identify key entities and events in the given scenario. E.g., 'From the given scenario, identify the main entities and events involved.'\n2) Causal Graph Construction Prompt: Instruct the model to construct a causal graph based on the identified entities and events. E.g., 'Based on the identified entities and events, construct a causal graph that captures the cause-and-effect relationships between them.'\n3) Response Generation Prompt: Ask the model to generate a response that considers the causal dependencies in the constructed graph. E.g., 'Generate a response to the given scenario, taking into account the causal relationships depicted in the causal graph.'\n4) Causal Consistency Verification Prompt: Prompt the model to verify the causal consistency of the generated response by comparing it with the causal graph. E.g., 'Check if the generated response is consistent with the causal relationships in the graph. Identify any inconsistencies.'\n5) Response Refinement Prompt: Guide the model to iteratively refine the response based on the identified inconsistencies. E.g., 'Refine the generated response to resolve the identified causal inconsistencies.'",
            "Step 3: Select Models": "Evaluate the performance of CRP on state-of-the-art LLMs, such as GPT-3.5 (text-davinci-003), GPT-4, and open-source models like LLaMA and Anthropic's Claude.",
            "Step 4: Establish Baselines": "Compare the performance of CRP with the following baselines:\n1) Zero-shot prompting: Directly prompt the model with the causal reasoning task without any additional guidance.\n2) Fine-tuning on causally annotated datasets: Fine-tune the LLMs on datasets with causal annotations and evaluate their performance on the target datasets.\n3) Causal prompting: Use prompts that encourage the model to identify causal relationships, without the multi-step approach of CRP.",
            "Step 5: Evaluate Results": "Assess the performance of CRP and the baselines using the following metrics:\n1) Accuracy: Measure the percentage of correctly answered questions or generated responses.\n2) Causal Consistency: Evaluate the causal consistency of the generated responses using metrics like precision and recall of causal relationships.\n3) Human Evaluation: Conduct a human evaluation to assess the quality and coherence of the generated responses, focusing on their causal reasoning capabilities.",
            "Step 6: Analyze and Interpret Results": "Analyze the results to determine the effectiveness of CRP in improving causal reasoning in LLMs. Identify the strengths and limitations of the approach, and discuss potential improvements or extensions."
        },
        "Test Case Examples": {
            "Example 1": {
                "Input": "Scenario: The grass is wet. What could be the cause?\nQuestion: What are the possible causes for the wet grass?",
                "Baseline Output (Zero-shot Prompting)": "There could be a few reasons why the grass is wet:\n1. It rained recently.\n2. The sprinklers were on.\n3. There was heavy dew in the morning.",
                "CRP Output": "Step 1: Entities and Events\nEntities: Grass\nEvents: Grass is wet\n\nStep 2: Causal Graph\nPossible Causes -> Grass is wet\nPossible Causes: Rain, Sprinklers, Dew\n\nStep 3: Response\nBased on the causal graph, the possible causes for the wet grass are:\n1. It rained recently, and the rain made the grass wet.\n2. The sprinklers were turned on, which sprayed water on the grass, making it wet.\n3. There was heavy dew formation in the morning, which accumulated on the grass, making it wet.\n\nStep 4: Causal Consistency Verification\nThe response is consistent with the causal graph. Each possible cause is linked to the wet grass event, explaining how it could lead to the grass being wet.\n\nStep 5: Refined Response\nThe response is causally consistent, and no further refinement is needed.",
                "Explanation": "The CRP approach helps the model generate a more comprehensive and causally consistent response by explicitly identifying the entities and events, constructing a causal graph, and generating a response that takes into account the causal relationships. The baseline output, while mentioning possible causes, does not explicitly link them to the wet grass event or explain the causal mechanisms."
            },
            "Example 2": {
                "Input": "Scenario: John was late for work. He missed an important meeting and his boss was angry. John's coworker, Sarah, covered for him in the meeting.\nQuestion: What were the consequences of John being late for work?",
                "Baseline Output (Causal Prompting)": "Based on the scenario, the consequences of John being late for work were:\n1. He missed an important meeting.\n2. His boss was angry.\n3. His coworker, Sarah, had to cover for him in the meeting.",
                "CRP Output": "Step 1: Entities and Events\nEntities: John, Sarah (coworker), Boss\nEvents: John was late for work, John missed an important meeting, Boss was angry, Sarah covered for John in the meeting\n\nStep 2: Causal Graph\nJohn was late for work -> John missed an important meeting\nJohn missed an important meeting -> Boss was angry\nJohn missed an important meeting -> Sarah covered for John in the meeting\n\nStep 3: Response\nAccording to the causal graph, the consequences of John being late for work were:\n1. John missed an important meeting because he was late.\n2. John's boss was angry because John missed the important meeting.\n3. Sarah, John's coworker, had to cover for him in the meeting because he was absent due to being late.\n\nStep 4: Causal Consistency Verification\nThe response is consistent with the causal graph. Each consequence is properly linked to John being late for work, either directly or indirectly.\n\nStep 5: Refined Response\nThe response is causally consistent, and no further refinement is needed.",
                "Explanation": "The CRP approach generates a response that clearly outlines the causal chain of events resulting from John being late for work. It shows how each consequence is connected to the initial event, providing a more coherent and causally consistent explanation compared to the baseline output, which simply lists the consequences without explicitly linking them."
            }
        },
        "Fallback Plan": "If the proposed CRP approach does not significantly improve the causal reasoning capabilities of LLMs compared to the baselines, consider the following alternative plans:\n1. Analyze the generated causal graphs and responses to identify potential weaknesses or limitations in the prompting strategy. This may involve examining the quality and relevance of the identified entities and events, the accuracy of the constructed causal graphs, and the coherence of the generated responses.\n2. Experiment with different prompting techniques or variations of the CRP approach. For example, try using more structured prompts, providing additional guidance or examples, or incorporating domain-specific knowledge into the prompts.\n3. Investigate the impact of different LLMs on the performance of CRP. Some models may be better suited for causal reasoning tasks than others, and it may be worth exploring a wider range of models or fine-tuning strategies.\n4. Consider combining CRP with other techniques, such as data augmentation or adversarial training, to further improve the causal reasoning capabilities of LLMs.\n5. If the CRP approach does not yield significant improvements, focus on analyzing the limitations and challenges of causal reasoning in LLMs. Conduct a thorough error analysis to identify common patterns or types of mistakes made by the models, and propose potential directions for future research based on these findings."
    },
    "novelty_queries": [
        "KeywordQuery(\"causal reasoning prompting language models\")",
        "KeywordQuery(\"improving factuality consistency language models\")",
        "KeywordQuery(\"multi-step prompting causal reasoning language models\")",
        "KeywordQuery(\"Causal Reasoning Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "6ac627f57b26354ab537734d820da4a6a7dde2c6",
            "paperId": "6ac627f57b26354ab537734d820da4a6a7dde2c6",
            "title": "CLadder: Assessing Causal Reasoning in Language Models",
            "abstract": "The ability to perform causal reasoning is widely considered a core feature of intelligence. In this work, we investigate whether large language models (LLMs) can coherently reason about causality. Much of the existing work in natural language processing (NLP) focuses on evaluating commonsense causal reasoning in LLMs, thus failing to assess whether a model can perform causal inference in accordance with a set of well-defined formal rules. To address this, we propose a new NLP task, causal inference in natural language, inspired by the\"causal inference engine\"postulated by Judea Pearl et al. We compose a large dataset, CLadder, with 10K samples: based on a collection of causal graphs and queries (associational, interventional, and counterfactual), we obtain symbolic questions and ground-truth answers, through an oracle causal inference engine. These are then translated into natural language. We evaluate multiple LLMs on our dataset, and we introduce and evaluate a bespoke chain-of-thought prompting strategy, CausalCoT. We show that our task is highly challenging for LLMs, and we conduct an in-depth analysis to gain deeper insights into the causal reasoning abilities of LLMs. Our data is open-sourced at https://huggingface.co/datasets/causalNLP/cladder, and our code can be found at https://github.com/causalNLP/cladder.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates whether large language models (LLMs) can coherently reason about causality, and proposes a new NLP task, causal inference in natural language, inspired by the \"causal inference engine\" proposed by Judea Pearl et al."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving factuality and consistency in language models by enhancing their causal reasoning abilities through a multi-step prompting approach called Causal Reasoning Prompting (CRP). The paper, on the other hand, focuses on assessing the causal reasoning abilities of language models by proposing a new NLP task called causal inference in natural language and creating a dataset called CLadder.\n\nWhile both the proposal and the paper deal with causal reasoning in language models, their primary objectives and approaches differ. The proposal aims to improve causal reasoning through a novel prompting technique, whereas the paper focuses on evaluating causal reasoning abilities using a new dataset and task.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "f30b720e34d405f200270a6ef2d09e98585fb4d1",
            "paperId": "f30b720e34d405f200270a6ef2d09e98585fb4d1",
            "title": "CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models",
            "abstract": "The ability to perform causal reasoning is widely considered a core feature of intelligence. In this work, we investigate whether large language models (LLMs) can coherently reason about causality. Much of the existing work in natural language processing (NLP) focuses on evaluating commonsense causal reasoning in LLMs, thus failing to assess whether a model can perform causal inference in accordance with a set of well-defined formal rules . To address this, we propose a new NLP task, causal inference in natural language , inspired by the \u201ccausal inference engine\u201d postulated by Judea Pearl et al. We compose a large dataset, CL ADDER , with 10K samples: based on a collection of causal graphs and queries (associational, interventional, and counterfactual), we obtain symbolic questions and ground-truth answers, through an oracle causal inference engine. These are then translated into natural language. We evaluate multiple LLMs on our dataset, and we introduce and evaluate a bespoke chain-of-thought prompting strategy, C AUSAL C O T. We show that our task is highly challenging for LLMs, and we conduct an in-depth analysis to gain deeper insight into the causal reasoning abilities of LLMs. 1",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates whether large language models (LLMs) can coherently reason about causality, and proposes a new NLP task, causal inference in natural language, inspired by the \u201ccausal inference engine\u201d postulated by Judea Pearl et al."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving factuality and consistency in language models by enhancing their causal reasoning capabilities. The proposed approach is Causal Reasoning Prompting (CRP), a multi-step prompting method that guides LLMs to reason about causal relationships in a given scenario.\n\nThe research problem in the paper is assessing the causal reasoning capabilities of language models. The proposed approach is creating a new NLP task called causal inference in natural language and a corresponding dataset, CLadder, to evaluate LLMs' performance on this task.\n\nWhile both the proposal and the paper focus on causal reasoning in language models, the proposal aims to improve LLMs' causal reasoning abilities through a novel prompting approach, while the paper focuses on assessing LLMs' existing causal reasoning capabilities using a new benchmark dataset and task.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "3a89e289e2dd29f5e52a2bf354a637762b661257",
            "paperId": "3a89e289e2dd29f5e52a2bf354a637762b661257",
            "title": "Fine-tuning Language Models for Factuality",
            "abstract": "The fluency and creativity of large pre-trained language models (LLMs) have led to their widespread use, sometimes even as a replacement for traditional search engines. Yet language models are prone to making convincing but factually inaccurate claims, often referred to as 'hallucinations.' These errors can inadvertently spread misinformation or harmfully perpetuate misconceptions. Further, manual fact-checking of model responses is a time-consuming process, making human factuality labels expensive to acquire. In this work, we fine-tune language models to be more factual, without human labeling and targeting more open-ended generation settings than past work. We leverage two key recent innovations in NLP to do so. First, several recent works have proposed methods for judging the factuality of open-ended text by measuring consistency with an external knowledge base or simply a large model's confidence scores. Second, the direct preference optimization algorithm enables straightforward fine-tuning of language models on objectives other than supervised imitation, using a preference ranking over possible model responses. We show that learning from automatically generated factuality preference rankings, generated either through existing retrieval systems or our novel retrieval-free approach, significantly improves the factuality (percent of generated claims that are correct) of Llama-2 on held-out topics compared with RLHF or decoding strategies targeted at factuality. At 7B scale, compared to Llama-2-chat, we observe 58% and 40% reduction in factual error rate when generating biographies and answering medical questions, respectively.",
            "year": 2023,
            "citationCount": 56,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that learning from automatically generated factuality preference rankings, generated either through existing retrieval systems or the novel retrieval-free approach, significantly improves the factuality of Llama-2 on held-out topics compared with RLHF or decoding strategies targeted at factuality."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving factuality and consistency in language models through causal reasoning prompting, while the paper focuses on fine-tuning language models for factuality using automatically generated factuality preference rankings.\n\nProposal: Improving causal reasoning in LLMs to generate more factual and consistent responses using a multi-step prompting approach called Causal Reasoning Prompting (CRP).\nPaper: Fine-tuning language models for factuality using automatically generated factuality preference rankings, without human labeling and targeting open-ended generation settings.\n\nAlthough both aim to improve factuality in language models, the proposal focuses on causal reasoning through prompting, while the paper uses fine-tuning with preference rankings. The approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "eed2a631d672a4130407f8d69a0ad9118a1e6e7d",
            "paperId": "eed2a631d672a4130407f8d69a0ad9118a1e6e7d",
            "title": "Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic",
            "abstract": "Recent advancements in large language models have showcased their remarkable generalizability across various domains. However, their reasoning abilities still have significant room for improvement, especially when confronted with scenarios requiring multi-step reasoning. Although large language models possess extensive knowledge, their reasoning often fails to effectively utilize this knowledge to establish a coherent thinking paradigm. These models sometimes show hallucinations as their reasoning procedures are unconstrained by logical principles. Aiming at improving the zero-shot chain-of-thought reasoning ability of large language models, we propose LoT (Logical Thoughts), a self-improvement prompting framework that leverages principles rooted in symbolic logic, particularly Reductio ad Absurdum, to systematically verify and rectify the reasoning processes step by step. Experimental evaluations conducted on language tasks in diverse domains, including arithmetic, commonsense, symbolic, causal inference, and social problems, demonstrate the efficacy of enhanced reasoning by logic. The implementation code for LoT can be accessed at: https://github.com/xf-zhao/LoT.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Aiming at improving the zero-shot chain-of-thought reasoning ability of large language models, LoT (Logical Thoughts), a self-improvement prompting framework that leverages principles rooted in symbolic logic, particularly Reductio ad Absurdum to systematically verify and rectify the reasoning processes step by step is proposed."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving factuality and consistency in language models through causal reasoning, while the approach is using a multi-step prompting method called Causal Reasoning Prompting (CRP).\n\nThe research problem in the paper is enhancing zero-shot chain-of-thought reasoning in large language models, and the approach is using a self-improvement prompting framework called LoT that leverages principles from symbolic logic.\n\nAlthough both the proposal and the paper aim to improve reasoning capabilities in language models, the specific focus and approaches differ. The proposal focuses on causal reasoning and uses a multi-step prompting method, while the paper focuses on general chain-of-thought reasoning and uses a logic-based prompting framework.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "c43a4a7b7ea4f4889de051321cb0073fd577f843",
            "paperId": "c43a4a7b7ea4f4889de051321cb0073fd577f843",
            "title": "Causal Reasoning of Entities and Events in Procedural Texts",
            "abstract": "Entities and events are crucial to natural language reasoning and common in procedural texts. Existing work has focused either exclusively on entity state tracking (e.g., whether a pan is hot) or on event reasoning (e.g., whether one would burn themselves by touching the pan), while these two tasks are often causally related. We propose CREPE, the first benchmark on causal reasoning of event plausibility and entity states. We show that most language models, including GPT-3, perform close to chance at .35 F1, lagging far behind human at .87 F1. We boost model performance to .59 F1 by creatively representing events as programming languages while prompting language models pretrained on code. By injecting the causal relations between entities and events as intermediate reasoning steps in our representation, we further boost the performance to .67 F1. Our findings indicate not only the challenge that CREPE brings for language models, but also the efficacy of code-like prompting combined with chain-of-thought prompting for multihop event reasoning.",
            "year": 2023,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes CREPE, the first benchmark on causal reasoning of event plausibility and entity states, and boosts model performance to .59 F1 by creatively representing events as programming languages while prompting language models pretrained on code."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving causal reasoning in large language models to generate more factual and consistent responses. The proposed approach is Causal Reasoning Prompting (CRP), a multi-step prompting method that guides LLMs to reason about causal relationships in a given scenario.\n\nThe research problem in the paper is causal reasoning of event plausibility and entity states in procedural texts. The proposed approach is representing events as programming languages and prompting language models pretrained on code, combined with injecting causal relations between entities and events as intermediate reasoning steps.\n\nWhile both the proposal and the paper focus on causal reasoning, the proposal targets improving causal reasoning in LLMs for general response generation, while the paper specifically focuses on causal reasoning of events and entities in procedural texts. The proposed methods are also different: the proposal uses a multi-step prompting approach, while the paper uses code-like representations and chain-of-thought prompting.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
            "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
            "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
            "year": 2022,
            "citationCount": 3517,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving factuality and consistency in language models through causal reasoning, while the approach is using a multi-step prompting method called Causal Reasoning Prompting (CRP).\n\nThe research problem in the paper is improving the complex reasoning abilities of large language models, and the approach is using chain of thought prompting, where intermediate reasoning steps are provided as exemplars.\n\nAlthough both works aim to improve certain aspects of language models, the specific problems they address (factuality/consistency vs. complex reasoning) and the approaches they propose (causal reasoning prompting vs. chain of thought prompting) are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "3cebb93c399db7e1434741338b0a24db19786b15",
            "paperId": "3cebb93c399db7e1434741338b0a24db19786b15",
            "title": "Prompt to be Consistent is Better than Self-Consistent? Few-Shot and Zero-Shot Fact Verification with Pre-trained Language Models",
            "abstract": "Few-shot or zero-shot fact verification only relies on a few or no labeled training examples. In this paper, we propose a novel method called ProToCo, to \\underline{Pro}mpt pre-trained language models (PLMs) \\underline{To} be \\underline{Co}nsistent, for improving the factuality assessment capability of PLMs in the few-shot and zero-shot settings. Given a claim-evidence pair, ProToCo generates multiple variants of the claim with different relations and frames a simple consistency mechanism as constraints for making compatible predictions across these variants. We update PLMs by using parameter-efficient fine-tuning (PEFT), leading to more accurate predictions in few-shot and zero-shot fact verification tasks. Our experiments on three public verification datasets show that ProToCo significantly outperforms state-of-the-art few-shot fact verification baselines. With a small number of unlabeled instances, ProToCo also outperforms the strong zero-shot learner T0 on zero-shot verification. Compared to large PLMs using in-context learning (ICL) method, ProToCo outperforms OPT-30B and the Self-Consistency-enabled OPT-6.7B model in both few- and zero-shot settings.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel method called ProToCo, to improve the factuality assessment capability of pre-trained language models (PLMs) by using parameter-efficient fine-tuning (PEFT), leading to more accurate predictions in few-shot and zero-shot fact verification tasks."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving causal reasoning in language models to generate more factual and consistent responses. The proposed approach is Causal Reasoning Prompting (CRP), a multi-step prompting method that guides LLMs to reason about causal relationships in a given scenario.\n\nThe research problem in the paper is improving the factuality assessment capability of pre-trained language models in few-shot and zero-shot fact verification. The proposed approach is ProToCo, which prompts PLMs to be consistent by generating multiple variants of the claim and framing a consistency mechanism as constraints for making compatible predictions across these variants.\n\nThe proposal focuses on causal reasoning and generating consistent responses, while the paper focuses on fact verification and making consistent predictions across claim variants. The methods, CRP and ProToCo, are different in their approach to improving consistency.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "e5c72b92c48d68594b290c84a8904da7c8335554",
            "paperId": "e5c72b92c48d68594b290c84a8904da7c8335554",
            "title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback",
            "abstract": "Large language models (LLMs), such as ChatGPT, are able to generate human-like, fluent responses for many downstream tasks, e.g., task-oriented dialog and question answering. However, applying LLMs to real-world, mission-critical applications remains challenging mainly due to their tendency to generate hallucinations and their inability to use external knowledge. This paper proposes a LLM-Augmenter system, which augments a black-box LLM with a set of plug-and-play modules. Our system makes the LLM generate responses grounded in external knowledge, e.g., stored in task-specific databases. It also iteratively revises LLM prompts to improve model responses using feedback generated by utility functions, e.g., the factuality score of a LLM-generated response. The effectiveness of LLM-Augmenter is empirically validated on two types of scenarios, task-oriented dialog and open-domain question answering. LLM-Augmenter significantly reduces ChatGPT's hallucinations without sacrificing the fluency and informativeness of its responses. We make the source code and models publicly available.",
            "year": 2023,
            "citationCount": 222,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A LLM-Augmenter system, which augments a black-box LLM with a set of plug-and-play modules to significantly reduces ChatGPT's hallucinations without sacrificing the fluency and informativeness of its responses."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve the factuality and consistency of large language models (LLMs) in causal reasoning tasks by using a multi-step prompting approach called Causal Reasoning Prompting (CRP). The paper focuses on augmenting LLMs with external knowledge and automated feedback to reduce hallucinations and improve the factuality of generated responses in task-oriented dialog and open-domain question answering.\n\nWhile both the project proposal and the paper aim to improve the factuality and consistency of LLMs, they differ in their specific focus and approach. The project proposal targets causal reasoning tasks and proposes a multi-step prompting method, while the paper focuses on incorporating external knowledge and automated feedback for task-oriented dialog and open-domain question answering.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "0d42221038c05cee8443c5b5af838505ee137dc3",
            "paperId": "0d42221038c05cee8443c5b5af838505ee137dc3",
            "title": "ART: Automatic multi-step reasoning and tool-use for large language models",
            "abstract": "Large language models (LLMs) can perform complex reasoning in few- and zero-shot settings by generating intermediate chain of thought (CoT) reasoning steps. Further, each reasoning step can rely on external tools to support computation beyond the core LLM capabilities (e.g. search/running code). Prior work on CoT prompting and tool use typically requires hand-crafting task-specific demonstrations and carefully scripted interleaving of model generations with tool use. We introduce Automatic Reasoning and Tool-use (ART), a framework that uses frozen LLMs to automatically generate intermediate reasoning steps as a program. Given a new task to solve, ART selects demonstrations of multi-step reasoning and tool use from a task library. At test time, ART seamlessly pauses generation whenever external tools are called, and integrates their output before resuming generation. ART achieves a substantial improvement over few-shot prompting and automatic CoT on unseen tasks in the BigBench and MMLU benchmarks, and matches performance of hand-crafted CoT prompts on a majority of these tasks. ART is also extensible, and makes it easy for humans to improve performance by correcting errors in task-specific programs or incorporating new tools, which we demonstrate by drastically improving performance on select tasks with minimal human intervention.",
            "year": 2023,
            "citationCount": 77,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Automatic Reasoning and Tool-use (ART), a framework that uses frozen LLMs to automatically generate intermediate reasoning steps as a program, achieves a substantial improvement over few-shot prompting and automatic CoT on unseen tasks in the BigBench and MMLU benchmarks, and matches performance of hand-crafted CoT prompts on a majority of these tasks."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving factuality and consistency in language models through causal reasoning, while the approach is a multi-step prompting method called Causal Reasoning Prompting (CRP).\n\nThe research problem in the paper is improving complex reasoning and tool use in language models, and the approach is an automatic framework called Automatic Reasoning and Tool-use (ART) that generates intermediate reasoning steps and integrates tool outputs.\n\nThe proposal focuses on causal reasoning and consistency, while the paper focuses on general complex reasoning and tool use. The methods, CRP and ART, also differ in their specific steps and implementation.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "a26fa1983e4bc7c5b55cd5a1296afe6f876baa03",
            "paperId": "a26fa1983e4bc7c5b55cd5a1296afe6f876baa03",
            "title": "Neuro-Symbolic Integration Brings Causal and Reliable Reasoning Proofs",
            "abstract": "Though prompting LLMs with various reasoning structures produces reasoning proofs along with answers, these proofs are not ensured to be causal and reliable due to the inherent defects of LLMs. Tracking such deficiencies, we present a neuro-symbolic integration method, in which a neural LLM is used to represent the knowledge of the problem while an LLM-free symbolic solver is adopted to do deliberative reasoning using the knowledge. Specifically, our customized meta-interpreters allow the production of reasoning proofs and support flexible search strategies. These reasoning proofs are ensured to be causal and reliable because of the deterministic executing nature of the symbolic solvers. Empirically, on ProofWriter, our method surpasses the CoT baseline by nearly double in accuracy and more than triple in proof similarity. On GSM8K, our method also shows accuracy improvements and nearly doubled proof similarity. Our code is released at https://github.com/DAMO-NLP-SG/CaRing",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A neuro-symbolic integration method is presented, in which a neural LLM is used to represent the knowledge of the problem while an LLM-free symbolic solver is adopted to do deliberative reasoning using the knowledge."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve causal reasoning in language models using a multi-step prompting approach called Causal Reasoning Prompting (CRP). The paper, on the other hand, proposes a neuro-symbolic integration method that combines a neural LLM for knowledge representation with an LLM-free symbolic solver for deliberative reasoning to ensure causal and reliable reasoning proofs.\n\nThe project focuses on developing a prompting strategy to guide LLMs in causal reasoning, while the paper integrates symbolic solvers with LLMs to achieve causal and reliable reasoning. Although both address the issue of causal reasoning in LLMs, their approaches differ significantly.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "498d1406fc4cddb05cd46477793f2e726a6fe238",
            "paperId": "498d1406fc4cddb05cd46477793f2e726a6fe238",
            "title": "The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code",
            "abstract": "Causal reasoning, the ability to identify cause-and-effect relationship, is crucial in human thinking. Although large language models (LLMs) succeed in many NLP tasks, it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning. Given the fact that programming code may express causal relations more often and explicitly with conditional statements like ``if``, we want to explore whether Code-LLMs acquire better causal reasoning abilities. Our experiments show that compared to text-only LLMs, Code-LLMs with code prompts are significantly better in causal reasoning. We further intervene on the prompts from different aspects, and discover that the programming structure is crucial in code prompt design, while Code-LLMs are robust towards format perturbations.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experiments show that compared to text-only LLMs, Code-LLMs with code prompts are significantly better in causal reasoning, and that the programming structure is crucial in code prompt design, while Code- LLMs are robust towards format perturbations."
            },
            "score": 6
        },
        {
            "id": "f0359e993b444e002479a3f2a7177c11e4e432ba",
            "paperId": "f0359e993b444e002479a3f2a7177c11e4e432ba",
            "title": "Causal Abstraction for Chain-of-Thought Reasoning in Arithmetic Word Problems",
            "abstract": "Recent work suggests that large language models (LLMs) achieve higher accuracy on multi-step reasoning tasks when prompted to generate intermediate reasoning steps, or a chain of thought (CoT), before their final answer. However, it is unclear how exactly CoTs improve LLMs\u2019 accuracy, and in particular, if LLMs use their CoTs to reason to their final answers. This paper tries to answer this question with respect to arithmetic word problems, by (i) evaluating the correctness of LLMs\u2019 CoTs, and (ii) using causal abstraction to assess if the intermediate tokens produced as part of a CoT causally impact LLMs\u2019 final answers, in line with the reasoning described by the CoT. We find that for CoT-prompted LLMs, correct answers to arithmetic problems are highly correlated with correct CoTs, and that when LLMs produce correct CoTs, they realize to a fairly large extent the causal models suggested by their CoTs. Higher degrees of realization also seem associated with better overall accuracy on the arithmetic problems. These findings suggest that some CoT-prompted LLMs may do better on multi-step arithmetic reasoning at least partly because they use their CoTs to reason to their final answers. However, for some LLMs, other internal processes may also be involved.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that for CoT-prompted LLMs, correct answers to arithmetic problems are highly correlated with correct CoTs, and that when LLMs produce correct CoT, they realize to a fairly large extent the causal models suggested by their CoTs."
            },
            "score": 6
        },
        {
            "id": "4cc6d310c0d5584f50836f1bd6bdbcac1c1c86a6",
            "paperId": "4cc6d310c0d5584f50836f1bd6bdbcac1c1c86a6",
            "title": "Towards Fine-grained Causal Reasoning and QA",
            "abstract": "Understanding causality is key to the success of NLP applications, especially in high-stakes domains. Causality comes in various perspectives such as enable and prevent that, despite their importance, have been largely ignored in the literature. This paper introduces a novel fine-grained causal reasoning dataset and presents a series of novel predictive tasks in NLP, such as causality detection, event causality extraction, and Causal QA. Our dataset contains human annotations of 25K cause-effect event pairs and 24K question-answering pairs within multi-sentence samples, where each can have multiple causal relationships. Through extensive experiments and analysis, we show that the complex relations in our dataset bring unique challenges to state-of-the-art methods across all three tasks and highlight potential research opportunities, especially in developing\"causal-thinking\"methods.",
            "year": 2022,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel fine-grained causal reasoning dataset is introduced and a series of novel predictive tasks in NLP, such as causality detection, event causality extraction, and Causal QA are presented."
            },
            "score": 6
        },
        {
            "id": "5a17155867261aff3773fd5f8600be05e1dd4356",
            "paperId": "5a17155867261aff3773fd5f8600be05e1dd4356",
            "title": "The Causal Reasoning Ability of Open Large Language Model: A Comprehensive and Exemplary Functional Testing",
            "abstract": "As the intelligent software, the development and application of large language models are extremely hot topics recently, bringing tremendous changes to general AI and software industry. Nonetheless, large language models, especially open source ones, incontrollably suffer from some potential software quality issues such as instability, inaccuracy, and insecurity, making software testing necessary. In this paper, we propose the first solution for functional testing of open large language models to check full-scene availability and conclude empirical principles for better steering large language models, particularly considering their black box and intelligence properties. Specifically, we focus on the model\u2019s causal reasoning ability, which is the core of artificial intelligence but almost ignored by most previous work. First, for comprehensive evaluation, we deconstruct the causal reasoning capability into five dimensions and summary the forms of causal reasoning task as causality identification and causality matching. Then, rich datasets are introduced and further modified to generate test cases along with different ability dimensions and task forms to improve the testing integrity. Moreover, we explore the ability boundary of open large language models in two usage modes: prompting and lightweight fine-tuning. Our work conducts comprehensive functional testing on the causal reasoning ability of open large language models, establishes benchmarks, and derives empirical insights for practical usage. The proposed testing solution can be transferred to other similar evaluation tasks as a general framework for large language models or their derivations.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes the first solution for functional testing of open large language models to check full-scene availability and concludes empirical principles for better steering large language models, particularly considering their black box and intelligence properties."
            },
            "score": 6
        },
        {
            "id": "71fd336f1ca337a638dffb236b432c29cdd19f3d",
            "paperId": "71fd336f1ca337a638dffb236b432c29cdd19f3d",
            "title": "Neuro-Symbolic Causal Language Planning with Commonsense Prompting",
            "abstract": "Language planning aims to implement complex high-level goals by decomposition into sequential simpler low-level steps. Such procedural reasoning ability is essential for applications such as household robots and virtual assistants. Although language planning is a basic skill set for humans in daily life, it remains a challenge for large language models (LLMs) that lack deep-level commonsense knowledge in the real world. Previous methods require either manual exemplars or annotated programs to acquire such ability from LLMs. In contrast, this paper proposes Neuro-Symbolic C ausal LA nguage P lanner (CLAP) that elicits procedural knowledge from the LLMs with commonsense-infused prompting. Pre-trained knowledge in LLMs is essentially an unobserved confounder that causes spurious correlations between tasks and action plans. Through the lens of a Structural Causal Model (SCM), we propose an effective strategy in CLAP to construct prompts as a causal intervention toward our SCM. Using graph sampling techniques and symbolic program executors, our strategy formalizes the structured causal prompts from commonsense knowledge bases. CLAP obtains state-of-the-art performance on WikiHow and RobotHow, achieving a relative improvement of 5 . 28% in human evaluations under the counterfactual setting. This indicates the superiority of CLAP in causal language planning semantically and sequentially.",
            "year": 2022,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes Neuro-Symbolic CLAP that elicits procedural knowledge from the LLMs with commonsense-infused prompting that indicates the superiority of CLAP in causal language planning semantically and sequentially."
            },
            "score": 5
        },
        {
            "id": "49e9352f4b87efdbf09a5f3048d11cf83016d37d",
            "paperId": "49e9352f4b87efdbf09a5f3048d11cf83016d37d",
            "title": "Chain of Logic: Rule-Based Reasoning with Large Language Models",
            "abstract": "Rule-based reasoning, a fundamental type of legal reasoning, enables us to draw conclusions by accurately applying a rule to a set of facts. We explore causal language models as rule-based reasoners, specifically with respect to compositional rules - rules consisting of multiple elements which form a complex logical expression. Reasoning about compositional rules is challenging because it requires multiple reasoning steps, and attending to the logical relationships between elements. We introduce a new prompting method, Chain of Logic, which elicits rule-based reasoning through decomposition (solving elements as independent threads of logic), and recomposition (recombining these sub-answers to resolve the underlying logical expression). This method was inspired by the IRAC (Issue, Rule, Application, Conclusion) framework, a sequential reasoning approach used by lawyers. We evaluate chain of logic across eight rule-based reasoning tasks involving three distinct compositional rules from the LegalBench benchmark and demonstrate it consistently outperforms other prompting methods, including chain of thought and self-ask, using open-source and commercial language models.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new prompting method is introduced, Chain of Logic, which elicits rule-based reasoning through decomposition, and recomposition, which elicits rule-based reasoning through decomposition (solving elements as independent threads of logic), and recomposition (recombining these sub-answers to resolve the underlying logical expression)."
            },
            "score": 5
        },
        {
            "id": "392d98aadc0ba0ac612ea48c2c60837b01fa849e",
            "paperId": "392d98aadc0ba0ac612ea48c2c60837b01fa849e",
            "title": "Large Language Model for Causal Decision Making",
            "abstract": "Large Language Models (LLMs) have shown their success in language understanding and reasoning on general topics. However, their capability to perform inference based on user-specified structured data and knowledge in corpus-rare concepts, such as causal decision-making is still limited. In this work, we explore the possibility of fine-tuning an open-sourced LLM into LLM4Causal, which can identify the causal task, execute a corresponding function, and interpret its numerical results based on users' queries and the provided dataset. Meanwhile, we propose a data generation process for more controllable GPT prompting and present two instruction-tuning datasets: (1) Causal-Retrieval-Bench for causal problem identification and input parameter extraction for causal function calling and (2) Causal-Interpret-Bench for in-context causal interpretation. By conducting end-to-end evaluations and two ablation studies, we showed that LLM4Causal can deliver end-to-end solutions for causal problems and provide easy-to-understand answers, which significantly outperforms the baselines.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that LLM4Causal can deliver end-to-end solutions for causal problems and provide easy-to-understand answers, which significantly outperforms the baselines."
            },
            "score": 5
        },
        {
            "id": "10632e0a667cbc3c52cc8f11a46d8e8e9c7739e3",
            "paperId": "10632e0a667cbc3c52cc8f11a46d8e8e9c7739e3",
            "title": "Causal Reasoning and Large Language Models: Opening a New Frontier for Causality",
            "abstract": "The causal capabilities of large language models (LLMs) is a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We further our understanding of LLMs and their causal implications, considering the distinctions between different types of causal reasoning tasks, as well as the entangled threats of construct and measurement validity. LLM-based methods establish new state-of-the-art accuracies on multiple causal benchmarks. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain), and actual causality (86% accuracy in determining necessary and sufficient causes in vignettes). At the same time, LLMs exhibit unpredictable failure modes and we provide some techniques to interpret their robustness. Crucially, LLMs perform these causal tasks while relying on sources of knowledge and methods distinct from and complementary to non-LLM based approaches. Specifically, LLMs bring capabilities so far understood to be restricted to humans, such as using collected knowledge to generate causal graphs or identifying background causal context from natural language. We envision LLMs to be used alongside existing causal methods, as a proxy for human domain knowledge and to reduce human effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods. We also see existing causal methods as promising tools for LLMs to formalize, validate, and communicate their reasoning especially in high-stakes scenarios. In capturing common sense and domain knowledge about causal mechanisms and supporting translation between natural language and formal methods, LLMs open new frontiers for advancing the research, practice, and adoption of causality.",
            "year": 2023,
            "citationCount": 96,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LLMs are envisioned to be used alongside existing causal methods, as a proxy for human domain knowledge and to reduce human effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods."
            },
            "score": 5
        },
        {
            "id": "62176de125738e3b95850d1227bac81fd646b78e",
            "paperId": "62176de125738e3b95850d1227bac81fd646b78e",
            "title": "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models",
            "abstract": "Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, Few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual efforts, Zero-shot-CoT concatenates the target problem statement with \u201cLet\u2019s think step by step\u201d as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.",
            "year": 2023,
            "citationCount": 115,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results over GPT-3 show that the proposed zero-shot prompting consistently outperforms Zero- shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem."
            },
            "score": 5
        },
        {
            "id": "261549439aebdda72b648ecc462448fd24857ac1",
            "paperId": "261549439aebdda72b648ecc462448fd24857ac1",
            "title": "Progressive-Hint Prompting Improves Reasoning in Large Language Models",
            "abstract": "The performance of Large Language Models (LLMs) in reasoning tasks depends heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency being critical methods that enhance this ability. However, these methods do not fully exploit the answers generated by the LLM to guide subsequent responses. This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers. PHP is orthogonal to CoT and self-consistency, making it easy to combine with state-of-the-art techniques to further improve performance. We conducted extensive and comprehensive experiments on seven benchmarks. The results show that PHP significantly improves accuracy while remaining highly efficient. For instance, with text-davinci-003, we observed a 4.2% improvement on GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction in sample paths with self-consistency. With GPT-4 and PHP, we achieve state-of-the-art performances on SVAMP (89.1% ->91.9%), GSM8K (92% ->95.5%), AQuA (76.4% ->79.9%) and MATH (50.3% ->53.9%).",
            "year": 2023,
            "citationCount": 64,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers."
            },
            "score": 5
        },
        {
            "id": "9efa81ec4954b0859c47dad8f42edfaf8bced69b",
            "paperId": "9efa81ec4954b0859c47dad8f42edfaf8bced69b",
            "title": "Boosting Language Models Reasoning with Chain-of-Knowledge Prompting",
            "abstract": "Recently, Chain-of-Thought (CoT) prompting has delivered success on complex reasoning tasks, which aims at designing a simple prompt like ``Let's think step by step'' or multiple in-context exemplars with well-designed rationales to elicit Large Language Models (LLMs) to generate intermediate reasoning steps. However, the generated rationales often come with mistakes, making unfactual and unfaithful reasoning chains. To mitigate this brittleness, we propose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple. This is inspired by our human behaviors, i.e., we can draw a mind map or knowledge map as the reasoning evidence in the brain before answering a complex question. Benefiting from CoK, we additionally introduce a F^2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness. For the unreliable response, the wrong evidence can be indicated to prompt the LLM to rethink. Extensive experiments demonstrate that our method can further improve the performance of commonsense, factual, symbolic, and arithmetic reasoning tasks.",
            "year": 2023,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel Chain-of-Knowledge prompting, where it aims at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple, and introduces a F^2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness."
            },
            "score": 5
        },
        {
            "id": "5437e8adab596d7294124c0e798708e050e25321",
            "paperId": "5437e8adab596d7294124c0e798708e050e25321",
            "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
            "abstract": "Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99% using just 14 exemplars, compared to only 16% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.",
            "year": 2022,
            "citationCount": 581,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts."
            },
            "score": 5
        },
        {
            "id": "8936192d69bb78afd09d83f3ceafd18dea32a8fd",
            "paperId": "8936192d69bb78afd09d83f3ceafd18dea32a8fd",
            "title": "Self-Consistent Decoding for More Factual Open Responses",
            "abstract": "Self-consistency has emerged as a powerful method for improving the accuracy of short answers generated by large language models. As previously defined, it only concerns the accuracy of a final answer parsed from generated text. In this work, we extend the idea to open response generation, by integrating voting into the decoding method. Each output sentence is selected from among multiple samples, conditioning on the previous selections, based on a simple token overlap score. We compare this\"Sample&Select\"method to greedy decoding, beam search, nucleus sampling, and the recently introduced hallucination avoiding decoders of DoLA, P-CRR, and S-CRR. We show that Sample&Select improves factuality by a 30% relative margin against these decoders in NLI-based evaluation on the subsets of CNN/DM and XSum used in the FRANK benchmark, while maintaining comparable ROUGE-1 F1 scores against reference summaries. We collect human verifications of the generated summaries, confirming the factual superiority of our method.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work extends the idea of self-consistency to open response generation, by integrating voting into the decoding method, and shows that Sample&Select improves factuality by a 30% relative margin against these decoders in NLI-based evaluation on the subsets of CNN/DM and XSum used in the FRANK benchmark."
            },
            "score": 5
        },
        {
            "id": "31bc64a991f8b739ac7e9824143d3c93bedf785f",
            "paperId": "31bc64a991f8b739ac7e9824143d3c93bedf785f",
            "title": "Benchmarking and Improving Generator-Validator Consistency of Language Models",
            "abstract": "As of September 2023, ChatGPT correctly answers\"what is 7+8\"with 15, but when asked\"7+8=15, True or False\"it responds with\"False\". This inconsistency between generating and validating an answer is prevalent in language models (LMs) and erodes trust. In this paper, we propose a framework for measuring the consistency between generation and validation (which we call generator-validator consistency, or GV-consistency), finding that even GPT-4, a state-of-the-art LM, is GV-consistent only 76% of the time. To improve the consistency of LMs, we propose to finetune on the filtered generator and validator responses that are GV-consistent, and call this approach consistency fine-tuning. We find that this approach improves GV-consistency of Alpaca-30B from 60% to 93%, and the improvement extrapolates to unseen tasks and domains (e.g., GV-consistency for positive style transfers extrapolates to unseen styles like humor). In addition to improving consistency, consistency fine-tuning improves both generator quality and validator accuracy without using any labeled data. Evaluated across 6 tasks, including math questions, knowledge-intensive QA, and instruction following, our method improves the generator quality by 16% and the validator accuracy by 6.3% across all tasks.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A framework for measuring the consistency between generation and validation is proposed, finding that even GPT-4, a state-of-the-art LM, is GV-consistent only 76% of the time, and consistency fine-tuning improves both generator quality and validator accuracy without using any labeled data."
            },
            "score": 5
        },
        {
            "id": "ed5020eeda1fbe8c29b1282d654b34abee22d90f",
            "paperId": "ed5020eeda1fbe8c29b1282d654b34abee22d90f",
            "title": "DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models",
            "abstract": "Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained LLMs that does not require conditioning on retrieved external knowledge nor additional fine-tuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an LLMs has generally been shown to be localized to particular transformer layers. We find that this Decoding by Contrasting Layers (DoLa) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA by 12-17% absolute points, demonstrating its potential in making LLMs reliably generate truthful facts.",
            "year": 2023,
            "citationCount": 35,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA by 12-17% absolute points, demonstrating its potential in making LLMs reliably generate truthful facts."
            },
            "score": 5
        },
        {
            "id": "c9e4af8b55db19f9bc9359806ca9283053366fe5",
            "paperId": "c9e4af8b55db19f9bc9359806ca9283053366fe5",
            "title": "Improving the Robustness of Large Language Models via Consistency Alignment",
            "abstract": "Large language models (LLMs) have shown tremendous success in following user instructions and generating helpful responses. Nevertheless, their robustness is still far from optimal, as they may generate significantly inconsistent responses due to minor changes in the verbalized instructions. Recent literature has explored this inconsistency issue, highlighting the importance of continued improvement in the robustness of response generation. However, systematic analysis and solutions are still lacking. In this paper, we quantitatively define the inconsistency problem and propose a two-stage training framework consisting of instruction-augmented supervised fine-tuning and consistency alignment training. The first stage helps a model generalize on following instructions via similar instruction augmentations. In the second stage, we improve the diversity and help the model understand which responses are more aligned with human expectations by differentiating subtle differences in similar responses. The training process is accomplished by self-rewards inferred from the trained model at the first stage without referring to external human preference resources. We conduct extensive experiments on recent publicly available LLMs on instruction-following tasks and demonstrate the effectiveness of our training framework.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper quantitatively defines the inconsistency problem and proposes a two-stage training framework consisting of instruction-augmented supervised fine-tuning and consistency alignment training that helps a model generalize on following instructions via similar instruction augmentations."
            },
            "score": 5
        },
        {
            "id": "be9447ccc05a0e8a07321272778c7574173cf00e",
            "paperId": "be9447ccc05a0e8a07321272778c7574173cf00e",
            "title": "Resprompt: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models",
            "abstract": "Chain-of-thought (CoT) prompting, which offers step-by-step problem-solving rationales, has impressively unlocked the reasoning potential of large language models (LLMs). Yet, the standard CoT is less effective in problems demanding multiple reasoning steps. This limitation arises from the complex reasoning process in multi-step problems: later stages often depend on the results of several steps earlier, not just the results of the immediately preceding step. Such complexities suggest the reasoning process is naturally represented as a graph. The almost linear and straightforward structure of CoT prompting, however, struggles to capture this complex reasoning graph. To address this challenge, we propose Residual Connection Prompting (RESPROMPT), a new prompting strategy that advances multi-step reasoning in LLMs. Our key idea is to reconstruct the reasoning graph within prompts. We achieve this by integrating necessary connections-links present in the reasoning graph but missing in the linear CoT flow-into the prompts. Termed\"residual connections\", these links are pivotal in morphing the linear CoT structure into a graph representation, effectively capturing the complex reasoning graphs inherent in multi-step problems. We evaluate RESPROMPT on six benchmarks across three diverse domains: math, sequential, and commonsense reasoning. For the open-sourced LLaMA family of models, RESPROMPT yields a significant average reasoning accuracy improvement of 12.5% on LLaMA-65B and 6.8% on LLaMA2-70B. Breakdown analysis further highlights RESPROMPT particularly excels in complex multi-step reasoning: for questions demanding at least five reasoning steps, RESPROMPT outperforms the best CoT based benchmarks by a remarkable average improvement of 21.1% on LLaMA-65B and 14.3% on LLaMA2-70B. Through extensive ablation studies and analyses, we pinpoint how to most effectively build residual connections.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes Residual Connection Prompting (RESPROMPT), a new prompting strategy that advances multi-step reasoning in LLMs by integrating necessary connections-links present in the reasoning graph but missing in the linear CoT flow-into the prompts."
            },
            "score": 5
        },
        {
            "id": "c88cafa3e980765a64febe369ceb7c2aa7261d2a",
            "paperId": "c88cafa3e980765a64febe369ceb7c2aa7261d2a",
            "title": "Complexity-Based Prompting for Multi-Step Reasoning",
            "abstract": "We study the task of prompting large-scale language models to perform multi-step reasoning. Existing work shows that when prompted with a chain of thoughts (CoT), sequences of short sentences describing intermediate reasoning steps towards a final answer, large language models can generate new reasoning chains and predict answers for new inputs. A central question is which reasoning examples make the most effective prompts. In this work, we propose complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning. We show that prompts with higher reasoning complexity, i.e., chains with more reasoning steps, achieve substantially better performance on multi-step reasoning tasks over strong baselines. We further extend our complexity-based criteria from prompting (selecting inputs) to decoding (selecting outputs), where we sample multiple reasoning chains from the model, then choose the majority of generated answers from complex reasoning chains (over simple chains). When used to prompt GPT-3 and Codex, our approach substantially improves multi-step reasoning accuracy and achieves new state-of-the-art (SOTA) performance on three math benchmarks (GSM8K, MultiArith, and MathQA) and two BigBenchHard tasks (Date Understanding and Penguins), with an average +5.3 and up to +18 accuracy improvements. Compared with existing example selection schemes like manual tuning or retrieval-based selection, selection based on reasoning complexity is intuitive, easy to implement, and annotation-efficient. Further results demonstrate the robustness of performance gains from complex prompts under format perturbation and distribution shift.",
            "year": 2022,
            "citationCount": 217,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes complexity-based prompting, a simple and effective example selection scheme for multi-step reasoning that substantially improves multi- step reasoning accuracy and achieves new state-of-the-art (SOTA) performance on three math benchmarks and two BigBenchHard tasks."
            },
            "score": 5
        },
        {
            "id": "0786c88990235414611478099e43611542d973b0",
            "paperId": "0786c88990235414611478099e43611542d973b0",
            "title": "Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models",
            "abstract": "We present Step-Back Prompting, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details. Using the concepts and principles to guide reasoning, LLMs significantly improve their abilities in following a correct reasoning path towards the solution. We conduct experiments of Step-Back Prompting with PaLM-2L, GPT-4 and Llama2-70B models, and observe substantial performance gains on various challenging reasoning-intensive tasks including STEM, Knowledge QA, and Multi-Hop Reasoning. For instance, Step-Back Prompting improves PaLM-2L performance on MMLU (Physics and Chemistry) by 7% and 11% respectively, TimeQA by 27%, and MuSiQue by 7%.",
            "year": 2023,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Step-Back Prompting is presented, a simple prompting technique that enables LLMs to do abstractions to derive high-level concepts and first principles from instances containing specific details and significantly improves LLMs' abilities in following a correct reasoning path towards the solution."
            },
            "score": 5
        },
        {
            "id": "9a9b1e2968302eb882870537d4af6e2c722dfd1a",
            "paperId": "9a9b1e2968302eb882870537d4af6e2c722dfd1a",
            "title": "Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement",
            "abstract": "To enhance the multi-step reasoning capabilities of large language models, researchers have extensively explored prompting methods, notably the Chain-of-Thought (CoT) method which explicitly elicits human-like rationales. However, they have inadvertently overlooked the potential of enhancing model reasoning performance by formulating higher-quality problems. In this work, we start from the problem side and propose Self-Polish (SP), a novel method that facilitates the model's reasoning by guiding it to progressively refine the given problems to be more comprehensible and solvable. We also explore several automatic prompting varients and propose the Self-Polish prompt bank for the community. SP is orthogonal to all other prompting methods of answer/reasoning side like CoT, allowing for seamless integration with state-of-the-art techniques for further improvement. Thorough experiments show that the proposed method attains notable and consistent effectiveness on five reasoning benchmarks across different models. Furthermore, our method also showcases impressive performance on robustness evaluation. Codes and prompts are available at https://github.com/WooooDyy/Self-Polish.",
            "year": 2023,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Self-Polish (SP) is proposed, a novel method that facilitates the model's reasoning by guiding it to progressively refine the given problems to be more comprehensible and solvable and seamless integration with state-of-the-art techniques for further improvement."
            },
            "score": 5
        },
        {
            "id": "f9b94bcaf706892904c058c37d0087628f371df4",
            "paperId": "f9b94bcaf706892904c058c37d0087628f371df4",
            "title": "e-CARE: a New Dataset for Exploring Explainable Causal Reasoning",
            "abstract": "Understanding causality has vital importance for various Natural Language Processing (NLP) applications. Beyond the labeled instances, conceptual explanations of the causality can provide deep understanding of the causal fact to facilitate the causal reasoning process. However, such explanation information still remains absent in existing causal reasoning resources. In this paper, we fill this gap by presenting a human-annotated explainable CAusal REasoning dataset (e-CARE), which contains over 20K causal reasoning questions, together with natural language formed explanations of the causal questions. Experimental results show that generating valid explanations for causal facts still remains especially challenging for the state-of-the-art models, and the explanation information can be helpful for promoting the accuracy and stability of causal reasoning models.",
            "year": 2022,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results show that generating valid explanations for causal facts still remains especially challenging for the state-of-the-art models, and the explanation information can be helpful for promoting the accuracy and stability of causal reasoning models."
            },
            "score": 5
        },
        {
            "id": "0457c6c4f3512beb0237e62292d905e962b391a9",
            "paperId": "0457c6c4f3512beb0237e62292d905e962b391a9",
            "title": "Learning a Structural Causal Model for Intuition Reasoning in Conversation",
            "abstract": "Reasoning, a crucial aspect of NLP research, has not been adequately addressed by prevailing models including Large Language Model. Conversation reasoning, as a critical component of it, remains largely unexplored due to the absence of a well-designed cognitive model. In this paper, inspired by intuition theory on conversation cognition, we develop a conversation cognitive model (CCM) that explains how each utterance receives and activates channels of information recursively. Besides, we algebraically transformed CCM into a structural causal model (SCM) under some mild assumptions, rendering it compatible with various causal discovery methods. We further propose a probabilistic implementation of the SCM for utterance-level relation reasoning. By leveraging variational inference, it explores substitutes for implicit causes, addresses the issue of their unobservability, and reconstructs the causal representations of utterances through the evidence lower bounds. Moreover, we constructed synthetic and simulated datasets incorporating implicit causes and complete cause labels, alleviating the current situation where all available datasets are implicit-causes-agnostic. Extensive experiments demonstrate that our proposed method significantly outperforms existing methods on synthetic, simulated, and real-world datasets. Finally, we analyze the performance of CCM under latent confounders and propose theoretical ideas for addressing this currently unresolved issue.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A conversation cognitive model (CCM) is developed that explains how each utterance receives and activates channels of information recursively and is transformed into a structural causal model (SCM) under some mild assumptions, rendering it compatible with various causal discovery methods."
            },
            "score": 5
        },
        {
            "id": "5dc15ac1c92ab7492f121471823fb13a95d273ba",
            "paperId": "5dc15ac1c92ab7492f121471823fb13a95d273ba",
            "title": "A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis",
            "abstract": "Mathematical reasoning in large language models (LMs) has garnered significant attention in recent work, but there is a limited understanding of how these models process and store information related to arithmetic tasks within their architecture. In order to improve our understanding of this aspect of language models, we present a mechanistic interpretation of Transformer-based LMs on arithmetic questions using a causal mediation analysis framework. By intervening on the activations of specific model components and measuring the resulting changes in predicted probabilities, we identify the subset of parameters responsible for specific predictions. This provides insights into how information related to arithmetic is processed by LMs. Our experimental results indicate that LMs process the input by transmitting the information relevant to the query from mid-sequence early layers to the final token using the attention mechanism. Then, this information is processed by a set of MLP modules, which generate result-related information that is incorporated into the residual stream. To assess the specificity of the observed activation dynamics, we compare the effects of different model components on arithmetic queries with other tasks, including number retrieval from prompts and factual knowledge questions.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A mechanistic interpretation of Transformer-based LMs on arithmetic questions using a causal mediation analysis framework is presented and results indicate that LMs process the input by transmitting the information relevant to the query from mid-sequence early layers to the final token using the attention mechanism."
            },
            "score": 5
        },
        {
            "id": "04e838c16f3d1fb8d69d34fe0a0a92c59717875b",
            "paperId": "04e838c16f3d1fb8d69d34fe0a0a92c59717875b",
            "title": "EchoPrompt: Instructing the Model to Rephrase Queries for Improved In-context Learning",
            "abstract": "Language models are achieving impressive performance on various tasks by aggressively adopting inference-time prompting techniques, such as zero-shot and few-shot prompting. In this work, we introduce EchoPrompt, a simple yet effective approach that prompts the model to rephrase its queries before answering them. EchoPrompt is adapted for both zero-shot and few-shot in-context learning with standard and chain-of-thought prompting. Experimental results show that EchoPrompt yields substantial improvements across all these settings for four families of causal language models. These improvements are observed across various numerical reasoning (e.g. GSM8K, SVAMP), reading comprehension (e.g. DROP), and logical reasoning (e.g. Coin Flipping) tasks. On average, EchoPrompt improves the Zero-shot-CoT performance of code-davinci-002 by 5% in numerical tasks and 13% in reading comprehension tasks. We investigate the factors contributing to EchoPrompt's effectiveness through ablation studies, which reveal that both the original query and the model-generated rephrased version are instrumental in its performance gains. Our empirical results indicate that EchoPrompt is an effective technique that enhances in-context learning performance. We recommend incorporating EchoPrompt into various baseline prompting strategies to achieve performance boosts.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "EchoPrompt is introduced, a simple yet effective approach that prompts the model to rephrase its queries before answering them that enhances in-context learning performance and is recommended for incorporating into various baseline prompting strategies to achieve performance boosts."
            },
            "score": 4
        },
        {
            "id": "089d3cde041a78bd9128feed10b4e1ff7c28398f",
            "paperId": "089d3cde041a78bd9128feed10b4e1ff7c28398f",
            "title": "Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing Framework",
            "abstract": "Large language models (LLMs) can easily generate biased and discriminative responses. As LLMs tap into consequential decision-making (e.g., hiring and healthcare), it is of crucial importance to develop strategies to mitigate these biases. This paper focuses on social bias, tackling the association between demographic information and LLM outputs. We propose a causality-guided debiasing framework that utilizes causal understandings of (1) the data-generating process of the training corpus fed to LLMs, and (2) the internal reasoning process of LLM inference, to guide the design of prompts for debiasing LLM outputs through selection mechanisms. Our framework unifies existing de-biasing prompting approaches such as inhibitive instructions and in-context contrastive examples, and sheds light on new ways of debiasing by encouraging bias-free reasoning. Our strong empirical performance on real-world datasets demonstrates that our framework provides principled guidelines on debiasing LLM outputs even with only the black-box access.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A causality-guided debiasing framework that utilizes causal understandings of the data-generating process of the training corpus fed to LLMs, and the internal reasoning process of LLM inference, to guide the design of prompts for debiasing LLM outputs through selection mechanisms is proposed."
            },
            "score": 4
        },
        {
            "id": "f8b8f926bbfa327c86c40796131fe2695db81126",
            "paperId": "f8b8f926bbfa327c86c40796131fe2695db81126",
            "title": "DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models",
            "abstract": "A long-standing goal of AI systems is to perform complex multimodal reasoning like humans. Recently, large language models (LLMs) have made remarkable strides in such multi-step reasoning on the language modality solely by leveraging the chain of thought (CoT) to mimic human thinking. However, the transfer of these advancements to multimodal contexts introduces heightened challenges, including but not limited to the impractical need for labor-intensive annotation and the limitations in terms of flexibility, generalizability, and explainability. To evoke CoT reasoning in multimodality, this work first conducts an in-depth analysis of these challenges posed by multimodality and presents two key insights:\"keeping critical thinking\"and\"letting everyone do their jobs\"in multimodal CoT reasoning. Furthermore, this study proposes a novel DDCoT prompting that maintains a critical attitude through negative-space prompting and incorporates multimodality into reasoning by first dividing the reasoning responsibility of LLMs into reasoning and recognition and then integrating the visual recognition capability of visual models into the joint reasoning process. The rationales generated by DDCoT not only improve the reasoning abilities of both large and small language models in zero-shot prompting and fine-tuning learning, significantly outperforming state-of-the-art methods but also exhibit impressive generalizability and explainability.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study proposes a novel DDCoT prompting that maintains a critical attitude through negative-space prompting and incorporates multimodality into reasoning by first dividing the reasoning responsibility of LLMs into reasoning and recognition and then integrating the visual recognition capability of visual models into the joint reasoning process."
            },
            "score": 4
        },
        {
            "id": "6ad26eb2d2aa6679d16d9c16fb75cd2cbe1127bc",
            "paperId": "6ad26eb2d2aa6679d16d9c16fb75cd2cbe1127bc",
            "title": "See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning",
            "abstract": "Large pre-trained vision and language models have demonstrated remarkable capacities for various tasks. However, solving the knowledge-based visual reasoning tasks remains challenging, which requires a model to comprehensively understand image content, connect the external world knowledge, and perform step-by-step reasoning to answer the questions correctly. To this end, we propose a novel framework named Interactive Prompting Visual Reasoner (IPVR) for few-shot knowledge-based visual reasoning. IPVR contains three stages, see, think and confirm. The see stage scans the image and grounds the visual concept candidates with a visual perception model. The think stage adopts a pre-trained large language model (LLM) to attend to the key concepts from candidates adaptively. It then transforms them into text context for prompting with a visual captioning model and adopts the LLM to generate the answer. The confirm stage further uses the LLM to generate the supporting rationale to the answer, verify the generated rationale with a cross-modality classifier and ensure that the rationale can infer the predicted output consistently. We conduct experiments on a range of knowledge-based visual reasoning datasets. We found our IPVR enjoys several benefits, 1). it achieves better performance than the previous few-shot learning baselines; 2). it enjoys the total transparency and trustworthiness of the whole reasoning process by providing rationales for each reasoning step; 3). it is computation-efficient compared with other fine-tuning baselines.",
            "year": 2023,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel framework named Interactive Prompting Visual Reasoner (IPVR), which achieves better performance than the previous few-shot learning baselines, and enjoys the total transparency and trustworthiness of the whole reasoning process by providing rationales for each reasoning step."
            },
            "score": 4
        },
        {
            "id": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
            "paperId": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
            "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such\"society of minds\"approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.",
            "year": 2023,
            "citationCount": 206,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer is presented, indicating that this approach significantly enhances mathematical and strategic reasoning across a number of tasks."
            },
            "score": 4
        },
        {
            "id": "a072dc70413c02e6bcf80769eade6bb0bc4c1f98",
            "paperId": "a072dc70413c02e6bcf80769eade6bb0bc4c1f98",
            "title": "Improving Consistency for Text Summarization with Energy Functions",
            "abstract": "Current abstractive summarization models often generate inconsistent content, i.e. texts that are not directly inferable from the source document, are not consistent with respect to world knowledge, or are self-contradictory. These inconsistencies motivate a new consistency taxonomy that we define as faithfulness, factuality, and self-supportiveness. However, most recent work on reducing inconsistency in document summarization only focuses on faithfulness detection and correction while ignoring other inconsistency phenomena, which limits the model\u2019s scalability. To improve the general consistency we introduce EnergySum, where we apply the Residual Energy-based Model by designing energy scorers that reflect each type of consistency. These energy scores are utilized in candidate re-ranking during the sampling process. Experiments on XSUM and CNN/DM datasets show that EnergySum mitigates the trade-off between accuracy and consistency.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "To improve the general consistency of abstractive summarization models, EnergySum is introduced, where the Residual Energy-based Model is applied by designing energy scorers that reflect each type of consistency."
            },
            "score": 4
        },
        {
            "id": "a5246f262bc0919bbfbc4b0fa66e85be41d92712",
            "paperId": "a5246f262bc0919bbfbc4b0fa66e85be41d92712",
            "title": "Improving Language Models Meaning Understanding and Consistency by Learning Conceptual Roles from Dictionary",
            "abstract": "The non-humanlike behaviour of contemporary pre-trained language models (PLMs) is a leading cause undermining their trustworthiness. A striking phenomenon of such faulty behaviours is the generation of inconsistent predictions, which produces logically contradictory results, such as generating different predictions for texts delivering the same meaning or violating logical properties. Previous studies exploited data augmentation or implemented specialised loss functions to alleviate the issue. However, their usage is limited, because they consume expensive training resources for large-sized PLMs and can only handle a certain consistency type. To this end, we propose a practical approach that alleviates the inconsistent behaviour issue by fundamentally improving PLMs' meaning awareness. Based on the conceptual role theory, our method allows PLMs to capture accurate meaning by learning precise interrelationships between concepts from word-definition pairs in a dictionary. Next, we propose an efficient parameter integration technique that updates only a few additional parameters to combine the learned interrelationship with PLMs' pre-trained knowledge. Our experimental results reveal that the approach can concurrently improve multiple types of consistency, enables efficient knowledge integration, and easily applies to other languages.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The method allows PLMs to capture accurate meaning by learning precise interrelationships between concepts from word-definition pairs in a dictionary, and proposes an efficient parameter integration technique that updates only a few additional parameters to combine the learned interrelationship with PLMs' pre-trained knowledge."
            },
            "score": 4
        },
        {
            "id": "4115a24474ef5f184f5cbae3f43aca4d3bb07bea",
            "paperId": "4115a24474ef5f184f5cbae3f43aca4d3bb07bea",
            "title": "Improving Language Models via Plug-and-Play Retrieval Feedback",
            "abstract": "Large language models (LLMs) exhibit remarkable performance across various NLP tasks. However, they often generate incorrect or hallucinated information, which hinders their practical applicability in real-world scenarios. Human feedback has been shown to effectively enhance the factuality and quality of generated content, addressing some of these limitations. However, this approach is resource-intensive, involving manual input and supervision, which can be time-consuming and expensive. Moreover, it cannot be provided during inference, further limiting its practical utility in dynamic and interactive applications. In this paper, we introduce ReFeed, a novel pipeline designed to enhance LLMs by providing automatic retrieval feedback in a plug-and-play framework without the need for expensive fine-tuning. ReFeed first generates initial outputs, then utilizes a retrieval model to acquire relevant information from large document collections, and finally incorporates the retrieved information into the in-context demonstration for output refinement, thereby addressing the limitations of LLMs in a more efficient and cost-effective manner. Experiments on four knowledge-intensive benchmark datasets demonstrate our proposed ReFeed could improve over +6.0% under zero-shot setting and +2.5% under few-shot setting, compared to baselines without using retrieval feedback.",
            "year": 2023,
            "citationCount": 34,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces ReFeed, a novel pipeline designed to enhance LLMs by providing automatic retrieval feedback in a plug-and-play framework without the need for expensive fine-tuning, thereby addressing the limitations of LLMs in a more efficient and cost-effective manner."
            },
            "score": 4
        },
        {
            "id": "73b6de24eb0e5f6ff4f9c3bdd9257f4554faca19",
            "paperId": "73b6de24eb0e5f6ff4f9c3bdd9257f4554faca19",
            "title": "Measuring and Improving Consistency in Pretrained Language Models",
            "abstract": "Abstract Consistency of a model\u2014that is, the invariance of its behavior under meaning-preserving alternations in its input\u2014is a highly desirable property in natural language processing. In this paper we study the question: Are Pretrained Language Models (PLMs) consistent with respect to factual knowledge? To this end, we create ParaRel\ud83e\udd18, a high-quality resource of cloze-style query English paraphrases. It contains a total of 328 paraphrases for 38 relations. Using ParaRel\ud83e\udd18, we show that the consistency of all PLMs we experiment with is poor\u2014 though with high variance between relations. Our analysis of the representational spaces of PLMs suggests that they have a poor structure and are currently not suitable for representing knowledge robustly. Finally, we propose a method for improving model consistency and experimentally demonstrate its effectiveness.1",
            "year": 2021,
            "citationCount": 226,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The creation of PARAREL, a high-quality resource of cloze-style query English paraphrases, and analysis of the representational spaces of PLMs suggest that they have a poor structure and are currently not suitable for representing knowledge in a robust way."
            },
            "score": 4
        },
        {
            "id": "99ad11764421bfff6261c9df8526c8b5af82d794",
            "paperId": "99ad11764421bfff6261c9df8526c8b5af82d794",
            "title": "Improving Logical Consistency in Pre-Trained Language Models using Natural Language Inference",
            "abstract": "Current state-of-the-art pre-trained language models (PTLMs) contain rich and vast amounts of world knowledge, demonstrating an ability to extrapolate information from contextual texts and to accurately answer questions [1]. However, the latent factual understanding captured by PTLMs can be irrational and inconsistent, causing PTLMs to be prone to generating contradictory statements [2]. We demonstrate that natural language inference (NLI) can provide additional signal about contradictory statements output by a PTLM. We explore several approaches for aggregating the entailment and contradiction probabilities acquired through NLI on a batch of PTLM predicted answers and define a scoring heuristic that balances between the NLI output and the PTLM\u2019s confidence in its answers. Predictions whose scores are below a tuned threshold are revised before outputting final answers. In addition, we investigate methods for using these NLI probabilities to define a MaxSAT problem that, when optimized, yields corrected predictions. Our results demonstrate that a system that uses either of our approaches to revise PTLM answers has better accuracy and logical consistency than a vanilla PTLM.",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that natural language inference (NLI) can provide additional signal about contradictory statements output by a PTLM and that a system that uses either of these approaches to revise PTLM answers has better accuracy and logical consistency than a vanilla PTLM."
            },
            "score": 4
        },
        {
            "id": "12f881588e6b461b86b34209c30a2bdfe67f904d",
            "paperId": "12f881588e6b461b86b34209c30a2bdfe67f904d",
            "title": "Improving Factuality of Abstractive Summarization without Sacrificing Summary Quality",
            "abstract": "Improving factual consistency of abstractive summarization has been a widely studied topic. However, most of the prior works on training factuality-aware models have ignored the negative effect it has on summary quality. We propose {pasted macro \u2018MODEL\u2019}name (i.e. Effective Factual Summarization), a candidate summary generation and ranking technique to improve summary factuality without sacrificing quality. We show that using a contrastive learning framework with our refined candidate summaries leads to significant gains on both factuality and similarity-based metrics. Specifically, we propose a ranking strategy in which we effectively combine two metrics, thereby preventing any conflict during training. Models trained using our approach show up to 6 points of absolute improvement over the base model with respect to FactCC on XSUM and 11 points on CNN/DM, without negatively affecting either similarity-based metrics or absractiveness.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that using a contrastive learning framework with the refined candidate summaries leads to significant gains on both factuality and similarity-based metrics, and a ranking strategy in which it effectively combine two metrics, thereby preventing any conflict during training."
            },
            "score": 4
        },
        {
            "id": "fbd49b25bdab98c171af49962a41139c73dacbde",
            "paperId": "fbd49b25bdab98c171af49962a41139c73dacbde",
            "title": "Specializing Smaller Language Models towards Multi-Step Reasoning",
            "abstract": "The surprising ability of Large Language Models (LLMs) to perform well on complex reasoning with only few-shot chain-of-thought prompts is believed to emerge only in very large-scale models (100+ billion parameters). We show that such abilities can, in fact, be distilled down from GPT-3.5 ($\\ge$ 175B) to T5 variants ($\\le$ 11B). We propose model specialization, to specialize the model's ability towards a target task. The hypothesis is that large models (commonly viewed as larger than 100B) have strong modeling power, but are spread on a large spectrum of tasks. Small models (commonly viewed as smaller than 10B) have limited model capacity, but if we concentrate their capacity on a specific target task, the model can achieve a decent improved performance. We use multi-step math reasoning as our testbed because it is a very typical emergent ability. We show two important aspects of model abilities: (1). there exists a very complex balance/ tradeoff between language models' multi-dimensional abilities; (2). by paying the price of decreased generic ability, we can clearly lift up the scaling curve of models smaller than 10B towards a specialized multi-step math reasoning ability. We further give comprehensive discussions about important design choices for better generalization, including the tuning data format, the start model checkpoint, and a new model selection method. We hope our practice and discoveries can serve as an important attempt towards specialized smaller models in the new research paradigm set by LLMs.",
            "year": 2023,
            "citationCount": 110,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work shows two important aspects of model abilities: there exists a very complex balance/ tradeoff between language models' multi-dimensional abilities, and by paying the price of decreased generic ability, it can clearly lift up the scaling curve of models smaller than 10B towards a specialized multi-step math reasoning ability."
            },
            "score": 4
        },
        {
            "id": "70ca99ac3c21f353b3db948004510a09fdebc4f2",
            "paperId": "70ca99ac3c21f353b3db948004510a09fdebc4f2",
            "title": "Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models",
            "abstract": "Recent work has shown that language models (LMs) have strong multi-step (i.e., procedural) reasoning capabilities. However, it is unclear whether LMs perform these tasks by cheating with answers memorized from pretraining corpus, or, via a multi-step reasoning mechanism. In this paper, we try to answer this question by exploring a mechanistic interpretation of LMs for multi-step reasoning tasks. Concretely, we hypothesize that the LM implicitly embeds a reasoning tree resembling the correct reasoning process within it. We test this hypothesis by introducing a new probing approach (called MechanisticProbe) that recovers the reasoning tree from the model's attention patterns. We use our probe to analyze two LMs: GPT-2 on a synthetic task (k-th smallest element), and LLaMA on two simple language-based reasoning tasks (ProofWriter&AI2 Reasoning Challenge). We show that MechanisticProbe is able to detect the information of the reasoning tree from the model's attentions for most examples, suggesting that the LM indeed is going through a process of multi-step reasoning within its architecture in many cases.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that MechanisticProbe is able to detect the information of the reasoning tree from the model's attentions for most examples, suggesting that the LM indeed is going through a process of multi-step reasoning within its architecture in many cases."
            },
            "score": 4
        },
        {
            "id": "72123a86eae2cb5c4eae8650f43524039d48875d",
            "paperId": "72123a86eae2cb5c4eae8650f43524039d48875d",
            "title": "Distilling Multi-Step Reasoning Capabilities of Large Language Models into Smaller Models via Semantic Decompositions",
            "abstract": "Step-by-step reasoning approaches like chain-of-thought ( CoT ) have proved to be a very effective technique to induce reasoning capabilities in large language models. However, the success of the CoT approach depends primarily on model size, and often billion parameter-scale models are needed to get CoT to work. In this paper, we propose a knowledge distillation approach, that leverages the step-by-step CoT reasoning capabilities of larger models and distils these reasoning abilities into smaller models. Our approach D ECOMPOSI - TIONAL D ISTILLATION learns a semantic decomposition of the original problem into a sequence of subproblems and uses it to train two models: a) a problem decomposer that learns to decompose the complex reasoning problem into a sequence of simpler sub-problems and b) a problem solver that uses the intermediate subproblems to solve the overall problem. On a multi-step math word problem dataset (GSM8K), we boost the performance of GPT-2 variants up to 35% when distilled with our approach compared to CoT . We show that using our approach, it is possible to train a GPT-2-large model (775M) that can outperform a 10X larger GPT-3 (6B) model trained using CoT reasoning. Finally, we also demonstrate that our approach of problem decomposition can also be used as an alternative to CoT prompting, which boosts the GPT-3 performance by 40% compared to CoT prompts.",
            "year": 2022,
            "citationCount": 41,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A knowledge distillation approach, that leverages the step-by-step CoT reasoning capabilities of larger models and distils these reasoning abilities into smaller models and boosts the performance of GPT-2 variants up to 35% when distilled with this approach compared to CoT."
            },
            "score": 4
        },
        {
            "id": "a8ee189a6b86efeba4d0565dca10b9fc8be86835",
            "paperId": "a8ee189a6b86efeba4d0565dca10b9fc8be86835",
            "title": "Graph-Guided Reasoning for Multi-Hop Question Answering in Large Language Models",
            "abstract": "Chain-of-Thought (CoT) prompting has boosted the multi-step reasoning capabilities of Large Language Models (LLMs) by generating a series of rationales before the final answer. We analyze the reasoning paths generated by CoT and find two issues in multi-step reasoning: (i) Generating rationales irrelevant to the question, (ii) Unable to compose subquestions or queries for generating/retrieving all the relevant information. To address them, we propose a graph-guided CoT prompting method, which guides the LLMs to reach the correct answer with graph representation/verification steps. Specifically, we first leverage LLMs to construct a\"question/rationale graph\"by using knowledge extraction prompting given the initial question and the rationales generated in the previous steps. Then, the graph verification step diagnoses the current rationale triplet by comparing it with the existing question/rationale graph to filter out irrelevant rationales and generate follow-up questions to obtain relevant information. Additionally, we generate CoT paths that exclude the extracted graph information to represent the context information missed from the graph extraction. Our graph-guided reasoning method shows superior performance compared to previous CoT prompting and the variants on multi-hop question answering benchmark datasets.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a graph-guided CoT prompting method, which guides the LLMs to reach the correct answer with graph representation/verification steps and shows superior performance compared to previous coT prompting and the variants on multi-hop question answering benchmark datasets."
            },
            "score": 4
        },
        {
            "id": "ba4aa83248a1d08b521392eb971e47d10b7c74e1",
            "paperId": "ba4aa83248a1d08b521392eb971e47d10b7c74e1",
            "title": "Boosting Logical Reasoning in Large Language Models through a New Framework: The Graph of Thought",
            "abstract": "Recent advancements in large-scale models, such as GPT-4, have showcased remarkable capabilities in addressing standard queries. However, when facing complex problems that require multi-step logical reasoning, their accuracy dramatically decreases. Current research has explored the realm of \\textit{prompting engineering} to bolster the inferential capacities of these models. Our paper unveils a pioneering prompting technique, dubbed \\textit{Graph of Thoughts (GoT)}. Through testing on a trio of escalating challenges: the 24-point game, resolution of high-degree polynomial equations, and derivation of formulas for recursive sequences, our method outperformed GPT-4, achieving accuracy improvements of $89.7\\%$, $86\\%$, and $56\\%$ for each respective task. Moreover, when juxtaposed with the state-of-the-art (SOTA) prompting method, \\textit{Tree of Thought (ToT)}, our approach registered an average accuracy boost of $23\\%$, $24\\%$, and $15\\%$.",
            "year": 2023,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper unveils a pioneering prompting technique, dubbed \\textit{Graph of Thoughts (GoT), which outperformed GPT-4 and when juxtaposed with the state-of-the-art (SOTA) prompting method, the Tree of Thought (ToT), registered an average accuracy boost."
            },
            "score": 4
        },
        {
            "id": "9ffefdf1fcd780cb71450b0a7a29247c66aa87be",
            "paperId": "9ffefdf1fcd780cb71450b0a7a29247c66aa87be",
            "title": "The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning",
            "abstract": "Does prompting a large language model (LLM) like GPT-3 with explanations improve in-context learning? We study this question on two NLP tasks that involve reasoning over text, namely question answering and natural language inference. We test the performance of four LLMs on three textual reasoning datasets using prompts that include explanations in multiple different styles. For these tasks, we find that including explanations in the prompts for OPT, GPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to moderate accuracy improvements over standard few-show learning. However, text-davinci-002 is able to benefit more substantially. We further show that explanations generated by the LLMs may not entail the models' predictions nor be factually grounded in the input, even on simple tasks with extractive explanations. However, these flawed explanations can still be useful as a way to verify LLMs' predictions post-hoc. Through analysis in our three settings, we show that explanations judged by humans to be good--logically consistent with the input and the prediction--more likely cooccur with accurate predictions. Following these observations, we train calibrators using automatically extracted scores that assess the reliability of explanations, allowing us to improve performance post-hoc across all of our datasets.",
            "year": 2022,
            "citationCount": 95,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work studies two NLP tasks that involve reasoning over text, namely question answering and natural language inference, and shows that explanations judged by humans to be good--logically consistent with the input and the prediction--more likely cooccur with accurate predictions."
            },
            "score": 4
        },
        {
            "id": "ecf0ee0b148f780562aeaeef8e64edbdf5b5342f",
            "paperId": "ecf0ee0b148f780562aeaeef8e64edbdf5b5342f",
            "title": "Compositional Visual Causal Reasoning with Language Prompts",
            "abstract": ". We propose a new visual causal reasoning framework that leverages compositional visual representations and language prompts to reason about counterfactuals. Our model learns to decompose visual scenes into objects and events, represent them compositionally, and generate natural language explanations describing potential causal relationships between them. These explanations are then used to infer counter-factuals in response to language prompts. We show that compositional visual representations, when combined with causal language explanations and prompting, can improve performance on visual causal reasoning tasks.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that compositional visual representations, when combined with causal language explanations and prompting, can improve performance on visual causal reasoning tasks."
            },
            "score": 4
        },
        {
            "id": "7747ecbc26b1688e6cad1a6ce83914efa2a3c04c",
            "paperId": "7747ecbc26b1688e6cad1a6ce83914efa2a3c04c",
            "title": "Prompting Contrastive Explanations for Commonsense Reasoning Tasks",
            "abstract": "Many commonsense reasoning NLP tasks involve choosing between one or more possible answers to a question or prompt based on knowledge that is often implicit. Large pretrained language models (PLMs) can achieve near-human performance on such tasks, while providing little human-interpretable evidence of the underlying reasoning they use. In this work, we show how to use these same models to generate such evidence: inspired by the contrastive nature of human explanations, we use PLMs to complete explanation prompts which contrast alternatives according to the key attribute(s) required to justify the correct answer (for example, peanuts are usually salty while raisins are sweet). Conditioning model decisions on these explanations improves performance on two commonsense reasoning benchmarks, as compared to previous non-contrastive alternatives. These explanations are also judged by humans to be more relevant for solving the task, and facilitate a novel method to evaluate explanation faithfulfness.",
            "year": 2021,
            "citationCount": 48,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Inspired by the contrastive nature of human explanations, this work uses PLMs to complete explanation prompts which contrast alternatives according to the key attribute(s) required to justify the correct answer (for example, peanuts are usually salty while raisins are sweet)."
            },
            "score": 4
        },
        {
            "id": "973b72dc176244194ee4fb426ba25e2a6a588b22",
            "paperId": "973b72dc176244194ee4fb426ba25e2a6a588b22",
            "title": "Why Attention is Not Explanation: Surgical Intervention and Causal Reasoning about Neural Models",
            "abstract": "As the demand for explainable deep learning grows in the evaluation of language technologies, the value of a principled grounding for those explanations grows as well. Here we study the state-of-the-art in explanation for neural models for NLP tasks from the viewpoint of philosophy of science. We focus on recent evaluation work that finds brittleness in explanations obtained through attention mechanisms. We harness philosophical accounts of explanation to suggest broader conclusions from these studies. From this analysis, we assert the impossibility of causal explanations from attention layers over text data. We then introduce NLP researchers to contemporary philosophy of science theories that allow robust yet non-causal reasoning in explanation, giving computer scientists a vocabulary for future research.",
            "year": 2020,
            "citationCount": 38,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The state-of-the-art in explanation for neural models for NLP tasks is studied from the viewpoint of philosophy of science, and the impossibility of causal explanations from attention layers over text data is asserted."
            },
            "score": 4
        },
        {
            "id": "ef9f78a53326a71a582385b6741b9a7c340234cd",
            "paperId": "ef9f78a53326a71a582385b6741b9a7c340234cd",
            "title": "On the Evaluations of ChatGPT and Emotion-enhanced Prompting for Mental Health Analysis",
            "abstract": "Automated mental health analysis shows great potential for enhancing the ef\ufb01ciency and accessibility of mental health care, whereas the recent dominant methods utilized pre-trained language models (PLMs) as the backbone and incorporated emotional information. The lat-est large language models (LLMs), such as ChatGPT, exhibit dramatic capabilities on diverse natural language processing tasks. However, existing studies on ChatGPT\u2019s zero-shot performance for mental health analysis have limitations in inadequate evaluation, utilization of emotional information, and explainabil-ity of methods. In this work, we comprehensively evaluate the mental health analysis and emotional reasoning ability of ChatGPT on 11 datasets across 5 tasks, including binary and multi-class mental health condition detection, cause/factor detection of mental health conditions, emotion recognition in conversations, and causal emotion entailment. We empirically analyze the impact of different prompting strategies with emotional cues on Chat-GPT\u2019s mental health analysis ability and ex-plainability. Experimental results show that ChatGPT outperforms traditional neural network methods but still has a signi\ufb01cant gap with advanced task-speci\ufb01c methods. The qualitative analysis shows its potential in ex-plainability compared with advanced black-box methods but also limitations on robustness and inaccurate reasoning. Prompt engineering with emotional cues is found to be effective in improving its performance on mental health analysis but requires the proper way of emotion infusion.",
            "year": 2023,
            "citationCount": 35,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results show that ChatGPT outperforms traditional neural network methods but still has a signi\ufb01cant gap with advanced task-speci\ufb01c methods, and the qualitative analysis shows its potential in ex-plainability compared with advanced black-box methods but also limitations on robustness and inaccurate reasoning."
            },
            "score": 3
        },
        {
            "id": "403fcde5c36533036e7f29705221376d80dd1dba",
            "paperId": "403fcde5c36533036e7f29705221376d80dd1dba",
            "title": "Super-Prompting: Utilizing Model-Independent Contextual Data to Reduce Data Annotation Required in Visual Commonsense Tasks",
            "abstract": "Pre-trained language models have shown excellent results in few-shot learning scenarios using in-context learning. Although it is impressive, the size of language models can be prohibitive to make them usable in on-device applications, such as sensors or smartphones. With smaller language models, task-specific data annotation is needed to fine-tune the language model for a specific purpose. However, data annotation can have a substantial financial and time burden for small research groups, startups, and even companies. In this paper, we analyze different prompt-based fine-tuning techniques to improve results on both language and multimodal causal transformer models. To evaluate our results, we use a dataset focusing on visual commonsense reasoning in time. Our results show that by simple model-agnostic prompt-based fine-tuning, comparable results can be reached by only using 35%-40% of the fine-tuning training dataset. The proposed approaches result in significant time and financial savings. As the proposed methods make minimal architectural assumptions, other researchers can use the results in their transformer models with minimal adaptations. We plan to release the source code freely to make it easier for the community to use and contribute to our work.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper analyzes different prompt-based fine-tuning techniques to improve results on both language and multimodal causal transformer models and shows that by simple model-agnostic prompt- based fine- Tuning, comparable results can be reached by only using 35%-40% of the fine- tuning training dataset."
            },
            "score": 3
        },
        {
            "id": "01bc871c0ecb7f586e54b9f67d2e50e08cbedf3d",
            "paperId": "01bc871c0ecb7f586e54b9f67d2e50e08cbedf3d",
            "title": "Models See Hallucinations: Evaluating the Factuality in Video Captioning",
            "abstract": "Video captioning aims to describe events in a video with natural language. In recent years, many works have focused on improving captioning models' performance. However, like other text generation tasks, it risks introducing factual errors not supported by the input video. These factual errors can seriously affect the quality of the generated text, sometimes making it completely unusable. Although factual consistency has received much research attention in text-to-text tasks (e.g., summarization), it is less studied in the context of vision-based text generation. In this work, we conduct a detailed human evaluation of the factuality in video captioning and collect two annotated factuality datasets. We find that 57.0% of the model-generated sentences have factual errors, indicating it is a severe problem in this field. However, existing evaluation metrics are mainly based on n-gram matching and show little correlation with human factuality annotation. We further propose a weakly-supervised, model-based factuality metric FactVC, which outperforms previous metrics on factuality evaluation of video captioning. The datasets and metrics will be released to promote future research for video captioning.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work conducts a detailed human evaluation of the factuality in video captioning and collects two annotated factuality datasets and proposes a weakly-supervised, model-based factuality metric FactVC, which outperforms previous metrics on factuality evaluation ofVideo captioning."
            },
            "score": 3
        },
        {
            "id": "8dce801e67f2f8923dc1f5b3059f70b462f4e4cf",
            "paperId": "8dce801e67f2f8923dc1f5b3059f70b462f4e4cf",
            "title": "DetectGPT-SC: Improving Detection of Text Generated by Large Language Models through Self-Consistency with Masked Predictions",
            "abstract": "General large language models (LLMs) such as ChatGPT have shown remarkable success, but it has also raised concerns among people about the misuse of AI-generated texts. Therefore, an important question is how to detect whether the texts are generated by ChatGPT or by humans. Existing detectors are built on the assumption that there is a distribution gap between human-generated and AI-generated texts. These gaps are typically identified using statistical information or classifiers. In contrast to prior research methods, we find that large language models such as ChatGPT exhibit strong self-consistency in text generation and continuation. Self-consistency capitalizes on the intuition that AI-generated texts can still be reasoned with by large language models using the same logical reasoning when portions of the texts are masked, which differs from human-generated texts. Using this observation, we subsequently proposed a new method for AI-generated texts detection based on self-consistency with masked predictions to determine whether a text is generated by LLMs. This method, which we call DetectGPT-SC. We conducted a series of experiments to evaluate the performance of DetectGPT-SC. In these experiments, we employed various mask scheme, zero-shot, and simple prompt for completing masked texts and self-consistency predictions. The results indicate that DetectGPT-SC outperforms the current state-of-the-art across different tasks.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that large language models such as ChatGPT exhibit strong self-consistency in text generation and continuation, and this method, which is called DetectGPT-SC, outperforms the current state-of-the-art across different tasks."
            },
            "score": 3
        },
        {
            "id": "663d743272e9ab04f54d9105a3c3a3f6e22dd1dd",
            "paperId": "663d743272e9ab04f54d9105a3c3a3f6e22dd1dd",
            "title": "FactKB: Generalizable Factuality Evaluation using Language Models Enhanced with Factual Knowledge",
            "abstract": "Evaluating the factual consistency of automatically generated summaries is essential for the progress and adoption of reliable summarization systems. Despite recent advances, existing factuality evaluation models are not robust, being especially prone to entity and relation errors in new domains. We propose FactKB, a simple new approach to factuality evaluation that is generalizable across domains, in particular with respect to entities and relations. FactKB is based on language models pretrained using facts extracted from external knowledge bases. We introduce three types of complementary factuality pretraining objectives based on direct entity facts, facts grounded in auxiliary knowledge about entities, and facts constructed compositionally through knowledge base walks. The resulting factuality evaluation model achieves state-of-the-art performance on two in-domain news summarization benchmarks as well as on three out-of-domain scientific literature datasets. Further analysis of FactKB shows improved ability to detect erroneous entities and relations in summaries and is robust and generalizable across domains.",
            "year": 2023,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "FactKB is a simple new approach to factuality evaluation that is generalizable across domains, in particular with respect to entities and relations and shows improved ability to detect erroneous entities and relation in summaries."
            },
            "score": 3
        },
        {
            "id": "234dad3daec8db281882c25bb704eefa8970bdc7",
            "paperId": "234dad3daec8db281882c25bb704eefa8970bdc7",
            "title": "Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs",
            "abstract": "In recent years, large language models (LLMs) have drawn significant attention due to their impressive emergent capabilities that were not observed in earlier language models. One emerging area where LLMs have been widely used in recent times is the utilization of LLMs as the evaluator of the texts generated by various generative models. In this paper, we also explore the possibility of whether LLMs are reliable in assessing the factual consistency of summaries generated by text generation models. We first propose a new approach to evaluate the factuality score using LLMs by utilizing the same LLM to perform all steps in the question-answering-based factuality scoring pipeline. Subsequently, we study the performance of various LLMs to directly score the factuality. Our evaluation is conducted in traditional benchmarks by comparing their correlation with human annotations. Contrary to expectations, our findings revealed that none of the factuality metrics showed any significant correlations (e.g., coefficient scores greater than 0.3) to human evaluations of factuality for GPT-4, PaLM-2, and Claude-2, with the only exception being GPT-3.5 in two subcategories of factuality. Nonetheless, our findings are consistent across almost all factual error types, suggesting a fundamental limitation in the ability of current LLMs to assess factuality.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new approach to evaluate the factuality score using LLMs is proposed by utilizing the same LLM to perform all steps in the question-answering-based factuality scoring pipeline, and it is revealed that none of the factuality metrics showed any significant correlations to human evaluations of factuality."
            },
            "score": 3
        },
        {
            "id": "152d9a231c00d4495c9bc4a466f42165ce2e2164",
            "paperId": "152d9a231c00d4495c9bc4a466f42165ce2e2164",
            "title": "Evaluating Factual Consistency of Summaries with Large Language Models",
            "abstract": "Detecting factual errors in summaries has been an important and challenging subject in summarization research. Inspired by the emergent ability of large language models (LLMs), we explore evaluating factual consistency of summaries by directly prompting LLMs. We present a comprehensive empirical study to assess the ability of LLMs as factual consistency evaluators, which consists of (1) analyzing different LLMs such as the GPT model series and Flan-T5; (2) investigating a variety of prompting methods including vanilla prompting, chain-of-thought prompting, and a sentence-by-sentence prompting method to tackle long summaries; and (3) evaluating on diverse summaries generated by multiple summarization systems, ranging from pre-transformer methods to SOTA pretrained models. Our experiments demonstrate that prompting LLMs is able to outperform the previous best factuality systems in all settings, by up to 12.2 absolute points in terms of the binary classification accuracy on inconsistency detection.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A comprehensive empirical study to assess the ability of LLMs as factual consistency evaluators, which consists of analyzing different LLMs such as the GPT model series and Flan-T5 and investigating a variety of prompting methods including vanilla prompting, chain-of-thought prompting, and a sentence-by-sentence prompting method to tackle long summaries."
            },
            "score": 3
        },
        {
            "id": "4ade9cb9f75236679029b6fd60d71c3bf513eedb",
            "paperId": "4ade9cb9f75236679029b6fd60d71c3bf513eedb",
            "title": "CoF-CoT: Enhancing Large Language Models with Coarse-to-Fine Chain-of-Thought Prompting for Multi-domain NLU Tasks",
            "abstract": "While Chain-of-Thought prompting is popular in reasoning tasks, its application to Large Language Models (LLMs) in Natural Language Understanding (NLU) is under-explored. Motivated by multi-step reasoning of LLMs, we propose Coarse-to-Fine Chain-of-Thought (CoF-CoT) approach that breaks down NLU tasks into multiple reasoning steps where LLMs can learn to acquire and leverage essential concepts to solve tasks from different granularities. Moreover, we propose leveraging semantic-based Abstract Meaning Representation (AMR) structured knowledge as an intermediate step to capture the nuances and diverse structures of utterances, and to understand connections between their varying levels of granularity. Our proposed approach is demonstrated effective in assisting the LLMs adapt to the multi-grained NLU tasks under both zero-shot and few-shot multi-domain settings.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes Coarse-to-Fine Chain-of-Thought (CoF-CoT) approach that breaks down NLU tasks into multiple reasoning steps where LLMs can learn to acquire and leverage essential concepts to solve tasks from different granularities."
            },
            "score": 3
        },
        {
            "id": "d42e04e2650b85495aa695a90aaf437b5ad90516",
            "paperId": "d42e04e2650b85495aa695a90aaf437b5ad90516",
            "title": "INFORM : Information eNtropy based multi-step reasoning FOR large language Models",
            "abstract": null,
            "year": 2023,
            "citationCount": 0,
            "tldr": null,
            "score": 3
        },
        {
            "id": "5851bf82b0a9db2de86828f62c3006a6e1b40798",
            "paperId": "5851bf82b0a9db2de86828f62c3006a6e1b40798",
            "title": "Team UTSA-NLP at SemEval 2024 Task 5: Prompt Ensembling for Argument Reasoning in Civil Procedures with GPT4",
            "abstract": "In this paper, we present our system for the SemEval Task 5, The Legal Argument Reasoning Task in Civil Procedure Challenge. Legal argument reasoning is an essential skill that all law students must master. Moreover, it is important to develop natural language processing solutions that can reason about a question given terse domain-specific contextual information. Our system explores a prompt-based solution using GPT4 to reason over legal arguments. We also evaluate an ensemble of prompting strategies, including chain-of-thought reasoning and in-context learning. Overall, our system results in a Macro F1 of .8095 on the validation dataset and .7315 (5th out of 21 teams) on the final test set. Code for this project is available at https://github.com/danschumac1/CivilPromptReasoningGPT4.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This system explores a prompt-based solution using GPT4 to reason over legal arguments, including chain-of-thought reasoning and in-context learning, and evaluates an ensemble of prompting strategies, including chain-of-thought reasoning and in-context learning."
            },
            "score": 3
        },
        {
            "id": "7e1809e6b201cd57df4340eeb08012ff07552c28",
            "paperId": "7e1809e6b201cd57df4340eeb08012ff07552c28",
            "title": "Improving students' causal reasoning skills with the computer modelling",
            "abstract": "Background and Objectives: Computer modelling helps a lot in learning comprehensive scientific concepts, including the causal mechanisms of phenomena, which is challenging for novice learners. Despite the many studies that have been published to show the effectiveness of using computers in the classroom, fewer studies have investigated the use of computer modelling and its effects on students' thinking. The causal structure of many natural and physics phenomena, the emphasis of science education standards on systems thinking development, and its improvement in students, the key role of causal reasoning in a better understanding of science, the increasing use of computer technologies in the physics classroom, the rapid development of computer software and Internet systems for modelling and simulating the real world in order to help physics teaching and learning, and to solve the shortcomings of paper modelling with the help of computers, prompted researchers to investigate the effectiveness of using computer modelling in the physics classroom to see how it would improve the students \u2019 causal reasoning. Investigating the effectiveness of computer modelling on students' understanding of causal links and reasoning in physics phenomena is the main goal of this research. Methods: A sample of 80 secondary high school students in the 11th grade was selected and participated in a semi-experimental design, consisting of two classes of 20 students (using computer modelling) and two classes of 20 students (using conceptual modelling on paper). The students' scores of the causal reasoning were collected in pre-test and post-test; to remove the pre-test effect (mental retention of answers), analysis of covariance was used. In this analysis, the effect of the pre-test scores on the post-test scores was first predicted with the help of simple linear regression, and after removing this effect, the difference between the post-test mean values of causal reasoning between the groups was explored with the analysis of variance. In this research, the mean difference was investigated both for the type of modelling (computer and paper) and for gender; therefore, due to having two independent variables, the analysis of covariance was two-way. With this analysis, the effect of the interaction between the gender variable and the teaching method was also measured. Findings: Compared to paper modelling, computer modelling was effective in increasing students' ability to present coherent causal expressions and better explanations of scientific evidence and ideas, and enriched their systems thinking. Recognizing the reasoning elements, gathering evidence and expressing their reasons in order to end reasoning, as well as the coherence of reasoning, were more difficult for students who were trained with paper modelling than for those who were trained with the help of computer modelling. The findings showed that the connection among the pieces of evidence was one of the most difficult parts of physics reasoning. In fact, the student's ability to integrate the pieces of evidence in order to conclude the argument and express the result was less than their other reasoning abilities. However, computer modelling could improve this ability better than paper modelling Conclusion: This quasi-experimental design helped us to reach important conclusions about the differences in causal reasoning between two different groups. Using computer tools can handle the learning of relatively complex cognitive skills such as causal reasoning. Computer simulation and conceptual models that are produced with computers can help to explain more causal links and more coherence of reasoning in physics classrooms. Therefore, we recommend curriculum designers and physics teachers use more computer simulation and modelling in order to strengthen system thinking in physics classrooms, and scientific explanations with the help of causal reasoning.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Compared to paper modelling, computer modelling was effective in increasing students' ability to present coherent causal expressions and better explanations of scientific evidence and ideas, and enriched their systems thinking."
            },
            "score": 3
        },
        {
            "id": "96de6983be2d1bf2d947b29e320fde4e946a6dca",
            "paperId": "96de6983be2d1bf2d947b29e320fde4e946a6dca",
            "title": "Improving language models fine-tuning with representation consistency targets",
            "abstract": "Fine-tuning contextualized representations 001 learned by pre-trained language models has 002 become a standard practice in the NLP field. 003 However, pre-trained representations are prone 004 to degradation (also known as representation 005 collapse) during fine-tuning, which leads to in-006 stability, sub-optimal performance, and weak 007 generalization. In this paper, we propose a 008 novel fine-tuning method that avoids represen-009 tation collapse during fine-tuning by discourag-010 ing undesirable changes of the representations. 011 We show that our approach matches or exceeds 012 the performance of the existing regularization-013 based fine-tuning methods across 13 language 014 understanding tasks (GLUE benchmark and six 015 additional datasets). We also demonstrate its 016 effectiveness in low-data settings and robust-017 ness to label perturbation. Furthermore, we 018 extend previous studies of representation col-019 lapse and propose several metrics to quantify it. 020 Using these metrics and previously proposed 021 experiments, we show that our approach ob-022 tains significant improvements in retaining the 023 expressive power of representations. 024",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel fine-tuning method that avoids represen-009 collapse during fine-tuning by discourag-010 ing undesirable changes of the representations and shows significant improvements in retaining the expressive power of representations."
            },
            "score": 2
        },
        {
            "id": "97dbad04ba10c2ffcc7044701667575d4699d8f6",
            "paperId": "97dbad04ba10c2ffcc7044701667575d4699d8f6",
            "title": "Multi-step Prompting for Few-shot Emotion-Grounded Conversations",
            "abstract": "Conversational systems have shown immense growth in their ability to communicate like humans. With the emergence of large pre-trained language models (PLMs) the ability to provide informative responses have improved significantly. Despite the success of PLMs, the ability to identify and generate engaging and empathetic responses is largely dependent on labelled-data. In this work, we design a prompting approach that identifies the emotion of a given utterance and uses the emotion information for generating the appropriate responses for conversational systems. We propose a two-step prompting method that first recognises the emotion in the dialogue utterance and in the second-step uses the predicted emotion to prompt the PLM to generate the corresponding em- pathetic response in a few-shot setting. Experimental results on three publicly available datasets show that our proposed approach outperforms the state-of-the-art approaches for both automatic and manual evaluation.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A two-step prompting method that first recognises the emotion in the dialogue utterance and in the second-step uses the predicted emotion to prompt the PLM to generate the corresponding em- pathetic response in a few-shot setting is proposed."
            },
            "score": 2
        }
    ],
    "novelty": "yes"
}