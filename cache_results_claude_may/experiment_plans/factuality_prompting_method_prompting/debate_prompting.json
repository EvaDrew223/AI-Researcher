{
    "topic_description": "novel prompting methods that can improve factuality and reduce hallucination of large language models",
    "idea_name": "Debate Prompting",
    "raw_idea": {
        "Problem": "Current language models can fall prey to the phenomenon of 'group think' when asked to verify their own generated outputs, as the model may share the same biases or knowledge gaps across multiple personas that are based on the same model.",
        "Existing Methods": "Existing methods that use a single language model to self-verify its outputs (via techniques such as generating self-consistency scores) are prone to model biases and blindspots.",
        "Motivation": "Debate is a powerful technique used by humans to expose blindspots in their reasoning. By prompting multiple language models to engage in a debate and take different stances on a topic, we can use the disagreements that surface to detect potential inconsistencies and hallucinations in model-generated text.",
        "Proposed Method": "We propose a debate prompting method involving at least two language models. Given an initial prompt, Model A generates a candidate output. Model B is then prompted to find flaws and counterarguments in Model A's output. Following this, Model A is prompted to refute Model B's arguments, and the debate continues over multiple rounds until a stopping criterion is met. Finally, the arguments for and against the initial output are aggregated and presented to a human judge or a scoring model to determine the validity of the candidate output. The candidate is accepted only if it withstands scrutiny from the debate.",
        "Experiment Plan": "We propose evaluating the debate prompting technique on fact verification datasets such as FEVER and VitaminC, as well as on long-form question answering tasks. Baselines include standard prompting, self-consistency, and techniques from the RAWLSNET (Reinforced Adaptive Writing Loss) framework. Metrics include the accuracy of fact verification and the faithfulness of generated answers (measured via both automatic scoring and human evaluation)."
    },
    "full_experiment_plan": {
        "Title": "Debate Prompting: Leveraging Disagreements Between Language Models to Reduce Hallucination",
        "Problem Statement": "Current language models can fall prey to the phenomenon of 'group think' when asked to verify their own generated outputs, as the model may share the same biases or knowledge gaps across multiple personas that are based on the same model. This can lead to the generation of factually incorrect or inconsistent information, known as hallucination.",
        "Motivation": "Existing methods that use a single language model to self-verify its outputs (via techniques such as generating self-consistency scores) are prone to model biases and blindspots. Debate is a powerful technique used by humans to expose blindspots in their reasoning. By prompting multiple language models to engage in a debate and take different stances on a topic, we can use the disagreements that surface to detect potential inconsistencies and hallucinations in model-generated text.",
        "Proposed Method": "We propose a debate prompting method involving at least two language models. Given an initial prompt, Model A generates a candidate output. Model B is then prompted to find flaws and counterarguments in Model A's output. Following this, Model A is prompted to refute Model B's arguments, and the debate continues over multiple rounds until a stopping criterion is met. Finally, the arguments for and against the initial output are aggregated and presented to a human judge or a scoring model to determine the validity of the candidate output. The candidate is accepted only if it withstands scrutiny from the debate.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We evaluate the debate prompting technique on fact verification datasets such as FEVER and VitaminC, as well as on long-form question answering tasks such as ELI5 and MS MARCO. These datasets contain claims or questions along with supporting or refuting evidence, which can be used to assess the factual accuracy of the model-generated outputs.",
            "Step 2: Construct Prompts": "For each example in the dataset, we construct a set of prompts for the debate:\n1. Initial Prompt (Model A): The original claim or question from the dataset, e.g., \"The Eiffel Tower is the tallest building in Paris.\"\n2. Critique Prompt (Model B): A prompt to find flaws and counterarguments in Model A's output, e.g., \"Identify any factual inaccuracies or inconsistencies in the following statement: [Model A's output]. Provide evidence to support your arguments.\"\n3. Rebuttal Prompt (Model A): A prompt to refute Model B's critique, e.g., \"Defend the factual accuracy of your original statement in light of the following counterarguments: [Model B's output]. Provide evidence to support your defense.\"\nThe debate continues for a fixed number of rounds or until a stopping criterion is met (e.g., both models agree or run out of new arguments).",
            "Step 3: Select Models": "We experiment with different pairings of language models for the debate, such as:\n1. GPT-3.5 (text-davinci-002) vs. GPT-3.5 (text-davinci-002)\n2. GPT-4 vs. GPT-4\n3. GPT-3.5 (text-davinci-002) vs. GPT-4\nWe also compare the performance of the debate prompting method against baselines that use a single model for output generation and verification, such as self-consistency scoring and chain-of-thought prompting.",
            "Step 4: Run the Debate": "For each example in the dataset, we run the debate between the selected pair of models for the specified number of rounds. We record the arguments generated by each model at each round of the debate.",
            "Step 5: Evaluate the Debate Outcome": "We aggregate the arguments for and against the initial output and present them to a human judge or a scoring model to determine the validity of the candidate output. The scoring model can be a pre-trained natural language inference (NLI) model that takes the initial output and the aggregated arguments as input and predicts a label of \"SUPPORTS\", \"REFUTES\", or \"NOT ENOUGH INFO\". We compare the predicted label against the ground truth label from the dataset to assess the accuracy of the debate outcome.",
            "Step 6: Analyze the Results": "We compare the performance of the debate prompting method against the baselines in terms of the accuracy of the final output (as determined by the human judge or scoring model). We also analyze the quality and diversity of the arguments generated by the models during the debate to gain insights into the strengths and weaknesses of the approach. Additionally, we examine the impact of factors such as the choice of models, the number of debate rounds, and the stopping criterion on the effectiveness of the method."
        },
        "Test Case Examples": {
            "Test Case 1": {
                "Initial Prompt (Model A)": "The Eiffel Tower is the tallest building in Paris.",
                "Ground Truth": "REFUTES",
                "Baseline Output (Self-Consistency)": "The Eiffel Tower is the tallest building in Paris. I am confident this statement is factually accurate. The Eiffel Tower stands at an impressive height of 324 meters, making it the tallest structure in Paris.",
                "Baseline Score": "INCORRECT",
                "Debate Round 1 (Model B)": "The statement \"The Eiffel Tower is the tallest building in Paris\" is inaccurate. While the Eiffel Tower is indeed a tall structure, it is not considered a building. The tallest building in Paris is actually the Tour Montparnasse, which stands at a height of 210 meters.",
                "Debate Round 2 (Model A)": "I concede that my original statement was imprecise. The Eiffel Tower is not a building, but rather a freestanding structure. However, it is still the tallest man-made structure in Paris, surpassing the height of the Tour Montparnasse.",
                "Debate Round 3 (Model B)": "You are correct that the Eiffel Tower is the tallest man-made structure in Paris. However, the original claim specifically referred to the tallest \"building\", which is a distinct category from freestanding structures like towers. Therefore, the statement remains inaccurate.",
                "Debate Outcome": "REFUTES",
                "Explanation": "The debate prompting method successfully identifies the factual inaccuracy in the initial statement by generating counterarguments that highlight the distinction between buildings and freestanding structures. The final outcome correctly refutes the claim, demonstrating the effectiveness of the approach in reducing hallucination."
            },
            "Test Case 2": {
                "Initial Prompt (Model A)": "What is the capital of France?",
                "Model A's Output": "The capital of France is Paris.",
                "Ground Truth": "SUPPORTS",
                "Baseline Output (Chain-of-Thought)": "Let's think step by step:\n1. France is a country located in Western Europe.\n2. The capital of a country is typically its largest and most important city.\n3. Paris is the largest city in France and is known for its iconic landmarks like the Eiffel Tower and the Louvre Museum.\n4. Paris is also the seat of the French government and the official residence of the President of France.\nTherefore, the capital of France is Paris.",
                "Baseline Score": "CORRECT",
                "Debate Round 1 (Model B)": "I agree with the statement that Paris is the capital of France. It is a well-established fact that Paris has been the capital of France since the 12th century. The city is the political, cultural, and economic center of the country.",
                "Debate Round 2 (Model A)": "Thank you for confirming the accuracy of my original statement. Paris is indeed the official capital of France, as recognized by the French government and the international community.",
                "Debate Outcome": "SUPPORTS",
                "Explanation": "In this test case, both models agree on the factual accuracy of the initial output. The debate does not generate any counterarguments, indicating that the original statement is well-supported by the models' knowledge. The final outcome correctly supports the claim, aligning with the ground truth label. This example demonstrates that the debate prompting method does not introduce false negatives when the initial output is already factually accurate."
            }
        },
        "Fallback Plan": "If the proposed debate prompting method does not significantly outperform the baselines in reducing hallucination, we can consider the following alternative approaches:\n1. Analyze the quality and diversity of the arguments generated by the models during the debate. If the arguments are weak, repetitive, or fail to address the relevant aspects of the claim, we can explore techniques to improve the argument generation process, such as using more targeted prompts or incorporating external knowledge sources.\n2. Investigate the impact of different model pairings on the debate outcome. If certain model combinations consistently underperform, we can experiment with alternative model selection strategies, such as using models with complementary strengths or training specialized models for debate.\n3. Examine the effectiveness of the stopping criterion used in the debate. If the debates are terminating prematurely or continuing for too many rounds without reaching a conclusive outcome, we can adjust the stopping criterion based on factors like argument quality, novelty, or convergence.\n4. Consider incorporating additional verification steps after the debate, such as using the aggregated arguments to guide a final fact-checking process or combining the debate outcome with other hallucination reduction techniques like retrieval-augmented generation.\n5. If the debate prompting method fails to provide significant improvements, we can pivot the project to focus on analyzing the limitations and failure modes of the approach. This could involve conducting error analysis on the datasets to identify common patterns or challenges, such as claims that require complex reasoning or domain-specific knowledge. The insights gained from this analysis can inform the development of future hallucination reduction techniques."
    },
    "novelty_queries": [
        "KeywordQuery(\"debate language models hallucination reduction\")",
        "KeywordQuery(\"language models disagreement reduce hallucination\")",
        "KeywordQuery(\"language models debate prompting factual consistency\")",
        "KeywordQuery(\"Debate Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
            "paperId": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
            "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such\"society of minds\"approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.",
            "year": 2023,
            "citationCount": 206,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer is presented, indicating that this approach significantly enhances mathematical and strategic reasoning across a number of tasks."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is reducing hallucination in language models, and the proposed approach is to use debate prompting between multiple language models to identify and correct inconsistencies and factual errors.\n\nThe research problem in the paper is improving factuality and reasoning in language models, and the proposed approach is to use a multiagent debate between language model instances to arrive at a common final answer.\n\nBoth the proposal and the paper aim to improve the factual accuracy and reasoning capabilities of language models using a debate-based approach involving multiple language models or instances. However, the proposal focuses specifically on reducing hallucination, while the paper has a broader scope of improving factuality and reasoning across various tasks.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "ec58a564fdda29e6a9a0a7bab5eeb4c290f716d7",
            "paperId": "ec58a564fdda29e6a9a0a7bab5eeb4c290f716d7",
            "title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate",
            "abstract": "Text evaluation has historically posed significant challenges, often demanding substantial labor and time cost. With the emergence of large language models (LLMs), researchers have explored LLMs' potential as alternatives for human evaluation. While these single-agent-based approaches show promise, experimental results suggest that further advancements are needed to bridge the gap between their current effectiveness and human-level evaluation quality. Recognizing that best practices of human evaluation processes often involve multiple human annotators collaborating in the evaluation, we resort to a multi-agent debate framework, moving beyond single-agent prompting strategies. The multi-agent-based approach enables a group of LLMs to synergize with an array of intelligent counterparts, harnessing their distinct capabilities and expertise to enhance efficiency and effectiveness in handling intricate tasks. In this paper, we construct a multi-agent referee team called ChatEval to autonomously discuss and evaluate the quality of generated responses from different models on open-ended questions and traditional natural language generation (NLG) tasks. Our analysis shows that ChatEval transcends mere textual scoring, offering a human-mimicking evaluation process for reliable assessments. Our code is available at https://github.com/chanchimin/ChatEval.",
            "year": 2023,
            "citationCount": 112,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A multi-agent referee team called ChatEval is constructed to autonomously discuss and evaluate the quality of generated responses from different models on open-ended questions and traditional natural language generation (NLG) tasks, offering a human-mimicking evaluation process for reliable assessments."
            },
            "score": 7,
            "novelty_score": "Research Problem:\nProposal: Reducing hallucination in language model outputs by leveraging disagreements between multiple models through debate prompting.\nPaper: Improving the effectiveness of language model-based evaluators for open-ended questions and NLG tasks using a multi-agent debate framework.\n\nApproach:\nProposal: Prompting multiple language models to engage in a debate, taking different stances on a topic, and using the disagreements to detect inconsistencies and hallucinations in the generated text.\nPaper: Constructing a multi-agent referee team (ChatEval) to autonomously discuss and evaluate the quality of generated responses from different models through collaboration and synergy.\n\nThe proposal focuses on reducing hallucination in language model outputs, while the paper aims to improve language model-based evaluators for assessing the quality of generated text. Although both involve multiple language models, the proposal uses debate to identify inconsistencies, while the paper employs a multi-agent framework for collaborative evaluation. The research problems and approaches, while related, are not directly aligned.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "cd2e04598909158494e556823d9de8baa692cee2",
            "paperId": "cd2e04598909158494e556823d9de8baa692cee2",
            "title": "Towards Mitigating Hallucination in Large Language Models via Self-Reflection",
            "abstract": "Large language models (LLMs) have shown promise for generative and knowledge-intensive tasks including question-answering (QA) tasks. However, the practical deployment still faces challenges, notably the issue of\"hallucination\", where models generate plausible-sounding but unfaithful or nonsensical information. This issue becomes particularly critical in the medical domain due to the uncommon professional concepts and potential social risks involved. This paper analyses the phenomenon of hallucination in medical generative QA systems using widely adopted LLMs and datasets. Our investigation centers on the identification and comprehension of common problematic answers, with a specific emphasis on hallucination. To tackle this challenge, we present an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation. Through this feedback process, our approach steadily enhances the factuality, consistency, and entailment of the generated answers. Consequently, we harness the interactivity and multitasking ability of LLMs and produce progressively more precise and accurate answers. Experimental results on both automatic and human evaluation demonstrate the superiority of our approach in hallucination reduction compared to baselines.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper analyses the phenomenon of hallucination in medical generative QA systems using widely adopted LLMs and datasets and presents an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation that steadily enhances the factuality, consistency, and entailment of the generated answers."
            },
            "score": 6,
            "novelty_score": "Research Problem:\nProposal: Reducing hallucination in language models by leveraging disagreements between multiple models through debate prompting.\nPaper: Mitigating hallucination in large language models for medical question-answering tasks.\n\nApproach:\nProposal: Prompting multiple language models to engage in a debate and using the disagreements to detect inconsistencies and hallucinations in the generated text.\nPaper: An interactive self-reflection methodology that incorporates knowledge acquisition and answer generation to enhance the factuality, consistency, and entailment of the generated answers.\n\nThe research problems are related, as both focus on reducing hallucination in language models. However, the approaches differ significantly. The proposal suggests using debate prompting between multiple models, while the paper proposes an interactive self-reflection methodology within a single model. Additionally, the paper specifically targets the medical domain, whereas the proposal does not mention any specific domain.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "3a89e289e2dd29f5e52a2bf354a637762b661257",
            "paperId": "3a89e289e2dd29f5e52a2bf354a637762b661257",
            "title": "Fine-tuning Language Models for Factuality",
            "abstract": "The fluency and creativity of large pre-trained language models (LLMs) have led to their widespread use, sometimes even as a replacement for traditional search engines. Yet language models are prone to making convincing but factually inaccurate claims, often referred to as 'hallucinations.' These errors can inadvertently spread misinformation or harmfully perpetuate misconceptions. Further, manual fact-checking of model responses is a time-consuming process, making human factuality labels expensive to acquire. In this work, we fine-tune language models to be more factual, without human labeling and targeting more open-ended generation settings than past work. We leverage two key recent innovations in NLP to do so. First, several recent works have proposed methods for judging the factuality of open-ended text by measuring consistency with an external knowledge base or simply a large model's confidence scores. Second, the direct preference optimization algorithm enables straightforward fine-tuning of language models on objectives other than supervised imitation, using a preference ranking over possible model responses. We show that learning from automatically generated factuality preference rankings, generated either through existing retrieval systems or our novel retrieval-free approach, significantly improves the factuality (percent of generated claims that are correct) of Llama-2 on held-out topics compared with RLHF or decoding strategies targeted at factuality. At 7B scale, compared to Llama-2-chat, we observe 58% and 40% reduction in factual error rate when generating biographies and answering medical questions, respectively.",
            "year": 2023,
            "citationCount": 55,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that learning from automatically generated factuality preference rankings, generated either through existing retrieval systems or the novel retrieval-free approach, significantly improves the factuality of Llama-2 on held-out topics compared with RLHF or decoding strategies targeted at factuality."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is reducing hallucination in language models by leveraging disagreements between multiple models through debate prompting. The approach involves prompting language models to engage in a debate, taking different stances on a topic, and using the disagreements to detect inconsistencies and hallucinations.\n\nThe research problem in the paper is improving the factuality of language models without human labeling. The approach involves fine-tuning language models using automatically generated factuality preference rankings, either through existing retrieval systems or a novel retrieval-free approach.\n\nWhile both the proposal and the paper aim to improve the factuality of language models, their approaches differ significantly. The proposal focuses on using debate prompting between multiple models, while the paper focuses on fine-tuning a single model using automatically generated preference rankings.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "79429814fd4d967b9277af2805c53f370e52ebb5",
            "paperId": "79429814fd4d967b9277af2805c53f370e52ebb5",
            "title": "Chain of Natural Language Inference for Reducing Large Language Model Ungrounded Hallucinations",
            "abstract": "Large language models (LLMs) can generate fluent natural language texts when given relevant documents as background context. This ability has attracted considerable interest in developing industry applications of LLMs. However, LLMs are prone to generate hallucinations that are not supported by the provided sources. In this paper, we propose a hierarchical framework to detect and mitigate such ungrounded hallucination. Our framework uses Chain of Natural Language Inference (CoNLI) for hallucination detection and hallucination reduction via post-editing. Our approach achieves state-of-the-art performance on hallucination detection and enhances text quality through rewrite, using LLMs without any fine-tuning or domain-specific prompt engineering. We show that this simple plug-and-play framework can serve as an effective choice for hallucination detection and reduction, achieving competitive performance across various contexts.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a hierarchical framework to detect and mitigate ungrounded hallucination, using Chain of Natural Language Inference (CoNLI) for hallucination detection and hallucination reduction via post-editing and shows that this simple plug-and-play framework can serve as an effective choice for hallucinations detection and reduction, achieving competitive performance across various contexts."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is reducing hallucination in language model outputs by leveraging disagreements between multiple models through debate prompting. The approach involves prompting two or more language models to engage in a debate, taking different stances on a topic, and using the disagreements to detect inconsistencies and hallucinations.\n\nThe research problem in the paper is also reducing hallucination in language model outputs, but the approach is different. The paper proposes using a hierarchical framework called Chain of Natural Language Inference (CoNLI) for hallucination detection and reduction through post-editing, without any fine-tuning or domain-specific prompt engineering.\n\nWhile both the proposal and the paper aim to address the problem of hallucination in language model outputs, their approaches differ significantly. The proposal focuses on leveraging disagreements between multiple models through debate prompting, while the paper uses a single-model hierarchical framework for hallucination detection and post-editing.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "4b0b56be0ae9479d2bd5c2f0943db1906343c10f",
            "paperId": "4b0b56be0ae9479d2bd5c2f0943db1906343c10f",
            "title": "Chain-of-Verification Reduces Hallucination in Large Language Models",
            "abstract": "Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (CoVe) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show CoVe decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.",
            "year": 2023,
            "citationCount": 53,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Chain-of-Verification (CoVe) method is developed, whereby the model first drafts an initial response; then plans verification questions to fact-check its draft; and answers those questions independently so the answers are not biased by other responses."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is reducing hallucination in language models by leveraging disagreements between multiple models through debate prompting. The approach involves prompting two or more language models to engage in a debate, taking different stances on a topic, and using the disagreements that surface to detect potential inconsistencies and hallucinations.\n\nThe research problem in the paper is also reducing hallucination in language models, but the approach is different. The paper proposes a Chain-of-Verification (CoVe) method where a single model deliberates on its own responses by drafting an initial response, planning verification questions to fact-check its draft, answering those questions independently, and generating a final verified response.\n\nWhile both the proposal and the paper aim to reduce hallucination in language models, their approaches differ. The proposal focuses on using multiple models to engage in a debate, while the paper uses a single model to deliberate on its own responses through a chain of verification steps.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "fbd4a876cee20eaf98f344aca597a55338f663f5",
            "paperId": "fbd4a876cee20eaf98f344aca597a55338f663f5",
            "title": "Examining the Inter-Consistency of Large Language Models: An In-depth Analysis via Debate",
            "abstract": "Large Language Models (LLMs) have demonstrated human-like intelligence and are widely used in various applications. However, LLMs still exhibit various kinds of inconsistency problems. Existing works mainly focus on the inconsistency issues within a single LLM, while we investigate the inter-consistency among multiple LLMs, which is critical for collaborating to solve a complex task. To examine whether LLMs can collaborate to ultimately achieve a consensus for the shared goal and whether LLMs easily change their viewpoints, we introduce a Formal Debate framework (FORD) With FORD, we conduct a three-stage debate aligned with real-world scenarios: fair debate, mismatched debate, and roundtable debate. Through extensive experiments on the commonsense reasoning task, LLMs not only become more inter-consistent but also achieve higher performance. More-over, we observe that stronger LLMs tend to dominate the debates by adhering to their perspectives, while weaker ones are more likely to change viewpoints. Additionally, we highlight the importance of a competent judge, such as GPT-4, to draw more proper conclusions. Our work contributes to understanding the inter-consistency among LLMs and lays the foundation for the development of future collaboration methods.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates the inter-consistency among multiple Large Language Models, and contributes to understanding the inter-consistency among LLMs and lays the foundation for the development of future collaboration methods."
            },
            "score": 6,
            "novelty_score": "Research Problem:\nProposal: Reducing hallucination in language models by leveraging disagreements between multiple models through debate prompting.\nPaper: Examining the inter-consistency among multiple large language models and their ability to collaborate and reach consensus through formal debates.\n\nApproach:\nProposal: Prompting multiple language models to engage in a structured debate, where they take different stances on a topic and present arguments and counterarguments to expose inconsistencies and hallucinations in the generated outputs.\nPaper: Introducing a formal debate framework (FORD) to conduct three-stage debates (fair debate, mismatched debate, and roundtable debate) among multiple large language models to examine their inter-consistency and collaboration abilities.\n\nThe research problems and approaches of the proposal and the paper are related but not directly the same. The proposal focuses on reducing hallucination by leveraging disagreements between models, while the paper examines the inter-consistency and collaboration abilities of models through formal debates. Although both involve debates between multiple models, the primary goals and the specific debate setups differ.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "f8cbcb106a48524edc39df23e2a95f1e6d4c739a",
            "paperId": "f8cbcb106a48524edc39df23e2a95f1e6d4c739a",
            "title": "Examining Inter-Consistency of Large Language Models Collaboration: An In-depth Analysis via Debate",
            "abstract": "Large Language Models (LLMs) have shown impressive capabilities in various applications, but they still face various inconsistency issues. Existing works primarily focus on the inconsistency issues within a single LLM, while we complementarily explore the inter-consistency among multiple LLMs for collaboration. To examine whether LLMs can collaborate effectively to achieve a consensus for a shared goal, we focus on commonsense reasoning, and introduce a formal debate framework (FORD) to conduct a three-stage debate among LLMs with real-world scenarios alignment: fair debate, mismatched debate, and roundtable debate. Through extensive experiments on various datasets, LLMs can effectively collaborate to reach a consensus despite noticeable inter-inconsistencies, but imbalances in their abilities can lead to domination by superior LLMs. Leveraging a more advanced LLM like GPT-4 as an authoritative judge can boost collaboration performance. Our work contributes to understanding the inter-consistency among LLMs and lays the foundation for developing future collaboration methods. Codes and data are available at https://github.com/Waste-Wood/FORD",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a formal debate framework (FORD) to conduct a three-stage debate among LLMs with real-world scenarios alignment: fair debate, mismatched debate, and roundtable debate to examine whether LLMs can collaborate effectively to achieve a consensus for a shared goal."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is reducing hallucination in language models by leveraging disagreements between multiple models through debate prompting. The approach involves prompting at least two language models to engage in a debate, taking different stances on a topic, and using the disagreements to detect inconsistencies and hallucinations.\n\nThe research problem in the paper is examining the inter-consistency among multiple large language models for collaboration. The approach introduces a formal debate framework (FORD) to conduct a three-stage debate among LLMs with real-world scenarios alignment.\n\nWhile both the proposal and the paper involve using debate between language models, the research problems and approaches are different. The proposal focuses on reducing hallucination in a single model's output, while the paper examines the inter-consistency and collaboration effectiveness among multiple models.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "eb291a2e237774b162d9c51c21c4868795589e94",
            "paperId": "eb291a2e237774b162d9c51c21c4868795589e94",
            "title": "Diving into the Inter-Consistency of Large Language Models: An Insightful Analysis through Debate",
            "abstract": "Large language models (LLMs) have demonstrated impressive zero-shot or few-shot commonsense reasoning performance on various natural language processing (NLP) tasks. However, despite their strong commonsense reasoning abilities, LLMs still exhibit various kinds of inconsistency problems. While previous researches mainly focused on the self-consistency within a single LLM, we propose to explore the inter-consistency issue between two or more LLMs, which is critical for diverse and precise decision-making processes. Since the LLMs possess human-like intelligence after instruction tuning and reinforcement learning with human feedback (RLHF), we design a formal debate framework to delve into the inter-consistency problem among LLMs with three-stage debate: fair debate, mismatched debate, and roundtable debate. Through extensive experiments on 7 common-sense reasoning datasets, LLMs not only become more inter-consistent by compromising and refuting but also achieve higher performance and stronger interpretability. Furthermore, we \ufb01nd a much stronger LLM would be dominant in mismatched debates, while it will be easily misled by relatively weaker LLMs in a more complex debate scenario such as roundtable debate.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A formal debate framework is designed to delve into the inter-consistency problem among LLMs with three-stage debate: fair debate, mismatched debate, and roundtable debate to explore the inter-consistency issue between two or more LLMs, which is critical for diverse and precise decision-making processes."
            },
            "score": 6,
            "novelty_score": "Research Problem:\nProposal: Reducing hallucination in language models by leveraging disagreements between multiple models through debate prompting.\nPaper: Exploring the inter-consistency issue between two or more large language models (LLMs) through a formal debate framework.\n\nApproach:\nProposal: Prompting multiple language models to engage in a debate, using the disagreements to detect inconsistencies and hallucinations in the generated text.\nPaper: Designing a three-stage debate framework (fair debate, mismatched debate, and roundtable debate) to investigate the inter-consistency problem among LLMs.\n\nWhile both the proposal and the paper focus on using debate between language models, the research problems and approaches are different. The proposal aims to reduce hallucination by leveraging disagreements, while the paper explores the inter-consistency issue among LLMs through a formal debate framework. The proposal uses debate to detect inconsistencies, whereas the paper employs debate to study how LLMs compromise, refute, and influence each other's performance and interpretability.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "12c826f4195da172b212a529f8fcf10cc79e35da",
            "paperId": "12c826f4195da172b212a529f8fcf10cc79e35da",
            "title": "Context-faithful Prompting for Large Language Models",
            "abstract": "Large language models (LLMs) encode parametric knowledge about world facts and have shown remarkable performance in knowledge-driven NLP tasks. However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks). In this paper, we seek to assess and enhance LLMs' contextual faithfulness in two aspects: knowledge conflict and prediction with abstention. We demonstrate that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies. In particular, we identify opinion-based prompts and counterfactual demonstrations as the most effective methods. Opinion-based prompts reframe the context as a narrator's statement and inquire about the narrator's opinions, while counterfactual demonstrations use instances containing false facts to improve faithfulness in knowledge conflict situations. Neither technique requires additional training. We conduct experiments on three datasets of two standard NLP tasks, machine reading comprehension and relation extraction, and the results demonstrate significant improvement in faithfulness to contexts. Code and data are released at https://github.com/wzhouad/context-faithful-llm.",
            "year": 2023,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies, and opinion-based prompts and counterfactual demonstrations are identified as the most effective methods."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is reducing hallucination in language model outputs by leveraging disagreements between multiple models through debate prompting. The approach involves prompting two or more language models to engage in a debate, taking different stances on a topic, and using the surfaced disagreements to detect inconsistencies and hallucinations.\n\nThe research problem in the paper is improving large language models' contextual faithfulness in knowledge-driven NLP tasks. The approach involves using carefully designed prompting strategies, such as opinion-based prompts and counterfactual demonstrations, to enhance the models' ability to consider contextual cues and handle knowledge conflicts.\n\nWhile both works aim to improve the performance of language models, the specific research problems and approaches differ. The proposal focuses on reducing hallucination through debate prompting, while the paper addresses contextual faithfulness using specialized prompting techniques.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "9ffefdf1fcd780cb71450b0a7a29247c66aa87be",
            "paperId": "9ffefdf1fcd780cb71450b0a7a29247c66aa87be",
            "title": "The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning",
            "abstract": "Does prompting a large language model (LLM) like GPT-3 with explanations improve in-context learning? We study this question on two NLP tasks that involve reasoning over text, namely question answering and natural language inference. We test the performance of four LLMs on three textual reasoning datasets using prompts that include explanations in multiple different styles. For these tasks, we find that including explanations in the prompts for OPT, GPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to moderate accuracy improvements over standard few-show learning. However, text-davinci-002 is able to benefit more substantially. We further show that explanations generated by the LLMs may not entail the models' predictions nor be factually grounded in the input, even on simple tasks with extractive explanations. However, these flawed explanations can still be useful as a way to verify LLMs' predictions post-hoc. Through analysis in our three settings, we show that explanations judged by humans to be good--logically consistent with the input and the prediction--more likely cooccur with accurate predictions. Following these observations, we train calibrators using automatically extracted scores that assess the reliability of explanations, allowing us to improve performance post-hoc across all of our datasets.",
            "year": 2022,
            "citationCount": 95,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work studies two NLP tasks that involve reasoning over text, namely question answering and natural language inference, and shows that explanations judged by humans to be good--logically consistent with the input and the prediction--more likely cooccur with accurate predictions."
            },
            "score": 6
        },
        {
            "id": "eee548fbd0b9dd954c692fbd8880e80d5f077bd7",
            "paperId": "eee548fbd0b9dd954c692fbd8880e80d5f077bd7",
            "title": "Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models",
            "abstract": "Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP). Although convenient for research and practical applications, open-source LLMs with fewer parameters often suffer from severe hallucinations compared to their larger counterparts. This paper focuses on measuring and reducing hallucinations in BLOOM 7B, a representative of such weaker open-source LLMs that are publicly available for research and commercial applications. We introduce HaloCheck, a lightweight BlackBox knowledge-free framework designed to quantify the severity of hallucinations in LLMs. Additionally, we explore techniques like knowledge injection and teacher-student approaches to alleviate hallucinations in low-parameter LLMs. Our experiments effectively demonstrate the reduction of hallucinations in challenging domains for these LLMs.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "HaloCheck is introduced, a lightweight BlackBox knowledge-free framework designed to quantify the severity of hallucinations in LLMs, and techniques like knowledge injection and teacher-student approaches to alleviate hallucinations in low-parameter LLMs are explored."
            },
            "score": 5
        },
        {
            "id": "bec41f0bfd3cf3d5b09a16be559543628868d412",
            "paperId": "bec41f0bfd3cf3d5b09a16be559543628868d412",
            "title": "Navigating Uncertainty: Optimizing API Dependency for Hallucination Reduction in Closed-Book Question Answering",
            "abstract": "While Large Language Models (LLM) are able to accumulate and restore knowledge, they are still prone to hallucination. Especially when faced with factual questions, LLM cannot only rely on knowledge stored in parameters to guarantee truthful and correct answers. Augmenting these models with the ability to search on external information sources, such as the web, is a promising approach to ground knowledge to retrieve information. However, searching in a large collection of documents introduces additional computational/time costs. An optimal behavior would be to query external resources only when the LLM is not confident about answers. In this paper, we propose a new LLM able to self-estimate if it is able to answer directly or needs to request an external tool. We investigate a supervised approach by introducing a hallucination masking mechanism in which labels are generated using a close book question-answering task. In addition, we propose to leverage parameter-efficient fine-tuning techniques to train our model on a small amount of data. Our model directly provides answers for $78.2\\%$ of the known queries and opts to search for $77.2\\%$ of the unknown ones. This results in the API being utilized only $62\\%$ of the time.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new LLM able to self-estimate if it is able to answer directly or needs to request an external tool is proposed, and a supervised approach is investigated by introducing a hallucination masking mechanism in which labels are generated using a close book question-answering task."
            },
            "score": 5
        },
        {
            "id": "daa171a25956b537b222a564c1488b2b6cfbb6bb",
            "paperId": "daa171a25956b537b222a564c1488b2b6cfbb6bb",
            "title": "Unlocking Anticipatory Text Generation: A Constrained Approach for Large Language Models Decoding",
            "abstract": "Large Language Models (LLMs) have demonstrated a powerful ability for text generation. However, achieving optimal results with a given prompt or instruction can be challenging, especially for billion-sized models. Additionally, undesired behaviors such as toxicity or hallucinations can manifest. While much larger models (e.g., ChatGPT) may demonstrate strength in mitigating these issues, there is still no guarantee of complete prevention. In this work, we propose formalizing text generation as a future-constrained generation problem to minimize undesirable behaviors and enforce faithfulness to instructions. The estimation of future constraint satisfaction, accomplished using LLMs, guides the text generation process. Our extensive experiments demonstrate the effectiveness of the proposed approach across three distinct text generation tasks: keyword-constrained generation (Lin et al., 2020), toxicity reduction (Gehman et al., 2020), and factual correctness in question-answering (Gao et al., 2023).",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes formalizing text generation as a future-constrained generation problem to minimize undesirable behaviors and enforce faithfulness to instructions and demonstrates the effectiveness of the proposed approach across three distinct text generation tasks."
            },
            "score": 5
        },
        {
            "id": "4e53b481beabba42aac027e5a8c69fed26ab4062",
            "paperId": "4e53b481beabba42aac027e5a8c69fed26ab4062",
            "title": "RHO ($\u03c1$): Reducing Hallucination in Open-domain Dialogues with Knowledge Grounding",
            "abstract": "Dialogue systems can leverage large pre-trained language models and knowledge to generate fluent and informative responses. However, these models are still prone to produce hallucinated responses not supported by the input source, which greatly hinders their application. The heterogeneity between external knowledge and dialogue context challenges representation learning and source integration, and further contributes to unfaithfulness. To handle this challenge and generate more faithful responses, this paper presents RHO ($\\rho$) utilizing the representations of linked entities and relation predicates from a knowledge graph (KG). We propose (1) local knowledge grounding to combine textual embeddings with the corresponding KG embeddings; and (2) global knowledge grounding to equip RHO with multi-hop reasoning abilities via the attention mechanism. In addition, we devise a response re-ranking technique based on walks over KG sub-graphs for better conversational reasoning. Experimental results on OpenDialKG show that our approach significantly outperforms state-of-the-art methods on both automatic and human evaluation by a large margin, especially in hallucination reduction (17.54% in FeQA).",
            "year": 2022,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "RHO is presented utilizing the representations of linked entities and relation predicates from a knowledge graph (KG) to equip RHO with multi-hop reasoning abilities via the attention mechanism and devise a response re-ranking technique based on walks over KG sub-graphs for better conversational reasoning."
            },
            "score": 5
        },
        {
            "id": "80fd20e175f83a699258b8780cf365418d1538b0",
            "paperId": "80fd20e175f83a699258b8780cf365418d1538b0",
            "title": "Chain of Knowledge: A Framework for Grounding Large Language Models with Structured Knowledge Bases",
            "abstract": "We introduce Chain of Knowledge (CoK), a framework that augments large language models with structured knowledge bases to improve factual correctness and reduce hallucination. Compared to previous works which only retrieve unstructured texts, CoK leverages structured knowledge bases which support complex queries and offer more direct factual statements. To assist large language models to effectively query knowledge bases, we propose a query generator model with contrastive instruction-tuning. As the query generator is separate from the frozen large language model, our framework is modular and thus easily adapted to various knowledge sources and models. Experiments show that our framework signi\ufb01cantly enhances the factual correctness of large language models on knowledge-intensive tasks. Our code is available at https://github.com/DAMO-NLP-SG/chain-of-knowledge.",
            "year": 2023,
            "citationCount": 34,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Chain of Knowledge framework is introduced, a framework that augments large language models with structured knowledge bases to improve factual correctness and reduce hallucination and a query generator model with contrastive instruction-tuning to assist largelanguage models to effectively query knowledge bases."
            },
            "score": 5
        },
        {
            "id": "6489640b1d30a8a3e7cb906bb6557f1ccd0d799d",
            "paperId": "6489640b1d30a8a3e7cb906bb6557f1ccd0d799d",
            "title": "Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models",
            "abstract": "Retrieval-augmented language models (RALMs) represent a substantial advancement in the capabilities of large language models, notably in reducing factual hallucination by leveraging external knowledge sources. However, the reliability of the retrieved information is not always guaranteed. The retrieval of irrelevant data can lead to misguided responses, and potentially causing the model to overlook its inherent knowledge, even when it possesses adequate information to address the query. Moreover, standard RALMs often struggle to assess whether they possess adequate knowledge, both intrinsic and retrieved, to provide an accurate answer. In situations where knowledge is lacking, these systems should ideally respond with\"unknown\"when the answer is unattainable. In response to these challenges, we introduces Chain-of-Noting (CoN), a novel approach aimed at improving the robustness of RALMs in facing noisy, irrelevant documents and in handling unknown scenarios. The core idea of CoN is to generate sequential reading notes for retrieved documents, enabling a thorough evaluation of their relevance to the given question and integrating this information to formulate the final answer. We employed ChatGPT to create training data for CoN, which was subsequently trained on an LLaMa-2 7B model. Our experiments across four open-domain QA benchmarks show that RALMs equipped with CoN significantly outperform standard RALMs. Notably, CoN achieves an average improvement of +7.9 in EM score given entirely noisy retrieved documents and +10.5 in rejection rates for real-time questions that fall outside the pre-training knowledge scope.",
            "year": 2023,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Chain-of-Noting (CoN) is introduced, a novel approach aimed at improving the robustness of RALMs in facing noisy, irrelevant documents and in handling unknown scenarios, and achieves an average improvement of +7.9 in EM score given entirely noisy retrieved documents and +10.5 in rejection rates for real-time questions that fall outside the pre-training knowledge scope."
            },
            "score": 5
        },
        {
            "id": "e468ed6b824e60f45ba9a20b034e4090c6630751",
            "paperId": "e468ed6b824e60f45ba9a20b034e4090c6630751",
            "title": "Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources",
            "abstract": "We present chain-of-knowledge (CoK), a novel framework that augments large language models (LLMs) by dynamically incorporating grounding information from heterogeneous sources. It results in more factual rationales and reduced hallucination in generation. Specifically, CoK consists of three stages: reasoning preparation, dynamic knowledge adapting, and answer consolidation. Given a knowledge-intensive question, CoK first prepares several preliminary rationales and answers while identifying the relevant knowledge domains. If there is no majority consensus among the answers from samples, CoK corrects the rationales step by step by adapting knowledge from the identified domains. These corrected rationales can plausibly serve as a better foundation for the final answer consolidation. Unlike prior studies that primarily use unstructured data, CoK also leverages structured knowledge sources such as Wikidata and tables that provide more reliable factual information. To access both unstructured and structured knowledge sources in the dynamic knowledge adapting stage, we propose an adaptive query generator that allows the generation of queries for various types of query languages, including SPARQL, SQL, and natural sentences. Moreover, to minimize error propagation between rationales, CoK corrects the rationales progressively using preceding corrected rationales to generate and correct subsequent rationales. Extensive experiments show that CoK consistently improves the performance of LLMs on knowledge-intensive tasks across different domains.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Chain-of-knowledge (CoK), a novel framework that augments large language models (LLMs) by dynamically incorporating grounding information from heterogeneous sources, results in more factual rationales and reduced hallucination in generation."
            },
            "score": 5
        },
        {
            "id": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
            "paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
            "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "abstract": "Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).",
            "year": 2022,
            "citationCount": 1390,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting that first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths."
            },
            "score": 5
        },
        {
            "id": "6af460d34bfc8e955e43fbe15cedcf329b48bc19",
            "paperId": "6af460d34bfc8e955e43fbe15cedcf329b48bc19",
            "title": "SAC3: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency",
            "abstract": "Hallucination detection is a critical step toward understanding the trustworthiness of modern language models (LMs). To achieve this goal, we re-examine existing detection approaches based on the self-consistency of LMs and uncover two types of hallucinations resulting from 1) question-level and 2) model-level, which cannot be effectively identified through self-consistency check alone. Building upon this discovery, we propose a novel sampling-based method, i.e., semantic-aware cross-check consistency (SAC3) that expands on the principle of self-consistency checking. Our SAC3 approach incorporates additional mechanisms to detect both question-level and model-level hallucinations by leveraging advances including semantically equivalent question perturbation and cross-model response consistency checking. Through extensive and systematic empirical analysis, we demonstrate that SAC3 outperforms the state of the art in detecting both non-factual and factual statements across multiple question-answering and open-domain generation benchmarks.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel sampling-based method, i.e., semantic-aware cross-check consistency (SAC3) that expands on the principle of self-consistency checking and demonstrates that SAC3 outperforms the state of the art in detecting both non-factual and factual statements across multiple question-answering and open-domain generation benchmarks."
            },
            "score": 5
        },
        {
            "id": "62176de125738e3b95850d1227bac81fd646b78e",
            "paperId": "62176de125738e3b95850d1227bac81fd646b78e",
            "title": "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models",
            "abstract": "Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, Few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual efforts, Zero-shot-CoT concatenates the target problem statement with \u201cLet\u2019s think step by step\u201d as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.",
            "year": 2023,
            "citationCount": 115,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results over GPT-3 show that the proposed zero-shot prompting consistently outperforms Zero- shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem."
            },
            "score": 5
        },
        {
            "id": "dfab0f3ee6f47e36cccee145794cd117773e6f73",
            "paperId": "dfab0f3ee6f47e36cccee145794cd117773e6f73",
            "title": "Towards LLM-based Fact Verification on News Claims with a Hierarchical Step-by-Step Prompting Method",
            "abstract": "While large pre-trained language models (LLMs) have shown their impressive capabilities in various NLP tasks, they are still under-explored in the misinformation domain. In this paper, we examine LLMs with in-context learning (ICL) for news claim verification, and find that only with 4-shot demonstration examples, the performance of several prompting methods can be comparable with previous supervised models. To further boost performance, we introduce a Hierarchical Step-by-Step (HiSS) prompting method which directs LLMs to separate a claim into several subclaims and then verify each of them via multiple questions-answering steps progressively. Experiment results on two public misinformation datasets show that HiSS prompting outperforms state-of-the-art fully-supervised approach and strong few-shot ICL-enabled baselines.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Hierarchical Step-by-Step (HiSS) prompting method is introduced which directs LLMs to separate a claim into several subclaims and then verify each of them via multiple questions-answering steps progressively."
            },
            "score": 5
        },
        {
            "id": "9c20d8d5cfc60f5b9aa058ff2968563f2af33398",
            "paperId": "9c20d8d5cfc60f5b9aa058ff2968563f2af33398",
            "title": "Temporal Insight Enhancement: Mitigating Temporal Hallucination in Multimodal Large Language Models",
            "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced the comprehension of multimedia content, bringing together diverse modalities such as text, images, and videos. However, a critical challenge faced by these models, especially when processing video inputs, is the occurrence of hallucinations - erroneous perceptions or interpretations, particularly at the event level. This study introduces an innovative method to address event-level hallucinations in MLLMs, focusing on specific temporal understanding in video content. Our approach leverages a novel framework that extracts and utilizes event-specific information from both the event query and the provided video to refine MLLMs' response. We propose a unique mechanism that decomposes on-demand event queries into iconic actions. Subsequently, we employ models like CLIP and BLIP2 to predict specific timestamps for event occurrences. Our evaluation, conducted using the Charades-STA dataset, demonstrates a significant reduction in temporal hallucinations and an improvement in the quality of event-related responses. This research not only provides a new perspective in addressing a critical limitation of MLLMs but also contributes a quantitatively measurable method for evaluating MLLMs in the context of temporal-related questions.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An innovative method to address event-level hallucinations in MLLMs, focusing on specific temporal understanding in video content, using a novel framework that extracts and utilizes event-specific information from both the event query and the provided video to refine MLLMs' response."
            },
            "score": 4
        },
        {
            "id": "7751f6cdec0f4473c1733eec91699744a7d5176f",
            "paperId": "7751f6cdec0f4473c1733eec91699744a7d5176f",
            "title": "HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding",
            "abstract": "While large vision-language models (LVLMs) have demonstrated impressive capabilities in interpreting multi-modal contexts, they invariably suffer from object hallucinations (OH). We introduce HALC, a novel decoding algorithm designed to mitigate OH in LVLMs. HALC leverages distinct fine-grained optimal visual information in vision-language tasks and operates on both local and global contexts simultaneously. Specifically, HALC integrates a robust auto-focal grounding mechanism (locally) to correct hallucinated tokens on the fly, and a specialized beam search algorithm (globally) to significantly reduce OH while preserving text generation quality. Additionally, HALC can be integrated into any LVLMs as a plug-and-play module without extra training. Extensive experimental studies demonstrate the effectiveness of HALC in reducing OH, outperforming state-of-the-arts across four benchmarks.",
            "year": 2024,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "HALC, a novel decoding algorithm designed to mitigate OH in LVLMs, is introduced, which leverages distinct fine-grained optimal visual information in vision-language tasks and operates on both local and global contexts simultaneously."
            },
            "score": 4
        },
        {
            "id": "a26fbe4e08cc37f48661c2adb538111248e42f71",
            "paperId": "a26fbe4e08cc37f48661c2adb538111248e42f71",
            "title": "ChatENT: Augmented Large Language Models for Expert Knowledge Retrieval in Otolaryngology - Head and Neck Surgery",
            "abstract": "The recent surge in popularity of Large Language Models (LLMs), such as ChatGPT, has showcased their proficiency in medical examinations and potential contributions to medical education. However, LLMs possess inherent limitations, including inconsistent accuracy, specific prompting requirements, and the risk of generating harmful hallucinations. A domain-specific, fine-tuned model would address these limitations effectively. OHNS-relevant data was systematically gathered from open-access internet sources and indexed into a database. We leveraged Retrieval-Augmented Language Modeling (RALM) to recall this information and used it for pre-training, which was then integrated into ChatGPT 4.0, creating a OHNS specific knowledge Q&A platform known as ChatENT. ChatENT showed enhanced performance in the analysis and interpretation of OHNS information, outperforming ChatGPT 4.0 in both the Canadian Royal College OHNS sample examination questions challenge and the US board practice questions challenge, with a 58.4% and 26.0% error reduction, respectively. ChatENT generated fewer hallucinations and demonstrated greater consistency.To the best of our knowledge, ChatENT is the first specialty-specific LLM in the medical field. It appears to have considerable promise in areas such as medical education, patient education, and clinical decision support. The fine-tuned model has demonstrated the capacity to overcome the limitations of existing LLMs, thereby signaling a future of more precise, safe, and user-friendly applications in the realm of OHNS.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The fine-tuned model has demonstrated the capacity to overcome the limitations of existing LLMs, thereby signaling a future of more precise, safe, and user-friendly applications in the realm of OHNS."
            },
            "score": 4
        },
        {
            "id": "6d763d3107bed2ce5ab4a79f1857f47a2e269a41",
            "paperId": "6d763d3107bed2ce5ab4a79f1857f47a2e269a41",
            "title": "Contrastive Error Attribution for Finetuned Language Models",
            "abstract": "Recent work has identified noisy and misannotated data as a core cause of hallucinations and unfaithful outputs in Natural Language Generation (NLG) tasks. Consequently, identifying and removing these examples is a key open challenge in creating reliable NLG systems. In this work, we introduce a framework to identify and remove low-quality training instances that lead to undesirable outputs, such as faithfulness errors in text summarization. We show that existing approaches for error tracing, such as gradient-based influence measures, do not perform reliably for detecting faithfulness errors in NLG datasets. We overcome the drawbacks of existing error tracing methods through a new, contrast-based estimate that compares undesired generations to human-corrected outputs. Our proposed method can achieve a mean average precision of 0.93 at detecting known data errors across synthetic tasks with known ground truth, substantially outperforming existing approaches. Using this approach and re-training models on cleaned data leads to a 70% reduction in entity hallucinations on the NYT dataset and a 55% reduction in semantic errors on the E2E dataset.",
            "year": 2022,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a framework to identify and remove low-quality training instances that lead to undesirable outputs, such as faithfulness errors in text summarization, and overcome the drawbacks of existing error tracing methods through a new, contrast-based estimate that compares undesired generations to human-corrected outputs."
            },
            "score": 4
        },
        {
            "id": "90a51ca64fc9bb3b84eb20a8c9d68ad78b49d4b7",
            "paperId": "90a51ca64fc9bb3b84eb20a8c9d68ad78b49d4b7",
            "title": "Trapping LLM Hallucinations Using Tagged Context Prompts",
            "abstract": "Recent advances in large language models (LLMs), such as ChatGPT, have led to highly sophisticated conversation agents. However, these models suffer from\"hallucinations,\"where the model generates false or fabricated information. Addressing this challenge is crucial, particularly with AI-driven platforms being adopted across various sectors. In this paper, we propose a novel method to recognize and flag instances when LLMs perform outside their domain knowledge, and ensuring users receive accurate information. We find that the use of context combined with embedded tags can successfully combat hallucinations within generative language models. To do this, we baseline hallucination frequency in no-context prompt-response pairs using generated URLs as easily-tested indicators of fabricated data. We observed a significant reduction in overall hallucination when context was supplied along with question prompts for tested generative engines. Lastly, we evaluated how placing tags within contexts impacted model responses and were able to eliminate hallucinations in responses with 98.88% effectiveness.",
            "year": 2023,
            "citationCount": 21,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that the use of context combined with embedded tags can successfully combat hallucinations within generative language models and is able to eliminate hallucinations in responses with 98.88% effectiveness."
            },
            "score": 4
        },
        {
            "id": "b3fd9f9245584ee41c0ba005cb262fd8f93ff3b5",
            "paperId": "b3fd9f9245584ee41c0ba005cb262fd8f93ff3b5",
            "title": "Skip \\n: A Simple Method to Reduce Hallucination in Large Vision-Language Models",
            "abstract": "Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination, such as generating text descriptions of objects that are not present in the visual information. However, the underlying fundamental reasons of multimodal hallucinations remain poorly explored. In this paper, we propose a new perspective, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations. Specifically, we systematically identify a semantic shift bias related to paragraph breaks (\\n\\n), where the content before and after '\\n\\n' in the training data frequently exhibit significant semantic changes. This pattern leads the model to infer that the contents following '\\n\\n' should be obviously different from the preceding contents with less hallucinatory descriptions, thereby increasing the probability of hallucinatory descriptions subsequent to the '\\n\\n'. We have validated this hypothesis on multiple publicly available LVLMs. Besides, we find that deliberately inserting '\\n\\n' at the generated description can induce more hallucinations. A simple method is proposed to effectively mitigate the hallucination of LVLMs by skipping the output of '\\n'.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new perspective is proposed, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations, and a simple method is proposed to effectively mitigate the hallucination of LVLMs by skipping the output of '\\n'."
            },
            "score": 4
        },
        {
            "id": "03764434729b83d4f04a8bd02f99f2500cd5bbae",
            "paperId": "03764434729b83d4f04a8bd02f99f2500cd5bbae",
            "title": "Teaching Language Models to Hallucinate Less with Synthetic Tasks",
            "abstract": "Large language models (LLMs) frequently hallucinate on abstractive summarization tasks such as document-based question-answering, meeting summarization, and clinical report generation, even though all necessary information is included in context. However, optimizing LLMs to hallucinate less on these tasks is challenging, as hallucination is hard to efficiently evaluate at each optimization step. In this work, we show that reducing hallucination on a synthetic task can also reduce hallucination on real-world downstream tasks. Our method, SynTra, first designs a synthetic task where hallucinations are easy to elicit and measure. It next optimizes the LLM's system message via prefix-tuning on the synthetic task, and finally transfers the system message to realistic, hard-to-optimize tasks. Across three realistic abstractive summarization tasks, SynTra reduces hallucination for two 13B-parameter LLMs using only a synthetic retrieval task for supervision. We also find that optimizing the system message rather than the model weights can be critical; fine-tuning the entire model on the synthetic task can counterintuitively increase hallucination. Overall, SynTra demonstrates that the extra flexibility of working with synthetic data can help mitigate undesired behaviors in practice.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Across three realistic abstractive summarization tasks, SynTra reduces hallucination for two 13B-parameter LLMs using only a synthetic retrieval task for supervision, demonstrating that the extra flexibility of working with synthetic data can help mitigate undesired behaviors in practice."
            },
            "score": 4
        },
        {
            "id": "705ffeccfde95c3b0723f197c4565f7d3f0451a1",
            "paperId": "705ffeccfde95c3b0723f197c4565f7d3f0451a1",
            "title": "Zero-Resource Hallucination Prevention for Large Language Models",
            "abstract": "The prevalent use of large language models (LLMs) in various domains has drawn attention to the issue of\"hallucination,\"which refers to instances where LLMs generate factually inaccurate or ungrounded information. Existing techniques for hallucination detection in language assistants rely on intricate fuzzy, specific free-language-based chain of thought (CoT) techniques or parameter-based methods that suffer from interpretability issues. Additionally, the methods that identify hallucinations post-generation could not prevent their occurrence and suffer from inconsistent performance due to the influence of the instruction format and model style. In this paper, we introduce a novel pre-detection self-evaluation technique, referred to as SELF-FAMILIARITY, which focuses on evaluating the model's familiarity with the concepts present in the input instruction and withholding the generation of response in case of unfamiliar concepts. This approach emulates the human ability to refrain from responding to unfamiliar topics, thus reducing hallucinations. We validate SELF-FAMILIARITY across four different large language models, demonstrating consistently superior performance compared to existing techniques. Our findings propose a significant shift towards preemptive strategies for hallucination mitigation in LLM assistants, promising improvements in reliability, applicability, and interpretability.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel pre-detection self-evaluation technique, referred to as SELF-FAMILIARITY, which focuses on evaluating the model's familiarity with the concepts present in the input instruction and withholding the generation of response in case of unfamiliar concepts, thus reducing hallucinations."
            },
            "score": 4
        },
        {
            "id": "5efa173323e8850dde3f504a8c023cdbb6309b55",
            "paperId": "5efa173323e8850dde3f504a8c023cdbb6309b55",
            "title": "Self-Specialization: Uncovering Latent Expertise within Large Language Models",
            "abstract": "Recent works have demonstrated the effectiveness of self-alignment in which a large language model is, by itself, aligned to follow general instructions through the automatic generation of instructional data using a handful of human-written seeds. Instead of general alignment, in this work, we focus on self-alignment for expert domain specialization (e.g., biomedicine), discovering it to be very effective for improving zero-shot and few-shot performance in target domains of interest. As a preliminary, we first present the benchmark results of existing aligned models within a specialized domain, which reveals the marginal effect that\"generic\"instruction-following training has on downstream expert domains' performance. To remedy this, we explore self-specialization that leverages domain-specific unlabelled data and a few labeled seeds for the self-alignment process. When augmented with retrieval to reduce hallucination and enhance concurrency of the alignment, self-specialization offers an effective (and efficient) way of\"carving out\"an expert model out of a\"generalist\", pre-trained LLM where different domains of expertise are originally combined in a form of\"superposition\". Our experimental results on a biomedical domain show that our self-specialized model (30B) outperforms its base model, MPT-30B by a large margin and even surpasses larger popular models based on LLaMA-65B, highlighting its potential and practicality for specialization, especially considering its efficiency in terms of data and parameters.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores self-specialization that leverages domain-specific unlabelled data and a few labeled seeds for the self-alignment process, finding it to be very effective for improving zero-shot and few-shot performance in target domains of interest."
            },
            "score": 4
        },
        {
            "id": "be177300487b6d0f25e6cade9a31900454b13281",
            "paperId": "be177300487b6d0f25e6cade9a31900454b13281",
            "title": "FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation",
            "abstract": "Most large language models (LLMs) are trained once and never updated; thus, they lack the ability to dynamically adapt to our ever-changing world. In this work, we perform a detailed study of the factuality of LLM-generated text in the context of answering questions that test current world knowledge. Specifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a diverse range of question and answer types, including questions that require fast-changing world knowledge as well as questions with false premises that need to be debunked. We benchmark a diverse array of both closed and open-source LLMs under a two-mode evaluation procedure that allows us to measure both correctness and hallucination. Through human evaluations involving more than 50K judgments, we shed light on limitations of these models and demonstrate significant room for improvement: for instance, all models (regardless of model size) struggle on questions that involve fast-changing knowledge and false premises. Motivated by these results, we present FreshPrompt, a simple few-shot prompting method that substantially boosts the performance of an LLM on FreshQA by incorporating relevant and up-to-date information retrieved from a search engine into the prompt. Our experiments show that FreshPrompt outperforms both competing search engine-augmented prompting methods such as Self-Ask (Press et al., 2022) as well as commercial systems such as Perplexity.AI. Further analysis of FreshPrompt reveals that both the number of retrieved evidences and their order play a key role in influencing the correctness of LLM-generated answers. Additionally, instructing the LLM to generate concise and direct answers helps reduce hallucination compared to encouraging more verbose answers. To facilitate future work, we release FreshQA at github.com/freshllms/freshqa and commit to updating it at regular intervals.",
            "year": 2023,
            "citationCount": 50,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A detailed study of the factuality of LLM-generated text in the context of answering questions that test current world knowledge and introduces FreshQA, a novel dynamic QA benchmark encompassing a diverse range of question and answer types, including questions that require fast-changing world knowledge as well as questions with false premises that need to be debunked."
            },
            "score": 4
        },
        {
            "id": "658cd67a91da86cf451e6f1b015f762b56015172",
            "paperId": "658cd67a91da86cf451e6f1b015f762b56015172",
            "title": "Detecting and Preventing Hallucinations in Large Vision Language Models",
            "abstract": "Instruction tuned Large Vision Language Models (LVLMs) have significantly advanced in generalizing across a diverse set of multi-modal tasks, especially for Visual Question Answering (VQA). However, generating detailed responses that are visually grounded is still a challenging task for these models. We find that even the current state-of-the-art LVLMs (InstructBLIP) still contain a staggering 30 percent of the hallucinatory text in the form of non-existent objects, unfaithful descriptions, and inaccurate relationships. To address this, we introduce M-HalDetect, a Multimodal Hallucination Detection Dataset that can be used to train and benchmark models for hallucination detection and prevention. M-HalDetect consists of 16k fine-grained annotations on VQA examples, making it the first comprehensive multi-modal hallucination detection dataset for detailed image descriptions. Unlike previous work that only consider object hallucination, we additionally annotate both entity descriptions and relationships that are unfaithful. To demonstrate the potential of this dataset for hallucination prevention, we optimize InstructBLIP through our novel Fine-grained Direct Preference Optimization (FDPO). We also train fine-grained multi-modal reward models from InstructBLIP and evaluate their effectiveness with best-of-n rejection sampling (RS). We perform human evaluation on both FDPO and rejection sampling, and find that they reduce hallucination rates in InstructBLIP by 41% and 55% respectively. We also find that our reward model generalizes to other multi-modal models, reducing hallucinations in LLaVA and mPLUG-OWL by 15% and 57% respectively, and has strong correlation with human evaluated accuracy scores. The dataset is available at https://github.com/hendryx-scale/mhal-detect.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "M-HalDetect is introduced, a Multimodal Hallucination Detection Dataset that can be used to train and benchmark models for hallucination detection and prevention, and is the first comprehensive multi-modal hallucination detection dataset for detailed image descriptions."
            },
            "score": 4
        },
        {
            "id": "56373d3fd0f1354a61f9e577db039cdb187d8d43",
            "paperId": "56373d3fd0f1354a61f9e577db039cdb187d8d43",
            "title": "Evaluating the Factual Consistency of Large Language Models Through News Summarization",
            "abstract": "While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information. To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB(Factual Inconsistency Benchmark) that focuses on the task of summarization. Specifically, our benchmark involves comparing the scores an LLM assigns to a factually consistent versus a factually inconsistent summary for an input news article. For factually consistent summaries, we use human-written reference summaries that we manually verify as factually consistent. To generate summaries that are factually inconsistent, we generate summaries from a suite of summarization models that we have manually annotated as factually inconsistent. A model's factual consistency is then measured according to its accuracy, i.e.\\ the proportion of documents where it assigns a higher score to the factually consistent summary. To validate the usefulness of FIB, we evaluate 23 large language models ranging from 1B to 176B parameters from six different model families including BLOOM and OPT. We find that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries. However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries. We validate design choices in our benchmark including the scoring method and source of distractor summaries. Our code and benchmark data can be found at https://github.com/r-three/fib.",
            "year": 2022,
            "citationCount": 50,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new benchmark called FIB(Factual Inconsistency Benchmark) that focuses on the task of summarization, and finds that existing LLMs generally assign a higher score to factually consistent summaries than tofactually inconsistent summaries."
            },
            "score": 4
        },
        {
            "id": "d998ff93b2e58c9b219a9103f8c2ad714a41e4b9",
            "paperId": "d998ff93b2e58c9b219a9103f8c2ad714a41e4b9",
            "title": "The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency of Language Models",
            "abstract": "Large Language Models (LLMs) make natural interfaces to factual knowledge, but their usefulness is limited by their tendency to deliver inconsistent answers to semantically equivalent questions. For example, a model might predict both\"Anne Redpath passed away in Edinburgh.\"and\"Anne Redpath's life ended in London.\"In this work, we identify potential causes of inconsistency and evaluate the effectiveness of two mitigation strategies: up-scaling and augmenting the LM with a retrieval corpus. Our results on the LLaMA and Atlas models show that both strategies reduce inconsistency while retrieval augmentation is considerably more efficient. We further consider and disentangle the consistency contributions of different components of Atlas. For all LMs evaluated we find that syntactical form and other evaluation task artifacts impact consistency. Taken together, our results provide a better understanding of the factors affecting the factual consistency of language models.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work identifies potential causes of inconsistency and evaluates the effectiveness of two mitigation strategies: up-scaling and augmenting the LM with a retrieval corpus, showing that both strategies reduce inconsistency while retrieval augmentation is considerably more efficient."
            },
            "score": 4
        },
        {
            "id": "f727f928e7e179307d8d4a1da2387393f2bd7915",
            "paperId": "f727f928e7e179307d8d4a1da2387393f2bd7915",
            "title": "Methods for Measuring, Updating, and Visualizing Factual Beliefs in Language Models",
            "abstract": "Language models can memorize a considerable amount of factual information during pretraining that can be elicited through prompting or finetuning models on tasks like question answering. In this paper, we discuss approaches to measuring model factual beliefs, updating incorrect factual beliefs in models, and visualizing graphical relationships between factual beliefs. Our main contributions include: (1) new metrics for evaluating belief-updating methods focusing on the logical consistency of beliefs, (2) a training objective for Sequential, Local, and Generalizing updates (SLAG) that improves the performance of existing hypernetwork approaches, and (3) the introduction of the belief graph, a new form of visualization for language models that shows relationships between stored model beliefs. Our experiments suggest that models show only limited consistency between factual beliefs, but update methods can both fix incorrect model beliefs and greatly improve their consistency. Although off-the-shelf optimizers are surprisingly strong belief-updating baselines, our learned optimizers can outperform them in more difficult settings than have been considered in past work.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experiments suggest that models show only limited consistency between factual beliefs, but update methods can both fix incorrect model beliefs and greatly improve their consistency, and off-the-shelf optimizers can outperform them in more difficult settings than have been considered in past work."
            },
            "score": 4
        },
        {
            "id": "f33d19614d78f4a6e107e768ccd0a2d4244fc89a",
            "paperId": "f33d19614d78f4a6e107e768ccd0a2d4244fc89a",
            "title": "Evaluating the Factual Consistency of Large Language Models Through Summarization",
            "abstract": "While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information. To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB ( F actual I nconsistency B enchmark) that focuses on the task of summarization. Specifically, our benchmark involves comparing the scores an LLM assigns to a factually consistent versus a factually inconsistent summary for an input news article. For factually consistent summaries, we use human-written reference summaries that we manually verify as fac-tually consistent. To generate summaries that are factually inconsistent, we generate summaries from a suite of summarization models that we have manually annotated as fac-tually inconsistent. A model\u2019s factual consistency is then measured according to its accuracy, i.e. the proportion of documents where it assigns a higher score to the factually consistent summary. To validate the usefulness of FIB , we evaluate 23 large language models ranging from 1B to 176B parameters from six different model families including BLOOM and OPT. We \ufb01nd that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries. However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries. We validate design choices in our benchmark including the scoring method and source of distractor summaries. Our code and",
            "year": 2022,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new benchmark called FIB is proposed that focuses on the task of summarization and shows that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries, and that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries."
            },
            "score": 4
        },
        {
            "id": "261549439aebdda72b648ecc462448fd24857ac1",
            "paperId": "261549439aebdda72b648ecc462448fd24857ac1",
            "title": "Progressive-Hint Prompting Improves Reasoning in Large Language Models",
            "abstract": "The performance of Large Language Models (LLMs) in reasoning tasks depends heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency being critical methods that enhance this ability. However, these methods do not fully exploit the answers generated by the LLM to guide subsequent responses. This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers. PHP is orthogonal to CoT and self-consistency, making it easy to combine with state-of-the-art techniques to further improve performance. We conducted extensive and comprehensive experiments on seven benchmarks. The results show that PHP significantly improves accuracy while remaining highly efficient. For instance, with text-davinci-003, we observed a 4.2% improvement on GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction in sample paths with self-consistency. With GPT-4 and PHP, we achieve state-of-the-art performances on SVAMP (89.1% ->91.9%), GSM8K (92% ->95.5%), AQuA (76.4% ->79.9%) and MATH (50.3% ->53.9%).",
            "year": 2023,
            "citationCount": 64,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers."
            },
            "score": 4
        },
        {
            "id": "41a41c75ba336dec98d58c563605f261019e5df0",
            "paperId": "41a41c75ba336dec98d58c563605f261019e5df0",
            "title": "\u201cAccording to . . . \u201d: Prompting Language Models Improves Quoting from Pre-Training Data",
            "abstract": "Large Language Models (LLMs) may hallucinate and generate fake information, despite pre-training on factual data. Inspired by the journalistic device of \u201caccording to sources\u201d, we propose according-to prompting: directing LLMs to ground responses against previously observed text. To quantify this grounding, we propose a novel evaluation metric (QUIP-Score) that measures the extent to which model-produced answers are directly found in underlying text corpora. We illustrate with experiments on three corpora (Wikipedia, PubMed, and the U.S. legal tax code) that these prompts improve grounding under our metrics, with the additional benefit of often improving end-task performance. Furthermore, prompts that ask the model to decrease grounding (or to ground to other corpora) indeed decrease QUIP-Score, indicating the ability of LLMs to increase or decrease grounded generations on request.",
            "year": 2023,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "According-to prompting is proposed: directing LLMs to ground responses against previously observed text, to quantify this grounding, and proposes a novel evaluation metric (QUIP-Score) that measures the extent to which model-produced answers are directly found in underlying text corpora."
            },
            "score": 4
        },
        {
            "id": "8bc313e04cbd39847eb50b22af0a698ff2971a35",
            "paperId": "8bc313e04cbd39847eb50b22af0a698ff2971a35",
            "title": "Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models: A Case Study on ChatGPT",
            "abstract": "Generative large language models (LLMs), e.g., ChatGPT, have demonstrated remarkable proficiency across several NLP tasks, such as machine translation, text summarization. Recent research (Kocmi and Federmann, 2023) has shown that utilizing LLMs for assessing the quality of machine translation (MT) achieves state-of-the-art performance at the system level but \\textit{performs poorly at the segment level}. To further improve the performance of LLMs on MT quality assessment, we investigate several prompting designs, and propose a new prompting method called \\textbf{\\texttt{Error Analysis Prompting}} (EAPrompt) by combining Chain-of-Thoughts (Wei et al., 2022) and Error Analysis (Lu et al., 2023). This technique emulates the commonly accepted human evaluation framework - Multidimensional Quality Metrics (MQM, Freitag et al. (2021)) and \\textit{produces explainable and reliable MT evaluations at both the system and segment level}. Experimental Results from the WMT22 metrics shared task validate the effectiveness of EAPrompt on various LLMs, with different structures. Further analysis confirms that EAPrompt effectively distinguishes major errors from minor ones, while also sharing a similar distribution of the number of errors with MQM. These findings highlight the potential of EAPrompt as a human-like evaluator prompting technique for MT evaluation.",
            "year": 2023,
            "citationCount": 58,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Findings highlight the potential of EAPrompt as a human-like evaluator prompting technique for MT evaluation, and investigate several prompting designs, and propose a new prompting method called EAPrompt by combining Chain-of-Thoughts and Error Analysis."
            },
            "score": 4
        },
        {
            "id": "d5a6fc6aa139066e3b66ba63002e7d84c109aebc",
            "paperId": "d5a6fc6aa139066e3b66ba63002e7d84c109aebc",
            "title": "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing",
            "abstract": "Large language models (LLMs) have shown remarkable capabilities in Natural Language Processing (NLP), especially in domains where labeled data is scarce or expensive, such as clinical domain. However, to unlock the clinical knowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform specific clinical NLP tasks without any task-specific training data. This is known as in-context learning, which is an art and science that requires understanding the strengths and weaknesses of different LLMs and prompt engineering approaches. In this paper, we present a comprehensive and systematic experimental study on prompt engineering for five clinical NLP tasks: Clinical Sense Disambiguation, Biomedical Evidence Extraction, Coreference Resolution, Medication Status Extraction, and Medication Attribute Extraction. We assessed the prompts proposed in recent literature, including simple prefix, simple cloze, chain of thought, and anticipatory prompts, and introduced two new types of prompts, namely heuristic prompting and ensemble prompting. We evaluated the performance of these prompts on three state-of-the-art LLMs: GPT-3.5, BARD, and LLAMA2. We also contrasted zero-shot prompting with few-shot prompting, and provide novel insights and guidelines for prompt engineering for LLMs in clinical NLP. To the best of our knowledge, this is one of the first works on the empirical evaluation of different prompt engineering approaches for clinical NLP in this era of generative AI, and we hope that it will inspire and inform future research in this area.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents a comprehensive and systematic experimental study on prompt engineering for five clinical NLP tasks: Clinical Sense Disambiguation, Biomedical Evidence Extraction, Coreference Resolution, Medication Status Ext extraction, and Medication Attribute Extraction."
            },
            "score": 4
        },
        {
            "id": "8d806a91e5f2166ee6823eb7e6e8e56826b6776d",
            "paperId": "8d806a91e5f2166ee6823eb7e6e8e56826b6776d",
            "title": "NLPBench: Evaluating Large Language Models on Solving NLP Problems",
            "abstract": "Recent developments in large language models (LLMs) have shown promise in enhancing the capabilities of natural language processing (NLP). Despite these successes, there remains a dearth of research dedicated to the NLP problem-solving abilities of LLMs. To fill the gap in this area, we present a unique benchmarking dataset, NLPBench, comprising 378 college-level NLP questions spanning various NLP topics sourced from Yale University's prior final exams. NLPBench includes questions with context, in which multiple sub-questions share the same public information, and diverse question types, including multiple choice, short answer, and math. Our evaluation, centered on LLMs such as GPT-3.5/4, PaLM-2, and LLAMA-2, incorporates advanced prompting strategies like the chain-of-thought (CoT) and tree-of-thought (ToT). Our study reveals that the effectiveness of the advanced prompting strategies can be inconsistent, occasionally damaging LLM performance, especially in smaller models like the LLAMA-2 (13b). Furthermore, our manual assessment illuminated specific shortcomings in LLMs' scientific problem-solving skills, with weaknesses in logical decomposition and reasoning notably affecting results.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study presents a unique benchmarking dataset, NLPBench, comprising 378 college-level NLP questions spanning various NLP topics sourced from Yale University's prior final exams, and reveals that the effectiveness of the advanced prompting strategies can be inconsistent, occasionally damaging LLM performance."
            },
            "score": 4
        },
        {
            "id": "7cfbd36c0043098589cbaf18dca2b41d8dc24abe",
            "paperId": "7cfbd36c0043098589cbaf18dca2b41d8dc24abe",
            "title": "Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre-training",
            "abstract": "Large-scale vision-language pre-trained (VLP) models are prone to hallucinate non-existent visual objects when generating text based on visual information. In this paper, we systematically study the object hallucination problem from three aspects. First, we examine recent state-of-the-art VLP models, showing that they still hallucinate frequently and models achieving better scores on standard metrics (e.g., CIDEr) could be more unfaithful. Second, we investigate how different types of image encoding in VLP influence hallucination, including region-based, grid-based, and patch-based. Surprisingly, we find that patch-based features perform the best and smaller patch resolution yields a non-trivial reduction in object hallucination. Third, we decouple various VLP objectives and demonstrate that token-level image-text alignment and controlled generation are crucial to reducing hallucination. Based on that, we propose a simple yet effective VLP loss named ObjMLM to further mitigate object hallucination. Results show that it reduces object hallucination by up to 17.4% when tested on two benchmarks (COCO Caption for in-domain and NoCaps for out-of-domain evaluation).",
            "year": 2022,
            "citationCount": 35,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a simple yet effective VLP loss named ObjMLM to further mitigate object hallucination and decouple various VLP objectives and demonstrates that token-level image-text alignment and controlled generation are crucial to reducing hallucination."
            },
            "score": 3
        },
        {
            "id": "d31f9151c5d371eedfb64c4d14ab94b86d39e1c5",
            "paperId": "d31f9151c5d371eedfb64c4d14ab94b86d39e1c5",
            "title": "Investigating Hallucinations in Pruned Large Language Models for Abstractive Summarization",
            "abstract": "Despite the remarkable performance of generative large language models (LLMs) on abstractive summarization, they face two significant challenges: their considerable size and tendency to hallucinate. Hallucinations are concerning because they erode reliability and raise safety issues. Pruning is a technique that reduces model size by removing redundant weights, enabling more efficient sparse inference. Pruned models yield downstream task performance comparable to the original, making them ideal alternatives when operating on a limited budget. However, the effect that pruning has upon hallucinations in abstractive summarization with LLMs has yet to be explored. In this paper, we provide an extensive empirical study across five summarization datasets, two state-of-the-art pruning methods, and five instruction-tuned LLMs. Surprisingly, we find that hallucinations from pruned LLMs are less prevalent than the original models. Our analysis suggests that pruned models tend to depend more on the source document for summary generation. This leads to a higher lexical overlap between the generated summary and the source document, which could be a reason for the reduction in hallucination risk.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Surprisingly, it is found that hallucinations from pruned LLMs are less prevalent than the original models, and analysis suggests that pruned models tend to depend more on the source document for summary generation, which could be a reason for the reduction in hallucination risk."
            },
            "score": 3
        },
        {
            "id": "3b0792f6d7f6aa6aadd316e73943116afef2979b",
            "paperId": "3b0792f6d7f6aa6aadd316e73943116afef2979b",
            "title": "Med-HALT: Medical Domain Hallucination Test for Large Language Models",
            "abstract": "This research paper focuses on the challenges posed by hallucinations in large language models (LLMs), particularly in the context of the medical domain. Hallucination, wherein these models generate plausible yet unverified or incorrect information, can have serious consequences in healthcare applications. We propose a new benchmark and dataset, Med-HALT (Medical Domain Hallucination Test), designed specifically to evaluate and reduce hallucinations. Med-HALT provides a diverse multinational dataset derived from medical examinations across various countries and includes multiple innovative testing modalities. Med-HALT includes two categories of tests reasoning and memory-based hallucination tests, designed to assess LLMs\u2019 problem-solving and information retrieval abilities. Our study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2, MPT, and Falcon, revealing significant differences in their performance. The paper provides detailed insights into the dataset, promoting transparency and reproducibility. Through this work, we aim to contribute to the development of safer and more reliable language models in healthcare. Our benchmark can be found at medhalt.github.io",
            "year": 2023,
            "citationCount": 33,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study evaluated leading LLMs, including Text Davinci, GPT-3.5, LlaMa-2, MPT, and Falcon, revealing significant differences in their performance, and proposes a new benchmark and dataset, Med-HALT, designed specifically to evaluate and reduce hallucinations."
            },
            "score": 3
        },
        {
            "id": "06b2ac5153e3d8d05c13c82f93d7f4e13eee6d0f",
            "paperId": "06b2ac5153e3d8d05c13c82f93d7f4e13eee6d0f",
            "title": "Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites",
            "abstract": "Large language models (LLMs) have shown remarkable performance in natural language processing (NLP) tasks. To comprehend and execute diverse human instructions over image data, instruction-tuned large vision-language models (LVLMs) have been introduced. However, LVLMs may suffer from different types of object hallucinations. Nevertheless, LVLMs are evaluated for coarse-grained object hallucinations only (i.e., generated objects non-existent in the input image). The fine-grained object attributes and behaviors non-existent in the image may still be generated but not measured by the current evaluation methods. In this paper, we thus focus on reducing fine-grained hallucinations of LVLMs. We propose \\textit{ReCaption}, a framework that consists of two components: rewriting captions using ChatGPT and fine-tuning the instruction-tuned LVLMs on the rewritten captions. We also propose a fine-grained probing-based evaluation method named \\textit{Fine-Grained Object Hallucination Evaluation} (\\textit{FGHE}). Our experiment results demonstrate that ReCaption effectively reduces fine-grained object hallucination for different LVLM options and improves their text generation quality. The code can be found at https://github.com/Anonymousanoy/FOHE.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ReCaption is proposed, a framework that consists of two components: rewriting captions using ChatGPT and fine-tuning the instruction-tuned LVLMs on the rewritten captions, and a fine-grained probing-based evaluation method named \\textit{Fine-Grained Object Hallucination Evaluation} (FGHE)."
            },
            "score": 3
        },
        {
            "id": "a082c9c93b5cc0d38e7ac14c6c9dfe186bb5c824",
            "paperId": "a082c9c93b5cc0d38e7ac14c6c9dfe186bb5c824",
            "title": "Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance",
            "abstract": "The advancement of Large Vision-Language Models (LVLMs) has increasingly highlighted the critical issue of their tendency to hallucinate non-existing objects in the images. To address this issue, previous works focused on using specially curated datasets or powerful LLMs (e.g., GPT-3.5) to rectify the outputs of LVLMs. However, these approaches require either expensive training/fine-tuning or API access to advanced LLMs to correct the model's output post-generation. In this paper, we tackle this challenge by introducing a framework called Mitigating hallucinAtion via classifieR-Free guIdaNcE (MARINE), which is both training-free and API-free, and can effectively and efficiently reduce object hallucinations during the generation process. Specifically, MARINE enriches the visual context of LVLMs by integrating existing open-source vision models, and employs classifier-free guidance to incorporate the additional object grounding features to improve the precision of LVLMs' generations. Through comprehensive evaluations across $6$ popular LVLMs with diverse evaluation metrics, we demonstrate the effectiveness of MARINE, which even outperforms existing fine-tuning-based methods. Remarkably, it not only reduces hallucinations but also improves the detailedness of LVLMs' generations, as assessed by GPT-4V.",
            "year": 2024,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a framework called Mitigating hallucinAtion via classifieR-Free guIdaNcE (MARINE), which is both training-free and API-free, and can effectively and efficiently reduce object hallucinations during the generation process, and even outperforms existing fine-tuning-based methods."
            },
            "score": 3
        },
        {
            "id": "681406324bdd5a26cbc094d5c74abf9b2cb25d1e",
            "paperId": "681406324bdd5a26cbc094d5c74abf9b2cb25d1e",
            "title": "Quantity Matters: Towards Assessing and Mitigating Number Hallucination in Large Vision-Language Models",
            "abstract": "Large-scale vision-language models have demonstrated impressive skill in handling tasks that involve both areas. Nevertheless, these models frequently experience significant issues with generating inaccurate information, which is hallucination. In this study, we concentrate on a specific type of hallucination-number hallucination, referring to models incorrectly identifying the number of certain objects in pictures. We perform quantitative evaluations regarding number hallucination, showing it to be critical in major open-source large vision-language models. Furthermore, we utilizes two related tasks to conduct an in-depth analysis of number hallucination, revealing the severe inner and outer inconsistency among all tasks. Based on this examination, we devise a training approach aimed at improving consistency to reduce number hallucinations, which leads to an 8% enhancement in performance over direct finetuning methods. Our code and dataset will be released to the community.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study focuses on a specific type of hallucination-number hallucination, referring to models incorrectly identifying the number of certain objects in pictures, and devise a training approach aimed at improving consistency to reduce number hallucinations."
            },
            "score": 3
        },
        {
            "id": "f41977c497c96c1da2e9e945315e9be6d6ad472e",
            "paperId": "f41977c497c96c1da2e9e945315e9be6d6ad472e",
            "title": "Towards reducing hallucination in extracting information from financial reports using Large Language Models",
            "abstract": "For a financial analyst, the question and answer (Q\\&A) segment of the company financial report is a crucial piece of information for various analysis and investment decisions. However, extracting valuable insights from the Q\\&A section has posed considerable challenges as the conventional methods such as detailed reading and note-taking lack scalability and are susceptible to human errors, and Optical Character Recognition (OCR) and similar techniques encounter difficulties in accurately processing unstructured transcript text, often missing subtle linguistic nuances that drive investor decisions. Here, we demonstrate the utilization of Large Language Models (LLMs) to efficiently and rapidly extract information from earnings report transcripts while ensuring high accuracy transforming the extraction process as well as reducing hallucination by combining retrieval-augmented generation technique as well as metadata. We evaluate the outcomes of various LLMs with and without using our proposed approach based on various objective metrics for evaluating Q\\&A systems, and empirically demonstrate superiority of our method.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work demonstrates the utilization of Large Language Models (LLMs) to efficiently and rapidly extract information from earnings report transcripts while ensuring high accuracy transforming the extraction process as well as reducing hallucination by combining retrieval-augmented generation technique aswell as metadata."
            },
            "score": 3
        },
        {
            "id": "328eb183007bf4aefbf42437b42a15db375803e3",
            "paperId": "328eb183007bf4aefbf42437b42a15db375803e3",
            "title": "Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding",
            "abstract": "Large Vision-Language Models (LVLMs) have advanced considerably, intertwining visual recognition and language understanding to generate content that is not only coherent but also contextually attuned. Despite their success, LVLMs still suffer from the issue of object hallucinations, where models generate plausible yet incorrect outputs that include objects that do not exist in the images. To mitigate this issue, we introduce Visual Contrastive Decoding (VCD), a simple and training-free method that contrasts output distributions derived from original and distorted visual inputs. The proposed VCD effectively reduces the over-reliance on statistical bias and unimodal priors, two essential causes of object hallucinations. This adjustment ensures the generated content is closely grounded to visual inputs, resulting in contextually accurate outputs. Our experiments show that VCD, without either additional training or the usage of external tools, significantly mitigates the object hallucination issue across different LVLM families. Beyond mitigating object hallucinations, VCD also excels in general LVLM benchmarks, highlighting its wide-ranging applicability.",
            "year": 2023,
            "citationCount": 25,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The proposed VCD effectively reduces the over-reliance on statistical bias and unimodal priors, two essential causes of object hallucinations, and significantly mitigates the object hallucination issue across different LVLM families."
            },
            "score": 3
        },
        {
            "id": "698d83e2ba10d94c2a0723e907eb297ff4a6249d",
            "paperId": "698d83e2ba10d94c2a0723e907eb297ff4a6249d",
            "title": "HallE-Switch: Rethinking and Controlling Object Existence Hallucinations in Large Vision Language Models for Detailed Caption",
            "abstract": "Current large vision-language models (LVLMs) achieve remarkable progress, yet there remains significant uncertainty regarding their ability to accurately apprehend visual details, that is, in performing detailed captioning. To address this, we introduce CCEval, a GPT-4 assisted evaluation method tailored for detailed captioning. Interestingly, while LVLMs demonstrate minimal object existence hallucination in existing VQA benchmarks, our proposed evaluation reveals continued susceptibility to such hallucinations. In this paper, we make the first attempt to investigate and attribute such hallucinations, including image resolution, the language decoder size, and instruction data amount, quality, granularity. Our findings underscore the unwarranted inference when the language description includes details at a finer object granularity than what the vision module can ground or verify, thus inducing hallucination. To control such hallucinations, we further attribute the reliability of captioning to contextual knowledge (involving only contextually grounded objects) and parametric knowledge (containing inferred objects by the model). Thus, we introduce HallE-Switch, a controllable LVLM in terms of Hallucination in object Existence. HallE-Switch can condition the captioning to shift between (i) exclusively depicting contextual knowledge for grounded objects and (ii) blending it with parametric knowledge to imagine inferred objects. Our method reduces hallucination by 44% compared to LLaVA7B and maintains the same object coverage.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces HallE-Switch, a controllable LVLM in terms of Hallucination in object Existence, and introduces CCEval, a GPT-4 assisted evaluation method tailored for detailed captioning."
            },
            "score": 3
        },
        {
            "id": "152d9a231c00d4495c9bc4a466f42165ce2e2164",
            "paperId": "152d9a231c00d4495c9bc4a466f42165ce2e2164",
            "title": "Evaluating Factual Consistency of Summaries with Large Language Models",
            "abstract": "Detecting factual errors in summaries has been an important and challenging subject in summarization research. Inspired by the emergent ability of large language models (LLMs), we explore evaluating factual consistency of summaries by directly prompting LLMs. We present a comprehensive empirical study to assess the ability of LLMs as factual consistency evaluators, which consists of (1) analyzing different LLMs such as the GPT model series and Flan-T5; (2) investigating a variety of prompting methods including vanilla prompting, chain-of-thought prompting, and a sentence-by-sentence prompting method to tackle long summaries; and (3) evaluating on diverse summaries generated by multiple summarization systems, ranging from pre-transformer methods to SOTA pretrained models. Our experiments demonstrate that prompting LLMs is able to outperform the previous best factuality systems in all settings, by up to 12.2 absolute points in terms of the binary classification accuracy on inconsistency detection.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A comprehensive empirical study to assess the ability of LLMs as factual consistency evaluators, which consists of analyzing different LLMs such as the GPT model series and Flan-T5 and investigating a variety of prompting methods including vanilla prompting, chain-of-thought prompting, and a sentence-by-sentence prompting method to tackle long summaries."
            },
            "score": 3
        },
        {
            "id": "d2d16333a4b0dc7e3463b280b9945e5ee6c53396",
            "paperId": "d2d16333a4b0dc7e3463b280b9945e5ee6c53396",
            "title": "TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models",
            "abstract": "Factual consistency evaluation is often conducted using Natural Language Inference (NLI) models, yet these models exhibit limited success in evaluating summaries. Previous work improved such models with synthetic training data. However, the data is typically based on perturbed human-written summaries, which often differ in their characteristics from real model-generated summaries and have limited coverage of possible factual errors. Alternatively, large language models (LLMs) have recently shown promising results in directly evaluating generative tasks, but are too computationally expensive for practical use. Motivated by these limitations, we introduce TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM. Unlike prior work, TrueTeacher does not rely on human-written summaries, and is multilingual by nature. Experiments on the TRUE benchmark show that a student model trained using our data, substantially outperforms both the state-of-the-art model with similar capacity, and the LLM teacher. In a systematic study, we compare TrueTeacher to existing synthetic data generation methods and demonstrate its superiority and robustness to domain-shift. We also show that our method generalizes to multilingual scenarios. Lastly, we release our large scale synthetic dataset (1.4M examples), generated using TrueTeacher, and a checkpoint trained on this data.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM, which does not rely on human-written summaries, and is multilingual by nature."
            },
            "score": 3
        },
        {
            "id": "e96348576b682e709b2ee06ea28cd81f8bfa102b",
            "paperId": "e96348576b682e709b2ee06ea28cd81f8bfa102b",
            "title": "Exploring the Factual Consistency in Dialogue Comprehension of Large Language Models",
            "abstract": "LLMs (Large Language Models) usually interact with users in the form of dialogue and generate responses following their instructions, which naturally require dialogue comprehension abilities. However, dialogue comprehension is a general language ability which is hard to be evaluated directly. In this work, we propose to perform the evaluation focusing on the factual consistency issue with the help of the dialogue summarization task. Besides evaluating and analyzing the dialogue summarization performance (DIAC-Sum) of different LLMs, we also derive factual questions from the generated summaries and use them as a more flexible measurement of dialogue comprehension (DIAC-QA). Our evaluation shows that, on average, 26.8% of the summaries generated by LLMs contain factual inconsistency. Even ChatGPT, the strongest model evaluated, has such errors in 16% of its summaries. For answering the factual questions, which is more challenging, the average error rate of all evaluated LLMs is 36.1%. Both results indicate serious deficiencies. Detailed analysis shows that the understanding of subject/object of the conversation is still challenging for LLMs. Furthermore, to stimulate and enhance the dialogue comprehension ability of LLMs, we propose a fine-tuning paradigm with auto-constructed multi-task data, which achieved a relative error rate reduction of 11% on DIAC-QA.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "To stimulate and enhance the dialogue comprehension ability of LLMs, a fine-tuning paradigm with auto-constructed multi-task data is proposed, which achieved a relative error rate reduction of 11% on DIAC-QA."
            },
            "score": 3
        },
        {
            "id": "468d1e2d75a23fecaf96fe65d8b01ff35ea5d0bd",
            "paperId": "468d1e2d75a23fecaf96fe65d8b01ff35ea5d0bd",
            "title": "Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models",
            "abstract": "Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts inserted in English reveal a clear pattern whereby the new piece of knowledge transfers only to languages with which English has a high RankC score.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy, and conducts an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level."
            },
            "score": 3
        },
        {
            "id": "36870ac7332b45edf59e5c111dc5bd24139dc1ce",
            "paperId": "36870ac7332b45edf59e5c111dc5bd24139dc1ce",
            "title": "Factual Consistency of Multilingual Pretrained Language Models",
            "abstract": "Pretrained language models can be queried for factual knowledge, with potential applications in knowledge base acquisition and tasks that require inference. However, for that, we need to know how reliable this knowledge is, and recent work has shown that monolingual English language models lack consistency when predicting factual knowledge, that is, they fill-in-the-blank differently for paraphrases describing the same fact. In this paper, we extend the analysis of consistency to a multilingual setting. We introduce a resource, mParaRel, and investigate (i) whether multilingual language models such as mBERT and XLM-R are more consistent than their monolingual counterparts;and (ii) if such models are equally consistent across languages.We find that mBERT is as inconsistent as English BERT in English paraphrases, but that both mBERT and XLM-R exhibit a high degree of inconsistency in English and even more so for all the other 45 languages.",
            "year": 2022,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "MBERT is as inconsistent as English BERT in English paraphrases, but that both mBERT and XLM-R exhibit a high degree of inconsistency in English and even more so for all the other 45 languages."
            },
            "score": 3
        },
        {
            "id": "f3cd3a0e1a0a29193fd88178ef75946dfe756622",
            "paperId": "f3cd3a0e1a0a29193fd88178ef75946dfe756622",
            "title": "Factual Consistency Evaluation of Summarisation in the Era of Large Language Models",
            "abstract": "Factual inconsistency with source documents in automatically generated summaries can lead to misinformation or pose risks. Existing factual consistency(FC) metrics are constrained by their performance, efficiency, and explainability. Recent advances in Large language models (LLMs) have demonstrated remarkable potential in text evaluation but their effectiveness in assessing FC in summarisation remains underexplored. Prior research has mostly focused on proprietary LLMs, leaving essential factors that affect their assessment capabilities unexplored. Additionally, current FC evaluation benchmarks are restricted to news articles, casting doubt on the generality of the FC methods tested on them. In this paper, we first address the gap by introducing TreatFact a dataset of LLM-generated summaries of clinical texts, annotated for FC by domain experts. Moreover, we benchmark 11 LLMs for FC evaluation across news and clinical domains and analyse the impact of model size, prompts, pre-training and fine-tuning data. Our findings reveal that despite proprietary models prevailing on the task, open-source LLMs lag behind. Nevertheless, there is potential for enhancing the performance of open-source LLMs through increasing model size, expanding pre-training data, and developing well-curated fine-tuning data. Experiments on TreatFact suggest that both previous methods and LLM-based evaluators are unable to capture factual inconsistencies in clinical summaries, posing a new challenge for FC evaluation.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "TreatFact, a dataset of LLM-generated summaries of clinical texts, annotated for FC by domain experts, is introduced and it is revealed that despite proprietary models prevailing on the task, open-source LLMs lag behind."
            },
            "score": 3
        },
        {
            "id": "663d743272e9ab04f54d9105a3c3a3f6e22dd1dd",
            "paperId": "663d743272e9ab04f54d9105a3c3a3f6e22dd1dd",
            "title": "FactKB: Generalizable Factuality Evaluation using Language Models Enhanced with Factual Knowledge",
            "abstract": "Evaluating the factual consistency of automatically generated summaries is essential for the progress and adoption of reliable summarization systems. Despite recent advances, existing factuality evaluation models are not robust, being especially prone to entity and relation errors in new domains. We propose FactKB, a simple new approach to factuality evaluation that is generalizable across domains, in particular with respect to entities and relations. FactKB is based on language models pretrained using facts extracted from external knowledge bases. We introduce three types of complementary factuality pretraining objectives based on direct entity facts, facts grounded in auxiliary knowledge about entities, and facts constructed compositionally through knowledge base walks. The resulting factuality evaluation model achieves state-of-the-art performance on two in-domain news summarization benchmarks as well as on three out-of-domain scientific literature datasets. Further analysis of FactKB shows improved ability to detect erroneous entities and relations in summaries and is robust and generalizable across domains.",
            "year": 2023,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "FactKB is a simple new approach to factuality evaluation that is generalizable across domains, in particular with respect to entities and relations and shows improved ability to detect erroneous entities and relation in summaries."
            },
            "score": 3
        },
        {
            "id": "4ea413e5a21a743d68c92e7f169535d0543f6051",
            "paperId": "4ea413e5a21a743d68c92e7f169535d0543f6051",
            "title": "On Improving Summarization Factual Consistency from Natural Language Feedback",
            "abstract": "Despite the recent progress in language generation models, their outputs may not always meet user expectations. In this work, we study whether informational feedback in natural language can be leveraged to improve generation quality and user preference alignment. To this end, we consider factual consistency in summarization, the quality that the summary should only contain information supported by the input documents, as the user-expected preference. We collect a high-quality dataset, DeFacto, containing human demonstrations and informational natural language feedback consisting of corrective instructions, edited summaries, and explanations with respect to the factual consistency of the summary. Using our dataset, we study three natural language generation tasks: (1) editing a summary by following the human feedback, (2) generating human feedback for editing the original summary, and (3) revising the initial summary to correct factual errors by generating both the human feedback and edited summary. We show that DeFacto can provide factually consistent human-edited summaries and further insights into summarization factual consistency thanks to its informational natural language feedback. We further demonstrate that fine-tuned language models can leverage our dataset to improve the summary factual consistency, while large language models lack the zero-shot learning ability in our proposed tasks that require controllable text generation.",
            "year": 2022,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "DeFacto can provide factually consistent human-edited summaries and further insights into summarization factual consistency thanks to its informational natural language feedback, and it is demonstrated that fine-tuned language models can leverage the dataset to improve the summary factual consistency."
            },
            "score": 3
        },
        {
            "id": "e10c2ed8de5568e6a41fb60c2adc4bf9c225da25",
            "paperId": "e10c2ed8de5568e6a41fb60c2adc4bf9c225da25",
            "title": "Is EVALITA Done? On the Impact of Prompting on the Italian NLP Evaluation Campaign",
            "abstract": "Prompt-based learning is a recent paradigm in NLP that leverages large pre-trained language models to perform a variety of tasks. With this technique, it is possible to build classifiers that do not need training data (zero-shot). In this paper, we assess the status of prompt-based learning applied to several text classification tasks in the Italian language. The results indicate that the performance gap towards current supervised methods is still relevant. However, the difference in performance between pre-trained models and the characteristic of the prompt-based classifier of operating in a zero-shot fashion open a discussion regarding the next generation of evaluation campaigns for NLP.",
            "year": 2022,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results indicate that the performance gap towards current supervised methods is still relevant, however, the difference in performance between pre-trained models and the characteristic of the prompt-based classifier of operating in a zero-shot fashion open a discussion regarding the next generation of evaluation campaigns for NLP."
            },
            "score": 3
        },
        {
            "id": "80c698688bb4488beaceaab5c64f701a946cb7ae",
            "paperId": "80c698688bb4488beaceaab5c64f701a946cb7ae",
            "title": "All in One: Multi-Task Prompting for Graph Neural Networks",
            "abstract": "Recently, \"pre-training and fine-tuning'' has been adopted as a standard workflow for many graph tasks since it can take general graph knowledge to relieve the lack of graph annotations from each application. However, graph tasks with node level, edge level, and graph level are far diversified, making the pre-training pretext often incompatible with these multiple tasks. This gap may even cause a \"negative transfer'' to the specific application, leading to poor results. Inspired by the prompt learning in natural language processing (NLP), which has presented significant effectiveness in leveraging prior knowledge for various NLP tasks, we study the prompting topic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models. Specifically, we first unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern. In this way, the prompting idea from NLP can be seamlessly introduced to the graph area. Then, to further narrow the gap between various graph tasks and state-of-the-art pre-training strategies, we further study the task space of various graph applications and reformulate downstream problems to the graph-level task. Afterward, we introduce meta-learning to efficiently learn a better initialization for the multi-task prompt of graphs so that our prompting framework can be more reliable and general for different tasks. We conduct extensive experiments, results from which demonstrate the superiority of our method.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel multi-task prompting method for graph models that unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern, and introduces meta-learning to efficiently learn a better initialization for the multi- task prompt of graphs so that the prompting framework can be more reliable and general for different tasks."
            },
            "score": 3
        },
        {
            "id": "9141480721653789597b6e537ee0eeab401f3e60",
            "paperId": "9141480721653789597b6e537ee0eeab401f3e60",
            "title": "PromptNER: Prompting For Named Entity Recognition",
            "abstract": "In a surprising turn, Large Language Models (LLMs) together with a growing arsenal of prompt-based heuristics now offer powerful off-the-shelf approaches providing few-shot solutions to myriad classic NLP problems. However, despite promising early results, these LLM-based few-shot methods remain far from the state of the art in Named Entity Recognition (NER), where prevailing methods include learning representations via end-to-end structural understanding and fine-tuning on standard labeled corpora. In this paper, we introduce PromptNER, a new state-of-the-art algorithm for few-Shot and cross-domain NER. To adapt to any new NER task PromptNER requires a set of entity definitions in addition to the standard few-shot examples. Given a sentence, PromptNER prompts an LLM to produce a list of potential entities along with corresponding explanations justifying their compatibility with the provided entity type definitions. Remarkably, PromptNER achieves state-of-the-art performance on few-shot NER, achieving a 4% (absolute) improvement in F1 score on the ConLL dataset, a 9% (absolute) improvement on the GENIA dataset, and a 4% (absolute) improvement on the FewNERD dataset. PromptNER also moves the state of the art on Cross Domain NER, outperforming prior methods (including those not limited to the few-shot setting), setting a new mark on 3/5 CrossNER target domains, with an average F1 gain of 3%, despite using less than 2% of the available data.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces PromptNER, a new state-of-the-art algorithm for few-Shot and cross-domain NER, and prompts an LLM to produce a list of potential entities along with corresponding explanations justifying their compatibility with the provided entity type definitions."
            },
            "score": 3
        },
        {
            "id": "c2408a3a8da4f12d3eb156fe359a96b428e5aff1",
            "paperId": "c2408a3a8da4f12d3eb156fe359a96b428e5aff1",
            "title": "Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models",
            "abstract": "In this paper, we take the initiative to investigate the performance of LLMs on complex planning tasks that require LLMs to understand a virtual spatial environment simulated via natural language and act correspondingly in text. We propose a benchmark named Natural Language Planning (NLP) composed of a set of novel tasks: Brick World, NLVR-based Manipulations, and Natural Language Navigation. We found that current popular LLMs such as ChatGPT still lack abilities in complex planning. This arises a question \u2013 do the LLMs have a good understanding of the environments described in natural language, or maybe other alternatives such as symbolic representations are neater and hence better to be understood by LLMs? To this end, we propose a novel method called C O S ( C hain- o f- S ymbol Prompting) that represents the complex environments with condensed symbolic spatial representations during the chained intermediate thinking steps. C O S is easy to use and does not need additional training on LLMs. Extensive experiments indicate that C O S clearly surpasses the performance of the Chain-of-Thought (CoT) Prompting in all three planning tasks with even fewer tokens used in the inputs compared with CoT on ChatGPT and InstructGPT. The performance gain is strong, by up to 60.8% accuracy (from 31.8% to 92.6%) on Brick World for ChatGPT. C O S also reduces the number of tokens in the prompt obviously, by up to 65.8% of the tokens (from 407 to 139) for the intermediate steps from demonstrations on Brick World.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel method called C O S is proposed that represents the complex environments with condensed symbolic spatial representations during the chained intermediate thinking steps that surpasses the performance of the Chain-of-Thought Prompting in all three planning tasks with even fewer tokens used in the inputs."
            },
            "score": 3
        },
        {
            "id": "5bb3bd2ec1e99b11a84ccd0e4dce4bdb2a776a5e",
            "paperId": "5bb3bd2ec1e99b11a84ccd0e4dce4bdb2a776a5e",
            "title": "Training Trajectories of Language Models Across Scales",
            "abstract": "Scaling up language models has led to unprecedented performance gains, but little is understood about how the training dynamics change as models get larger. How do language models of different sizes learn during pre-training? Why do larger language models demonstrate more desirable behaviors? In this paper, we analyze the intermediate training checkpoints of differently sized OPT models (Zhang et al., 2022)\u2014from 125M to 175B parameters\u2014on next-token prediction, sequence-level generation and downstream tasks. We find that 1) at a given perplexity and independent of model sizes, a similar subset of training tokens see the most significant reduction in loss, with the rest stagnating or showing double-descent behavior (Nakkiran et al., 2020); 2) early in training, all models learn to reduce the perplexity of grammatical sequences that contain hallucinations, with small models halting at this suboptimal distribution and larger ones eventually learning to assign these sequences lower probabilities; and 3) perplexity is a strong predictor of in-context learning performance on 74 multiple-choice tasks from BIG-Bench, and this holds independent of the model size. Together, these results show that perplexity is more predictive of model behaviors than model size or training computation.",
            "year": 2022,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Analysis of intermediate training checkpoints of differently sized OPT models shows that perplexity is more predictive of model behaviors than model size or training computation."
            },
            "score": 2
        },
        {
            "id": "b2b8d5181b1206850d86fbb12890f4823a7b05e7",
            "paperId": "b2b8d5181b1206850d86fbb12890f4823a7b05e7",
            "title": "Panel: Multimodal Large Foundation Models",
            "abstract": "The surprisingly fluent predictive performance of LLM (Large Language Models) as well as the high-quality photo-realistic rendering of Diffusion Models has heralded a new beginning in the area of Generative AI. Such kinds of deep learning based models with billions of parameters and pre-trained on massive-scale data-sets are also called Large Foundation Models (LFM). These models not only have caught the public imagination but also have led to an unprecedented surge in interest towards the applications of these models. Instead of the previous approach of developing AI models for specific tasks, more and more researchers are developing large task-agnostic models pre-trained on massive data, which can then be adapted to a variety of downstream tasks via fine-tuning, fewshot learning, or zero-shot learning. Some examples are ChatGPT, LLaMA, GPT-4, Flamingo, MidJourney, Stable-Diffusion and DALLE. Some of them can handle text (e.g., ChatGPT, LLaMA) while some others (e.g., GPT-4 and Flamingo) can utilize multimodal data and can hence be considered Multimodal Large Foundation Models (MLFM). Several recent studies have shown that when adapted to specific tasks (e.g., visual question answering), the foundation models can often surpass the performance of state-of-the-art, fully supervised AI models. However, applying foundation models to specialized domain tasks (e.g., medical diagnosis, financial recommendation etc.) raises many ethical issues (e.g., privacy, model bias or hallucinations). The panel members will discuss the emerging trends in the development and use of large multimodal foundation models. Some of the issues to be discussed are: Research issues in going from LLM to MLFM Behaviour of MLFM Application Potential of MLFM Trust issues in MLFM Limitations of MLFM Societal, Legal and Regulatory issues of MLFM Promising future research in MLFM This panel will bring together several leading experts from universities, research institutions, and industry who will discuss and debate together with the audience. We invite everybody to participate and contribute towards this important and promising research direction.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This panel will bring together several leading experts from universities, research institutions, and industry who will discuss and debate together with the audience the emerging trends in the development and use of large multimodal foundation models."
            },
            "score": 2
        },
        {
            "id": "5cd671efa2af8456c615c5faf54d1be4950f3819",
            "paperId": "5cd671efa2af8456c615c5faf54d1be4950f3819",
            "title": "Hallucination is Inevitable: An Innate Limitation of Large Language Models",
            "abstract": "Hallucination has been widely recognized to be a significant drawback for large language models (LLMs). There have been many works that attempt to reduce the extent of hallucination. These efforts have mostly been empirical so far, which cannot answer the fundamental question whether it can be completely eliminated. In this paper, we formalize the problem and show that it is impossible to eliminate hallucination in LLMs. Specifically, we define a formal world where hallucination is defined as inconsistencies between a computable LLM and a computable ground truth function. By employing results from learning theory, we show that LLMs cannot learn all of the computable functions and will therefore always hallucinate. Since the formal world is a part of the real world which is much more complicated, hallucinations are also inevitable for real world LLMs. Furthermore, for real world LLMs constrained by provable time complexity, we describe the hallucination-prone tasks and empirically validate our claims. Finally, using the formal world framework, we discuss the possible mechanisms and efficacies of existing hallucination mitigators as well as the practical implications on the safe deployment of LLMs.",
            "year": 2024,
            "citationCount": 19,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper formalizes the problem and shows that it is impossible to eliminate hallucination in LLMs, and defines a formal world where hallucination is defined as inconsistencies between a computable LLM and a computable ground truth function."
            },
            "score": 2
        },
        {
            "id": "8e0dbc206db278c29c4b70eae2060db2818f72dd",
            "paperId": "8e0dbc206db278c29c4b70eae2060db2818f72dd",
            "title": "Generalization and Hallucination of Large Vision-Language Models through a Camouflaged Lens",
            "abstract": "Large Vision-Language Model (LVLM) has seen burgeoning development and increasing attention recently. In this paper, we propose a novel framework, camo-perceptive vision-language framework (CPVLF), to explore whether LVLM can generalize to the challenging camouflaged object detection (COD) scenario in a training-free manner. During the process of generalization, we find that due to hallucination issues within LVLM, it can erroneously perceive objects in camouflaged scenes, producing counterfactual concepts. Moreover, as LVLM is not specifically trained for the precise localization of camouflaged objects, it exhibits a degree of uncertainty in accurately pinpointing these objects. Therefore, we propose chain of visual perception, which enhances LVLM's perception of camouflaged scenes from both linguistic and visual perspectives, reducing the hallucination issue and improving its capability in accurately locating camouflaged objects. We validate the effectiveness of CPVLF on three widely used COD datasets, and the experiments show the potential of LVLM in the COD task.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel framework, camo-perceptive vision-language framework (CPVLF), is proposed, which enhances LVLM's perception of camouflaged scenes from both linguistic and visual perspectives, reducing the hallucination issue and improving its capability in accurately locating camouflaged objects."
            },
            "score": 2
        },
        {
            "id": "12c200e731017f9851afb1a6fe3fc7f76e6439c6",
            "paperId": "12c200e731017f9851afb1a6fe3fc7f76e6439c6",
            "title": "Explicit Visual Prompting for Low-Level Structure Segmentations",
            "abstract": "We consider the generic problem of detecting low-level structures in images, which includes segmenting the manipulated parts, identifying out-of-focus pixels, separating shadow regions, and detecting concealed objects. Whereas each such topic has been typically addressed with a domain-specific solution, we show that a unified approach performs well across all of them. We take inspiration from the widely-used pre-training and then prompt tuning protocols in NLP and propose a new visual prompting model, named Explicit Visual Prompting (EVP). Different from the previous visual prompting which is typically a dataset-level implicit embedding, our key insight is to enforce the tunable parameters focusing on the explicit visual content from each individual image, i.e., the features from frozen patch embeddings and the input's high-frequency components. The proposed EVP significantly outperforms other parameter-efficient tuning protocols under the same amount of tunable parameters (5.7% extra trainable parameters of each task). EVP also achieves state-of-the-art performances on diverse low-level structure segmentation tasks compared to task-specific solutions. Our code is available at: https://github.com/NiFangBaAGe/Explicit-Visual-Prompt.",
            "year": 2023,
            "citationCount": 42,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work takes inspiration from the widely-used pre-training and then prompt tuning protocols in NLP and proposes a new visual prompting model, named Explicit Visual Prompting (EVP), which significantly outperforms other parameter-efficient tuning protocols under the same amount of tunable parameters."
            },
            "score": 2
        },
        {
            "id": "98fc8faae839cc9b9dbfcf8e3ec3a7e2f7729db2",
            "paperId": "98fc8faae839cc9b9dbfcf8e3ec3a7e2f7729db2",
            "title": "NOTABLE: Transferable Backdoor Attacks Against Prompt-based NLP Models",
            "abstract": "Prompt-based learning is vulnerable to backdoor attacks. Existing backdoor attacks against prompt-based models consider injecting backdoors into the entire embedding layers or word embedding vectors. Such attacks can be easily affected by retraining on downstream tasks and with different prompting strategies, limiting the transferability of backdoor attacks. In this work, we propose transferable backdoor attacks against prompt-based models, called NOTABLE, which is independent of downstream tasks and prompting strategies. Specifically, NOTABLE injects backdoors into the encoders of PLMs by utilizing an adaptive verbalizer to bind triggers to specific words (i.e., anchors). It activates the backdoor by pasting input with triggers to reach adversary-desired anchors, achieving independence from downstream tasks and prompting strategies. We conduct experiments on six NLP tasks, three popular models, and three prompting strategies. Empirical results show that NOTABLE achieves superior attack performance (i.e., attack success rate over 90% on all the datasets), and outperforms two state-of-the-art baselines. Evaluations on three defenses show the robustness of NOTABLE. Our code can be found at https://github.com/RU-System-Software-and-Security/Notable.",
            "year": 2023,
            "citationCount": 19,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes transferable backdoor attacks against prompt-based models, called NOTABLE, which is independent of downstream tasks and prompting strategies, and achieves superior attack performance and outperforms two state-of-the-art baselines."
            },
            "score": 2
        },
        {
            "id": "0a25c137edc7c9752aa6d99ae4084683c3fe6b56",
            "paperId": "0a25c137edc7c9752aa6d99ae4084683c3fe6b56",
            "title": "Visual Prompting via Image Inpainting",
            "abstract": "How does one adapt a pre-trained visual model to novel downstream tasks without task-specific finetuning or any model modification? Inspired by prompting in NLP, this paper investigates visual prompting: given input-output image example(s) of a new task at test time and a new input image, the goal is to automatically produce the output image, consistent with the given examples. We show that posing this problem as simple image inpainting - literally just filling in a hole in a concatenated visual prompt image - turns out to be surprisingly effective, provided that the inpainting algorithm has been trained on the right data. We train masked auto-encoders on a new dataset that we curated - 88k unlabeled figures from academic papers sources on Arxiv. We apply visual prompting to these pretrained models and demonstrate results on various downstream image-to-image tasks, including foreground segmentation, single object detection, colorization, edge detection, etc.",
            "year": 2022,
            "citationCount": 109,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper investigates visual prompting: given input-output image example(s) of a new task at test time and a new input image, the goal is to automatically produce the output image, consistent with the given examples, and shows that posing this problem as simple image inpainting turns out to be surprisingly effective."
            },
            "score": 2
        },
        {
            "id": "f953dbea152b4160a4fc7463cc6dcd3214111117",
            "paperId": "f953dbea152b4160a4fc7463cc6dcd3214111117",
            "title": "Aligning with Whom? Large Language Models Have Gender and Racial Biases in Subjective NLP Tasks",
            "abstract": "Human perception of language depends on personal backgrounds like gender and ethnicity. While existing studies have shown that large language models (LLMs) hold values that are closer to certain societal groups, it is unclear whether their prediction behaviors on subjective NLP tasks also exhibit a similar bias. In this study, leveraging the POPQUORN dataset which contains annotations of diverse demographic backgrounds, we conduct a series of experiments on four popular LLMs to investigate their capability to understand group differences and potential biases in their predictions for politeness and offensiveness. We find that for both tasks, model predictions are closer to the labels from White and female participants. We further explore prompting with the target demographic labels and show that including the target demographic in the prompt actually worsens the model's performance. More specifically, when being prompted to respond from the perspective of\"Black\"and\"Asian\"individuals, models show lower performance in predicting both overall scores as well as the scores from corresponding groups. Our results suggest that LLMs hold gender and racial biases for subjective NLP tasks and that demographic-infused prompts alone may be insufficient to mitigate such effects. Code and data are available at https://github.com/Jiaxin-Pei/LLM-Group-Bias.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that for both tasks, model predictions are closer to the labels from White and female participants, and that demographic-infused prompts alone may be insufficient to mitigate such effects."
            },
            "score": 2
        },
        {
            "id": "3a22aad6c18a9559be3bbb197494b434b872a05a",
            "paperId": "3a22aad6c18a9559be3bbb197494b434b872a05a",
            "title": "Diffusion-NAT: Self-Prompting Discrete Diffusion for Non-Autoregressive Text Generation",
            "abstract": "Recently, continuous diffusion models (CDM) have been introduced into non-autoregressive (NAR) text-to-text generation. However, the discrete nature of text increases the difficulty of CDM to generate coherent and fluent texts, and also causes the incompatibility problem between CDM and advanced NLP techniques, especially the popular pre-trained language models (PLMs).To solve it, we propose Diffusion-NAT, which introduces discrete diffusion models (DDM) into NAR text-to-text generation and integrates BART to improve the performance.By revising the decoding process of BART and the typical settings of DDM, we unify the inference process of BART and the denoising process of DDM into the same NAR masked tokens recovering task.In this way, DDM can rely on BART to perform denoising, which can benefit from both the rich pre-learned knowledge of BART and the iterative refining paradigm of DDM.Besides, we also propose the iterative self-prompting strategy to further improve the generation quality.Experimental results on 7 datasets show that our approach can outperform competitive NAR methods, and even surpass autoregressive methods.Our code and data are released at https://github.com/RUCAIBox/DiffusionNAT.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Diffusion-NAT is proposed, which introduces discrete diffusion models (DDM) into NAR text-to-text generation and integrates BART to improve the performance and proposes the iterative self-prompting strategy to further improve the generation quality."
            },
            "score": 2
        },
        {
            "id": "472bd5b90c289b715340708536ade437d20b237e",
            "paperId": "472bd5b90c289b715340708536ade437d20b237e",
            "title": "Hybrid Language Models Using Mixed Types of Sub-Lexical Units for Open Vocabulary German LVCSR",
            "abstract": "German is a highly inflected language with a large number of words derived from the same root. It makes use of a high degree of word compounding leading to high Out-of-vocabulary (OOV) rates, and Language Model (LM) perplexities. For such languages the use of sub-lexical units for Large Vocabulary Continuous Speech Recognition (LVCSR) becomes a natural choice. In this paper, we investigate the use of mixed types of sub-lexical units in the same recognition lexicon. Namely, morphemic or syllabic units combined with pronunciations called graphones, normal graphemic morphemes or syllables along with full-words. This mixture of units is used for building hybrid LMs suitable for open vocabulary LVCSR where the system operates over an open, constantly changing vocabulary like in broadcast news, political debates, etc. A relative reduction of around 5.0% in Word Error Rate (WER) is obtained compared to a traditional full-words system. Moreover, around 40% of the OOVs are recognized. Index Terms: open vocabulary, morpheme, syllable, graphone",
            "year": 2011,
            "citationCount": 46,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper investigates the use of mixed types of sub-lexical units in the same recognition lexicon in German, including morphemic or syllabic units combined with pronunciations called graphones, normal graphemic morphemes or syllables along with full-words."
            },
            "score": 1
        },
        {
            "id": "bdfabfdad78df5eab9aaa7cff98e349aa704b1a4",
            "paperId": "bdfabfdad78df5eab9aaa7cff98e349aa704b1a4",
            "title": "VALL-T: Decoder-Only Generative Transducer for Robust and Decoding-Controllable Text-to-Speech",
            "abstract": "Recent TTS models with decoder-only Transformer architecture, such as SPEAR-TTS and VALL-E, achieve impressive naturalness and demonstrate the ability for zero-shot adaptation given a speech prompt. However, such decoder-only TTS models lack monotonic alignment constraints, sometimes leading to hallucination issues such as mispronunciation, word skipping and repeating. To address this limitation, we propose VALL-T, a generative Transducer model that introduces shifting relative position embeddings for input phoneme sequence, explicitly indicating the monotonic generation process while maintaining the architecture of decoder-only Transformer. Consequently, VALL-T retains the capability of prompt-based zero-shot adaptation and demonstrates better robustness against hallucinations with a relative reduction of 28.3% in the word error rate. Furthermore, the controllability of alignment in VALL-T during decoding facilitates the use of untranscribed speech prompts, even in unknown languages. It also enables the synthesis of lengthy speech by utilizing an aligned context window.",
            "year": 2024,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "VALL-T is proposed, a generative Transducer model that introduces shifting relative position embeddings for input phoneme sequence, explicitly indicating the monotonic generation process while maintaining the architecture of decoder-only Transformer."
            },
            "score": 1
        },
        {
            "id": "fa4447a0ad0540c9fecd47ea249b18a973e1f860",
            "paperId": "fa4447a0ad0540c9fecd47ea249b18a973e1f860",
            "title": "Spoofing Presidential Hopefuls: The Roles of Affective Disposition and Positive Emotions in Prompting the Social Transmission of Debate Parody",
            "abstract": "Exploring factors that contribute to the social transmission of debate parody, this study employs the conceptual lenses of affective disposition and discrete emotions. An online experiment was conducted within days of Saturday Night Live \u2019s original airing of its parody of the first presidential debate between Donald Trump and Hillary Clinton. Participants ( N = 472) were randomly assigned to view either the parody of the debate or a non-politics-related parody sketch. The debate parody was significantly more mirth and hope inducing when participants had an unfavorable disposition toward Trump; there was no difference in mirth and hope between the exposure conditions among those who had a more favorable disposition toward Trump. Furthermore, mirth and hope were demonstrated to predict willingness to share the humor. Both positive emotions served as significant mediating mechanisms for debate parody\u2019s relationship with willingness to share, as amplified by one\u2019s negative affective disposition for Trump.",
            "year": 2020,
            "citationCount": 10,
            "tldr": null,
            "score": 1
        },
        {
            "id": "433524f9fb236477f5d9d1437b794fe0c1a9241a",
            "paperId": "433524f9fb236477f5d9d1437b794fe0c1a9241a",
            "title": "A regulatory challenge for natural language processing (NLP)\u2010based tools such as ChatGPT to be legally used for healthcare decisions. Where are we now?",
            "abstract": "In the global debate about the use ofNatural Language Processing (NLP)-based tools such as ChatGPT in healthcare decisions, the question of their use as regulatory-approved Software as Medical Device (SaMD) has not yet been sufficiently clarified. Currently, this discussion is conducted with an astonishing euphoria about countless clinical applications, including their opportunities, but also their pitfalls with potential errors in clinical use.1\u20135 Although the FDA and international regulatory authorities have already issued initial guideline documents for the development and approval of machine learning (ML)/artificial intelligence (AI)-based tools as SaMD, a mandatory regulatory process for NLP-based tools has not yet been fully clarified. ChatGPT therefore plays a special role in these considerations.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 1
        }
    ],
    "novelty": "yes"
}