{
    "topic_description": "novel prompting methods that can improve factuality and reduce hallucination of large language models",
    "idea_name": "Adaptive Prompting",
    "raw_idea": {
        "Problem": "Existing prompting methods often rely on fixed prompts that do not adapt to the specific characteristics or difficulty of the input, leading to suboptimal performance and increased risk of hallucinations.",
        "Existing Methods": "Current prompting approaches typically use a fixed set of prompts for all inputs, regardless of their complexity or domain. Some methods attempt to customize prompts based on input features, but they often require extensive manual engineering or domain-specific knowledge.",
        "Motivation": "Adapting prompts to the specific characteristics of each input can help LLMs generate more accurate and consistent responses. By dynamically selecting or generating prompts based on the input's complexity, domain, or other relevant features, we can guide LLMs to focus on the most important aspects of the problem and reduce the risk of hallucinations.",
        "Proposed Method": "We propose Adaptive Prompting (AP), a dynamic prompting approach that customizes prompts based on the characteristics of each input. The key steps include: 1) Extracting relevant features from the input, such as its complexity, domain, or topic; 2) Selecting or generating a suitable prompt based on these features, using a meta-learning model trained on a diverse set of prompts and their performance on various inputs; 3) Applying the selected prompt to guide the LLM in generating a response; 4) Evaluating the quality of the generated response using metrics that capture factuality, consistency, and relevance; 5) Updating the meta-learning model based on the performance of the selected prompt to improve future prompt selection.",
        "Experiment Plan": "Evaluate AP on diverse benchmarks that cover multiple domains and difficulty levels, such as MultiNLI, DecaNLP, and MMLU. Compare performance with baselines such as fixed prompting and manually engineered prompt selection methods. Assess the factuality, consistency, and relevance of generated responses using metrics like accuracy, F1 score, and human evaluation. Analyze the effectiveness of the meta-learning model in selecting suitable prompts for different types of inputs."
    },
    "full_experiment_plan": {
        "Title": "Adaptive Prompting: Customizing Prompts for Improved Factuality and Reduced Hallucination in Large Language Models",
        "Problem Statement": "Existing prompting methods often rely on fixed prompts that do not adapt to the specific characteristics or difficulty of the input, leading to suboptimal performance and increased risk of hallucinations.",
        "Motivation": "Current prompting approaches typically use a fixed set of prompts for all inputs, regardless of their complexity or domain. Some methods attempt to customize prompts based on input features, but they often require extensive manual engineering or domain-specific knowledge. Adapting prompts to the specific characteristics of each input can help LLMs generate more accurate and consistent responses. By dynamically selecting or generating prompts based on the input's complexity, domain, or other relevant features, we can guide LLMs to focus on the most important aspects of the problem and reduce the risk of hallucinations.",
        "Proposed Method": "We propose Adaptive Prompting (AP), a dynamic prompting approach that customizes prompts based on the characteristics of each input. The key steps include:\n1. Extracting relevant features from the input, such as its complexity, domain, or topic.\n2. Selecting or generating a suitable prompt based on these features, using a meta-learning model trained on a diverse set of prompts and their performance on various inputs.\n3. Applying the selected prompt to guide the LLM in generating a response.\n4. Evaluating the quality of the generated response using metrics that capture factuality, consistency, and relevance.\n5. Updating the meta-learning model based on the performance of the selected prompt to improve future prompt selection.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "Evaluate AP on diverse benchmarks that cover multiple domains and difficulty levels, such as MultiNLI (for natural language inference), DecaNLP (for multi-task question answering), and MMLU (for multi-domain multiple choice questions). These datasets contain inputs with varying complexity and domain, making them suitable for testing the effectiveness of adaptive prompting.",
            "Step 2: Construct Prompts": "1. Collect a diverse set of prompts for each dataset, covering different styles, lengths, and levels of specificity. For example, for MultiNLI, include prompts like \"Determine if the hypothesis is entailed by, contradicted by, or neutral to the premise,\" \"Does the hypothesis follow logically from the premise? Yes, No, or Maybe,\" and \"Premise: [premise] Hypothesis: [hypothesis] The relationship between the premise and hypothesis is:\".\n2. Train a meta-learning model (e.g., a neural network) to predict the performance of each prompt on a given input based on extracted features. The input features can include the input's length, vocabulary, syntactic complexity, domain-specific keywords, and topic distribution. The meta-learning model is trained on a subset of the data, with the goal of minimizing the loss between the predicted and actual performance of each prompt.",
            "Step 3: Select Models": "Evaluate AP on state-of-the-art LLMs, such as GPT-3.5 (text-davinci-002), GPT-4, and PaLM. These models have shown strong performance across various tasks and are widely used in prompting-based approaches.",
            "Step 4: Establish Baselines": "Compare the performance of AP with the following baselines:\n1. Fixed prompting: Use a single, manually designed prompt for all inputs in a dataset.\n2. Random prompting: Randomly select a prompt from the collected set for each input.\n3. Input-dependent prompting: Use heuristics or simple rules to select prompts based on input features (e.g., using shorter prompts for shorter inputs).",
            "Step 5: Evaluate Performance": "1. For each dataset and model, generate responses using AP and the baseline methods.\n2. Evaluate the quality of the generated responses using metrics that capture factuality (e.g., accuracy on fact-checking datasets like MultiFC), consistency (e.g., entailment score between the input and response), and relevance (e.g., ROUGE score between the input and response).\n3. Analyze the effectiveness of the meta-learning model in selecting suitable prompts for different types of inputs by comparing the performance of AP with the baselines across various input characteristics (e.g., complexity, domain).",
            "Step 6: Ablation Studies": "1. Investigate the impact of different input features on the performance of AP by ablating each feature group (e.g., complexity features, domain features) and comparing the results.\n2. Evaluate the sensitivity of AP to the size and diversity of the prompt set by varying the number and types of prompts used in the meta-learning model.\n3. Analyze the effect of the meta-learning model's architecture and training procedure on the performance of AP by experimenting with different model designs and hyperparameters."
        },
        "Test Case Examples": {
            "Test Case 1": {
                "Input": "Premise: The old man walked slowly across the room. Hypothesis: The man moved quickly.",
                "Baseline Prompt": "Determine if the hypothesis is entailed by, contradicted by, or neutral to the premise.",
                "Baseline Output": "Neutral",
                "AP Prompt": "Premise: The old man walked slowly across the room. Hypothesis: The man moved quickly. Does the hypothesis contradict the premise? Yes or No.",
                "AP Output": "Yes",
                "Explanation": "The baseline prompt is too general and does not focus on the key aspect of the input, which is the contradiction between \"slowly\" and \"quickly.\" AP selects a more targeted prompt that directly asks about the contradiction, leading to the correct output."
            },
            "Test Case 2": {
                "Input": "What is the capital of France?",
                "Baseline Prompt": "Answer the following question:",
                "Baseline Output": "Paris is the capital of France.",
                "AP Prompt": "What is the capital city of the country France? Provide a brief, factual answer.",
                "AP Output": "Paris.",
                "Explanation": "The baseline prompt is generic and does not provide any guidance on the desired response format. AP selects a prompt that emphasizes brevity and factuality, resulting in a concise and accurate output."
            }
        },
        "Fallback Plan": "If AP does not outperform the baselines, consider the following alternative approaches:\n1. Analyze the quality of the collected prompts and experiment with different prompt engineering techniques to improve their effectiveness.\n2. Investigate alternative input features or feature extraction methods that may better capture the relevant characteristics of the inputs.\n3. Explore different meta-learning architectures or training procedures to improve the prompt selection model's performance.\n4. Conduct a detailed error analysis to identify the types of inputs or prompts that AP struggles with and use this information to guide further improvements.\nIf AP still does not yield satisfactory results, focus on analyzing the relationship between input characteristics, prompt properties, and output quality to gain insights into the factors that influence the effectiveness of prompting methods. This analysis can inform the design of new prompting approaches or highlight the limitations of current methods."
    },
    "novelty_queries": [
        "KeywordQuery(\"adaptive prompting language models\")",
        "KeywordQuery(\"dynamic prompt selection language models\")",
        "KeywordQuery(\"meta-learning prompt generation language models\")",
        "KeywordQuery(\"input-dependent prompting language models\")",
        "KeywordQuery(\"Adaptive Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "2f83650af54f632be8c1009cc89e1e2a62663a18",
            "paperId": "2f83650af54f632be8c1009cc89e1e2a62663a18",
            "title": "Automatic Prompt Selection for Large Language Models",
            "abstract": "Large Language Models (LLMs) can perform various natural language processing tasks with suitable instruction prompts. However, designing effective prompts manually is challenging and time-consuming. Existing methods for automatic prompt optimization either lack flexibility or efficiency. In this paper, we propose an effective approach to automatically select the optimal prompt for a given input from a finite set of synthetic candidate prompts. Our approach consists of three steps: (1) clustering the training data and generating candidate prompts for each cluster using an LLM-based prompt generator; (2) synthesizing a dataset of input-prompt-output tuples for training a prompt evaluator to rank the prompts based on their relevance to the input; (3) using the prompt evaluator to select the best prompt for a new input at test time. Our approach balances prompt generality-specificity and eliminates the need for resource-intensive training and inference. It demonstrates competitive performance on zero-shot question-answering datasets: GSM8K, MultiArith, and AQuA.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes an effective approach to automatically select the optimal prompt for a given input from a finite set of synthetic candidate prompts and demonstrates competitive performance on zero-shot question-answering datasets: GSM8K, MultiArith, and AQuA."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is improving factuality and reducing hallucination in LLMs by adapting prompts to specific input characteristics. The approach is to extract input features, select suitable prompts based on these features using a meta-learning model, and update the model based on the performance of the selected prompts.\n\nThe research problem in the paper is automatically selecting optimal prompts for LLMs from a set of candidate prompts. The approach is to cluster training data, generate candidate prompts for each cluster, train a prompt evaluator to rank prompts based on input relevance, and use the evaluator to select the best prompt for new inputs.\n\nWhile both works aim to improve LLM performance through prompting, the proposal focuses on adapting prompts to input characteristics to improve factuality, while the paper focuses on automatically selecting optimal prompts from a candidate set to improve efficiency. The methods proposed are also different: the proposal uses a meta-learning model to select prompts based on input features, while the paper uses clustering and a prompt evaluator.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "4610ffb1b016acaa82a2065ffd1a3adbae1ce722",
            "paperId": "4610ffb1b016acaa82a2065ffd1a3adbae1ce722",
            "title": "Large Language Models Are Human-Level Prompt Engineers",
            "abstract": "By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the\"program,\"optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer.",
            "year": 2022,
            "citationCount": 403,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is improving factuality and reducing hallucination in large language models through adaptive prompting based on input characteristics. The approach is to use a meta-learning model to select or generate suitable prompts for each input.\n\nThe research problem in the paper is automatic prompt generation and selection to improve task performance of large language models. The approach is to search over a pool of instruction candidates proposed by a language model to find the one that maximizes a chosen score function.\n\nWhile both works aim to improve the performance of large language models through prompting, the proposal focuses specifically on factuality and hallucination issues, while the paper tackles general task performance. The methods also differ: the proposal uses a meta-learning model for adaptive prompt selection, while the paper searches over a pool of generated prompt candidates.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "717392dac099d1506b766787382d61b277863163",
            "paperId": "717392dac099d1506b766787382d61b277863163",
            "title": "Better Zero-Shot Reasoning with Self-Adaptive Prompting",
            "abstract": "Modern large language models (LLMs) have demonstrated impressive capabilities at sophisticated tasks, often through step-by-step reasoning similar to humans. This is made possible by their strong few and zero-shot abilities -- they can effectively learn from a handful of handcrafted, completed responses (\"in-context examples\"), or are prompted to reason spontaneously through specially designed triggers. Nonetheless, some limitations have been observed. First, performance in the few-shot setting is sensitive to the choice of examples, whose design requires significant human effort. Moreover, given the diverse downstream tasks of LLMs, it may be difficult or laborious to handcraft per-task labels. Second, while the zero-shot setting does not require handcrafting, its performance is limited due to the lack of guidance to the LLMs. To address these limitations, we propose Consistency-based Self-adaptive Prompting (COSP), a novel prompt design method for LLMs. Requiring neither handcrafted responses nor ground-truth labels, COSP selects and builds the set of examples from the LLM zero-shot outputs via carefully designed criteria that combine consistency, diversity and repetition. In the zero-shot setting for three different LLMs, we show that using only LLM predictions, COSP improves performance up to 15% compared to zero-shot baselines and matches or exceeds few-shot baselines for a range of reasoning tasks.",
            "year": 2023,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Consistency-based Self-adaptive Prompting (COSP), a novel prompt design method for LLMs that selects and builds the set of examples from the LLM zero-shot outputs via carefully designed criteria that combine consistency, diversity and repetition."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving factuality and reducing hallucination in large language models through adaptive prompting based on input characteristics. The approach is to use a meta-learning model to select or generate suitable prompts for each input.\n\nThe research problem in the paper is improving zero-shot reasoning performance of large language models. The approach is to select and build in-context examples from the model's own zero-shot outputs using consistency, diversity, and repetition criteria.\n\nWhile both works aim to improve the performance of large language models, the specific research problems and approaches are different. The proposal focuses on factuality and hallucination, while the paper focuses on zero-shot reasoning. The proposal uses adaptive prompting based on input characteristics, while the paper uses self-generated in-context examples.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "930e86d49477c9d3305cd1f9d01b93749f85bb8b",
            "paperId": "930e86d49477c9d3305cd1f9d01b93749f85bb8b",
            "title": "Universal Self-adaptive Prompting",
            "abstract": "A hallmark of modern large language models (LLMs) is their impressive general zero-shot and few-shot abilities, often elicited through in-context learning (ICL) via prompting. However, while highly coveted and being the most general, zero-shot performances in LLMs are still typically weaker due to the lack of guidance and the difficulty of applying existing automatic prompt design methods in general tasks when ground-truth labels are unavailable. In this study, we address this by presenting Universal Self-Adaptive Prompting (USP), an automatic prompt design approach specifically tailored for zero-shot learning (while compatible with few-shot). Requiring only a small amount of unlabeled data and an inference-only LLM, USP is highly versatile: to achieve universal prompting, USP categorizes a possible NLP task into one of the three possible task types and then uses a corresponding selector to select the most suitable queries and zero-shot model-generated responses as pseudo-demonstrations, thereby generalizing ICL to the zero-shot setup in a fully automated way. We evaluate USP with PaLM and PaLM 2 models and demonstrate performances that are considerably stronger than standard zero-shot baselines and often comparable to or even superior to few-shot baselines across more than 40 natural language understanding, natural language generation, and reasoning tasks.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study presents Universal Self-Adaptive Prompting (USP), an automatic prompt design approach specifically tailored for zero-shot learning (while compatible with few-shot) and evaluates USP with PaLM and PaLM 2 models and demonstrates performances that are considerably stronger than standard zero- shot baselines."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving factuality and reducing hallucination in large language models through adaptive prompting based on input characteristics. The approach is to use a meta-learning model to select or generate suitable prompts for each input.\n\nThe research problem in the paper is improving zero-shot performance in large language models through universal self-adaptive prompting. The approach is to categorize tasks into types and use a selector to choose suitable queries and model-generated responses as pseudo-demonstrations.\n\nWhile both works aim to improve prompting for large language models, the proposal focuses on factuality and hallucination issues, while the paper targets zero-shot performance. The proposed approaches also differ, with the proposal using a meta-learning model for prompt selection and the paper using task categorization and a selector for pseudo-demonstrations.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "0aaf7a76507248d80f65b6a49e200d2370bcb2c9",
            "paperId": "0aaf7a76507248d80f65b6a49e200d2370bcb2c9",
            "title": "Self-prompted Chain-of-Thought on Large Language Models for Open-domain Multi-hop Reasoning",
            "abstract": "In open-domain question-answering (ODQA), most existing questions require single-hop reasoning on commonsense. To further extend this task, we officially introduce open-domain multi-hop reasoning (ODMR) by answering multi-hop questions with explicit reasoning steps in open-domain setting. Recently, large language models (LLMs) have found significant utility in facilitating ODQA without external corpus. Furthermore, chain-of-thought (CoT) prompting boosts the reasoning capability of LLMs to a greater extent with manual or automated paradigms. However, existing automated methods lack of quality assurance, while manual approaches suffer from limited scalability and poor diversity, hindering the capabilities of LLMs. In this paper, we propose Self-prompted Chain-of-Thought (SP-CoT), an automated framework to mass-produce high quality CoTs of LLMs, by LLMs and for LLMs. SP-CoT introduces an automated generation pipeline of high quality ODMR datasets, an adaptive sampler for in-context CoT selection and self-prompted inference via in-context learning. Extensive experiments on four multi-hop question-answering benchmarks show that our proposed SP-CoT not only significantly surpasses the previous SOTA methods on large-scale (175B) LLMs, but also nearly doubles the zero-shot performance of small-scale (13B) LLMs. Further analysis reveals the remarkable capability of SP-CoT to elicit direct and concise intermediate reasoning steps by recalling $\\sim$50\\% of intermediate answers on MuSiQue-Ans dataset.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Self-prompted Chain-of-Thought (SP-CoT), an automated framework to mass-produce high quality CoTs of LLMs, by LLMs and forLLMs, introduces an automated generation pipeline of high quality ODMR datasets, an adaptive sampler for in-context CoT selection and self-Prompted inference via in- context learning."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving factuality and reducing hallucination in large language models through adaptive prompting. The approach is to customize prompts based on input characteristics using a meta-learning model.\n\nThe research problem in the paper is open-domain multi-hop reasoning in question-answering. The approach is to automatically generate high-quality chain-of-thought datasets and use them for in-context learning with large language models.\n\nThe proposal focuses on improving factuality and reducing hallucination, while the paper focuses on multi-hop reasoning. The proposal uses adaptive prompting, while the paper uses automatically generated chain-of-thought datasets for in-context learning.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "98251be3c31d5c2363326130c1b315b32b37ed2a",
            "paperId": "98251be3c31d5c2363326130c1b315b32b37ed2a",
            "title": "Meta-Learning of Prompt Generation for Lightweight Prompt Engineering on Language-Model-as-a-Service",
            "abstract": null,
            "year": 2023,
            "citationCount": 0,
            "tldr": null,
            "score": 7,
            "novelty_score": "The project proposal aims to improve the factuality and reduce hallucinations in large language models by adapting prompts based on input characteristics. The proposed method involves extracting relevant features from the input, selecting or generating suitable prompts using a meta-learning model, and updating the model based on the performance of the selected prompt.\n\nThe paper proposes a method for lightweight prompt engineering by meta-learning prompt generation. It aims to reduce the effort required for prompt engineering while maintaining the performance of large language models.\n\nWhile both the project proposal and the paper involve meta-learning and prompt engineering, their research problems and approaches differ. The project proposal focuses on improving factuality and reducing hallucinations by adapting prompts based on input characteristics, while the paper aims to reduce the effort required for prompt engineering by meta-learning prompt generation.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "62176de125738e3b95850d1227bac81fd646b78e",
            "paperId": "62176de125738e3b95850d1227bac81fd646b78e",
            "title": "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models",
            "abstract": "Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, Few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual efforts, Zero-shot-CoT concatenates the target problem statement with \u201cLet\u2019s think step by step\u201d as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.",
            "year": 2023,
            "citationCount": 115,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results over GPT-3 show that the proposed zero-shot prompting consistently outperforms Zero- shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving factuality and reducing hallucination in large language models through adaptive prompting. The approach is to customize prompts based on input characteristics using a meta-learning model.\n\nThe research problem in the paper is improving zero-shot chain-of-thought reasoning in large language models. The approach is to use a two-step plan-and-solve prompting strategy that first devises a plan to divide the task into subtasks and then carries out the subtasks.\n\nWhile both works aim to improve the performance of large language models, the specific research problems and approaches are different. The proposal focuses on factuality and hallucination, while the paper focuses on multi-step reasoning. The proposal uses adaptive prompting, while the paper uses a plan-and-solve prompting strategy.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "b0435af3063195e8ae880489e64ccde64e6d7563",
            "paperId": "b0435af3063195e8ae880489e64ccde64e6d7563",
            "title": "Guiding Large Language Models via Directional Stimulus Prompting",
            "abstract": "We introduce Directional Stimulus Prompting, a novel framework for guiding black-box large language models (LLMs) toward specific desired outputs. Instead of directly adjusting LLMs, our method employs a small tunable policy model (e.g., T5) to generate an auxiliary directional stimulus prompt for each input instance. These directional stimulus prompts act as nuanced, instance-specific hints and clues to guide LLMs in generating desired outcomes, such as including specific keywords in the generated summary. Our approach sidesteps the challenges of direct LLM tuning by optimizing the policy model to explore directional stimulus prompts that align LLMs with desired behaviors. The policy model can be optimized through 1) supervised fine-tuning using labeled data and 2) reinforcement learning from offline or online rewards based on the LLM's output. We assess our method across summarization, dialogue response generation, and chain-of-thought reasoning tasks. Our experiments demonstrate that the framework consistently improves LLMs' (e.g., ChatGPT, Codex, InstructGPT) performance on these supervised tasks using minimal labeled data. Notably, using just 80 dialogues on the MultiWOZ dataset, our approach enhances ChatGPT's performance by an impressive 41.4%, matching or surpassing some fully supervised start-of-the-art models. Additionally, the instance-specific chain-of-thought prompt generated by our approach improves InstructGPT's reasoning accuracy compared to human-crafted or automatically generated prompts. The code and data are publicly available at \\url{https://github.com/Leezekun/Directional-Stimulus-Prompting}.",
            "year": 2023,
            "citationCount": 37,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Directional Stimulus Prompting, a novel framework for guiding black-box large language models (LLMs) toward specific desired outputs, sidesteps the challenges of direct LLM tuning by optimizing the policy model to explore directional stimulus prompts that align LLMs with desired behaviors."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving factuality and reducing hallucination in large language models through adaptive prompting. The approach is to customize prompts based on input characteristics using a meta-learning model.\n\nThe research problem in the paper is guiding large language models toward desired outputs. The approach is to use a small tunable policy model to generate instance-specific directional stimulus prompts.\n\nWhile both works aim to guide LLMs using prompts, the proposal focuses on adapting prompts to input characteristics to improve factuality, while the paper focuses on using a separate model to generate instance-specific prompts to achieve desired outputs.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "8a25739903cab07c74556b8c2d9743749e1be1e5",
            "paperId": "8a25739903cab07c74556b8c2d9743749e1be1e5",
            "title": "AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting",
            "abstract": "With the advent and widespread deployment of Multimodal Large Language Models (MLLMs), the imperative to ensure their safety has become increasingly pronounced. However, with the integration of additional modalities, MLLMs are exposed to new vulnerabilities, rendering them prone to structured-based jailbreak attacks, where semantic content (e.g.,\"harmful text\") has been injected into the images to mislead MLLMs. In this work, we aim to defend against such threats. Specifically, we propose \\textbf{Ada}ptive \\textbf{Shield} Prompting (\\textbf{AdaShield}), which prepends inputs with defense prompts to defend MLLMs against structure-based jailbreak attacks without fine-tuning MLLMs or training additional modules (e.g., post-stage content detector). Initially, we present a manually designed static defense prompt, which thoroughly examines the image and instruction content step by step and specifies response methods to malicious queries. Furthermore, we introduce an adaptive auto-refinement framework, consisting of a target MLLM and a LLM-based defense prompt generator (Defender). These components collaboratively and iteratively communicate to generate a defense prompt. Extensive experiments on the popular structure-based jailbreak attacks and benign datasets show that our methods can consistently improve MLLMs' robustness against structure-based jailbreak attacks without compromising the model's general capabilities evaluated on standard benign tasks. Our code is available at https://github.com/rain305f/AdaShield.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes AdaShield Prompting, which prepends inputs with defense prompts to defend MLLMs against structure-based jailbreak attacks without fine-tuning MLLMs or training additional modules (e.g., post-stage content detector)."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving factuality and reducing hallucination in large language models through adaptive prompting. The approach is to customize prompts based on input characteristics using a meta-learning model.\n\nThe research problem in the paper is defending multimodal large language models against structure-based jailbreak attacks. The approach is to prepend inputs with adaptive defense prompts without fine-tuning the model or training additional modules.\n\nWhile both works involve adaptive prompting, the research problems and approaches are different. The proposal focuses on improving factuality and reducing hallucination, while the paper aims to defend against jailbreak attacks. The proposal customizes prompts based on input characteristics, while the paper prepends inputs with defense prompts.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "b9833a5354be3a0b137ee45a39abfb49b2612129",
            "paperId": "b9833a5354be3a0b137ee45a39abfb49b2612129",
            "title": "A Case-Based Reasoning Framework for Adaptive Prompting in Cross-Domain Text-to-SQL",
            "abstract": ". Recent advancements in Large Language Models (LLMs), such as Codex, ChatGPT and GPT-4 have signi\ufb01cantly impacted the AI community, including Text-to-SQL tasks. Some evaluations and analyses on LLMs show their potential to generate SQL queries but they point out poorly designed prompts (e.g. simplistic construction or random sampling) limit LLMs\u2019 performance and may cause unnecessary or irrelevant outputs. To address these issues, we propose CBR-ApSQL, a Case-Based Reasoning (CBR)-based framework combined with GPT-3.5 for precise control over case-relevant and case-irrelevant knowledge in Text-to-SQL tasks. We design adaptive prompts for \ufb02exibly adjusting inputs for GPT-3.5, which involves (1) adaptively retrieving cases according to the question intention by de-semantizing the input question, and (2) an adaptive fallback mechanism to ensure the informativeness of the prompt, as well as the relevance between cases and the prompt. In the de-semanticization phase, we designed Semantic Domain Relevance Evaluator(SDRE), combined with Poincar\u00b4e detector(mining implicit semantics in hyperbolic space), TextAlign(discovering explicit matches), and Positector (part-of-speech detector). SDRE semantically and syntactically generates in-context exemplar annotations for the new case. On the three cross-domain datasets, our framework outperforms the state-of-the-art(SOTA) model in execution accuracy by 3.7%, 2.5%, and 8.2%, respectively",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "CBR-ApSQL, a Case-Based Reasoning (CBR)-based framework combined with GPT-3.5 for precise control over case-relevant and case-irrelevant knowledge in Text-to-SQL tasks is proposed."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving factuality and reducing hallucination in large language models through adaptive prompting. The approach is to customize prompts based on input characteristics using a meta-learning model.\n\nThe research problem in the paper is improving the performance of Text-to-SQL tasks using large language models. The approach is a case-based reasoning framework combined with GPT-3.5 that adaptively retrieves relevant cases and generates informative prompts.\n\nWhile both works involve adaptive prompting, the proposal focuses on improving factuality and reducing hallucination in general language model outputs, while the paper specifically targets Text-to-SQL tasks. The methods also differ, with the proposal using a meta-learning model and the paper employing case-based reasoning.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "8bbcc26b4b8edf730f1b19997b318e9e63e80159",
            "paperId": "8bbcc26b4b8edf730f1b19997b318e9e63e80159",
            "title": "LLM Instruction-Example Adaptive Prompting (LEAP) Framework for Clinical Relation Extraction",
            "abstract": "Objective: To investigate the demonstration in Large Language Models (LLMs) for clinical relation extraction. We focus on examining two types of adaptive demonstration: instruction adaptive prompting, and example adaptive prompting to understand their impacts and effectiveness. Materials and Methods: The study unfolds in two stages. Initially, we explored a range of demonstration components vital to LLMs clinical data extraction, such as task descriptions and examples, and tested their combinations. Subsequently, we introduced the Instruction-Example Adaptive Prompting (LEAP) Framework, a system that integrates two types of adaptive prompts: one preceding instruction and another before examples. This framework is designed to systematically explore both adaptive task description and adaptive examples within the demonstration. We evaluated the performance of LEAP framework on the DDI and BC5CDR chemical interaction datasets, applying it across LLMs such as Llama2-7b, Llama2-13b, and MedLLaMA_13B. Results: The study revealed that Instruction + Options + Examples and its expanded form substantially raised F1-scores over the standard Instruction + Options mode. LEAP framework excelled, especially with example adaptive prompting that outdid traditional instruction tuning across models. Notably, the MedLLAMA-13b model scored an impressive 95.13 F1 on the BC5CDR dataset with this method. Significant improvements were also seen in the DDI 2013 dataset, confirming the robustness of method in sophisticated data extraction. Conclusion: The LEAP framework presents a promising avenue for refining LLM training strategies, steering away from extensive finetuning towards more contextually rich and dynamic prompting methodologies.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The LEAP framework presents a promising avenue for refining LLM training strategies, steering away from extensive finetuning towards more contextually rich and dynamic prompting methodologies."
            },
            "score": 6
        },
        {
            "id": "22cc37aac18f0d371355baf83bc043f1d141bd0b",
            "paperId": "22cc37aac18f0d371355baf83bc043f1d141bd0b",
            "title": "Large Language Models for Intent-Driven Session Recommendations",
            "abstract": "Intent-aware session recommendation (ISR) is pivotal in discerning user intents within sessions for precise predictions. Traditional approaches, however, face limitations due to their presumption of a uniform number of intents across all sessions. This assumption overlooks the dynamic nature of user sessions, where the number and type of intentions can significantly vary. In addition, these methods typically operate in latent spaces, thus hinder the model's transparency.Addressing these challenges, we introduce a novel ISR approach, utilizing the advanced reasoning capabilities of large language models (LLMs). First, this approach begins by generating an initial prompt that guides LLMs to predict the next item in a session, based on the varied intents manifested in user sessions. Then, to refine this process, we introduce an innovative prompt optimization mechanism that iteratively self-reflects and adjusts prompts. Furthermore, our prompt selection module, built upon the LLMs' broad adaptability, swiftly selects the most optimized prompts across diverse domains. This new paradigm empowers LLMs to discern diverse user intents at a semantic level, leading to more accurate and interpretable session recommendations. Our extensive experiments on three real-world datasets demonstrate the effectiveness of our method, marking a significant advancement in ISR systems.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a novel ISR approach, utilizing the advanced reasoning capabilities of large language models (LLMs) to discern diverse user intents at a semantic level, leading to more accurate and interpretable session recommendations."
            },
            "score": 6
        },
        {
            "id": "07d23fad3cbffefeae63b5410e949715c6c4ae23",
            "paperId": "07d23fad3cbffefeae63b5410e949715c6c4ae23",
            "title": "FedLogic: Interpretable Federated Multi-Domain Chain-of-Thought Prompt Selection for Large Language Models",
            "abstract": "Leveraging ``chain-of-thought (CoT)'' reasoning to elicit rapid and precise responses from large language models (LLMs) is rapidly attracting research interest. A notable challenge here is how to design or select optimal prompts. The process of prompt selection relies on trial and error, involving continuous adjustments and combinations of input prompts by users based on the corresponding new responses generated from LLMs. Furthermore, minimal research has been conducted to explore how LLMs employ the mathematical problem-solving capabilities learned from user interactions to address issues in narrative writing. To improve interpretability and explore the balance principle between generality and personalization under a multi-domain CoT prompt selection scenario, we propose the Federated Logic rule learning approach (FedLogic). We introduce a theoretical formalization and interactive emulation of the multi-domain CoT prompt selection dilemma in the context of federated LLMs. We cast the problem of joint probability modeling as a bilevel program, where the CoT prompt selection intricacy can be likened to a fuzzy score-based rule selection with the LLMs function as rule generators. FedLogic solves this problem through variational expectation maximization (V-EM). In addition, we incorporate two KL-divergence constraints within this probabilistic modeling framework to surmount the intricacies of managing extensive search spaces and accomplishing cross-domain personalization of CoTs. To the best of our knowledge, FedLogic is the first interpretable and principled federated multi-domain CoT prompt selection approach for LLMs.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "FedLogic is the first interpretable and principled federated multi-domain CoT prompt selection approach for LLMs and incorporates two KL-divergence constraints within this probabilistic modeling framework to surmount the intricacies of managing extensive search spaces and accomplishing cross-domain personalization of CoTs."
            },
            "score": 6
        },
        {
            "id": "327e0290fd71609bfc1a30478a95f690668fe622",
            "paperId": "327e0290fd71609bfc1a30478a95f690668fe622",
            "title": "Enhancing Few-shot Text-to-SQL Capabilities of Large Language Models: A Study on Prompt Design Strategies",
            "abstract": "In-context learning (ICL) has emerged as a new approach to various natural language processing tasks, utilizing large language models (LLMs) to make predictions based on context that has been supplemented with a few examples or task-specific instructions. In this paper, we aim to extend this method to question answering tasks that utilize structured knowledge sources, and improve Text-to-SQL systems by exploring various prompt design strategies for employing LLMs. We conduct a systematic investigation into different demonstration selection methods and optimal instruction formats for prompting LLMs in the Text-to-SQL task. Our approach involves leveraging the syntactic structure of an example's SQL query to retrieve demonstrations, and we demonstrate that pursuing both diversity and similarity in demonstration selection leads to enhanced performance. Furthermore, we show that LLMs benefit from database-related knowledge augmentations. Our most effective strategy outperforms the state-of-the-art system by 2.5 points (Execution Accuracy) and the best fine-tuned system by 5.1 points on the Spider dataset. These results highlight the effectiveness of our approach in adapting LLMs to the Text-to-SQL task, and we present an analysis of the factors contributing to the success of our strategy.",
            "year": 2023,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper conducts a systematic investigation into different demonstration selection methods and optimal instruction formats for prompting LLMs in the Text-to-SQL task, and presents an analysis of the factors contributing to the success of this strategy."
            },
            "score": 6
        },
        {
            "id": "12ad6e798487223f4c17aac69c9853bca8bc7830",
            "paperId": "12ad6e798487223f4c17aac69c9853bca8bc7830",
            "title": "Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models",
            "abstract": "Pre-trained vision-language models (e.g., CLIP) have shown promising zero-shot generalization in many downstream tasks with properly designed text prompts. Instead of relying on hand-engineered prompts, recent works learn prompts using the training data from downstream tasks. While effective, training on domain-specific data reduces a model's generalization capability to unseen new domains. In this work, we propose test-time prompt tuning (TPT), a method that can learn adaptive prompts on the fly with a single test sample. For image classification, TPT optimizes the prompt by minimizing the entropy with confidence selection so that the model has consistent predictions across different augmented views of each test sample. In evaluating generalization to natural distribution shifts, TPT improves the zero-shot top-1 accuracy of CLIP by 3.6% on average, surpassing previous prompt tuning approaches that require additional task-specific training data. In evaluating cross-dataset generalization with unseen categories, TPT performs on par with the state-of-the-art approaches that use additional training data. Project page: https://azshue.github.io/TPT.",
            "year": 2022,
            "citationCount": 131,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Test-time prompt tuning (TPT) is proposed, a method that can learn adaptive prompts on the fly with a single test sample and performs on par with the state-of-the-art approaches that use additional training data."
            },
            "score": 6
        },
        {
            "id": "6149600309757f286e29ad3f858ed32b40423908",
            "paperId": "6149600309757f286e29ad3f858ed32b40423908",
            "title": "Retrieval-Based Prompt Selection for Code-Related Few-Shot Learning",
            "abstract": "Large language models trained on massive code corpora can generalize to new tasks without the need for task-specific fine-tuning. In few-shot learning, these models take as input a prompt, composed of natural language instructions, a few instances of task demonstration, and a query and generate an output. However, the creation of an effective prompt for code-related tasks in few-shot learning has received little attention. We present a technique for prompt creation that automatically retrieves code demonstrations similar to the developer task, based on embedding or frequency analysis. We apply our approach, Cedar, to two different programming languages, statically and dynamically typed, and two different tasks, namely, test assertion generation and program repair. For each task, we compare Cedar with state-of-the-art task-specific and fine-tuned models. The empirical results show that, with only a few relevant code demonstrations, our prompt creation technique is effective in both tasks with an accuracy of 76% and 52% for exact matches in test assertion generation and program repair tasks, respectively. For assertion generation, Cedar outperforms existing task-specific and fine-tuned models by 333% and 11%, respectively. For program repair, Cedar yields 189% better accuracy than task-specific models and is competitive with recent fine-tuned models. These findings have practical implications for practitioners, as Cedar could potentially be applied to multilingual and multitask settings without task or language-specific training with minimal examples and effort.",
            "year": 2023,
            "citationCount": 32,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A technique for prompt creation that automatically retrieves code demonstrations similar to the developer task, based on embedding or frequency analysis is presented, which could potentially be applied to multilingual and multitask settings without task or language-specific training with minimal examples and effort."
            },
            "score": 6
        },
        {
            "id": "4a450ddb7bb22ffd482111163eb8e0959676ef91",
            "paperId": "4a450ddb7bb22ffd482111163eb8e0959676ef91",
            "title": "Query-Focused Submodular Demonstration Selection for In-Context Learning in Large Language Models",
            "abstract": "The increase in dataset and parameter size of large language models has given rise to an emergent ability known as In-context Learning (ICL). This approach allows models to perform tasks based on human instructions and a few demonstration examples in a prompt. ICL differs from traditional fine-tuning methods by enabling the adaptation of pretrained models to new tasks without modifying their core parameters or requiring gradient updates. Despite its potential, the intri-cacies of ICL, particularly the methods for choosing effective demonstration examples to enhance predictive performance, are not fully understood, with prior research often relying on random selection. Our research addresses this gap in two ways. Firstly, we advocate the use of query-focused submodular mutual information functions for selecting demonstration examples in ICL. These functions help identify examples that are both diverse and representative, thereby improving few-shot performance in comparison to random and zero-shot baselines. Our experiments validate this approach. Secondly, we introduce an interactive tool to explore the impact of hyperparameters on model performance. These parameters include the quantity and generation methods of demonstration examples, and their influence on data manifolds and clusters. Our results show that carefully chosen examples can lead to performance improvements of up to 20%. For instance, in sentiment classification, we observed an f1-score of 88.35% compared to 51.95%, and in topic classification, 90.56% versus 31.38%.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This research advocates the use of query-focused submodular mutual information functions for selecting demonstration examples in ICL, and introduces an interactive tool to explore the impact of hyperparameters on model performance."
            },
            "score": 6
        },
        {
            "id": "3c414e3125c12dbd23f62e6c1b85c1a4dc9a522e",
            "paperId": "3c414e3125c12dbd23f62e6c1b85c1a4dc9a522e",
            "title": "Boosting Natural Language Generation from Instructions with Meta-Learning",
            "abstract": "Recent work has shown that language models (LMs) trained with multi-task instructional learning (MTIL) can solve diverse NLP tasks in zero- and few-shot settings with improved performance compared to prompt tuning. MTIL illustrates that LMs can extract and use information about the task from instructions beyond the surface patterns of the inputs and outputs. This suggests that meta-learning may further enhance the utilization of instructions for effective task transfer. In this paper we investigate whether meta-learning applied to MTIL can further improve generalization to unseen tasks in a zero-shot setting. Specifically, we propose to adapt meta-learning to MTIL in three directions: 1) Model Agnostic Meta Learning (MAML), 2) Hyper-Network (HNet) based adaptation to generate task specific parameters conditioned on instructions, and 3) an approach combining HNet and MAML. Through extensive experiments on the large scale Natural Instructions V2 dataset, we show that our proposed approaches significantly improve over strong baselines in zero-shot settings. In particular, meta-learning improves the effectiveness of instructions and is most impactful when the test tasks are strictly zero-shot (i.e. no similar tasks in the training set) and are \u201chard\u201d for LMs, illustrating the potential of meta-learning for MTIL for out-of-distribution tasks.",
            "year": 2022,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes to adapt meta-learning to MTIL in three directions: Model Agnostic Meta Learning (MAML), Hyper-Network (HNet) based adaptation to generate task specific parameters conditioned on instructions, and an approach combining HNet and MAML."
            },
            "score": 6
        },
        {
            "id": "1aa206426a20b0b549cda5068b90b8da353b3434",
            "paperId": "1aa206426a20b0b549cda5068b90b8da353b3434",
            "title": "Repository-Level Prompt Generation for Large Language Models of Code",
            "abstract": "With the success of large language models (LLMs) of code and their use as code assistants (e.g. Codex used in GitHub Copilot), techniques for introducing domain-specific knowledge in the prompt design process become important. In this work, we propose a framework called Repo-Level Prompt Generator that learns to generate example-specific prompts using prompt proposals. The prompt proposals take context from the entire repository, thereby incorporating both the structure of the repository and the context from other relevant files (e.g. imports, parent class files). Our technique doesn't require any access to the weights of the LLM, making it applicable in cases where we only have black-box access to the LLM. We conduct experiments on the task of single-line code-autocompletion using code repositories taken from Google Code archives. We demonstrate that an oracle constructed from our prompt proposals gives a remarkably high relative improvement of 36% over Codex, showing the quality of these proposals. Further, we show that when we train a model to predict a prompt proposal, we can achieve significant performance gains over Codex and other baselines. We release our code, data, and trained checkpoints at: \\url{https://github.com/shrivastavadisha/repo_level_prompt_generation}.",
            "year": 2022,
            "citationCount": 52,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a framework called Repo-Level Prompt Generator that learns to generate example-specific prompts using prompt proposals that take context from the entire repository, thereby incorporating both the structure of the repository and the context from other relevant files."
            },
            "score": 6
        },
        {
            "id": "7c4be464e68a11c8f254d9608f31280e9bcda85c",
            "paperId": "7c4be464e68a11c8f254d9608f31280e9bcda85c",
            "title": "CPL: Counterfactual Prompt Learning for Vision and Language Models",
            "abstract": "Prompt tuning is a new few-shot transfer learning technique that only tunes the learnable prompt for pre-trained vision and language models such as CLIP. However, existing prompt tuning methods tend to learn spurious or entangled representations, which leads to poor generalization to unseen concepts.Towards non-spurious and efficient prompt learning from limited examples, this paper presents a novel Counterfactual Prompt Learning (CPL) method for vision and language models, which simultaneously employs counterfactual generation and contrastive learning in a joint optimization framework.Particularly, CPL constructs counterfactual by identifying minimal non-spurious feature change between semantically-similar positive and negative samples that causes concept change, and learns more generalizable prompt representation from both factual and counterfactual examples via contrastive learning. Extensive experiments demonstrate that CPL can obtain superior few-shot performance on different vision and language tasks than previous prompt tuning methods on CLIP. On image classification, we achieve 3.55% average relative improvement on unseen classes across seven datasets; on image-text retrieval and visual question answering, we gain up to 4.09% and 25.08% relative improvements across three few-shot scenarios on unseen test sets respectively.",
            "year": 2022,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel Counterfactual Prompt Learning method for vision and language models, which simultaneously employs counterfactual generation and contrastive learning in a joint optimization framework, and can obtain superior few-shot performance on different vision andlanguage tasks than previous prompt tuning methods on CLIP."
            },
            "score": 6
        },
        {
            "id": "fb49e88c6bd676516898e911e42b4f8479e6f1bf",
            "paperId": "fb49e88c6bd676516898e911e42b4f8479e6f1bf",
            "title": "Ask Me Anything: A simple strategy for prompting language models",
            "abstract": "Large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt that demonstrates how to perform the task and no additional training. Prompting is a brittle process wherein small modifications to the prompt can cause large variations in the model predictions, and therefore significant effort is dedicated towards designing a painstakingly\"perfect prompt\"for a task. To mitigate the high degree of effort involved in prompt-design, we instead ask whether producing multiple effective, yet imperfect, prompts and aggregating them can lead to a high quality prompting strategy. Our observations motivate our proposed prompting method, ASK ME ANYTHING (AMA). We first develop an understanding of the effective prompt formats, finding that question-answering (QA) prompts, which encourage open-ended generation (\"Who went to the park?\") tend to outperform those that restrict the model outputs (\"John went to the park. Output True or False.\"). Our approach recursively uses the LLM itself to transform task inputs to the effective QA format. We apply the collected prompts to obtain several noisy votes for the input's true label. We find that the prompts can have very different accuracies and complex dependencies and thus propose to use weak supervision, a procedure for combining the noisy predictions, to produce the final predictions for the inputs. We evaluate AMA across open-source model families (e.g., EleutherAI, BLOOM, OPT, and T0) and model sizes (125M-175B parameters), demonstrating an average performance lift of 10.2% over the few-shot baseline. This simple strategy enables the open-source GPT-J-6B model to match and exceed the performance of few-shot GPT3-175B on 15 of 20 popular benchmarks. Averaged across these tasks, the GPT-J-6B model outperforms few-shot GPT3-175B. We release our code here: https://github.com/HazyResearch/ama_prompting",
            "year": 2022,
            "citationCount": 117,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work develops an understanding of the effective prompt formats and proposes to use weak supervision, a procedure for combining the noisy predictions, to produce the final predictions for the inputs of a large language model."
            },
            "score": 6
        },
        {
            "id": "af705d648b5b16daa3dcc593bc593f2574d76c07",
            "paperId": "af705d648b5b16daa3dcc593bc593f2574d76c07",
            "title": "Grammar Prompting for Domain-Specific Language Generation with Large Language Models",
            "abstract": "Large language models (LLMs) can learn to perform a wide range of natural language tasks from just a handful of in-context examples. However, for generating strings from highly structured languages (e.g., semantic parsing to complex domain-specific languages), it is challenging for the LLM to generalize from just a few exemplars. We explore $\\textbf{grammar prompting}$ as a simple approach for enabling LLMs to use external knowledge and domain-specific constraints, expressed through a grammar expressed in Backus--Naur Form (BNF), during in-context learning. Grammar prompting augments each demonstration example with a specialized grammar that is minimally sufficient for generating the particular output example, where the specialized grammar is a subset of the full DSL grammar. For inference, the LLM first predicts a BNF grammar given a test input, and then generates the output according to the rules of the grammar. Experiments demonstrate that grammar prompting can enable LLMs to perform competitively on a diverse set of DSL generation tasks, including semantic parsing (SMCalFlow, Overnight, GeoQuery), PDDL planning, and even molecule generation (SMILES).",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experiments demonstrate that grammar prompting can enable LLMs to perform competitively on a diverse set of DSL generation tasks, including semantic parsing, Overnight, GeoQuery, PDDL planning, and even molecule generation (SMILES)."
            },
            "score": 6
        },
        {
            "id": "d235a9085e0543fcbe502fbc269f9a8ee01dcbab",
            "paperId": "d235a9085e0543fcbe502fbc269f9a8ee01dcbab",
            "title": "AdaPrompt: Adaptive Model Training for Prompt-based NLP",
            "abstract": "Prompt-based learning, with its capability to tackle zero-shot and few-shot NLP tasks, has gained much attention in community. The main idea is to bridge the gap between NLP downstream tasks and language modeling (LM), by mapping these tasks into natural language prompts, which are then filled by pre-trained language models (PLMs). However, for prompt learning, there are still two salient gaps between NLP tasks and pretraining. First, prompt information is not necessarily sufficiently present during LM pretraining. Second, task-specific data are not necessarily well represented during pretraining. We address these two issues by proposing AdaPrompt, adaptively retrieving external data for continual pretraining of PLMs by making use of both task and prompt characteristics. In addition, we make use of knowledge in Natural Language Inference models for deriving adaptive verbalizers. Experimental results on five NLP benchmarks show that AdaPrompt can improve over standard PLMs in few-shot settings. In addition, in zero-shot settings, our method outperforms standard prompt-based methods by up to 26.35\\% relative error reduction.",
            "year": 2022,
            "citationCount": 35,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results on five NLP benchmarks show that AdaPrompt can improve over standard PLMs in few-shot settings and make use of knowledge in Natural Language Inference models for deriving adaptive verbalizers."
            },
            "score": 6
        },
        {
            "id": "dfab0f3ee6f47e36cccee145794cd117773e6f73",
            "paperId": "dfab0f3ee6f47e36cccee145794cd117773e6f73",
            "title": "Towards LLM-based Fact Verification on News Claims with a Hierarchical Step-by-Step Prompting Method",
            "abstract": "While large pre-trained language models (LLMs) have shown their impressive capabilities in various NLP tasks, they are still under-explored in the misinformation domain. In this paper, we examine LLMs with in-context learning (ICL) for news claim verification, and find that only with 4-shot demonstration examples, the performance of several prompting methods can be comparable with previous supervised models. To further boost performance, we introduce a Hierarchical Step-by-Step (HiSS) prompting method which directs LLMs to separate a claim into several subclaims and then verify each of them via multiple questions-answering steps progressively. Experiment results on two public misinformation datasets show that HiSS prompting outperforms state-of-the-art fully-supervised approach and strong few-shot ICL-enabled baselines.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Hierarchical Step-by-Step (HiSS) prompting method is introduced which directs LLMs to separate a claim into several subclaims and then verify each of them via multiple questions-answering steps progressively."
            },
            "score": 6
        },
        {
            "id": "9fe222cb8464e8157b3654ab96ef719331dd2357",
            "paperId": "9fe222cb8464e8157b3654ab96ef719331dd2357",
            "title": "Meta-Learning the Difference: Preparing Large Language Models for Efficient Adaptation",
            "abstract": "Abstract Large pretrained language models (PLMs) are often domain- or task-adapted via finetuning or prompting. Finetuning requires modifying all of the parameters and having enough data to avoid overfitting while prompting requires no training and few examples but limits performance. Instead, we prepare PLMs for data- and parameter-efficient adaptation by learning to learn the difference between general and adapted PLMs. This difference is expressed in terms of model weights and sublayer structure through our proposed dynamic low-rank reparameterization and learned architecture controller. Experiments on few-shot dialogue completion, low-resource abstractive summarization, and multi-domain language modeling show improvements in adaptation time and performance over direct finetuning or preparation via domain-adaptive pretraining. Ablations show our task-adaptive reparameterization (TARP) and model search (TAMS) components individually improve on other parameter-efficient transfer like adapters and structure-learning methods like learned sparsification.",
            "year": 2022,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experiments on few-shot dialogue completion, low-resource abstractive summarization, and multi-domain language modeling show improvements in adaptation time and performance over direct finetuning or preparation via domain-adaptive pretraining."
            },
            "score": 5
        },
        {
            "id": "a679c736d26dbcab41b483f2dbbc417da62c7a16",
            "paperId": "a679c736d26dbcab41b483f2dbbc417da62c7a16",
            "title": "UP-DP: Unsupervised Prompt Learning for Data Pre-Selection with Vision-Language Models",
            "abstract": "In this study, we investigate the task of data pre-selection, which aims to select instances for labeling from an unlabeled dataset through a single pass, thereby optimizing performance for undefined downstream tasks with a limited annotation budget. Previous approaches to data pre-selection relied solely on visual features extracted from foundation models, such as CLIP and BLIP-2, but largely ignored the powerfulness of text features. In this work, we argue that, with proper design, the joint feature space of both vision and text can yield a better representation for data pre-selection. To this end, we introduce UP-DP, a simple yet effective unsupervised prompt learning approach that adapts vision-language models, like BLIP-2, for data pre-selection. Specifically, with the BLIP-2 parameters frozen, we train text prompts to extract the joint features with improved representation, ensuring a diverse cluster structure that covers the entire dataset. We extensively compare our method with the state-of-the-art using seven benchmark datasets in different settings, achieving up to a performance gain of 20%. Interestingly, the prompts learned from one dataset demonstrate significant generalizability and can be applied directly to enhance the feature extraction of BLIP-2 from other datasets. To the best of our knowledge, UP-DP is the first work to incorporate unsupervised prompt learning in a vision-language model for data pre-selection.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "UP-DP is the first work to incorporate unsupervised prompt learning in a vision-language model for data pre-selection and demonstrates significant generalizability and can be applied directly to enhance the feature extraction of BLIP-2 from other datasets."
            },
            "score": 5
        },
        {
            "id": "b879450f50a6113f44a5baf0bcd5b4331eeb7bbc",
            "paperId": "b879450f50a6113f44a5baf0bcd5b4331eeb7bbc",
            "title": "Conditional Prompt Learning for Vision-Language Models",
            "abstract": "With the rise of powerful pre-trained vision-language models like CLIP, it becomes essential to investigate ways to adapt these models to downstream datasets. A recently proposed method named Context Optimization (CoOp) introduces the concept of prompt learning\u2014a recent trend in NLP\u2014to the vision domain for adapting pre-trained vision-language models. Specifically, CoOp turns context words in a prompt into a set of learnable vectors and, with only a few labeled images for learning, can achieve huge improvements over intensively-tuned manual prompts. In our study we identify a critical problem of CoOp: the learned context is not generalizable to wider unseen classes within the same dataset, suggesting that CoOp overfits base classes observed during training. To address the problem, we propose Conditional Context Optimization (CoCoOp), which extends CoOp by further learning a lightweight neural network to generate for each image an input-conditional token (vector). Compared to CoOp's static prompts, our dynamic prompts adapt to each instance and are thus less sensitive to class shift. Extensive experiments show that CoCoOp generalizes much better than CoOp to unseen classes, even showing promising transferability beyond a single dataset; and yields stronger domain generalization performance as well. Code is available at https://github.com/KaiyangZhou/CoOp.",
            "year": 2022,
            "citationCount": 644,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Conditional Context Optimization (CoCoOp), which extends CoOp by further learning a lightweight neural network to generate for each image an input-conditional token (vector), and yields stronger domain generalization performance as well."
            },
            "score": 5
        },
        {
            "id": "e034b664362640cc88dfa5e2e11c0524fc7cb323",
            "paperId": "e034b664362640cc88dfa5e2e11c0524fc7cb323",
            "title": "Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation",
            "abstract": "Large language models (LLMs) have emerged as a new paradigm for Text-to-SQL task. However, the absence of a systematical benchmark inhibits the development of designing effective, efficient and economic LLM-based Text-to-SQL solutions. To address this challenge, in this paper, we first conduct a systematical and extensive comparison over existing prompt engineering methods, including question representation, example selection and example organization, and with these experimental results, we elaborate their pros and cons. Based on these findings, we propose a new integrated solution, named DAIL-SQL, which refreshes the Spider leaderboard with 86.6% execution accuracy and sets a new bar. To explore the potential of open-source LLM, we investigate them in various scenarios, and further enhance their performance with supervised fine-tuning. Our explorations highlight open-source LLMs' potential in Text-to-SQL, as well as the advantages and disadvantages of the supervised fine-tuning. Additionally, towards an efficient and economic LLM-based Text-to-SQL solution, we emphasize the token efficiency in prompt engineering and compare the prior studies under this metric. We hope that our work provides a deeper understanding of Text-to-SQL with LLMs, and inspires further investigations and broad applications.",
            "year": 2023,
            "citationCount": 41,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A systematical and extensive comparison over existing prompt engineering methods, including question representation, example selection and example organization, are conducted, and with these experimental results, their pros and cons are elaborated."
            },
            "score": 5
        },
        {
            "id": "dcd40f8c4828e125591a74d9d00dff81f1bfe90d",
            "paperId": "dcd40f8c4828e125591a74d9d00dff81f1bfe90d",
            "title": "Active Prompt Learning in Vision Language Models",
            "abstract": "Pre-trained Vision Language Models (VLMs) have demonstrated notable progress in various zero-shot tasks, such as classification and retrieval. Despite their performance, because improving performance on new tasks requires task-specific knowledge, their adaptation is essential. While labels are needed for the adaptation, acquiring them is typically expensive. To overcome this challenge, active learning, a method of achieving a high performance by obtaining labels for a small number of samples from experts, has been studied. Active learning primarily focuses on selecting unlabeled samples for labeling and leveraging them to train models. In this study, we pose the question,\"how can the pre-trained VLMs be adapted under the active learning framework?\"In response to this inquiry, we observe that (1) simply applying a conventional active learning framework to pre-trained VLMs even may degrade performance compared to random selection because of the class imbalance in labeling candidates, and (2) the knowledge of VLMs can provide hints for achieving the balance before labeling. Based on these observations, we devise a novel active learning framework for VLMs, denoted as PCB. To assess the effectiveness of our approach, we conduct experiments on seven different real-world datasets, and the results demonstrate that PCB surpasses conventional active learning and random sampling methods. Code will be available in https://github.com/kaist-dmlab/pcb .",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel active learning framework for VLMs is devised, denoted as PCB, and the results demonstrate that PCB surpasses conventional active learning and random sampling methods."
            },
            "score": 5
        },
        {
            "id": "43ec80eeb6f22431ae741796996b25ca3b6bf3e2",
            "paperId": "43ec80eeb6f22431ae741796996b25ca3b6bf3e2",
            "title": "Adapting Pre-trained Language Models to Vision-Language Tasks via Dynamic Visual Prompting",
            "abstract": "Pre-trained language models (PLMs) have played an increasing role in multimedia research. In terms of vision-language (VL) tasks, they often serve as a language encoder and still require an additional fusion network for VL reasoning, resulting in excessive memory overhead. In this paper, we focus on exploring PLMs as a stand-alone model for VL reasoning tasks. Inspired by the recently popular prompt tuning, we first prove that the processed visual features can be also projected onto the semantic space of PLMs and act as prompt tokens to bridge the gap between single- and multi-modal learning. However, this solution exhibits obvious redundancy in visual information and model inference, and the placement of prompt tokens also greatly affects the final performance. Based on these observations, we further propose a novel transfer learning approach for PLMs, termed Dynamic Visual Prompting (DVP). Concretely, DVP first deploys a cross-attention module to obtain text-related and compact visual prompt tokens, thereby greatly reducing the input length of PLMs. To obtain the optimal placement, we also equip DVP with a reinforcement-learning based search algorithm, which can automatically merge DVP with PLMs for different VL tasks via a very short search process. In addition, we also experiment DVP with the recently popular adapter approach to keep the most parameters of PLMs intact when adapting to VL tasks, helping PLMs achieve a quick shift between single- and multi-modal tasks. We apply DVP to two representative PLMs, namely BERT and T5, and conduct extensive experiments on a set of VL reasoning benchmarks including VQA2.0, GQA and SNLIVE. The experimental results not only show the advantage of DVP on efficiency and performance, but also confirm its superiority in adapting pre-trained language models to VL tasks.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results not only show the advantage of DVP on efficiency and performance, but also confirm its superiority in adapting pre-trained language models to VL tasks."
            },
            "score": 5
        },
        {
            "id": "59ef1b67c5f238d5d6d175d84fb6b239b4221a97",
            "paperId": "59ef1b67c5f238d5d6d175d84fb6b239b4221a97",
            "title": "Stabilized In-Context Learning with Pre-trained Language Models for Few Shot Dialogue State Tracking",
            "abstract": "Prompt-based methods with large pre-trained language models (PLMs) have shown impressive unaided performance across many NLP tasks. These models improve even further with the addition of a few labeled in-context exemplars to guide output generation. However, for more complex tasks such as dialogue state tracking (DST), designing prompts that reliably convey the desired intent is nontrivial, leading to unstable results. Furthermore, building in-context exemplars for dialogue tasks is difficult because conversational contexts are long while model input lengths are relatively short.To overcome these issues we first adapt a meta-learning scheme to the dialogue domain which stabilizes the ability of the model to perform well under various prompts. We additionally design a novel training method to improve upon vanilla retrieval mechanisms to find ideal in-context examples. Finally, we introduce a saliency model to limit dialogue text length, allowing us to include more exemplars per query. In effect, we are able to achieve highly competitive results for few-shot DST on MultiWOZ.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A meta-learning scheme is adapted to the dialogue domain which stabilizes the ability of the model to perform well under various prompts and introduces a saliency model to limit dialogue text length, allowing for highly competitive results for few-shot DST on MultiWOZ."
            },
            "score": 5
        },
        {
            "id": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
            "paperId": "d766bffc357127e0dc86dd69561d5aeb520d6f4c",
            "title": "Training language models to follow instructions with human feedback",
            "abstract": "Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",
            "year": 2022,
            "citationCount": 5935,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent and showing improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets."
            },
            "score": 5
        },
        {
            "id": "44d74b0d77b4056ddd4c6611a76711c8bab2e0a7",
            "paperId": "44d74b0d77b4056ddd4c6611a76711c8bab2e0a7",
            "title": "Dehallucinating Large Language Models Using Formal Methods Guided Iterative Prompting",
            "abstract": "Large language models (LLMs) such as ChatGPT have been trained to generate human-like responses to natural language prompts. LLMs use a vast corpus of text data for training, and can generate coherent and contextually relevant responses to a wide range of questions and statements. Despite this remarkable progress, LLMs are prone to hallucinations making their application to safety-critical applications such as autonomous systems difficult. The hallucinations in LLMs refer to instances where the model generates responses that are not factually accurate or contextually appropriate. These hallucinations can occur due to a variety of factors, such as the model\u2019s lack of real-world knowledge, the influence of biased or inaccurate training data, or the model\u2019s tendency to generate responses based on statistical patterns rather than a true understanding of the input. While these hallucinations are a nuisance in tasks such as text summarization and question-answering, they can be catastrophic when LLMs are used in autonomy-relevant applications such as planning. In this paper, we focus on the application of LLMs in autonomous systems and sketch a novel self-monitoring and iterative prompting architecture that uses formal methods to detect these errors in the LLM response automatically. We exploit the dialog capability of LLMs to iteratively steer them to responses that are consistent with our correctness specification. We report preliminary experiments that show the promise of the proposed approach on tasks such as automated planning.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper sketches a novel self-monitoring and iterative prompting architecture that uses formal methods to detect errors in the LLM response automatically and exploits the dialog capability of LLMs to iteratively steer them to responses that are consistent with the correctness specification."
            },
            "score": 5
        },
        {
            "id": "97992c13baa6185c03d9e672f53185bc59822596",
            "paperId": "97992c13baa6185c03d9e672f53185bc59822596",
            "title": "Chain-of-Dictionary Prompting Elicits Translation in Large Language Models",
            "abstract": "Large language models (LLMs) have shown surprisingly good performance in multilingual neural machine translation (MNMT) even when trained without parallel data. Yet, despite the fact that the amount of training data is gigantic, they still struggle with translating rare words, particularly for low-resource languages. Even worse, it is usually unrealistic to retrieve relevant demonstrations for in-context learning with low-resource languages on LLMs, which restricts the practical use of LLMs for translation -- how should we mitigate this problem? To this end, we present a novel method, CoD, which augments LLMs with prior knowledge with the chains of multilingual dictionaries for a subset of input words to elicit translation abilities for LLMs. Extensive experiments indicate that augmenting ChatGPT with CoD elicits large gains by up to 13x chrF++ points for MNMT (3.08 to 42.63 for English to Serbian written in Cyrillic script) on FLORES-200 full devtest set. We further demonstrate the importance of chaining the multilingual dictionaries, as well as the superiority of CoD to few-shot demonstration for low-resource languages.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel method, CoD, is presented, which augments LLMs with prior knowledge with the chains of multilingual dictionaries for a subset of input words to elicit translation abilities for LLMs, and indicates that augmenting ChatGPT with CoD elicits large gains for MNMT."
            },
            "score": 5
        },
        {
            "id": "12c826f4195da172b212a529f8fcf10cc79e35da",
            "paperId": "12c826f4195da172b212a529f8fcf10cc79e35da",
            "title": "Context-faithful Prompting for Large Language Models",
            "abstract": "Large language models (LLMs) encode parametric knowledge about world facts and have shown remarkable performance in knowledge-driven NLP tasks. However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks). In this paper, we seek to assess and enhance LLMs' contextual faithfulness in two aspects: knowledge conflict and prediction with abstention. We demonstrate that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies. In particular, we identify opinion-based prompts and counterfactual demonstrations as the most effective methods. Opinion-based prompts reframe the context as a narrator's statement and inquire about the narrator's opinions, while counterfactual demonstrations use instances containing false facts to improve faithfulness in knowledge conflict situations. Neither technique requires additional training. We conduct experiments on three datasets of two standard NLP tasks, machine reading comprehension and relation extraction, and the results demonstrate significant improvement in faithfulness to contexts. Code and data are released at https://github.com/wzhouad/context-faithful-llm.",
            "year": 2023,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies, and opinion-based prompts and counterfactual demonstrations are identified as the most effective methods."
            },
            "score": 5
        },
        {
            "id": "8fcc5dc9563d3b0d2b88987e8a8c4710e800aab2",
            "paperId": "8fcc5dc9563d3b0d2b88987e8a8c4710e800aab2",
            "title": "Making Large Language Models Better Knowledge Miners for Online Marketing with Progressive Prompting Augmentation",
            "abstract": "Nowadays, the rapid development of mobile economy has promoted the flourishing of online marketing campaigns, whose success greatly hinges on the efficient matching between user preferences and desired marketing campaigns where a well-established Marketing-oriented Knowledge Graph (dubbed as MoKG) could serve as the critical\"bridge\"for preference propagation. In this paper, we seek to carefully prompt a Large Language Model (LLM) with domain-level knowledge as a better marketing-oriented knowledge miner for marketing-oriented knowledge graph construction, which is however non-trivial, suffering from several inevitable issues in real-world marketing scenarios, i.e., uncontrollable relation generation of LLMs,insufficient prompting ability of a single prompt, the unaffordable deployment cost of LLMs. To this end, we propose PAIR, a novel Progressive prompting Augmented mIning fRamework for harvesting marketing-oriented knowledge graph with LLMs. In particular, we reduce the pure relation generation to an LLM based adaptive relation filtering process through the knowledge-empowered prompting technique. Next, we steer LLMs for entity expansion with progressive prompting augmentation,followed by a reliable aggregation with comprehensive consideration of both self-consistency and semantic relatedness. In terms of online serving, we specialize in a small and white-box PAIR (i.e.,LightPAIR),which is fine-tuned with a high-quality corpus provided by a strong teacher-LLM. Extensive experiments and practical applications in audience targeting verify the effectiveness of the proposed (Light)PAIR.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes PAIR, a novel Progressive prompting Augmented Augmented prompting technique for harvesting marketing-oriented knowledge graph with LLMs, and reduces the pure relation generation to an LLM based adaptive relation filtering process through the knowledge-empowered prompting technique."
            },
            "score": 4
        },
        {
            "id": "4e3c65511292a800b17be6653bd057e7a545a0b0",
            "paperId": "4e3c65511292a800b17be6653bd057e7a545a0b0",
            "title": "An Empirical Evaluation of Using Large Language Models for Automated Unit Test Generation",
            "abstract": "Unit tests play a key role in ensuring the correctness of software. However, manually creating unit tests is a laborious task, motivating the need for automation. Large Language Models (LLMs) have recently been applied to various aspects of software development, including their suggested use for automated generation of unit tests, but while requiring additional training or few-shot learning on examples of existing tests. This paper presents a large-scale empirical evaluation on the effectiveness of LLMs for automated unit test generation without requiring additional training or manual effort. Concretely, we consider an approach where the LLM is provided with prompts that include the signature and implementation of a function under test, along with usage examples extracted from documentation. Furthermore, if a generated test fails, our approach attempts to generate a new test that fixes the problem by re-prompting the model with the failing test and error message. We implement our approach in <sc>TestPilot</sc>, an adaptive LLM-based test generation tool for JavaScript that automatically generates unit tests for the methods in a given project's API. We evaluate <sc>TestPilot</sc> using OpenAI's <italic>gpt3.5-turbo</italic> LLM on 25 npm packages with a total of 1,684 API functions. The generated tests achieve a median statement coverage of 70.2% and branch coverage of 52.8%. In contrast, the state-of-the feedback-directed JavaScript test generation technique, Nessie, achieves only 51.3% statement coverage and 25.6% branch coverage. Furthermore, experiments with excluding parts of the information included in the prompts show that all components contribute towards the generation of effective test suites. We also find that 92.8% of <sc>TestPilot</sc>'s generated tests have <inline-formula><tex-math notation=\"LaTeX\">$\\leq$</tex-math><alternatives><mml:math display=\"inline\"><mml:mo>\u2264</mml:mo></mml:math><inline-graphic xlink:href=\"schaefer-ieq1-3334955.gif\"/></alternatives></inline-formula> 50% similarity with existing tests (as measured by normalized edit distance), with none of them being exact copies. Finally, we run <sc>TestPilot</sc> with two additional LLMs, OpenAI's older <italic>code-cushman-002</italic> LLM and <italic>StarCoder</italic>, an LLM for which the training process is publicly documented. Overall, we observed similar results with the former (68.2% median statement coverage), and somewhat worse results with the latter (54.0% median statement coverage), suggesting that the effectiveness of the approach is influenced by the size and training set of the LLM, but does not fundamentally depend on the specific model.",
            "year": 2023,
            "citationCount": 39,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A large-scale empirical evaluation on the effectiveness of large Language Models for automated unit test generation without requiring additional training or manual effort is presented."
            },
            "score": 4
        },
        {
            "id": "e8b73abefd998229f35e810f465854bdea7512f8",
            "paperId": "e8b73abefd998229f35e810f465854bdea7512f8",
            "title": "Debiased Fine-Tuning for Vision-language Models by Prompt Regularization",
            "abstract": "We present a new paradigm for fine-tuning large-scale vision-language pre-trained models on downstream task, dubbed Prompt Regularization (ProReg). Different from traditional fine-tuning which easily overfits to the downstream task data, ProReg uses the prediction by prompting the pretrained model to regularize the fine-tuning. The motivation is: by prompting the large model \u201ca photo of a [CLASS]\u201d, the fill-in answer is only dependent on the pretraining encyclopedic knowledge while independent of the task data distribution, which is usually biased. Specifically, given a training sample prediction during fine-tuning, we first calculate its Kullback-Leibler loss of the prompt prediction and Cross-Entropy loss of the ground-truth label, and then combine them with a proposed sample-wise adaptive trade- off weight, which automatically adjusts the transfer between the pretrained and downstream domains. On various out-of-distribution benchmarks, we show the consistently strong performance of ProReg compared with conventional fine-tuning, zero-shot prompt, prompt tuning, and other state-of-the-art methods.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The motivation is: by prompting the large model \u201ca photo of a [CLASS]\u201d, the fill-in answer is only dependent on the pretraining encyclopedic knowledge while independent of the task data distribution, which is usually biased."
            },
            "score": 4
        },
        {
            "id": "80b8d03f910bb252cb251f7f74ceaaf8f4c4aabe",
            "paperId": "80b8d03f910bb252cb251f7f74ceaaf8f4c4aabe",
            "title": "Adaptive Test Generation Using a Large Language Model",
            "abstract": "\u2014Unit tests play a key role in ensuring the correctness of software. However, manually creating unit tests is a laborious task, motivating the need for automation. This paper presents T EST P ILOT , an adaptive test generation technique that leverages Large Language Models (LLMs). T EST P ILOT uses Codex, an off-the-shelf LLM, to automatically generate unit tests for a given program without requiring additional training or few-shot learning on examples of existing tests. In our approach, Codex is provided with prompts that include the signature and implementation of a function under test, along with usage examples extracted from documentation. If a generated test fails, T EST P ILOT \u2019s adaptive component attempts to generate a new test that \ufb01xes the problem by re-prompting the model with the failing test and error message. We created an implementation of T EST P ILOT for JavaScript and evaluated it on 25 npm packages with a total of 1,684 API functions to generate tests for. Our results show that the generated tests achieve up to 93.1% statement coverage (median 68.2%). Moreover, on average, 58.5% of the generated tests contain at least one assertion that exercises functionality from the package under test. Our experiments with excluding parts of the information included in the prompts show that all components contribute towards the generation of effective test suites. Finally, we \ufb01nd that T EST P ILOT does not generate memorized tests: 92.7% of our generated tests have \u2264 50% similarity with existing tests (as measured by normalized edit distance), with none of them being exact copies.",
            "year": 2023,
            "citationCount": 39,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "T EST P ILOT uses Codex, an off-the-shelf LLM, to automatically generate unit tests for a given program without requiring additional training or few-shot learning on examples of existing tests, and does not generate memorized tests."
            },
            "score": 4
        },
        {
            "id": "f606f17f36774551739a7db6587b0709fcff0897",
            "paperId": "f606f17f36774551739a7db6587b0709fcff0897",
            "title": "Prompt Tuning or Fine-Tuning - Investigating Relational Knowledge in Pre-Trained Language Models",
            "abstract": "Extracting relational knowledge from large pre-trained language models by a cloze-style sentence serving as a query has shown promising results. In particular, language models can be queried similar to knowledge graphs. The performance of the relational fact extraction task depends signi\ufb01cantly on the query sentence, also known under the term prompt . Tuning these prompts has shown to increase the precision on standard language models by a maximum of around 12% points. However, usually large amounts of data in the form of existing knowledge graph facts and large text corpora are needed to train the required additional model. In this work, we propose using a completely di\ufb00erent approach: Instead of spending resources on training an additional model, we simply perform an adaptive \ufb01ne-tuning of the pre-trained language model on the standard \ufb01ll-mask task using a small training dataset of existing facts from a knowledge graph. We investigate the di\ufb00erences between complex prompting techniques and adaptive \ufb01ne-tuning in an extensive evaluation. Remarkably, adaptive \ufb01ne-tuning outperforms all baselines, even by using signi\ufb01cantly fewer training facts. Additionally, we analyze the transfer learning capabilities of this adapted language model by training on a restricted set of relations to show that even fewer training relations are needed to achieve high knowledge extraction quality.",
            "year": 2021,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work performs an adaptive tuning of the pre-trained language model on the standard task using a small training dataset of existing facts from a knowledge graph to show that even fewer training relations are needed to achieve high knowledge extraction quality."
            },
            "score": 4
        },
        {
            "id": "5006e5be4c3cb1c4af84e4a2717ef886b3a22464",
            "paperId": "5006e5be4c3cb1c4af84e4a2717ef886b3a22464",
            "title": "DART: Dual-Modal Adaptive Online Prompting and Knowledge Retention for Test-Time Adaptation",
            "abstract": "As an up-and-coming area, CLIP-based pre-trained vision-language models can readily facilitate downstream tasks through the zero-shot or few-shot fine-tuning manners. However, they still face critical challenges in test-time generalization due to the shifts between the training and test data distributions, hindering the further improvement of the performance. To address this crucial problem, the latest works have introduced Test-Time Adaptation (TTA) techniques to CLIP which dynamically learn text prompts using only test samples. However, their limited learning capacity due to the overlook of visual modality information, and the underutilization of knowledge in previously seen test samples result in reduced performance. In this paper, we propose a novel Dual-modal Adaptive online prompting and knowledge ReTention method called DART to overcome these challenges. To increase the learning capacity, DART captures knowledge from each test sample by learning class-specific text prompts and instance-level image prompts. Additionally, to fully leverage the knowledge from previously seen test samples, DART utilizes dual-modal knowledge retention prompts to adaptively retain the acquired knowledge, thereby enhancing the predictions on subsequent test samples. Extensive experiments on various large-scale benchmarks demonstrate the effectiveness of our proposed DART against state-of-the-art methods.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "To increase the learning capacity, DART captures knowledge from each test sample by learning class-specific text prompts and instance-level image prompts, and utilizes dual-modal knowledge retention prompts to adaptively retain the acquired knowledge, thereby enhancing the predictions on subsequent test samples."
            },
            "score": 4
        },
        {
            "id": "b70075b496c1f519093884945be5670c32cbceed",
            "paperId": "b70075b496c1f519093884945be5670c32cbceed",
            "title": "Prompt a Robot to Walk with Large Language Models",
            "abstract": "Large language models (LLMs) pre-trained on vast internet-scale data have showcased remarkable capabilities across diverse domains. Recently, there has been escalating interest in deploying LLMs for robotics, aiming to harness the power of foundation models in real-world settings. However, this approach faces significant challenges, particularly in grounding these models in the physical world and in generating dynamic robot motions. To address these issues, we introduce a novel paradigm in which we use few-shot prompts collected from the physical environment, enabling the LLM to autoregressively generate low-level control commands for robots without task-specific fine-tuning. Experiments across various robots and environments validate that our method can effectively prompt a robot to walk. We thus illustrate how LLMs can proficiently function as low-level feedback controllers for dynamic motion control even in high-dimensional robotic systems. The project website and source code can be found at: https://prompt2walk.github.io/ .",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel paradigm is introduced in which few-shot prompts collected from the physical environment are used, enabling the LLM to autoregressively generate low-level control commands for robots without task-specific fine-tuning, illustrating how LLMs can proficiently function as low- level feedback controllers for dynamic motion control even in high-dimensional robotic systems."
            },
            "score": 4
        },
        {
            "id": "ce17ea81f018c51e02c68861e55a2d2b07f25285",
            "paperId": "ce17ea81f018c51e02c68861e55a2d2b07f25285",
            "title": "A Novel Evaluation Framework for Assessing Resilience Against Prompt Injection Attacks in Large Language Models",
            "abstract": "Prompt injection attacks exploit vulnerabilities in large language models (LLMs) to manipulate the model into unintended actions or generate malicious content. As LLM-integrated applications gain wider adoption, they face growing susceptibility to such attacks. This study introduces a novel evaluation framework for quantifying the resilience of applications. The framework incorporates innovative techniques designed to ensure representativeness, interpretability, and robustness. To ensure the representativeness of simulated attacks on the application, a meticulous selection process was employed, resulting in 115 carefully chosen attacks based on coverage and relevance. For enhanced interpretability, a second LLM was utilized to evaluate the responses generated from these simulated attacks. Unlike conventional malicious content classifiers that provide only a confidence score, the LLM-based evaluation produces a score accompanied by an explanation, thereby enhancing interpretability. Subsequently, a resilience score is computed by assigning higher weights to attacks with greater impact, thus providing a robust measurement of the application's resilience. To assess the framework's efficacy, it was applied on two LLMs, namely Llama2 and ChatGLM. Results revealed that Llama2, the newer model exhibited higher resilience compared to ChatGLM. This finding substantiates the effectiveness of the framework, aligning with the prevailing notion that newer models tend to possess greater resilience. Moreover, the framework exhibited exceptional versatility, requiring only minimal adjustments to accommodate emerging attack techniques and classifications, thereby establishing itself as an effective and practical solution. Overall, the framework offers valuable insights that empower organizations to make well-informed decisions to fortify their applications against potential threats from prompt injection.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel evaluation framework for quantifying the resilience of applications that exhibited exceptional versatility, requiring only minimal adjustments to accommodate emerging attack techniques and classifications, thereby establishing itself as an effective and practical solution."
            },
            "score": 4
        },
        {
            "id": "8eb9a8d756e93530eb35e9f0e26a2a0190c1dd7c",
            "paperId": "8eb9a8d756e93530eb35e9f0e26a2a0190c1dd7c",
            "title": "The Biases of Pre-Trained Language Models: An Empirical Study on Prompt-Based Sentiment Analysis and Emotion Detection",
            "abstract": "Thanks to the breakthrough of large-scale pre-trained language model (PLM) technology, prompt-based classification tasks, e.g., sentiment analysis and emotion detection, have raised increasing attention. Such tasks are formalized as masked language prediction tasks which are in line with the pre-training objects of most language models. Thus, one can use a PLM to infer the masked words in a downstream task, then obtaining label predictions with manually defined label-word mapping templates. Prompt-based affective computing takes the advantages of both neural network modeling and explainable symbolic representations. However, there still remain many unclear issues related to the mechanisms of PLMs and prompt-based classification. We conduct a systematic empirical study on prompt-based sentiment analysis and emotion detection to study the biases of PLMs towards affective computing. We find that PLMs are biased in sentiment analysis and emotion detection tasks with respect to the number of label classes, emotional label-word selections, prompt templates and positions, and the word forms of emotion lexicons.",
            "year": 2023,
            "citationCount": 98,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that PLMs are biased in sentiment analysis and emotion detection tasks with respect to the number of label classes, emotional label-word selections, prompt templates and positions, and the word forms of emotion lexicons."
            },
            "score": 4
        },
        {
            "id": "d9d9ceb4c360c95f3305c0381a31389e0a43e45f",
            "paperId": "d9d9ceb4c360c95f3305c0381a31389e0a43e45f",
            "title": "Instance-Aware Hierarchical Structured Policy for Prompt Learning in Vision-Language Models",
            "abstract": "In recent years, learnable prompts have emerged as a major prompt learning paradigm, enhancing the performance of large-scale vision-language pre-trained models in few-shot image classification. However, enhancing methods are often time-consuming and inflexible because 1) class-specific prompts are inefficient in certain situations; 2) instance-specific prompts are put in a fixed position. To address these issues, inspired by the coarse-to-fine decision-making paradigm of human, we propose an Instance-Aware Hierarchical-Structured Policy (IAHSP) that integrates instance-specific prompt selection and appropriate position selection using a reinforcement learning fashion. Specifically, IAHSP consists of two sub-policies: 1) the root policy selects the most suitable prompt from the prompts pool, and 2) the leaf policy identifies the optimal position for inserting the selected prompt. We train these two policies iteratively with rewards constraining the prompts while maintaining their diversity. Extensive experiments on 11 public benchmarks demonstrate that our IAHSP significantly boosts the few-shot image classification performance of vision-language pre-trained models, while also exhibiting superior generalization performance.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An Instance-Aware Hierarchical-Structured Policy (IAHSP) is proposed that integrates instance-specific prompt selection and appropriate position selection using a reinforcement learning fashion and significantly boosts the few-shot image classification performance of vision-language pre-trained models, while also exhibiting superior generalization performance."
            },
            "score": 4
        },
        {
            "id": "eb4afff0eca0026fcc26a5f0c8a73184485e3a25",
            "paperId": "eb4afff0eca0026fcc26a5f0c8a73184485e3a25",
            "title": "Investigating Prompt Learning for Chinese Few-Shot Text Classification with Pre-Trained Language Models",
            "abstract": "Text classification aims to assign predefined labels to unlabeled sentences, which tend to struggle in real-world applications when only a few annotated samples are available. Previous works generally focus on using the paradigm of meta-learning to overcome the classification difficulties brought by insufficient data, where a set of auxiliary tasks is given. Accordingly, prompt-based approaches are proposed to deal with the low-resource issue. However, existing prompt-based methods mainly focus on English tasks, which generally apply English pretrained language models that can not directly adapt to Chinese tasks due to structural and grammatical differences. Thus, we propose a prompt-based Chinese text classification framework that uses generated natural language sequences as hints, which can alleviate the classification bottleneck well in low-resource scenarios. In detail, we first design a prompt-based fine-tuning together with a novel pipeline for automating prompt generation in Chinese. Then, we propose a refined strategy for dynamically and selectively incorporating demonstrations into each context. We present a systematic evaluation for analyzing few-shot performance on a wide range of Chinese text classification tasks. Our approach makes few assumptions about task resources and expertise and therefore constitutes a powerful, task-independent approach for few-shot learning.",
            "year": 2022,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a prompt-based Chinese text classification framework that uses generated natural language sequences as hints, which can alleviate the classification bottleneck well in low-resource scenarios and constitutes a powerful, task-independent approach for few-shot learning."
            },
            "score": 4
        },
        {
            "id": "57063498396aeba8d22039348e1ef6d12ee414a5",
            "paperId": "57063498396aeba8d22039348e1ef6d12ee414a5",
            "title": "Formulating Few-shot Fine-tuning Towards Language Model Pre-training: A Pilot Study on Named Entity Recognition",
            "abstract": "Fine-tuning pre-trained language models has recently become a common practice in building NLP models for various tasks, especially few-shot tasks. We argue that under the few-shot setting, formulating fine-tuning closer to the pre-training objectives shall be able to unleash more benefits from the pre-trained language models. In this work, we take few-shot named entity recognition (NER) for a pilot study, where existing fine-tuning strategies are much different from pre-training. We propose a novel few-shot fine-tuning framework for NER, FFF-NER. Specifically, we introduce three new types of tokens,\"is-entity\",\"which-type\"and bracket, so we can formulate the NER fine-tuning as (masked) token prediction or generation, depending on the choice of pre-trained language models. In our experiments, we apply FFF-NER to fine-tune both BERT and BART for few-shot NER on several benchmark datasets and observe significant improvements over existing fine-tuning strategies, including sequence labeling, prototype meta-learning, and prompt-based approaches. We further perform a series of ablation studies, showing few-shot NER performance is strongly correlated with the similarity between fine-tuning and pre-training.",
            "year": 2022,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel few-shot fine-tuning framework for NER, FFF-NER, which introduces three new types of tokens,\"is-entity\",\"which-type\"and bracket, so it can formulate the NER fine- Tuning as (masked) token prediction or generation, depending on the choice of pre-trained language models."
            },
            "score": 4
        },
        {
            "id": "c53e3020e4b8f9e1cd7b6ed35221480a2647ea26",
            "paperId": "c53e3020e4b8f9e1cd7b6ed35221480a2647ea26",
            "title": "Meta-Learning Online Adaptation of Language Models",
            "abstract": "Large language models encode impressively broad world knowledge in their parameters. However, the knowledge in static language models falls out of date, limiting the model's effective\"shelf life.\"While online fine-tuning can reduce this degradation, we find that naively fine-tuning on a stream of documents leads to a low level of information uptake. We hypothesize that online fine-tuning does not sufficiently attend to important information. That is, the gradient signal from important tokens representing factual information is drowned out by the gradient from inherently noisy tokens, suggesting that a dynamic, context-aware learning rate may be beneficial. We therefore propose learning which tokens to upweight. We meta-train a small, autoregressive model to reweight the language modeling loss for each token during online fine-tuning, with the objective of maximizing the out-of-date base question-answering model's ability to answer questions about a document after a single weighted gradient step. We call this approach Context-aware Meta-learned Loss Scaling (CaMeLS). Across three different distributions of documents, our experiments find that CaMeLS provides substantially improved information uptake on streams of thousands of documents compared with standard fine-tuning and baseline heuristics for reweighting token losses.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work meta-train a small, autoregressive model to reweight the language modeling loss for each token during online fine-tuning, with the objective of maximizing the out-of-date base question-answering model's ability to answer questions about a document after a single weighted gradient step."
            },
            "score": 4
        },
        {
            "id": "6d1433f3342fbee85ad1e2809e62734aec5c3853",
            "paperId": "6d1433f3342fbee85ad1e2809e62734aec5c3853",
            "title": "Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models",
            "abstract": "Large-scale multimodal generative modeling has created milestones in text-to-image and text-to-video generation. Its application to audio still lags behind for two main reasons: the lack of large-scale datasets with high-quality text-audio pairs, and the complexity of modeling long continuous audio data. In this work, we propose Make-An-Audio with a prompt-enhanced diffusion model that addresses these gaps by 1) introducing pseudo prompt enhancement with a distill-then-reprogram approach, it alleviates data scarcity with orders of magnitude concept compositions by using language-free audios; 2) leveraging spectrogram autoencoder to predict the self-supervised audio representation instead of waveforms. Together with robust contrastive language-audio pretraining (CLAP) representations, Make-An-Audio achieves state-of-the-art results in both objective and subjective benchmark evaluation. Moreover, we present its controllability and generalization for X-to-Audio with\"No Modality Left Behind\", for the first time unlocking the ability to generate high-definition, high-fidelity audios given a user-defined modality input. Audio samples are available at https://Text-to-Audio.github.io",
            "year": 2023,
            "citationCount": 118,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes Make-An- audio with a prompt-enhanced diffusion model that alleviates data scarcity with orders of magnitude concept compositions by using language-free audios, and presents its controllability and generalization for X-to-Audio with \"No Modality Left Behind\", for the first time unlocking the ability to generate high-definition, high-fidelity audios given a user-defined modality input."
            },
            "score": 4
        },
        {
            "id": "d86ca0894cb4d165eb5ef45b73526ca8b4cdd725",
            "paperId": "d86ca0894cb4d165eb5ef45b73526ca8b4cdd725",
            "title": "Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers",
            "abstract": "Large pretrained language models have shown surprising In-Context Learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without additional parameter updates. Despite the great success in performance, the working mechanism of ICL still remains an open problem. In order to better understand how ICL works, this paper explains language models as meta-optimizers and understands ICL as a kind of implicit \ufb01netuning. Theoretically, we \ufb01gure out that the Transformer attention has a dual form of gradient descent based optimization. On top of it, we understand ICL as follows: GPT \ufb01rst produces meta-gradients according to the demonstration examples, and then these meta-gradients are applied to the original GPT to build an ICL model. Experimentally, we comprehensively compare the behavior of ICL and explicit \ufb01netuning based on real tasks to provide empirical evidence that supports our understanding. The results prove that ICL behaves similarly to explicit \ufb01netuning at the prediction level, the representation level, and the attention behavior level. Further, inspired by our understanding of meta-optimization, we design a momentum-based attention by analogy with the momentum-based gradient descent algorithm. Its consistently better performance over vanilla attention supports our understanding again from another aspect, and more impor-tantly, it shows the potential to utilize our understanding for future model designing.",
            "year": 2023,
            "citationCount": 164,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Inspired by the understanding of meta-optimization, a momentum-based attention is designed by analogy with the momentum- based gradient descent algorithm and its consistently better performance over vanilla attention supports the understanding from another aspect."
            },
            "score": 4
        },
        {
            "id": "2f604e6e42d79ab35fa4cf6a88540521d1a4cdc5",
            "paperId": "2f604e6e42d79ab35fa4cf6a88540521d1a4cdc5",
            "title": "Fixed Input Parameterization for Efficient Prompting",
            "abstract": "Recent works have shown that attaching prompts to the input is effective at conditioning Language Models (LM) to perform specific tasks. However, prompts are always included in the input text during inference, even when they are fixed, thus incurring substantial computational and memory overhead. Also, there is currently no straightforward method of utilizing prompts that are longer than the maximum input length of the LMs without incurring additional costs during inference. We formally define Fixed Input Parameterization (FIP) problem that focuses on injecting the fixed prompt into the parameters of an LM to be an efficient alternative to attaching fixed prompts to the input. We show that in scenarios with long fixed prompts, FIP can be up to 280 times more efficient in terms of total FLOPs than previous approaches. We further explore methodologies for FIP and show promising results in persona-dependent conversation, semantic parsing, and zero-shot learning with task instructions. Through these explorations, we show that FIP can be a promising direction for conditioning language models, in scenarios with long and fixed prompts 1 .",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that in scenarios with long fixed prompts, FIP can be up to 280 times more efficient in terms of total FLOPs than previous approaches and further explore methodologies for FIP and shows promising results in persona-dependent conversation, semantic parsing, and zero-shot learning with task instructions."
            },
            "score": 4
        },
        {
            "id": "64ce6ef1f5cf227bf2bf917c87273386ae16256f",
            "paperId": "64ce6ef1f5cf227bf2bf917c87273386ae16256f",
            "title": "Dictionary-based Phrase-level Prompting of Large Language Models for Machine Translation",
            "abstract": "Large language models (LLMs) demonstrate remarkable machine translation (MT) abilities via prompting, even though they were not explicitly trained for this task. However, even given the incredible quantities of data they are trained on, LLMs can struggle to translate inputs with rare words, which are common in low resource or domain transfer scenarios. We show that LLM prompting can provide an effective solution for rare words as well, by using prior knowledge from bilingual dictionaries to provide control hints in the prompts. We propose a novel method, DiPMT, that provides a set of possible translations for a subset of the input words, thereby enabling fine-grained phrase-level prompted control of the LLM. Extensive experiments show that DiPMT outperforms the baseline both in low-resource MT, as well as for out-of-domain MT. We further provide a qualitative analysis of the benefits and limitations of this approach, including the overall level of controllability that is achieved.",
            "year": 2023,
            "citationCount": 30,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that LLM prompting can provide an effective solution for rare words as well, by using prior knowledge from bilingual dictionaries to provide control hints in the prompts, thereby enabling fine-grained phrase-level prompted control of the LLM."
            },
            "score": 4
        },
        {
            "id": "006aa1580fae5968417538c7acb4662c7b58088f",
            "paperId": "006aa1580fae5968417538c7acb4662c7b58088f",
            "title": "LLM-Rec: Personalized Recommendation via Prompting Large Language Models",
            "abstract": "We investigate various prompting strategies for enhancing personalized content recommendation performance with large language models (LLMs) through input augmentation . Our proposed approach, termed LLM-Rec , encompasses four distinct prompting strategies: (1) basic prompting, (2) recommendation-driven prompting, (3) engagement-guided prompting",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The proposed approach, termed LLM-Rec, encompasses four distinct prompting strategies: (1) basic prompting, (2) recommendation-driven prompting, (3) engagement-guided prompting, and (4) engagement-guided prompting."
            },
            "score": 4
        },
        {
            "id": "d96e228ce2e10d2215fa1d9833238bb1f2157656",
            "paperId": "d96e228ce2e10d2215fa1d9833238bb1f2157656",
            "title": "The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics",
            "abstract": "Generative large language models (LLMs) have seen many breakthroughs over the last year. With an increasing number of parameters and pre-training data, they have shown remarkable capabilities to solve tasks with minimal or no task-related examples. Notably, LLMs have been successfully employed as evaluation metrics in text generation tasks. Strategies employed in this context differ in the choice of input prompts, the selection of samples for demonstration, and the methodology used to construct scores grading the generations. Approaches often differ in the input prompts, the samples that are selected for demonstration and the construction process of scores from the output. Within this context, we introduce the Eval4NLP 2023 shared task that asks participants to explore such approaches for machine translation evaluation and summarization eval- uation. Specifically, we select a list of allowed LLMs and disallow fine-tuning to ensure a focus on prompting. We test the approaches of the participants on a new reference-free test-set spanning 3 language pairs for machine transla- tion as well as a summarization dataset. Further, we present an overview of the approaches taken by the participants, present their results on the test set and analyze paths for future work. Fi- nally, as a separate track, we perform a human evaluation of the plausibility of explanations given by the LLMs and its effect on model performance. We make parts of our code and datasets available.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A human evaluation of the plausibility of explanations given by the LLMs and its effect on model performance is performed and parts of the code and datasets are made available."
            },
            "score": 4
        },
        {
            "id": "3436ff7a1dd4c6547ba78968d3eec2545a6dccb9",
            "paperId": "3436ff7a1dd4c6547ba78968d3eec2545a6dccb9",
            "title": "Fairness-guided Few-shot Prompting for Large Language Models",
            "abstract": "Large language models have demonstrated surprising ability to perform in-context learning, i.e., these models can be directly applied to solve numerous downstream tasks by conditioning on a prompt constructed by a few input-output examples. However, prior research has shown that in-context learning can suffer from high instability due to variations in training examples, example order, and prompt formats. Therefore, the construction of an appropriate prompt is essential for improving the performance of in-context learning. In this paper, we revisit this problem from the view of predictive bias. Specifically, we introduce a metric to evaluate the predictive bias of a fixed prompt against labels or a given attributes. Then we empirically show that prompts with higher bias always lead to unsatisfactory predictive quality. Based on this observation, we propose a novel search strategy based on the greedy search to identify the near-optimal prompt for improving the performance of in-context learning. We perform comprehensive experiments with state-of-the-art mainstream models such as GPT-3 on various downstream tasks. Our results indicate that our method can enhance the model's in-context learning performance in an effective and interpretable manner.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a metric to evaluate the predictive bias of a fixed prompt against labels or a given attributes and proposes a novel search strategy based on the greedy search to identify the near-optimal prompt for improving the performance of in-context learning."
            },
            "score": 4
        },
        {
            "id": "9e9f675830caceddca3619c2c4d4441e519d894f",
            "paperId": "9e9f675830caceddca3619c2c4d4441e519d894f",
            "title": "Prophet: Prompting Large Language Models with Complementary Answer Heuristics for Knowledge-based Visual Question Answering",
            "abstract": "Knowledge-based visual question answering (VQA) requires external knowledge beyond the image to answer the question. Early studies retrieve required knowledge from explicit knowledge bases (KBs), which often introduces irrelevant information to the question, hence restricting the performance of their models. Recent works have resorted to using a powerful large language model (LLM) as an implicit knowledge engine to acquire the necessary knowledge for answering. Despite the encouraging results achieved by these methods, we argue that they have not fully activated the capacity of the blind LLM as the provided textual input is insufficient to depict the required visual information to answer the question. In this paper, we present Prophet -- a conceptually simple, flexible, and general framework designed to prompt LLM with answer heuristics for knowledge-based VQA. Specifically, we first train a vanilla VQA model on a specific knowledge-based VQA dataset without external knowledge. After that, we extract two types of complementary answer heuristics from the VQA model: answer candidates and answer-aware examples. Finally, the two types of answer heuristics are jointly encoded into a formatted prompt to facilitate the LLM's understanding of both the image and question, thus generating a more accurate answer. By incorporating the state-of-the-art LLM GPT-3, Prophet significantly outperforms existing state-of-the-art methods on four challenging knowledge-based VQA datasets. To demonstrate the generality of our approach, we instantiate Prophet with the combinations of different VQA models (i.e., both discriminative and generative ones) and different LLMs (i.e., both commercial and open-source ones).",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Prophet -- a conceptually simple, flexible, and general framework designed to prompt LLM with answer heuristics for knowledge-based VQA, which significantly outperforms existing state-of-the-art methods on four challenging knowledge-based VQA datasets."
            },
            "score": 4
        },
        {
            "id": "71d68782c3da41b77866c2fd0cb65726f60b3af1",
            "paperId": "71d68782c3da41b77866c2fd0cb65726f60b3af1",
            "title": "Analyzing Chain-of-Thought Prompting in Large Language Models via Gradient-based Feature Attributions",
            "abstract": "Chain-of-thought (CoT) prompting has been shown to empirically improve the accuracy of large language models (LLMs) on various question answering tasks. While understanding why CoT prompting is effective is crucial to ensuring that this phenomenon is a consequence of desired model behavior, little work has addressed this; nonetheless, such an understanding is a critical prerequisite for responsible model deployment. We address this question by leveraging gradient-based feature attribution methods which produce saliency scores that capture the influence of input tokens on model output. Specifically, we probe several open-source LLMs to investigate whether CoT prompting affects the relative importances they assign to particular input tokens. Our results indicate that while CoT prompting does not increase the magnitude of saliency scores attributed to semantically relevant tokens in the prompt compared to standard few-shot prompting, it increases the robustness of saliency scores to question perturbations and variations in model output.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work probes several open-source LLMs to investigate whether CoT prompting affects the relative importances they assign to particular input tokens, and results indicate that while coT prompting does not increase the magnitude of saliency scores attributed to semantically relevant tokens in the prompt compared to standard few-shot prompting, it increases the robustness ofsaliency scores to question perturbations and variations in model output."
            },
            "score": 4
        },
        {
            "id": "be7cb8f79bc018e57467168fc0c7f8ad59bba04f",
            "paperId": "be7cb8f79bc018e57467168fc0c7f8ad59bba04f",
            "title": "Adaptive Testing and Debugging of NLP Models",
            "abstract": "Current approaches to testing and debugging NLP models rely on highly variable human creativity and extensive labor, or only work for a very restrictive class of bugs. We present AdaTest, a process which uses large scale language models (LMs) in partnership with human feedback to automatically write unit tests highlighting bugs in a target model. Such bugs are then addressed through an iterative text-fix-retest loop, inspired by traditional software development. In experiments with expert and non-expert users and commercial / research models for 8 different tasks, AdaTest makes users 5-10x more effective at finding bugs than current approaches, and helps users effectively fix bugs without adding new bugs.",
            "year": 2022,
            "citationCount": 54,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "AdaTest, a process which uses large scale language models in partnership with human feedback to automatically write unit tests highlighting bugs in a target model, makes users 5-10x more effective at finding bugs than current approaches, and helps users effectively fix bugs without adding new bugs."
            },
            "score": 4
        },
        {
            "id": "55c4a747855c74210919c45f7899e1f79e4c97f5",
            "paperId": "55c4a747855c74210919c45f7899e1f79e4c97f5",
            "title": "Conditionally Adaptive Multi-Task Learning: Improving Transfer Learning in NLP Using Fewer Parameters & Less Data",
            "abstract": "Multi-Task Learning (MTL) has emerged as a promising approach for transferring learned knowledge across different tasks. However, multi-task learning must deal with challenges such as: overfitting to low resource tasks, catastrophic forgetting, and negative task transfer, or learning interference. Additionally, in Natural Language Processing (NLP), MTL alone has typically not reached the performance level possible through per-task fine-tuning of pretrained models. However, many fine-tuning approaches are both parameter inefficient, e.g. potentially involving one new model per task, and highly susceptible to losing knowledge acquired during pretraining. We propose a novel transformer based architecture consisting of a new conditional attention mechanism as well as a set of task conditioned modules that facilitate weight sharing. Through this construction we achieve more efficient parameter sharing and mitigate forgetting by keeping half of the weights of a pretrained model fixed. We also use a new multi-task data sampling strategy to mitigate the negative effects of data imbalance across tasks. Using this approach we are able to surpass single-task fine-tuning methods while being parameter and data efficient. With our base model, we attain 2.2% higher performance compared to a full fine-tuned BERT large model on the GLUE benchmark, adding only 5.6% more trained parameters per task (whereas naive fine-tuning potentially adds 100% of the trained parameters per task) and needing only 64.6% of the data. We show that a larger variant of our single multi-task model approach performs competitively across 26 NLP tasks and yields state-of-the-art results on a number of test and development sets.",
            "year": 2020,
            "citationCount": 80,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel transformer based architecture consisting of a new conditional attention mechanism as well as a set of task conditioned modules that facilitate weight sharing is proposed that is able to surpass single-task fine-tuning methods while being parameter and data efficient."
            },
            "score": 4
        },
        {
            "id": "8bc313e04cbd39847eb50b22af0a698ff2971a35",
            "paperId": "8bc313e04cbd39847eb50b22af0a698ff2971a35",
            "title": "Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models: A Case Study on ChatGPT",
            "abstract": "Generative large language models (LLMs), e.g., ChatGPT, have demonstrated remarkable proficiency across several NLP tasks, such as machine translation, text summarization. Recent research (Kocmi and Federmann, 2023) has shown that utilizing LLMs for assessing the quality of machine translation (MT) achieves state-of-the-art performance at the system level but \\textit{performs poorly at the segment level}. To further improve the performance of LLMs on MT quality assessment, we investigate several prompting designs, and propose a new prompting method called \\textbf{\\texttt{Error Analysis Prompting}} (EAPrompt) by combining Chain-of-Thoughts (Wei et al., 2022) and Error Analysis (Lu et al., 2023). This technique emulates the commonly accepted human evaluation framework - Multidimensional Quality Metrics (MQM, Freitag et al. (2021)) and \\textit{produces explainable and reliable MT evaluations at both the system and segment level}. Experimental Results from the WMT22 metrics shared task validate the effectiveness of EAPrompt on various LLMs, with different structures. Further analysis confirms that EAPrompt effectively distinguishes major errors from minor ones, while also sharing a similar distribution of the number of errors with MQM. These findings highlight the potential of EAPrompt as a human-like evaluator prompting technique for MT evaluation.",
            "year": 2023,
            "citationCount": 58,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Findings highlight the potential of EAPrompt as a human-like evaluator prompting technique for MT evaluation, and investigate several prompting designs, and propose a new prompting method called EAPrompt by combining Chain-of-Thoughts and Error Analysis."
            },
            "score": 4
        },
        {
            "id": "12c200e731017f9851afb1a6fe3fc7f76e6439c6",
            "paperId": "12c200e731017f9851afb1a6fe3fc7f76e6439c6",
            "title": "Explicit Visual Prompting for Low-Level Structure Segmentations",
            "abstract": "We consider the generic problem of detecting low-level structures in images, which includes segmenting the manipulated parts, identifying out-of-focus pixels, separating shadow regions, and detecting concealed objects. Whereas each such topic has been typically addressed with a domain-specific solution, we show that a unified approach performs well across all of them. We take inspiration from the widely-used pre-training and then prompt tuning protocols in NLP and propose a new visual prompting model, named Explicit Visual Prompting (EVP). Different from the previous visual prompting which is typically a dataset-level implicit embedding, our key insight is to enforce the tunable parameters focusing on the explicit visual content from each individual image, i.e., the features from frozen patch embeddings and the input's high-frequency components. The proposed EVP significantly outperforms other parameter-efficient tuning protocols under the same amount of tunable parameters (5.7% extra trainable parameters of each task). EVP also achieves state-of-the-art performances on diverse low-level structure segmentation tasks compared to task-specific solutions. Our code is available at: https://github.com/NiFangBaAGe/Explicit-Visual-Prompt.",
            "year": 2023,
            "citationCount": 42,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work takes inspiration from the widely-used pre-training and then prompt tuning protocols in NLP and proposes a new visual prompting model, named Explicit Visual Prompting (EVP), which significantly outperforms other parameter-efficient tuning protocols under the same amount of tunable parameters."
            },
            "score": 4
        },
        {
            "id": "943d42ce1c8983251c227e9b995dc069c477aa90",
            "paperId": "943d42ce1c8983251c227e9b995dc069c477aa90",
            "title": "Diversity-Aware Meta Visual Prompting",
            "abstract": "We present Diversity-Aware Meta Visual Prompting (DAM-VP), an efficient and effective prompting method for transferring pre-trained models to downstream tasks with frozen backbone. A challenging issue in visual prompting is that image datasets sometimes have a large data diversity whereas a per-dataset generic prompt can hardly handle the complex distribution shift toward the original pretraining data distribution properly. To address this issue, we propose a dataset Diversity-Aware prompting strategy whose initialization is realized by a Meta-prompt. Specifically, we cluster the downstream dataset into small homogeneity subsets in a diversity-adaptive way, with each subset has its own prompt optimized separately. Such a divide-and-conquer design reduces the optimization difficulty greatly and significantly boosts the prompting performance. Furthermore, all the prompts are initialized with a meta-prompt, which is learned across several datasets. It is a bootstrapped paradigm, with the key observation that the prompting knowledge learned from previous datasets could help the prompt to converge faster and perform better on a new dataset. During inference, we dynamically select a proper prompt for each input, based on the feature distance between the input and each subset. Through extensive experiments, our DAM-VP demonstrates superior efficiency and effectiveness, clearly surpassing previous prompting methods in a series of downstream datasets for different pretraining models. Our code is available at: https://github.com/shikiw/DAM-VP.",
            "year": 2023,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "DAM-VP is an efficient and effective prompting method for transferring pre-trained models to downstream tasks with frozen backbone, clearly surpassing previous prompting methods in a series of downstream datasets for different pretraining models."
            },
            "score": 4
        },
        {
            "id": "9141480721653789597b6e537ee0eeab401f3e60",
            "paperId": "9141480721653789597b6e537ee0eeab401f3e60",
            "title": "PromptNER: Prompting For Named Entity Recognition",
            "abstract": "In a surprising turn, Large Language Models (LLMs) together with a growing arsenal of prompt-based heuristics now offer powerful off-the-shelf approaches providing few-shot solutions to myriad classic NLP problems. However, despite promising early results, these LLM-based few-shot methods remain far from the state of the art in Named Entity Recognition (NER), where prevailing methods include learning representations via end-to-end structural understanding and fine-tuning on standard labeled corpora. In this paper, we introduce PromptNER, a new state-of-the-art algorithm for few-Shot and cross-domain NER. To adapt to any new NER task PromptNER requires a set of entity definitions in addition to the standard few-shot examples. Given a sentence, PromptNER prompts an LLM to produce a list of potential entities along with corresponding explanations justifying their compatibility with the provided entity type definitions. Remarkably, PromptNER achieves state-of-the-art performance on few-shot NER, achieving a 4% (absolute) improvement in F1 score on the ConLL dataset, a 9% (absolute) improvement on the GENIA dataset, and a 4% (absolute) improvement on the FewNERD dataset. PromptNER also moves the state of the art on Cross Domain NER, outperforming prior methods (including those not limited to the few-shot setting), setting a new mark on 3/5 CrossNER target domains, with an average F1 gain of 3%, despite using less than 2% of the available data.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces PromptNER, a new state-of-the-art algorithm for few-Shot and cross-domain NER, and prompts an LLM to produce a list of potential entities along with corresponding explanations justifying their compatibility with the provided entity type definitions."
            },
            "score": 4
        },
        {
            "id": "411b16add23976ffcdf6422f932453f6ebcca119",
            "paperId": "411b16add23976ffcdf6422f932453f6ebcca119",
            "title": "EvoPrompting: Language Models for Code-Level Neural Architecture Search",
            "abstract": "Given the recent impressive accomplishments of language models (LMs) for code generation, we explore the use of LMs as adaptive mutation and crossover operators for an evolutionary neural architecture search (NAS) algorithm. While NAS still proves too difficult a task for LMs to succeed at solely through prompting, we find that the combination of evolutionary prompt engineering with soft prompt-tuning, a method we term EvoPrompting, consistently finds diverse and high performing models. We first demonstrate that EvoPrompting is effective on the computationally efficient MNIST-1D dataset, where EvoPrompting produces convolutional architecture variants that outperform both those designed by human experts and naive few-shot prompting in terms of accuracy and model size. We then apply our method to searching for graph neural networks on the CLRS Algorithmic Reasoning Benchmark, where EvoPrompting is able to design novel architectures that outperform current state-of-the-art models on 21 out of 30 algorithmic reasoning tasks while maintaining similar model size. EvoPrompting is successful at designing accurate and efficient neural network architectures across a variety of machine learning tasks, while also being general enough for easy adaptation to other tasks beyond neural network design.",
            "year": 2023,
            "citationCount": 35,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "EvoPrompting is successful at designing accurate and efficient neural network architectures across a variety of machine learning tasks, while also being general enough for easy adaptation to other tasks beyond neural network design."
            },
            "score": 3
        },
        {
            "id": "7397f9861a3c8110540a0ca55072d3d6ea54cba5",
            "paperId": "7397f9861a3c8110540a0ca55072d3d6ea54cba5",
            "title": "Rewriting Math Word Problems with Large Language Models",
            "abstract": "Large Language Models have recently achieved high performance on many writing tasks. In a recent study, math word problems in Carnegie Learning\u2019s MATHia adaptive learning software were rewritten by human authors to improve their clarity and specificity. The randomized experiment found that emerging readers who received the rewritten word problems spent less time completing the problems and also achieved higher mastery compared to emerging readers who received the original content. We used GPT-4 to rewrite the same set of math word problems, prompting it to follow the same guidelines that the human authors followed. We lay out our prompt engineering process, comparing several prompting strategies: zero-shot, few-shot, and chain-of-thought prompting. Additionally, we overview how we leveraged GPT\u2019s ability to write python code in order to encode mathematical components of word problems. We report text analysis of the original, human-rewritten, and GPT-rewritten problems. GPT rewrites had the most optimal readability, lexical diversity, and cohesion scores but used more low frequency words. We present our plan to test the GPT outputs in upcoming randomized field trials in MATHia.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work used GPT-4 to rewrite the same set of math word problems, prompting it to follow the same guidelines that the human authors followed, and found that GPT rewrites had the most optimal readability, lexical diversity, and cohesion scores but used more low frequency words."
            },
            "score": 3
        },
        {
            "id": "f4cfee20b4a4ef76cfd80b99aebdfacf4509be0a",
            "paperId": "f4cfee20b4a4ef76cfd80b99aebdfacf4509be0a",
            "title": "Category-Specific Prompts for Animal Action Recognition with Pretrained Vision-Language Models",
            "abstract": "Animal action recognition has a wide range of applications. However, the field largely remains unexplored due to the greater challenges compared to human action recognition, such as lack of annotated training data, large intra-class variation, and interference of cluttered background. Most of the existing methods directly apply human action recognition techniques, which essentially require a large amount of annotated data. In recent years, contrastive vision-language pretraining has demonstrated strong zero-shot generalization ability and has been used for human action recognition. Inspired by the success, we develop a highly performant action recognition framework based on the CLIP model. Our model addresses the above challenges via a novel category-specific prompting module to generate adaptive prompts for both text and video based on the animal category detected in input videos. On one hand, it can generate more precise and customized textual descriptions for each action and animal category pair, being helpful in the alignment of textual and visual space. On the other hand, it allows the model to focus on video features of the target animal in the video and reduce the interference of video background noise. Experimental results demonstrate that our method outperforms five previous action recognition methods on the Animal Kingdom dataset and has shown best generalization ability on unseen animals.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results demonstrate that the method outperforms five previous action recognition methods on the Animal Kingdom dataset and has shown best generalization ability on unseen animals."
            },
            "score": 3
        },
        {
            "id": "a051a013d45e4b059586be689eea37e9637a1d6b",
            "paperId": "a051a013d45e4b059586be689eea37e9637a1d6b",
            "title": "TAP: Targeted Prompting for Task Adaptive Generation of Textual Training Instances for Visual Classification",
            "abstract": "Vision and Language Models (VLMs), such as CLIP, have enabled visual recognition of a potentially unlimited set of categories described by text prompts. However, for the best visual recognition performance, these models still require tuning to better fit the data distributions of the downstream tasks, in order to overcome the domain shift from the web-based pre-training data. Recently, it has been shown that it is possible to effectively tune VLMs without any paired data, and in particular to effectively improve VLMs visual recognition performance using text-only training data generated by Large Language Models (LLMs). In this paper, we dive deeper into this exciting text-only VLM training approach and explore ways it can be significantly further improved taking the specifics of the downstream task into account when sampling text data from LLMs. In particular, compared to the SOTA text-only VLM training approach, we demonstrate up to 8.4% performance improvement in (cross) domain-specific adaptation, up to 8.7% improvement in fine-grained recognition, and 3.1% overall average improvement in zero-shot classification compared to strong baselines.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Compared to the SOTA text-only VLM training approach, this paper demonstrates up to 8.4% performance improvement in (cross) domain-specific adaptation, and up to 3.1% overall average improvement in zero-shot classification compared to strong baselines."
            },
            "score": 3
        },
        {
            "id": "51cec4b81405b7881a6f7f66d6b88015f71f93b7",
            "paperId": "51cec4b81405b7881a6f7f66d6b88015f71f93b7",
            "title": "Can Large Language Models Reason About Goal-Oriented Tasks?",
            "abstract": "Most adults can complete a sequence of steps to achieve a certain goal, such as making a sandwich or repairing a bicycle tire. In completing these goal-oriented tasks, or simply tasks in this paper, one must use sequential reasoning to understand the relationship between the sequence of steps and the goal. LLMs have shown impressive capabilities across various natural language understanding tasks. However, prior work has mainlyfocused on logical reasoning tasks (e.g. arithmetic, commonsense QA); how well LLMs can perform on more complex reasoning tasks like sequential reasoning is not clear. In this paper, we address this gap and conduct a comprehensive evaluation of how well LLMs are able to conduct this reasoning for tasks and how they scale w.r.t multiple dimensions(e.g. adaptive prompting strategies, number of in-context examples, varying complexity of the sequential task). Our findings reveal that while Chain of Thought (CoT) prompting can significantly enhance LLMs\u2019 sequential reasoning in certain scenarios, it can also be detrimental in others, whereas Tree of Thoughts (ToT) reasoning is less effective for this type of task. Additionally, we discover that an increase in model size or in-context examples does not consistently lead to improved performance.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The findings reveal that while Chain of Thought (CoT) prompting can significantly enhance LLMs\u2019 sequential reasoning in certain scenarios, it can also be detrimental in others, whereas Tree of Thoughts (ToT) reasoning is less effective for this type of task."
            },
            "score": 3
        },
        {
            "id": "cf56a7b28fb27279b1c94fb920b5722cf50c8852",
            "paperId": "cf56a7b28fb27279b1c94fb920b5722cf50c8852",
            "title": "AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs",
            "abstract": "While recently Large Language Models (LLMs) have achieved remarkable successes, they are vulnerable to certain jailbreaking attacks that lead to generation of inappropriate or harmful content. Manual red-teaming requires finding adversarial prompts that cause such jailbreaking, e.g. by appending a suffix to a given instruction, which is inefficient and time-consuming. On the other hand, automatic adversarial prompt generation often leads to semantically meaningless attacks that can easily be detected by perplexity-based filters, may require gradient information from the TargetLLM, or do not scale well due to time-consuming discrete optimization processes over the token space. In this paper, we present a novel method that uses another LLM, called the AdvPrompter, to generate human-readable adversarial prompts in seconds, $\\sim800\\times$ faster than existing optimization-based approaches. We train the AdvPrompter using a novel algorithm that does not require access to the gradients of the TargetLLM. This process alternates between two steps: (1) generating high-quality target adversarial suffixes by optimizing the AdvPrompter predictions, and (2) low-rank fine-tuning of the AdvPrompter with the generated adversarial suffixes. The trained AdvPrompter generates suffixes that veil the input instruction without changing its meaning, such that the TargetLLM is lured to give a harmful response. Experimental results on popular open source TargetLLMs show state-of-the-art results on the AdvBench dataset, that also transfer to closed-source black-box LLM APIs. Further, we demonstrate that by fine-tuning on a synthetic dataset generated by AdvPrompter, LLMs can be made more robust against jailbreaking attacks while maintaining performance, i.e. high MMLU scores.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents a novel method that uses another LLM, called the AdvPrompter, to generate human-readable adversarial prompts in seconds, faster than existing optimization-based approaches, and demonstrates that by fine-tuning on a synthetic dataset generated by AdvPrompter, LLMs can be made more robust against jailbreaking attacks while maintaining performance, i.e. high MMLU scores."
            },
            "score": 3
        },
        {
            "id": "12315711b0b9bbd60a46ceb81c578b3195f2e852",
            "paperId": "12315711b0b9bbd60a46ceb81c578b3195f2e852",
            "title": "Enhancing Answer Selection in Community Question Answering with Pre-trained and Large Language Models",
            "abstract": "Community Question Answering (CQA) becomes increasingly prevalent in recent years. However, there are a large number of answers, which is difficult for users to select the relevant answers. Therefore, answer selection is a very significant subtask of CQA. In this paper, we first propose the Question-Answer cross attention networks (QAN) with pre-trained models for answer selection and utilize large language model (LLM) to perform answer selection with knowledge augmentation. Specifically, we apply the BERT model as the encoder layer to do pre-training for question subjects, question bodies and answers, respectively, then the cross attention mechanism selects the most relevant answer for different questions. Experiments show that the QAN model achieves state-of-the-art performance on two datasets, SemEval2015 and SemEval2017. Moreover, we use the LLM to generate external knowledge from questions and correct answers to achieve knowledge augmentation for the answer selection task by LLM, while optimizing the prompt of LLM in different aspects. The results show that the introduction of external knowledge can improve the correct answer selection rate of LLM on datasets SemEval2015 and SemEval2017. Meanwhile, LLM can also select the correct answer on more questions by optimized prompt.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes the Question-Answer cross attention networks (QAN) with pre-trained models for answer selection and utilizes large language model (LLM) to perform answer selection with knowledge augmentation and uses the LLM to generate external knowledge from questions and correct answers to achieveknowledge augmentation for the answer selection task."
            },
            "score": 3
        },
        {
            "id": "6f0e7f51f0ceb317061a803ae5d5eb79ef668b4c",
            "paperId": "6f0e7f51f0ceb317061a803ae5d5eb79ef668b4c",
            "title": "Knowledge Base Construction from Pre-trained Language Models by Prompt learning",
            "abstract": "Pre-trained language models (LMs) have advanced the state-of-the-art for many semantic tasks and have also been proven effective for extracting knowledge from the models itself. Although several works have explored the capability of the LMs for constructing knowledge bases, including prompt learning, this potential has not yet been fully explored. In this work, we propose a method of extracting factual knowledge from LMs for given subject-relation pairs and explore the most effective strategy to generate blank object entities for each relation of triples. We design prompt templates for each relation using personal knowledge and the descriptive information available on the web such as WikiData. The probing approach of our proposed LMs is tested on the dataset provided by the International Semantic Web Conference (ISWC 2022) LM-KBC Challenge. To cope with the problem of varying performance for each relation, we designed a parameter selection strategy for each relation. Using the test dataset, we obtain an F1-score of 0.4935%, which is higher than the baseline of 31.08%.",
            "year": 2022,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a method of extracting factual knowledge from LMs for given subject-relation pairs and explores the most effective strategy to generate blank object entities for each relation of triples."
            },
            "score": 3
        },
        {
            "id": "0fd1a48877f62d8fa5e4c9adc8189e84b507a349",
            "paperId": "0fd1a48877f62d8fa5e4c9adc8189e84b507a349",
            "title": "Contextual Transformer for Offline Meta Reinforcement Learning",
            "abstract": "The pretrain-finetuning paradigm in large-scale sequence models has made significant progress in natural language processing and computer vision tasks. However, such a paradigm is still hindered by several challenges in Reinforcement Learning (RL), including the lack of self-supervised pretraining algorithms based on offline data and efficient fine-tuning/prompt-tuning over unseen downstream tasks. In this work, we explore how prompts can improve sequence modeling-based offline reinforcement learning (offline-RL) algorithms. Firstly, we propose prompt tuning for offline RL, where a context vector sequence is concatenated with the input to guide the conditional policy generation. As such, we can pretrain a model on the offline dataset with self-supervised loss and learn a prompt to guide the policy towards desired actions. Secondly, we extend our framework to Meta-RL settings and propose Contextual Meta Transformer (CMT); CMT leverages the context among different tasks as the prompt to improve generalization on unseen tasks. We conduct extensive experiments across three different offline-RL settings: offline single-agent RL on the D4RL dataset, offline Meta-RL on the MuJoCo benchmark, and offline MARL on the SMAC benchmark. Superior results validate the strong performance, and generality of our methods.",
            "year": 2022,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes prompt tuning for offline RL, where a context vector sequence is concatenated with the input to guide the conditional policy generation and proposes Contextual Meta Transformer (CMT), which leverages the context among different tasks as the prompt to improve generalization on unseen tasks."
            },
            "score": 3
        },
        {
            "id": "136d968598ab14715cec3393153355c3b535201e",
            "paperId": "136d968598ab14715cec3393153355c3b535201e",
            "title": "Meta Learning to Bridge Vision and Language Models for Multimodal Few-Shot Learning",
            "abstract": "Multimodal few-shot learning is challenging due to the large domain gap between vision and language modalities. Existing methods are trying to communicate visual concepts as prompts to frozen language models, but rely on hand-engineered task induction to reduce the hypothesis space. To make the whole process learnable, we introduce a multimodal meta-learning approach. Specifically, our approach decomposes the training of the model into a set of related multimodal few-shot tasks. We define a meta-mapper network, acting as a meta-learner, to efficiently bridge frozen large-scale vision and language models and leverage their already learned capacity. By updating the learnable parameters only of the meta-mapper, it learns to accrue shared meta-knowledge among these tasks. Thus, it can rapidly adapt to newly presented samples with only a few gradient updates. Importantly, it induces the task in a completely data-driven manner, with no need for a hand-engineered task induction. We evaluate our approach on recently proposed multimodal few-shot benchmarks, measuring how rapidly the model can bind novel visual concepts to words and answer visual questions by observing only a limited set of labeled examples. The experimental results show that our meta-learning approach outperforms the baseline across multiple datasets and various training settings while being computationally more efficient.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results show that the multimodal meta-learning approach outperforms the baseline across multiple datasets and various training settings while being computationally more efficient."
            },
            "score": 3
        },
        {
            "id": "3a021e1acc7588897df3f58e3ad928122846122f",
            "paperId": "3a021e1acc7588897df3f58e3ad928122846122f",
            "title": "Forging Multiple Training Objectives for Pre-trained Language Models via Meta-Learning",
            "abstract": "Multiple pre-training objectives fill the vacancy of the understanding capability of single-objective language modeling, which serves the ultimate purpose of pre-trained language models (PrLMs), generalizing well on a mass of scenarios. However, learning multiple training objectives in a single model is challenging due to the unknown relative significance as well as the potential contrariety between them. Empirical studies have shown that the current objective sampling in an ad-hoc manual setting makes the learned language representation barely converge to the desired optimum. Thus, we propose \\textit{MOMETAS}, a novel adaptive sampler based on meta-learning, which learns the latent sampling pattern on arbitrary pre-training objectives. Such a design is lightweight with negligible additional training overhead. To validate our approach, we adopt five objectives and conduct continual pre-training with BERT-base and BERT-large models, where MOMETAS demonstrates universal performance gain over other rule-based sampling strategies on 14 natural language processing tasks.",
            "year": 2022,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes MOMETAS, a novel adaptive sampler based on meta-learning, which learns the latent sampling pattern on arbitrary pre-training objectives and demonstrates universal performance gain over other rule-based sampling strategies on 14 natural language processing tasks."
            },
            "score": 3
        },
        {
            "id": "37ba9c33025fb31f25436010e12c65a0bafc0e1f",
            "paperId": "37ba9c33025fb31f25436010e12c65a0bafc0e1f",
            "title": "Meta-Learning Fast Weight Language Models",
            "abstract": "Dynamic evaluation of language models (LMs) adapts model parameters at test time using gradient information from previous tokens and substantially improves LM performance. However, it requires over 3x more compute than standard inference. We present Fast Weight Layers (FWLs), a neural component that provides the benefits of dynamic evaluation much more efficiently by expressing gradient updates as linear attention. A key improvement over dynamic evaluation is that FWLs can also be applied at training time, so the model learns to make good use of gradient updates. FWLs can easily be added on top of existing transformer models, require relatively little extra compute or memory to run, and significantly improve language modeling perplexity.",
            "year": 2022,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Fast Weight Layers are presented, a neural component that provides the benefits of dynamic evaluation much more efficiently by expressing gradient updates as linear attention and can also be applied at training time, so the model learns to make good use of gradient updates."
            },
            "score": 3
        },
        {
            "id": "763221f613adcc2f1f306fd76ab1912fcb50255b",
            "paperId": "763221f613adcc2f1f306fd76ab1912fcb50255b",
            "title": "SpeechX: Neural Codec Language Model as a Versatile Speech Transformer",
            "abstract": "Recent advancements in generative speech models based on audio-text prompts have enabled remarkable innovations like high-quality zero-shot text-to-speech. However, existing models still face limitations in handling diverse audio-text speech generation tasks involving transforming input speech and processing audio captured in adverse acoustic conditions. This paper introduces SpeechX, a versatile speech generation model capable of zero-shot TTS and various speech transformation tasks, dealing with both clean and noisy signals. SpeechX combines neural codec language modeling with multi-task learning using task-dependent prompting, enabling unified and extensible modeling and providing a consistent way for leveraging textual input in speech enhancement and transformation tasks. Experimental results show SpeechX's efficacy in various tasks, including zero-shot TTS, noise suppression, target speaker extraction, speech removal, and speech editing with or without background noise, achieving comparable or superior performance to specialized models across tasks. See https://aka.ms/speechx for demo samples.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "SpeechX combines neural codec language modeling with multi-task learning using task-dependent prompting, enabling unified and extensible modeling and providing a consistent way for leveraging textual input in speech enhancement and transformation tasks."
            },
            "score": 3
        },
        {
            "id": "4ee96f0757e517928590a2300af5d40ba768a5a7",
            "paperId": "4ee96f0757e517928590a2300af5d40ba768a5a7",
            "title": "PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents",
            "abstract": "Strategies such as chain-of-thought prompting improve the performance of large language models (LLMs) on complex reasoning tasks by decomposing input examples into intermediate steps. However, it remains unclear how to apply such methods to reason over long input documents, in which both the decomposition and the output of each intermediate step are non-trivial to obtain. In this work, we propose PEARL, a prompting framework to improve reasoning over long documents, which consists of three stages: action mining, plan formulation, and plan execution. More specifically, given a question about a long document, PEARL decomposes the question into a sequence of actions (e.g., SUMMARIZE, FIND_EVENT, FIND_RELATION) and then executes them over the document to obtain the answer. Each stage of PEARL is implemented via zero-shot or few-shot prompting of LLMs (in our work, GPT-4) with minimal human input. We evaluate PEARL on a challenging subset of the QuALITY dataset, which contains questions that require complex reasoning over long narrative texts. PEARL outperforms zero-shot and chain-of-thought prompting on this dataset, and ablation experiments show that each stage of PEARL is critical to its performance. Overall, PEARL is a first step towards leveraging LLMs to reason over long documents.",
            "year": 2023,
            "citationCount": 25,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes PEARL, a prompting framework to improve reasoning over long documents, which consists of three stages: action mining, plan formulation, and plan execution, which outperforms zero-shot and chain-of-thought prompting on a challenging subset of the QuALITY dataset."
            },
            "score": 3
        },
        {
            "id": "0e0d72be9950fde9b5e8996e2147d1318f216ebb",
            "paperId": "0e0d72be9950fde9b5e8996e2147d1318f216ebb",
            "title": "Prompting is not a substitute for probability measurements in large language models",
            "abstract": "Prompting is now a dominant method for evaluating the linguistic knowledge of large language models (LLMs). While other methods directly read out models' probability distributions over strings, prompting requires models to access this internal information by processing linguistic input, thereby implicitly testing a new type of emergent ability: metalinguistic judgment. In this study, we compare metalinguistic prompting and direct probability measurements as ways of measuring models' linguistic knowledge. Broadly, we find that LLMs' metalinguistic judgments are inferior to quantities directly derived from representations. Furthermore, consistency gets worse as the prompt query diverges from direct measurements of next-word probabilities. Our findings suggest that negative results relying on metalinguistic prompts cannot be taken as conclusive evidence that an LLM lacks a particular linguistic generalization. Our results also highlight the value that is lost with the move to closed APIs where access to probability distributions is limited.",
            "year": 2023,
            "citationCount": 11,
            "tldr": null,
            "score": 3
        },
        {
            "id": "3d68522abfadfc8ee6b7ec9edaaf91f1b2f38e5e",
            "paperId": "3d68522abfadfc8ee6b7ec9edaaf91f1b2f38e5e",
            "title": "Large Language Models Can Be Easily Distracted by Irrelevant Context",
            "abstract": "Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of cutting-edge prompting techniques for large language models, and find that the model performance is dramatically decreased when irrelevant information is included. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.",
            "year": 2023,
            "citationCount": 212,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context, and introduces Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description."
            },
            "score": 3
        },
        {
            "id": "c2329c685f11efa25c562f97be71ff03103423fd",
            "paperId": "c2329c685f11efa25c562f97be71ff03103423fd",
            "title": "Prompting Is Programming: A Query Language for Large Language Models",
            "abstract": "Large language models have demonstrated outstanding performance on a wide range of tasks such as question answering and code generation. On a high level, given an input, a language model can be used to automatically complete the sequence in a statistically-likely way. Based on this, users prompt these models with language instructions or examples, to implement a variety of downstream tasks. Advanced prompting methods can even imply interaction between the language model, a user, and external tools such as calculators. However, to obtain state-of-the-art performance or adapt language models for specific tasks, complex task- and model-specific programs have to be implemented, which may still require ad-hoc interaction. Based on this, we present the novel idea of Language Model Programming (LMP). LMP generalizes language model prompting from pure text prompts to an intuitive combination of text prompting and scripting. Additionally, LMP allows constraints to be specified over the language model output. This enables easy adaption to many tasks while abstracting language model internals and providing high-level semantics. To enable LMP, we implement LMQL (short for Language Model Query Language), which leverages the constraints and control flow from an LMP prompt to generate an efficient inference procedure that minimizes the number of expensive calls to the underlying language model. We show that LMQL can capture a wide range of state-of-the-art prompting methods in an intuitive way, especially facilitating interactive flows that are challenging to implement with existing high-level APIs. Our evaluation shows that we retain or increase the accuracy on several downstream tasks, while also significantly reducing the required amount of computation or cost in the case of pay-to-use APIs (26-85% cost savings).",
            "year": 2022,
            "citationCount": 38,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LMQL is implemented, which leverages the constraints and control flow from an LMP prompt to generate an efficient inference procedure that minimizes the number of expensive calls to the underlying language model."
            },
            "score": 3
        },
        {
            "id": "0d47154aa8eabf7421c1cf70a648f3bd1329751d",
            "paperId": "0d47154aa8eabf7421c1cf70a648f3bd1329751d",
            "title": "Medical Visual Prompting (MVP): A Unified Framework for Versatile and High-Quality Medical Image Segmentation",
            "abstract": "Accurate segmentation of lesion regions is crucial for clinical diagnosis and treatment across various diseases. While deep convolutional networks have achieved satisfactory results in medical image segmentation, they face challenges such as loss of lesion shape information due to continuous convolution and downsampling, as well as the high cost of manually labeling lesions with varying shapes and sizes. To address these issues, we propose a novel medical visual prompting (MVP) framework that leverages pre-training and prompting concepts from natural language processing (NLP). The framework utilizes three key components: Super-Pixel Guided Prompting (SPGP) for superpixelating the input image, Image Embedding Guided Prompting (IEGP) for freezing patch embedding and merging with superpixels to provide visual prompts, and Adaptive Attention Mechanism Guided Prompting (AAGP) for pinpointing prompt content and efficiently adapting all layers. By integrating SPGP, IEGP, and AAGP, the MVP enables the segmentation network to better learn shape prompting information and facilitates mutual learning across different tasks. Extensive experiments conducted on five datasets demonstrate superior performance of this method in various challenging medical image tasks, while simplifying single-task medical segmentation models. This novel framework offers improved performance with fewer parameters and holds significant potential for accurate segmentation of lesion regions in various medical tasks, making it clinically valuable.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel medical visual prompting (MVP) framework that leverages pre-training and prompting concepts from natural language processing (NLP) and holds significant potential for accurate segmentation of lesion regions in various medical tasks, making it clinically valuable."
            },
            "score": 3
        },
        {
            "id": "8b5f4b383008bfb365cee72e5301ee04a24221f7",
            "paperId": "8b5f4b383008bfb365cee72e5301ee04a24221f7",
            "title": "Vita-CLIP: Video and text adaptive CLIP via Multimodal Prompting",
            "abstract": "Adopting contrastive image-text pretrained models like CLIP towards video classification has gained attention due to its cost-effectiveness and competitive performance. However, recent works in this area face a trade-off. Finetuning the pretrained model to achieve strong supervised performance results in low zero-shot generalization. Similarly, freezing the backbone to retain zero-shot capability causes significant drop in supervised accuracy. Because of this, recent works in literature typically train separate models for supervised and zero-shot action recognition. In this work, we propose a multimodal prompt learning scheme that works to balance the supervised and zero-shot performance under a single unified training. Our prompting approach on the vision side caters for three aspects: 1) Global video-level prompts to model the data distribution; 2) Local frame-level prompts to provide per-frame discriminative conditioning; and 3) a summary prompt to extract a condensed video representation. Additionally, we define a prompting scheme on the text side to augment the textual context. Through this prompting scheme, we can achieve state-of-the-art zero-shot performance on Kinetics-600, HMDB51 and UCF101 while remaining competitive in the supervised setting. By keeping the pretrained backbone frozen, we optimize a much lower number of parameters and retain the existing general representation which helps achieve the strong zero-shot performance. Our codes/models will be released at https://github.com/TalalWasim/Vita-Clip..",
            "year": 2023,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a multimodal prompt learning scheme that works to balance the supervised and zero- shot performance under a single unified training and can achieve state-of-the-art zero-shot performance on Kinetics-600, HMDB51 and UCF101 while remaining competitive in the supervised setting."
            },
            "score": 3
        },
        {
            "id": "e10c2ed8de5568e6a41fb60c2adc4bf9c225da25",
            "paperId": "e10c2ed8de5568e6a41fb60c2adc4bf9c225da25",
            "title": "Is EVALITA Done? On the Impact of Prompting on the Italian NLP Evaluation Campaign",
            "abstract": "Prompt-based learning is a recent paradigm in NLP that leverages large pre-trained language models to perform a variety of tasks. With this technique, it is possible to build classifiers that do not need training data (zero-shot). In this paper, we assess the status of prompt-based learning applied to several text classification tasks in the Italian language. The results indicate that the performance gap towards current supervised methods is still relevant. However, the difference in performance between pre-trained models and the characteristic of the prompt-based classifier of operating in a zero-shot fashion open a discussion regarding the next generation of evaluation campaigns for NLP.",
            "year": 2022,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results indicate that the performance gap towards current supervised methods is still relevant, however, the difference in performance between pre-trained models and the characteristic of the prompt-based classifier of operating in a zero-shot fashion open a discussion regarding the next generation of evaluation campaigns for NLP."
            },
            "score": 3
        },
        {
            "id": "80c698688bb4488beaceaab5c64f701a946cb7ae",
            "paperId": "80c698688bb4488beaceaab5c64f701a946cb7ae",
            "title": "All in One: Multi-Task Prompting for Graph Neural Networks",
            "abstract": "Recently, \"pre-training and fine-tuning'' has been adopted as a standard workflow for many graph tasks since it can take general graph knowledge to relieve the lack of graph annotations from each application. However, graph tasks with node level, edge level, and graph level are far diversified, making the pre-training pretext often incompatible with these multiple tasks. This gap may even cause a \"negative transfer'' to the specific application, leading to poor results. Inspired by the prompt learning in natural language processing (NLP), which has presented significant effectiveness in leveraging prior knowledge for various NLP tasks, we study the prompting topic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models. Specifically, we first unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern. In this way, the prompting idea from NLP can be seamlessly introduced to the graph area. Then, to further narrow the gap between various graph tasks and state-of-the-art pre-training strategies, we further study the task space of various graph applications and reformulate downstream problems to the graph-level task. Afterward, we introduce meta-learning to efficiently learn a better initialization for the multi-task prompt of graphs so that our prompting framework can be more reliable and general for different tasks. We conduct extensive experiments, results from which demonstrate the superiority of our method.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel multi-task prompting method for graph models that unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern, and introduces meta-learning to efficiently learn a better initialization for the multi- task prompt of graphs so that the prompting framework can be more reliable and general for different tasks."
            },
            "score": 3
        },
        {
            "id": "1a39f5c73193a5680f40b9750ccd39c66aabb045",
            "paperId": "1a39f5c73193a5680f40b9750ccd39c66aabb045",
            "title": "SwarMind: Harnessing Large Language Models for Flock Dynamics",
            "abstract": "The deployment of autonomous agent swarms has witnessed a rapid increase in a variety of fields, from logistics to surveillance. This prevalence stems from the complex objectives they can accomplish through simple interactions, their adaptive behavior, and their inherent robustness to potential disturbances. Despite these advantages, achieving control over such systems remains a formidable challenge, often necessitating the use of approximations or domain-specific heuristics. As demonstrated by recent works, large language models (LLMs) display a robust capacity to excel across a diverse array of tasks. In this work, we extend the exploration of LLM's capabilities into the niche domain of flock driving. Specifically, our study presents a comparative analysis of LLMs and reinforcement learning (RL) in a fair setting, scrutinizing their performances under various prompting strategies. Furthermore, it investigates the potential of eliciting more sophisticated behaviors from LLMs through textual instructions, offering a deeper understanding of their limitations and strengths in swarm control and management. The results illuminate several potential shortcomings while con-currently uncovering exciting prospects and extensions. This research therefore advances our understanding of the applicability of LLMs to the intricate field of swarm control, opening doors to their potential use in domains hitherto unexplored.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study presents a comparative analysis of LLMs and reinforcement learning in a fair setting, examining their performances under various prompting strategies, and investigates the potential of eliciting more sophisticated behaviors from LLMs through textual instructions, offering a deeper understanding of their limitations and strengths in swarm control and management."
            },
            "score": 2
        },
        {
            "id": "30b072f58dd7a50a179a2c73adb930a1c29c01d6",
            "paperId": "30b072f58dd7a50a179a2c73adb930a1c29c01d6",
            "title": "Toward AI-Assisted Clinical Assessment for Patients with Multiple Myeloma: Feature Selection for Large Language Models",
            "abstract": "\n Large language models (LLM) can potentially revolutionize the healthcare industry. They could reduce the burden on healthcare, increase care accessibility in areas with shortage and provide multilingual support to break down language barriers. Although, these models equipped with vast amounts of medical knowledge and the ability to understand and generate human-like text, require a proper set of feeding data (i.e., prompt engineering) for accurate diagnosis and providing reliable personalized treatment plans for patients.\n Multiple Myeloma (MM) is a complex hematological malignancy characterized by the uncontrolled proliferation of plasma cells in the bone marrow. Disease management for MM becomes particularly challenging due to its multisystemic nature based on the varying volume of malignant cells within the bone marrow. Implementing LLMs for clinical assessment of patients with MM needs feature selection to develop the most effective prompt for these models. Here, we utilized a machine learning (ML) approach to define salient features in a typical visit day that correlates most significantly with disease volume on the same day. These features could be the best candidate to reflect the multisystemic and dynamic nature of MM in each visit, and they could be candidates to be incorporated into LLMs to develop a system-based assessment in clinic visits.\n Methods: This study examined 1,472 clinical observations. To select a curated list of features associated with same-day M-spike values, 43 clinical and lab variables were input into an ML model. Random Forest (RF), an ensemble of regression trees suitable for nonlinear multiple regression, was selected as the model. The data were randomly divided into a training set (80%) and a test set (20%) for model validation. Using bootstrapping and generating 500 data sets, a random forest of regression trees was constructed, and results and estimates were aggregated across the trees. To determine the importance of each covariate, their inclusion and exclusion were compared in the models.\n Results: The residual distribution of the RF model indicated that nearly all M-spike values determined using the 43 variables distributed equally on either side of zero (Fig. 1). The weighted value of each of the 43 independent variables was determined by individually removing a variable from the ML algorithm and measuring its effect on the mean squared error (MSE) (Fig.2). Removal of the first lagged M-spike, serum total protein, second-lagged M-spike, serum IgG, serum IgM, and serum IgA, had the greatest effects on the ML algorithm. M-spike values determined using the ML algorithm correlated highly with M-spike values determined using the laboratory measured SPEP values as indicated by the proximity of the Pearson and Spearman correlation coefficients to +1. Using the 43 variables, the Pearson coefficient was 0.96 and the Spearman coefficient was 0.91. Feature selected modeling was performed to reduce the variables needed to predict the M-spike. Five RF models with different predictors were selected for comparison. Model A included all 43 predictors, Model B included the ten most important variables, Model C the top five variables, Model D included the first and second-lagged M-spike and serum total protein, and Model E the first-lagged M-spike and serum total protein. The Pearson's r and RMSE (root mean square error) values were used to compare the models. Pearson's r values for Model A, B, C, and D were 0.96, 0.96, 0.96, and 0.95 respectively, and the RMSE values were 0.21, 0.19, 0.19, and 0.22. In Model E, feature selection used only two variables and accurately predicted the M-spike value (Pearson's r = 0.95; RMSE 0.22). The Pearson's r values for feature selected models A, B, C, D, and E were 0.95, 0.96, 0.96, 0.95 and 0.91.\n Conclusion: Accurate prompt engineering to create global assessment of Myeloma clone by LLM requires a curated set of variables that correlated with disease volume. Here, we developed an ML model for feature selection utilizing the same-day available data in the patient chart. Features could be used in order of importance to provide focused, comprehensive prompts that aligned with the patient's context. The quality of AI-assisted disease assessment using these models should be compared with assessments performed by real world providers in future studies to ensure that the LLMs generate a written assessment that accurately reflects the patient's health status.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A machine learning (ML) approach was utilized to define salient features in a typical visit day that correlates most significantly with disease volume on the same day and these features could be candidates to be incorporated into LLMs to develop a system-based assessment in clinic visits."
            },
            "score": 2
        },
        {
            "id": "e314d182fd9d35a05870b38a56ee38eb3149b47d",
            "paperId": "e314d182fd9d35a05870b38a56ee38eb3149b47d",
            "title": "Attack Prompt Generation for Red Teaming and Defending Large Language Models",
            "abstract": "Large language models (LLMs) are susceptible to red teaming attacks, which can induce LLMs to generate harmful content. Previous research constructs attack prompts via manual or automatic methods, which have their own limitations on construction cost and quality. To address these issues, we propose an integrated approach that combines manual and automatic methods to economically generate high-quality attack prompts. Specifically, considering the impressive capabilities of newly emerged LLMs, we propose an attack framework to instruct LLMs to mimic human-generated prompts through in-context learning. Furthermore, we propose a defense framework that fine-tunes victim LLMs through iterative interactions with the attack framework to enhance their safety against red teaming attacks. Extensive experiments on different LLMs validate the effectiveness of our proposed attack and defense frameworks. Additionally, we release a series of attack prompts datasets named SAP with varying sizes, facilitating the safety evaluation and enhancement of more LLMs. Our code and dataset is available on https://github.com/Aatrox103/SAP .",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes an attack framework to instruct LLMs to mimic human-generated prompts through in-context learning to enhance their safety against red teaming attacks, and proposes a defense framework that fine-tunes victim LLMs through iterative interactions with the attack framework."
            },
            "score": 2
        },
        {
            "id": "321cb42c7b97755b19cf7fae43fb2cb6b349841d",
            "paperId": "321cb42c7b97755b19cf7fae43fb2cb6b349841d",
            "title": "Towards Low-Resource Automatic Program Repair with Meta-Learning and Pretrained Language Models",
            "abstract": ",",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 2
        },
        {
            "id": "394236c1017a8460c0486e97ecb6379a0441f06c",
            "paperId": "394236c1017a8460c0486e97ecb6379a0441f06c",
            "title": "I Wish to Have an Argument: Argumentative Reasoning in Large Language Models",
            "abstract": "We evaluate the ability of contemporary large language models (LLMs) to perform argumentative reasoning. We frame our experiments in terms of the argument mining (AM) and argument pair extraction (APE) tasks, and evaluate their ability to perform reasoning at increasing levels of abstraction in the input and output representations (e.g., arbitrary label sets, semantic graphs). We find that, although LLMs are able to match or surpass the state-of-the-art in AM and APE, their argumentative reasoning performance is very dependent on the input and output representation. We also find an\"exemplar effect\", where too many exemplars increasingly become detrimental for task performance, and about 4-5 being the optimal amount. Neither result extends to chain-of-thought (CoT) prompting: we find the exemplar effect to be nullified, and our results suggest that CoT allows for better performance under ill-conditioned problems. We hope that the work reported contributes to the improvement of argumentative reasoning in LLMs.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The exemplar effect is found to be nullified, and the results suggest that CoT allows for better performance under ill-conditioned problems, and that the work reported contributes to the improvement of argumentative reasoning in LLMs."
            },
            "score": 2
        },
        {
            "id": "7637ed79d30d0139901175ae4abedd822c217ab4",
            "paperId": "7637ed79d30d0139901175ae4abedd822c217ab4",
            "title": "3D-LLM: Injecting the 3D World into Large Language Models",
            "abstract": "Large language models (LLMs) and Vision-Language Models (VLMs) have been proven to excel at multiple tasks, such as commonsense reasoning. Powerful as these models can be, they are not grounded in the 3D physical world, which involves richer concepts such as spatial relationships, affordances, physics, layout, and so on. In this work, we propose to inject the 3D world into large language models and introduce a whole new family of 3D-LLMs. Speci\ufb01cally, 3D-LLMs can take 3D point clouds and their features as input and perform a diverse set of 3D-related tasks, including captioning, dense captioning, 3D question answering, task decomposition, 3D grounding, 3D-assisted dialog, navigation, and so on. Using three types of prompting mechanisms that we design, we are able to collect over 300k 3D-language data covering these tasks. To ef\ufb01ciently train 3D-LLMs, we \ufb01rst utilize a 3D feature extractor that obtains 3D features from rendered multi-view images. Then, we use 2D VLMs as our backbones to train our 3D-LLMs. By introducing a 3D localization mechanism, 3D-LLMs can better capture 3D spatial information. Experiments on ScanQA show that our model outperforms state-of-the-art baselines by a large margin ( e.g. , the BLEU-1 score surpasses state-of-the-art score by 9%). Furthermore, experiments on our held-in datasets for 3D captioning, task composition, and 3D-assisted dialogue show that our model outperforms 2D VLMs. Qualitative examples also show that our model could perform more tasks beyond the scope of existing LLMs and VLMs. Project Page: : https:",
            "year": 2023,
            "citationCount": 70,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes to inject the 3D world into large language models and introduce a whole new family of 3D-LLMs, and introduces a 3D localization mechanism, which can better capture 3D spatial information."
            },
            "score": 2
        },
        {
            "id": "98fc8faae839cc9b9dbfcf8e3ec3a7e2f7729db2",
            "paperId": "98fc8faae839cc9b9dbfcf8e3ec3a7e2f7729db2",
            "title": "NOTABLE: Transferable Backdoor Attacks Against Prompt-based NLP Models",
            "abstract": "Prompt-based learning is vulnerable to backdoor attacks. Existing backdoor attacks against prompt-based models consider injecting backdoors into the entire embedding layers or word embedding vectors. Such attacks can be easily affected by retraining on downstream tasks and with different prompting strategies, limiting the transferability of backdoor attacks. In this work, we propose transferable backdoor attacks against prompt-based models, called NOTABLE, which is independent of downstream tasks and prompting strategies. Specifically, NOTABLE injects backdoors into the encoders of PLMs by utilizing an adaptive verbalizer to bind triggers to specific words (i.e., anchors). It activates the backdoor by pasting input with triggers to reach adversary-desired anchors, achieving independence from downstream tasks and prompting strategies. We conduct experiments on six NLP tasks, three popular models, and three prompting strategies. Empirical results show that NOTABLE achieves superior attack performance (i.e., attack success rate over 90% on all the datasets), and outperforms two state-of-the-art baselines. Evaluations on three defenses show the robustness of NOTABLE. Our code can be found at https://github.com/RU-System-Software-and-Security/Notable.",
            "year": 2023,
            "citationCount": 19,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes transferable backdoor attacks against prompt-based models, called NOTABLE, which is independent of downstream tasks and prompting strategies, and achieves superior attack performance and outperforms two state-of-the-art baselines."
            },
            "score": 2
        }
    ],
    "novelty": "yes"
}