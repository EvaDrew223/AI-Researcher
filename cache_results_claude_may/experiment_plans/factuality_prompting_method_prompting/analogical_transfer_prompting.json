{
    "topic_description": "novel prompting methods that can improve factuality and reduce hallucination of large language models",
    "idea_name": "Analogical Transfer Prompting",
    "raw_idea": {
        "Problem": "LLMs often struggle to transfer knowledge from one domain to another, leading to hallucinations and inconsistencies when encountering novel situations or tasks.",
        "Existing Methods": "Current approaches to improve knowledge transfer in LLMs include fine-tuning on diverse datasets or using prompts that encourage drawing analogies between different domains. However, these methods often fail to capture the deep structural similarities between seemingly disparate domains.",
        "Motivation": "Analogical reasoning is a powerful tool for transferring knowledge from well-understood domains to novel situations. By explicitly prompting LLMs to draw analogies between different domains, we can improve their ability to generate more factual and consistent responses in unfamiliar contexts.",
        "Proposed Method": "We propose Analogical Transfer Prompting (ATP), a multi-step prompting approach that guides LLMs to transfer knowledge across domains using analogical reasoning. The steps include: 1) Identifying the target domain and a source domain that shares structural similarities; 2) Mapping the key elements and relationships between the source and target domains; 3) Generating a response in the target domain by adapting the solution from the source domain based on the analogical mapping; 4) Verifying the factuality and consistency of the generated response by comparing it with the target domain knowledge; 5) Iteratively refining the response to incorporate domain-specific nuances.",
        "Experiment Plan": "Evaluate ATP on cross-domain transfer learning benchmarks such as XNLI and XQA. Compare performance with baselines such as zero-shot prompting and fine-tuning on multi-domain datasets. Assess the factuality and consistency of generated responses using metrics like F1 score and human evaluation."
    },
    "full_experiment_plan": {
        "Title": "Analogical Transfer Prompting: Improving Cross-Domain Knowledge Transfer in Large Language Models",
        "Problem Statement": "Large Language Models (LLMs) often struggle to transfer knowledge from one domain to another, leading to hallucinations and inconsistencies when encountering novel situations or tasks.",
        "Motivation": "Current approaches to improve knowledge transfer in LLMs, such as fine-tuning on diverse datasets or using prompts that encourage drawing analogies between different domains, often fail to capture the deep structural similarities between seemingly disparate domains. Analogical reasoning is a powerful tool for transferring knowledge from well-understood domains to novel situations. By explicitly prompting LLMs to draw analogies between different domains, we can improve their ability to generate more factual and consistent responses in unfamiliar contexts.",
        "Proposed Method": "We propose Analogical Transfer Prompting (ATP), a multi-step prompting approach that guides LLMs to transfer knowledge across domains using analogical reasoning. The steps include:\n1. Identifying the target domain and a source domain that shares structural similarities.\n2. Mapping the key elements and relationships between the source and target domains.\n3. Generating a response in the target domain by adapting the solution from the source domain based on the analogical mapping.\n4. Verifying the factuality and consistency of the generated response by comparing it with the target domain knowledge.\n5. Iteratively refining the response to incorporate domain-specific nuances.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "Evaluate ATP on cross-domain transfer learning benchmarks such as XNLI (cross-lingual natural language inference), XQA (cross-lingual question answering), and XCOPA (cross-lingual choice of plausible alternatives). These datasets cover a diverse range of domains and languages, making them suitable for testing the effectiveness of ATP in improving cross-domain knowledge transfer.",
            "Step 2: Construct Prompts": "Design a set of prompts for each step of the ATP approach:\n1. Domain Identification Prompt: Guide the LLM to identify the target domain and a structurally similar source domain. Example: \"Identify the target domain in the given question and a source domain that shares similar underlying principles or structures.\"\n2. Domain Mapping Prompt: Instruct the LLM to map the key elements and relationships between the source and target domains. Example: \"Map the key concepts, entities, and their relationships from the source domain to their counterparts in the target domain.\"\n3. Response Generation Prompt: Ask the LLM to generate a response in the target domain by adapting the solution from the source domain based on the analogical mapping. Example: \"Using the analogical mapping between the source and target domains, generate a response to the given question by adapting the solution from the source domain.\"\n4. Factuality Verification Prompt: Prompt the LLM to verify the factuality and consistency of the generated response by comparing it with the target domain knowledge. Example: \"Check the generated response for factual accuracy and consistency with the known information in the target domain.\"\n5. Response Refinement Prompt: Guide the LLM to iteratively refine the response by incorporating domain-specific nuances. Example: \"Refine the generated response by considering the unique characteristics and nuances of the target domain.\"",
            "Step 3: Select Models": "Evaluate ATP using state-of-the-art LLMs such as GPT-3.5 (text-davinci-003), GPT-4, and other multilingual models like mT5 and XLM-RoBERTa. Compare the performance of these models with and without ATP to assess the effectiveness of the proposed approach.",
            "Step 4: Establish Baselines": "Compare the performance of ATP with the following baselines:\n1. Zero-shot prompting: Directly prompt the LLMs with the target domain questions without any analogical reasoning or knowledge transfer.\n2. Fine-tuning on multi-domain datasets: Fine-tune the LLMs on diverse datasets covering multiple domains and evaluate their performance on the target domain questions.\n3. Existing analogical reasoning prompts: Use prompts that encourage drawing analogies between different domains without the structured multi-step approach proposed in ATP.",
            "Step 5: Evaluate and Analyze Results": "1. Measure the performance of ATP and the baselines using standard metrics such as accuracy, F1 score, and perplexity.\n2. Conduct a qualitative analysis of the generated responses to assess the factuality, consistency, and domain-specific nuances captured by ATP compared to the baselines.\n3. Investigate the impact of different factors, such as the choice of source domain, the quality of analogical mapping, and the number of refinement iterations, on the performance of ATP.\n4. Analyze the limitations and failure cases of ATP to identify potential areas for improvement."
        },
        "Test Case Examples": {
            "Test Case 1": {
                "Input": {
                    "Question": "In the context of social media, what is the equivalent of a virus in the biological domain?",
                    "Source Domain": "Biology",
                    "Target Domain": "Social Media"
                },
                "Baseline Output (Zero-Shot Prompting)": "In the context of social media, a piece of content that spreads rapidly and uncontrollably, such as a viral video or meme, could be considered equivalent to a virus in the biological domain.",
                "ATP Output": {
                    "Step 1: Domain Mapping": "Key elements in the biological domain: Virus, Host, Infection, Spread, Immune System\nCorresponding elements in the social media domain: Viral Content, Users, Sharing, Rapid Spread, Content Moderation",
                    "Step 2: Response Generation": "In the social media domain, a piece of viral content, such as a video, meme, or news article, can be considered equivalent to a virus in the biological domain. Just as a virus infects a host and spreads rapidly from one individual to another, viral content is shared by users and can spread quickly across social media platforms.",
                    "Step 3: Factuality Verification": "The generated response is factually accurate and consistent with the known characteristics of viral content on social media platforms. The analogy between a virus in the biological domain and viral content in the social media domain holds true.",
                    "Step 4: Response Refinement": "To further refine the response, we can consider the specific mechanisms by which viral content spreads on social media, such as algorithmic recommendations, trending topics, and user engagement. Additionally, the role of content moderation in controlling the spread of viral content can be likened to the immune system's response to a virus in the biological domain."
                },
                "Explanation": "The ATP approach guides the LLM to draw a structured analogy between the biological and social media domains, resulting in a more comprehensive and accurate response compared to the baseline zero-shot prompting. The multi-step process ensures that the key elements and relationships are mapped correctly, and the response is refined to incorporate domain-specific nuances."
            },
            "Test Case 2": {
                "Input": {
                    "Question": "In the context of a company, what is the equivalent of the heart in the human body?",
                    "Source Domain": "Human Body",
                    "Target Domain": "Company"
                },
                "Baseline Output (Fine-tuning on Multi-Domain Datasets)": "In the context of a company, the equivalent of the heart in the human body could be the CEO or the leadership team, as they play a central role in the functioning and decision-making of the organization.",
                "ATP Output": {
                    "Step 1: Domain Mapping": "Key elements in the human body domain: Heart, Blood, Circulation, Oxygen, Nutrients\nCorresponding elements in the company domain: Core Product/Service, Revenue, Distribution, Resources, Value",
                    "Step 2: Response Generation": "In a company, the core product or service can be considered equivalent to the heart in the human body. Just as the heart pumps blood to circulate oxygen and nutrients throughout the body, the core product or service generates revenue and distributes value to all parts of the organization.",
                    "Step 3: Factuality Verification": "The generated response is factually accurate and consistent with the known characteristics of a company's core product or service. The analogy between the heart in the human body and the core product or service in a company holds true.",
                    "Step 4: Response Refinement": "To further refine the response, we can consider how the core product or service is developed, marketed, and delivered to customers, similar to how the heart works in conjunction with other organs to maintain the body's overall health. Additionally, the importance of maintaining the quality and competitiveness of the core product or service can be likened to the importance of maintaining a healthy heart in the human body."
                },
                "Explanation": "The ATP approach generates a more accurate and nuanced response compared to the baseline fine-tuning on multi-domain datasets. By systematically mapping the key elements and relationships between the human body and company domains, ATP ensures that the analogy is coherent and meaningful. The iterative refinement process further enhances the response by incorporating domain-specific details."
            }
        },
        "Fallback Plan": "If the proposed ATP approach does not yield significant improvements over the baselines, consider the following alternative plans:\n1. Analyze the quality of the analogical mappings generated by the LLM to identify potential weaknesses or inconsistencies. Develop techniques to improve the mapping process, such as using structured knowledge bases or incorporating human feedback.\n2. Investigate the impact of different prompt formulations on the performance of ATP. Experiment with alternative prompts that may better guide the LLM in drawing meaningful analogies and transferring knowledge across domains.\n3. Explore the use of other knowledge transfer techniques in combination with ATP, such as meta-learning or transfer learning, to further improve the LLM's ability to adapt to novel domains.\n4. Conduct a thorough error analysis to identify the common types of mistakes made by ATP and the baselines. Use these insights to develop targeted strategies for addressing specific challenges in cross-domain knowledge transfer.\n5. If the performance of ATP remains unsatisfactory, consider pivoting the research to focus on understanding the limitations of current LLMs in analogical reasoning and cross-domain knowledge transfer. Conduct a comprehensive analysis of the factors that contribute to these limitations and propose potential directions for future research in this area."
    },
    "novelty_queries": [
        "KeywordQuery(\"analogical transfer prompting language models\")",
        "KeywordQuery(\"cross-domain knowledge transfer language models\")",
        "KeywordQuery(\"analogical reasoning language models\")",
        "KeywordQuery(\"Analogical Transfer Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "d4bf36cbc5855ea87235d7a64f406717ac6aa3c9",
            "paperId": "d4bf36cbc5855ea87235d7a64f406717ac6aa3c9",
            "title": "Large Language Models as Analogical Reasoners",
            "abstract": "Chain-of-thought (CoT) prompting for language models demonstrates impressive performance across reasoning tasks, but typically needs labeled exemplars of the reasoning process. In this work, we introduce a new prompting approach, analogical prompting, designed to automatically guide the reasoning process of large language models. Inspired by analogical reasoning, a cognitive process in which humans draw from relevant past experiences to tackle new problems, our approach prompts language models to self-generate relevant exemplars or knowledge in the context, before proceeding to solve the given problem. This method presents several advantages: it obviates the need for labeling or retrieving exemplars, offering generality and convenience; it can also tailor the generated exemplars and knowledge to each problem, offering adaptability. Experimental results show that our approach outperforms 0-shot CoT and manual few-shot CoT in a variety of reasoning tasks, including math problem solving in GSM8K and MATH, code generation in Codeforces, and other reasoning tasks in BIG-Bench.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results show that this approach outperforms 0-shot CoT and manual few-shot CoT in a variety of reasoning tasks, including math problem solving in GSM8K and MATH, code generation in Codeforces, and other reasoning tasks in BIG-Bench."
            },
            "score": 8,
            "novelty_score": "The project proposal aims to improve cross-domain knowledge transfer in large language models using a multi-step analogical prompting approach called Analogical Transfer Prompting (ATP). The paper proposes a new prompting approach called analogical prompting, which guides the reasoning process of large language models by prompting them to self-generate relevant exemplars or knowledge before solving the given problem.\n\nWhile both the project proposal and the paper focus on improving the reasoning capabilities of large language models using analogical reasoning, there are some key differences:\n\n1. The project proposal specifically targets cross-domain knowledge transfer, while the paper addresses general reasoning tasks.\n2. The project proposal introduces a structured, multi-step prompting approach (ATP), whereas the paper proposes a more general analogical prompting method that generates relevant exemplars or knowledge.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "3784fd84b61d482b52f7ef72aac66bcb886b892b",
            "paperId": "3784fd84b61d482b52f7ef72aac66bcb886b892b",
            "title": "Thought Propagation: An Analogical Approach to Complex Reasoning with Large Language Models",
            "abstract": "Large Language Models (LLMs) have achieved remarkable success in reasoning tasks with the development of prompting methods. However, existing prompting approaches cannot reuse insights of solving similar problems and suffer from accumulated errors in multi-step reasoning, since they prompt LLMs to reason \\textit{from scratch}. To address these issues, we propose \\textbf{\\textit{Thought Propagation} (TP)}, which explores the analogous problems and leverages their solutions to enhance the complex reasoning ability of LLMs. These analogous problems are related to the input one, with reusable solutions and problem-solving strategies. Thus, it is promising to propagate insights of solving previous analogous problems to inspire new problem-solving. To achieve this, TP first prompts LLMs to propose and solve a set of analogous problems that are related to the input one. Then, TP reuses the results of analogous problems to directly yield a new solution or derive a knowledge-intensive plan for execution to amend the initial solution obtained from scratch. TP is compatible with existing prompting approaches, allowing plug-and-play generalization and enhancement in a wide range of tasks without much labor in task-specific prompt engineering. Experiments across three challenging tasks demonstrate TP enjoys a substantial improvement over the baselines by an average of 12\\% absolute increase in finding the optimal solutions in Shortest-path Reasoning, 13\\% improvement of human preference in Creative Writing, and 15\\% enhancement in the task completion rate of LLM-Agent Planning.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Thought Propagation (TP) is proposed, which explores the analogous problems and leverages their solutions to enhance the complex reasoning ability of LLMs and is compatible with existing prompting approaches, allowing plug-and-play generalization and enhancement in a wide range of tasks without much labor in task-specific prompt engineering."
            },
            "score": 8,
            "novelty_score": "The project proposal aims to improve cross-domain knowledge transfer in large language models using analogical reasoning through a multi-step prompting approach called Analogical Transfer Prompting (ATP).\n\nThe paper proposes Thought Propagation (TP), which leverages solutions from analogous problems to enhance the complex reasoning ability of large language models.\n\nWhile both the project proposal and the paper use analogical reasoning to improve the performance of large language models, the project proposal focuses specifically on cross-domain knowledge transfer, while the paper addresses complex reasoning tasks in general. Additionally, the project proposal introduces a structured multi-step prompting approach (ATP), whereas the paper's method (TP) involves proposing and solving analogous problems to inspire new problem-solving.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "8c33ffd4b3bc0e0af6d85c1b71f3cecf7cde7160",
            "paperId": "8c33ffd4b3bc0e0af6d85c1b71f3cecf7cde7160",
            "title": "Meta Distant Transfer Learning for Pre-trained Language Models",
            "abstract": "With the wide availability of Pre-trained Language Models (PLMs), multi-task fine-tuning across domains has been extensively applied. For tasks related to distant domains with different class label sets, PLMs may memorize non-transferable knowledge for the target domain and suffer from negative transfer. Inspired by meta-learning, we propose the Meta Distant Transfer Learning (Meta-DTL) framework to learn the cross-task knowledge for PLM-based methods. Meta-DTL first employs task representation learning to mine implicit relations among multiple tasks and classes. Based on the results, it trains a PLM-based meta-learner to capture the transferable knowledge across tasks. The weighted maximum entropy regularizers are proposed to make meta-learner more task-agnostic and unbiased. Finally, the meta-learner can be fine-tuned to fit each task with better parameter initialization. We evaluate Meta-DTL using both BERT and ALBERT on seven public datasets. Experiment results confirm the superiority of Meta-DTL as it consistently outperforms strong baselines. We find that Meta-DTL is highly effective when very few data is available for the target task.",
            "year": 2021,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Inspired by meta-learning, the Meta Distant Transfer Learning (Meta-DTL) framework is proposed to learn the cross-task knowledge for PLM-based methods and is found to be highly effective when very few data is available for the target task."
            },
            "score": 7,
            "novelty_score": "The research problem in the project proposal is improving cross-domain knowledge transfer in large language models using analogical reasoning. The proposed approach is Analogical Transfer Prompting (ATP), a multi-step prompting method that guides LLMs to transfer knowledge across domains using analogical reasoning.\n\nThe research problem in the paper is improving cross-domain knowledge transfer for pre-trained language models in distant domains with different class label sets. The proposed approach is Meta Distant Transfer Learning (Meta-DTL), which employs task representation learning to mine implicit relations among multiple tasks and classes, and trains a PLM-based meta-learner to capture the transferable knowledge across tasks.\n\nWhile both the project proposal and the paper aim to improve cross-domain knowledge transfer in language models, their approaches differ. The project proposal focuses on using analogical reasoning and multi-step prompting, while the paper proposes a meta-learning framework to learn cross-task knowledge.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "0366177b44ed13d86b9d704a3a82ea3750e5abed",
            "paperId": "0366177b44ed13d86b9d704a3a82ea3750e5abed",
            "title": "In-Context Analogical Reasoning with Pre-Trained Language Models",
            "abstract": "Analogical reasoning is a fundamental capacity of human cognition that allows us to reason abstractly about novel situations by relating them to past experiences. While it is thought to be essential for robust reasoning in AI systems, conventional approaches require significant training and/or hard-coding of domain knowledge to be applied to benchmark tasks. Inspired by cognitive science research that has found connections between human language and analogy-making, we explore the use of intuitive language-based abstractions to support analogy in AI systems. Specifically, we apply large pre-trained language models (PLMs) to visual Raven\u2019s Progressive Matrices (RPM), a common relational reasoning test. By simply encoding the perceptual features of the problem into language form, we find that PLMs exhibit a striking capacity for zero-shot relational reasoning, exceeding human performance and nearing supervised vision-based methods. We explore different encodings that vary the level of abstraction over task features, finding that higher-level abstractions further strengthen PLMs\u2019 analogical reasoning. Our detailed analysis reveals insights on the role of model complexity, in-context learning, and prior knowledge in solving RPM tasks.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work applies large pre-trained language models (PLMs) to visual Raven\u2019s Progressive Matrices, a common relational reasoning test, and finds that PLMs exhibit a striking capacity for zero-shot relational reasoning, exceeding human performance and nearing supervised vision-based methods."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving cross-domain knowledge transfer in large language models using analogical reasoning. The proposed approach is Analogical Transfer Prompting (ATP), a multi-step prompting method that guides LLMs to transfer knowledge across domains using analogical reasoning.\n\nThe research problem in the paper is exploring the use of language-based abstractions to support analogical reasoning in AI systems. The approach is applying large pre-trained language models to visual Raven's Progressive Matrices by encoding the perceptual features of the problem into language form.\n\nWhile both the proposal and the paper focus on analogical reasoning, the domains and specific approaches differ. The proposal targets cross-domain knowledge transfer in language models, while the paper explores analogical reasoning in visual problem-solving using language-based abstractions.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "64410909714f421c153ac123f975f86cc15c1fec",
            "paperId": "64410909714f421c153ac123f975f86cc15c1fec",
            "title": "StoryAnalogy: Deriving Story-level Analogies from Large Language Models to Unlock Analogical Understanding",
            "abstract": "Analogy-making between narratives is crucial for human reasoning. In this paper, we evaluate the ability to identify and generate analogies by constructing a first-of-its-kind large-scale story-level analogy corpus, \\textsc{StoryAnalogy}, which contains 24K story pairs from diverse domains with human annotations on two similarities from the extended Structure-Mapping Theory. We design a set of tests on \\textsc{StoryAnalogy}, presenting the first evaluation of story-level analogy identification and generation. Interestingly, we find that the analogy identification tasks are incredibly difficult not only for sentence embedding models but also for the recent large language models (LLMs) such as ChatGPT and LLaMa. ChatGPT, for example, only achieved around 30% accuracy in multiple-choice questions (compared to over 85% accuracy for humans). Furthermore, we observe that the data in \\textsc{StoryAnalogy} can improve the quality of analogy generation in LLMs, where a fine-tuned FlanT5-xxl model achieves comparable performance to zero-shot ChatGPT.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is observed that the data in \\textsc{StoryAnalogy} can improve the quality of analogy generation in LLMs, where a fine-tuned FlanT5-xxl model achieves comparable performance to zero-shot ChatGPT."
            },
            "score": 7,
            "novelty_score": "The research problem in the project proposal is improving cross-domain knowledge transfer in large language models using analogical reasoning. The proposed approach is Analogical Transfer Prompting (ATP), a multi-step prompting method that guides LLMs to transfer knowledge across domains using analogical reasoning.\n\nThe research problem in the paper is evaluating the ability of large language models to identify and generate story-level analogies. The approach is constructing a large-scale story-level analogy corpus, StoryAnalogy, and designing a set of tests to evaluate the performance of LLMs on analogy identification and generation tasks.\n\nWhile both the project proposal and the paper focus on analogical reasoning in large language models, the specific research problems and approaches are different. The project proposal aims to improve cross-domain knowledge transfer using a novel prompting method, while the paper focuses on evaluating the ability of LLMs to identify and generate story-level analogies using a new dataset.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "3301fc6a084c7b09a60f24edcf5502c00e421989",
            "paperId": "3301fc6a084c7b09a60f24edcf5502c00e421989",
            "title": "ARN: Analogical Reasoning on Narratives",
            "abstract": "As a core cognitive skill that enables the transferability of information across domains, analogical reasoning has been extensively studied for both humans and computational models. However, while cognitive theories of analogy often focus on narratives and study the distinction between surface, relational, and system similarities, existing work in natural language processing has a narrower focus as far as relational analogies between word pairs. This gap brings a natural question: can state-of-the-art large language models (LLMs) detect system analogies between narratives? To gain insight into this question and extend word-based relational analogies to relational system analogies, we devise a comprehensive computational framework that operationalizes dominant theories of analogy, using narrative elements to create surface and system mappings. Leveraging the interplay between these mappings, we create a binary task and benchmark for Analogical Reasoning on Narratives (ARN), covering four categories of far (cross-domain)/near (within-domain) analogies and disanalogies. We show that while all LLMs can largely recognize near analogies, even the largest ones struggle with far analogies in a zero-shot setting, with GPT4.0 scoring below random. Guiding the models through solved examples and chain-of-thought reasoning enhances their analogical reasoning ability. Yet, since even in the few-shot setting, the best model only performs halfway between random and humans, ARN opens exciting directions for computational analogical reasoners.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A comprehensive computational framework that operationalizes dominant theories of analogy, using narrative elements to create surface and system mappings, and creates a binary task and benchmark for Analogical Reasoning on Narratives (ARN), covering four categories of far (cross-domain)/near (within-domain) analogies and disanalogies."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve cross-domain knowledge transfer in large language models using a multi-step analogical prompting approach called Analogical Transfer Prompting (ATP). The paper, on the other hand, focuses on evaluating the ability of large language models to detect system analogies between narratives using a comprehensive computational framework and a benchmark called Analogical Reasoning on Narratives (ARN).\n\nWhile both the project proposal and the paper deal with analogical reasoning in large language models, their research problems and approaches differ. The project proposal seeks to enhance knowledge transfer across domains using analogical prompting, whereas the paper aims to assess the models' capability to recognize system analogies between narratives using a benchmark dataset.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "b2ee9afe5142bd47163c4261a544e77a622023e0",
            "paperId": "b2ee9afe5142bd47163c4261a544e77a622023e0",
            "title": "Relevant or Random: Can LLMs Truly Perform Analogical Reasoning?",
            "abstract": "Analogical reasoning is a unique ability of humans to address unfamiliar challenges by transferring strategies from relevant past experiences. One key finding in psychology is that compared with irrelevant past experiences, recalling relevant ones can help humans better handle new tasks. Coincidentally, the NLP community has also recently found that self-generating relevant examples in the context can help large language models (LLMs) better solve a given problem than hand-crafted prompts. However, it is yet not clear whether relevance is the key factor eliciting such capability, i.e., can LLMs benefit more from self-generated relevant examples than irrelevant ones? In this work, we systematically explore whether LLMs can truly perform analogical reasoning on a diverse set of reasoning tasks. With extensive experiments and analysis, we show that self-generated random examples can surprisingly achieve comparable or even better performance, e.g., 4% performance boost on GSM8K with random biological examples. We find that the accuracy of self-generated examples is the key factor and subsequently design two improved methods with significantly reduced inference costs. Overall, we aim to advance a deeper understanding of LLM analogical reasoning and hope this work stimulates further research in the design of self-generated contexts.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work systematically explores whether LLMs can truly perform analogical reasoning on a diverse set of reasoning tasks and finds that the accuracy of self-generated examples is the key factor and subsequently design two improved methods with significantly reduced inference costs."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving cross-domain knowledge transfer in large language models using analogical reasoning. The proposed approach is Analogical Transfer Prompting (ATP), a multi-step prompting method that guides LLMs to transfer knowledge across domains using analogical reasoning.\n\nThe research problem in the paper is exploring whether large language models can truly perform analogical reasoning on various tasks. The approach is to systematically investigate if self-generated relevant examples are more beneficial than irrelevant ones for LLMs in solving problems.\n\nWhile both the proposal and the paper focus on analogical reasoning in large language models, their specific research problems and approaches differ. The proposal aims to improve cross-domain knowledge transfer using a novel prompting method, while the paper investigates the importance of relevance in self-generated examples for LLMs' analogical reasoning capabilities.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "fb49e88c6bd676516898e911e42b4f8479e6f1bf",
            "paperId": "fb49e88c6bd676516898e911e42b4f8479e6f1bf",
            "title": "Ask Me Anything: A simple strategy for prompting language models",
            "abstract": "Large language models (LLMs) transfer well to new tasks out-of-the-box simply given a natural language prompt that demonstrates how to perform the task and no additional training. Prompting is a brittle process wherein small modifications to the prompt can cause large variations in the model predictions, and therefore significant effort is dedicated towards designing a painstakingly\"perfect prompt\"for a task. To mitigate the high degree of effort involved in prompt-design, we instead ask whether producing multiple effective, yet imperfect, prompts and aggregating them can lead to a high quality prompting strategy. Our observations motivate our proposed prompting method, ASK ME ANYTHING (AMA). We first develop an understanding of the effective prompt formats, finding that question-answering (QA) prompts, which encourage open-ended generation (\"Who went to the park?\") tend to outperform those that restrict the model outputs (\"John went to the park. Output True or False.\"). Our approach recursively uses the LLM itself to transform task inputs to the effective QA format. We apply the collected prompts to obtain several noisy votes for the input's true label. We find that the prompts can have very different accuracies and complex dependencies and thus propose to use weak supervision, a procedure for combining the noisy predictions, to produce the final predictions for the inputs. We evaluate AMA across open-source model families (e.g., EleutherAI, BLOOM, OPT, and T0) and model sizes (125M-175B parameters), demonstrating an average performance lift of 10.2% over the few-shot baseline. This simple strategy enables the open-source GPT-J-6B model to match and exceed the performance of few-shot GPT3-175B on 15 of 20 popular benchmarks. Averaged across these tasks, the GPT-J-6B model outperforms few-shot GPT3-175B. We release our code here: https://github.com/HazyResearch/ama_prompting",
            "year": 2022,
            "citationCount": 117,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work develops an understanding of the effective prompt formats and proposes to use weak supervision, a procedure for combining the noisy predictions, to produce the final predictions for the inputs of a large language model."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve cross-domain knowledge transfer in large language models using analogical reasoning and a multi-step prompting approach called Analogical Transfer Prompting (ATP).\n\nThe paper abstract proposes a simple prompting strategy called ASK ME ANYTHING (AMA) that uses the language model itself to transform task inputs into effective question-answering prompts and aggregates the noisy predictions using weak supervision to improve performance on various benchmarks.\n\nThe project proposal and the paper abstract address different research problems and propose different approaches. The project focuses on cross-domain knowledge transfer using analogical reasoning, while the paper focuses on improving prompting strategies by transforming inputs into question-answering format and aggregating noisy predictions.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "02ea85153de3b43162892e1e740975500878bbfd",
            "paperId": "02ea85153de3b43162892e1e740975500878bbfd",
            "title": "Heuristic-Driven Link-of-Analogy Prompting: Enhancing Large Language Models for Document-Level Event Argument Extraction",
            "abstract": "In this study, we investigate in-context learning (ICL) in document-level event argument extraction (EAE) to alleviate the dependency on large-scale labeled data for this task. We introduce the Heuristic-Driven Link-of-Analogy (HD-LoA) prompting to address the challenge of example selection and to develop a prompting strategy tailored for EAE. Specifically, we hypothesize and validate that LLMs learn task-specific heuristics from demonstrations via ICL. Building upon this hypothesis, we introduce an explicit heuristic-driven demonstration construction approach, which transforms the haphazard example selection process into a methodical method that emphasizes task heuristics. Additionally, inspired by the analogical reasoning of human, we propose the link-of-analogy prompting, which enables LLMs to process new situations by drawing analogies to known situations, enhancing their performance on unseen classes beyond limited ICL examples. Experiments show that our method outperforms existing prompting methods and few-shot supervised learning methods on document-level EAE datasets. Additionally, the HD-LoA prompting shows effectiveness in diverse tasks like sentiment analysis and natural language inference, demonstrating its broad adaptability.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Heuristic-Driven Link-of-Analogy prompting is introduced, which enables LLMs to process new situations by drawing analogies to known situations, enhancing their performance on unseen classes beyond limited ICL examples."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve cross-domain knowledge transfer in large language models using analogical transfer prompting, while the paper focuses on enhancing document-level event argument extraction using heuristic-driven link-of-analogy prompting.\n\nThe project proposal addresses the problem of LLMs struggling to transfer knowledge across domains and proposes a multi-step analogical prompting approach to guide LLMs in transferring knowledge using analogical reasoning. On the other hand, the paper investigates in-context learning for document-level event argument extraction and introduces a heuristic-driven demonstration construction approach and link-of-analogy prompting to improve performance on unseen classes.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "f118a713952c49f96382d9b8fdc153d9f54bcd3c",
            "paperId": "f118a713952c49f96382d9b8fdc153d9f54bcd3c",
            "title": "Multimodal Hate Speech Detection via Cross-Domain Knowledge Transfer",
            "abstract": "Nowadays, the hate speech diffusion of texts and images in social network has become the mainstream compared with the diffusion of texts-only, raising the pressing needs of multimodal hate speech detection task. Current research on this task mainly focuses on the construction of multimodal models without considering the influence of the unbalanced and widely distributed samples for various attacks in hate speech. In this situation, introducing enhanced knowledge is necessary for understanding the attack category of hate speech comprehensively. Due to the high correlation between hate speech detection and sarcasm detection tasks, this paper makes an initial attempt of common knowledge transfer based on the above two tasks, where hate speech detection and sarcasm detection are defined as primary and auxiliary tasks, respectively. A scalable cross-domain knowledge transfer (CDKT) framework is proposed, where the mainstream vision-language transformer could be employed as backbone flexibly. Three modules are included, bridging the semantic, definition and domain gaps simultaneously between primary and auxiliary tasks. Specifically, semantic adaptation module formulates the irrelevant parts between image and text in primary and auxiliary tasks, and disentangles with the text representation to align the visual and word tokens. Definition adaptation module assigns different weights to the training samples of auxiliary task by measuring the correlation between samples of the auxiliary and primary task. Domain adaptation module minimizes the feature distribution gap of samples in two tasks. Extensive experiments show that the proposed CDKT provides a stable improvement compared with baselines and produces a competitive performance compared with some existing multimodal hate speech detection methods.",
            "year": 2022,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An initial attempt of common knowledge transfer based on the high correlation between hate speech detection and sarcasm detection tasks is made, and a scalable cross-domain knowledge transfer (CDKT) framework is proposed, where the mainstream vision-language transformer could be employed as backbone flexibly."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve cross-domain knowledge transfer in large language models using analogical transfer prompting, while the paper focuses on multimodal hate speech detection by transferring knowledge from the sarcasm detection task using a cross-domain knowledge transfer framework.\n\nProject Proposal: Improving cross-domain knowledge transfer in LLMs using analogical transfer prompting.\nPaper: Multimodal hate speech detection via cross-domain knowledge transfer from sarcasm detection.\n\nThe project proposal and the paper address different research problems and propose different approaches, although both involve cross-domain knowledge transfer.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "acaa8c957d75b67088eabc49bea7ccbaccf8a4f3",
            "paperId": "acaa8c957d75b67088eabc49bea7ccbaccf8a4f3",
            "title": "Cross-Domain Sentiment Classification With Bidirectional Contextualized Transformer Language Models",
            "abstract": "Cross-domain sentiment classification is an important Natural Language Processing (NLP) task that aims at leveraging knowledge obtained from a source domain to train a high-performance learner for sentiment classification on a target domain. Existing transfer learning methods applied on cross-domain sentiment classification mostly focus on inducing a low-dimensional feature representation shared across domains based on pivots and non-pivots, which is still a low-level representation of sequence data. Recently, there have been great progress in the NLP literature in developing high-level representation language models based on Transformer architecture, which are pre-trained on large text corpus and fine-tuned for specific task with an additional layer on top. Among such language models, the bidirectional contextualized Transformer language models of BERT and XLNet have greatly impacted NLP research field. In this paper, we fine-tune BERT and XLNet for the cross-domain sentiment classification. We then explore their transferability in the context of cross-domain sentiment classification through in-depth analysis of two models\u2019 performances and update the state-of-the-arts with a significant margin of improvement. Our results show that such bidirectional contextualized language models outperform the previous state-of-the-arts methods for cross-domain sentiment classification while using up to 120 times less data.",
            "year": 2019,
            "citationCount": 38,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results show that such bidirectional contextualized Transformer language models outperform the previous state-of-the-arts methods for cross-domain sentiment classification while using up to 120 times less data."
            },
            "score": 6
        },
        {
            "id": "3ea6575d90f72948b16860cd52abacdaf5d90471",
            "paperId": "3ea6575d90f72948b16860cd52abacdaf5d90471",
            "title": "Neural machine translation of clinical text: an empirical investigation into multilingual pre-trained language models and transfer-learning",
            "abstract": "Clinical text and documents contain very rich information and knowledge in healthcare, and their processing using state-of-the-art language technology becomes very important for building intelligent systems for supporting healthcare and social good. This processing includes creating language understanding models and translating resources into other natural languages to share domain-specific cross-lingual knowledge. In this work, we conduct investigations on clinical text machine translation by examining multilingual neural network models using deep learning such as Transformer based structures. Furthermore, to address the language resource imbalance issue, we also carry out experiments using a transfer learning methodology based on massive multilingual pre-trained language models (MMPLMs). The experimental results on three sub-tasks including (1) clinical case (CC), (2) clinical terminology (CT), and (3) ontological concept (OC) show that our models achieved top-level performances in the ClinSpEn-2022 shared task on English-Spanish clinical domain data. Furthermore, our expert-based human evaluations demonstrate that the small-sized pre-trained language model (PLM) outperformed the other two extra-large language models by a large margin in the clinical domain fine-tuning, which finding was never reported in the field. Finally, the transfer learning method works well in our experimental setting using the WMT21fb model to accommodate a new language space Spanish that was not seen at the pre-training stage within WMT21fb itself, which deserves more exploitation for clinical knowledge transformation, e.g. to investigate into more languages. These research findings can shed some light on domain-specific machine translation development, especially in clinical and healthcare fields. Further research projects can be carried out based on our work to improve healthcare text analytics and knowledge transformation. Our data is openly available for research purposes at: https://github.com/HECTA-UoM/ClinicalNMT.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Investigations on clinical text machine translation are conducted by examining multilingual neural network models using deep learning such as Transformer based structures and a transfer learning methodology based on massive multilingual pre-trained language models (MMPLMs) are carried out."
            },
            "score": 6
        },
        {
            "id": "5033e1be6d12d8812798240d30769dcd1f39eca9",
            "paperId": "5033e1be6d12d8812798240d30769dcd1f39eca9",
            "title": "One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain NER",
            "abstract": "Cross-domain NER is a challenging task to address the low-resource problem in practical scenarios. Previous typical solutions mainly obtain a NER model by pre-trained language models (PLMs) with data from a rich-resource domain and adapt it to the target domain. Owing to the mismatch issue among entity types in different domains, previous approaches normally tune all parameters of PLMs, ending up with an entirely new NER model for each domain. Moreover, current models only focus on leveraging knowledge in one general source domain while failing to successfully transfer knowledge from multiple sources to the target. To address these issues, we introduce Collaborative Domain-Prefix Tuning for cross-domain NER (CP-NER) based on text-to-text generative PLMs. Specifically, we present text-to-text generation grounding domain-related instructors to transfer knowledge to new domain NER tasks without structural modifications. We utilize frozen PLMs and conduct collaborative domain-prefix tuning to stimulate the potential of PLMs to handle NER tasks across various domains. Experimental results on the Cross-NER benchmark show that the proposed approach has flexible transfer ability and performs better on both one-source and multiple-source cross-domain NER tasks.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results on the Cross-NER benchmark show that the proposed approach has flexible transfer ability and performs better on both one-source and multiple-source cross-domain NER tasks."
            },
            "score": 6
        },
        {
            "id": "4ade6bf63d8c90ab3631c1800c1fbb9045aa69cd",
            "paperId": "4ade6bf63d8c90ab3631c1800c1fbb9045aa69cd",
            "title": "Dynamic Gazetteer Integration in Multilingual Models for Cross-Lingual and Cross-Domain Named Entity Recognition",
            "abstract": "Named entity recognition (NER) in a real-world setting remains challenging and is impacted by factors like text genre, corpus quality, and data availability. NER models trained on CoNLL do not transfer well to other domains, even within the same language. This is especially the case for multi-lingual models when applied to low-resource languages, and is mainly due to missing entity information. We propose an approach that with limited effort and data, addresses the NER knowledge gap across languages and domains. Our novel approach uses a token-level gating layer to augment pre-trained multilingual transformers with gazetteers containing named entities (NE) from a target language or domain.This approach provides the flexibility to jointly integrate both textual and gazetteer information dynamically: entity knowledge from gazetteers is used only when a token\u2019s textual representation is insufficient for the NER task.Evaluation on several languages and domains demonstrates: (i) a high mismatch of reported NER performance on CoNLL vs. domain specific datasets, (ii) gazetteers significantly improve NER performance across languages and domains, and (iii) gazetteers can be flexibly incorporated to guide knowledge transfer. On cross-lingual transfer we achieve an improvement over the baseline with F1=+17.6%, and with F1=+21.3% for cross-domain transfer.",
            "year": 2022,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This novel approach uses a token-level gating layer to augment pre-trained multilingual transformers with gazetteers containing named entities (NE) from a target language or domain to address the NER knowledge gap across languages and domains."
            },
            "score": 6
        },
        {
            "id": "0adf1a0caf60b805919c7ef5d6b09a99d6adedf9",
            "paperId": "0adf1a0caf60b805919c7ef5d6b09a99d6adedf9",
            "title": "Cross-Lingual Knowledge Transfer for Clinical Phenotyping",
            "abstract": "Clinical phenotyping enables the automatic extraction of clinical conditions from patient records, which can be beneficial to doctors and clinics worldwide. However, current state-of-the-art models are mostly applicable to clinical notes written in English. We therefore investigate cross-lingual knowledge transfer strategies to execute this task for clinics that do not use the English language and have a small amount of in-domain data available. Our results reveal two strategies that outperform the state-of-the-art: Translation-based methods in combination with domain-specific encoders and cross-lingual encoders plus adapters. We find that these strategies perform especially well for classifying rare phenotypes and we advise on which method to prefer in which situation. Our results show that using multilingual data overall improves clinical phenotyping models and can compensate for data sparseness.",
            "year": 2022,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates cross-lingual knowledge transfer strategies to execute this task for clinics that do not use the English language and have a small amount of in-domain data available, and shows that using multilingual data overall improves clinical phenotyping models and can compensate for data sparseness."
            },
            "score": 6
        },
        {
            "id": "48c7430d4ed6028550b8f293e7ec13309b4ee5b0",
            "paperId": "48c7430d4ed6028550b8f293e7ec13309b4ee5b0",
            "title": "Unsupervised Cross-Domain Adaptation for Response Selection Using Self-Supervised and Adversarial Training",
            "abstract": "Recently, many neural context-response matching models have been developed for retrieval-based dialogue systems. Although existing models achieve impressive performance through learning on a large amount of in-domain parallel dialogue data, they usually perform worse in another new domain. How to transfer a response retrieval model trained in high-resource domains to other low-resource domains is a crucial problem for scalable dialogue systems. To this end, we investigate the unsupervised cross-domain adaptation for response selection when the target domain has no parallel dialogue data. Specifically, we propose a two-stage method to adapt a response selection model to a new domain using self-supervised and adversarial training based on pre-trained language models (PLMs). To efficiently incorporate domain awareness and target-domain knowledge to PLMs, we first design a self-supervised post-training procedure, including domain discrimination (DD) task, target-domain masked language model (MLM) task and target-domain next sentence prediction (NSP) task. Based on this, we further conduct the adversarial fine-tuning to empower the model to match the proper response with extracted domain-shared features as much as possible. Experimental results show that our proposed method achieves consistent and significant improvements on several cross-domain response selection datasets.",
            "year": 2022,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a two-stage method to adapt a response selection model to a new domain using self-supervised and adversarial training based on pre-trained language models (PLMs) to efficiently incorporate domain awareness and target-domain knowledge to PLMs."
            },
            "score": 6
        },
        {
            "id": "45efb5974a3b9288f5bbc5076a700d22df5e380c",
            "paperId": "45efb5974a3b9288f5bbc5076a700d22df5e380c",
            "title": "PDALN: Progressive Domain Adaptation over a Pre-trained Model for Low-Resource Cross-Domain Named Entity Recognition",
            "abstract": "Cross-domain Named Entity Recognition (NER) transfers the NER knowledge from high-resource domains to the low-resource target domain. Due to limited labeled resources and domain shift, cross-domain NER is a challenging task. To address these challenges, we propose a progressive domain adaptation Knowledge Distillation (KD) approach \u2013 PDALN. It achieves superior domain adaptability by employing three components: (1) Adaptive data augmentation techniques, which alleviate cross-domain gap and label sparsity simultaneously; (2) Multi-level Domain invariant features, derived from a multi-grained MMD (Maximum Mean Discrepancy) approach, to enable knowledge transfer across domains; (3) Advanced KD schema, which progressively enables powerful pre-trained language models to perform domain adaptation. Extensive experiments on four benchmarks show that PDALN can effectively adapt high-resource domains to low-resource target domains, even if they are diverse in terms and writing styles. Comparison with other baselines indicates the state-of-the-art performance of PDALN.",
            "year": 2021,
            "citationCount": 19,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "PDALN can effectively adapt high-resource domains to low-resource target domains, even if they are diverse in terms and writing styles, and comparison with other baselines indicates the state-of-the-art performance of PDALN."
            },
            "score": 6
        },
        {
            "id": "3cbffab9d7981da6662d474aaa056dcbd3c1701e",
            "paperId": "3cbffab9d7981da6662d474aaa056dcbd3c1701e",
            "title": "Emergent analogical reasoning in large language models",
            "abstract": null,
            "year": 2022,
            "citationCount": 135,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that new artificial intelligence language models, such as Generative Pre-trained Transformer 3, are able to solve analogical reasoning problems at a human-like level of performance."
            },
            "score": 6
        },
        {
            "id": "f7f577438201651655847762cd49839ebb8378d3",
            "paperId": "f7f577438201651655847762cd49839ebb8378d3",
            "title": "Can language models learn analogical reasoning? Investigating training objectives and comparisons to human performance",
            "abstract": "While analogies are a common way to evaluate word embeddings in NLP, it is also of interest to investigate whether or not analogical reasoning is a task in itself that can be learned. In this paper, we test several ways to learn basic analogical reasoning, specifically focusing on analogies that are more typical of what is used to evaluate analogical reasoning in humans than those in commonly used NLP benchmarks. Our experiments find that models are able to learn analogical reasoning, even with a small amount of data. We additionally compare our models to a dataset with a human baseline, and find that after training, models approach human performance.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper tests several ways to learn basic analogical reasoning, specifically focusing on analogies that are more typical of what is used to evaluate analogical Reasoning in humans than those in commonly used NLP benchmarks."
            },
            "score": 6
        },
        {
            "id": "d0cbc7cdf5c9a2fd9426f255acb5ee29f3351640",
            "paperId": "d0cbc7cdf5c9a2fd9426f255acb5ee29f3351640",
            "title": "Fluid Transformers and Creative Analogies: Exploring Large Language Models\u2019 Capacity for Augmenting Cross-Domain Analogical Creativity",
            "abstract": "Cross-domain analogical reasoning is a core creative ability that can be challenging for humans. Recent work has shown some proofs-of-concept of Large language Models\u2019 (LLMs) ability to generate cross-domain analogies. However, the reliability and potential usefulness of this capacity for augmenting human creative work has received little systematic exploration. In this paper, we systematically explore LLMs capacity to augment cross-domain analogical reasoning. Across three studies, we found: 1) LLM-generated cross-domain analogies were frequently judged as helpful in the context of a problem reformulation task (median 4 out of 5 helpfulness rating), and frequently (\u223c 80% of cases) led to observable changes in problem formulations, and 2) there was an upper bound of \u223c 25% of outputs being rated as potentially harmful, with a majority due to potentially upsetting content, rather than biased or toxic content. These results demonstrate the potential utility \u2014 and risks \u2014 of LLMs for augmenting cross-domain analogical creativity.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LLM-generated cross-domain analogies were frequently judged as helpful in the context of a problem reformulation task, and frequently led to observable changes in problem formulations, and there was an upper bound of \u223c 25% of outputs being rated as potentially harmful."
            },
            "score": 6
        },
        {
            "id": "cdaa895850578d495c0837d70b811dc53241f4c8",
            "paperId": "cdaa895850578d495c0837d70b811dc53241f4c8",
            "title": "E-KAR: A Benchmark for Rationalizing Natural Language Analogical Reasoning",
            "abstract": "The ability to recognize analogies is fundamental to human cognition. Existing benchmarks to test word analogy do not reveal the underneath process of analogical reasoning of neural models. Holding the belief that models capable of reasoning should be right for the right reasons, we propose a first-of-its-kind Explainable Knowledge-intensive Analogical Reasoning benchmark (E-KAR). Our benchmark consists of 1,655 (in Chinese) and 1,251 (in English) problems sourced from the Civil Service Exams, which require intensive background knowledge to solve. More importantly, we design a free-text explanation scheme to explain whether an analogy should be drawn, and manually annotate them for each and every question and candidate answer. Empirical results suggest that this benchmark is very challenging for some state-of-the-art models for both explanation generation and analogical question answering tasks, which invites further research in this area.",
            "year": 2022,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A first-of-its-kind Explainable Knowledge-intensive Analogical Reasoning benchmark (E-KAR), which consists of 1,655 problems sourced from the Civil Service Exams, and a free-text explanation scheme to explain whether an analogy should be drawn."
            },
            "score": 6
        },
        {
            "id": "2521eea72ae96ecc4c5ef221b2fbae87caa4f716",
            "paperId": "2521eea72ae96ecc4c5ef221b2fbae87caa4f716",
            "title": "An Analogical Reasoning Method Based on Multi-task Learning with Relational Clustering",
            "abstract": "Analogical QA task is a challenging natural language processing problem. When two word pairs are similar in their relationships, we refer to their relations as analogous. Although the analogy method based on word embedding is well developed, the analogy reasoning is far beyond this scope. At present, the methods based on pre-trained language models have explored only the tip of the iceberg. In this paper, we proposed a multi-task learning method for analogical QA task. First, we obtain word-pair representations by leveraging the output embeddings of the [MASK] token in the pre-trained language model. The representations are prepared for two tasks. The first task aims to train an analogical classifier by supervised learning. The second task is an auxiliary task based on relation clustering to generate relation pseudo-labels for word pairs and train relation classifier. Our method guides the model to analyze the relation similarity in analogical reasoning without relation labels. The experiments show that our method achieve excellent performance on four analogical reasoning datasets without the help of external corpus and knowledge. In the most difficult data set E-KAR, it has increased by at least 4%.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper obtains word-pair representations by leveraging the output embeddings of the [MASK] token in the pre-trained language model and guides the model to analyze the relation similarity in analogical reasoning without relation labels."
            },
            "score": 6
        },
        {
            "id": "80c698688bb4488beaceaab5c64f701a946cb7ae",
            "paperId": "80c698688bb4488beaceaab5c64f701a946cb7ae",
            "title": "All in One: Multi-Task Prompting for Graph Neural Networks",
            "abstract": "Recently, \"pre-training and fine-tuning'' has been adopted as a standard workflow for many graph tasks since it can take general graph knowledge to relieve the lack of graph annotations from each application. However, graph tasks with node level, edge level, and graph level are far diversified, making the pre-training pretext often incompatible with these multiple tasks. This gap may even cause a \"negative transfer'' to the specific application, leading to poor results. Inspired by the prompt learning in natural language processing (NLP), which has presented significant effectiveness in leveraging prior knowledge for various NLP tasks, we study the prompting topic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models. Specifically, we first unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern. In this way, the prompting idea from NLP can be seamlessly introduced to the graph area. Then, to further narrow the gap between various graph tasks and state-of-the-art pre-training strategies, we further study the task space of various graph applications and reformulate downstream problems to the graph-level task. Afterward, we introduce meta-learning to efficiently learn a better initialization for the multi-task prompt of graphs so that our prompting framework can be more reliable and general for different tasks. We conduct extensive experiments, results from which demonstrate the superiority of our method.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel multi-task prompting method for graph models that unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern, and introduces meta-learning to efficiently learn a better initialization for the multi- task prompt of graphs so that the prompting framework can be more reliable and general for different tasks."
            },
            "score": 6
        },
        {
            "id": "c76bfa689922cb341c13cda437917cac8d0a7c4c",
            "paperId": "c76bfa689922cb341c13cda437917cac8d0a7c4c",
            "title": "All in One: Multi-Task Prompting for Graph Neural Networks (Extended Abstract)",
            "abstract": "This paper is an extended abstract of our original work published in KDD23, where we won the best research paper award (Xiangguo Sun, Hong Cheng, Jia Li, Bo Liu, and Jihong Guan. All in one: Multi-task prompting for graph neural networks. KDD 23) The paper introduces a novel approach to bridging the gap between pre-trained graph models and the diverse tasks they're applied to, inspired by the success of prompt learning in NLP. Recognizing the challenge of aligning pre-trained models with varied graph tasks (node level, edge level, and graph level), which can lead to negative transfer and poor performance, we propose a multi-task prompting method for graphs. This method involves unifying graph and language prompt formats, enabling NLP's prompting strategies to be adapted for graph tasks. By analyzing the task space of graph applications, we reformulate problems to fit graph-level tasks and apply meta-learning to improve prompt initialization for multiple tasks. Experiments show our method's effectiveness in enhancing model performance across different graph tasks. Beyond the original work, in this extended abstract, we further discuss the graph prompt from a bigger picture and provide some of the latest work toward this area.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a novel approach to bridging the gap between pre-trained graph models and the diverse tasks they're applied to, inspired by the success of prompt learning in NLP."
            },
            "score": 6
        },
        {
            "id": "ccfdea3722ec43e45fc33e170d1fe908dca003a5",
            "paperId": "ccfdea3722ec43e45fc33e170d1fe908dca003a5",
            "title": "TransPrompt v2: A Transferable Prompting Framework for Cross-task Text Classification",
            "abstract": "Text classification is one of the most imperative tasks in natural language processing (NLP). Recent advances with pre-trained language models (PLMs) have shown remarkable success on this task. However, the satisfying results obtained by PLMs heavily depend on the large amounts of task-specific labeled data, which may not be feasible in many application scenarios due to data access and privacy constraints. The recently-proposed prompt-based fine-tuning paradigm improves the performance of PLMs for few-shot text classification with task-specific templates. Yet, it is unclear how the prompting knowledge can be transferred across tasks, for the purpose of mutual reinforcement. We propose TransPrompt v2, a novel transferable prompting framework for few-shot learning across similar or distant text classification tasks. For learning across similar tasks, we employ a multi-task meta-knowledge acquisition (MMA) procedure to train a meta-learner that captures the cross-task transferable knowledge. For learning across distant tasks, we further inject the task type descriptions into the prompt, and capture the intra-type and inter-type prompt embeddings among multiple distant tasks. Additionally, two de-biasing techniques are further designed to make the trained meta-learner more task-agnostic and unbiased towards any tasks. After that, the meta-learner can be adapted to each specific task with better parameters initialization. Extensive experiments show that TransPrompt v2 outperforms single-task and cross-task strong baselines over multiple NLP tasks and datasets. We further show that the meta-learner can effectively improve the performance of PLMs on previously unseen tasks. In addition, TransPrompt v2 also outperforms strong fine-tuning baselines when learning with full training sets.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Extensive experiments show that TransPrompt v2 outperforms single-task and cross-task strong baselines over multiple NLP tasks and datasets, and shows that the meta-learner can effectively improve the performance of PLMs on previously unseen tasks."
            },
            "score": 6
        },
        {
            "id": "f8b8f926bbfa327c86c40796131fe2695db81126",
            "paperId": "f8b8f926bbfa327c86c40796131fe2695db81126",
            "title": "DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models",
            "abstract": "A long-standing goal of AI systems is to perform complex multimodal reasoning like humans. Recently, large language models (LLMs) have made remarkable strides in such multi-step reasoning on the language modality solely by leveraging the chain of thought (CoT) to mimic human thinking. However, the transfer of these advancements to multimodal contexts introduces heightened challenges, including but not limited to the impractical need for labor-intensive annotation and the limitations in terms of flexibility, generalizability, and explainability. To evoke CoT reasoning in multimodality, this work first conducts an in-depth analysis of these challenges posed by multimodality and presents two key insights:\"keeping critical thinking\"and\"letting everyone do their jobs\"in multimodal CoT reasoning. Furthermore, this study proposes a novel DDCoT prompting that maintains a critical attitude through negative-space prompting and incorporates multimodality into reasoning by first dividing the reasoning responsibility of LLMs into reasoning and recognition and then integrating the visual recognition capability of visual models into the joint reasoning process. The rationales generated by DDCoT not only improve the reasoning abilities of both large and small language models in zero-shot prompting and fine-tuning learning, significantly outperforming state-of-the-art methods but also exhibit impressive generalizability and explainability.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study proposes a novel DDCoT prompting that maintains a critical attitude through negative-space prompting and incorporates multimodality into reasoning by first dividing the reasoning responsibility of LLMs into reasoning and recognition and then integrating the visual recognition capability of visual models into the joint reasoning process."
            },
            "score": 5
        },
        {
            "id": "7a79a3074e736bdd9e4ce1d46f1efe3abd6623fd",
            "paperId": "7a79a3074e736bdd9e4ce1d46f1efe3abd6623fd",
            "title": "A Recipe for Arbitrary Text Style Transfer with Large Language Models",
            "abstract": "In this paper, we leverage large language models (LLMs) to perform zero-shot text style transfer. We present a prompting method that we call augmented zero-shot learning, which frames style transfer as a sentence rewriting task and requires only a natural language instruction, without model fine-tuning or exemplars in the target style. Augmented zero-shot learning is simple and demonstrates promising results not just on standard style transfer tasks such as sentiment, but also on arbitrary transformations such as \u2018make this melodramatic\u2019 or \u2018insert a metaphor.\u2019",
            "year": 2021,
            "citationCount": 90,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Augmented zero-shot learning is simple and demonstrates promising results not just on standard style transfer tasks such as sentiment, but also on arbitrary transformations such as \u2018make this melodramatic\u2019 or \u2018insert a metaphor\u2019."
            },
            "score": 5
        },
        {
            "id": "bcb40a7f2009259e660e14d12ec75bae1c184b1e",
            "paperId": "bcb40a7f2009259e660e14d12ec75bae1c184b1e",
            "title": "Identifying the Limits of Cross-Domain Knowledge Transfer for Pretrained Models",
            "abstract": "There is growing evidence that pretrained language models improve task-specific fine-tuning even where the task examples are radically different from those seen in training. We study an extreme case of transfer learning by providing a systematic exploration of how much transfer occurs when models are denied any information about word identity via random scrambling. In four classification tasks and two sequence labeling tasks, we evaluate LSTMs using GloVe embeddings, BERT, and baseline models. Among these models, we find that only BERT shows high rates of transfer into our scrambled domains, and for classification but not sequence labeling tasks. Our analyses seek to explain why transfer succeeds for some tasks but not others, to isolate the separate contributions of pretraining versus fine-tuning, to show that the fine-tuning process is not merely learning to unscramble the scrambled inputs, and to quantify the role of word frequency. Furthermore, our results suggest that current benchmarks may overestimate the degree to which current models actually understand language.",
            "year": 2021,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work provides a systematic exploration of how much transfer occurs when models are denied any information about word identity via random scrambling, and suggests that current benchmarks may overestimate the degree to which current models actually understand language."
            },
            "score": 5
        },
        {
            "id": "9a3b0ffe5e0199022019ecaec231c71774a956cc",
            "paperId": "9a3b0ffe5e0199022019ecaec231c71774a956cc",
            "title": "Cross-Domain Generalization and Knowledge Transfer in Transformers Trained on Legal Data",
            "abstract": "We analyze the ability of pre-trained language models to transfer knowledge among datasets annotated with different type systems and to generalize beyond the domain and dataset they were trained on. We create a meta task, over multiple datasets focused on the prediction of rhetorical roles. Prediction of the rhetorical role a sentence plays in a case decision is an important and often studied task in AI&Law. Typically, it requires the annotation of a large number of sentences to train a model, which can be time-consuming and expensive. Further, the application of the models is restrained to the same dataset it was trained on. We fine-tune language models and evaluate their performance across datasets, to investigate the models' ability to generalize across domains. Our results suggest that the approach could be helpful in overcoming the cold-start problem in active or interactvie learning, and shows the ability of the models to generalize across datasets and domains.",
            "year": 2021,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 5
        },
        {
            "id": "e3bea580a88fd9f9db403863dd3ae6aad54a5220",
            "paperId": "e3bea580a88fd9f9db403863dd3ae6aad54a5220",
            "title": "Cross-lingual and Cross-domain Transfer Learning for Automatic Term Extraction from Low Resource Data",
            "abstract": "Automatic Term Extraction (ATE) is a key component for domain knowledge understanding and an important basis for further natural language processing applications. Even with persistent improvements, ATE still exhibits weak results exacerbated by small training data inherent to specialized domain corpora. Recently, transformers-based deep neural models, such as BERT, have proven to be efficient in many downstream NLP tasks. However, no systematic evaluation of ATE has been conducted so far. In this paper, we run an extensive study on fine-tuning pre-trained BERT models for ATE. We propose strategies that empirically show BERT\u2019s effectiveness using cross-lingual and cross-domain transfer learning to extract single and multi-word terms. Experiments have been conducted on four specialized domains in three languages. The obtained results suggest that BERT can capture cross-domain and cross-lingual terminologically-marked contexts shared by terms, opening a new design-pattern for ATE.",
            "year": 2022,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An extensive study on fine-tuning pre-trained BERT models for ATE and proposes strategies that empirically show BERT\u2019s effectiveness using cross-lingual and cross-domain transfer learning to extract single and multi-word terms."
            },
            "score": 5
        },
        {
            "id": "a8608c44646bf7b1fbce9663b643207987fdec72",
            "paperId": "a8608c44646bf7b1fbce9663b643207987fdec72",
            "title": "Language-Aware Domain Generalization Network for Cross-Scene Hyperspectral Image Classification",
            "abstract": "Text information including extensive prior knowledge about land cover classes has been ignored in hyperspectral image (HSI) classification tasks. It is necessary to explore the effectiveness of linguistic mode in assisting HSI classification. In addition, the large-scale pretraining image\u2013text foundation models have demonstrated great performance in a variety of downstream applications, including zero-shot transfer. However, most domain generalization methods have never addressed mining linguistic modal knowledge to improve the generalization performance of model. To compensate for the inadequacies listed above, a language-aware domain generalization network (LDGnet) is proposed to learn cross-domain-invariant representation from cross-domain shared prior knowledge. The proposed method only trains on the source domain (SD) and then transfers the model to the target domain (TD). The dual-stream architecture including the image encoder and text encoder is used to extract visual and linguistic features, in which coarse-grained and fine-grained text representations are designed to extract two levels of linguistic features. Furthermore, linguistic features are used as cross-domain shared semantic space, and visual\u2013linguistic alignment is completed by supervised contrastive learning in semantic space. Extensive experiments on three datasets demonstrate the superiority of the proposed method when compared with the state-of-the-art techniques. The codes will be available from the website: https://github.com/YuxiangZhang-BIT/IEEE_TGRS_LDGnet.",
            "year": 2022,
            "citationCount": 54,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A language-aware domain generalization network (LDGnet) is proposed to learn cross-domain-invariant representation from cross- domain shared prior knowledge to improve the generalization performance of model."
            },
            "score": 5
        },
        {
            "id": "12358df20ccf4085e6c8a45d3ab5fa15714abcd6",
            "paperId": "12358df20ccf4085e6c8a45d3ab5fa15714abcd6",
            "title": "Investigating Continual Pretraining in Large Language Models: Insights and Implications",
            "abstract": "This paper studies the evolving domain of Continual Learning (CL) in large language models (LLMs), with a focus on developing strategies for efficient and sustainable training. Our primary emphasis is on continual domain-adaptive pretraining, a process designed to equip LLMs with the ability to integrate new information from various domains while retaining previously learned knowledge and enhancing cross-domain knowledge transfer without relying on domain-specific identification. Unlike previous studies, which mostly concentrate on a limited selection of tasks or domains and primarily aim to address the issue of forgetting, our research evaluates the adaptability and capabilities of LLMs to changing data landscapes in practical scenarios. To this end, we introduce a new benchmark designed to measure the adaptability of LLMs to these evolving data environments, offering a comprehensive framework for evaluation. We examine the impact of model size on learning efficacy and forgetting, as well as how the progression and similarity of emerging domains affect the knowledge transfer within these models. Our findings uncover several key insights: (i) when the sequence of domains shows semantic similarity, continual pretraining enables LLMs to better specialize in the current domain compared to stand-alone fine-tuning, (ii) training across a diverse range of domains enhances both backward and forward knowledge transfer, and (iii) smaller models are particularly sensitive to continual pretraining, showing the most significant rates of both forgetting and learning. We posit that our research marks a shift towards establishing a more realistic benchmark for investigating CL in LLMs, and has the potential to play a key role in guiding the direction of future research in the field.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This research introduces a new benchmark designed to measure the adaptability of LLMs to changing data landscapes in practical scenarios, and finds that smaller models are particularly sensitive to continual pretraining, showing the most significant rates of both forgetting and learning."
            },
            "score": 5
        },
        {
            "id": "bad287184c6739fd6f476f89cb83e09415982d9f",
            "paperId": "bad287184c6739fd6f476f89cb83e09415982d9f",
            "title": "ANALOGYKB: Unlocking Analogical Reasoning of Language Models with A Million-scale Knowledge Base",
            "abstract": "Analogical reasoning is a fundamental cognitive ability of humans. However, current language models (LMs) still struggle to achieve human-like performance in analogical reasoning tasks due to a lack of resources for model training. In this work, we address this gap by proposing ANALOGYKB, a million-scale analogy knowledge base (KB) derived from existing knowledge graphs (KGs). ANALOGYKB identifies two types of analogies from the KGs: 1) analogies of the same relations, which can be directly extracted from the KGs, and 2) analogies of analogous relations, which are identified with a selection and filtering pipeline enabled by large LMs (InstructGPT), followed by minor human efforts for data quality control. Evaluations on a series of datasets of two analogical reasoning tasks (analogy recognition and generation) demonstrate that ANALOGYKB successfully enables LMs to achieve much better results than previous state-of-the-art methods.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Evaluations on a series of datasets of two analogical reasoning tasks (analogy recognition and generation) demonstrate that ANALOGYKB successfully enables LMs to achieve much better results than previous state-of-the-art methods."
            },
            "score": 5
        },
        {
            "id": "b69ae70abf1e8519a565c40bed07ab499576a8f6",
            "paperId": "b69ae70abf1e8519a565c40bed07ab499576a8f6",
            "title": "Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models",
            "abstract": "Large language models (LLMs) have performed well on several reasoning benchmarks, including ones that test analogical reasoning abilities. However, it has been debated whether they are actually performing humanlike abstract reasoning or instead employing less general processes that rely on similarity to what has been seen in their training data. Here we investigate the generality of analogy-making abilities previously claimed for LLMs (Webb, Holyoak,&Lu, 2023). We take one set of analogy problems used to evaluate LLMs and create a set of\"counterfactual\"variants-versions that test the same abstract reasoning abilities but that are likely dissimilar from any pre-training data. We test humans and three GPT models on both the original and counterfactual problems, and show that, while the performance of humans remains high for all the problems, the GPT models' performance declines sharply on the counterfactual set. This work provides evidence that, despite previously reported successes of LLMs on analogical reasoning, these models lack the robustness and generality of human analogy-making.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work takes one set of analogy problems used to evaluate LLMs and creates a set of \"counterfactual\"variants-versions that test the same abstract reasoning abilities but that are likely dissimilar from any pre-training data, providing evidence that these models lack the robustness and generality of human analogy-making."
            },
            "score": 5
        },
        {
            "id": "370cea8b4220917f45a69358c0303df71f5063c7",
            "paperId": "370cea8b4220917f45a69358c0303df71f5063c7",
            "title": "ThinkSum: Probabilistic reasoning over sets using large language models",
            "abstract": "Large language models (LLMs) have a substantial capacity for high-level analogical reasoning: reproducing patterns in linear text that occur in their training data (zero-shot evaluation) or in the provided context (few-shot in-context learning). However, recent studies show that even the more advanced LLMs fail in scenarios that require reasoning over multiple objects or facts and making sequences of logical deductions. We propose a two-stage probabilistic inference paradigm, ThinkSum, which reasons over sets of objects or facts in a structured manner. In the first stage (Think \u2013 retrieval of associations), a LLM is queried in parallel over a set of phrases extracted from the prompt or an auxiliary model call. In the second stage (Sum \u2013 probabilistic inference or reasoning), the results of these queries are aggregated to make the final prediction. We demonstrate the possibilities and advantages of ThinkSum on the BIG-bench suite of LLM evaluation tasks, achieving improvements over the state of the art using GPT-family models on thirteen difficult tasks, often with far smaller model variants. We also compare and contrast ThinkSum with other proposed modifications to direct prompting of LLMs, such as variants of chain-of-thought prompting. Our results suggest that because the probabilistic inference in ThinkSum is performed outside of calls to the LLM, ThinkSum is less sensitive to prompt design, yields more interpretable predictions, and can be flexibly combined with latent variable models to extract structured knowledge from LLMs. Overall, our proposed paradigm represents a promising approach for enhancing the reasoning capabilities of LLMs.",
            "year": 2022,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Because the probabilistic inference in ThinkSum is performed outside of calls to the LLM, thinkSum is less sensitive to prompt design, yields more interpretable predictions, and can be flexibly combined with latent variable models to extract structured knowledge from LLMs."
            },
            "score": 5
        },
        {
            "id": "c10ce97f538adecfc0bc15e6ca39dd5d5f002bc1",
            "paperId": "c10ce97f538adecfc0bc15e6ca39dd5d5f002bc1",
            "title": "Beneath Surface Similarity: Large Language Models Make Reasonable Scientific Analogies after Structure Abduction",
            "abstract": "The vital role of analogical reasoning in human cognition allows us to grasp novel concepts by linking them with familiar ones through shared relational structures. Despite the attention previous research has given to word analogies, this work suggests that Large Language Models (LLMs) often overlook the structures that underpin these analogies, raising questions about the efficacy of word analogies as a measure of analogical reasoning skills akin to human cognition. In response to this, our paper introduces a task of analogical structure abduction, grounded in cognitive psychology, designed to abduce structures that form an analogy between two systems. In support of this task, we establish a benchmark called SCAR, containing 400 scientific analogies from 13 distinct fields, tailored for evaluating analogical reasoning with structure abduction. The empirical evidence underlines the continued challenges faced by LLMs, including ChatGPT and GPT-4, in mastering this task, signifying the need for future exploration to enhance their abilities.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a task of analogical structure abduction, grounded in cognitive psychology, designed to abduce structures that form an analogy between two systems, and establishes a benchmark called SCAR, containing 400 scientific analogies, tailored for evaluating analogical reasoning with structure abduction."
            },
            "score": 5
        },
        {
            "id": "b558751a7e0a14a34d2ff6cb406a0121c17892f1",
            "paperId": "b558751a7e0a14a34d2ff6cb406a0121c17892f1",
            "title": "Prompting learning and transfer of analogical reasoning: Is working memory a piece of the puzzle?",
            "abstract": "Dynamic testing is an assessment approach that aims to assess potential for learning by measuring performance improvement as a response to training while testing. In this study we use this approach in order to: (1) determine whether training children in analogical reasoning affects transfer of inductive reasoning skills to other tasks and (2) explore the relationship between working memory, training and transfer effects. This was investigated using a pretest-training-posttest control group design with 64 participants, aged 7-8 years (M = 7.6 years; SD = 4.7 months). All of the children were tested on four inductive reasoning tasks. Half of the children were trained in solving figural analogies according to the graduated prompts method, while the control group practiced with these items. Initial ability and performance change from pretest to posttest were estimated using Embretson\u2019s (1991b) item response theory Multidimensional Rasch Model of Learning and Change. We found that the short training procedure improved figural analogical reasoning more than practice. Working memory was strongly related to initial performance on each of the inductive reasoning tasks. Yet, we found that performance change and knowledge transfer were only somewhat related to initial ability and unrelated to working memory. This indicates that performance change and ability to transfer trained skills to new tasks may be separate constructs and of possible importance in the assessment of learning and cognitive potential.",
            "year": 2012,
            "citationCount": 0,
            "tldr": null,
            "score": 5
        },
        {
            "id": "8b32aa33601514976d88fabcb060a5cd38d34006",
            "paperId": "8b32aa33601514976d88fabcb060a5cd38d34006",
            "title": "Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning",
            "abstract": "Prompt tuning, in which a base pretrained model is adapted to each task via conditioning on learned prompt vectors, has emerged as a promising approach for efficiently adapting large language models to multiple downstream tasks. However, existing methods typically learn soft prompt vectors from scratch, and it has not been clear how to exploit the rich cross-task knowledge with prompt vectors in a multitask learning setting. We propose multitask prompt tuning (MPT), which first learns a single transferable prompt by distilling knowledge from multiple task-specific source prompts. We then learn multiplicative low rank updates to this shared prompt to efficiently adapt it to each downstream target task. Extensive experiments on 23 NLP datasets demonstrate that our proposed approach outperforms the state-of-the-art methods, including the full finetuning baseline in some cases, despite only tuning 0.035% as many task-specific parameters.",
            "year": 2023,
            "citationCount": 54,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes multitask prompt tuning (MPT), which first learns a single transferable prompt by distilling knowledge from multiple task-specific source prompts, then learns multiplicative low rank updates to this shared prompt to efficiently adapt it to each downstream target task."
            },
            "score": 5
        },
        {
            "id": "cea096abda294395c77d41cda3ec38547bd7a0ff",
            "paperId": "cea096abda294395c77d41cda3ec38547bd7a0ff",
            "title": "Change in Spontaneous Analogical Transfer in Young Children: A Microgenetic Study.",
            "abstract": "This study assessed the development of spontaneous analogical transfer from story problems to physical tasks by examining the effects of practice alone, without intervention or explicit prompting. Participants were 216 children aged 5\u20138 years. The microgenetic technique was incorporated with each age group by following them for six consecutive sessions of three trails each. The role of practice and age was investigated, not only on children's task behaviour, but also on their explanations of their actions. The results indicated that the ability to use analogy spontaneously improves over time with practice regardless of age, and that it develops over a protracted age range, with some children showing competence early, while others show poor performance at the age of 8 years. Furthermore, the study found development of analogical transfer to involve within-participant variability, as children showed competence on some trials but not on others. Although not a focus of the study, the results revealed intra-individual utilization deficiency in the domain of analogical transfer. It is concluded that there is strong evidence for gradual rather than step-like change in the development of analogical transfer. Copyright \u00a9 2007 John Wiley & Sons, Ltd.",
            "year": 2007,
            "citationCount": 21,
            "tldr": null,
            "score": 5
        },
        {
            "id": "9db2c2637751ed6862d28c584381195b89328b1a",
            "paperId": "9db2c2637751ed6862d28c584381195b89328b1a",
            "title": "Transfer effects of prompted and self-reported analogical comparison and self-explanation",
            "abstract": "We compared types of transfer facilitated by instructions to engage in analogical comparison or self-explanation. Participants received learning materials and worked examples with prompts supporting analogical comparison, selfexplanation, or instructional explanation study. Learners also self-reported their use of analogical comparison and selfexplanation on a series of questionnaires. We evaluated condition effects on self-reports and transfer, and the relations between self-reports and transfer. Receiving materials with analogical-comparison support and reporting greater levels of analogical comparison were both associated with worse transfer performance, while reporting greater levels of selfexplanation was associated with better performance. Learners\u2019 self-reports of analogical comparison and selfexplanation were not related to condition assignment, suggesting that the questionnaires did not measure the same processes promoted by the intervention, or that individual differences are robust even when learners are instructed to engage in analogical comparison or self-explanation.",
            "year": 2015,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Learners\u2019 self-reports of analogical comparison and selfexplanation were not related to condition assignment, suggesting that the questionnaires did not measure the same processes promoted by the intervention, or that individual differences are robust even when learners are instructed to engage in analogical comparisons or self-explanations."
            },
            "score": 5
        },
        {
            "id": "64ce6ef1f5cf227bf2bf917c87273386ae16256f",
            "paperId": "64ce6ef1f5cf227bf2bf917c87273386ae16256f",
            "title": "Dictionary-based Phrase-level Prompting of Large Language Models for Machine Translation",
            "abstract": "Large language models (LLMs) demonstrate remarkable machine translation (MT) abilities via prompting, even though they were not explicitly trained for this task. However, even given the incredible quantities of data they are trained on, LLMs can struggle to translate inputs with rare words, which are common in low resource or domain transfer scenarios. We show that LLM prompting can provide an effective solution for rare words as well, by using prior knowledge from bilingual dictionaries to provide control hints in the prompts. We propose a novel method, DiPMT, that provides a set of possible translations for a subset of the input words, thereby enabling fine-grained phrase-level prompted control of the LLM. Extensive experiments show that DiPMT outperforms the baseline both in low-resource MT, as well as for out-of-domain MT. We further provide a qualitative analysis of the benefits and limitations of this approach, including the overall level of controllability that is achieved.",
            "year": 2023,
            "citationCount": 30,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that LLM prompting can provide an effective solution for rare words as well, by using prior knowledge from bilingual dictionaries to provide control hints in the prompts, thereby enabling fine-grained phrase-level prompted control of the LLM."
            },
            "score": 4
        },
        {
            "id": "4bf921e7d4b11e8a546129188c8c5358d3d30caf",
            "paperId": "4bf921e7d4b11e8a546129188c8c5358d3d30caf",
            "title": "Few-Shot Cross-Lingual Transfer for Prompting Large Language Models in Low-Resource Languages",
            "abstract": "Large pre-trained language models (PLMs) are at the forefront of advances in Natural Language Processing. One widespread use case of PLMs is\"prompting\"- or in-context learning - where a user provides a description of a task and some completed examples of the task to a PLM as context before prompting the PLM to perform the task on a new example. Only the largest, most capable PLMs are able to perform in-context learning effectively, and these models are typically trained with a predominantly English corpus, leaving all other languages behind. The data limitations in most languages preclude the training of language-specific PLMs capable of prompting. Albeit the surge in work of prompting settings, it is still unclear how PLMs should be adapted cross-lingually specifically for prompting. We evaluate the possible methods to adapt LLaMa, a 7B parameter open-source PLM mainly trained in English, for prompting in low-resource languages, namely for Kinyarwanda, Hausa, and Luganda. We consider three methods: few-shot prompting (prompt), language-adaptive fine-tuning (LAFT), and neural machine translation (translate), and evaluate on abstractive summarization, multi-class topic classification, and named-entity recognition. Although LAFT carries the greatest compute cost and intuitively should lead to the best results, our experiments exhibit that LAFT is only occasionally the optimal choice for adapting PLMs for prompting. Rather, the translate and prompt settings are a compute-efficient and cost-effective method of few-shot prompting for the selected low-resource languages. We find that the results are task and language dependent but find that the prompting method is the best on average across all tasks and languages. Results show that the prompt setting performs better than both translating and LAFT with statistical significance for all shots when aggregated across all tasks and languages.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The possible methods to adapt LLaMa, a 7B parameter open-source PLM mainly trained in English, for prompting in low-resource languages, namely for Kinyarwanda, Hausa, and Luganda are considered, and three methods are considered: few-shot prompting, language-adaptive fine-tuning (LAFT), and neural machine translation (translate)."
            },
            "score": 4
        },
        {
            "id": "0d6bb585493e34975f0437faa3179db3a02f6ae8",
            "paperId": "0d6bb585493e34975f0437faa3179db3a02f6ae8",
            "title": "Prompt-and-Rerank: A Method for Zero-Shot and Few-Shot Arbitrary Textual Style Transfer with Small Language Models",
            "abstract": "We propose a method for arbitrary textual style transfer (TST)\u2014the task of transforming a text into any given style\u2014utilizing general-purpose pre-trained language models. Our method, Prompt-and-Rerank, is based on a mathematical formulation of the TST task, decomposing it into three constituent components: textual similarity, target style strength, and fluency. Our method uses zero-shot or few-shot prompting to obtain a set of candidate generations in the target style, and then re-ranks them according to the three components. Our method enables small pre-trained language models to perform on par with state-of-the-art large-scale models while using two orders of magnitude less compute and memory. We also investigate the effect of model size and prompt design (e.g., prompt paraphrasing and delimiter-pair choice) on style transfer quality across seven diverse textual style transfer datasets, finding, among other things, that delimiter-pair choice has a large impact on performance, and that models have biases on the direction of style transfer.",
            "year": 2022,
            "citationCount": 36,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a method for arbitrary textual style transfer (TST), based on a mathematical formulation of the TST task, that enables small pre-trained language models to perform on par with state-of-the-art large-scale models while using two orders of magnitude less compute and memory."
            },
            "score": 4
        },
        {
            "id": "43ec80eeb6f22431ae741796996b25ca3b6bf3e2",
            "paperId": "43ec80eeb6f22431ae741796996b25ca3b6bf3e2",
            "title": "Adapting Pre-trained Language Models to Vision-Language Tasks via Dynamic Visual Prompting",
            "abstract": "Pre-trained language models (PLMs) have played an increasing role in multimedia research. In terms of vision-language (VL) tasks, they often serve as a language encoder and still require an additional fusion network for VL reasoning, resulting in excessive memory overhead. In this paper, we focus on exploring PLMs as a stand-alone model for VL reasoning tasks. Inspired by the recently popular prompt tuning, we first prove that the processed visual features can be also projected onto the semantic space of PLMs and act as prompt tokens to bridge the gap between single- and multi-modal learning. However, this solution exhibits obvious redundancy in visual information and model inference, and the placement of prompt tokens also greatly affects the final performance. Based on these observations, we further propose a novel transfer learning approach for PLMs, termed Dynamic Visual Prompting (DVP). Concretely, DVP first deploys a cross-attention module to obtain text-related and compact visual prompt tokens, thereby greatly reducing the input length of PLMs. To obtain the optimal placement, we also equip DVP with a reinforcement-learning based search algorithm, which can automatically merge DVP with PLMs for different VL tasks via a very short search process. In addition, we also experiment DVP with the recently popular adapter approach to keep the most parameters of PLMs intact when adapting to VL tasks, helping PLMs achieve a quick shift between single- and multi-modal tasks. We apply DVP to two representative PLMs, namely BERT and T5, and conduct extensive experiments on a set of VL reasoning benchmarks including VQA2.0, GQA and SNLIVE. The experimental results not only show the advantage of DVP on efficiency and performance, but also confirm its superiority in adapting pre-trained language models to VL tasks.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results not only show the advantage of DVP on efficiency and performance, but also confirm its superiority in adapting pre-trained language models to VL tasks."
            },
            "score": 4
        },
        {
            "id": "e4be613cc875e61b8c1c6c60d958f1c20d12d6c0",
            "paperId": "e4be613cc875e61b8c1c6c60d958f1c20d12d6c0",
            "title": "Task and Motion Planning with Large Language Models for Object Rearrangement",
            "abstract": "Multi-object rearrangement is a crucial skill for service robots, and commonsense reasoning is frequently needed in this process. However, achieving commonsense arrangements requires knowledge about objects, which is hard to transfer to robots. Large language models (LLMs) are one potential source of this knowledge, but they do not naively capture information about plausible physical arrangements of the world. We propose LLM-GROP, which uses prompting to extract commonsense knowledge about semantically valid object configurations from an LLM and instantiates them with a task and motion planner in order to generalize to varying scene geometry. LLM-GROP allows us to go from natural-language commands to human-aligned object rearrangement in varied environments. Based on human evaluations, our approach achieves the highest rating while outperforming competitive baselines in terms of success rate while maintaining comparable cumulative action costs. Finally, we demonstrate a practical implementation of LLM-GROP on a mobile manipulator in real-world scenarios. Supplementary materials are available at: https://sites.google.com/view/llm-grop",
            "year": 2023,
            "citationCount": 80,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LLM-GROP is proposed, which uses prompting to extract commonsense knowledge about semantically valid object configurations from an LLM and instantiates them with a task and motion planner in order to generalize to varying scene geometry."
            },
            "score": 4
        },
        {
            "id": "2c676ecdc954ec24e1907c76accb1e8ac06deec0",
            "paperId": "2c676ecdc954ec24e1907c76accb1e8ac06deec0",
            "title": "Data-Efficiency with a Single GPU: An Exploration of Transfer Methods for Small Language Models",
            "abstract": "Multi-task learning (MTL), instruction tuning, and prompting have recently been shown to improve the generalizability of large language models to new tasks. However, the benefits of such methods are less well-documented in smaller language models, with some studies finding contradictory results. In this work, we explore and isolate the effects of (i) model size, (ii) general purpose MTL, (iii) in-domain MTL, (iv) instruction tuning, and (v) few-shot fine-tuning for models with fewer than 500 million parameters. Our experiments in the zero-shot setting demonstrate that models gain 31% relative improvement, on average, from general purpose MTL, with an additional 37.6% relative gain from in-domain MTL. Contradictory to prior works on large models, we find that instruction tuning provides a modest 2% performance improvement for small models.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that instruction tuning provides a modest 2% performance improvement for small models,Contradictory to prior works on large models, the effects of model size, instruction tuning, and in-domain MTL are explored."
            },
            "score": 4
        },
        {
            "id": "e596b8adbffa546dbc163e817fb3de72744ec4f6",
            "paperId": "e596b8adbffa546dbc163e817fb3de72744ec4f6",
            "title": "HTLM: Hyper-Text Pre-Training and Prompting of Language Models",
            "abstract": "We introduce HTLM, a hyper-text language model trained on a large-scale web crawl. Modeling hyper-text has a number of advantages: (1) it is easily gathered at scale, (2) it provides rich document-level and end-task-adjacent supervision (e.g. class and id attributes often encode document category information), and (3) it allows for new structured prompting that follows the established semantics of HTML (e.g. to do zero-shot summarization by infilling title tags for a webpage that contains the input text). We show that pretraining with a BART-style denoising loss directly on simplified HTML provides highly effective transfer for a wide range of end tasks and supervision levels. HTLM matches or exceeds the performance of comparably sized text-only LMs for zero-shot prompting and fine-tuning for classification benchmarks, while also setting new state-of-the-art performance levels for zero-shot summarization. We also find that hyper-text prompts provide more value to HTLM, in terms of data efficiency, than plain text prompts do for existing LMs, and that HTLM is highly effective at auto-prompting itself, by simply generating the most likely hyper-text formatting for any available training data. We will release all code and models to support future HTLM research.",
            "year": 2021,
            "citationCount": 62,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found thathyper-text prompts provide more value to HTLM, in terms of data efficiency, than plain text prompts do for existing LMs, and that HTLM is highly effective at auto-prompting itself, by simply generating the most likely hyper-text formatting for any available training data."
            },
            "score": 4
        },
        {
            "id": "126a4776ff8315fd506766cb8f3c722cf746ad9e",
            "paperId": "126a4776ff8315fd506766cb8f3c722cf746ad9e",
            "title": "Teaching Small Language Models to Reason",
            "abstract": "Chain of thought prompting successfully improves the reasoning capabilities of large language models, achieving state of the art results on a range of datasets. However, these reasoning capabilities only appear to emerge in models with at least tens of billions of parameters. In this paper, we explore the transfer of such reasoning capabilities to smaller models via knowledge distillation, also investigating model and dataset size trade-off. Specifically, we finetune a student model on the chain of thought outputs generated by a larger teacher model. Our experiments show that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets. For example, the accuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% and 18.42% when finetuned on PaLM 540B and GPT-3 175B generated chains of thought, respectively.",
            "year": 2022,
            "citationCount": 121,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper finetune a student model on the chain of thought outputs generated by a larger teacher model, and shows that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets."
            },
            "score": 4
        },
        {
            "id": "79a8a493fe073a087a1010dcca857bc6d5ca0eee",
            "paperId": "79a8a493fe073a087a1010dcca857bc6d5ca0eee",
            "title": "GTLNLP: A Mathematical Exploration of Cross-Domain Knowledge Transfer for Text Generation for Generative Transfer Learning in Natural Language Processing",
            "abstract": "In the field of Cross-Domain Knowledge Transfer generative transfer learning in Natural Language Processing (NLP) used for creating text using generative math. The paper aims to improve the usefulness of text creation models in a variety of areas by using cutting edge deep learning and neural network methods. We come up with a new system that makes it easier for information to move from one domain to another, even when the language and situation are different in each domain. Using domain adaptation techniques to match feature distributions and reduce domain gaps is what our method is based on mathematically. It is a complex version of transfer learning principles. We test our model's abilities on a wide range of tasks by doing a lot of careful experiments. We focus on how well it can share information and write text that makes sense and is relevant to the situation across different areas. This study not only adds to our theoretical understanding of cross-domain knowledge transfer, but it also gives us useful tips on how to make NLP models more flexible and useful in the real world. The results of our study could help improve the state of the art in generative transfer learning and make text generation systems that work better and more reliably in a variety of language settings.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study comes up with a new system that makes it easier for information to move from one domain to another, even when the language and situation are different in each domain."
            },
            "score": 4
        },
        {
            "id": "529880cc9d8ac13f63f4263932db704f1b9553e7",
            "paperId": "529880cc9d8ac13f63f4263932db704f1b9553e7",
            "title": "WAVER: Writing-style Agnostic Text-Video Retrieval via Distilling Vision-Language Models Through Open-Vocabulary Knowledge",
            "abstract": "Text-video retrieval, a prominent sub-field within the domain of multimodal information retrieval, has witnessed remarkable growth in recent years. However, existing methods assume video scenes are consistent with unbiased descriptions. These limitations fail to align with real-world scenarios since descriptions can be influenced by annotator biases, diverse writing styles, and varying textual perspectives. To overcome the aforementioned problems, we introduce $\\texttt{WAVER}$, a cross-domain knowledge distillation framework via vision-language models through open-vocabulary knowledge designed to tackle the challenge of handling different writing styles in video descriptions. $\\texttt{WAVER}$ capitalizes on the open-vocabulary properties that lie in pre-trained vision-language models and employs an implicit knowledge distillation approach to transfer text-based knowledge from a teacher model to a vision-based student. Empirical studies conducted across four standard benchmark datasets, encompassing various settings, provide compelling evidence that $\\texttt{WAVER}$ can achieve state-of-the-art performance in text-video retrieval task while handling writing-style variations. The code is available at: https://github.com/Fsoft-AIC/WAVER",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A cross-domain knowledge distillation framework via vision-language models through open-vocabulary knowledge designed to tackle the challenge of handling different writing styles in video descriptions, and employs an implicit knowledge distillation approach to transfer text-based knowledge from a teacher model to a vision-based student."
            },
            "score": 4
        },
        {
            "id": "2931d8103aaff3a2514eb29008b0da3e399e4bac",
            "paperId": "2931d8103aaff3a2514eb29008b0da3e399e4bac",
            "title": "Multi-Modal Cross-Domain Alignment Network for Video Moment Retrieval",
            "abstract": "As an increasingly popular task in multimedia information retrieval, video moment retrieval (VMR) aims to localize the target moment from an untrimmed video according to a given language query. Most previous methods depend heavily on numerous manual annotations (i.e., moment boundaries), which are extremely expensive to acquire in practice. In addition, due to the domain gap between different datasets, directly applying these pre-trained models to an unseen domain leads to a significant performance drop. In this paper, we focus on a novel task: cross-domain VMR, where fully-annotated datasets are available in one domain (\u201csource domain\u201d), but the domain of interest (\u201ctarget domain\u201d) only contains unannotated datasets. As far as we know, we present the first study on cross-domain VMR. To address this new task, we propose a novel Multi-Modal Cross-Domain Alignment (MMCDA) network to transfer the annotation knowledge from the source domain to the target domain. However, due to the domain discrepancy between the source and target domains and the semantic gap between videos and queries, directly applying trained models to the target domain generally leads to a performance drop. To solve this problem, we develop three novel modules: (i) a domain alignment module is designed to align the feature distributions between different domains of each modality; (ii) a cross-modal alignment module aims to map both video and query features into a joint embedding space and to align the feature distributions between different modalities in the target domain; and (iii) a specific alignment module tries to obtain the fine-grained similarity between a specific frame and the given query for optimal localization. By jointly training these three modules, our MMCDA can learn domain-invariant and semantic-aligned cross-modal representations. Extensive experiments on three challenging benchmarks (ActivityNet Captions, Charades-STA and TACoS) illustrate that our cross-domain method MMCDA outperforms all state-of-the-art single-domain methods. Impressively, MMCDA raises the performance by more than 7% in representative cases, which demonstrates its effectiveness.",
            "year": 2022,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel Multi-Modal Cross-Domain Alignment (MMCDA) network to transfer the annotation knowledge from the source domain to the target domain and raises the performance by more than 7% in representative cases, which demonstrates its effectiveness."
            },
            "score": 4
        },
        {
            "id": "e9acf2a53c406849c717ef4bae6492cd1076d5db",
            "paperId": "e9acf2a53c406849c717ef4bae6492cd1076d5db",
            "title": "UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural Language",
            "abstract": "Foundation models like ChatGPT and GPT-4 have revolutionized artificial intelligence, exhibiting remarkable abilities to generalize across a wide array of tasks and applications beyond their initial training objectives. However, when this concept is applied to graph learning, a stark contrast emerges. Graph learning has predominantly focused on single-graph models, tailored to specific tasks or datasets, lacking the ability to transfer learned knowledge to different domains. This limitation stems from the inherent complexity and diversity of graph structures, along with the different feature and label spaces specific to graph data. In this paper, we present our UniGraph framework, designed to train a graph foundation model capable of generalizing to unseen graphs and tasks across diverse domains. Unlike single-graph models that use pre-computed node features of varying dimensions as input, our approach leverages Text-Attributed Graphs (TAGs) for unifying node representations. We propose a cascaded architecture of Language Models (LMs) and Graph Neural Networks (GNNs) as backbone networks with a self-supervised training objective based on Masked Graph Modeling (MGM). We introduce graph instruction tuning using Large Language Models (LLMs) to enable zero-shot prediction ability. Our comprehensive experiments across various graph learning tasks and domains demonstrate the model's effectiveness in self-supervised representation learning on unseen graphs, few-shot in-context transfer, and zero-shot transfer, even surpassing or matching the performance of GNNs that have undergone supervised training on target datasets.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a cascaded architecture of Language Models (LMs) and Graph Neural Networks (GNNs) as backbone networks with a self-supervised training objective based on Masked Graph Modeling (MGM), and introduces graph instruction tuning using Large Language Models (LLMs) to enable zero-shot prediction ability."
            },
            "score": 4
        },
        {
            "id": "0caf98b5f5ff8e9476d55027473e15fca54292ff",
            "paperId": "0caf98b5f5ff8e9476d55027473e15fca54292ff",
            "title": "All in One and One for All: A Simple yet Effective Method towards Cross-domain Graph Pretraining",
            "abstract": "Large Language Models (LLMs) have revolutionized the fields of computer vision (CV) and natural language processing (NLP). One of the most notable advancements of LLMs is that a single model is trained on vast and diverse datasets spanning multiple domains -- a paradigm we term `All in One'. This methodology empowers LLMs with super generalization capabilities, facilitating an encompassing comprehension of varied data distributions. Leveraging these capabilities, a single LLM demonstrates remarkable versatility across a variety of domains -- a paradigm we term `One for All'. However, applying this idea to the graph field remains a formidable challenge, with cross-domain pretraining often resulting in negative transfer. This issue is particularly important in few-shot learning scenarios, where the paucity of training data necessitates the incorporation of external knowledge sources. In response to this challenge, we propose a novel approach called Graph COordinators for PrEtraining (GCOPE), that harnesses the underlying commonalities across diverse graph datasets to enhance few-shot learning. Our novel methodology involves a unification framework that amalgamates disparate graph datasets during the pretraining phase to distill and transfer meaningful knowledge to target tasks. Extensive experiments across multiple graph datasets demonstrate the superior efficacy of our approach. By successfully leveraging the synergistic potential of multiple graph datasets for pretraining, our work stands as a pioneering contribution to the realm of graph foundational model.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel approach called Graph COordinators for PrEtraining (GCOPE), that harnesses the underlying commonalities across diverse graph datasets to enhance few-shot learning and stands as a pioneering contribution to the realm of graph foundational model."
            },
            "score": 4
        },
        {
            "id": "c32873b62b7e186a56b941c688ef7cf64e6289d0",
            "paperId": "c32873b62b7e186a56b941c688ef7cf64e6289d0",
            "title": "Response: Emergent analogical reasoning in large language models",
            "abstract": "In their recent Nature Human Behaviour paper,\"Emergent analogical reasoning in large language models,\"(Webb, Holyoak, and Lu, 2023) the authors argue that\"large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems.\"In this response, we provide counterexamples of the letter string analogies. In our tests, GPT-3 fails to solve simplest variations of the original tasks, whereas human performance remains consistently high across all modified versions. Zero-shot reasoning is an extraordinary claim that requires extraordinary evidence. We do not see that evidence in our experiments. To strengthen claims of humanlike reasoning such as zero-shot reasoning, it is important that the field develop approaches that rule out data memorization.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "In this response, counterexamples of the letter string analogies are provided, showing that GPT-3 fails to solve simplest variations of the original tasks, whereas human performance remains consistently high across all modified versions."
            },
            "score": 4
        },
        {
            "id": "e67dd9230cd4a8199ca6c9c9ebaa31ab29be23d2",
            "paperId": "e67dd9230cd4a8199ca6c9c9ebaa31ab29be23d2",
            "title": "Evidence from counterfactual tasks supports emergent analogical reasoning in large language models",
            "abstract": "We recently reported evidence that large language models are capable of solving a wide range of text-based analogy problems in a zero-shot manner, indicating the presence of an emergent capacity for analogical reasoning. Two recent commentaries have challenged these results, citing evidence from so-called `counterfactual' tasks in which the standard sequence of the alphabet is arbitrarily permuted so as to decrease similarity with materials that may have been present in the language model's training data. Here, we reply to these critiques, clarifying some misunderstandings about the test materials used in our original work, and presenting evidence that language models are also capable of generalizing to these new counterfactual task variants.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 4
        },
        {
            "id": "0c7ca22dfbe1ce02cb0f0658292499457db8ec6e",
            "paperId": "0c7ca22dfbe1ce02cb0f0658292499457db8ec6e",
            "title": "InfiMM-Eval: Complex Open-Ended Reasoning Evaluation For Multi-Modal Large Language Models",
            "abstract": "Multi-modal Large Language Models (MLLMs) are increasingly prominent in the field of artificial intelligence. These models not only excel in traditional vision-language tasks but also demonstrate impressive performance in contemporary multi-modal benchmarks. Although many of these benchmarks attempt to holistically evaluate MLLMs, they typically concentrate on basic reasoning tasks, often yielding only simple yes/no or multi-choice responses. These methods naturally lead to confusion and difficulties in conclusively determining the reasoning capabilities of MLLMs. To mitigate this issue, we manually curate a benchmark dataset specifically designed for MLLMs, with a focus on complex reasoning tasks. Our benchmark comprises three key reasoning categories: deductive, abductive, and analogical reasoning. The queries in our dataset are intentionally constructed to engage the reasoning capabilities of MLLMs in the process of generating answers. For a fair comparison across various MLLMs, we incorporate intermediate reasoning steps into our evaluation criteria. In instances where an MLLM is unable to produce a definitive answer, its reasoning ability is evaluated by requesting intermediate reasoning steps. If these steps align with our manual annotations, appropriate scores are assigned. This evaluation scheme resembles methods commonly used in human assessments, such as exams or assignments, and represents what we consider a more effective assessment technique compared with existing benchmarks. We evaluate a selection of representative MLLMs using this rigorously developed open-ended multi-step elaborate reasoning benchmark, designed to challenge and accurately measure their reasoning capabilities. The code and data will be released at https://infimm.github.io/InfiMM-Eval/",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work manually curates a benchmark dataset specifically designed for MLLMs, with a focus on complex reasoning tasks, and develops an open-ended multi-step elaborate reasoning benchmark, designed to challenge and accurately measure their reasoning capabilities."
            },
            "score": 4
        },
        {
            "id": "8e6aa54db2acf4cdb212ebb7b934f00f1f10142c",
            "paperId": "8e6aa54db2acf4cdb212ebb7b934f00f1f10142c",
            "title": "Elucidating STEM Concepts through Generative AI: A Multi-modal Exploration of Analogical Reasoning",
            "abstract": "This study explores the integration of generative artificial intelligence (AI), specifically large language models, with multi-modal analogical reasoning as an innovative approach to enhance science, technology, engineering, and mathematics (STEM) education. We have developed a novel system that utilizes the capacities of generative AI to transform intricate principles in mathematics, physics, and programming into comprehensible metaphors. To further augment the educational experience, these metaphors are subsequently converted into visual form. Our study aims to enhance the learners' understanding of STEM concepts and their learning engagement by using the visual metaphors. We examine the efficacy of our system via a randomized A/B/C test, assessing learning gains and motivation shifts among the learners. Our study demonstrates the potential of applying large language models to educational practice on STEM subjects. The results will shed light on the design of educational system in terms of harnessing AI's potential to empower educational stakeholders.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study develops a novel system that utilizes the capacities of generative AI to transform intricate principles in mathematics, physics, and programming into comprehensible metaphors, and these metaphors are subsequently converted into visual form to enhance the learners' understanding of STEM concepts and their learning engagement by using the visual metaphors."
            },
            "score": 4
        },
        {
            "id": "e0fb32da90c344c61c43cf309fb673892c98e97b",
            "paperId": "e0fb32da90c344c61c43cf309fb673892c98e97b",
            "title": "Concept-aware Training Improves In-context Learning Ability of Language Models",
            "abstract": "Many recent language models (LMs) of Transformers family exhibit so-called in-context learning (ICL) ability, manifested in the LMs' ability to modulate their function by a task described in a natural language input. Previous work curating these models assumes that ICL emerges from vast over-parametrization or the scale of multi-task training. However, a complementary branch of recent theoretical work attributes ICL emergence to specific properties of training data and creates functional in-context learners in small-scale, synthetic settings. Inspired by recent findings on data properties driving the emergence of ICL, we propose a method to create LMs able to better utilize the in-context information, by constructing training scenarios where it is beneficial for the LM to capture the analogical reasoning concepts. We measure that data sampling of Concept-aware Training (CoAT) consistently improves models' reasoning ability. As a result, the in-context learners trained with CoAT on only two datasets of a single (QA) task perform comparably to larger models trained on 1600+ tasks.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a method to create LMs able to better utilize the in-context information, by constructing training scenarios where it is beneficial for the LM to capture the analogical reasoning concepts, and measures that data sampling of Concept-aware Training (CoAT) consistently improves models' reasoning ability."
            },
            "score": 4
        },
        {
            "id": "79729ed54ad03fd403d88f8d1543b49c7a58b973",
            "paperId": "79729ed54ad03fd403d88f8d1543b49c7a58b973",
            "title": "The language of prompting: What linguistic properties make a prompt successful?",
            "abstract": "The latest generation of LLMs can be prompted to achieve impressive zero-shot or few-shot performance in many NLP tasks. However, since performance is highly sensitive to the choice of prompts, considerable effort has been devoted to crowd-sourcing prompts or designing methods for prompt optimisation. Yet, we still lack a systematic understanding of how linguistic properties of prompts correlate with task performance. In this work, we investigate how LLMs of different sizes, pre-trained and instruction-tuned, perform on prompts that are semantically equivalent, but vary in linguistic structure. We investigate both grammatical properties such as mood, tense, aspect and modality, as well as lexico-semantic variation through the use of synonyms. Our findings contradict the common assumption that LLMs achieve optimal performance on lower perplexity prompts that reflect language use in pretraining or instruction-tuning data. Prompts transfer poorly between datasets or models, and performance cannot generally be explained by perplexity, word frequency, ambiguity or prompt length. Based on our results, we put forward a proposal for a more robust and comprehensive evaluation standard for prompting research.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates how LLMs of different sizes, pre-trained and instruction-tuned, perform on prompts that are semantically equivalent, but vary in linguistic structure, and puts forward a proposal for a more robust and comprehensive evaluation standard for prompting research."
            },
            "score": 4
        },
        {
            "id": "d61e6cbf87c945f289d201fb030871fc8ef956a3",
            "paperId": "d61e6cbf87c945f289d201fb030871fc8ef956a3",
            "title": "Partial Analogical Transfer in Problem Solving: Roles of Centrality and Order",
            "abstract": "Partial Analogical Transfer in Problem Solving: Roles of Centrality and Order Tsunhin J. Wong (thjwong@hkusua.hku.hk) Albert W. L. Chau (awlchau@hkucc.hku.hk) Department of Psychology, University of Hong Kong Pokfulam Road, Hong Kong Antonietti (1991) first presented the idea of partial analogies in solving an ill-defined problem. Prior to that, studies looking into the role of analogies in problem solving used complete analogies. Antonietti concluded that partial analogies helped problem solving only when all the cues were present and presented in the correct order. The conditions for partial analogy to work as identified by Antonietti seem to be overly stringent. Partial analogy should be effective in most situations as most analogical cues available in daily life are partial in nature. This study therefore looks into the role of analogical transfer of partial analogies. Our study differs from Antonietti (1991) in a few ways. First, as it is logical to assume that some analogical cues are more crucial than the others, the notion of centrality of an analogy was examined. Second, we also revisited if analogical cues have to be presented in the exact order in order to be effective. Finally, we presented the partial analogies as problems for participants to solve rather than disguised them as arithmetic problems. Method Forty undergraduates at the University of Hong Kong participated in the experiment as part of a course requirement. None of them had been exposed to the problems used in the study. Every participant completed the experiment on a computer. The problems were written in Flash with both texts and diagrams. Procedure Two types of problems were used in the experiment. The analogy problems were concerned about how to direct water to a target location. They were used to prime participants to use the two strategies which are necessary for solving the target problems: divergence which is dividing the flow to avoid overload and convergence which is pulling together the divided flows to achieve the intensity needed. The target problems were the Fortress problem and Duncker\u2019s radiation problem. The first problem deals with how to organize soldiers to conquer and castle while the other deals with how to use X-ray to destroy a tumor. Each participant was first told some basic concepts subjects in fluid dynamics. S/he then proceeded to solve one of the four versions of the analogy problem depending on the experimental condition to which s/he was assigned: i) partial analogies presented in the correct order (divergence then convergence), ii) partial analogies presented in the reverse order (convergence then divergence), iii) complete analogy; and iv) unrelated analogy. Finally the participant was asked to solve the two target problems. If s/he could not solve a problem within 5 minutes, s/he was then told that the problems they had tackled earlier might help them. A total of 10 minutes were allowed to solve each problem. Results and Discussion 43.9% of the subjects solved the Duncker\u2019s problem without being prompted to use the previous problems, with the highest in the partial-correct condition (81.8%) and the lowest in the unrelated condition (20%). Solving the Fortress problem in the unrelated condition required more time than in all the other conditions. Time required to solve the Duncker\u2019s problem was in the decreasing order of: partial-correct, partial-reverse, complete, and unrelated. Besides, the partial-correct condition required the least prompting for using the partial analogy condition. Contrary to Antonietti\u2019s study, participants in the partial-reverse condition were able to solve the target problems spontaneously. In other words, exposure to partial analogies is sufficient for priming to occur though the exact temporal order can provide additional facilitation. Second, it was also found that partial analogies were more effective than complete analogy in priming the participant to solve the target problem. This is possibly due to the complexity involved in solving the complete analogy problem. Third, the present findings suggested that convergence plays a more central role in analogical transfer. This is in line with Pedone, et al (2001) who found that convergence alone was sufficient to trigger spontaneous analogical transfer. The second and third findings together suggested that convergence is the more central partial analogy. Only it should be presented to achieve the greatest priming or facilitatory effect in problem solving. Presenting a less useful partial analogy (divergence) lowers instead of enhances the transfer in problem solving performance. This may be explained by introducing a weight system in the branches of the structural mapping theory (Gentner, 1983) or constraints in the multiconstraint theory (Holyoak & Thagard, 1980). References Antonietti, A. (1991). Effects of partial analogies on solving an ill-defined problem. Psychological Reports, 68, 947-60. Pedone, R., Hummel, J. E., & Holyoak, K. J. (2001). The use of diagrams in analogical problem solving. Memory & Cognition, 29(2), 214-221. Gentner, D. (1983). Structural-mapping: A theoretical framework for analogy. Cognitive Science, 7, 155-170. Holyoak, K. J., & Thagard, P. (1980). Analogical mapping by constraint satisfaction. Cognitive Science, 13, 295-355.",
            "year": 2019,
            "citationCount": 0,
            "tldr": null,
            "score": 4
        },
        {
            "id": "1d4cc479f1423dc30e80c077a4ac20673e64323a",
            "paperId": "1d4cc479f1423dc30e80c077a4ac20673e64323a",
            "title": "Leveraging Weakly Annotated Data for Hate Speech Detection in Code-Mixed Hinglish: A Feasibility-Driven Transfer Learning Approach with Large Language Models",
            "abstract": "The advent of Large Language Models (LLMs) has advanced the benchmark in various Natural Language Processing (NLP) tasks. However, large amounts of labelled training data are required to train LLMs. Furthermore, data annotation and training are computationally expensive and time-consuming. Zero and few-shot learning have recently emerged as viable options for labelling data using large pre-trained models. Hate speech detection in mix-code low-resource languages is an active problem area where the use of LLMs has proven beneficial. In this study, we have compiled a dataset of 100 YouTube comments, and weakly labelled them for coarse and fine-grained misogyny classification in mix-code Hinglish. Weak annotation was applied due to the labor-intensive annotation process. Zero-shot learning, one-shot learning, and few-shot learning and prompting approaches have then been applied to assign labels to the comments and compare them to human-assigned labels. Out of all the approaches, zero-shot classification using the Bidirectional Auto-Regressive Transformers (BART) large model and few-shot prompting using Generative Pre-trained Transformer- 3 (ChatGPT-3) achieve the best results",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study has compiled a dataset of 100 YouTube comments, and weakly labelled them for coarse and fine-grained misogyny classification in mix-code Hinglish, and zero-shot learning, one-shot learning, and few-shot learning and prompting approaches have been applied to assign labels to the comments and compare them to human-assigned labels."
            },
            "score": 4
        },
        {
            "id": "63179e3321c29b455b16fdbfdf59017113b66071",
            "paperId": "63179e3321c29b455b16fdbfdf59017113b66071",
            "title": "Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning",
            "abstract": "Cross-lingual transfer of language models trained on high-resource languages like English has been widely studied for many NLP tasks, but focus on conversational tasks has been rather limited. This is partly due to the high cost of obtaining non-English conversational data, which results in limited coverage. In this work, we introduce for cross-lingual alignment pretraining, a parallel and large-scale multilingual conversation dataset that we created by translating the English-only Schema-Guided Dialogue (SGD) dataset (Rastogi et al., 2020) into 105 other languages. XSGD contains about 330k utterances per language. To facilitate aligned cross-lingual representations, we develop an efficient prompt-tuning-based method for learning alignment prompts. We also investigate two different classifiers: NLI-based and vanilla classifiers, and test cross-lingual capability enabled by the aligned prompts. We evaluate our model\u2019s cross-lingual generalization capabilities on two conversation tasks: slot-filling and intent classification. Our results demonstrate strong and efficient modeling ability of NLI-based classifiers and the large cross-lingual transfer improvements achieved by our aligned prompts, particularly in few-shot settings. We also conduct studies on large language models (LLMs) such as text-davinci-003 and ChatGPT in both zero- and few-shot settings. While LLMs exhibit impressive performance in English, their cross-lingual capabilities in other languages, particularly low-resource ones, are limited.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces for cross-lingual alignment pretraining, a parallel and large-scale multilingual conversation dataset that is created by translating the English-only Schema-Guided Dialogue (SGD) dataset into 105 other languages and develops an efficient prompt-tuning-based method for learning alignment prompts."
            },
            "score": 4
        },
        {
            "id": "dcb3381f33e5313fe4f0db31c0cbc0ff7e533cef",
            "paperId": "dcb3381f33e5313fe4f0db31c0cbc0ff7e533cef",
            "title": "CAT-LLM: Prompting Large Language Models with Text Style Definition for Chinese Article-style Transfer",
            "abstract": "Text style transfer is increasingly prominent in online entertainment and social media. However, existing research mainly concentrates on style transfer within individual English sentences, while ignoring the complexity of long Chinese texts, which limits the wider applicability of style transfer in digital media realm. To bridge this gap, we propose a Chinese Article-style Transfer framework (CAT-LLM), leveraging the capabilities of Large Language Models (LLMs). CAT-LLM incorporates a bespoke, pluggable Text Style Definition (TSD) module aimed at comprehensively analyzing text features in articles, prompting LLMs to efficiently transfer Chinese article-style. The TSD module integrates a series of machine learning algorithms to analyze article-style from both words and sentences levels, thereby aiding LLMs thoroughly grasp the target style without compromising the integrity of the original text. In addition, this module supports dynamic expansion of internal style trees, showcasing robust compatibility and allowing flexible optimization in subsequent research. Moreover, we select five Chinese articles with distinct styles and create five parallel datasets using ChatGPT, enhancing the models' performance evaluation accuracy and establishing a novel paradigm for evaluating subsequent research on article-style transfer. Extensive experimental results affirm that CAT-LLM outperforms current research in terms of transfer accuracy and content preservation, and has remarkable applicability to various types of LLMs.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Chinese Article-style Transfer framework (CAT-LLM), leveraging the capabilities of Large Language Models, which outperforms current research in terms of transfer accuracy and content preservation, and has remarkable applicability to various types of LLMs."
            },
            "score": 3
        },
        {
            "id": "dfffba50d7630f1e68d9cc67d4a9a1c6519b93cd",
            "paperId": "dfffba50d7630f1e68d9cc67d4a9a1c6519b93cd",
            "title": "Text Style Transfer Evaluation Using Large Language Models",
            "abstract": "Evaluating Text Style Transfer (TST) is a complex task due to its multifaceted nature. The quality of the generated text is measured based on challenging factors, such as style transfer accuracy, content preservation, and overall fluency. While human evaluation is considered to be the gold standard in TST assessment, it is costly and often hard to reproduce. Therefore, automated metrics are prevalent in these domains. Nevertheless, it remains unclear whether these automated metrics correlate with human evaluations. Recent strides in Large Language Models (LLMs) have showcased their capacity to match and even exceed average human performance across diverse, unseen tasks. This suggests that LLMs could be a feasible alternative to human evaluation and other automated metrics in TST evaluation. We compare the results of different LLMs in TST using multiple input prompts. Our findings highlight a strong correlation between (even zero-shot) prompting and human evaluation, showing that LLMs often outperform traditional automated metrics. Furthermore, we introduce the concept of prompt ensembling, demonstrating its ability to enhance the robustness of TST evaluation. This research contributes to the ongoing evaluation of LLMs in diverse tasks, offering insights into successful outcomes and areas of limitation.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A strong correlation between (even zero-shot) prompting and human evaluation is highlighted, showing that LLMs often outperform traditional automated metrics in TST evaluation, and the concept of prompt ensembling is introduced, demonstrating its ability to enhance the robustness of T ST evaluation."
            },
            "score": 3
        },
        {
            "id": "5c5ab276b00c1f19fbb0a3d2c38d532becac9442",
            "paperId": "5c5ab276b00c1f19fbb0a3d2c38d532becac9442",
            "title": "A Brief History of Prompt: Leveraging Language Models. (Through Advanced Prompting)",
            "abstract": "This paper presents a comprehensive exploration of the evolution of prompt engineering and generation in the field of natural language processing (NLP). Starting from the early language models and information retrieval systems, we trace the key developments that have shaped prompt engineering over the years. The introduction of attention mechanisms in 2015 revolutionized language understanding, leading to advancements in controllability and context-awareness. Subsequent breakthroughs in reinforcement learning techniques further enhanced prompt engineering, addressing issues like exposure bias and biases in generated text. We examine the significant contributions in 2018 and 2019, focusing on fine-tuning strategies, control codes, and template-based generation. The paper also discusses the growing importance of fairness, human-AI collaboration, and low-resource adaptation. In 2020 and 2021, contextual prompting and transfer learning gained prominence, while 2022 and 2023 witnessed the emergence of advanced techniques like unsupervised pre-training and novel reward shaping. Throughout the paper, we reference specific research studies that exemplify the impact of various developments on prompt engineering. The journey of prompt engineering continues, with ethical considerations being paramount for the responsible and inclusive future of AI systems.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The journey of prompt engineering continues, with ethical considerations being paramount for the responsible and inclusive future of AI systems."
            },
            "score": 3
        },
        {
            "id": "fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c",
            "paperId": "fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c",
            "title": "Language Is Not All You Need: Aligning Perception with Language Models",
            "abstract": "A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Specifically, we train Kosmos-1 from scratch on web-scale multimodal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. We evaluate various settings, including zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range of tasks without any gradient updates or finetuning. Experimental results show that Kosmos-1 achieves impressive performance on (i) language understanding, generation, and even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language. In addition, we introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs.",
            "year": 2023,
            "citationCount": 294,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context, and follow instructions, and shows that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodals, and from multimodal to language."
            },
            "score": 3
        },
        {
            "id": "084e4d013c43ca4686f0dac75b3495c6624545c8",
            "paperId": "084e4d013c43ca4686f0dac75b3495c6624545c8",
            "title": "Exploring the Transferability of Visual Prompting for Multimodal Large Language Models",
            "abstract": "Although Multimodal Large Language Models (MLLMs) have demonstrated promising versatile capabilities, their performance is still inferior to specialized models on downstream tasks, which makes adaptation necessary to enhance their utility. However, fine-tuning methods require independent training for every model, leading to huge computation and memory overheads. In this paper, we propose a novel setting where we aim to improve the performance of diverse MLLMs with a group of shared parameters optimized for a downstream task. To achieve this, we propose Transferable Visual Prompting (TVP), a simple and effective approach to generate visual prompts that can transfer to different models and improve their performance on downstream tasks after trained on only one model. We introduce two strategies to address the issue of cross-model feature corruption of existing visual prompting methods and enhance the transferability of the learned prompts, including 1) Feature Consistency Alignment: which imposes constraints to the prompted feature changes to maintain task-agnostic knowledge; 2) Task Semantics Enrichment: which encourages the prompted images to contain richer task-specific semantics with language guidance. We validate the effectiveness of TVP through extensive experiments with 6 modern MLLMs on a wide variety of tasks ranging from object recognition and counting to multimodal reasoning and hallucination correction.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes Transferable Visual Prompting (TVP), a simple and effective approach to generate visual prompts that can transfer to different models and improve their performance on downstream tasks after trained on only one model."
            },
            "score": 3
        },
        {
            "id": "e0095740512aa1b1e991b6ee920ccba7f8beb569",
            "paperId": "e0095740512aa1b1e991b6ee920ccba7f8beb569",
            "title": "Unveiling A Core Linguistic Region in Large Language Models",
            "abstract": "Brain localization, which describes the association between specific regions of the brain and their corresponding functions, is widely accepted in the field of cognitive science as an objective fact. Today's large language models (LLMs) possess human-level linguistic competence and can execute complex tasks requiring abstract knowledge and reasoning. To deeply understand the inherent mechanisms of intelligence emergence in LLMs, this paper conducts an analogical research using brain localization as a prototype. We have discovered a core region in LLMs that corresponds to linguistic competence, accounting for approximately 1% of the total model parameters. This core region exhibits significant dimension dependency, and perturbations to even a single parameter on specific dimensions can lead to a loss of linguistic competence. Furthermore, we observe that an improvement in linguistic competence does not necessarily accompany an elevation in the model's knowledge level, which might imply the existence of regions of domain knowledge that are dissociated from the linguistic region. Overall, exploring the LLMs' functional regions provides insights into the foundation of their intelligence. In the future, we will continue to investigate knowledge regions within LLMs and the interactions between them.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A core region in LLMs that corresponds to linguistic competence is discovered, accounting for approximately 1% of the total model parameters, and it is observed that an improvement in linguistic competence does not necessarily accompany an elevation in the model's knowledge level, which might imply the existence of regions of domain knowledge that are dissociated from the linguistic region."
            },
            "score": 3
        },
        {
            "id": "71cf75ea133313897a272841809c3ef6c17c72bf",
            "paperId": "71cf75ea133313897a272841809c3ef6c17c72bf",
            "title": "Comparing LLM prompting with Cross-lingual transfer performance on Indigenous and Low-resource Brazilian Languages",
            "abstract": "Large Language Models are transforming NLP for a variety of tasks. However, how LLMs perform NLP tasks for low-resource languages (LRLs) is less explored. In line with the goals of the AmericasNLP workshop, we focus on 12 LRLs from Brazil, 2 LRLs from Africa and 2 high-resource languages (HRLs) (e.g., English and Brazilian Portuguese). Our results indicate that the LLMs perform worse for the part of speech (POS) labeling of LRLs in comparison to HRLs. We explain the reasons behind this failure and provide an error analysis through examples observed in our data set.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work focuses on 12 LRLs from Brazil, 2 LRLs from Africa and 2 high-resource languages (HRLs) and indicates that the LLMs perform worse for the part of speech (POS) labeling of LRLs in comparison to HRLs."
            },
            "score": 3
        },
        {
            "id": "139902331c9a97b402d01bd5a263efb78c0dc247",
            "paperId": "139902331c9a97b402d01bd5a263efb78c0dc247",
            "title": "This joke is [MASK]: Recognizing Humor and Offense with Prompting",
            "abstract": "Humor is a magnetic component in everyday human interactions and communications. Computationally modeling humor enables NLP systems to entertain and engage with users. We investigate the effectiveness of prompting, a new transfer learning paradigm for NLP, for humor recognition. We show that prompting performs similarly to finetuning when numerous annotations are available, but gives stellar performance in low-resource humor recognition. The relationship between humor and offense is also inspected by applying influence functions to prompting; we show that models could rely on offense to determine humor during transfer.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates the effectiveness of prompting, a new transfer learning paradigm for NLP, for humor recognition, and shows that prompting performs similarly to finetuning when numerous annotations are available, but gives stellar performance in low-resource humor recognition."
            },
            "score": 3
        },
        {
            "id": "68c257b73deac07d9ef3cb5287fe7c82ade312ae",
            "paperId": "68c257b73deac07d9ef3cb5287fe7c82ade312ae",
            "title": "Retrieval-based Knowledge Transfer: An Effective Approach for Extreme Large Language Model Compression",
            "abstract": "Large-scale pre-trained language models (LLMs) have demonstrated exceptional performance in various natural language processing (NLP) tasks. However, the massive size of these models poses huge challenges for their deployment in real-world applications. While numerous model compression techniques have been proposed, most of them are not well-suited for achieving extreme model compression when there is a significant gap in model scale. In this paper, we introduce a novel compression paradigm called Retrieval-based Knowledge Transfer (RetriKT), which effectively transfers the knowledge of LLMs to extremely small-scale models (e.g., 1%). In particular, our approach extracts knowledge from LLMs to construct a knowledge store, from which the small-scale model can retrieve relevant information and leverage it for effective inference. To improve the quality of the model, soft prompt tuning and Proximal Policy Optimization (PPO) reinforcement learning techniques are employed. Extensive experiments are conducted on low-resource tasks from SuperGLUE and GLUE benchmarks. The results demonstrate that the proposed approach significantly enhances the performance of small-scale models by leveraging the knowledge from LLMs.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel compression paradigm called Retrieval-based Knowledge Transfer (RetriKT), which effectively transfers the knowledge of LLMs to extremely small-scale models (e.g., 1%) and significantly enhances the performance of small- scale models by leveraging the knowledge from LLMs."
            },
            "score": 3
        },
        {
            "id": "b18daa14486920016c4664c3ed1759f2de1ba854",
            "paperId": "b18daa14486920016c4664c3ed1759f2de1ba854",
            "title": "Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models",
            "abstract": "Medical image segmentation with deep learning is an important and widely studied topic because segmentation enables quantifying target structure size and shape that can help in disease diagnosis, prognosis, surgery planning, and understanding. Recent advances in the foundation VLMs and their adaptation to segmentation tasks in natural images with VLSMs have opened up a unique opportunity to build potentially powerful segmentation models for medical images that enable providing helpful information via language prompt as input, leverage the extensive range of other medical imaging datasets by pooled dataset training, adapt to new classes, and be robust against out-of-distribution data with human-in-the-loop prompting during inference. Although transfer learning from natural to medical images for image-only segmentation models has been studied, no studies have analyzed how the joint representation of vision-language transfers to medical images in segmentation problems and understand gaps in leveraging their full potential. We present the first benchmark study on transfer learning of VLSMs to 2D medical images with thoughtfully collected 11 existing 2D medical image datasets of diverse modalities with carefully presented 9 types of language prompts from 14 attributes. Our results indicate that VLSMs trained in natural image-text pairs transfer reasonably to the medical domain in zero-shot settings when prompted appropriately for non-radiology photographic modalities; when finetuned, they obtain comparable performance to conventional architectures, even in X-rays and ultrasound modalities. However, the additional benefit of language prompts during finetuning may be limited, with image features playing a more dominant role; they can better handle training on pooled datasets combining diverse modalities and are potentially more robust to domain shift than the conventional segmentation models.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results indicate that VLSMs trained in natural image-text pairs transfer reasonably to the medical domain in zero-shot settings when prompted appropriately for non-radiology photographic modalities; when finetuned, they obtain comparable performance to conventional architectures, even in X-rays and ultrasound modalities."
            },
            "score": 2
        },
        {
            "id": "98fc8faae839cc9b9dbfcf8e3ec3a7e2f7729db2",
            "paperId": "98fc8faae839cc9b9dbfcf8e3ec3a7e2f7729db2",
            "title": "NOTABLE: Transferable Backdoor Attacks Against Prompt-based NLP Models",
            "abstract": "Prompt-based learning is vulnerable to backdoor attacks. Existing backdoor attacks against prompt-based models consider injecting backdoors into the entire embedding layers or word embedding vectors. Such attacks can be easily affected by retraining on downstream tasks and with different prompting strategies, limiting the transferability of backdoor attacks. In this work, we propose transferable backdoor attacks against prompt-based models, called NOTABLE, which is independent of downstream tasks and prompting strategies. Specifically, NOTABLE injects backdoors into the encoders of PLMs by utilizing an adaptive verbalizer to bind triggers to specific words (i.e., anchors). It activates the backdoor by pasting input with triggers to reach adversary-desired anchors, achieving independence from downstream tasks and prompting strategies. We conduct experiments on six NLP tasks, three popular models, and three prompting strategies. Empirical results show that NOTABLE achieves superior attack performance (i.e., attack success rate over 90% on all the datasets), and outperforms two state-of-the-art baselines. Evaluations on three defenses show the robustness of NOTABLE. Our code can be found at https://github.com/RU-System-Software-and-Security/Notable.",
            "year": 2023,
            "citationCount": 19,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes transferable backdoor attacks against prompt-based models, called NOTABLE, which is independent of downstream tasks and prompting strategies, and achieves superior attack performance and outperforms two state-of-the-art baselines."
            },
            "score": 2
        }
    ],
    "novelty": "yes"
}