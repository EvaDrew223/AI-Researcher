{
    "topic_description": "novel prompting methods that can improve factuality and reduce hallucination of large language models",
    "idea_name": "Context-Aware Verification Prompting",
    "raw_idea": {
        "Problem": "Large language models often generate hallucinated content that is inconsistent with the input context or world knowledge, leading to factual inaccuracies in the generated output.",
        "Existing Methods": "Current methods for reducing hallucination include using retrieval-augmented generation, incorporating external knowledge bases, or using reinforcement learning with factual consistency rewards.",
        "Motivation": "We hypothesize that large language models have the ability to verify the factual consistency of their own generated content, if prompted to do so with the relevant context. By explicitly prompting the model to cross-reference its output with the input context and world knowledge, we can encourage the model to generate more factually consistent output.",
        "Proposed Method": "We propose Context-Aware Verification Prompting, a multi-step prompting approach to reduce hallucination. First, we prompt the model to generate an initial output given the input context. Second, we prompt the model to verify the factual consistency of its generated output by cross-referencing with the input context and world knowledge, and to identify any inconsistencies or unsupported claims. Third, we prompt the model to revise its initial output to address the identified inconsistencies and generate a more factually consistent final output.",
        "Experiment Plan": "We will evaluate our proposed method on factual consistency benchmarks such as the Fact Extraction and VERification (FEVER) dataset and the Fact-Checking Assistant task. We will compare our method with baseline prompting approaches without verification, as well as state-of-the-art methods for reducing hallucination."
    },
    "full_experiment_plan": {
        "Title": "Context-Aware Verification Prompting for Reducing Hallucination in Large Language Models",
        "Problem Statement": "Large language models often generate hallucinated content that is inconsistent with the input context or world knowledge, leading to factual inaccuracies in the generated output.",
        "Motivation": "Current methods for reducing hallucination, such as retrieval-augmented generation, incorporating external knowledge bases, or using reinforcement learning with factual consistency rewards, require additional training or external resources. We hypothesize that large language models have the inherent ability to verify the factual consistency of their own generated content, if prompted to do so with the relevant context. By explicitly prompting the model to cross-reference its output with the input context and world knowledge, we can encourage the model to generate more factually consistent output without the need for additional training or resources.",
        "Proposed Method": "We propose Context-Aware Verification Prompting, a multi-step prompting approach to reduce hallucination. First, we prompt the model to generate an initial output given the input context. Second, we prompt the model to verify the factual consistency of its generated output by cross-referencing with the input context and world knowledge, and to identify any inconsistencies or unsupported claims. Third, we prompt the model to revise its initial output to address the identified inconsistencies and generate a more factually consistent final output.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We will evaluate our proposed method on factual consistency benchmarks such as the Fact Extraction and VERification (FEVER) dataset and the Fact-Checking Assistant task. FEVER consists of claims generated from Wikipedia articles, labeled as SUPPORTED, REFUTED, or NOTENOUGHINFO, along with the relevant evidence sentences. The Fact-Checking Assistant task involves generating factual claims from a given text and then verifying their correctness.",
            "Step 2: Construct Prompts": "We will compare our method with baseline prompting approaches without verification, as well as state-of-the-art methods for reducing hallucination. The baseline prompt will be a simple instruction to generate a claim or answer a question based on the given context. For our proposed method, we will use a three-step prompting approach:\n1. Initial Generation Prompt: Generate a claim or answer the question based on the given context.\n2. Verification Prompt: Verify the factual consistency of the generated claim or answer by cross-referencing with the given context and world knowledge. Identify any inconsistencies or unsupported claims.\n3. Revision Prompt: Revise the initial claim or answer to address the identified inconsistencies and generate a more factually consistent output.",
            "Step 3: Select Models": "We will evaluate our method on state-of-the-art large language models such as GPT-3.5 (text-davinci-003), GPT-4, and open-source models like LLaMA and PaLM. This will help us understand the effectiveness of our approach across different model architectures and sizes.",
            "Step 4: Get Results": "For each dataset and model combination, we will generate outputs using both the baseline and our proposed prompting method. We will evaluate the factual consistency of the generated outputs using established metrics such as the FEVER score (for the FEVER dataset) and human evaluation (for the Fact-Checking Assistant task). We will also measure the reduction in hallucination by comparing the number of unsupported or inconsistent claims between the baseline and our method.",
            "Step 5: Analyze Results": "We will compare the performance of our proposed method with the baseline prompting approaches and state-of-the-art methods for reducing hallucination. We will analyze the effectiveness of each step in our prompting approach (initial generation, verification, and revision) and identify any patterns or trends in the types of inconsistencies detected and corrected by our method. We will also examine the impact of different model architectures and sizes on the effectiveness of our approach."
        },
        "Test Case Examples": {
            "Baseline Prompt Input": "Context: The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower. Locally nicknamed \"La dame de fer\" (French for \"Iron Lady\"), it was constructed from 1887 to 1889 as the centerpiece of the 1889 World's Fair and was initially criticized by some of France's leading artists and intellectuals for its design, but it has become a global cultural icon of France and one of the most recognizable structures in the world.\nQuestion: How tall is the Eiffel Tower?",
            "Baseline Prompt Expected Output": "The Eiffel Tower is 324 meters (1,063 feet) tall.",
            "Proposed Prompt Input (Step 1: Initial Generation)": "Context: The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower. Locally nicknamed \"La dame de fer\" (French for \"Iron Lady\"), it was constructed from 1887 to 1889 as the centerpiece of the 1889 World's Fair and was initially criticized by some of France's leading artists and intellectuals for its design, but it has become a global cultural icon of France and one of the most recognizable structures in the world.\nQuestion: How tall is the Eiffel Tower?\nInitial Answer:",
            "Proposed Prompt Expected Output (Step 1: Initial Generation)": "The Eiffel Tower stands at a height of 984 feet (300 meters).",
            "Proposed Prompt Input (Step 2: Verification)": "Context: The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower. Locally nicknamed \"La dame de fer\" (French for \"Iron Lady\"), it was constructed from 1887 to 1889 as the centerpiece of the 1889 World's Fair and was initially criticized by some of France's leading artists and intellectuals for its design, but it has become a global cultural icon of France and one of the most recognizable structures in the world.\nQuestion: How tall is the Eiffel Tower?\nInitial Answer: The Eiffel Tower stands at a height of 984 feet (300 meters).\nVerification: Check if the initial answer is factually consistent with the given context and world knowledge. Identify any inconsistencies or unsupported claims.",
            "Proposed Prompt Expected Output (Step 2: Verification)": "The initial answer is inconsistent with world knowledge. The Eiffel Tower is actually 324 meters (1,063 feet) tall, not 300 meters (984 feet) as stated in the initial answer. The given context does not provide the exact height of the tower, so the initial answer cannot be verified based on the context alone.",
            "Proposed Prompt Input (Step 3: Revision)": "Context: The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower. Locally nicknamed \"La dame de fer\" (French for \"Iron Lady\"), it was constructed from 1887 to 1889 as the centerpiece of the 1889 World's Fair and was initially criticized by some of France's leading artists and intellectuals for its design, but it has become a global cultural icon of France and one of the most recognizable structures in the world.\nQuestion: How tall is the Eiffel Tower?\nInitial Answer: The Eiffel Tower stands at a height of 984 feet (300 meters).\nVerification: The initial answer is inconsistent with world knowledge. The Eiffel Tower is actually 324 meters (1,063 feet) tall, not 300 meters (984 feet) as stated in the initial answer. The given context does not provide the exact height of the tower, so the initial answer cannot be verified based on the context alone.\nRevised Answer:",
            "Proposed Prompt Expected Output (Step 3: Revision)": "The Eiffel Tower is 324 meters (1,063 feet) tall.",
            "Explanation": "The proposed Context-Aware Verification Prompting method helps to identify and correct factual inconsistencies in the generated output. In this example, the initial answer provides an incorrect height for the Eiffel Tower. The verification step detects this inconsistency by cross-referencing with world knowledge and notes that the given context does not support the initial answer. The revision step then generates a more factually consistent answer based on the verification feedback."
        },
        "Fallback Plan": "If the proposed Context-Aware Verification Prompting method does not significantly reduce hallucination compared to the baselines, we can conduct additional analysis to understand the limitations and potential improvements:\n1. Analyze the quality and relevance of the verification prompts. Are the prompts effectively guiding the model to identify factual inconsistencies? Can we improve the prompts to elicit more accurate verification?\n2. Examine the types of inconsistencies that are not being detected or corrected by the method. Are there certain categories of hallucination that are more challenging to address? This analysis can inform the development of more targeted prompting strategies.\n3. Investigate the impact of different model architectures and sizes on the effectiveness of the method. Are certain models more amenable to verification prompting than others? This can help identify the most suitable models for reducing hallucination.\n4. Consider incorporating additional external knowledge sources or fact-checking tools to enhance the verification process. Can we leverage existing knowledge bases or fact-checking APIs to improve the accuracy of the verification step?\n5. Explore alternative prompting strategies, such as iterative refinement or adversarial prompting, to further reduce hallucination. Can we combine verification prompting with other prompting techniques to achieve better results?\nBased on the insights gained from this additional analysis, we can refine our proposed method or develop new approaches to address the limitations and improve the effectiveness of reducing hallucination in large language models. If the proposed method and its variations still do not yield satisfactory results, we can pivot the project to focus on understanding the factors that contribute to hallucination and the challenges in mitigating them, potentially leading to new research directions in this area."
    },
    "novelty_queries": [
        "KeywordQuery(\"language models hallucination reduction prompting\")",
        "KeywordQuery(\"language models factual consistency verification\")",
        "KeywordQuery(\"language models context-aware prompting\")",
        "KeywordQuery(\"Context-Aware Verification Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "4b0b56be0ae9479d2bd5c2f0943db1906343c10f",
            "paperId": "4b0b56be0ae9479d2bd5c2f0943db1906343c10f",
            "title": "Chain-of-Verification Reduces Hallucination in Large Language Models",
            "abstract": "Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (CoVe) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show CoVe decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.",
            "year": 2023,
            "citationCount": 53,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Chain-of-Verification (CoVe) method is developed, whereby the model first drafts an initial response; then plans verification questions to fact-check its draft; and answers those questions independently so the answers are not biased by other responses."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is reducing hallucination in large language models, and the proposed approach is to use a multi-step prompting method called Context-Aware Verification Prompting to make the model verify its own output for factual consistency.\n\nThe research problem in the paper is also reducing hallucination in large language models, and the proposed approach is a method called Chain-of-Verification (CoVe) where the model drafts a response, plans verification questions, answers them independently, and generates a final verified response.\n\nBoth the proposal and the paper aim to address the same problem of reducing hallucination in large language models, and both propose a multi-step approach involving self-verification by the model. While the specific steps and details may differ, the high-level concept and approach are directly relevant and similar.\n\nYes",
            "novelty_judgment": "yes"
        },
        {
            "id": "dfab0f3ee6f47e36cccee145794cd117773e6f73",
            "paperId": "dfab0f3ee6f47e36cccee145794cd117773e6f73",
            "title": "Towards LLM-based Fact Verification on News Claims with a Hierarchical Step-by-Step Prompting Method",
            "abstract": "While large pre-trained language models (LLMs) have shown their impressive capabilities in various NLP tasks, they are still under-explored in the misinformation domain. In this paper, we examine LLMs with in-context learning (ICL) for news claim verification, and find that only with 4-shot demonstration examples, the performance of several prompting methods can be comparable with previous supervised models. To further boost performance, we introduce a Hierarchical Step-by-Step (HiSS) prompting method which directs LLMs to separate a claim into several subclaims and then verify each of them via multiple questions-answering steps progressively. Experiment results on two public misinformation datasets show that HiSS prompting outperforms state-of-the-art fully-supervised approach and strong few-shot ICL-enabled baselines.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Hierarchical Step-by-Step (HiSS) prompting method is introduced which directs LLMs to separate a claim into several subclaims and then verify each of them via multiple questions-answering steps progressively."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is reducing hallucination in large language models, and the proposed approach is using a multi-step prompting method with verification to improve factual consistency.\n\nThe research problem in the paper is exploring large language models for news claim verification, and the proposed approach is using a hierarchical step-by-step prompting method to separate claims into subclaims and verify each progressively.\n\nWhile both works involve using prompting methods with large language models, the research problems and approaches are different. The proposal focuses on reducing hallucination in general language model outputs, while the paper specifically targets news claim verification. The prompting methods also differ in their design and objectives.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "620a1a585a5e433a47103c112de17553a81fcbe6",
            "paperId": "620a1a585a5e433a47103c112de17553a81fcbe6",
            "title": "Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks",
            "abstract": "Although remarkable progress has been achieved in preventing large language model (LLM) hallucinations using instruction tuning and retrieval augmentation, it remains challenging to measure the reliability of LLMs using human-crafted evaluation data which is not available for many tasks and domains and could suffer from data leakage. Inspired by adversarial machine learning, this paper aims to develop a method of automatically generating evaluation data by appropriately modifying existing data on which LLMs behave faithfully. Specifically, this paper presents AutoDebug, an LLM-based framework to use prompting chaining to generate transferable adversarial attacks in the form of question-answering examples. We seek to understand the extent to which these examples trigger the hallucination behaviors of LLMs. We implement AutoDebug using ChatGPT and evaluate the resulting two variants of a popular open-domain question-answering dataset, Natural Questions (NQ), on a collection of open-source and proprietary LLMs under various prompting settings. Our generated evaluation data is human-readable and, as we show, humans can answer these modified questions well. Nevertheless, we observe pronounced accuracy drops across multiple LLMs including GPT-4. Our experimental results show that LLMs are likely to hallucinate in two categories of question-answering scenarios where (1) there are conflicts between knowledge given in the prompt and their parametric knowledge, or (2) the knowledge expressed in the prompt is complex. Finally, we find that the adversarial examples generated by our method are transferable across all considered LLMs. The examples generated by a small model can be used to debug a much larger model, making our approach cost-effective.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents AutoDebug, an LLM-based framework to use prompting chaining to generate transferable adversarial attacks in the form of question-answering examples, and finds that the adversarial examples generated by the method are transferable across all considered LLMs."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to reduce hallucination in large language models by using a multi-step prompting approach called Context-Aware Verification Prompting, which involves generating an initial output, verifying its factual consistency, and revising it accordingly.\n\nThe paper focuses on automatically generating evaluation data using adversarial attacks to assess the hallucination behavior of aligned large language models, and understanding the scenarios in which LLMs are likely to hallucinate.\n\nWhile both the project proposal and the paper address the issue of hallucination in large language models, their approaches differ significantly. The project proposal suggests a prompting-based method to reduce hallucination during the generation process, while the paper proposes an evaluation framework to assess hallucination using automatically generated adversarial examples.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "152d9a231c00d4495c9bc4a466f42165ce2e2164",
            "paperId": "152d9a231c00d4495c9bc4a466f42165ce2e2164",
            "title": "Evaluating Factual Consistency of Summaries with Large Language Models",
            "abstract": "Detecting factual errors in summaries has been an important and challenging subject in summarization research. Inspired by the emergent ability of large language models (LLMs), we explore evaluating factual consistency of summaries by directly prompting LLMs. We present a comprehensive empirical study to assess the ability of LLMs as factual consistency evaluators, which consists of (1) analyzing different LLMs such as the GPT model series and Flan-T5; (2) investigating a variety of prompting methods including vanilla prompting, chain-of-thought prompting, and a sentence-by-sentence prompting method to tackle long summaries; and (3) evaluating on diverse summaries generated by multiple summarization systems, ranging from pre-transformer methods to SOTA pretrained models. Our experiments demonstrate that prompting LLMs is able to outperform the previous best factuality systems in all settings, by up to 12.2 absolute points in terms of the binary classification accuracy on inconsistency detection.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A comprehensive empirical study to assess the ability of LLMs as factual consistency evaluators, which consists of analyzing different LLMs such as the GPT model series and Flan-T5 and investigating a variety of prompting methods including vanilla prompting, chain-of-thought prompting, and a sentence-by-sentence prompting method to tackle long summaries."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is reducing hallucination in large language models, and the proposed approach is to use context-aware verification prompting to encourage the model to generate more factually consistent output.\n\nThe research problem in the paper is evaluating factual consistency of summaries, and the proposed approach is to use large language models with different prompting methods to detect factual errors in summaries.\n\nWhile both works involve factual consistency and large language models, the proposal focuses on reducing hallucination during generation, while the paper focuses on evaluating factual consistency of already generated summaries. The methods proposed are also different: the proposal uses a multi-step prompting approach for generation, while the paper uses various prompting methods for evaluation.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "6af460d34bfc8e955e43fbe15cedcf329b48bc19",
            "paperId": "6af460d34bfc8e955e43fbe15cedcf329b48bc19",
            "title": "SAC3: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency",
            "abstract": "Hallucination detection is a critical step toward understanding the trustworthiness of modern language models (LMs). To achieve this goal, we re-examine existing detection approaches based on the self-consistency of LMs and uncover two types of hallucinations resulting from 1) question-level and 2) model-level, which cannot be effectively identified through self-consistency check alone. Building upon this discovery, we propose a novel sampling-based method, i.e., semantic-aware cross-check consistency (SAC3) that expands on the principle of self-consistency checking. Our SAC3 approach incorporates additional mechanisms to detect both question-level and model-level hallucinations by leveraging advances including semantically equivalent question perturbation and cross-model response consistency checking. Through extensive and systematic empirical analysis, we demonstrate that SAC3 outperforms the state of the art in detecting both non-factual and factual statements across multiple question-answering and open-domain generation benchmarks.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel sampling-based method, i.e., semantic-aware cross-check consistency (SAC3) that expands on the principle of self-consistency checking and demonstrates that SAC3 outperforms the state of the art in detecting both non-factual and factual statements across multiple question-answering and open-domain generation benchmarks."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to reduce hallucination in large language models by using a multi-step prompting approach called Context-Aware Verification Prompting, which involves generating an initial output, verifying its factual consistency, and revising it based on the verification feedback.\n\nThe paper proposes a method called SAC3 (semantic-aware cross-check consistency) to detect hallucinations in black-box language models by leveraging semantically equivalent question perturbation and cross-model response consistency checking.\n\nWhile both the project proposal and the paper address the issue of hallucination in language models, their approaches differ. The project proposal focuses on reducing hallucination through a prompting technique, while the paper proposes a method for detecting hallucinations in a black-box setting.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "9ffefdf1fcd780cb71450b0a7a29247c66aa87be",
            "paperId": "9ffefdf1fcd780cb71450b0a7a29247c66aa87be",
            "title": "The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning",
            "abstract": "Does prompting a large language model (LLM) like GPT-3 with explanations improve in-context learning? We study this question on two NLP tasks that involve reasoning over text, namely question answering and natural language inference. We test the performance of four LLMs on three textual reasoning datasets using prompts that include explanations in multiple different styles. For these tasks, we find that including explanations in the prompts for OPT, GPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to moderate accuracy improvements over standard few-show learning. However, text-davinci-002 is able to benefit more substantially. We further show that explanations generated by the LLMs may not entail the models' predictions nor be factually grounded in the input, even on simple tasks with extractive explanations. However, these flawed explanations can still be useful as a way to verify LLMs' predictions post-hoc. Through analysis in our three settings, we show that explanations judged by humans to be good--logically consistent with the input and the prediction--more likely cooccur with accurate predictions. Following these observations, we train calibrators using automatically extracted scores that assess the reliability of explanations, allowing us to improve performance post-hoc across all of our datasets.",
            "year": 2022,
            "citationCount": 95,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work studies two NLP tasks that involve reasoning over text, namely question answering and natural language inference, and shows that explanations judged by humans to be good--logically consistent with the input and the prediction--more likely cooccur with accurate predictions."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to reduce hallucination in large language models by using a multi-step prompting approach called Context-Aware Verification Prompting, which involves generating an initial output, verifying its factual consistency, and revising it based on the verification feedback.\n\nThe paper studies whether including explanations in prompts improves the performance of large language models on question answering and natural language inference tasks, and finds that the effectiveness varies across different models. It also explores the reliability of LLM-generated explanations and proposes using calibrators to assess their quality post-hoc.\n\nThe project proposal focuses on reducing hallucination through a novel prompting method, while the paper investigates the impact of including explanations in prompts on model performance and the reliability of LLM-generated explanations. Although both involve prompting, the research problems and approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "e811e771f5950d86eafe50655c0d1e5b571e19b6",
            "paperId": "e811e771f5950d86eafe50655c0d1e5b571e19b6",
            "title": "The Unreliability of Explanations in Few-Shot In-Context Learning",
            "abstract": "How can prompting a large language model like GPT-3 with explanations improve in-context learning? We focus speci\ufb01cally on two NLP tasks that involve reasoning over text, namely question answering and natural language inference. Including explanations in the prompt and having the model generate them does not consistently improve performance in the settings we study, contrary to recent results on symbolic reasoning tasks (Nye et al., 2021; Wei et al., 2022). Despite careful prompting, explanations generated by GPT-3 may not even be factually grounded in the input, even on simple tasks with straightforward extractive explanations. However, these \ufb02awed explanations can still be useful as a way to verify GPT-3\u2019s predictions post-hoc. Through analysis in three settings, we show that explanations judged as good by humans\u2014those that are logically consistent with the input and the prediction\u2014usually indicate more accurate predictions. Following these observations, we present a framework for calibrating model predictions based on the reliability of the explanations. Our framework trains calibrators using automatically extracted scores that approximately assess the reliability of explanations, which helps improve performance across three different datasets",
            "year": 2022,
            "citationCount": 29,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A framework for calibrating model predictions based on the reliability of explanations is presented, showing that explanations judged as good by humans\u2014those that are logically consistent with the input and the prediction\u2014usually indicate more accurate predictions."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to reduce hallucination in large language models by using a multi-step prompting approach called Context-Aware Verification Prompting, which involves generating an initial output, verifying its factual consistency, and revising it based on the verification feedback.\n\nThe paper focuses on investigating the reliability of explanations in few-shot in-context learning for question answering and natural language inference tasks, and proposes a framework for calibrating model predictions based on the reliability of the explanations.\n\nThe project proposal and the paper address different research problems and propose different approaches. The project proposal focuses on reducing hallucination in language models, while the paper studies the reliability of explanations in few-shot learning. The proposed methods are also different: multi-step prompting for the project proposal and a calibration framework based on explanation reliability for the paper.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "8a9ede4be8458629d54451d241a8d0ae318d471b",
            "paperId": "8a9ede4be8458629d54451d241a8d0ae318d471b",
            "title": "Prompting with Divide-and-Conquer Program Makes Large Language Models Discerning to Hallucination and Deception",
            "abstract": "Foundation models, such as Large language Models (LLMs), have attracted significant amount of interest due to their large number of applications. Existing works show that appropriate prompt design, such as Chain-of-Thoughts, can unlock LLM's powerful capacity in diverse areas. However, when handling tasks involving repetitive sub-tasks and/or deceptive contents, such as arithmetic calculation and article-level fake news detection, existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination. To make LLM more discerning to such intermediate errors, we propose to guide LLM with a Divide-and-Conquer program that simultaneously ensures superior expressive power and disentangles task decomposition, sub-task resolution, and resolution assembly process. Theoretic analysis reveals that our strategy can guide LLM to extend the expressive power of fixed-depth Transformer. Experiments indicate that our proposed method can achieve better performance than typical prompting strategies in tasks bothered by intermediate errors and deceptive contents, such as large integer multiplication, hallucination detection and misinformation detection.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes to guide LLM with a Divide-and-Conquer program that simultaneously ensures superior expressive power and disentangles task decomposition, sub-task resolution, and resolution assembly process andoretic analysis reveals that this strategy can guide LLM to extend the expressive power of fixed-depth Transformer."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to reduce hallucination in large language models by using a multi-step prompting approach called Context-Aware Verification Prompting, which involves generating an initial output, verifying its factual consistency, and revising it based on the verification feedback.\n\nThe paper proposes a Divide-and-Conquer program to guide large language models in handling tasks involving repetitive sub-tasks and deceptive content, such as arithmetic calculation and fake news detection, to make the models more discerning to intermediate errors and hallucination.\n\nWhile both the project proposal and the paper address the issue of hallucination in large language models, their approaches differ. The project proposal focuses on a multi-step prompting method for reducing hallucination, while the paper proposes a Divide-and-Conquer program to guide the models in handling specific types of tasks.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "7c1707db9aafd209aa93db3251e7ebd593d55876",
            "paperId": "7c1707db9aafd209aa93db3251e7ebd593d55876",
            "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
            "abstract": "Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose\"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods.",
            "year": 2023,
            "citationCount": 155,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes \"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database, and demonstrates that it can detect non-factual and factual sentences and rank passages in terms of factuality."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to reduce hallucination in large language models by using a multi-step prompting approach called Context-Aware Verification Prompting, which involves generating an initial output, verifying its factual consistency, and revising it based on the verification feedback.\n\nThe paper proposes SelfCheckGPT, a sampling-based approach to detect hallucination in black-box language models without using external databases. It relies on the idea that sampled responses for known facts will be consistent, while those for hallucinated facts will diverge and contradict each other.\n\nWhile both the project proposal and the paper address the problem of hallucination in large language models, their approaches differ significantly. The project proposal uses a prompting-based method to encourage the model to verify and revise its own output, while the paper uses a sampling-based approach to detect hallucination by comparing multiple generated responses.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "cd2e04598909158494e556823d9de8baa692cee2",
            "paperId": "cd2e04598909158494e556823d9de8baa692cee2",
            "title": "Towards Mitigating Hallucination in Large Language Models via Self-Reflection",
            "abstract": "Large language models (LLMs) have shown promise for generative and knowledge-intensive tasks including question-answering (QA) tasks. However, the practical deployment still faces challenges, notably the issue of\"hallucination\", where models generate plausible-sounding but unfaithful or nonsensical information. This issue becomes particularly critical in the medical domain due to the uncommon professional concepts and potential social risks involved. This paper analyses the phenomenon of hallucination in medical generative QA systems using widely adopted LLMs and datasets. Our investigation centers on the identification and comprehension of common problematic answers, with a specific emphasis on hallucination. To tackle this challenge, we present an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation. Through this feedback process, our approach steadily enhances the factuality, consistency, and entailment of the generated answers. Consequently, we harness the interactivity and multitasking ability of LLMs and produce progressively more precise and accurate answers. Experimental results on both automatic and human evaluation demonstrate the superiority of our approach in hallucination reduction compared to baselines.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper analyses the phenomenon of hallucination in medical generative QA systems using widely adopted LLMs and datasets and presents an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation that steadily enhances the factuality, consistency, and entailment of the generated answers."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to reduce hallucination in large language models by using a multi-step prompting approach called Context-Aware Verification Prompting, which involves generating an initial output, verifying its factual consistency, and revising it based on the verification feedback.\n\nThe paper proposes an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation to mitigate hallucination in medical generative question-answering systems using large language models.\n\nWhile both the project proposal and the paper address the problem of hallucination in large language models, their approaches differ. The project proposal focuses on a prompting-based method, while the paper proposes an interactive self-reflection methodology specific to the medical domain.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
            "paperId": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
            "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such\"society of minds\"approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.",
            "year": 2023,
            "citationCount": 206,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer is presented, indicating that this approach significantly enhances mathematical and strategic reasoning across a number of tasks."
            },
            "score": 6
        },
        {
            "id": "56373d3fd0f1354a61f9e577db039cdb187d8d43",
            "paperId": "56373d3fd0f1354a61f9e577db039cdb187d8d43",
            "title": "Evaluating the Factual Consistency of Large Language Models Through News Summarization",
            "abstract": "While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information. To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB(Factual Inconsistency Benchmark) that focuses on the task of summarization. Specifically, our benchmark involves comparing the scores an LLM assigns to a factually consistent versus a factually inconsistent summary for an input news article. For factually consistent summaries, we use human-written reference summaries that we manually verify as factually consistent. To generate summaries that are factually inconsistent, we generate summaries from a suite of summarization models that we have manually annotated as factually inconsistent. A model's factual consistency is then measured according to its accuracy, i.e.\\ the proportion of documents where it assigns a higher score to the factually consistent summary. To validate the usefulness of FIB, we evaluate 23 large language models ranging from 1B to 176B parameters from six different model families including BLOOM and OPT. We find that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries. However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries. We validate design choices in our benchmark including the scoring method and source of distractor summaries. Our code and benchmark data can be found at https://github.com/r-three/fib.",
            "year": 2022,
            "citationCount": 51,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new benchmark called FIB(Factual Inconsistency Benchmark) that focuses on the task of summarization, and finds that existing LLMs generally assign a higher score to factually consistent summaries than tofactually inconsistent summaries."
            },
            "score": 6
        },
        {
            "id": "f33d19614d78f4a6e107e768ccd0a2d4244fc89a",
            "paperId": "f33d19614d78f4a6e107e768ccd0a2d4244fc89a",
            "title": "Evaluating the Factual Consistency of Large Language Models Through Summarization",
            "abstract": "While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information. To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB ( F actual I nconsistency B enchmark) that focuses on the task of summarization. Specifically, our benchmark involves comparing the scores an LLM assigns to a factually consistent versus a factually inconsistent summary for an input news article. For factually consistent summaries, we use human-written reference summaries that we manually verify as fac-tually consistent. To generate summaries that are factually inconsistent, we generate summaries from a suite of summarization models that we have manually annotated as fac-tually inconsistent. A model\u2019s factual consistency is then measured according to its accuracy, i.e. the proportion of documents where it assigns a higher score to the factually consistent summary. To validate the usefulness of FIB , we evaluate 23 large language models ranging from 1B to 176B parameters from six different model families including BLOOM and OPT. We \ufb01nd that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries. However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries. We validate design choices in our benchmark including the scoring method and source of distractor summaries. Our code and",
            "year": 2022,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new benchmark called FIB is proposed that focuses on the task of summarization and shows that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries, and that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries."
            },
            "score": 6
        },
        {
            "id": "f3cd3a0e1a0a29193fd88178ef75946dfe756622",
            "paperId": "f3cd3a0e1a0a29193fd88178ef75946dfe756622",
            "title": "Factual Consistency Evaluation of Summarisation in the Era of Large Language Models",
            "abstract": "Factual inconsistency with source documents in automatically generated summaries can lead to misinformation or pose risks. Existing factual consistency(FC) metrics are constrained by their performance, efficiency, and explainability. Recent advances in Large language models (LLMs) have demonstrated remarkable potential in text evaluation but their effectiveness in assessing FC in summarisation remains underexplored. Prior research has mostly focused on proprietary LLMs, leaving essential factors that affect their assessment capabilities unexplored. Additionally, current FC evaluation benchmarks are restricted to news articles, casting doubt on the generality of the FC methods tested on them. In this paper, we first address the gap by introducing TreatFact a dataset of LLM-generated summaries of clinical texts, annotated for FC by domain experts. Moreover, we benchmark 11 LLMs for FC evaluation across news and clinical domains and analyse the impact of model size, prompts, pre-training and fine-tuning data. Our findings reveal that despite proprietary models prevailing on the task, open-source LLMs lag behind. Nevertheless, there is potential for enhancing the performance of open-source LLMs through increasing model size, expanding pre-training data, and developing well-curated fine-tuning data. Experiments on TreatFact suggest that both previous methods and LLM-based evaluators are unable to capture factual inconsistencies in clinical summaries, posing a new challenge for FC evaluation.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "TreatFact, a dataset of LLM-generated summaries of clinical texts, annotated for FC by domain experts, is introduced and it is revealed that despite proprietary models prevailing on the task, open-source LLMs lag behind."
            },
            "score": 6
        },
        {
            "id": "45653ad43124f02dc2cf2db3357be1d1d78ddb18",
            "paperId": "45653ad43124f02dc2cf2db3357be1d1d78ddb18",
            "title": "Language Models Hallucinate, but May Excel at Fact Verification",
            "abstract": "Recent progress in natural language processing (NLP) owes much to remarkable advances in large language models (LLMs). Nevertheless, LLMs frequently\"hallucinate,\"resulting in non-factual outputs. Our carefully-designed human evaluation substantiates the serious hallucination issue, revealing that even GPT-3.5 produces factual outputs less than 25% of the time. This underscores the importance of fact verifiers in order to measure and incentivize progress. Our systematic investigation affirms that LLMs can be repurposed as effective fact verifiers with strong correlations with human judgments. Surprisingly, FLAN-T5-11B, the least factual generator in our study, performs the best as a fact verifier, even outperforming more capable LLMs like GPT3.5 and ChatGPT. Delving deeper, we analyze the reliance of these LLMs on high-quality evidence, as well as their deficiencies in robustness and generalization ability. Our study presents insights for developing trustworthy generation models.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Surprisingly, FLAN-T5-11B, the least factual generator in this study, performs the best as a fact verifier, even outperforming more capable LLMs like GPT3.5 and ChatGPT."
            },
            "score": 6
        },
        {
            "id": "2aba5bba16dac5cd62683bab9de5d6faaaed0de1",
            "paperId": "2aba5bba16dac5cd62683bab9de5d6faaaed0de1",
            "title": "Shepherd Pre-trained Language Models to Develop a Train of Thought: An Iterative Prompting Approach",
            "abstract": "While Pre-trained Language Models (PLMs) 001 internalize a great amount of world knowledge, 002 they have been shown incapable of recalling 003 these knowledge to solve tasks requiring com-004 plex & multi-step inference procedures. Simi-005 lar to how humans develop a \u201ctrain of thought\u201d 006 for these tasks, how can we equip PLMs with 007 such abilities? In this work, we explore an iter-008 ative prompting framework, a new prompting 009 paradigm which progressively elicits relevant 010 knowledge from PLMs for multi-step inference 011 tasks. We identify key limitations of existing 012 prompting methods, namely they are either re-013 stricted to queries with a single identifiable re-014 lation/predicate, or being agnostic to input con-015 texts, which makes it difficult to capture vari-016 abilities across different inference steps. We 017 propose an iterative context-aware prompter, 018 which addresses these limitations by learning 019 to dynamically synthesize prompts conditioned 020 on the current step\u2019s contexts. Experiments on 021 three datasets involving multi-step inference 022 show the effectiveness of the iterative scheme 023 and the context-aware prompter design. 1 024",
            "year": 2022,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores an iter-008 ative prompting framework, a new prompting 009 paradigm which progressively elicits relevant 010 knowledge from PLMs for multi-step inference 011 tasks, and proposes an iterative context-aware prompter design which addresses key limitations of existing prompting methods."
            },
            "score": 6
        },
        {
            "id": "3f4d11971f2c64be9125a7fe99c019588bbebf16",
            "paperId": "3f4d11971f2c64be9125a7fe99c019588bbebf16",
            "title": "Iteratively Prompt Pre-trained Language Models for Chain of Thought",
            "abstract": "While Pre-trained Language Models (PLMs) internalize a great amount of world knowledge, they have been shown incapable of recalling these knowledge to solve tasks requiring complex & multi-step reasoning. Similar to how humans develop a \u201cchain of thought\u201d for these tasks, how can we equip PLMs with such abilities? In this work, we explore an iterative prompting framework, a new prompting paradigm which progressively elicits relevant knowledge from PLMs for multi-step inference. We identify key limitations of existing prompting methods, namely they are either restricted to queries with a single identifiable relation/predicate, or being agnostic to input contexts, which makes it difficult to capture variabilities across different inference steps. We propose an iterative context-aware prompter, which addresses these limitations by learning to dynamically synthesize prompts conditioned on the current step\u2019s contexts. Experiments on three datasets involving multi-step reasoning show the effectiveness of the iterative scheme and the context-aware prompter design.",
            "year": 2022,
            "citationCount": 58,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An iterative prompting framework is explored, a new prompting paradigm which progressively elicits relevant knowledge from PLMs for multi-step inference by learning to dynamically synthesize prompts conditioned on the current step\u2019s contexts."
            },
            "score": 6
        },
        {
            "id": "9df13cace8a02644847841cfdd01d9fb209a790c",
            "paperId": "9df13cace8a02644847841cfdd01d9fb209a790c",
            "title": "Context-Aware Prompt: Customize A Unique Prompt For Each Input",
            "abstract": "After the proposal of BERT, pre-trained lan001 guage models have become the dominant ap002 proach for solving many NLP tasks. Typically, 003 a linear classifier is added to the head of the 004 model for fine-tuning to fit downstream tasks, 005 while a more recent approach, also known as 006 prompt-based learning or prompt-learning, us007 ing prompts to perform various downstream 008 tasks, is considered to be able to uncover the 009 potential of the language model. 010 Prior study, however, attempted to find a uni011 versal prompt for a certain task across all sam012 ples. Therefore, we propose a novel method, 013 Context-Aware Prompt (CAP), which provides 014 a unique continuous prompt for each sample 015 input by combining contextual information to 016 further investigate the potential capabilities of 017 the language models. On the SuperGlue bench018 mark, our method outperforms multiple models 019 with vanilla fine-tuning. Furthermore, we ex020 tend the use of prompts to include Replaced 021 Token Detection (RTD) type prompts, allow022 ing models like ELECTRA and DeBERTaV3 023 that employ RTD as a training objective to use 024 prompts for downstream tasks.1 025",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel method, 013 Context-Aware Prompt (CAP), which provides 014 a unique continuous prompt for each sample sample input by combining contextual information to further investigate the potential capabilities of 017 the language models."
            },
            "score": 6
        },
        {
            "id": "a2e1039157257a8fae038259d1642536e1641b3e",
            "paperId": "a2e1039157257a8fae038259d1642536e1641b3e",
            "title": "Se2: Sequential Example Selection for In-Context Learning",
            "abstract": "The remarkable capability of large language models (LLMs) for in-context learning (ICL) needs to be activated by demonstration examples. Prior work has extensively explored the selection of examples for ICL, predominantly following the\"select then organize\"paradigm, such approaches often neglect the internal relationships between examples and exist an inconsistency between the training and inference. In this paper, we formulate the problem as a $\\textit{se}$quential $\\textit{se}$lection problem and introduce $Se^2$, a sequential-aware method that leverages the LLM's feedback on varying context, aiding in capturing inter-relationships and sequential information among examples, significantly enriching the contextuality and relevance of ICL prompts. Meanwhile, we utilize beam search to seek and construct example sequences, enhancing both quality and diversity. Extensive experiments across 23 NLP tasks from 8 distinct categories illustrate that $Se^2$ markedly surpasses competitive baselines and achieves 42% relative improvement over random selection. Further in-depth analysis show the effectiveness of proposed strategies, highlighting $Se^2$'s exceptional stability and adaptability across various scenarios. Our code will be released to facilitate future research.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces $Se^2$, a sequential-aware method that leverages the LLM's feedback on varying context, aiding in capturing inter-relationships and sequential information among examples, significantly enriching the contextuality and relevance of ICL prompts."
            },
            "score": 6
        },
        {
            "id": "51a2a13266f3284c3f1f0c829f86451d6cd6f4b3",
            "paperId": "51a2a13266f3284c3f1f0c829f86451d6cd6f4b3",
            "title": "In-context Prompt Learning for Test-time Vision Recognition with Frozen Vision-language Model",
            "abstract": "Existing pre-trained vision-language models, e.g., CLIP, have demonstrated impressive zero-shot generalization capabilities in various downstream tasks. However, the performance of these models will degrade significantly when test inputs present different distributions. To this end, we explore the concept of test-time prompt tuning (TTPT), which enables the adaptation of the CLIP model to novel downstream tasks through only one step of optimization on an unsupervised objective that involves the test sample. Motivated by in-context learning within field of natural language processing (NLP), we propose In-Context Prompt Learning (InCPL) for test-time visual recognition task. InCPL involves associating a new test sample with very few or even just one labeled example as its in-context prompt. As a result, it can reliably estimate a label for the test sample, thereby facilitating the model adaptation process. InCPL first employs a token net to represent language descriptions as visual prompts that the vision encoder of a CLIP model can comprehend. Paired with in-context examples, we further propose a context-aware unsupervised loss to optimize test sample-aware visual prompts. This optimization allows a pre-trained, frozen CLIP model to be adapted to a test sample from any task using its learned adaptive prompt. Our method has demonstrated superior performance and achieved state-of-the-art results across various downstream datasets.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The concept of test-time prompt tuning (TTPT) is explored, which enables the adaptation of the CLIP model to novel downstream tasks through only one step of optimization on an unsupervised objective that involves the test sample."
            },
            "score": 6
        },
        {
            "id": "261549439aebdda72b648ecc462448fd24857ac1",
            "paperId": "261549439aebdda72b648ecc462448fd24857ac1",
            "title": "Progressive-Hint Prompting Improves Reasoning in Large Language Models",
            "abstract": "The performance of Large Language Models (LLMs) in reasoning tasks depends heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency being critical methods that enhance this ability. However, these methods do not fully exploit the answers generated by the LLM to guide subsequent responses. This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers. PHP is orthogonal to CoT and self-consistency, making it easy to combine with state-of-the-art techniques to further improve performance. We conducted extensive and comprehensive experiments on seven benchmarks. The results show that PHP significantly improves accuracy while remaining highly efficient. For instance, with text-davinci-003, we observed a 4.2% improvement on GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction in sample paths with self-consistency. With GPT-4 and PHP, we achieve state-of-the-art performances on SVAMP (89.1% ->91.9%), GSM8K (92% ->95.5%), AQuA (76.4% ->79.9%) and MATH (50.3% ->53.9%).",
            "year": 2023,
            "citationCount": 64,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers."
            },
            "score": 5
        },
        {
            "id": "44d74b0d77b4056ddd4c6611a76711c8bab2e0a7",
            "paperId": "44d74b0d77b4056ddd4c6611a76711c8bab2e0a7",
            "title": "Dehallucinating Large Language Models Using Formal Methods Guided Iterative Prompting",
            "abstract": "Large language models (LLMs) such as ChatGPT have been trained to generate human-like responses to natural language prompts. LLMs use a vast corpus of text data for training, and can generate coherent and contextually relevant responses to a wide range of questions and statements. Despite this remarkable progress, LLMs are prone to hallucinations making their application to safety-critical applications such as autonomous systems difficult. The hallucinations in LLMs refer to instances where the model generates responses that are not factually accurate or contextually appropriate. These hallucinations can occur due to a variety of factors, such as the model\u2019s lack of real-world knowledge, the influence of biased or inaccurate training data, or the model\u2019s tendency to generate responses based on statistical patterns rather than a true understanding of the input. While these hallucinations are a nuisance in tasks such as text summarization and question-answering, they can be catastrophic when LLMs are used in autonomy-relevant applications such as planning. In this paper, we focus on the application of LLMs in autonomous systems and sketch a novel self-monitoring and iterative prompting architecture that uses formal methods to detect these errors in the LLM response automatically. We exploit the dialog capability of LLMs to iteratively steer them to responses that are consistent with our correctness specification. We report preliminary experiments that show the promise of the proposed approach on tasks such as automated planning.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper sketches a novel self-monitoring and iterative prompting architecture that uses formal methods to detect errors in the LLM response automatically and exploits the dialog capability of LLMs to iteratively steer them to responses that are consistent with the correctness specification."
            },
            "score": 5
        },
        {
            "id": "eee548fbd0b9dd954c692fbd8880e80d5f077bd7",
            "paperId": "eee548fbd0b9dd954c692fbd8880e80d5f077bd7",
            "title": "Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models",
            "abstract": "Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP). Although convenient for research and practical applications, open-source LLMs with fewer parameters often suffer from severe hallucinations compared to their larger counterparts. This paper focuses on measuring and reducing hallucinations in BLOOM 7B, a representative of such weaker open-source LLMs that are publicly available for research and commercial applications. We introduce HaloCheck, a lightweight BlackBox knowledge-free framework designed to quantify the severity of hallucinations in LLMs. Additionally, we explore techniques like knowledge injection and teacher-student approaches to alleviate hallucinations in low-parameter LLMs. Our experiments effectively demonstrate the reduction of hallucinations in challenging domains for these LLMs.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "HaloCheck is introduced, a lightweight BlackBox knowledge-free framework designed to quantify the severity of hallucinations in LLMs, and techniques like knowledge injection and teacher-student approaches to alleviate hallucinations in low-parameter LLMs are explored."
            },
            "score": 5
        },
        {
            "id": "d2d16333a4b0dc7e3463b280b9945e5ee6c53396",
            "paperId": "d2d16333a4b0dc7e3463b280b9945e5ee6c53396",
            "title": "TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models",
            "abstract": "Factual consistency evaluation is often conducted using Natural Language Inference (NLI) models, yet these models exhibit limited success in evaluating summaries. Previous work improved such models with synthetic training data. However, the data is typically based on perturbed human-written summaries, which often differ in their characteristics from real model-generated summaries and have limited coverage of possible factual errors. Alternatively, large language models (LLMs) have recently shown promising results in directly evaluating generative tasks, but are too computationally expensive for practical use. Motivated by these limitations, we introduce TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM. Unlike prior work, TrueTeacher does not rely on human-written summaries, and is multilingual by nature. Experiments on the TRUE benchmark show that a student model trained using our data, substantially outperforms both the state-of-the-art model with similar capacity, and the LLM teacher. In a systematic study, we compare TrueTeacher to existing synthetic data generation methods and demonstrate its superiority and robustness to domain-shift. We also show that our method generalizes to multilingual scenarios. Lastly, we release our large scale synthetic dataset (1.4M examples), generated using TrueTeacher, and a checkpoint trained on this data.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM, which does not rely on human-written summaries, and is multilingual by nature."
            },
            "score": 5
        },
        {
            "id": "d998ff93b2e58c9b219a9103f8c2ad714a41e4b9",
            "paperId": "d998ff93b2e58c9b219a9103f8c2ad714a41e4b9",
            "title": "The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency of Language Models",
            "abstract": "Large Language Models (LLMs) make natural interfaces to factual knowledge, but their usefulness is limited by their tendency to deliver inconsistent answers to semantically equivalent questions. For example, a model might predict both\"Anne Redpath passed away in Edinburgh.\"and\"Anne Redpath's life ended in London.\"In this work, we identify potential causes of inconsistency and evaluate the effectiveness of two mitigation strategies: up-scaling and augmenting the LM with a retrieval corpus. Our results on the LLaMA and Atlas models show that both strategies reduce inconsistency while retrieval augmentation is considerably more efficient. We further consider and disentangle the consistency contributions of different components of Atlas. For all LMs evaluated we find that syntactical form and other evaluation task artifacts impact consistency. Taken together, our results provide a better understanding of the factors affecting the factual consistency of language models.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work identifies potential causes of inconsistency and evaluates the effectiveness of two mitigation strategies: up-scaling and augmenting the LM with a retrieval corpus, showing that both strategies reduce inconsistency while retrieval augmentation is considerably more efficient."
            },
            "score": 5
        },
        {
            "id": "f727f928e7e179307d8d4a1da2387393f2bd7915",
            "paperId": "f727f928e7e179307d8d4a1da2387393f2bd7915",
            "title": "Methods for Measuring, Updating, and Visualizing Factual Beliefs in Language Models",
            "abstract": "Language models can memorize a considerable amount of factual information during pretraining that can be elicited through prompting or finetuning models on tasks like question answering. In this paper, we discuss approaches to measuring model factual beliefs, updating incorrect factual beliefs in models, and visualizing graphical relationships between factual beliefs. Our main contributions include: (1) new metrics for evaluating belief-updating methods focusing on the logical consistency of beliefs, (2) a training objective for Sequential, Local, and Generalizing updates (SLAG) that improves the performance of existing hypernetwork approaches, and (3) the introduction of the belief graph, a new form of visualization for language models that shows relationships between stored model beliefs. Our experiments suggest that models show only limited consistency between factual beliefs, but update methods can both fix incorrect model beliefs and greatly improve their consistency. Although off-the-shelf optimizers are surprisingly strong belief-updating baselines, our learned optimizers can outperform them in more difficult settings than have been considered in past work.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experiments suggest that models show only limited consistency between factual beliefs, but update methods can both fix incorrect model beliefs and greatly improve their consistency, and off-the-shelf optimizers can outperform them in more difficult settings than have been considered in past work."
            },
            "score": 5
        },
        {
            "id": "27cb586fcea5ec076b984750e9c77f0d7fc976e5",
            "paperId": "27cb586fcea5ec076b984750e9c77f0d7fc976e5",
            "title": "AlignScore: Evaluating Factual Consistency with A Unified Alignment Function",
            "abstract": "Many text generation applications require the generated text to be factually consistent with input information. Automatic evaluation of factual consistency is challenging. Previous work has developed various metrics that often depend on specific functions, such as natural language inference (NLI) or question answering (QA), trained on limited data. Those metrics thus can hardly assess diverse factual inconsistencies (e.g., contradictions, hallucinations) that occur in varying inputs/outputs (e.g., sentences, documents) from different tasks. In this paper, we propose AlignScore, a new holistic metric that applies to a variety of factual inconsistency scenarios as above. AlignScore is based on a general function of information alignment between two arbitrary text pieces. Crucially, we develop a unified training framework of the alignment function by integrating a large diversity of data sources, resulting in 4.7M training examples from 7 well-established tasks (NLI, QA, paraphrasing, fact verification, information retrieval, semantic similarity, and summarization). We conduct extensive experiments on large-scale benchmarks including 22 evaluation datasets, where 19 of the datasets were never seen in the alignment training. AlignScore achieves substantial improvement over a wide range of previous metrics. Moreover, AlignScore (355M parameters) matches or even outperforms metrics based on ChatGPT and GPT-4 that are orders of magnitude larger.",
            "year": 2023,
            "citationCount": 37,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "AlignScore, a new holistic metric that applies to a variety of factual inconsistency scenarios, is proposed, based on a general function of information alignment between two arbitrary text pieces that achieves substantial improvement over a wide range of previous metrics."
            },
            "score": 5
        },
        {
            "id": "0d22f06a1f5ad9f62b2f35c126b514f927586c85",
            "paperId": "0d22f06a1f5ad9f62b2f35c126b514f927586c85",
            "title": "Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency",
            "abstract": "Large language models (LLMs) have exhibited remarkable ability in code generation. However, generating the correct solution in a single attempt still remains a challenge. Prior works utilize verification properties in software engineering to verify and re-rank solutions in a majority voting manner. But the assumption behind them that generated verification properties have better qualities than solutions may not always hold. In this paper, we treat them equally as different perspectives of LLMs' reasoning processes. We propose the Multi-Perspective Self-Consistency (MPSC) framework incorporating both inter- and intra-consistency across outputs from multiple perspectives. Specifically, we prompt LLMs to generate diverse outputs from three perspectives, Solution, Specification and Test case, constructing a 3-partite graph. With two measure functions of consistency, we embed both inter- and intra-consistency information into the graph. The optimal choice of solutions is then determined based on analysis in the graph. MPSC significantly boosts performance of foundation models (ChatGPT in this paper) on various benchmarks, including HumanEval (+15.91%), MBPP (+6.43%) and CodeContests (+9.37%), even surpassing GPT-4.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Multi-Perspective Self-Consistency (MPSC) framework is proposed incorporating both inter- and intra-consistency across outputs from multiple perspectives and significantly boosts performance of foundation models on various benchmarks, including HumanEval, MBPP and CodeContests."
            },
            "score": 5
        },
        {
            "id": "dfea29ea51bcc6dece773d4054e6f7658cb33bf8",
            "paperId": "dfea29ea51bcc6dece773d4054e6f7658cb33bf8",
            "title": "Context Aware Query Rewriting for Text Rankers using LLM",
            "abstract": "Query rewriting refers to an established family of approaches that are applied to underspecified and ambiguous queries to overcome the vocabulary mismatch problem in document ranking. Queries are typically rewritten during query processing time for better query modelling for the downstream ranker. With the advent of large-language models (LLMs), there have been initial investigations into using generative approaches to generate pseudo documents to tackle this inherent vocabulary gap. In this work, we analyze the utility of LLMs for improved query rewriting for text ranking tasks. We find that there are two inherent limitations of using LLMs as query re-writers -- concept drift when using only queries as prompts and large inference costs during query processing. We adopt a simple, yet surprisingly effective, approach called context aware query rewriting (CAR) to leverage the benefits of LLMs for query understanding. Firstly, we rewrite ambiguous training queries by context-aware prompting of LLMs, where we use only relevant documents as context.Unlike existing approaches, we use LLM-based query rewriting only during the training phase. Eventually, a ranker is fine-tuned on the rewritten queries instead of the original queries during training. In our extensive experiments, we find that fine-tuning a ranker using re-written queries offers a significant improvement of up to 33% on the passage ranking task and up to 28% on the document ranking task when compared to the baseline performance of using original queries.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work adopts a simple, yet surprisingly effective, approach called context aware query rewriting (CAR) to leverage the benefits of LLMs for query understanding and finds that fine-tuning a ranker using re-written queries offers a significant improvement on the passage ranking task and the document ranking task when compared to the baseline performance of using original queries."
            },
            "score": 5
        },
        {
            "id": "4782872d91ea69e7f36c13fbcefbb88a76fbe531",
            "paperId": "4782872d91ea69e7f36c13fbcefbb88a76fbe531",
            "title": "TREACLE: Thrifty Reasoning via Context-Aware LLM and Prompt Selection",
            "abstract": "Recent successes in natural language processing have led to the proliferation of large language models (LLMs) by multiple providers. Each LLM offering has different inference accuracy, monetary cost, and latency, and their accuracy further depends on the exact wording of the question (i.e., the specific prompt). At the same time, users often have a limit on monetary budget and latency to answer all their questions, and they do not know which LLMs to choose for each question to meet their accuracy and long-term budget requirements. To navigate this rich design space, we propose TREACLE (Thrifty Reasoning via Context-Aware LLM and Prompt Selection), a reinforcement learning policy that jointly selects the model and prompting scheme while respecting the user's monetary cost and latency constraints. TREACLE uses the problem context, including question text embeddings (reflecting the type or difficulty of a query) and the response history (reflecting the consistency of previous responses) to make smart decisions. Our evaluations on standard reasoning datasets (GSM8K, CSQA, and LLC ) with various LLMs and prompts show that TREACLE enables cost savings of up to 85% compared to baselines while maintaining high accuracy. Importantly, it provides the user with the ability to gracefully trade off accuracy for cost.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "TREACLE (Thrifty Reasoning via Context-Aware LLM and Prompt Selection), a reinforcement learning policy that jointly selects the model and prompting scheme while respecting the user's monetary cost and latency constraints, is proposed."
            },
            "score": 5
        },
        {
            "id": "967475d9f00bf7d7529384931896f3501f9daf0d",
            "paperId": "967475d9f00bf7d7529384931896f3501f9daf0d",
            "title": "ReActIn: Infusing Human Feedback into Intermediate Prompting Steps of Large Language Model",
            "abstract": "This paper introduces ReActIn, a framework designed to infuse human feedback into the intermediate prompting steps of large language models. The practicality and effectiveness of ReActIn are validated through experiments that apply four established prompting strategies, evaluated both with and without human feedback integration. The proposed architecture's performance is compared against traditional large language models across various tasks using four standard evaluation tests. Our findings reveal that the integration of human feedback has a direct impact on the reasoning, action prompting, and overall decision-making capabilities of the language models. This study underscores the potential of ReActIn to shape a future where sophisticated, context-aware AI systems, empowered by human feedback, can effectively navigate complex real-world scenarios.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study underscores the potential of ReActIn to shape a future where sophisticated, context-aware AI systems, empowered by human feedback, can effectively navigate complex real-world scenarios."
            },
            "score": 5
        },
        {
            "id": "03532123ccffae8d411264320e8a5ae2b6eddea0",
            "paperId": "03532123ccffae8d411264320e8a5ae2b6eddea0",
            "title": "Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP",
            "abstract": "Retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). Existing work has combined these in simple\"retrieve-then-read\"pipelines in which the RM retrieves passages that are inserted into the LM prompt. To begin to fully realize the potential of frozen LMs and RMs, we propose Demonstrate-Search-Predict (DSP), a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LM and RM can handle more reliably. We have written novel DSP programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art in-context learning results and delivering 37-120%, 8-39%, and 80-290% relative gains against the vanilla LM (GPT-3.5), a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline, respectively. We release DSP at https://github.com/stanfordnlp/dsp",
            "year": 2022,
            "citationCount": 133,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Demonstrate-Search-Predict (DSP) is proposed, a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM and can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions."
            },
            "score": 5
        },
        {
            "id": "25df7f3f539e6741ff66438f81452f14f5fb9288",
            "paperId": "25df7f3f539e6741ff66438f81452f14f5fb9288",
            "title": "Knowledge-Aware Prompt Tuning for Generalizable Vision-Language Models",
            "abstract": "Pre-trained vision-language models, e.g., CLIP, working with manually designed prompts have demonstrated great capacity of transfer learning. Recently, learnable prompts achieve state-of-the-art performance, which however are prone to overfit to seen classes, failing to generalize to unseen classes. In this paper, we propose a Knowledge-Aware Prompt Tuning (KAPT) framework for vision-language models. Our approach takes the inspiration from human intelligence in which external knowledge is usually incorporated into recognizing novel categories of objects. Specifically, we design two complementary types of knowledge-aware prompts for the text encoder to leverage the distinctive characteristics of category-related external knowledge. The discrete prompt extracts the key information from descriptions of an object category, and the learned continuous prompt captures overall contexts. We further design an adaptation head for the visual encoder to aggregate salient attentive visual cues, which establishes discriminative and task-aware visual representations. We conduct extensive experiments on 11 widely-used benchmark datasets and the results verify the effectiveness in few-shot image classification, especially in generalizing to unseen categories. Compared with the state-of-the-art CoCoOp method, KAPT exhibits favorable performance and achieves an absolute gain of 3.22% on new classes and 2.57% in terms of harmonic mean.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper designs two complementary types of knowledge-aware prompts for the text encoder to leverage the distinctive characteristics of category-related external knowledge and designs an adaptation head for the visual encoder to aggregate salient attentive visual cues, which establishes discriminative and task-aware visual representations."
            },
            "score": 5
        },
        {
            "id": "e7c97e953849f1a8e5d85ceb4cfcc0a5d54d2365",
            "paperId": "e7c97e953849f1a8e5d85ceb4cfcc0a5d54d2365",
            "title": "Enabling Large Language Models to Generate Text with Citations",
            "abstract": "Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. In this work, our aim is to allow LLMs to generate text with citations, improving their factual correctness and verifiability. Existing work mainly relies on commercial search engines and human evaluation, making it challenging to reproduce and compare different modeling approaches. We propose ALCE, the first benchmark for Automatic LLMs' Citation Evaluation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations. We develop automatic metrics along three dimensions -- fluency, correctness, and citation quality -- and demonstrate their strong correlation with human judgements. Our experiments with state-of-the-art LLMs and novel prompting strategies show that current systems have considerable room for improvement -- For example, on the ELI5 dataset, even the best models lack complete citation support 50% of the time. Our analyses further highlight promising future directions, including developing better retrievers, advancing long-context LLMs, and improving the ability to synthesize information from multiple sources.",
            "year": 2023,
            "citationCount": 102,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes ALCE, the first benchmark for Automatic LLMs' Citation Evaluation, and develops automatic metrics along three dimensions -- fluency, correctness, and citation quality -- and demonstrates their strong correlation with human judgements."
            },
            "score": 4
        },
        {
            "id": "ca261cb681b082e90ca6c7a9d325b4265ed1dc28",
            "paperId": "ca261cb681b082e90ca6c7a9d325b4265ed1dc28",
            "title": "MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models",
            "abstract": "Large language models (LLMs) have achieved remarkable performance in natural language understanding and generation tasks. However, they often suffer from limitations such as difficulty in incorporating new knowledge, generating hallucinations, and explaining their reasoning process. To address these challenges, we propose a novel prompting pipeline, named \\method, that leverages knowledge graphs (KGs) to enhance LLMs' inference and transparency. Our method enables LLMs to comprehend KG inputs and infer with a combination of implicit and external knowledge. Moreover, our method elicits the mind map of LLMs, which reveals their reasoning pathways based on the ontology of knowledge. We evaluate our method on diverse question \\&answering tasks, especially in medical domains, and show significant improvements over baselines. We also introduce a new hallucination evaluation benchmark and analyze the effects of different components of our method. Our results demonstrate the effectiveness and robustness of our method in merging knowledge from LLMs and KGs for combined inference. To reproduce our results and extend the framework further, we make our codebase available at https://github.com/wyl-willing/MindMap.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel prompting pipeline is proposed that leverages knowledge graphs (KGs) to enhance LLMs' inference and transparency and elicits the mind map of LLMs, which reveals their reasoning pathways based on the ontology of knowledge."
            },
            "score": 4
        },
        {
            "id": "99832586d55f540f603637e458a292406a0ed75d",
            "paperId": "99832586d55f540f603637e458a292406a0ed75d",
            "title": "LANGUAGE MODELS",
            "abstract": "While large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. Furthermore, on two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.",
            "year": 2023,
            "citationCount": 600,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces."
            },
            "score": 4
        },
        {
            "id": "cfce709a65f90312d2bdc1a6cf0380c19becf694",
            "paperId": "cfce709a65f90312d2bdc1a6cf0380c19becf694",
            "title": "RAGTruth: A Hallucination Corpus for Developing Trustworthy Retrieval-Augmented Language Models",
            "abstract": "Retrieval-augmented generation (RAG) has become a main technique for alleviating hallucinations in large language models (LLMs). Despite the integration of RAG, LLMs may still present unsupported or contradictory claims to the retrieved contents. In order to develop effective hallucination prevention strategies under RAG, it is important to create benchmark datasets that can measure the extent of hallucination. This paper presents RAGTruth, a corpus tailored for analyzing word-level hallucinations in various domains and tasks within the standard RAG frameworks for LLM applications. RAGTruth comprises nearly 18,000 naturally generated responses from diverse LLMs using RAG. These responses have undergone meticulous manual annotations at both the individual cases and word levels, incorporating evaluations of hallucination intensity. We not only benchmark hallucination frequencies across different LLMs, but also critically assess the effectiveness of several existing hallucination detection methodologies. Furthermore, we show that using a high-quality dataset such as RAGTruth, it is possible to finetune a relatively small LLM and achieve a competitive level of performance in hallucination detection when compared to the existing prompt-based approaches using state-of-the-art large language models such as GPT-4.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "RAGTruth is presented, a corpus tailored for analyzing word-level hallucinations in various domains and tasks within the standard RAG frameworks for LLM applications and it is shown that using a high-quality dataset such as RAGTruth, it is possible to finetune a relatively small LLM and achieve a competitive level of performance in hallucination detection when compared to the existing prompt-based approaches using state-of-the-art large language models such as GPT-4."
            },
            "score": 4
        },
        {
            "id": "1ea48c40366d78ffd0503ee025c889cc1fbd1024",
            "paperId": "1ea48c40366d78ffd0503ee025c889cc1fbd1024",
            "title": "Transferable and Efficient Non-Factual Content Detection via Probe Training with Offline Consistency Checking",
            "abstract": "Detecting non-factual content is a longstanding goal to increase the trustworthiness of large language models (LLMs) generations. Current factuality probes, trained using humanannotated labels, exhibit limited transferability to out-of-distribution content, while online selfconsistency checking imposes extensive computation burden due to the necessity of generating multiple outputs. This paper proposes PINOSE, which trains a probing model on offline self-consistency checking results, thereby circumventing the need for human-annotated data and achieving transferability across diverse data distributions. As the consistency check process is offline, PINOSE reduces the computational burden of generating multiple responses by online consistency verification. Additionally, it examines various aspects of internal states prior to response decoding, contributing to more effective detection of factual inaccuracies. Experiment results on both factuality detection and question answering benchmarks show that PINOSE achieves surpassing results than existing factuality detection methods. Our code and datasets are publicly available on this anonymized repository.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "PITOS, which trains a probing model on offline self-consistency checking results, thereby circumventing the need for human-annotated data and achieving transferability across diverse data distributions, and achieves surpassing results than existing factuality detection methods."
            },
            "score": 4
        },
        {
            "id": "e96348576b682e709b2ee06ea28cd81f8bfa102b",
            "paperId": "e96348576b682e709b2ee06ea28cd81f8bfa102b",
            "title": "Exploring the Factual Consistency in Dialogue Comprehension of Large Language Models",
            "abstract": "LLMs (Large Language Models) usually interact with users in the form of dialogue and generate responses following their instructions, which naturally require dialogue comprehension abilities. However, dialogue comprehension is a general language ability which is hard to be evaluated directly. In this work, we propose to perform the evaluation focusing on the factual consistency issue with the help of the dialogue summarization task. Besides evaluating and analyzing the dialogue summarization performance (DIAC-Sum) of different LLMs, we also derive factual questions from the generated summaries and use them as a more flexible measurement of dialogue comprehension (DIAC-QA). Our evaluation shows that, on average, 26.8% of the summaries generated by LLMs contain factual inconsistency. Even ChatGPT, the strongest model evaluated, has such errors in 16% of its summaries. For answering the factual questions, which is more challenging, the average error rate of all evaluated LLMs is 36.1%. Both results indicate serious deficiencies. Detailed analysis shows that the understanding of subject/object of the conversation is still challenging for LLMs. Furthermore, to stimulate and enhance the dialogue comprehension ability of LLMs, we propose a fine-tuning paradigm with auto-constructed multi-task data, which achieved a relative error rate reduction of 11% on DIAC-QA.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "To stimulate and enhance the dialogue comprehension ability of LLMs, a fine-tuning paradigm with auto-constructed multi-task data is proposed, which achieved a relative error rate reduction of 11% on DIAC-QA."
            },
            "score": 4
        },
        {
            "id": "36870ac7332b45edf59e5c111dc5bd24139dc1ce",
            "paperId": "36870ac7332b45edf59e5c111dc5bd24139dc1ce",
            "title": "Factual Consistency of Multilingual Pretrained Language Models",
            "abstract": "Pretrained language models can be queried for factual knowledge, with potential applications in knowledge base acquisition and tasks that require inference. However, for that, we need to know how reliable this knowledge is, and recent work has shown that monolingual English language models lack consistency when predicting factual knowledge, that is, they fill-in-the-blank differently for paraphrases describing the same fact. In this paper, we extend the analysis of consistency to a multilingual setting. We introduce a resource, mParaRel, and investigate (i) whether multilingual language models such as mBERT and XLM-R are more consistent than their monolingual counterparts;and (ii) if such models are equally consistent across languages.We find that mBERT is as inconsistent as English BERT in English paraphrases, but that both mBERT and XLM-R exhibit a high degree of inconsistency in English and even more so for all the other 45 languages.",
            "year": 2022,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "MBERT is as inconsistent as English BERT in English paraphrases, but that both mBERT and XLM-R exhibit a high degree of inconsistency in English and even more so for all the other 45 languages."
            },
            "score": 4
        },
        {
            "id": "663d743272e9ab04f54d9105a3c3a3f6e22dd1dd",
            "paperId": "663d743272e9ab04f54d9105a3c3a3f6e22dd1dd",
            "title": "FactKB: Generalizable Factuality Evaluation using Language Models Enhanced with Factual Knowledge",
            "abstract": "Evaluating the factual consistency of automatically generated summaries is essential for the progress and adoption of reliable summarization systems. Despite recent advances, existing factuality evaluation models are not robust, being especially prone to entity and relation errors in new domains. We propose FactKB, a simple new approach to factuality evaluation that is generalizable across domains, in particular with respect to entities and relations. FactKB is based on language models pretrained using facts extracted from external knowledge bases. We introduce three types of complementary factuality pretraining objectives based on direct entity facts, facts grounded in auxiliary knowledge about entities, and facts constructed compositionally through knowledge base walks. The resulting factuality evaluation model achieves state-of-the-art performance on two in-domain news summarization benchmarks as well as on three out-of-domain scientific literature datasets. Further analysis of FactKB shows improved ability to detect erroneous entities and relations in summaries and is robust and generalizable across domains.",
            "year": 2023,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "FactKB is a simple new approach to factuality evaluation that is generalizable across domains, in particular with respect to entities and relations and shows improved ability to detect erroneous entities and relation in summaries."
            },
            "score": 4
        },
        {
            "id": "4ea413e5a21a743d68c92e7f169535d0543f6051",
            "paperId": "4ea413e5a21a743d68c92e7f169535d0543f6051",
            "title": "On Improving Summarization Factual Consistency from Natural Language Feedback",
            "abstract": "Despite the recent progress in language generation models, their outputs may not always meet user expectations. In this work, we study whether informational feedback in natural language can be leveraged to improve generation quality and user preference alignment. To this end, we consider factual consistency in summarization, the quality that the summary should only contain information supported by the input documents, as the user-expected preference. We collect a high-quality dataset, DeFacto, containing human demonstrations and informational natural language feedback consisting of corrective instructions, edited summaries, and explanations with respect to the factual consistency of the summary. Using our dataset, we study three natural language generation tasks: (1) editing a summary by following the human feedback, (2) generating human feedback for editing the original summary, and (3) revising the initial summary to correct factual errors by generating both the human feedback and edited summary. We show that DeFacto can provide factually consistent human-edited summaries and further insights into summarization factual consistency thanks to its informational natural language feedback. We further demonstrate that fine-tuned language models can leverage our dataset to improve the summary factual consistency, while large language models lack the zero-shot learning ability in our proposed tasks that require controllable text generation.",
            "year": 2022,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "DeFacto can provide factually consistent human-edited summaries and further insights into summarization factual consistency thanks to its informational natural language feedback, and it is demonstrated that fine-tuned language models can leverage the dataset to improve the summary factual consistency."
            },
            "score": 4
        },
        {
            "id": "6d1ef4436904de111c8b1975bbf25d3fe2f165f7",
            "paperId": "6d1ef4436904de111c8b1975bbf25d3fe2f165f7",
            "title": "DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting",
            "abstract": "Recent progress has shown that large-scale pre-training using contrastive image-text pairs can be a promising alternative for high-quality visual representation learning from natural language supervision. Benefiting from a broader source of supervision, this new paradigm exhibits impressive transferability to downstream classification tasks and datasets. However, the problem of transferring the knowledge learned from image-text pairs to more complex dense prediction tasks has barely been visited. In this work, we present a new framework for dense prediction by implicitly and explicitly leveraging the pre-trained knowledge from CLIP. Specifically, we convert the original image-text matching problem in CLIP to a pixel-text matching problem and use the pixel-text score maps to guide the learning of dense prediction models. By further using the contextual information from the image to prompt the language model, we are able to facilitate our model to better exploit the pretrained knowledge. Our method is model-agnostic, which can be applied to arbitrary dense prediction systems and various pre-trained visual backbones including both CLIP models and ImageNet pre-trained models. Extensive experiments demonstrate the superior performance of our methods on semantic segmentation, object detection, and instance segmentation tasks. Code is available at https://github.com/raoyongming/DenseCLIP.",
            "year": 2021,
            "citationCount": 336,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents a new framework for dense prediction by implicitly and explicitly leveraging the pre-trained knowledge from CLIP and converts the original image-text matching problem in CLIP to a pixel- text matching problem and uses the pixel-text score maps to guide the learning of dense prediction models."
            },
            "score": 4
        },
        {
            "id": "9692d4e9de9492347486508b3f15438c0f8d749c",
            "paperId": "9692d4e9de9492347486508b3f15438c0f8d749c",
            "title": "Prompt-based Context- and Domain-aware Pretraining for Vision and Language Navigation",
            "abstract": "Pretrained visual-language models have extensive world knowledge and are widely used in visual and language navigation (VLN). However, they are not sensitive to indoor scenarios for VLN tasks. Another challenge for VLN is how the agent understands the contextual relations between actions on a path and performs cross-modal alignment sequentially. In this paper, we propose a novel Prompt-bAsed coNtext- and inDoor-Aware (PANDA) pretraining framework to address these problems. It performs prompting in two stages. In the indoor-aware stage, we apply an efficient tuning paradigm to learn deep visual prompts from an indoor dataset, in order to augment pretrained models with inductive biases towards indoor environments. This can enable more sample-efficient adaptation for VLN agents. Furthermore, in the context-aware stage, we design a set of hard context prompts to capture the sequence-level semantics in the instruction. They enable further tuning of the pretrained models via contrastive learning. Experimental results on both R2R and REVERIE show the superiority of PANDA compared to existing state-of-the-art methods.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel Prompt-bAsed coNtext- and inDoor-Aware (PANDA) pretraining framework to address problems of VLN adaptation and experimental results on both R2R and REVERIE show the superiority of PANDA compared to existing state-of-the-art methods."
            },
            "score": 4
        },
        {
            "id": "b534e67794f36fbb2ad7849f624b183765448fc6",
            "paperId": "b534e67794f36fbb2ad7849f624b183765448fc6",
            "title": "PANDA: Prompt-based Context- and Domain-aware Pretraining for Vision and Language Navigation",
            "abstract": "With strong representation capabilities, pretrained vision-language models are widely used in vision and language navigation (VLN). However, most of them are trained onweb-crawled general-purpose datasets, which incurs a considerable domain gap when used for VLN tasks. Another challenge for VLN is how the agent understands the contextual relations between actions on a trajectory and performs cross-modal alignment sequentially. In this paper, we propose a novel Prompt-bAsed coNtextand Domain-Aware (PANDA) pretraining framework to address these problems. It performs prompting in two stages. In the domain-aware stage, we apply a low-cost prompt tuning paradigm to learn soft visual prompts from an in-domain dataset for equipping the pretrained models with object-level and scene-level cross-modal alignment in VLN tasks. Furthermore, in the context-aware stage, we design a set of hard context prompts to capture the sequence-level semantics and instill both out-of-context and contextual knowledge in the instruction into cross-modal representations. They enable further tuning of the pretrained models via contrastive learning. Experimental results on both R2R and REVERIE show the superiority of PANDA compared to previous state-of-the-art methods.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel Prompt-bAsed coNtextand Domain-Aware (PANDA) pretraining framework to address problems of VLN and experimental results on both R2R and REVERIE show the superiority of PANDA compared to previous state-of-the-art methods."
            },
            "score": 4
        },
        {
            "id": "01d088de0c8e3c96a2924d4db2aac32b3d42a877",
            "paperId": "01d088de0c8e3c96a2924d4db2aac32b3d42a877",
            "title": "An Investigation of Large Language Models for Real-World Hate Speech Detection",
            "abstract": "Hate speech has emerged as a major problem plaguing our social spaces today. While there have been significant efforts to address this problem, existing methods are still significantly limited in effectively detecting hate speech online. A major limitation of existing methods is that hate speech detection is a highly contextual problem, and these methods cannot fully capture the context of hate speech to make accurate predictions. Recently, large language models (LLMs) have demonstrated state-of-the-art performance in several natural language tasks. LLMs have undergone extensive training using vast amounts of natural language data, enabling them to grasp intricate contextual details. Hence, they could be used as knowledge bases for context-aware hate speech detection. However, a fundamental problem with using LLMs to detect hate speech is that there are no studies on effectively prompting LLMs for context-aware hate speech detection. In this study, we conduct a large-scale study of hate speech detection, employing five established hate speech datasets. We discover that LLMs not only match but often surpass the performance of current benchmark machine learning models in identifying hate speech. By proposing four diverse prompting strategies that optimize the use of LLMs in detecting hate speech. Our study reveals that a meticulously crafted reasoning prompt can effectively capture the context of hate speech by fully utilizing the knowledge base in LLMs, significantly outperforming existing techniques. Furthermore, although LLMs can provide a rich knowledge base for the contextual detection of hate speech, suitable prompting strategies play a crucial role in effectively leveraging this knowledge base for efficient detection.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study reveals that a meticulously crafted reasoning prompt can effectively capture the context of hate speech by fully utilizing the knowledge base in LLMs, significantly outperforming existing techniques."
            },
            "score": 4
        },
        {
            "id": "46db418ac45d17f4381b26daab73e8e3e0728d99",
            "paperId": "46db418ac45d17f4381b26daab73e8e3e0728d99",
            "title": "Code-Aware Prompting: A study of Coverage Guided Test Generation in Regression Setting using LLM",
            "abstract": "Testing plays a pivotal role in ensuring software quality, yet conventional Search Based Software Testing (SBST) methods often struggle with complex software units, achieving suboptimal test coverage. Recent works using large language models (LLMs) for test generation have focused on improving generation quality through optimizing the test generation context and correcting errors in model outputs, but use fixed prompting strategies that prompt the model to generate tests without additional guidance. As a result LLM-generated testsuites still suffer from low coverage. In this paper, we present SymPrompt, a code-aware prompting strategy for LLMs in test generation. SymPrompt's approach is based on recent work that demonstrates LLMs can solve more complex logical problems when prompted to reason about the problem in a multi-step fashion. We apply this methodology to test generation by deconstructing the testsuite generation process into a multi-stage sequence, each of which is driven by a specific prompt aligned with the execution paths of the method under test, and exposing relevant type and dependency focal context to the model. Our approach enables pretrained LLMs to generate more complete test cases without any additional training. We implement SymPrompt using the TreeSitter parsing framework and evaluate on a benchmark challenging methods from open source Python projects. SymPrompt enhances correct test generations by a factor of 5 and bolsters relative coverage by 26% for CodeGen2. Notably, when applied to GPT-4, SymPrompt improves coverage by over 2x compared to baseline prompting strategies.",
            "year": 2024,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "SymPrompt is presented, a code-aware prompting strategy for LLMs in test generation that enables pretrained LLMs to generate more complete test cases without any additional training and enhances correct test generations by a factor of 5 and bolsters relative coverage by 26% for CodeGen2."
            },
            "score": 4
        },
        {
            "id": "ddb938f86f66478bfde2ef0c871b0a8940e045ed",
            "paperId": "ddb938f86f66478bfde2ef0c871b0a8940e045ed",
            "title": "Self-Demos: Eliciting Out-of-Demonstration Generalizability in Large Language Models",
            "abstract": "Large language models (LLMs) have shown promising abilities of in-context learning (ICL), adapting swiftly to new tasks with only few-shot demonstrations. However, current few-shot methods heavily depend on high-quality, query-specific demos, which are often lacking. When faced with out-of-demonstration (OOD) queries, methods that rely on hand-crafted demos or external retrievers might fail. To bridge the gap between limited demos and OOD queries, we propose Self-Demos, a novel prompting method that elicits the inherent generalizability in LLMs by query-aware demo generation. The generated demos strategically interpolate between existing demos and the given query, transforming the query from OOD to ID. To evaluate the effectiveness of our approach, we manually constructed OOD-Toolset, a dataset in the tool-using scenario with over 300 real-world APIs and 1000 instances, each consisting of three tool-use cases as demos and an OOD query. Thorough experiments on our dataset and two public math benchmarks have shown that our method can outperform state-of-the-art baselines in the OOD setting. Moreover, we conduct a range of analyses to validate Self-Demos's generalization and provide more insights.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes Self-Demos, a novel prompting method that elicits the inherent generalizability in LLMs by query-aware demo generation that can outperform state-of-the-art baselines in the OOD setting."
            },
            "score": 4
        },
        {
            "id": "52bea123acbbefac6ebb3b40b6482465845ef014",
            "paperId": "52bea123acbbefac6ebb3b40b6482465845ef014",
            "title": "ClarET: Pre-training a Correlation-Aware Context-To-Event Transformer for Event-Centric Generation and Classification",
            "abstract": "Generating new events given context with correlated ones plays a crucial role in many event-centric reasoning tasks. Existing works either limit their scope to specific scenarios or overlook event-level correlations. In this paper, we propose to pre-train a general Correlation-aware context-to-Event Transformer (ClarET) for event-centric reasoning. To achieve this, we propose three novel event-centric objectives, i.e., whole event recovering, contrastive event-correlation encoding and prompt-based event locating, which highlight event-level correlations with effective training. The proposed ClarET is applicable to a wide range of event-centric reasoning scenarios, considering its versatility of (i) event-correlation types (e.g., causal, temporal, contrast), (ii) application formulations (i.e., generation and classification), and (iii) reasoning types (e.g., abductive, counterfactual and ending reasoning). Empirical fine-tuning results, as well as zero- and few-shot learning, on 9 benchmarks (5 generation and 4 classification tasks covering 4 reasoning types with diverse event correlations), verify its effectiveness and generalization ability.",
            "year": 2022,
            "citationCount": 21,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes to pre-train a general Correlation-aware context-to-Event Transformer (ClarET) for event-centric reasoning, and proposes three novelevent-centric objectives, i.e., whole event recovering, contrastive event-correlation encoding and prompt-based event locating, which highlight event-level correlations with effective training."
            },
            "score": 4
        },
        {
            "id": "5c5ab276b00c1f19fbb0a3d2c38d532becac9442",
            "paperId": "5c5ab276b00c1f19fbb0a3d2c38d532becac9442",
            "title": "A Brief History of Prompt: Leveraging Language Models. (Through Advanced Prompting)",
            "abstract": "This paper presents a comprehensive exploration of the evolution of prompt engineering and generation in the field of natural language processing (NLP). Starting from the early language models and information retrieval systems, we trace the key developments that have shaped prompt engineering over the years. The introduction of attention mechanisms in 2015 revolutionized language understanding, leading to advancements in controllability and context-awareness. Subsequent breakthroughs in reinforcement learning techniques further enhanced prompt engineering, addressing issues like exposure bias and biases in generated text. We examine the significant contributions in 2018 and 2019, focusing on fine-tuning strategies, control codes, and template-based generation. The paper also discusses the growing importance of fairness, human-AI collaboration, and low-resource adaptation. In 2020 and 2021, contextual prompting and transfer learning gained prominence, while 2022 and 2023 witnessed the emergence of advanced techniques like unsupervised pre-training and novel reward shaping. Throughout the paper, we reference specific research studies that exemplify the impact of various developments on prompt engineering. The journey of prompt engineering continues, with ethical considerations being paramount for the responsible and inclusive future of AI systems.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The journey of prompt engineering continues, with ethical considerations being paramount for the responsible and inclusive future of AI systems."
            },
            "score": 4
        },
        {
            "id": "7cc2aac18db1493142240ae3982dce688e856f58",
            "paperId": "7cc2aac18db1493142240ae3982dce688e856f58",
            "title": "Discourse-Aware Prompt for Argument Impact Classification",
            "abstract": "Discourse information behind the arguments attracts a lot of attention from the field of Natural Language Processing (NLP) and computational argumentation. Durmus et al. [10] launched a new study on the influence of discourse contexts on determining argument impact. Argument Impact Classification is an intriguing but challenging task to classify whether the argumentative unit or an argument is impactful in a conversation. This paper empirically demonstrates that the discourse marker (e.g., \"for example,\" \"in other words\") can be represented by the learnable continuous prompt to align with discourse information existing in Pre-trained Language Model (PLM). This discourse information helps the Pre-trained Language Model understand the input template and elicit the discourse information to improve the performance on this task. Therefore, based on this intuition, we propose a prompt model DAPA and surpass the previous state-of-the-art model with a 2.5% F1 score.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper empirically demonstrates that the discourse marker can be represented by the learnable continuous prompt to align with discourse information existing in Pre-trained Language Model (PLM) and proposes a prompt model DAPA to surpass the previous state-of-the-art model."
            },
            "score": 4
        },
        {
            "id": "ed1437fbcd4a3fcb8fcbea3593dd732f7ba64084",
            "paperId": "ed1437fbcd4a3fcb8fcbea3593dd732f7ba64084",
            "title": "Gradient constrained sharpness-aware prompt learning for vision-language models",
            "abstract": "This paper targets a novel trade-off problem in generalizable prompt learning for vision-language models (VLM), i.e., improving the performance on unseen classes while maintaining the performance on seen classes. Comparing with existing generalizable methods that neglect the seen classes degradation, the setting of this problem is more strict and fits more closely with practical applications. To solve this problem, we start from the optimization perspective, and leverage the relationship between loss landscape geometry and model generalization ability. By analyzing the loss landscapes of the state-of-the-art method and vanilla Sharpness-aware Minimization (SAM) based method, we conclude that the trade-off performance correlates to both loss value and loss sharpness, while each of them is indispensable. However, we find the optimizing gradient of existing methods cannot maintain high relevance to both loss value and loss sharpness during optimization, which severely affects their trade-off performance. To this end, we propose a novel SAM-based method for prompt learning, denoted as Gradient Constrained Sharpness-aware Context Optimization (GCSCoOp), to dynamically constrain the optimizing gradient, thus achieving above two-fold optimization objective simultaneously. Extensive experiments verify the effectiveness of GCSCoOp in the trade-off problem.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel SAM-based method for prompt learning is proposed, denoted as Gradient Constrained Sharpness-aware Context Optimization (GCSCoOp), to dynamically constrain the optimizing gradient, thus achieving above two-fold optimization objective simultaneously."
            },
            "score": 4
        },
        {
            "id": "0875651b68e6602d45ae08bee67cf63c02faa512",
            "paperId": "0875651b68e6602d45ae08bee67cf63c02faa512",
            "title": "Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models",
            "abstract": "Large language models (LLMs) have scaled up to unlock a wide range of complex reasoning tasks with the aid of various prompting methods. However, current prompting methods generate natural language intermediate steps to help reasoning, which can cause imperfect task reduction and confusion. To mitigate such limitations, we explore code prompting, a neural symbolic prompting method with both zero-shot and few-shot versions which triggers code as intermediate steps. We conduct experiments on 7 widely-used benchmarks involving symbolic reasoning and arithmetic reasoning. Code prompting generally outperforms chain-of-thought (CoT) prompting. To further understand the performance and limitations of code prompting, we perform extensive ablation studies and error analyses, and identify several exclusive advantages of using symbolic promptings compared to natural language. We also consider the ensemble of code prompting and CoT prompting to combine the strengths of both. Finally, we show through experiments how code annotations and their locations affect code prompting.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores code prompting, a neural symbolic prompting method with both zero-shot and few-shot versions which triggers code as intermediate steps, and performs extensive ablation studies and error analyses and identifies several exclusive advantages of using symbolic promptings compared to natural language."
            },
            "score": 3
        },
        {
            "id": "08c43a7b09cead543bca41d2f4f64260f3b9d574",
            "paperId": "08c43a7b09cead543bca41d2f4f64260f3b9d574",
            "title": "Language Models in the Loop: Incorporating Prompting into Weak Supervision",
            "abstract": "We propose a new strategy for applying large pre-trained language models to novel tasks when labeled training data is limited. Rather than apply the model in a typical zero-shot or few-shot fashion, we treat the model as the basis for labeling functions in a weak supervision framework. To create a classifier, we first prompt the model to answer multiple distinct queries about an example and define how the possible responses should be mapped to votes for labels and abstentions. We then denoise these noisy label sources using the Snorkel system and train an end classifier with the resulting training data. Our experimental evaluation shows that prompting large language models within a weak supervision framework can provide significant gains in accuracy. On the WRENCH weak supervision benchmark, this approach can significantly improve over zero-shot performance, an average 19.5% reduction in errors. We also find that this approach produces classifiers with comparable or superior accuracy to those trained from hand-engineered rules.",
            "year": 2022,
            "citationCount": 36,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a new strategy for applying large pre-trained language models to novel tasks when labeled training data is limited and treats the model as the basis for labeling functions in a weak supervision framework, and produces classifiers with comparable or superior accuracy to those trained from hand-engineered rules."
            },
            "score": 3
        },
        {
            "id": "9c20d8d5cfc60f5b9aa058ff2968563f2af33398",
            "paperId": "9c20d8d5cfc60f5b9aa058ff2968563f2af33398",
            "title": "Temporal Insight Enhancement: Mitigating Temporal Hallucination in Multimodal Large Language Models",
            "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced the comprehension of multimedia content, bringing together diverse modalities such as text, images, and videos. However, a critical challenge faced by these models, especially when processing video inputs, is the occurrence of hallucinations - erroneous perceptions or interpretations, particularly at the event level. This study introduces an innovative method to address event-level hallucinations in MLLMs, focusing on specific temporal understanding in video content. Our approach leverages a novel framework that extracts and utilizes event-specific information from both the event query and the provided video to refine MLLMs' response. We propose a unique mechanism that decomposes on-demand event queries into iconic actions. Subsequently, we employ models like CLIP and BLIP2 to predict specific timestamps for event occurrences. Our evaluation, conducted using the Charades-STA dataset, demonstrates a significant reduction in temporal hallucinations and an improvement in the quality of event-related responses. This research not only provides a new perspective in addressing a critical limitation of MLLMs but also contributes a quantitatively measurable method for evaluating MLLMs in the context of temporal-related questions.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An innovative method to address event-level hallucinations in MLLMs, focusing on specific temporal understanding in video content, using a novel framework that extracts and utilizes event-specific information from both the event query and the provided video to refine MLLMs' response."
            },
            "score": 3
        },
        {
            "id": "5838b56f2c7ca3dd946428dae07bdc26a9265c67",
            "paperId": "5838b56f2c7ca3dd946428dae07bdc26a9265c67",
            "title": "Deficiency of Large Language Models in Finance: An Empirical Examination of Hallucination",
            "abstract": "The hallucination issue is recognized as a fundamental deficiency of large language models (LLMs), especially when applied to fields such as finance, education, and law. Despite the growing concerns, there has been a lack of empirical investigation. In this paper, we provide an empirical examination of LLMs' hallucination behaviors in financial tasks. First, we empirically investigate LLM model's ability of explaining financial concepts and terminologies. Second, we assess LLM models' capacity of querying historical stock prices. Third, to alleviate the hallucination issue, we evaluate the efficacy of four practical methods, including few-shot learning, Decoding by Contrasting Layers (DoLa), the Retrieval Augmentation Generation (RAG) method and the prompt-based tool learning method for a function to generate a query command. Finally, our major finding is that off-the-shelf LLMs experience serious hallucination behaviors in financial tasks. Therefore, there is an urgent need to call for research efforts in mitigating LLMs' hallucination.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The major finding is that off-the-shelf LLMs experience serious hallucination behaviors in financial tasks, and there is an urgent need to call for research efforts in mitigating LLMs' hallucination."
            },
            "score": 3
        },
        {
            "id": "468d1e2d75a23fecaf96fe65d8b01ff35ea5d0bd",
            "paperId": "468d1e2d75a23fecaf96fe65d8b01ff35ea5d0bd",
            "title": "Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models",
            "abstract": "Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts inserted in English reveal a clear pattern whereby the new piece of knowledge transfers only to languages with which English has a high RankC score.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy, and conducts an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level."
            },
            "score": 3
        },
        {
            "id": "18bd7909fab53be640d365cc8fce3ffceddae87d",
            "paperId": "18bd7909fab53be640d365cc8fce3ffceddae87d",
            "title": "Beyond Imitation: Generating Human Mobility from Context-aware Reasoning with Large Language Models",
            "abstract": "Human mobility behaviours are closely linked to various important societal problems such as traffic congestion, and epidemic control. However, collecting mobility data can be prohibitively expensive and involves serious privacy issues, posing a pressing need for high-quality generative mobility models. Previous efforts focus on learning the behaviour distribution from training samples, and generate new mobility data by sampling the learned distributions. They cannot effectively capture the coherent intentions that drive mobility behavior, leading to low sample efficiency and semantic-awareness. Inspired by the emergent reasoning ability in LLMs, we propose a radical perspective shift that reformulates mobility generation as a commonsense reasoning problem. In this paper, we design a novel Mobility Generation as Reasoning (MobiGeaR) framework that prompts LLM to recursively generate mobility behaviour. Specifically, we design a context-aware chain-of-thoughts prompting technique to align LLMs with context-aware mobility behaviour by few-shot in-context learning. Besides, MobiGeaR employ a divide-and-coordinate mechanism to exploit the synergistic effect between LLM reasoning and mechanistic gravity model. It leverages the step-by-step LLM reasoning to recursively generate a temporal template of activity intentions, which are then mapped to physical locations with a mechanistic gravity model. Experiments on two real-world datasets show MobiGeaR achieves state-of-the-art performance across all metrics, and substantially reduces the size of training samples at the same time. Besides, MobiGeaR also significantly improves the semantic-awareness of mobility generation by improving the intention accuracy by 62.23% and the generated mobility data is proven effective in boosting the performance of downstream applications. The implementation of our approach is available in the paper.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel Mobility Generation as Reasoning (MobiGeaR) framework that prompts LLM to recursively generate mobility behaviour by designing a context-aware chain-of-thoughts prompting technique to align LLMs with context-aware mobility behaviour by few-shot in-context learning."
            },
            "score": 3
        },
        {
            "id": "316aaca2b691c350aba317a392a603f304d7926f",
            "paperId": "316aaca2b691c350aba317a392a603f304d7926f",
            "title": "Kinematic-aware Prompting for Generalizable Articulated Object Manipulation with LLMs",
            "abstract": "Generalizable articulated object manipulation is essential for home-assistant robots. Recent efforts focus on imitation learning from demonstrations or reinforcement learning in simulation, however, due to the prohibitive costs of real-world data collection and precise object simulation, it still remains challenging for these works to achieve broad adaptability across diverse articulated objects. Recently, many works have tried to utilize the strong in-context learning ability of Large Language Models (LLMs) to achieve generalizable robotic manipulation, but most of these researches focus on high-level task planning, sidelining low-level robotic control. In this work, building on the idea that the kinematic structure of the object determines how we can manipulate it, we propose a kinematic-aware prompting framework that prompts LLMs with kinematic knowledge of objects to generate low-level motion trajectory waypoints, supporting various object manipulation. To effectively prompt LLMs with the kinematic structure of different objects, we design a unified kinematic knowledge parser, which represents various articulated objects as a unified textual description containing kinematic joints and contact location. Building upon this unified description, a kinematic-aware planner model is proposed to generate precise 3D manipulation waypoints via a designed kinematic-aware chain-of-thoughts prompting method. Our evaluation spanned 48 instances across 16 distinct categories, revealing that our framework not only outperforms traditional methods on 8 seen categories but also shows a powerful zero-shot capability for 8 unseen articulated object categories. Moreover, the real-world experiments on 7 different object categories prove our framework's adaptability in practical scenarios. Code is released at https://github.com/GeWu-Lab/LLM_articulated_object_manipulation/tree/main.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A kinematic-aware prompting framework that prompts LLMs with kinematic knowledge of objects to generate low-level motion trajectory waypoints, supporting various object manipulation is proposed."
            },
            "score": 3
        },
        {
            "id": "1df72f625cef4a4a3b60d926890f8382b1b958fb",
            "paperId": "1df72f625cef4a4a3b60d926890f8382b1b958fb",
            "title": "Look Before You Leap: Towards Decision-Aware and Generalizable Tool-Usage for Large Language Models",
            "abstract": "Tool-augmented large language models (LLMs) are attracting widespread attention when accessing up-to-date knowledge and alleviating hallucination issues. Nowadays, advanced closed-source LLMs (e.g., ChatGPT) have demonstrated surprising tool-usage capabilities through prompting and in-context learning techniques. To empower the capabilities of open-source LLMs (e.g., LLaMA) in manipulating tools, current efforts focus on either template-driven or token-triggered tool-usage. However, the former hampers LLMs' flexibility to address diverse user's queries due to constrained tool interactions, while the latter limits the generalizability when engaging with new tools, since tool-usage learning is based on task- and tool-specific datasets. To alleviate these concerns, in this paper, we propose a decision-aware and generalizable tool-usage framework (DEER). Specifically, we first construct the tool-usage samples with multiple decision branches via an automatic generation pipeline, thereby inspiring the decision-making awareness of LLMs under diverse scenarios. Meanwhile, we propose a novel tool sampling strategy to enhance the generalizability of LLMs over unseen tools. Extensive experiments demonstrate that our proposed DEER is effective and significantly outperforms baselines across various datasets.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A decision-aware and generalizable tool-usage framework (DEER), which first construct the tool-usage samples with multiple decision branches via an automatic generation pipeline, thereby inspiring the decision-making awareness of LLMs under diverse scenarios and proposing a novel tool sampling strategy to enhance the generalizability of LLMs over unseen tools."
            },
            "score": 3
        },
        {
            "id": "e31a820dd9324791ab294f89528455ac380d0d87",
            "paperId": "e31a820dd9324791ab294f89528455ac380d0d87",
            "title": "Style-Aware Radiology Report Generation with RadGraph and Few-Shot Prompting",
            "abstract": "Automatically generated reports from medical images promise to improve the workflow of radiologists. Existing methods consider an image-to-report modeling task by directly generating a fully-fledged report from an image. However, this conflates the content of the report (e.g., findings and their attributes) with its style (e.g., format and choice of words), which can lead to clinically inaccurate reports. To address this, we propose a two-step approach for radiology report generation. First, we extract the content from an image; then, we verbalize the extracted content into a report that matches the style of a specific radiologist. For this, we leverage RadGraph -- a graph representation of reports -- together with large language models (LLMs). In our quantitative evaluations, we find that our approach leads to beneficial performance. Our human evaluation with clinical raters highlights that the AI-generated reports are indistinguishably tailored to the style of individual radiologist despite leveraging only a few examples as context.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a two-step approach for radiology report generation that leverages RadGraph -- a graph representation of reports -- together with large language models (LLMs) and finds that this approach leads to beneficial performance."
            },
            "score": 3
        },
        {
            "id": "619184447595337a9fe3dca72c4e951e7ab7467c",
            "paperId": "619184447595337a9fe3dca72c4e951e7ab7467c",
            "title": "To See is to Believe: Prompting GPT-4V for Better Visual Instruction Tuning",
            "abstract": "Existing visual instruction tuning methods typically prompt large language models with textual descriptions to generate instruction-following data. Despite the promising performance achieved, these descriptions are derived from image annotations, which are oftentimes coarse-grained. Furthermore, the instructions might even contradict the visual content without observing the entire visual context. To address this challenge, we introduce a fine-grained visual instruction dataset, LVIS-Instruct4V, which contains 220K visually aligned and context-aware instructions produced by prompting the powerful GPT-4V with images from LVIS. Through experimental validation and case studies, we demonstrate that high-quality visual instructional data could improve the performance of LLaVA-1.5, a state-of-the-art large multimodal model, across a wide spectrum of benchmarks by clear margins. Notably, by simply replacing the LLaVA-Instruct with our LVIS-Instruct4V, we achieve better results than LLaVA on most challenging LMM benchmarks, e.g., LLaVA$^w$ (76.7 vs. 70.7) and MM-Vet (40.2 vs. 35.4). We release our data and model at https://github.com/X2FD/LVIS-INSTRUCT4V.",
            "year": 2023,
            "citationCount": 25,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a fine-grained visual instruction dataset, LVIS-Instruct4V, which contains 220K visually aligned and context-aware instructions produced by prompting the powerful GPT-4V with images from LVIS, and demonstrates that high-quality visual instructional data could improve the performance of LLaVA-1.5, a state-of-the-art large multimodal model, across a wide spectrum of benchmarks."
            },
            "score": 3
        },
        {
            "id": "027ec9a2aaa81b01d190e8607b2250779e5834dd",
            "paperId": "027ec9a2aaa81b01d190e8607b2250779e5834dd",
            "title": "Selective In-Context Data Augmentation for Intent Detection using Pointwise V-Information",
            "abstract": "This work focuses on in-context data augmentation for intent detection. Having found that augmentation via in-context prompting of large pre-trained language models (PLMs) alone does not improve performance, we introduce a novel approach based on PLMs and pointwise V-information (PVI), a metric that can measure the usefulness of a datapoint for training a model. Our method first fine-tunes a PLM on a small seed of training data and then synthesizes new datapoints - utterances that correspond to given intents. It then employs intent-aware filtering, based on PVI, to remove datapoints that are not helpful to the downstream intent classifier. Our method is thus able to leverage the expressive power of large language models to produce diverse training data. Empirical results demonstrate that our method can produce synthetic training data that achieve state-of-the-art performance on three challenging intent detection datasets under few-shot settings (1.28% absolute improvement in 5-shot and 1.18% absolute in 10-shot, on average) and perform on par with the state-of-the-art in full-shot settings (within 0.01% absolute, on average).",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel approach based on PLMs and pointwise V-information (PVI), a metric that can measure the usefulness of a datapoint for training a model, is introduced, able to leverage the expressive power of large language models to produce diverse training data."
            },
            "score": 3
        },
        {
            "id": "b9bd18499dd7070f36139906c8889cfe3dfbf5d2",
            "paperId": "b9bd18499dd7070f36139906c8889cfe3dfbf5d2",
            "title": "GCNs-Based Context-Aware Short Text Similarity Model",
            "abstract": "Semantic textual similarity is a fundamental task in text mining and natural language processing (NLP), which has profound research value. The essential step for text similarity is text representation learning. Recently, researches have explored the graph convolutional network (GCN) techniques on text representation, since GCN does well in handling complex structures and preserving syntactic information. However, current GCN models are usually limited to very shallow layers due to the vanishing gradient problem, which can not capture non-local dependency information of sentences. In this paper, we propose a GCNs-based context-aware (GCSTS) model that applies iterated GCN blocks to train deeper GCNs. Recurrently employing the same GCN block prevents over-fitting and provides broad effective input width. Combined with dense connections, GCSTS can be trained more deeply. Besides, we use dynamic graph structures in the block, which further extends the receptive field of each vertex in graphs, learning better sentence representations. Experiments show that our model outperforms existing models on several text similarity datasets, while also verify that GCNs-based text representation models can be trained in a deeper manner, rather than being trained in two or three layers.",
            "year": 2021,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A GCNs-based context-aware (GCSTS) model that applies iterated GCN blocks to train deeper GCNs, which outperforms existing models on several text similarity datasets and can be trained in a deeper manner, rather than being trained in two or three layers."
            },
            "score": 3
        },
        {
            "id": "d0dfbf14af09e514d139c617aa3288ef96268fba",
            "paperId": "d0dfbf14af09e514d139c617aa3288ef96268fba",
            "title": "BadCLIP: Trigger-Aware Prompt Learning for Backdoor Attacks on CLIP",
            "abstract": "Contrastive Vision-Language Pre-training, known as CLIP, has shown promising effectiveness in addressing downstream image recognition tasks. However, recent works revealed that the CLIP model can be implanted with a downstream-oriented backdoor. On downstream tasks, one victim model performs well on clean samples but predicts a specific target class whenever a specific trigger is present. For injecting a backdoor, existing attacks depend on a large amount of additional data to maliciously fine-tune the entire pre-trained CLIP model, which makes them inapplicable to data-limited scenarios. In this work, motivated by the recent success of learnable prompts, we address this problem by injecting a backdoor into the CLIP model in the prompt learning stage. Our method named BadCLIP is built on a novel and effective mechanism in backdoor attacks on CLIP, i.e., influencing both the image and text encoders with the trigger. It consists of a learnable trigger applied to images and a trigger-aware context generator, such that the trigger can change text features via trigger-aware prompts, resulting in a powerful and generalizable attack. Extensive experiments conducted on 11 datasets verify that the clean accuracy of BadCLIP is similar to those of advanced prompt learning methods and the attack success rate is higher than 99% in most cases. BadCLIP is also generalizable to unseen classes, and shows a strong generalization capability under cross-dataset and cross-domain settings.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "GoodCLIP is built on a novel and effective mechanism in backdoor attacks on CLIP, i.e., influencing both the image and text encoders with the trigger, and shows a strong generalization capability under cross-dataset and cross-domain settings."
            },
            "score": 3
        },
        {
            "id": "68850153b0210615c86f9a72624f34e2913bcddf",
            "paperId": "68850153b0210615c86f9a72624f34e2913bcddf",
            "title": "Document-Level Machine Translation with Large Language Models",
            "abstract": "Large language models (LLMs) such as ChatGPT can produce coherent, cohesive, relevant, and fluent answers for various natural language processing (NLP) tasks. Taking document-level machine translation (MT) as a testbed, this paper provides an in-depth evaluation of LLMs' ability on discourse modeling. The study focuses on three aspects: 1) Effects of Context-Aware Prompts, where we investigate the impact of different prompts on document-level translation quality and discourse phenomena; 2) Comparison of Translation Models, where we compare the translation performance of ChatGPT with commercial MT systems and advanced document-level MT methods; 3) Analysis of Discourse Modelling Abilities, where we further probe discourse knowledge encoded in LLMs and shed light on impacts of training techniques on discourse modeling. By evaluating on a number of benchmarks, we surprisingly find that LLMs have demonstrated superior performance and show potential to become a new paradigm for document-level translation: 1) leveraging their powerful long-text modeling capabilities, GPT-3.5 and GPT-4 outperform commercial MT systems in terms of human evaluation; 2) GPT-4 demonstrates a stronger ability for probing linguistic knowledge than GPT-3.5. This work highlights the challenges and opportunities of LLMs for MT, which we hope can inspire the future design and evaluation of LLMs.We release our data and annotations at https://github.com/longyuewangdcu/Document-MT-LLM.",
            "year": 2023,
            "citationCount": 53,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An in-depth evaluation of LLMs' ability on discourse modeling using document-level machine translation (MT) as a testbed finds that LLMs have demonstrated superior performance and show potential to become a new paradigm for document- level translation."
            },
            "score": 3
        },
        {
            "id": "7c9f69848d28e0a7cbb00942ee83dab9773c23e4",
            "paperId": "7c9f69848d28e0a7cbb00942ee83dab9773c23e4",
            "title": "GPT-NER: Named Entity Recognition via Large Language Models",
            "abstract": "Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model. In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text\"Columbus is a city\"is transformed to generate the text sequence\"@@Columbus## is a city\", where special tokens @@## marks the entity to extract. To efficiently address the\"hallucination\"issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a self-verification strategy by prompting LLMs to ask itself whether the extracted entities belong to a labeled entity tag. We conduct experiments on five widely adopted NER datasets, and GPT-NER achieves comparable performances to fully supervised baselines, which is the first time as far as we are concerned. More importantly, we find that GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, GPT-NER performs significantly better than supervised models. This demonstrates the capabilities of GPT-NER in real-world NER applications where the number of labeled examples is limited.",
            "year": 2023,
            "citationCount": 58,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, and performs significantly better than supervised models, which demonstrates the capabilities of GPT-ner in real-world NER applications where the number of labeled examples is limited."
            },
            "score": 2
        },
        {
            "id": "53c68c7a3e42807a4ed3f0bed448adc1fac2e3d4",
            "paperId": "53c68c7a3e42807a4ed3f0bed448adc1fac2e3d4",
            "title": "Semantic Consistency in Enterprise Models - Through Seamless Modelling and Execution Support",
            "abstract": "Semantic consistency, in context of a socio-technical view on Enterprise Information Systems (EIS), concern consistency between Enterprise Models, Conceptual Models expressed in professional language, Information Systems models and the factual data manipulation and functions implemented in runtime IT artefacts. CoreWeb, an environment for modelling and testing conceptual models, implemented as an easy to use web service, provide support for model editing and execution, semantic consistency exploration and checking as well as producing documentation for either requirement specifications and/or for execution in model driven runtime environments such as CorePro. CoreWeb modelling views and features are presented in the paper and illustrated in the Demo comprising a walkthrough of a case supporting dialog and model verification between users, domain experts, modellers and system designers.",
            "year": 2017,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "CoreWeb modelling views and features are presented in the paper and illustrated in the Demo comprising a walkthrough of a case supporting dialog and model verification between users, domain experts, modellers and system designers."
            },
            "score": 2
        },
        {
            "id": "a1c297c2b0d54817670532edfa937444e595249f",
            "paperId": "a1c297c2b0d54817670532edfa937444e595249f",
            "title": "OntoType: Ontology-Guided Zero-Shot Fine-Grained Entity Typing with Weak Supervision from Pre-Trained Language Models",
            "abstract": "Fine-grained entity typing (FET), which assigns entities in text with context-sensitive, fine-grained semantic types, will play an important role in natural language understanding. A supervised FET method, which typically relies on human-annotated corpora for training, is costly and difficult to scale. Recent studies leverage pre-trained language models (PLMs) to generate rich and context-aware weak supervision for FET. However, a PLM may still generate a mixture of rough and fine-grained types, or tokens unsuitable for typing. In this study, we vision that an ontology provides a semantics-rich, hierarchical structure, which will help select the best results generated by multiple PLM models and head words. Specifically, we propose a novel zero-shot, ontology-guided FET method, OntoType, which follows a type ontological structure, from coarse to fine, ensembles multiple PLM prompting results to generate a set of type candidates, and refines its type resolution, under the local context with a natural language inference model. Our experiments on the Ontonotes, FIGER, and NYT datasets using their associated ontological structures demonstrate that our method outperforms the state-of-the-art zero-shot fine-grained entity typing methods. Our error analysis shows that refinement of the existing ontology structures will further improve fine-grained entity typing.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel zero-shot, ontology-guided FET method, OntoType, which follows a type ontological structure, from coarse to fine, ensembles multiple PLM prompting results to generate a set of type candidates, and refines its type resolution, under the local context with a natural language inference model."
            },
            "score": 2
        },
        {
            "id": "acc7a0a881d0914a5856768e22b7347ce06bd445",
            "paperId": "acc7a0a881d0914a5856768e22b7347ce06bd445",
            "title": "Building Accurate Translation-Tailored LLMs with Language Aware Instruction Tuning",
            "abstract": "Translation-tailored Large language models (LLMs) exhibit remarkable translation capabilities, even competing with supervised-trained commercial translation systems. However, off-target translation remains an unsolved problem, especially for low-resource languages, hindering us from developing accurate LLMs-based translation models. To mitigate the off-target translation problem and enhance the performance of LLMs on translation, recent works have either designed advanced prompting strategies to highlight the functionality of translation instructions or exploited the in-context learning ability of LLMs by feeding few-shot demonstrations. However, these methods essentially do not improve LLM's ability to follow translation instructions, especially the language direction information. In this work, we design a two-stage fine-tuning algorithm to improve the instruction-following ability (especially the translation direction) of LLMs. Specifically, we first tune LLMs with the maximum likelihood estimation loss on the translation dataset to elicit the basic translation capabilities. In the second stage, we construct instruction-conflicting samples by randomly replacing the translation directions with a wrong one within the instruction, and then introduce an extra unlikelihood loss to learn those samples. Experiments on IWSLT and WMT benchmarks upon the LLaMA model spanning 16 zero-shot directions show that, compared to the competitive baseline -- translation-finetuned LLama, our method could effectively reduce the off-target translation ratio (averagely -53.3\\%), thus improving translation quality with average +5.7 SacreBLEU and +16.4 BLEURT. Analysis shows that our method could preserve the model's general task performance on AlpacaEval. Code and models will be released at \\url{https://github.com/alphadl/LanguageAware_Tuning}.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A two-stage fine-tuning algorithm to improve the instruction-following ability (especially the translation direction) of LLMs, which could effectively reduce the off-target translation ratio and improve translation quality."
            },
            "score": 2
        },
        {
            "id": "2ce979753808340dd74e697e145dea2e7c5c6560",
            "paperId": "2ce979753808340dd74e697e145dea2e7c5c6560",
            "title": "Context-Aware IoT Device Functionality Extraction from Specifications for Ensuring Consumer Security",
            "abstract": "Internet of Thing (IoT) devices are being widely used in smart homes and organizations. An IoT device has some intended purposes, but may also have hidden functionalities. Typically, the device is installed in a home or an organization and the network traffic associated with the device is captured and analyzed to infer high-level functionality to the extent possible. However, such analysis is dynamic in nature, and requires the installation of the device and access to network data which is often hard to get for privacy and confidentiality reasons. We propose an alternative static approach which can infer the functionality of a device from vendor materials using Natural Language Processing (NLP) techniques. Information about IoT device functionality can be used in various applications, one of which is ensuring security in a smart home. We demonstrate how security policies associated with device functionality in a smart home can be formally represented using the NIST Next Generation Access Control (NGAC) model and automatically analyzed using Alloy, which is a formal verification tool. This will provide assurance to the consumer that these devices will be compliant to the home or organizational policy even before they have been purchased.",
            "year": 2021,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work demonstrates how security policies associated with device functionality in a smart home can be formally represented using the NIST Next Generation Access Control (NGAC) model and automatically analyzed using Alloy, which is a formal verification tool."
            },
            "score": 2
        },
        {
            "id": "5a6c442c3ed9dd9ead8e228f417d51e8b18efebb",
            "paperId": "5a6c442c3ed9dd9ead8e228f417d51e8b18efebb",
            "title": "Towards Modeling Role-Aware Centrality for Dialogue Summarization",
            "abstract": "Role-oriented dialogue summarization generates summaries for different roles in dialogue (e.g. doctor and patient). Existing methods consider roles separately where interactions among different roles are not fully explored. In this paper, we propose a novel Role-Aware Centrality (RAC) model to capture role interactions, which can be easily applied to any seq2seq models. The RAC assigns each role a specific sentence-level centrality score by involving role prompts to control what kind of summary to generate. The RAC measures both the importance of utterances and the relevance between roles and utterances. Then we use RAC to re-weight context representations, which are used by the decoder to generate role summaries. We verify RAC on two public benchmark datasets, CSDS and MC. Experimental results show that the proposed method achieves new state-of-the-art results on the two datasets. Extensive analyses have demonstrated that the role-aware centrality helps generate summaries more precisely.",
            "year": 2022,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel Role-Aware Centrality (RAC) model to capture role interactions, which can be easily applied to any seq2seq models, and achieves new state-of-the-art results on the two datasets."
            },
            "score": 2
        },
        {
            "id": "b9df0d4631f9fab1432c152765e243ae4cd667f4",
            "paperId": "b9df0d4631f9fab1432c152765e243ae4cd667f4",
            "title": "Effective Prompt Extraction from Language Models",
            "abstract": "The text generated by large language models is commonly controlled by prompting, where a prompt prepended to a user's query guides the model's output. The prompts used by companies to guide their models are often treated as secrets, to be hidden from the user making the query. They have even been treated as commodities to be bought and sold. However, anecdotal reports have shown adversarial users employing prompt extraction attacks to recover these prompts. In this paper, we present a framework for systematically measuring the effectiveness of these attacks. In experiments with 3 different sources of prompts and 11 underlying large language models, we find that simple text-based attacks can in fact reveal prompts with high probability. Our framework determines with high precision whether an extracted prompt is the actual secret prompt, rather than a model hallucination. Prompt extraction experiments on real systems such as Bing Chat and ChatGPT suggest that system prompts can be revealed by an adversary despite existing defenses in place.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents a framework for systematically measuring the effectiveness of prompt extraction attacks and determines with high precision whether an extracted prompt is the actual secret prompt, rather than a model hallucination."
            },
            "score": 1
        },
        {
            "id": "34d200f0c5ab62a6866f9576a5a8bc097623ba28",
            "paperId": "34d200f0c5ab62a6866f9576a5a8bc097623ba28",
            "title": "Context-aware Task Offloading and Resource Allocation in Cloud-Edge System for Electric Data Processing",
            "abstract": "With the development of intelligent applications, more and more complex intelligent tasks put forward high requirements for response delay. The cloud computing mode can meet the computing needs of various terminal applications because of its powerful computing power, but the response quality of intelligent tasks is reduced due to the large transmission delay from the terminal device to the cloud computing center. Edge computing reduces latency by shortening the data transmission distance, but the heterogeneity of edge nodes leads to tasks with different response qualities when offloaded to different servers. In addition, limited bandwidth resources prompt a rational resource allocation strategy to improve system efficiency. Therefore, this paper jointly studies task offloading and resource allocation strategies in the cloud-edge collaborative environment with the goal of minimizing the response time. In order to reasonably utilize the limited edge resources, we firstly divide tasks into delay-tolerant, delay-hypersensitive and delay-hyposensitive tasks according to their delay tolerance. We combine the greedy strategy to propose a classification offloading strategy and allocate the bandwidth resource amount according to the offloading location, which reduces the task response time while ensuring resource utilization. Finally, simulation experiments are carried out to verify the performance of the proposed strategy. Compared with other algorithms in comparative experiments, the algorithm proposed in this paper effectively reduces the response time of tasks and improves the performance of the system.",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The greedy strategy is combined to propose a classification offloading strategy and allocate the bandwidth resource amount according to the offloading location, which reduces the task response time while ensuring resource utilization."
            },
            "score": 1
        },
        {
            "id": "b4d4ccb6803548922505d99bf626eff7a35402e9",
            "paperId": "b4d4ccb6803548922505d99bf626eff7a35402e9",
            "title": "A Context-Aware Middleware-Level Solution towards a Ubiquitous Healthcare System",
            "abstract": "Recent advances in wireless technology, sensors and portable devices offer interesting opportunities to enable ubiquitous assistance to individuals in need of prompt help. Providing healthcare services to mobile users, such as, patients, elders, or potential drug abusers, is a rather challenging task. Novel middleware-level supports are required to integrate sensor infrastructures capable of detecting changes in the monitored subjects\u2019 health conditions and of alerting medical personnel, and the victim\u2019s relatives and friends in case of emergency situations. Along this line, the paper envisions a context-aware middleware-level solution dubbed Pervasive Environment for Affective Healthcare (PEACH). PEACH integrates together various sensors in a Wireless Body Area Network (WBAN) to detect alterations of monitored subjects\u2019 affective and physical conditions, aggregate the sensed information, and also detect potentially dangerous situations for the monitored subject. Finally, PEACH aims at providing outdoor assistance to the victim/patient by quickly promoting the formation of ad hoc rescue groups comprising nearby volunteers. Through encouraging results obtained from both simulations and a practical drug-rehabilitation application testbed, the effectiveness of the envisioned PEACH framework is verified.",
            "year": 2009,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A context-aware middleware-level solution dubbed Pervasive Environment for Affective Healthcare (PEACH), which integrates together various sensors in a Wireless Body Area Network to detect alterations of monitored subjects\u2019 affective and physical conditions, aggregate the sensed information, and also detect potentially dangerous situations for the monitored subject."
            },
            "score": 1
        }
    ],
    "novelty": "no"
}