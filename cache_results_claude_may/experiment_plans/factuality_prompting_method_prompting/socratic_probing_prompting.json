{
    "topic_description": "novel prompting methods that can improve factuality and reduce hallucination of large language models",
    "idea_name": "Socratic Probing Prompting",
    "raw_idea": {
        "Problem": "Large language models can generate fluent text that lacks factual grounding, especially when generating long-form responses.",
        "Existing Methods": "Existing methods include incorporating retrieval and using learned fact-checking models.",
        "Motivation": "Socratic questioning is a technique used to probe assumptions, expose contradictions, and encourage deeper reasoning. We can use Socratic probing to guide LLMs towards more factual and coherent generation.",
        "Proposed Method": "The procedure involves: 1) Generating an initial response to a query. 2) Prompting the model with a series of targeted Socratic questions about the generated text, e.g., 'What is the source of this claim?', 'Is there any evidence that contradicts this?', 'Are there any implicit assumptions being made?'. 3) Using the model's answers to these probes to score the factuality of each sentence. 4) Prompting the model to refine the low-scoring sentences using retrieved evidence.",
        "Experiment Plan": "Evaluate on long-form QA datasets like ELI5 and Wikihow. Compare to baselines like vanilla prompting and retrieval-augmented generation. Metrics include factuality scores from a learned fact-checker, and human evaluation of factual consistency and coherence."
    },
    "full_experiment_plan": {
        "Title": "Socratic Probing: Guiding Language Models to Factual and Coherent Generation",
        "Problem Statement": "Large language models can generate fluent text that lacks factual grounding, especially when generating long-form responses. This can lead to hallucination and inconsistency in the generated text.",
        "Motivation": "Existing methods to address this issue include incorporating retrieval and using learned fact-checking models. However, these approaches often require additional training or external knowledge sources. Socratic questioning is a technique used to probe assumptions, expose contradictions, and encourage deeper reasoning. By using Socratic probing to guide LLMs during generation, we aim to improve the factuality and coherence of the generated text without relying on external resources or additional training.",
        "Proposed Method": "The Socratic Probing procedure involves the following steps:\n1. Generate an initial response to a query using the LLM.\n2. Prompt the model with a series of targeted Socratic questions about the generated text, e.g., 'What is the source of this claim?', 'Is there any evidence that contradicts this?', 'Are there any implicit assumptions being made?'.\n3. Use the model's answers to these probes to score the factuality of each sentence in the generated response.\n4. Prompt the model to refine the low-scoring sentences using retrieved evidence or by generating more coherent alternatives.\n5. Repeat steps 2-4 for a fixed number of iterations or until a satisfactory level of factuality and coherence is achieved.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Dataset Selection": "Evaluate the proposed method on long-form question-answering datasets such as ELI5 and Wikihow. These datasets contain open-ended questions that require multi-sentence explanations, making them suitable for assessing the factuality and coherence of generated responses.",
            "Step 2: Baseline Models": "Compare the performance of Socratic Probing to the following baselines:\n1. Vanilla prompting: Generate responses directly from the LLM without any additional probing or refinement.\n2. Retrieval-augmented generation: Use a retrieval model to find relevant passages from an external knowledge source (e.g., Wikipedia) and condition the LLM on these passages during generation.",
            "Step 3: Socratic Question Generation": "Design a set of Socratic question templates that probe various aspects of factuality and coherence, such as:\n- 'What is the source of this claim?'\n- 'Is there any evidence that supports or contradicts this statement?'\n- 'Are there any implicit assumptions being made here?'\n- 'How does this sentence relate to the previous one?'\n- 'Is this information relevant to answering the original question?'\nPrompt the LLM to generate specific Socratic questions by filling in these templates with sentences from the generated response.",
            "Step 4: Factuality Scoring": "Use the LLM's answers to the Socratic questions to assign a factuality score to each sentence in the generated response. For example, if the model cannot provide a credible source for a claim or identifies contradictory evidence, the corresponding sentence should receive a low factuality score.",
            "Step 5: Response Refinement": "For sentences with low factuality scores, prompt the LLM to refine them by either:\n1. Retrieving relevant evidence from an external knowledge source and conditioning the model on this evidence to generate a more factual alternative.\n2. Prompting the model to generate a more coherent alternative that better fits the context of the surrounding sentences.",
            "Step 6: Iteration": "Repeat steps 3-5 for a fixed number of iterations (e.g., 3) or until a satisfactory level of factuality and coherence is achieved (e.g., all sentences have a factuality score above a certain threshold).",
            "Step 7: Evaluation Metrics": "Evaluate the generated responses using the following metrics:\n1. Factuality: Use a learned fact-checking model to assess the factual accuracy of the generated text. Alternatively, manually annotate a subset of the responses for factual correctness.\n2. Coherence: Use a learned coherence model or human evaluation to assess the logical flow and consistency of the generated responses.\n3. Relevance: Manually evaluate whether the generated responses adequately answer the original question.",
            "Step 8: Model Selection": "Experiment with different LLMs (e.g., GPT-3.5, GPT-4) to assess the effectiveness of Socratic Probing across various model sizes and architectures."
        },
        "Test Case Examples": {
            "Test Case 1": {
                "Query": "How do airplanes fly?",
                "Baseline Output": "Airplanes fly by generating lift using their wings. The wings are shaped like an airfoil, which means they are curved on top and flatter on the bottom. This shape causes the air moving over the wing to travel faster than the air moving under the wing, creating an area of low pressure above the wing. The higher air pressure beneath the wing pushes the plane upward, overcoming gravity and allowing the plane to fly. The engines of the plane propel it forward, which also helps generate lift by forcing air over the wings. The pilot controls the plane using various surfaces such as the rudder, ailerons, and elevators, which change the shape of the wings and tail to steer and maintain stable flight.",
                "Socratic Probing": {
                    "Question 1": "What is the source of the claim that wings are shaped like an airfoil?",
                    "Answer 1": "The shape of airplane wings being similar to an airfoil is a well-established fact in aerodynamics and can be found in many educational resources and scientific publications on the topic.",
                    "Question 2": "Is there any evidence that contradicts the statement about air pressure differences causing lift?",
                    "Answer 2": "No, the explanation of lift being generated by a pressure difference due to the shape of the wing is widely accepted and supported by scientific evidence.",
                    "Question 3": "Are there any implicit assumptions made about the role of the plane's engines?",
                    "Answer 3": "The response assumes that the reader has some basic knowledge of how engines work to propel the plane forward. It could be made more explicit that the engines generate thrust, which is necessary to maintain the airflow over the wings and generate lift.",
                    "Refined Output": "Airplanes fly by generating lift using their wings. The wings are shaped like an airfoil, which is a design that is curved on top and flatter on the bottom. This shape causes the air moving over the wing to travel faster than the air moving under the wing, creating an area of low pressure above the wing. The higher air pressure beneath the wing pushes the plane upward, overcoming gravity and allowing the plane to fly. This principle is known as Bernoulli's law and is well-established in aerodynamics.\n\nThe engines of the plane generate thrust to propel it forward, which is necessary to maintain the airflow over the wings and generate lift. As the plane moves through the air, the shape of the wings forces the air to move faster over the top of the wing, creating the pressure difference that generates lift.\n\nThe pilot controls the plane using various control surfaces such as the rudder, ailerons, and elevators. These surfaces change the shape of the wings and tail to steer the plane and maintain stable flight. By adjusting these controls, the pilot can change the amount of lift generated and the direction of the plane's movement."
                }
            },
            "Test Case 2": {
                "Query": "What is the process of photosynthesis in plants?",
                "Baseline Output": "Photosynthesis is the process by which plants convert sunlight into chemical energy that can be used to fuel the plant's growth and development. The process takes place primarily in the leaves of the plant, where specialized structures called chloroplasts contain a green pigment called chlorophyll. Chlorophyll absorbs sunlight and uses the energy to convert carbon dioxide and water into glucose, a simple sugar that the plant can use for energy. Oxygen is released as a byproduct of this process.\n\nThe overall equation for photosynthesis is: 6CO2 + 6H2O + light energy \u2192 C6H12O6 + 6O2\n\nThis means that six molecules of carbon dioxide (CO2) and six molecules of water (H2O) are converted into one molecule of glucose (C6H12O6) and six molecules of oxygen (O2) using light energy.\n\nThe process of photosynthesis occurs in two stages: the light-dependent reactions and the light-independent reactions (also known as the Calvin cycle). In the light-dependent reactions, sunlight is absorbed by chlorophyll and the energy is used to split water molecules into hydrogen and oxygen. The oxygen is released as a waste product, while the hydrogen is used to create energy-carrying molecules called ATP and NADPH.\n\nIn the light-independent reactions, the ATP and NADPH from the light-dependent reactions are used to convert carbon dioxide into glucose through a series of chemical reactions known as the Calvin cycle. This process occurs in the stroma, the fluid surrounding the thylakoids in the chloroplast.\n\nPhotosynthesis is essential for life on Earth, as it provides the energy that is used by nearly all living things to fuel their growth and development. It also plays a crucial role in the carbon cycle by removing carbon dioxide from the atmosphere and replacing it with oxygen, which is necessary for most forms of life to survive.",
                "Socratic Probing": {
                    "Question 1": "What is the source of the claim that photosynthesis takes place primarily in the leaves of plants?",
                    "Answer 1": "The fact that photosynthesis occurs mainly in plant leaves is widely accepted and can be found in many scientific textbooks and publications on the subject.",
                    "Question 2": "Is there any evidence that contradicts the equation for photosynthesis?",
                    "Answer 2": "No, the overall equation for photosynthesis (6CO2 + 6H2O + light energy \u2192 C6H12O6 + 6O2) is well-established and supported by extensive scientific evidence.",
                    "Question 3": "Are there any implicit assumptions made about the role of chlorophyll in photosynthesis?",
                    "Answer 3": "The response assumes that the reader has some prior knowledge of what chlorophyll is and its role in absorbing sunlight. It could be made more explicit that chlorophyll is essential for the light-dependent reactions of photosynthesis and that it is responsible for the green color of most plants.",
                    "Question 4": "How does the information about the Calvin cycle relate to the overall process of photosynthesis?",
                    "Answer 4": "The Calvin cycle, also known as the light-independent reactions, is a crucial part of the photosynthesis process. It uses the energy-carrying molecules (ATP and NADPH) produced in the light-dependent reactions to convert carbon dioxide into glucose. By explaining the Calvin cycle, the response provides a more complete picture of how photosynthesis works.",
                    "Refined Output": "Photosynthesis is the process by which plants, algae, and some bacteria convert sunlight into chemical energy that can be used to fuel the organism's activities. In plants, photosynthesis occurs primarily in the leaves, where specialized structures called chloroplasts contain a green pigment called chlorophyll. Chlorophyll is essential for the light-dependent reactions of photosynthesis, as it absorbs sunlight and uses the energy to drive the production of energy-carrying molecules (ATP and NADPH).\n\nThe overall equation for photosynthesis is: 6CO2 + 6H2O + light energy \u2192 C6H12O6 + 6O2. This means that six molecules of carbon dioxide (CO2) and six molecules of water (H2O) are converted into one molecule of glucose (C6H12O6) and six molecules of oxygen (O2) using light energy. This equation is a simplified representation of the complex series of chemical reactions that occur during photosynthesis.\n\nPhotosynthesis occurs in two main stages: the light-dependent reactions and the light-independent reactions (also known as the Calvin cycle). In the light-dependent reactions, sunlight is absorbed by chlorophyll in the thylakoid membranes of the chloroplast. The energy from the sunlight is used to split water molecules into hydrogen and oxygen. The oxygen is released as a byproduct, while the hydrogen ions (protons) are used to create a proton gradient across the thylakoid membrane. This proton gradient is then used to power the synthesis of ATP and NADPH.\n\nIn the light-independent reactions, which take place in the stroma of the chloroplast, the ATP and NADPH produced in the light-dependent reactions are used to convert carbon dioxide into glucose through a series of enzymatic reactions known as the Calvin cycle. The key enzyme in this process is RuBisCO (ribulose bisphosphate carboxylase oxygenase), which catalyzes the fixation of carbon dioxide to a 5-carbon sugar called ribulose bisphosphate (RuBP). The fixed carbon is then reduced to form 3-carbon sugars, which are eventually converted into glucose and other organic compounds that the plant can use for energy and growth.\n\nPhotosynthesis is a vital process for life on Earth, as it provides the energy that is used by nearly all living things to fuel their growth and development. It also plays a crucial role in the global carbon cycle by removing carbon dioxide from the atmosphere and replacing it with oxygen, which is necessary for most forms of life to survive. Understanding the mechanisms of photosynthesis is essential for developing strategies to improve crop yields, mitigate the effects of climate change, and develop sustainable sources of energy."
                }
            }
        },
        "Fallback Plan": "If the proposed Socratic Probing method does not significantly improve the factuality and coherence of the generated responses compared to the baselines, there are several alternative approaches that could be explored:\n\n1. Analyze the generated Socratic questions and answers to identify potential weaknesses in the probing process. For example, if the questions are not sufficiently probing or the answers are not informative enough, the probing templates or scoring criteria may need to be adjusted.\n\n2. Experiment with different types of Socratic questions that focus on other aspects of factuality and coherence, such as logical consistency, relevance to the original query, or the presence of contradictory statements.\n\n3. Investigate alternative methods for refining the generated responses based on the Socratic probing results. For example, instead of using retrieved evidence or generating coherent alternatives, the model could be prompted to directly edit the problematic sentences or to provide explanations for why certain statements are factual or non-factual.\n\n4. Conduct a more in-depth analysis of the types of factual errors and coherence issues that occur in the generated responses, and use this information to develop targeted probing strategies or post-processing techniques.\n\n5. Explore the use of external knowledge sources or fact-checking models to augment the Socratic Probing process, while still maintaining the focus on using the LLM's own knowledge and reasoning capabilities.\n\n6. Consider alternative evaluation metrics or human evaluation protocols to better assess the factuality and coherence of the generated responses, as the current metrics may not fully capture the effectiveness of the Socratic Probing method.\n\nIf these alternative approaches do not yield satisfactory results, the project could be adapted into an analysis of the limitations of using Socratic Probing for improving the factuality and coherence of LLM-generated responses. This analysis could provide valuable insights into the challenges of controlling the output of LLMs and guide future research on more effective methods for ensuring factual accuracy and logical consistency in generated text."
    },
    "novelty_queries": [
        "KeywordQuery(\"socratic probing language models\")",
        "KeywordQuery(\"language models factual coherent generation\")",
        "KeywordQuery(\"guiding language models socratic questioning\")",
        "KeywordQuery(\"Socratic Probing Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "69335077fcacbff7a7cf25697da1949e6bdfa968",
            "paperId": "69335077fcacbff7a7cf25697da1949e6bdfa968",
            "title": "The Art of SOCRATIC QUESTIONING: Recursive Thinking with Large Language Models",
            "abstract": "Chain-of-Thought (CoT) prompting enables large language models to solve complex reasoning problems by generating intermediate steps. However, confined by its inherent single-pass and sequential generation process, CoT heavily relies on the initial decisions, causing errors in early steps to accumulate and impact the final answers. In contrast, humans adopt recursive thinking when tackling complex reasoning problems, i.e., iteratively breaking the original problem into approachable sub-problems and aggregating their answers to resolve the original one. Inspired by the human cognitive process, we propose SOCRATIC QUESTIONING, a divide-and-conquer style algorithm that mimics the recursive thinking process. Specifically, SOCRATIC QUESTIONING leverages large language models to raise and answer sub-questions until collecting enough information to tackle the original question. Unlike CoT, SOCRATIC QUESTIONING explicitly navigates the thinking space, stimulates effective recursive thinking, and is more robust towards errors in the thinking process. Extensive experiments on several complex reasoning tasks, including MMLU, MATH, LogiQA, and visual question-answering demonstrate significant performance improvements over the state-of-the-art prompting methods, such as CoT, and Tree-of-Thought. The qualitative analysis clearly shows that the intermediate reasoning steps elicited by SOCRATIC QUESTIONING are similar to humans' recursively thinking process of complex reasoning problems.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The qualitative analysis clearly shows that the intermediate reasoning steps elicited by SOCRATIC QUESTIONING are similar to humans' recursively thinking process of complex reasoning problems."
            },
            "score": 8,
            "novelty_score": "The project proposal aims to improve the factuality and coherence of long-form text generated by language models using Socratic probing, which involves iteratively asking targeted questions about the generated text to refine it. The paper proposes SOCRATIC QUESTIONING, a recursive thinking approach that breaks down complex reasoning problems into sub-questions and aggregates their answers to solve the original problem.\n\nWhile both the project proposal and the paper use the concept of Socratic questioning, their research problems and approaches differ. The project proposal focuses on improving factuality and coherence in generated text, while the paper aims to enhance complex reasoning capabilities. The project proposal uses Socratic probing to refine generated text, whereas the paper employs a divide-and-conquer algorithm to navigate the thinking space and stimulate recursive thinking.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "d7386e8859b22e05ce9c4a972613d4b1e1e44198",
            "paperId": "d7386e8859b22e05ce9c4a972613d4b1e1e44198",
            "title": "Prompting Large Language Models With the Socratic Method",
            "abstract": "This paper presents a systematic approach to using the Socratic method in developing prompt templates that effectively interact with large language models, including GPT-3. Various methods are examined, and those that yield precise answers and justifications while fostering creativity and imagination to enhance creative writing are identified. Techniques such as definition, elenchus, dialectic, maieutics, generalization, and counterfactual reasoning are discussed for their application in engineering prompt templates and their connections to inductive, deductive, and abductive reasoning. Through examples, the effectiveness of these dialogue and reasoning methods is demonstrated. An interesting observation is made that when the task's goal and user intent are conveyed to GPT-3 via ChatGPT before the start of a dialogue, the large language model seems to connect to the external context expressed in the intent and perform more effectively.",
            "year": 2023,
            "citationCount": 20,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An interesting observation is made that when the task's goal and user intent are conveyed to GPT-3 via ChatGPT before the start of a dialogue, the large language model seems to connect to the external context expressed in the intent and perform more effectively."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the factuality and coherence of text generated by large language models, and the proposed approach is using Socratic probing to guide the generation process without relying on external resources or additional training.\n\nThe research problem in the paper is developing effective prompt templates for interacting with large language models, and the proposed approach is using various Socratic methods such as definition, elenchus, dialectic, maieutics, generalization, and counterfactual reasoning.\n\nWhile both the proposal and the paper involve using the Socratic method with large language models, the research problems and approaches are different. The proposal focuses on improving the factuality and coherence of generated text, while the paper focuses on developing effective prompt templates for interaction.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "12dc76b5f04fb1fc34f94f26632d193305a97e7e",
            "paperId": "12dc76b5f04fb1fc34f94f26632d193305a97e7e",
            "title": "SocREval: Large Language Models with the Socratic Method for Reference-Free Reasoning Evaluation",
            "abstract": "To comprehensively gauge the capacity of current models for complex reasoning, it is crucial to assess their step-by-step reasoning in a scalable manner. Established reference-based evaluation metrics rely on human-annotated reasoning chains as references to assess the model-derived chains. However, such\"gold-standard\"human-written reasoning chains may not be unique and their acquisition is often labor-intensive. Existing reference-free reasoning evaluation metrics, while eliminating the need for human-crafted reasoning chains as references, often require fine-tuning with human-derived chains before evaluation, complicating the process and questioning their adaptability to other datasets. To address these challenges, we harness GPT-4 to automatically evaluate reasoning chain quality, thereby removing the dependency on human-written reasoning chains for both model fine-tuning and evaluative purposes. Leveraging the Socratic method, we develop SocREval ({\\bf Soc}ratic Method-Inspired {\\bf R}easoning {\\bf Eval}uation), a novel approach for prompt design in reference-free reasoning evaluation. Empirical results from four human annotated datasets reveal that SocREval significantly improves GPT-4's performance, surpassing existing reference-free and reference-based reasoning evaluation metrics. Beyond its demonstrated efficacy, SocREval, proves to be both cost-efficient and robust to prompt writing and example selection, as substantiated by our in-depth analysis.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "SocREval, a novel approach for prompt design in reference-free reasoning evaluation, harnesses GPT-4 to automatically evaluate reasoning chain quality, thereby removing the dependency on human-written reasoning chains for both model fine-tuning and evaluative purposes."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the factuality and coherence of long-form text generated by language models using Socratic probing, without relying on external resources or additional training.\n\nThe paper abstract proposes SocREval, a reference-free reasoning evaluation approach that uses GPT-4 and the Socratic method to automatically evaluate the quality of reasoning chains, eliminating the need for human-written reasoning chains.\n\nWhile both the project proposal and the paper abstract involve the use of the Socratic method, they address different research problems. The project proposal focuses on improving the factuality and coherence of generated text, while the paper abstract focuses on evaluating the reasoning capabilities of language models.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "a77f498235f12be4173f87bfca503b597c00f30e",
            "paperId": "a77f498235f12be4173f87bfca503b597c00f30e",
            "title": "Factuality Enhanced Language Models for Open-Ended Text Generation",
            "abstract": "Pretrained language models (LMs) are susceptible to generate text with nonfactual information. In this work, we measure and improve the factual accuracy of large-scale LMs for open-ended text generation. We design the FactualityPrompts test set and metrics to measure the factuality of LM generations. Based on that, we study the factual accuracy of LMs with parameter sizes ranging from 126M to 530B. Interestingly, we find that larger LMs are more factual than smaller ones, although a previous study suggests that larger LMs can be less truthful in terms of misconceptions. In addition, popular sampling algorithms (e.g., top-p) in open-ended text generation can harm the factuality due to the ''uniform randomness'' introduced at every sampling step. We propose the factual-nucleus sampling algorithm that dynamically adapts the randomness to improve the factuality of generation while maintaining quality. Furthermore, we analyze the inefficiencies of the standard training method in learning correct associations between entities from factual text corpus (e.g., Wikipedia). We propose a factuality-enhanced training method that uses TopicPrefix for better awareness of facts and sentence completion as the training objective, which can vastly reduce the factual errors. We release our code and FactualityPrompts benchmark at: https://github.com/nayeon7lee/FactualityPrompt.",
            "year": 2022,
            "citationCount": 100,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work measures and improves the factual accuracy of large-scale LMs for open-ended text generation, and proposes a factuality-enhanced training method that uses TopicPrefix for better awareness of facts and sentence completion as the training objective, which can vastly reduce the factual errors."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the factuality and coherence of text generated by language models using Socratic probing, while the paper focuses on measuring and improving the factual accuracy of language models for open-ended text generation using a new test set, sampling algorithm, and training method.\n\nThe proposal aims to use Socratic questioning to guide language models during generation, while the paper proposes a new sampling algorithm (factual-nucleus sampling) and a factuality-enhanced training method using TopicPrefix and sentence completion.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "3a89e289e2dd29f5e52a2bf354a637762b661257",
            "paperId": "3a89e289e2dd29f5e52a2bf354a637762b661257",
            "title": "Fine-tuning Language Models for Factuality",
            "abstract": "The fluency and creativity of large pre-trained language models (LLMs) have led to their widespread use, sometimes even as a replacement for traditional search engines. Yet language models are prone to making convincing but factually inaccurate claims, often referred to as 'hallucinations.' These errors can inadvertently spread misinformation or harmfully perpetuate misconceptions. Further, manual fact-checking of model responses is a time-consuming process, making human factuality labels expensive to acquire. In this work, we fine-tune language models to be more factual, without human labeling and targeting more open-ended generation settings than past work. We leverage two key recent innovations in NLP to do so. First, several recent works have proposed methods for judging the factuality of open-ended text by measuring consistency with an external knowledge base or simply a large model's confidence scores. Second, the direct preference optimization algorithm enables straightforward fine-tuning of language models on objectives other than supervised imitation, using a preference ranking over possible model responses. We show that learning from automatically generated factuality preference rankings, generated either through existing retrieval systems or our novel retrieval-free approach, significantly improves the factuality (percent of generated claims that are correct) of Llama-2 on held-out topics compared with RLHF or decoding strategies targeted at factuality. At 7B scale, compared to Llama-2-chat, we observe 58% and 40% reduction in factual error rate when generating biographies and answering medical questions, respectively.",
            "year": 2023,
            "citationCount": 56,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that learning from automatically generated factuality preference rankings, generated either through existing retrieval systems or the novel retrieval-free approach, significantly improves the factuality of Llama-2 on held-out topics compared with RLHF or decoding strategies targeted at factuality."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the factuality and coherence of text generated by language models, while the approach is using Socratic probing to guide the generation process without relying on external resources or additional training.\n\nThe research problem in the paper is also improving the factuality of language model outputs, but the approach is fine-tuning the models using automatically generated factuality preference rankings, either through existing retrieval systems or a novel retrieval-free approach.\n\nWhile both works aim to improve the factuality of language model outputs, the proposal focuses on an inference-time approach using Socratic probing, while the paper proposes a training-time approach using fine-tuning with factuality preference rankings.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "6fd7184bc15bf694902c0e465692e5bec533f69f",
            "paperId": "6fd7184bc15bf694902c0e465692e5bec533f69f",
            "title": "Improving Socratic Question Generation using Data Augmentation and Preference Optimization",
            "abstract": "The Socratic method is a way of guiding students toward solving a problem independently without directly revealing the solution to the problem. Although this method has been shown to significantly improve student learning outcomes, it remains a complex labor-intensive task for instructors. Large language models (LLMs) can be used to augment human effort by automatically generating Socratic questions for students. However, existing methods that involve prompting these LLMs sometimes produce invalid outputs, e.g., those that directly reveal the solution to the problem or provide irrelevant or premature questions. To alleviate this problem, inspired by reinforcement learning with AI feedback (RLAIF), we first propose a data augmentation method to enrich existing Socratic questioning datasets with questions that are invalid in specific ways. Next, we propose a method to optimize open-source LLMs such as LLama 2 to prefer ground-truth questions over generated invalid ones, using direct preference optimization (DPO). Our experiments on a Socratic questions dataset for student code debugging show that a DPO-optimized 7B LLama 2 model can effectively avoid generating invalid questions, and as a result, outperforms existing state-of-the-art prompting methods.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A data augmentation method to enrich existing Socratic questioning datasets with questions that are invalid in specific ways and a method to optimize open-source LLMs such as LLama 2 to prefer ground-truth questions over generated invalid ones, using direct preference optimization (DPO)."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the factuality and coherence of text generated by language models, and the proposed approach is using Socratic probing to guide the generation process without relying on external resources or additional training.\n\nThe research problem in the paper is improving the quality of automatically generated Socratic questions by language models, and the proposed approach is using data augmentation to enrich existing datasets with invalid questions and optimizing the model to prefer ground-truth questions over invalid ones.\n\nThe two works have different research problems (improving generated text factuality vs. improving generated Socratic question quality) and different approaches (Socratic probing during generation vs. data augmentation and preference optimization).\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "13b5b69355555e0c8b702261c5de3b4172ba653c",
            "paperId": "13b5b69355555e0c8b702261c5de3b4172ba653c",
            "title": "The Art of SOCRATIC QUESTIONING: Zero-shot Multimodal Reasoning with Recursive Thinking and Self-Questioning",
            "abstract": "Chain-of-Thought prompting (CoT) enables large-scale language models to solve complex reasoning problems by decomposing the problem and tackling it step-by-step. However, Chain-of-Thought is a greedy thinking process that requires the language model to come up with a starting point and generate the next step solely based on previous steps. This thinking process is different from how humans approach a complex problem e.g., we proactively raise sub-problems related to the original problem and recursively answer them. In this work, we propose S OCRATIC Q UESTIONING , a divide-and-conquer fashion algorithm that simulates the self-questioning and recursive thinking process. S OCRATIC Q UESTIONING is driven by a S ELF -Q UESTIONING module that employs a large-scale language model to propose sub-problems related to the original problem as intermediate steps and S OCRATIC Q UESTION - ING recursively backtracks and answers the sub-problems until reaches the original problem. We apply our proposed algorithm to the visual question-answering task as a case study and by evaluating it on three public benchmark datasets, we observe a significant performance improvement over all baselines on (almost) all datasets. In addition, the qualitative analysis clearly demonstrates the intermediate thinking steps elicited by S OCRATIC Q UESTIONING are similar to the human\u2019s recursively thinking process of a complex reasoning problem.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Qualitative analysis clearly demonstrates the intermediate thinking steps elicited by S OCRATIC Q UESTIONING are similar to the human\u2019s recursively thinking process of a complex reasoning problem."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the factuality and coherence of long-form text generated by language models using Socratic probing, which involves iteratively asking the model targeted questions about the generated text to refine the output.\n\nThe paper abstract proposes a divide-and-conquer algorithm called SOCRATIC QUESTIONING that simulates self-questioning and recursive thinking to solve complex reasoning problems in visual question-answering tasks.\n\nWhile both the project proposal and the paper abstract involve the concept of Socratic questioning, they differ in their research problems and approaches. The project proposal focuses on improving text generation factuality and coherence, while the paper abstract addresses complex reasoning in visual question-answering. The project proposal uses Socratic probing to refine generated text, whereas the paper abstract employs a divide-and-conquer algorithm for recursive thinking.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "8fd462f6248d5e3f1b6602697c09489086b5655f",
            "paperId": "8fd462f6248d5e3f1b6602697c09489086b5655f",
            "title": "Distilling Reasoning Capabilities into Smaller Language Models",
            "abstract": "Step-by-step reasoning approaches like chain of thought (CoT) have proved to be very effective in inducing reasoning capabilities in large language models. However, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get CoT to work. In this paper, we propose a knowledge distillation approach that leverages the step-by-step CoT reasoning capabilities of larger models and distills these abilities into smaller models. In this work, we propose an alternative reasoning scheme, Socratic CoT, that learns a decomposition of the original problem into a sequence of subproblems and uses it to guide the intermediate reasoning steps. We use Socratic CoT to train a combination of two small distilled models: a problem decomposer and a subproblem solver. In practice, given a new problem, the two distilled models work in sync to decompose and solve complex problems. On multiple reasoning datasets (GSM8K, StrategyQA, and SVAMP), our proposed distillation strategies boosts the performance of smaller models over 70% compared to the baselines. Finally, we investigate when Socratic CoT is an effective alternative to CoT, demonstrating cases where a much smaller model (GPT-2 large) can outperform a 10X larger model (GPT-3 6B). Our code is available here: https://github.com/kumar-shridhar/Distiiling-LM",
            "year": 2022,
            "citationCount": 61,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes an alternative reasoning scheme, Socratic CoT, that learns a decomposition of the original problem into a sequence of subproblems and uses it to guide the intermediate reasoning steps, and uses this to train a combination of two small distilled models."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the factuality and coherence of text generated by language models, and the proposed approach is using Socratic probing to guide the generation process without relying on external resources or additional training.\n\nThe research problem in the paper is inducing reasoning capabilities in smaller language models, and the proposed approach is using knowledge distillation from larger models that perform step-by-step reasoning, along with a Socratic questioning-based problem decomposition scheme.\n\nWhile both works involve using Socratic questioning to improve language model performance, the specific research problems and approaches are different. The proposal focuses on improving factuality and coherence in generated text, while the paper aims to distill reasoning capabilities into smaller models.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "e17d250eca28e6256a8f3b07442eb2b5ed3cabac",
            "paperId": "e17d250eca28e6256a8f3b07442eb2b5ed3cabac",
            "title": "AiSocrates: Towards Answering Ethical Quandary Questions",
            "abstract": "Considerable advancements have been made in various NLP tasks based on the impressive power of large pre-trained language models (LLMs). These results have inspired efforts to understand the limits of LLMs so as to evaluate how far we are from achieving human level general natural language understanding. In this work, we challenge the capability of LLMs with the new task of E THICAL Q UANDARY G ENERATIVE Q UES - TION A NSWERING . Ethical quandary questions are more challenging to address because multiple con\ufb02icting answers may exist to a single quandary. We propose a system, A I S OCRATES , that provides an answer with a deliberative exchange of different perspectives to an ethical quandary, in the approach of Socratic philosophy, instead of providing a closed answer like an oracle. A I S OCRATES searches for different ethical principles applicable to the ethical quandary and generates an answer conditioned on the chosen principles through prompt-based few-shot learning. We also address safety concerns by providing a human controllability option in choosing ethical principles. We show that A I S OCRATES generates promising answers to ethical quandary questions with multiple perspectives, 6.92% more often than answers written by human philosophers by one measure, but the system still needs improvement to match the coherence of human philosophers fully. We argue that A I S OCRATES is a promising step toward developing an NLP system that incorporates human values explicitly by prompt instructions. We are releasing the code for research purposes.",
            "year": 2022,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is argued that A I S OCRATES is a promising step toward developing an NLP system that incorporates human values explicitly by prompt instructions, and addressed safety concerns by providing a human controllability option in choosing ethical principles."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the factuality and coherence of text generated by language models, and the proposed approach is using Socratic probing to guide the generation process without relying on external resources or additional training.\n\nThe research problem in the paper is answering ethical quandary questions, and the proposed approach is using prompt-based few-shot learning to generate answers conditioned on different ethical principles.\n\nThe proposal focuses on factuality and coherence of general text generation, while the paper specifically addresses answering ethical quandary questions. The methods also differ, with the proposal using Socratic probing and the paper using prompt-based few-shot learning with ethical principles.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "0376b7ff6bd5fd3df5dc766cb24f9ca8736ea34e",
            "paperId": "0376b7ff6bd5fd3df5dc766cb24f9ca8736ea34e",
            "title": "Towards Answering Open-ended Ethical Quandary Questions",
            "abstract": "Considerable advancements have been made in various NLP tasks based on the impressive power of large language models (LLMs) and many NLP applications are deployed in our daily lives. In this work, we challenge the capability of LLMs with the new task of Ethical Quandary Generative Question Answering. Ethical quandary questions are more challenging to address because multiple conflicting answers may exist to a single quandary. We explore the current capability of LLMs in providing an answer with a deliberative exchange of different perspectives to an ethical quandary, in the approach of Socratic philosophy, instead of providing a closed answer like an oracle. We propose a model that searches for different ethical principles applicable to the ethical quandary and generates an answer conditioned on the chosen principles through prompt-based few-shot learning. We also discuss the remaining challenges and ethical issues involved in this task and suggest the direction toward developing responsible NLP systems by incorporating human values explicitly.",
            "year": 2022,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A model is proposed that searches for different ethical principles applicable to the ethical quandary and generates an answer conditioned on the chosen principles through prompt-based few-shot learning and suggests the direction toward developing responsible NLP systems by incorporating human values explicitly."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the factuality and coherence of long-form text generated by language models, and the proposed approach is using Socratic probing to guide the generation process. The research problem in the paper is exploring the capability of language models in answering open-ended ethical quandary questions, and the proposed approach is using prompt-based few-shot learning to generate answers conditioned on different ethical principles.\n\nThe proposal focuses on factual and coherent text generation, while the paper focuses on answering ethical quandary questions. The proposal uses Socratic probing to guide the generation process, while the paper uses prompt-based few-shot learning to generate answers based on ethical principles.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "ada81a4de88a6ce474df2e2446ad11fea480616e",
            "paperId": "ada81a4de88a6ce474df2e2446ad11fea480616e",
            "title": "Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language",
            "abstract": "Large pretrained (e.g.,\"foundation\") models exhibit distinct capabilities depending on the domain of data they are trained on. While these domains are generic, they may only barely overlap. For example, visual-language models (VLMs) are trained on Internet-scale image captions, but large language models (LMs) are further trained on Internet-scale text with no images (e.g., spreadsheets, SAT questions, code). As a result, these models store different forms of commonsense knowledge across different domains. In this work, we show that this diversity is symbiotic, and can be leveraged through Socratic Models (SMs): a modular framework in which multiple pretrained models may be composed zero-shot i.e., via multimodal-informed prompting, to exchange information with each other and capture new multimodal capabilities, without requiring finetuning. With minimal engineering, SMs are not only competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, but also enable new applications such as (i) answering free-form questions about egocentric video, (ii) engaging in multimodal assistive dialogue with people (e.g., for cooking recipes) by interfacing with external APIs and databases (e.g., web search), and (iii) robot perception and planning.",
            "year": 2022,
            "citationCount": 380,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Socratic Models (SMs) are shown to be competitive with state-of-the-art zero-shot image captioning and video-to-text retrieval, and enable new applications such as answering free-form questions about egocentric video, and engaging in multimodal assistive dialogue with people."
            },
            "score": 6
        },
        {
            "id": "287d45f2090adba6ed8fc4834a029cd47373a88e",
            "paperId": "287d45f2090adba6ed8fc4834a029cd47373a88e",
            "title": "Can Language Models Employ the Socratic Method? Experiments with Code Debugging",
            "abstract": "When employing the Socratic method of teaching, instructors guide students toward solving a problem on their own rather than providing the solution directly. While this strategy can substantially improve learning outcomes, it is usually time-consuming and cognitively demanding. Automated Socratic conversational agents can augment human instruction and provide the necessary scale, however their development is hampered by the lack of suitable data for training and evaluation. In this paper, we introduce a manually created dataset of multi-turn Socratic advice that is aimed at helping a novice programmer fix buggy solutions to simple computational problems. The dataset is then used for benchmarking the Socratic debugging abilities of a number of language models, ranging from fine-tuning the instruction-based text-to-text transformer Flan-T5 to zero-shot and chain of thought prompting of the much larger GPT-4. The code and datasets are made freely available for research at the link below. https://github.com/taisazero/socratic-debugging-benchmark",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A manually created dataset of multi-turn Socratic advice aimed at helping a novice programmer fix buggy solutions to simple computational problems and is used for benchmarking the Socratic debugging abilities of a number of language models."
            },
            "score": 6
        },
        {
            "id": "bd5deadc58ee45b5e004378ba1d54a96bc947b4a",
            "paperId": "bd5deadc58ee45b5e004378ba1d54a96bc947b4a",
            "title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
            "abstract": "Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly. In this paper, we introduce FACTSCORE, a new evaluation that breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. We conduct an extensive human evaluation to obtain FACTSCOREs of people biographies generated by several state-of-the-art commercial LMs -- InstructGPT, ChatGPT, and the retrieval-augmented PerplexityAI -- and report new analysis demonstrating the need for such a fine-grained score (e.g., ChatGPT only achieves 58%). Since human evaluation is costly, we also introduce an automated model that estimates FACTSCORE using retrieval and a strong language model, with less than a 2% error rate. Finally, we use this automated metric to evaluate 6,500 generations from a new set of 13 recent LMs that would have cost $26K if evaluated by humans, with various findings: GPT-4 and ChatGPT are more factual than public models, and Vicuna and Alpaca are some of the best public models. FACTSCORE is available for public use via `pip install factscore`.",
            "year": 2023,
            "citationCount": 189,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An automated model is introduced that estimates FACTSCORE using retrieval and a strong language model, and is used to evaluate 6,500 generations from a new set of 13 recent LMs that would have cost $26K if evaluated by humans, with various findings."
            },
            "score": 6
        },
        {
            "id": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
            "paperId": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
            "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such\"society of minds\"approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.",
            "year": 2023,
            "citationCount": 206,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer is presented, indicating that this approach significantly enhances mathematical and strategic reasoning across a number of tasks."
            },
            "score": 6
        },
        {
            "id": "f63fdbbdf9005245d960ac1912cf4d0805e274a8",
            "paperId": "f63fdbbdf9005245d960ac1912cf4d0805e274a8",
            "title": "Minimizing Factual Inconsistency and Hallucination in Large Language Models",
            "abstract": "Large Language Models (LLMs) are widely used in critical fields such as healthcare, education, and finance due to their remarkable proficiency in various language-related tasks. However, LLMs are prone to generating factually incorrect responses or\"hallucinations,\"which can lead to a loss of credibility and trust among users. To address this issue, we propose a multi-stage framework that generates the rationale first, verifies and refines incorrect ones, and uses them as supporting references to generate the answer. The generated rationale enhances the transparency of the answer and our framework provides insights into how the model arrived at this answer, by using this rationale and the references to the context. In this paper, we demonstrate its effectiveness in improving the quality of responses to drug-related inquiries in the life sciences industry. Our framework improves traditional Retrieval Augmented Generation (RAG) by enabling OpenAI GPT-3.5-turbo to be 14-25% more faithful and 16-22% more accurate on two datasets. Furthermore, fine-tuning samples based on our framework improves the accuracy of smaller open-access LLMs by 33-42% and competes with RAG on commercial models.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A multi-stage framework that generates the rationale first, verifies and refines incorrect ones, and uses them as supporting references to generate the answer enhances the transparency of the answer and provides insights into how the model arrived at this answer, by using this rationale and the references to the context."
            },
            "score": 6
        },
        {
            "id": "e6745fb621481ccb0ed53c267a37292e499c1b42",
            "paperId": "e6745fb621481ccb0ed53c267a37292e499c1b42",
            "title": "Automatic Generation of Socratic Subquestions for Teaching Math Word Problems",
            "abstract": "Socratic questioning is an educational method that allows students to discover answers to complex problems by asking them a series of thoughtful questions. Generation of didactically sound questions is challenging, requiring understanding of the reasoning process involved in the problem. We hypothesize that such questioning strategy can not only enhance the human performance, but also assist the math word problem (MWP) solvers.In this work, we explore the ability of large language models (LMs) in generating sequential questions for guiding math word problem-solving. We propose various guided question generation schemes based on input conditioning and reinforcement learning.On both automatic and human quality evaluations, we find that LMs constrained with desirable question properties generate superior questions and improve the overall performance of a math word problem solver. We conduct a preliminary user study to examine the potential value of such question generation models in the education domain. Results suggest that the difficulty level of problems plays an important role in determining whether questioning improves or hinders human performance. We discuss the future of using such questioning strategies in education.",
            "year": 2022,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores the ability of large language models (LMs) in generating sequential questions for guiding math word problem-solving and proposes various guided question generation schemes based on input conditioning and reinforcement learning."
            },
            "score": 6
        },
        {
            "id": "9b77d139e530302a537ecd1cf4ef20e3703a0a2b",
            "paperId": "9b77d139e530302a537ecd1cf4ef20e3703a0a2b",
            "title": "Response Generation for Cognitive Behavioral Therapy with Large Language Models: Comparative Study with Socratic Questioning",
            "abstract": "Dialogue systems controlled by predefined or rule-based scenarios derived from counseling techniques, such as cognitive behavioral therapy (CBT), play an important role in mental health apps. Despite the need for responsible responses, it is conceivable that using the newly emerging LLMs to generate contextually relevant utterances will enhance these apps. In this study, we construct dialogue modules based on a CBT scenario focused on conventional Socratic questioning using two kinds of LLMs: a Transformer-based dialogue model further trained with a social media empathetic counseling dataset, provided by Osaka Prefecture (OsakaED), and GPT-4, a state-of-the art LLM created by OpenAI. By comparing systems that use LLM-generated responses with those that do not, we investigate the impact of generated responses on subjective evaluations such as mood change, cognitive change, and dialogue quality (e.g., empathy). As a result, no notable improvements are observed when using the OsakaED model. When using GPT-4, the amount of mood change, empathy, and other dialogue qualities improve significantly. Results suggest that GPT-4 possesses a high counseling ability. However, they also indicate that even when using a dialogue model trained with a human counseling dataset, it does not necessarily yield better outcomes compared to scenario-based dialogues. While presenting LLM-generated responses, including GPT-4, and having them interact directly with users in real-life mental health care services may raise ethical issues, it is still possible for human professionals to produce example responses or response templates using LLMs in advance in systems that use rules, scenarios, or example responses.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study constructs dialogue modules based on a CBT scenario focused on conventional Socratic questioning using two kinds of LLMs: a Transformer-based dialogue model further trained with a social media empathetic counseling dataset, provided by Osaka Prefecture, and GPT-4, a state-of-the art LLM created by OpenAI."
            },
            "score": 6
        },
        {
            "id": "b408c712ab9425462855b8305d614c3421ad1216",
            "paperId": "b408c712ab9425462855b8305d614c3421ad1216",
            "title": "Socratic Questioning of Novice Debuggers: A Benchmark Dataset and Preliminary Evaluations",
            "abstract": "Socratic questioning is a teaching strategy where the student is guided towards solving a problem on their own, instead of being given the solution directly. In this paper, we introduce a dataset of Socratic conversations where an instructor helps a novice programmer fix buggy solutions to simple computational problems. The dataset is then used for benchmarking the Socratic debugging abilities of GPT-based language models. While GPT-4 is observed to perform much better than GPT-3.5, its precision, and recall still fall short of human expert abilities, motivating further work in this area.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a dataset of Socratic conversations where an instructor helps a novice programmer fix buggy solutions to simple computational problems, and uses this dataset for benchmarking the Socratic debugging abilities of GPT-based language models."
            },
            "score": 6
        },
        {
            "id": "1b715867e5933971e5157d1c927a0e59ee9d6bc1",
            "paperId": "1b715867e5933971e5157d1c927a0e59ee9d6bc1",
            "title": "Socratic Reasoning Improves Positive Text Rewriting",
            "abstract": "Reframing a negative into a positive thought is at the crux of several cognitive approaches to mental health and psychotherapy that could be made more accessible by large language model-based solutions. Such reframing is typically non-trivial and requires multiple rationalization steps to uncover the underlying issue of a negative thought and transform it to be more positive. However, this rationalization process is currently neglected by both datasets and models which reframe thoughts in one step. In this work, we address this gap by augmenting open-source datasets for positive text rewriting with synthetically-generated Socratic rationales using a novel framework called \\textsc{SocraticReframe}. \\textsc{SocraticReframe} uses a sequence of question-answer pairs to rationalize the thought rewriting process. We show that such Socratic rationales significantly improve positive text rewriting for different open-source LLMs according to both automatic and human evaluations guided by criteria from psychotherapy research.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work augments open-source datasets for positive text rewriting with synthetically-generated Socratic rationales using a novel framework called SocraticReframe that significantly improve positive text rewriting for different open-source LLMs according to both automatic and human evaluations guided by criteria from psychotherapy research."
            },
            "score": 6
        },
        {
            "id": "5c5fed4aaa79c57924e518ad8932d59340617908",
            "paperId": "5c5fed4aaa79c57924e518ad8932d59340617908",
            "title": "Dog Bite Reflections--Socratic Questioning Revisited.",
            "abstract": "In the online environment, the asynchronous discussion is an important tool for creating community, developing critical thinking skills, and checking for understanding. As students learn how to use Socratic questions for effective interactions, the discussion boards can become the most exciting part of the course. This sequel to the article \u201cDoes Your Dog Bite? Creating Good Questions for Online Discussions,\u201d applies sound communication principles and the prior question of trust to show online instructors how to phrase probing questions to increase comfort for learners\u2019 use. Based on the questions from the original \u201cDoes Your Dog Bite?\u201d article, a variety of prompts are provided for asking probing questions in a non-threatening way.",
            "year": 2015,
            "citationCount": 7,
            "tldr": null,
            "score": 6
        },
        {
            "id": "f495d2741f804cb37a6afc3c039f1d3964b964a8",
            "paperId": "f495d2741f804cb37a6afc3c039f1d3964b964a8",
            "title": "Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization",
            "abstract": "While large language models (LLMs) already achieve strong performance on standard generic summarization benchmarks, their performance on more complex summarization task settings is less studied. Therefore, we benchmark LLMs on instruction controllable text summarization, where the model input consists of both a source article and a natural language requirement for the desired summary characteristics. To this end, we curate an evaluation-only dataset for this task setting and conduct human evaluation on 5 LLM-based summarization systems. We then benchmark LLM-based automatic evaluation for this task with 4 different evaluation protocols and 11 LLMs, resulting in 40 evaluation methods in total. Our study reveals that instruction controllable text summarization remains a challenging task for LLMs, since (1) all LLMs evaluated still make factual and other types of errors in their summaries; (2) all LLM-based evaluation methods cannot achieve a strong alignment with human annotators when judging the quality of candidate summaries; (3) different LLMs show large performance gaps in summary generation and evaluation. We make our collected benchmark, InstruSum, publicly available to facilitate future research in this direction.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study reveals that instruction controllable text summarization remains a challenging task for LLMs, since all LLMs evaluated still make factual and other types of errors in their summaries, and all LLM-based evaluation methods cannot achieve a strong alignment with human annotators when judging the quality of candidate summaries."
            },
            "score": 5
        },
        {
            "id": "f8292d4ddf7a6dfe240eeaa9685f5d18eed9a3f6",
            "paperId": "f8292d4ddf7a6dfe240eeaa9685f5d18eed9a3f6",
            "title": "Language Models that Seek for Knowledge: Modular Search & Generation for Dialogue and Prompt Completion",
            "abstract": "Language models (LMs) have recently been shown to generate more factual responses by employing modularity (Zhou et al., 2021) in combination with retrieval (Adolphs et al., 2021). We extend the recent approach of Adolphs et al. (2021) to include internet search as a module. Our SeeKeR (Search engine->Knowledge->Response) method thus applies a single LM to three modular tasks in succession: search, generating knowledge, and generating a final response. We show that, when using SeeKeR as a dialogue model, it outperforms the state-of-the-art model BlenderBot 2 (Chen et al., 2021) on open-domain knowledge-grounded conversations for the same number of parameters, in terms of consistency, knowledge and per-turn engagingness. SeeKeR applied to topical prompt completions as a standard language model outperforms GPT2 (Radford et al., 2019) and GPT3 (Brown et al., 2020) in terms of factuality and topicality, despite GPT3 being a vastly larger model. Our code and models are made publicly available.",
            "year": 2022,
            "citationCount": 97,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that, when using SeeKeR as a dialogue model, it outperforms the state-of-the-art model BlenderBot 2 on open-domain knowledge-grounded conversations for the same number of parameters, in terms of consistency, knowledge and per-turn engagingness."
            },
            "score": 5
        },
        {
            "id": "76e5069425547d4f53b5aa843a765a305b7fa470",
            "paperId": "76e5069425547d4f53b5aa843a765a305b7fa470",
            "title": "Discursive Socratic Questioning: (Unsupervised) Interpreting Neural Language Models for Discourse Understanding",
            "abstract": "Do neural language models (NLMs) understand 001 the discourse they are processing? Traditional 002 interpretation methods that address this ques-003 tion require pre-annotated explanations, which 004 defeats the purpose of unsupervised explana-005 tion. We propose unsupervised Discursive So-006 cratic Questioning ( D I SQ ), a two-step interpre-007 tative measure. 008 D I SQ first generates Socratic-style questions 009 about the discourse and then queries NLMs 010 about these questions. A model\u2019s understand-011 ing is measured by its responses to these ques-012 tions. We apply D I SQ to examine two fun-013 damental discourse phenomena, namely dis-014 course relation and discourse coherence. We 015 find NLMs demonstrate non-trivial capacities 016 without being trained on any discourse data: 017 Q&A pairs in D I SQ are shown to be evidence 018 for discourse relation and cohesive devices for 019 discourse coherence. D I SQ brings initial evi-020 dence that NLMs understand discourse through 021 reasoning. We find larger models perform bet-022 ter, but contradictions and hallucinations are 023 still problems. We recommend D I SQ as a uni-024 versal diagnostic for discursive NLMs and us-025 ing its output for self-supervision. 026",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "D I SQ brings initial evi-020 dence that NLMs understand discourse through 021 reasoning, and is recommended as a unsupervised versal diagnostic for discursive NLMs and its output for self-supervision."
            },
            "score": 5
        },
        {
            "id": "ebb09a4ba72467ba7f0d3c684abd09c21b63ea03",
            "paperId": "ebb09a4ba72467ba7f0d3c684abd09c21b63ea03",
            "title": "Muse Alpha: Primary Study of AI chatbot for Psychotherapy with Socratic Methods",
            "abstract": "Artificial Intelligence (AI) technology has revolutionized how we interact with information and entertainment with ChatG PT, a language model developed by OpenAI, one of its prominent applications. Recently, chatGPT has used Socratic questioning for medical treatment purposes by avoiding the reinforcement of negative stereotypes from the users (potential patients). However, we introduced an AI chatbot called Muse Alpha, in which Socratic questioning is also applied, to evaluate a client's automatic thinking in cognitive therapy (CT), inducing the client to evaluate whether their automatic thinking is realistically valid and to have more realistic thoughts. In this paper, we have shown three examples of how Socratic dialogue is applied differently between chatGPT and Muse Alpha, and it shows that Muse Alpha seems to achieve the actual moment of cognitive change by uncovering the assumptions and evidence that underpin their thoughts in respect of problems, while socratic dialogues in chatGPT involve asking open-ended questions, just leading to reflection and a new question. We therefore concluded that Muse alpha examines assumptions and explores different perspectives to arrive at a deeper understanding of the topic through critical thinking and discussion.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An AI chatbot called Muse Alpha is introduced, in which Socratic questioning is also applied, to evaluate a client's automatic thinking in cognitive therapy (CT), inducing the client to evaluate whether their automatic thinking is realistically valid and to have more realistic thoughts."
            },
            "score": 5
        },
        {
            "id": "c2f9006993d9d84d48eb894aab3ba60f946d0e15",
            "paperId": "c2f9006993d9d84d48eb894aab3ba60f946d0e15",
            "title": "EduChat: A Large-Scale Language Model-based Chatbot System for Intelligent Education",
            "abstract": "EduChat (https://www.educhat.top/) is a large-scale language model (LLM)-based chatbot system in the education domain. Its goal is to support personalized, fair, and compassionate intelligent education, serving teachers, students, and parents. Guided by theories from psychology and education, it further strengthens educational functions such as open question answering, essay assessment, Socratic teaching, and emotional support based on the existing basic LLMs. Particularly, we learn domain-specific knowledge by pre-training on the educational corpus and stimulate various skills with tool use by fine-tuning on designed system prompts and instructions. Currently, EduChat is available online as an open-source project, with its code, data, and model parameters available on platforms (e.g., GitHub https://github.com/icalk-nlp/EduChat, Hugging Face https://huggingface.co/ecnu-icalk ). We also prepare a demonstration of its capabilities online (https://vimeo.com/851004454). This initiative aims to promote research and applications of LLMs for intelligent education.",
            "year": 2023,
            "citationCount": 29,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This initiative aims to promote research and applications of LLMs for intelligent education by learning domain-specific knowledge by pre-training on the educational corpus and stimulating various skills with tool use by fine- tuning on designed system prompts and instructions."
            },
            "score": 5
        },
        {
            "id": "407a9bb8aac47a37748a581e4e98ddd0156dac97",
            "paperId": "407a9bb8aac47a37748a581e4e98ddd0156dac97",
            "title": "Infusing Knowledge into Large Language Models with Contextual Prompts",
            "abstract": "Knowledge infusion is a promising method for enhancing Large Language Models for domain-specific NLP tasks rather than pre-training models over large data from scratch. These augmented LLMs typically depend on additional pre-training or knowledge prompts from an existing knowledge graph, which is impractical in many applications. In contrast, knowledge infusion directly from relevant documents is more generalisable and alleviates the need for structured knowledge graphs while also being useful for entities that are usually not found in any knowledge graph. With this motivation, we propose a simple yet generalisable approach for knowledge infusion by generating prompts from the context in the input text. Our experiments show the effectiveness of our approach which we evaluate by probing the fine-tuned LLMs.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a simple yet generalisable approach for knowledge infusion by generating prompts from the context in the input text by probing the fine-tuned LLMs."
            },
            "score": 5
        },
        {
            "id": "775c439186b037c09cd9f95b9daf81d23ca21b54",
            "paperId": "775c439186b037c09cd9f95b9daf81d23ca21b54",
            "title": "WinoDict: Probing language models for in-context word acquisition",
            "abstract": "We introduce a new in-context learning paradigm to measure Large Language Models\u2019 (LLMs) ability to learn novel words during inference. In particular, we rewrite Winograd-style co-reference resolution problems by replacing the key concept word with a synthetic but plausible word that the model must understand to complete the task. Solving this task requires the model to make use of the dictionary definition of the new word given in the prompt. This benchmark addresses word acquisition, one important aspect of the diachronic degradation known to afflict LLMs. As LLMs are frozen in time at the moment they are trained, they are normally unable to reflect the way language changes over time. We show that the accuracy of LLMs compared to the original Winograd tasks decreases radically in our benchmark, thus identifying a limitation of current models and providing a benchmark to measure future improvements in LLMs ability to do in-context learning.",
            "year": 2022,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new in-context learning paradigm is introduced to measure Large Language Models\u2019 (LLMs) ability to learn novel words during inference by rewriting Winograd-style co-reference resolution problems by replacing the key concept word with a synthetic but plausible word that the model must understand to complete the task."
            },
            "score": 4
        },
        {
            "id": "c7091540c1fa77f1c6b27482f349330f8e559d6f",
            "paperId": "c7091540c1fa77f1c6b27482f349330f8e559d6f",
            "title": "Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks",
            "abstract": "We consider the questions of whether or not large language models (LLMs) have beliefs, and, if they do, how we might measure them. First, we evaluate two existing approaches, one due to Azaria and Mitchell (2023) and the other to Burns et al. (2022). We provide empirical results that show that these methods fail to generalize in very basic ways. We then argue that, even if LLMs have beliefs, these methods are unlikely to be successful for conceptual reasons. Thus, there is still no lie-detector for LLMs. After describing our empirical results we take a step back and consider whether or not we should expect LLMs to have something like beliefs in the first place. We consider some recent arguments aiming to show that LLMs cannot have beliefs. We show that these arguments are misguided. We provide a more productive framing of questions surrounding the status of beliefs in LLMs, and highlight the empirical nature of the problem. We conclude by suggesting some concrete paths for future work.",
            "year": 2023,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Empirical results show that, even if LLMs have beliefs, these methods are unlikely to be successful for conceptual reasons, and it is argued that there is still no lie-detector for LLMs."
            },
            "score": 4
        },
        {
            "id": "ddc9aeac18638575bbb90ede4c6829ec15c2947e",
            "paperId": "ddc9aeac18638575bbb90ede4c6829ec15c2947e",
            "title": "Prompting as Probing: Using Language Models for Knowledge Base Construction",
            "abstract": "Language Models (LMs) have proven to be useful in various downstream applications, such as summarisation, translation, question answering and text classification. LMs are becoming increasingly important tools in Artificial Intelligence, because of the vast quantity of information they can store. In this work, we present ProP (Prompting as Probing), which utilizes GPT-3, a large Language Model originally proposed by OpenAI in 2020, to perform the task of Knowledge Base Construction (KBC). ProP implements a multi-step approach that combines a variety of prompting techniques to achieve this. Our results show that manual prompt curation is essential, that the LM must be encouraged to give answer sets of variable lengths, in particular including empty answer sets, that true/false questions are a useful device to increase precision on suggestions generated by the LM, that the size of the LM is a crucial factor, and that a dictionary of entity aliases improves the LM score. Our evaluation study indicates that these proposed techniques can substantially enhance the quality of the final predictions: ProP won track 2 of the LM-KBC competition, outperforming the baseline by 36.4 percentage points. Our implementation is available on https://github.com/HEmile/iswc-challenge.",
            "year": 2022,
            "citationCount": 32,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ProP (Prompting as Probing), which utilizes GPT-3, a large Language Model originally proposed by OpenAI in 2020, to perform the task of Knowledge Base Construction (KBC), implements a multi-step approach that combines a variety of prompting techniques to achieve this."
            },
            "score": 4
        },
        {
            "id": "52b785b398e618ed13d9f1c0eb04f7e19894b595",
            "paperId": "52b785b398e618ed13d9f1c0eb04f7e19894b595",
            "title": "Et Machina: Exploring the Use of Conversational Agents Such as ChatGPT in Scientific Writing",
            "abstract": "Background: Advancements in natural language processing have led to the development of powerful language models like ChatGPT, capable of generating coherent text with minimal human input. While the potential for using ChatGPT in scientific paper writing is of growing interest, its full potential and usefulness remain unexplored. Method: We investigated the use of ChatGPT to write scientific papers on the topic of conversational agents in scientific writing. ChatGPT served as a tool to facilitate scientific writing, and we evaluated the quality, factuality, and coherence of the generated text. In this paper, we include information about the generation, processing, or adjustments made to the text in differently-colored blocks. \u2217 Position: We argue that large-scale trained conversational agents like ChatGPT can generate concise, grammatically correct, and well-structured text. However, occasional errors in factual accuracy, clarity, and scientific rigor were observed. We acknowledge the potential of ChatGPT and similar models as tools for generating initial drafts of scientific work. Conclusion: ChatGPT demonstrates promise as a scientific writing tool, but unsupervised use in this context is not yet recommended. Further research is necessary to enhance the accuracy and scientific validity of the generated text. Researchers should exercise caution and view ChatGPT as a complement to human writing rather than a replacement. We provide key recommendations for using conversational agents in scientific writing, including human oversight and training the model on scientific literature and unbiased data. By utilizing AI in the authorship process, we present an example of how AI involvement can be reported.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is argued that large-scale trained conversational agents like ChatGPT can generate concise, grammatically correct, and well-structured text, however, occasional errors in factual accuracy, clarity, and scientific rigor were observed."
            },
            "score": 4
        },
        {
            "id": "f70bf522a90c09ed06c32c9bf36b7ee14b8a9856",
            "paperId": "f70bf522a90c09ed06c32c9bf36b7ee14b8a9856",
            "title": "Knowledge Graph-Augmented Language Models for Knowledge-Grounded Dialogue Generation",
            "abstract": "Language models have achieved impressive performances on dialogue generation tasks. However, when generating responses for a conversation that requires factual knowledge, they are far from perfect, due to an absence of mechanisms to retrieve, encode, and reflect the knowledge in the generated responses. Some knowledge-grounded dialogue generation methods tackle this problem by leveraging facts from Knowledge Graphs (KGs); however, they do not guarantee that the model utilizes a relevant piece of knowledge from the KG. To overcome this limitation, we propose SUbgraph Retrieval-augmented GEneration (SURGE), a framework for generating context-relevant and knowledge-grounded dialogues with the KG. Specifically, our SURGE framework first retrieves the relevant subgraph from the KG, and then enforces consistency across facts by perturbing their word embeddings conditioned by the retrieved subgraph. Then, we utilize contrastive learning to ensure that the generated texts have high similarity to the retrieved subgraphs. We validate our SURGE framework on OpendialKG and KOMODIS datasets, showing that it generates high-quality dialogues that faithfully reflect the knowledge from KG.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The proposed SUbgraph Retrieval-augmented GEneration (SURGE), a framework for generating context-relevant and knowledge-grounded dialogues with the KG, first retrieves the relevant subgraph from the KGs, and then enforces consistency across facts by perturbing their word embeddings conditioned by the retrieved subgraph."
            },
            "score": 4
        },
        {
            "id": "6fb5c0eff3696ef252aca9638e10176ecce7cecb",
            "paperId": "6fb5c0eff3696ef252aca9638e10176ecce7cecb",
            "title": "Generating Images with Multimodal Language Models",
            "abstract": "We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text -- outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence.",
            "year": 2023,
            "citationCount": 85,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces, and exhibits a wider range of capabilities compared to prior multimodal language models."
            },
            "score": 4
        },
        {
            "id": "4923e93f043091da51ac41b039be1698a1158d80",
            "paperId": "4923e93f043091da51ac41b039be1698a1158d80",
            "title": "Causal-CoG: A Causal-Effect Look at Context Generation for Boosting Multi-modal Language Models",
            "abstract": "While Multi-modal Language Models (MLMs) demonstrate impressive multimodal ability, they still struggle on providing factual and precise responses for tasks like visual question answering (VQA). In this paper, we address this challenge from the perspective of contextual information. We propose Causal Context Generation, Causal-CoG, which is a prompting strategy that engages contextual information to enhance precise VQA during inference. Specifically, we prompt MLMs to generate contexts, i.e, text description of an image, and engage the generated contexts for question answering. Moreover, we investigate the advantage of contexts on VQA from a causality perspective, introducing causality filtering to select samples for which contextual information is helpful. To show the effectiveness of Causal-CoG, we run extensive experiments on 10 multimodal benchmarks and show consistent improvements, e.g., +6.30% on POPE, +13.69% on Vizwiz and +6.43% on VQAv2 compared to direct decoding, surpassing existing methods. We hope Casual-CoG inspires explorations of context knowledge in multimodal models, and serves as a plug-and-play strategy for MLM decoding.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Causal Context Generation, Causal-CoG is proposed, which is a prompting strategy that engages contextual information to enhance precise VQA during inference, and hopes it inspires explorations of context knowledge in multimodal models, and serves as a plug-and-play strategy for MLM decoding."
            },
            "score": 4
        },
        {
            "id": "32e84115068a3b62cfec385ba1aac10270756158",
            "paperId": "32e84115068a3b62cfec385ba1aac10270756158",
            "title": "Exploring Critical Questioning Among In-Service ESL Teachers Using Socratic Questioning Technique (SQT)",
            "abstract": "The development of pupils' critical thinking depends on their ability to ask thoughtful questions. Although the advantages of critical questioning to develop students' critical thinking abilities have been widely recognized, teachers' poor questioning abilities become the main area of concern among educators. This paper aims to highlight this issue by exploring the use of Socratic Questioning Technique (SQT) in enhancing in-service teachers\u2019 critical questioning skills when writing reflections. An Action Research Design was adopted using multiple methods such as interview, document analysis and reflections. 13 in-service ESL teachers who enrolled in a one-semester Master of Education course with a focus on English Language Teaching participated in this study. This action research involved three phases which are: problem identification, intervention and evaluation. Two moments of intervention were conducted using Gibbs\u2019 Reflective Model and Socratic Critical Questioning techniques. The data were analyzed thematically. The findings revealed that SQT managed to enhance their critical questioning technique as they started asking variety of questions such as clarification, evidence, reasoning and implication questions during the intervention. The interview also showed that they learned to be more critical, they knew how to ask critical questions and they also became more confident in giving feedback to others. In other words, SQT trained these teachers about dialogic skills which are pertinent for fostering the capacity for critical thought. Thus, SQT should be incorporated in Malaysia's teacher education system .",
            "year": 2023,
            "citationCount": 0,
            "tldr": null,
            "score": 4
        },
        {
            "id": "09b1ff2317cb3b3a24ca098da25721a66c0d0d0e",
            "paperId": "09b1ff2317cb3b3a24ca098da25721a66c0d0d0e",
            "title": "Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4",
            "abstract": "This paper introduces 26 guiding principles designed to streamline the process of querying and prompting large language models. Our goal is to simplify the underlying concepts of formulating questions for various scales of large language models, examining their abilities, and enhancing user comprehension on the behaviors of different scales of large language models when feeding into different prompts. Extensive experiments are conducted on LLaMA-1/2 (7B, 13B and 70B), GPT-3.5/4 to verify the effectiveness of the proposed principles on instructions and prompts design. We hope that this work can provide a better guide for researchers working on the prompting of large language models. Project page is available at https://github.com/VILA-Lab/ATLAS.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "26 guiding principles designed to streamline the process of querying and prompting large language models are introduced and hope that this work can provide a better guide for researchers working on the prompting of large language models."
            },
            "score": 4
        },
        {
            "id": "c7479407bbfec591f9e448f05faa27e154d4d432",
            "paperId": "c7479407bbfec591f9e448f05faa27e154d4d432",
            "title": "Leveraging LLMs for Adaptive Testing and Learning in Taiwan Adaptive Learning Platform (TALP)",
            "abstract": "Artificial Intelligence (AI) and Large Language Models (LLMs) have gained prominence in the educational context, revolutionizing various aspects of teaching and learning. This study focuses on the feasibility of integrating LLMs into the Taiwan Adaptive Learning Platform (TALP) to improve its current adaptive mechanism and enhance the learning experience of students. Through an in-depth exploration, the study identifies several potential benefits of incorporating LLMs into TALP. Firstly, by harnessing the power of LLMs and combining them with the existing knowledge structure in TALP, qualitative responses from open-ended questions can be analyzed more effectively. This enables a more precise assessment of students' understanding and significantly reduces the number of unnecessary testing items, saving valuable time and resources. Additionally, the integration of a chatbot into TALP's diagnostic report provides an innovative approach for scaffolding during remediation. The chatbot can engage in Socratic interactions with students, guiding them through the learning process and addressing misconceptions in real-time. This personalized support fosters a deeper understanding of the material and facilitates more effective remediation. Furthermore, the study highlights the potential of LLMs in detecting and addressing individual learning weaknesses. By leveraging the deep interaction capabilities of LLMs, TALP can analyze student responses and identify cross-grade misconceptions more efficiently. This study also provides examples of how GPT-3.5 can be applied for the above purposes. Finally, the implementation of LLMs in TALP also presents challenges, which are discussed. In conclusion, integrating LLMs into TALP holds great potential to enhance its adaptive mechanism, provide personalized learning experiences, and address individual learning weaknesses.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "In conclusion, integrating LLMs into TALP holds great potential to enhance its adaptive mechanism, provide personalized learning experiences, and address individual learning weaknesses."
            },
            "score": 4
        },
        {
            "id": "0de7c8acf94edab2f1098619905f9e4419dd81c0",
            "paperId": "0de7c8acf94edab2f1098619905f9e4419dd81c0",
            "title": "RLRF:Reinforcement Learning from Reflection through Debates as Feedback for Bias Mitigation in LLMs",
            "abstract": "Biases and stereotypes in Large Language Models (LLMs) can have negative implications for user experience and societal outcomes. Current approaches to bias mitigation like Reinforcement Learning from Human Feedback (RLHF) rely on costly manual feedback. While LLMs have the capability to understand logic and identify biases in text, they often struggle to effectively acknowledge and address their own biases due to factors such as prompt influences, internal mechanisms, and policies. We found that informing LLMs that the content they generate is not their own and questioning them about potential biases in the text can significantly enhance their recognition and improvement capabilities regarding biases. Based on this finding, we propose RLRF (Reinforcement Learning from Reflection through Debates as Feedback), replacing human feedback with AI for bias mitigation. RLRF engages LLMs in multi-role debates to expose biases and gradually reduce biases in each iteration using a ranking scoring mechanism. The dialogue are then used to create a dataset with high-bias and low-bias instances to train the reward model in reinforcement learning. This dataset can be generated by the same LLMs for self-reflection or a superior LLMs guiding the former in a student-teacher mode to enhance its logical reasoning abilities. Experimental results demonstrate the significant effectiveness of our approach in bias reduction.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that informing LLMs that the content they generate is not their own and questioning them about potential biases in the text can significantly enhance their recognition and improvement capabilities regarding biases."
            },
            "score": 4
        },
        {
            "id": "de2fd685f45ee916b9142bcb983d306b7da643a4",
            "paperId": "de2fd685f45ee916b9142bcb983d306b7da643a4",
            "title": "A Prompting-based Approach for Adversarial Example Generation and Robustness Enhancement",
            "abstract": "Recent years have seen the wide application of NLP models in crucial areas such as finance, medical treatment, and news media, raising concerns of the model robustness and vulnerabilities. In this paper, we propose a novel prompt-based adversarial attack to compromise NLP models and robustness enhancement technique. We first construct malicious prompts for each instance and generate adversarial examples via mask-and-filling under the effect of a malicious purpose. Our attack technique targets the inherent vulnerabilities of NLP models, allowing us to generate samples even without interacting with the victim NLP model, as long as it is based on pre-trained language models (PLMs). Furthermore, we design a prompt-based adversarial training method to improve the robustness of PLMs. As our training method does not actually generate adversarial samples, it can be applied to large-scale training sets efficiently. The experimental results show that our attack method can achieve a high attack success rate with more diverse, fluent and natural adversarial examples. In addition, our robustness enhancement method can significantly improve the robustness of models to resist adversarial attacks. Our work indicates that prompting paradigm has great potential in probing some fundamental flaws of PLMs and fine-tuning them for downstream tasks.",
            "year": 2022,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel prompt-based adversarial attack to compromise NLP models and robustness enhancement technique that can significantly improve the robustness of models to resist adversarial attacks and indicates that prompting paradigm has great potential in probing some fundamental flaws of PLMs and fine-tuning them for downstream tasks."
            },
            "score": 4
        },
        {
            "id": "acc277e6408dab69e3bf722819d561567fc9f8a6",
            "paperId": "acc277e6408dab69e3bf722819d561567fc9f8a6",
            "title": "Learning to Generalize for Cross-domain QA",
            "abstract": "There have been growing concerns regarding the out-of-domain generalization ability of natural language processing (NLP) models, particularly in question-answering (QA) tasks. Current synthesized data augmentation methods for QA are hampered by increased training costs. To address this issue, we propose a novel approach that combines prompting methods and linear probing then fine-tuning strategy, which does not entail additional cost. Our method has been theoretically and empirically shown to be effective in enhancing the generalization ability of both generative and discriminative models. Our approach outperforms state-of-the-art baselines, with an average increase in F1 score of 4.5%-7.9%. Furthermore, our method can be easily integrated into any pre-trained models and offers a promising solution to the under-explored cross-domain QA task. We release our source code at GitHub*.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel approach that combines prompting methods and linear probing then fine-tuning strategy, which has been theoretically and empirically shown to be effective in enhancing the generalization ability of both generative and discriminative models."
            },
            "score": 4
        },
        {
            "id": "ad402080a4aa66ef3c57a46ce4685a47a3cc0a61",
            "paperId": "ad402080a4aa66ef3c57a46ce4685a47a3cc0a61",
            "title": "Quantifying Uncertainty in Natural Language Explanations of Large Language Models",
            "abstract": "Large Language Models (LLMs) are increasingly used as powerful tools for several high-stakes natural language processing (NLP) applications. Recent prompting works claim to elicit intermediate reasoning steps and key tokens that serve as proxy explanations for LLM predictions. However, there is no certainty whether these explanations are reliable and reflect the LLMs behavior. In this work, we make one of the first attempts at quantifying the uncertainty in explanations of LLMs. To this end, we propose two novel metrics -- $\\textit{Verbalized Uncertainty}$ and $\\textit{Probing Uncertainty}$ -- to quantify the uncertainty of generated explanations. While verbalized uncertainty involves prompting the LLM to express its confidence in its explanations, probing uncertainty leverages sample and model perturbations as a means to quantify the uncertainty. Our empirical analysis of benchmark datasets reveals that verbalized uncertainty is not a reliable estimate of explanation confidence. Further, we show that the probing uncertainty estimates are correlated with the faithfulness of an explanation, with lower uncertainty corresponding to explanations with higher faithfulness. Our study provides insights into the challenges and opportunities of quantifying uncertainty in LLM explanations, contributing to the broader discussion of the trustworthiness of foundation models.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes two novel metrics -- verbalized uncertainty and probing uncertainty -- to quantify the uncertainty of generated explanations of large Language Models, and shows that the probing uncertainty estimates are correlated with the faithfulness of an explanation, with lower uncertainty corresponding to explanations with higher faithfulness."
            },
            "score": 4
        },
        {
            "id": "4c5f4ddc68be643fb34ea969bf2c105ff7538995",
            "paperId": "4c5f4ddc68be643fb34ea969bf2c105ff7538995",
            "title": "Can Language Models be Biomedical Knowledge Bases?",
            "abstract": "Pre-trained language models (LMs) have become ubiquitous in solving various natural language processing (NLP) tasks. There has been increasing interest in what knowledge these LMs contain and how we can extract that knowledge, treating LMs as knowledge bases (KBs). While there has been much work on probing LMs in the general domain, there has been little attention to whether these powerful LMs can be used as domain-specific KBs. To this end, we create the BioLAMA benchmark, which is comprised of 49K biomedical factual knowledge triples for probing biomedical LMs. We find that biomedical LMs with recently proposed probing methods can achieve up to 18.51% Acc@5 on retrieving biomedical knowledge. Although this seems promising given the task difficulty, our detailed analyses reveal that most predictions are highly correlated with prompt templates without any subjects, hence producing similar results on each relation and hindering their capabilities to be used as domain-specific KBs. We hope that BioLAMA can serve as a challenging benchmark for biomedical factual probing.",
            "year": 2021,
            "citationCount": 77,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The BioLAMA benchmark is created, which is comprised of 49K biomedical factual knowledge triples for probing biomedical LMs and reveals that most predictions are highly correlated with prompt templates without any subjects, hindering their capabilities to be used as domain-specific KBs."
            },
            "score": 4
        },
        {
            "id": "2c72ab10e7a5f2fd32e6f85b20c77bf64e6e220d",
            "paperId": "2c72ab10e7a5f2fd32e6f85b20c77bf64e6e220d",
            "title": "A prompt-based approach to adversarial example generation and robustness enhancement",
            "abstract": null,
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel robust training approach based on prompt paradigm which incorporates prompt texts as the alternatives to adversarial examples and enhances robustness under a lightweight minimax-style optimization framework is proposed."
            },
            "score": 4
        },
        {
            "id": "174a0e6da0dfb7f96d4a0a4076eed154c439e41a",
            "paperId": "174a0e6da0dfb7f96d4a0a4076eed154c439e41a",
            "title": "Probing Language Models for Understanding of Temporal Expressions",
            "abstract": "We present three Natural Language Inference (NLI) challenge sets that can evaluate NLI models on their understanding of temporal expressions. More specifically, we probe these models for three temporal properties: (a) the order between points in time, (b) the duration between two points in time, (c) the relation between the magnitude of times specified in different units. We find that although large language models fine-tuned on MNLI have some basic perception of the order between points in time, at large, these models do not have a thorough understanding of the relation between temporal expressions.",
            "year": 2021,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that although large language models fine-tuned on MNLI have some basic perception of the order between points in time, at large, these models do not have a thorough understanding of the relation between temporal expressions."
            },
            "score": 3
        },
        {
            "id": "b2c68b708a9f98996b18c8d21b53a815a2c46a8b",
            "paperId": "b2c68b708a9f98996b18c8d21b53a815a2c46a8b",
            "title": "ProPILE: Probing Privacy Leakage in Large Language Models",
            "abstract": "The rapid advancement and widespread use of large language models (LLMs) have raised significant concerns regarding the potential leakage of personally identifiable information (PII). These models are often trained on vast quantities of web-collected data, which may inadvertently include sensitive personal data. This paper presents ProPILE, a novel probing tool designed to empower data subjects, or the owners of the PII, with awareness of potential PII leakage in LLM-based services. ProPILE lets data subjects formulate prompts based on their own PII to evaluate the level of privacy intrusion in LLMs. We demonstrate its application on the OPT-1.3B model trained on the publicly available Pile dataset. We show how hypothetical data subjects may assess the likelihood of their PII being included in the Pile dataset being revealed. ProPILE can also be leveraged by LLM service providers to effectively evaluate their own levels of PII leakage with more powerful prompts specifically tuned for their in-house models. This tool represents a pioneering step towards empowering the data subjects for their awareness and control over their own data on the web.",
            "year": 2023,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ProPILE lets data subjects formulate prompts based on their own PII to evaluate the level of privacy intrusion in LLMs, and can be leveraged by LLM service providers to effectively evaluate their own levels of PII leakage with more powerful prompts specifically tuned for their in-house models."
            },
            "score": 3
        },
        {
            "id": "df2113c867c4836a12dad9c697d11654539ae35e",
            "paperId": "df2113c867c4836a12dad9c697d11654539ae35e",
            "title": "Metaphors in Pre-Trained Language Models: Probing and Generalization Across Datasets and Languages",
            "abstract": "Human languages are full of metaphorical expressions. Metaphors help people understand the world by connecting new concepts and domains to more familiar ones. Large pre-trained language models (PLMs) are therefore assumed to encode metaphorical knowledge useful for NLP systems. In this paper, we investigate this hypothesis for PLMs, by probing metaphoricity information in their encodings, and by measuring the cross-lingual and cross-dataset generalization of this information. We present studies in multiple metaphor detection datasets and in four languages (i.e., English, Spanish, Russian, and Farsi). Our extensive experiments suggest that contextual representations in PLMs do encode metaphorical knowledge, and mostly in their middle layers. The knowledge is transferable between languages and datasets, especially when the annotation is consistent across training and testing sets. Our findings give helpful insights for both cognitive and NLP scientists.",
            "year": 2022,
            "citationCount": 32,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The authors' extensive experiments suggest that contextual representations in PLMs do encode metaphorical knowledge, and mostly in their middle layers, and the knowledge is transferable between languages and datasets, especially when the annotation is consistent across training and testing sets."
            },
            "score": 3
        },
        {
            "id": "468c1d2d8e384472f313ff0487839839727b8934",
            "paperId": "468c1d2d8e384472f313ff0487839839727b8934",
            "title": "Social Bias Probing: Fairness Benchmarking for Language Models",
            "abstract": "Large language models have been shown to encode a variety of social biases, which carries the risk of downstream harms. While the impact of these biases has been recognized, prior methods for bias evaluation have been limited to binary association tests on small datasets, offering a constrained view of the nature of societal biases within language models. In this paper, we propose an original framework for probing language models for societal biases. We collect a probing dataset to analyze language models' general associations, as well as along the axes of societal categories, identities, and stereotypes. To this end, we leverage a novel perplexity-based fairness score. We curate a large-scale benchmarking dataset addressing drawbacks and limitations of existing fairness collections, expanding to a variety of different identities and stereotypes. When comparing our methodology with prior work, we demonstrate that biases within language models are more nuanced than previously acknowledged. In agreement with recent findings, we find that larger model variants exhibit a higher degree of bias. Moreover, we expose how identities expressing different religions lead to the most pronounced disparate treatments across all models.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An original framework for probing language models for societal biases, using a novel perplexity-based fairness score and a large-scale benchmarking dataset addressing drawbacks and limitations of existing fairness collections is proposed."
            },
            "score": 3
        },
        {
            "id": "ddb4d582a5e1e377be8ed220bdbdf8b6fae797d6",
            "paperId": "ddb4d582a5e1e377be8ed220bdbdf8b6fae797d6",
            "title": "DLAMA: A Framework for Curating Culturally Diverse Facts for Probing the Knowledge of Pretrained Language Models",
            "abstract": "A few benchmarking datasets have been released to evaluate the factual knowledge of pretrained language models. These benchmarks (e.g., LAMA, and ParaRel) are mainly developed in English and later are translated to form new multilingual versions (e.g., mLAMA, and mParaRel). Results on these multilingual benchmarks suggest that using English prompts to recall the facts from multilingual models usually yields significantly better and more consistent performance than using non-English prompts. Our analysis shows that mLAMA is biased toward facts from Western countries, which might affect the fairness of probing models. We propose a new framework for curating factual triples from Wikidata that are culturally diverse. A new benchmark DLAMA-v1 is built of factual triples from three pairs of contrasting cultures having a total of 78,259 triples from 20 relation predicates. The three pairs comprise facts representing the (Arab and Western), (Asian and Western), and (South American and Western) countries respectively. Having a more balanced benchmark (DLAMA-v1) supports that mBERT performs better on Western facts than non-Western ones, while monolingual Arabic, English, and Korean models tend to perform better on their culturally proximate facts. Moreover, both monolingual and multilingual models tend to make a prediction that is culturally or geographically relevant to the correct label, even if the prediction is wrong.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a new framework for curating factual triples from Wikidata that are culturally diverse and supports that mBERT performs better on Western facts than non-Western ones, while monolingual Arabic, English, and Korean models tend to perform better on their culturally proximate facts."
            },
            "score": 3
        },
        {
            "id": "08dc4b1f21d90e65be1ec5b94d018e314580aab1",
            "paperId": "08dc4b1f21d90e65be1ec5b94d018e314580aab1",
            "title": "The Amazing World of Neural Language Generation",
            "abstract": "Neural Language Generation (NLG) \u2013 using neural network models to generate coherent text \u2013 is among the most promising methods for automated text creation. Recent years have seen a paradigm shift in neural text generation, caused by the advances in deep contextual language modeling (e.g., LSTMs, GPT, GPT2) and transfer learning (e.g., ELMo, BERT). While these tools have dramatically improved the state of NLG, particularly for low resources tasks, state-of-the-art NLG models still face many challenges: a lack of diversity in generated text, commonsense violations in depicted situations, difficulties in making use of factual information, and difficulties in designing reliable evaluation metrics. In this tutorial, we will present an overview of the current state-of-the-art in neural network architectures, and how they shaped recent research directions in text generation. We will discuss how and why these models succeed/fail at generating coherent text, and provide insights on several applications.",
            "year": 2020,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This tutorial will present an overview of the current state of the art in neural network architectures, and how they shaped recent research directions in text generation, and discuss how and why these models succeed/fail at generating coherent text, and provide insights on several applications."
            },
            "score": 3
        },
        {
            "id": "31c87314bff4e79ce2a0082a8001f4a9f720482b",
            "paperId": "31c87314bff4e79ce2a0082a8001f4a9f720482b",
            "title": "Using Large Language Models for Zero-Shot Natural Language Generation from Knowledge Graphs",
            "abstract": "In any system that uses structured knowledge graph (KG) data as its underlying knowledge representation, KG-to-text generation is a useful tool for turning parts of the graph data into text that can be understood by humans. Recent work has shown that models that make use of pretraining on large amounts of text data can perform well on the KG-to-text task, even with relatively little training data on the specific graph-to-text task. In this paper, we build on this concept by using large language models to perform zero-shot generation based on nothing but the model\u2019s understanding of the triple structure from what it can read. We show that ChatGPT achieves near state-of-the-art performance on some measures of the WebNLG 2020 challenge, but falls behind on others. Additionally, we compare factual, counter-factual and fictional statements, and show that there is a significant connection between what the LLM already knows about the data it is parsing and the quality of the output text.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper uses large language models to perform zero-shot generation based on nothing but the model\u2019s understanding of the triple structure from what it can read, and shows that ChatGPT achieves near state-of-the-art performance on some measures of the WebNLG 2020 challenge, but falls behind on others."
            },
            "score": 3
        },
        {
            "id": "05bcf9999525656cfaa59bc71f8572d771ff3776",
            "paperId": "05bcf9999525656cfaa59bc71f8572d771ff3776",
            "title": "Language Models Can See: Plugging Visual Controls in Text Generation",
            "abstract": "Generative language models (LMs) such as GPT-2/3 can be prompted to generate text with remarkable quality. While they are designed for text-prompted generation, it remains an open question how the generation process could be guided by modalities beyond text such as images. In this work, we propose a training-free framework, called MAGIC (iMAge-Guided text generatIon with CLIP), for plugging in visual controls in the generation process and enabling LMs to perform multimodal tasks (e.g., image captioning) in a zero-shot manner. MAGIC is a simple yet efficient plug-and-play framework, which directly combines an off-the-shelf LM (i.e., GPT-2) and an image-text matching model (i.e., CLIP) for image-grounded text generation. During decoding, MAGIC influences the generation of the LM by introducing a CLIP-induced score, called magic score, which regularizes the generated result to be semantically related to a given image while being coherent to the previously generated context. Notably, the proposed decoding scheme does not involve any gradient update operation, therefore being computationally efficient. On the challenging task of zero-shot image captioning, MAGIC outperforms the state-of-the-art method by notable margins with a nearly 27 times decoding speedup. MAGIC is a flexible framework and is theoretically compatible with any text generation tasks that incorporate image grounding. In the experiments, we showcase that it is also capable of performing visually grounded story generation given both an image and a text prompt.",
            "year": 2022,
            "citationCount": 72,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A training-free framework for plugging in visual controls in the generation process and enabling LMs to perform multimodal tasks (e.g., image captioning) in a zero-shot manner, which outperforms the state-of-the-art method by notable margins with a nearly 27 times decoding speedup."
            },
            "score": 3
        },
        {
            "id": "2d79d2ea57c87912bd46ae8e8bda1b26c4773855",
            "paperId": "2d79d2ea57c87912bd46ae8e8bda1b26c4773855",
            "title": "FiLM: Fill-in Language Models for Any-Order Generation",
            "abstract": "Language models have become the backbone of today's AI systems. However, their predominant left-to-right generation limits the use of bidirectional context, which is essential for tasks that involve filling text in the middle. We propose the Fill-in Language Model (FiLM), a new language modeling approach that allows for flexible generation at any position without adhering to a specific generation order. Its training extends the masked language modeling objective by adopting varying mask probabilities sampled from the Beta distribution to enhance the generative capabilities of FiLM. During inference, FiLM can seamlessly insert missing phrases, sentences, or paragraphs, ensuring that the outputs are fluent and are coherent with the surrounding context. In both automatic and human evaluations, FiLM outperforms existing infilling methods that rely on left-to-right language models trained on rearranged text segments. FiLM is easy to implement and can be either trained from scratch or fine-tuned from a left-to-right language model. Notably, as the model size grows, FiLM's perplexity approaches that of strong left-to-right language models of similar sizes, indicating FiLM's scalability and potential as a large language model.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Fill-in Language Model is proposed, a new language modeling approach that allows for flexible generation at any position without adhering to a specific generation order, and its training extends the masked language modeling objective by adopting varying mask probabilities sampled from the Beta distribution to enhance the generative capabilities of FiLM."
            },
            "score": 3
        },
        {
            "id": "eb2658d7217d350b6027a09dc49e579bc501bb91",
            "paperId": "eb2658d7217d350b6027a09dc49e579bc501bb91",
            "title": "Developing Critical Thinking through Socratic Questioning: An Action Research Study.",
            "abstract": "An action research study was conducted among 24 Form 4 level Malaysian students, aged 16. The duration of the study was five months and constituted 16 one-hour literature lessons (short stories from the secondary level Malaysian English Language Upper Secondary Level school syllabus). This paper describes my experience as a teacher-as-researcher to assist students to respond to teacher questions through Paul\u2019s (1993) model of Socratic Questioning which claims to develop students\u2019 critical thinking. Data was collected through researcher\u2019s field notes, students\u2019 writing tasks and student interviews which were analysed after each cycle of the action research study. Changes and adaptations were consequently made based on the data collected and upon teacher reflection to improve practice. The results of this study indicate that repeated practice of Socratic Questioning had a positive effect on student responses and writing tasks. Some of the factors affecting students\u2019 performance included students\u2019 language proficiency, weak reading ability and students\u2019 anxiety towards the questioning method. These issues had to be addressed and dealt with, before Socratic Questioning could be properly implemented in the classroom. Keywords: Socratic questioning, teacher questioning, critical thinking, action research",
            "year": 2016,
            "citationCount": 17,
            "tldr": null,
            "score": 3
        },
        {
            "id": "c0b9c2dacf76f2a55575291c117fec7162f745d3",
            "paperId": "c0b9c2dacf76f2a55575291c117fec7162f745d3",
            "title": "A Latent-Variable Model for Intrinsic Probing",
            "abstract": "The success of pre-trained contextualized representations has prompted researchers to analyze them for the presence of linguistic information. \nIndeed, it is natural to assume that these pre-trained representations do encode some level of linguistic knowledge as they have brought about large empirical improvements on a wide variety of NLP tasks, which suggests they are learning true linguistic generalization.\nIn this work, we focus on intrinsic probing, an analysis technique where the goal is not only to identify whether a representation encodes a linguistic attribute but also to pinpoint where this attribute is encoded.\nWe propose a novel latent-variable formulation for constructing intrinsic probes and derive a tractable variational approximation to the log-likelihood.\nOur results show that our model is versatile and yields tighter mutual information estimates than two intrinsic probes previously proposed in the literature.\nFinally, we find empirical evidence that pre-trained representations \ndevelop a cross-lingually entangled notion of morphosyntax.",
            "year": 2022,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel latent-variable formulation for constructing intrinsic probes and derives a tractable variational approximation to the log-likelihood and finds empirical evidence that pre-trained representations develop a cross-lingually entangled notion of morphosyntax."
            },
            "score": 3
        },
        {
            "id": "080df61ee1c15ff3c8e5d0d82d60bfd80e372e38",
            "paperId": "080df61ee1c15ff3c8e5d0d82d60bfd80e372e38",
            "title": "Probing Toxic Content in Large Pre-Trained Language Models",
            "abstract": "Large pre-trained language models (PTLMs) have been shown to carry biases towards different social groups which leads to the reproduction of stereotypical and toxic content by major NLP systems. We propose a method based on logistic regression classifiers to probe English, French, and Arabic PTLMs and quantify the potentially harmful content that they convey with respect to a set of templates. The templates are prompted by a name of a social group followed by a cause-effect relation. We use PTLMs to predict masked tokens at the end of a sentence in order to examine how likely they enable toxicity towards specific communities. We shed the light on how such negative content can be triggered within unrelated and benign contexts based on evidence from a large-scale study, then we explain how to take advantage of our methodology to assess and mitigate the toxicity transmitted by PTLMs.",
            "year": 2021,
            "citationCount": 75,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A method based on logistic regression classifiers is proposed to probe English, French, and Arabic PTLMs and quantify the potentially harmful content that they convey with respect to a set of templates to assess and mitigate the toxicity transmitted by PTL Ms."
            },
            "score": 3
        },
        {
            "id": "bf722dc893ddaad5045fca5646212ec3badf3c5a",
            "paperId": "bf722dc893ddaad5045fca5646212ec3badf3c5a",
            "title": "Prompt-Learning for Fine-Grained Entity Typing",
            "abstract": "As an effective approach to tune pre-trained language models (PLMs) for specific tasks, prompt-learning has recently attracted much attention from researchers. By using \\textit{cloze}-style language prompts to stimulate the versatile knowledge of PLMs, prompt-learning can achieve promising results on a series of NLP tasks, such as natural language inference, sentiment classification, and knowledge probing. In this work, we investigate the application of prompt-learning on fine-grained entity typing in fully supervised, few-shot and zero-shot scenarios. We first develop a simple and effective prompt-learning pipeline by constructing entity-oriented verbalizers and templates and conducting masked language modeling. Further, to tackle the zero-shot regime, we propose a self-supervised strategy that carries out distribution-level optimization in prompt-learning to automatically summarize the information of entity types. Extensive experiments on three fine-grained entity typing benchmarks (with up to 86 classes) under fully supervised, few-shot and zero-shot settings show that prompt-learning methods significantly outperform fine-tuning baselines, especially when the training data is insufficient.",
            "year": 2021,
            "citationCount": 113,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work develops a simple and effective prompt-learning pipeline, and proposes a self-supervised strategy that carries out distribution-level optimization in prompt- learning to automatically summarize the information of entity types in the zero-shot regime."
            },
            "score": 3
        },
        {
            "id": "68850153b0210615c86f9a72624f34e2913bcddf",
            "paperId": "68850153b0210615c86f9a72624f34e2913bcddf",
            "title": "Document-Level Machine Translation with Large Language Models",
            "abstract": "Large language models (LLMs) such as ChatGPT can produce coherent, cohesive, relevant, and fluent answers for various natural language processing (NLP) tasks. Taking document-level machine translation (MT) as a testbed, this paper provides an in-depth evaluation of LLMs' ability on discourse modeling. The study focuses on three aspects: 1) Effects of Context-Aware Prompts, where we investigate the impact of different prompts on document-level translation quality and discourse phenomena; 2) Comparison of Translation Models, where we compare the translation performance of ChatGPT with commercial MT systems and advanced document-level MT methods; 3) Analysis of Discourse Modelling Abilities, where we further probe discourse knowledge encoded in LLMs and shed light on impacts of training techniques on discourse modeling. By evaluating on a number of benchmarks, we surprisingly find that LLMs have demonstrated superior performance and show potential to become a new paradigm for document-level translation: 1) leveraging their powerful long-text modeling capabilities, GPT-3.5 and GPT-4 outperform commercial MT systems in terms of human evaluation; 2) GPT-4 demonstrates a stronger ability for probing linguistic knowledge than GPT-3.5. This work highlights the challenges and opportunities of LLMs for MT, which we hope can inspire the future design and evaluation of LLMs.We release our data and annotations at https://github.com/longyuewangdcu/Document-MT-LLM.",
            "year": 2023,
            "citationCount": 53,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An in-depth evaluation of LLMs' ability on discourse modeling using document-level machine translation (MT) as a testbed finds that LLMs have demonstrated superior performance and show potential to become a new paradigm for document- level translation."
            },
            "score": 3
        },
        {
            "id": "5b9bf4a82da690e738821ac0460b96c2770ed5dd",
            "paperId": "5b9bf4a82da690e738821ac0460b96c2770ed5dd",
            "title": "Are Large Language Models Temporally Grounded?",
            "abstract": "Are Large language models (LLMs) temporally grounded? Since LLMs cannot perceive and interact with the environment, it is impossible to answer this question directly. Instead, we provide LLMs with textual narratives and probe them with respect to their common-sense knowledge of the structure and duration of events, their ability to order events along a timeline, and self-consistency within their temporal model (e.g., temporal relations such as after and before are mutually exclusive for any pair of events). We evaluate state-of-the-art LLMs (such as LLaMA 2 and GPT-4) on three tasks reflecting these abilities. Generally, we find that LLMs lag significantly behind both human performance as well as small-scale, specialised LMs. In-context learning, instruction tuning, and chain-of-thought prompting reduce this gap only to a limited degree. Crucially, LLMs struggle the most with self-consistency, displaying incoherent behaviour in at least 27.23% of their predictions. Contrary to expectations, we also find that scaling the model size does not guarantee positive gains in performance. To explain these results, we study the sources from which LLMs may gather temporal information: we find that sentence ordering in unlabelled texts, available during pre-training, is only weakly correlated with event ordering. Moreover, public instruction tuning mixtures contain few temporal tasks. Hence, we conclude that current LLMs lack a consistent temporal model of textual narratives. Code, datasets, and LLM outputs are available at https://github.com/yfqiu-nlp/temporal-llms.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is concluded that current LLMs lack a consistent temporal model of textual narratives, and study the sources from which LLMs may gather temporal information finds that sentence ordering in unlabelled texts is only weakly correlated with event ordering."
            },
            "score": 3
        },
        {
            "id": "30ea884a5b8e4cffe18ebfef6c033e5c37b79eeb",
            "paperId": "30ea884a5b8e4cffe18ebfef6c033e5c37b79eeb",
            "title": "World Models for Math Story Problems",
            "abstract": "Solving math story problems is a complex task for students and NLP models alike, requiring them to understand the world as described in the story and reason over it to compute an answer. Recent years have seen impressive performance on automatically solving these problems with large pre-trained language models and innovative techniques to prompt them. However, it remains unclear if these models possess accurate representations of mathematical concepts. This leads to lack of interpretability and trustworthiness which impedes their usefulness in various applications. In this paper, we consolidate previous work on categorizing and representing math story problems and develop MathWorld, which is a graph-based semantic formalism specific for the domain of math story problems. With MathWorld, we can assign world models to math story problems which represent the situations and actions introduced in the text and their mathematical relationships. We combine math story problems from several existing datasets and annotate a corpus of 1,019 problems and 3,204 logical forms with MathWorld. Using this data, we demonstrate the following use cases of MathWorld: (1) prompting language models with synthetically generated question-answer pairs to probe their reasoning and world modeling abilities, and (2) generating new problems by using the world models as a design space.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper consolidates previous work on categorizing and representing math story problems and develops MathWorld, which is a graph-based semantic formalism specific for the domain of math story Problems, which represents the situations and actions introduced in the text and their mathematical relationships."
            },
            "score": 3
        },
        {
            "id": "0a0b121dcc127be734c5199d121946bbe8b1ae5d",
            "paperId": "0a0b121dcc127be734c5199d121946bbe8b1ae5d",
            "title": "Quantifying confidence shifts in a BERT-based question answering system evaluated on perturbed instances",
            "abstract": "Recent work on transformer-based neural networks has led to impressive advances on multiple-choice natural language processing (NLP) problems, such as Question Answering (QA) and abductive reasoning. Despite these advances, there is limited work still on systematically evaluating such models in ambiguous situations where (for example) no correct answer exists for a given prompt among the provided set of choices. Such ambiguous situations are not infrequent in real world applications. We design and conduct an experimental study of this phenomenon using three probes that aim to \u2018confuse\u2019 the model by perturbing QA instances in a consistent and well-defined manner. Using a detailed set of results based on an established transformer-based multiple-choice QA system on two established benchmark datasets, we show that the model\u2019s confidence in its results is very different from that of an expected model that is \u2018agnostic\u2019 to all choices that are incorrect. Our results suggest that high performance on idealized QA instances should not be used to infer or extrapolate similarly high performance on more ambiguous instances. Auxiliary results suggest that the model may not be able to distinguish between these two situations with sufficient certainty. Stronger testing protocols and benchmarking may hence be necessary before such models are deployed in front-facing systems or ambiguous decision making with significant human impact.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is suggested that high performance on idealized QA instances should not be used to infer or extrapolate similarly high performance on more ambiguous instances, and that stronger testing protocols and benchmarking may be necessary before transformer-based multiple-choice NLP models are deployed in front-facing systems or ambiguous decision making with significant human impact."
            },
            "score": 3
        },
        {
            "id": "189b518d70ad34c8de6f613bf3bd5051077608bc",
            "paperId": "189b518d70ad34c8de6f613bf3bd5051077608bc",
            "title": "Probing Language Models for Common Ground with Visual Representations",
            "abstract": "While large-scale language models have enjoyed great success recently, much remains to be understood about what is encoded in their representations. In this work, we propose a method for characterizing how language representations of concrete nouns relate to the physical appearance of the objects they refer to. Our approach uses a probing model that examines how useful language representations are in discerning between different visual representations. We show evidence of a surprising common ground with the visual domain, \ufb01nding representations of many language models to be useful in retrieving semantically aligned image patches. In control experiments where language and visual representations are intentionally mismatched, we observe much weaker results. Furthermore, we examine the impact of textual context in our experiments, \ufb01nding, for instance, that nouns accompanied by adjectives lead to more accurate retrieval. Finally, we show that the examined models substantially under-perform humans in retrieval. Al-together, our \ufb01ndings shed new empirical insights on language grounding, suggesting that some physical properties are being captured by trained language models, and highlighting large room for future progress.",
            "year": 2020,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A method for characterizing how language representations of concrete nouns relate to the physical appearance of the objects they refer to is proposed, using a probing model that examines how useful language representations are in discerning between different visual representations."
            },
            "score": 2
        },
        {
            "id": "c435ecd0321dcec1f25e458bf930311f9e1d04b6",
            "paperId": "c435ecd0321dcec1f25e458bf930311f9e1d04b6",
            "title": "Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality",
            "abstract": "We present a novel task and dataset for evaluating the ability of vision and language models to conduct visio-linguistic compositional reasoning, which we call Winoground. Given two images and two captions, the goal is to match them correctly-but crucially, both captions contain a completely identical set of words, only in a different order. The dataset was carefully hand-curated by expert annotators and is labeled with a rich set offine-grained tags to assist in analyzing model performance. We probe a diverse range of state-of-the-art vision and language models and find that, surprisingly, none of them do much better than chance. Evidently, these models are not as skilled at visio-linguistic compositional reasoning as we might have hoped. We perform an extensive analysis to obtain insights into how future work might try to mitigate these models' shortcomings. We aim for Winoground to serve as a useful evaluation set for advancing the state of the art and driving further progress in the field. The dataset is available at https://huggingface.co/datasets/facebook/winoground.",
            "year": 2022,
            "citationCount": 219,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel task and dataset for evaluating the ability of vision and language models to conduct visio-linguistic compositional reasoning, which is called Winoground, and it is found that, surprisingly, none of them do much better than chance."
            },
            "score": 2
        },
        {
            "id": "9fe29c834afbe1848d9df713ae6e0ca3bd053605",
            "paperId": "9fe29c834afbe1848d9df713ae6e0ca3bd053605",
            "title": "Probing the Role of Positional Information in Vision-Language Models",
            "abstract": "In most Vision-Language models (VL), the understanding of the image structure is enabled by injecting the position information (PI) about objects in the image. In our case study of LXMERT, a state-of-the-art VL model, we probe the use of the PI in the representation and study its effect on Visual Question Answering. We show that the model is not capable of leveraging the PI for the image-text matching task on a challenge set where only position differs. Yet, our experiments with probing confirm that the PI is indeed present in the representation. We introduce two strategies to tackle this: (i) Positional Information Pre-training and (ii) Contrastive Learning on PI using Cross-Modality Matching. Doing so, the model can correctly classify if images with detailed PI statements match. Additionally to the 2D information from bounding boxes, we introduce the object's depth as new feature for a better object localization in the space. Even though we were able to improve the model properties as defined by our probes, it only has a negligible effect on the downstream performance. Our results thus highlight an important issue of multimodal modeling: the mere presence of information detectable by a probing classifier is not a guarantee that the information is available in a cross-modal setup.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This case study of LXMERT, a state-of-the-art VL model, probes the use of the PI in the representation and studies its effect on Visual Question Answering, highlighting an important issue of multimodal modeling: the mere presence of information detectable by a probing classifier is not a guarantee that the information is available in a cross-modal setup."
            },
            "score": 2
        },
        {
            "id": "384affb888eda2c437207115de0555b19c68664b",
            "paperId": "384affb888eda2c437207115de0555b19c68664b",
            "title": "Probing Conceptual Understanding of Large Visual-Language Models",
            "abstract": "In recent years large visual-language (V+L) models have achieved great success in various downstream tasks. However, it is not well studied whether these models have a conceptual grasp of the visual content. In this work we focus on conceptual understanding of these large V+L models. To facilitate this study, we propose novel benchmarking datasets for probing three different aspects of content understanding, 1) \\textit{relations}, 2) \\textit{composition}, and 3) \\textit{context}. Our probes are grounded in cognitive science and help determine if a V+L model can, for example, determine if snow garnished with a man is implausible, or if it can identify beach furniture by knowing it is located on a beach. We experimented with many recent state-of-the-art V+L models and observe that these models mostly \\textit{fail to demonstrate} a conceptual understanding. This study reveals several interesting insights such as that \\textit{cross-attention} helps learning conceptual understanding, and that CNNs are better with \\textit{texture and patterns}, while Transformers are better at \\textit{color and shape}. We further utilize some of these insights and investigate a \\textit{simple finetuning technique} that rewards the three conceptual understanding measures with promising initial results. The proposed benchmarks will drive the community to delve deeper into conceptual understanding and foster advancements in the capabilities of large V+L models. The code and dataset is available at: \\url{https://tinyurl.com/vlm-robustness}",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes novel benchmarking datasets for probing three different aspects of content understanding, and investigates a \\textit{simple finetuning technique} that rewards the three conceptual understanding measures with promising initial results."
            },
            "score": 2
        },
        {
            "id": "49c9707742c9ec423c1825e61bc3bc08d8812452",
            "paperId": "49c9707742c9ec423c1825e61bc3bc08d8812452",
            "title": "Probing Numeracy and Logic of Language Models of Code",
            "abstract": "Machine learning techniques have found a widespread use in the software engineering community. In particular, language models (LMs) trained on code form the backbone of a majority of these applications, spanning tasks such as code completion, summarization, refactoring, execution prediction, and test generation. These tasks require reasoning about both the syntax and semantics of code. Recent work has shown that language models learn to capture the syntactic properties of code, but it is unclear to what extent they can reason about the semantics of code. In this work, we explore the ability of 3 language models of code to reason about a specific kind of semantics: numerical and logical properties of code. We propose several probing tasks to test the numerical and logical reasoning abilities of these models. We find that the models we explore - CodeBERT, GraphCodeBERT and CodeGen do indeed learn many numerical and logical properties of code, such as finding maximum in a list of numbers, comparing numbers, evaluating boolean expressions and representing numbers. They do not perform as well on complex tasks such as evaluating arithmetic expressions and substituting variables in such expressions. Our results indicate that while these models hold promise, there is a lot of room for improvement of their numeric and logical reasoning abilities.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores the ability of 3 language models of code to reason about a specific kind of semantics: numerical and logical properties of code, and finds that the models - CodeBERT, GraphCodeBERT and CodeGen do indeed learn many numerical and logic properties ofcode."
            },
            "score": 2
        },
        {
            "id": "714740e38dbb0642475ff3eae5681ae0a4103670",
            "paperId": "714740e38dbb0642475ff3eae5681ae0a4103670",
            "title": "GPT4GEO: How a Language Model Sees the World's Geography",
            "abstract": "Large language models (LLMs) have shown remarkable capabilities across a broad range of tasks involving question answering and the generation of coherent text and code. Comprehensively understanding the strengths and weaknesses of LLMs is beneficial for safety, downstream applications and improving performance. In this work, we investigate the degree to which GPT-4 has acquired factual geographic knowledge and is capable of using this knowledge for interpretative reasoning, which is especially important for applications that involve geographic data, such as geospatial analysis, supply chain management, and disaster response. To this end, we design and conduct a series of diverse experiments, starting from factual tasks such as location, distance and elevation estimation to more complex questions such as generating country outlines and travel networks, route finding under constraints and supply chain analysis. We provide a broad characterisation of what GPT-4 (without plugins or Internet access) knows about the world, highlighting both potentially surprising capabilities but also limitations.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A broad characterisation of what GPT-4 (without plugins or Internet access) knows about the world is provided, highlighting both potentially surprising capabilities but also limitations."
            },
            "score": 2
        },
        {
            "id": "b1750d2a6e3480e690999916a86c8b3876577b39",
            "paperId": "b1750d2a6e3480e690999916a86c8b3876577b39",
            "title": "Large Language Models are Frame-level Directors for Zero-shot Text-to-Video Generation",
            "abstract": "In the paradigm of AI-generated content (AIGC), there has been increasing attention to transferring knowledge from pre-trained text-to-image (T2I) models to text-to-video (T2V) generation. Despite their effectiveness, these frameworks face challenges in maintaining consistent narratives and handling shifts in scene composition or object placement from a single abstract user prompt. Exploring the ability of large language models (LLMs) to generate time-dependent, frame-by-frame prompts, this paper introduces a new framework, dubbed DirecT2V. DirecT2V leverages instruction-tuned LLMs as directors, enabling the inclusion of time-varying content and facilitating consistent video generation. To maintain temporal consistency and prevent mapping the value to a different object, we equip a diffusion model with a novel value mapping method and dual-softmax filtering, which do not require any additional training. The experimental results validate the effectiveness of our framework in producing visually coherent and storyful videos from abstract user prompts, successfully addressing the challenges of zero-shot video generation.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "DirecT2V leverages instruction-tuned LLMs as directors, enabling the inclusion of time-varying content and facilitating consistent video generation, and equip a diffusion model with a novel value mapping method and dual-softmax filtering, which do not require any additional training."
            },
            "score": 2
        },
        {
            "id": "011d9ca139dadcacb1c357adefa186fa9bef11bd",
            "paperId": "011d9ca139dadcacb1c357adefa186fa9bef11bd",
            "title": "KL Regularized Normalization Framework for Low Resource Tasks",
            "abstract": "Large pre-trained models, such as Bert, GPT, and Wav2Vec, have demonstrated great potential for learning representations that are transferable to a wide variety of downstream tasks . It is difficult to obtain a large quantity of supervised data due to the limited availability of resources and time. In light of this, a significant amount of research has been conducted in the area of adopting large pre-trained datasets for diverse downstream tasks via fine tuning, linear probing, or prompt tuning in low resource settings. Normalization techniques are essential for accelerating training and improving the generalization of deep neural networks and have been successfully used in a wide variety of applications. A lot of normalization techniques have been proposed but the success of normalization in low resource downstream NLP and speech tasks is limited. One of the reasons is the inability to capture expressiveness by rescaling parameters of normalization. We propose KullbackLeibler(KL) Regularized normalization (KL-Norm) which make the normalized data well behaved and helps in better generalization as it reduces over-fitting, generalises well on out of domain distributions and removes irrelevant biases and features with negligible increase in model parameters and memory overheads. Detailed experimental evaluation on multiple low resource NLP and speech tasks, demonstrates the superior performance of KL-Norm as compared to other popular normalization and regularization techniques.",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "KullbackLeibler(KL) Regularized normalization (KL-Norm) which make the normalized data well behaved and helps in better generalization as it reduces over-fitting, generalises well on out of domain distributions and removes irrelevant biases and features with negligible increase in model parameters and memory overheads is proposed."
            },
            "score": 2
        },
        {
            "id": "d8de74b04f00e603cdc43ec9c5218e11ba8c5841",
            "paperId": "d8de74b04f00e603cdc43ec9c5218e11ba8c5841",
            "title": "Sequence Labeling as Non-Autoregressive Dual-Query Set Generation",
            "abstract": "Sequence labeling is a crucial task in the NLP community that aims at identifying and assigning spans within the input sentence. It has wide applications in various fields such as information extraction, dialogue system, and sentiment analysis. However, previously proposed span-based or sequence-to-sequence models conduct locating and assigning in order, resulting in problems of error propagation and unnecessary training loss, respectively. This paper addresses the problem by reformulating the sequence labeling as a non-autoregressive set generation to realize locating and assigning in parallel. Herein, we propose a Dual-Query Set Generation (DQSetGen) model for unified sequence labeling tasks. Specifically, the dual-query set, including a prompted type query and a positional query with anchor span, is fed into the non-autoregressive decoder to probe the spans which correspond to the positional query and have similar patterns with the type query. By avoiding the autoregressive nature of previous approaches, our method significantly improves efficiency and reduces error propagation. Experimental results illustrate that our approach can obtain superior performance on 5 sub-tasks across 11 benchmark datasets. The non-autoregressive nature of our method allows for parallel computation, achieving faster inference speed than compared baselines. In conclusion, our proposed non-autoregressive dual-query set generation method offers a more efficient and accurate approach to sequence labeling tasks in NLP. Its advantages in terms of performance and efficiency make it a promising solution for various applications in data mining and other related fields.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a Dual-Query Set Generation (DQSetGen) model for unified sequence labeling tasks, reformulating the sequence labeling as a non-autoregressive set generation to realize locating and assigning in parallel."
            },
            "score": 2
        },
        {
            "id": "a92ac3993bc4e8ea51c91550cea4207a9d50b080",
            "paperId": "a92ac3993bc4e8ea51c91550cea4207a9d50b080",
            "title": "Inferring Offensiveness In Images From Natural Language Supervision",
            "abstract": "Probing or fine-tuning (large-scale) pre-trained models results in state-of-the-art performance for many NLP tasks and, more recently, even for computer vision tasks when combined with image data. Unfortunately, these approaches also entail severe risks. In particular, large image datasets automatically scraped from the web may contain derogatory terms as categories and offensive images, and may also underrepresent specific classes. Consequently, there is an urgent need to carefully document datasets and curate their content. Unfortunately, this process is tedious and error-prone. We show that pre-trained transformers themselves provide a methodology for the automated curation of large-scale vision datasets. Based on human-annotated examples and the implicit knowledge of a CLIP based model, we demonstrate that one can select relevant prompts for rating the offensiveness of an image. In addition to e.g. privacy violation and pornographic content previously identified in ImageNet, we demonstrate that our approach identifies further inappropriate and potentially offensive content.",
            "year": 2021,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that pre-trained transformers themselves provide a methodology for the automated curation of large-scale vision datasets and it is demonstrated that one can select relevant prompts for rating the offensiveness of an image."
            },
            "score": 2
        }
    ],
    "novelty": "yes"
}