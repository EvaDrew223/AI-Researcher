{
    "topic_description": "novel prompting methods that can improve factuality and reduce hallucination of large language models",
    "idea_name": "World Representation Prompting",
    "raw_idea": {
        "Problem": "Large language models can generate statements that are inconsistent with how the real world works, as they do not have an explicit representation of the world state during generation.",
        "Existing Methods": "Datasets like bAbI and ProPara evaluate world state tracking in generated text. Baselines include standard language modeling and knowledge-augmented generation.",
        "Motivation": "We can prompt LLMs to generate an explicit world state representation after each sentence generation step, and use it to guide future generation to avoid inconsistencies with the world state.",
        "Proposed Method": "We propose world representation prompting, where after generating each sentence, we prompt the LLM to generate a world state representation in a structured format (e.g., a set of logical predicates or a scene graph). The world state is then used as an additional input to the LLM when generating the next sentence, by constructing a prompt like 'Given the current world state [...], what happens next?'. This encourages the LLM to generate sentences that are consistent with the current world state. To handle multi-step reasoning, we can also prompt the LLM to update the world state representation based on each generated sentence. The generated world states can be used to visualize the reasoning process.",
        "Experiment Plan": "Evaluate on world state tracking datasets like bAbI and ProPara. Compare with baselines like standard language modeling and knowledge-augmented generation. Metrics include accuracy of the generated world states and consistency of the generated text."
    },
    "full_experiment_plan": {
        "Title": "World Representation Prompting: Improving Factuality and Consistency in Language Models via Explicit World State Tracking",
        "Problem Statement": "Large language models can generate statements that are inconsistent with how the real world works, as they do not have an explicit representation of the world state during generation.",
        "Motivation": "Datasets like bAbI and ProPara evaluate world state tracking in generated text. Existing methods like standard language modeling and knowledge-augmented generation still struggle with maintaining consistency with the world state. We hypothesize that prompting LLMs to generate an explicit world state representation after each sentence generation step, and using it to guide future generation, can help avoid inconsistencies with the world state.",
        "Proposed Method": "We propose world representation prompting, where after generating each sentence, we prompt the LLM to generate a world state representation in a structured format (e.g., a set of logical predicates or a scene graph). The world state is then used as an additional input to the LLM when generating the next sentence, by constructing a prompt like 'Given the current world state [...], what happens next?'. This encourages the LLM to generate sentences that are consistent with the current world state. To handle multi-step reasoning, we can also prompt the LLM to update the world state representation based on each generated sentence. The generated world states can be used to visualize the reasoning process.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "Evaluate on world state tracking datasets like bAbI and ProPara. bAbI is a set of 20 tasks for evaluating text understanding, with each task requiring a different type of reasoning (e.g., tracking the location of objects). ProPara is a dataset of procedural text (e.g., science processes) with annotated entity states (e.g., location, existence) after each sentence.",
            "Step 2: Construct Prompts": "For each dataset, construct a prompt template that includes: 1) the input text, 2) a prompt to generate the world state representation after each sentence, and 3) a prompt to generate the next sentence given the current world state. The world state prompt should be tailored to each dataset (e.g., tracking entity states for ProPara, tracking object locations for bAbI). Here are some example prompts:\n\nProPara:\nInput: A seed is planted in the ground. The seed grows into a tree. The tree produces fruit.\nPrompt: After each sentence, represent the state of each entity mentioned so far, in the format of 'Entity: State'.\nSentence 1: A seed is planted in the ground.\nWorld State 1: Seed: planted in ground\nSentence 2: The seed grows into a tree.\nWorld State 2: Seed: grew into tree, Tree: exists\nSentence 3: The tree produces fruit.\nWorld State 3: Seed: grew into tree, Tree: exists, Fruit: produced by tree\n\nbAbI:\nInput: John picked up the apple. John went to the office.\nPrompt: After each sentence, represent the location of each object and person, in the format of 'Entity: Location'.\nSentence 1: John picked up the apple.\nWorld State 1: John: has apple, Apple: with John\nSentence 2: John went to the office.\nWorld State 2: John: in office, Apple: with John\n\nThen for generation, prompt the model with 'Given the current world state of [World State], what happens next in the story?'",
            "Step 3: Select Models": "Evaluate the proposed method on GPT-3 (davinci) and GPT-4 via the OpenAI API. For comparison, also evaluate baseline methods like standard language modeling (i.e., no world state prompting) and knowledge-augmented generation (e.g., retrieving relevant knowledge from Wikipedia for each sentence).",
            "Step 4: Metrics": "For bAbI, report the accuracy of answering the reasoning questions after each story. For ProPara, report the precision, recall, and F1 of the predicted entity states after each sentence, compared to the gold annotations.",
            "Step 5: Analyze Results": "Compare the performance of world representation prompting with the baselines. Analyze the types of reasoning that the proposed method helps with the most (e.g., tracking location vs. existence of entities). Qualitatively examine the generated world states to see if they capture the key information needed for consistent generation. Visualize the world states to see if they provide interpretable reasoning traces."
        },
        "Test Case Examples": {
            "Test Case 1 - Baseline": {
                "Input": "John picked up the apple. John went to the office. Where is the apple?",
                "Expected Output": "John picked up the apple and went to the office. Therefore, the apple is with John in the office.",
                "Explanation": "The baseline language model is able to correctly answer the question by implicitly tracking the location of the apple with John. However, it does not explicitly represent the world state, so it may fail on more complex reasoning tasks."
            },
            "Test Case 2 - Proposed Method": {
                "Input": "John picked up the apple. John went to the office. Sandra picked up the milk. Sandra went to the kitchen. Where is the apple? Where is the milk?",
                "World State Prompts": "After each sentence, represent the location of each object and person, in the format of 'Entity: Location'.",
                "Expected World States": [
                    "Sentence 1: John picked up the apple.\nWorld State 1: John: has apple, Apple: with John",
                    "Sentence 2: John went to the office.\nWorld State 2: John: in office, Apple: with John",
                    "Sentence 3: Sandra picked up the milk.\nWorld State 3: John: in office, Apple: with John, Sandra: has milk, Milk: with Sandra",
                    "Sentence 4: Sandra went to the kitchen.\nWorld State 4: John: in office, Apple: with John, Sandra: in kitchen, Milk: with Sandra"
                ],
                "Expected Output": "Based on the world states, the apple is with John in the office, and the milk is with Sandra in the kitchen.",
                "Explanation": "By explicitly prompting the model to generate world states that track the location of each entity after each sentence, the model can more robustly handle questions that require tracking multiple entities across multiple locations. The generated world states provide a clear reasoning trace to arrive at the final answer."
            }
        },
        "Fallback Plan": "If the proposed world representation prompting does not outperform the baselines, there are a few alternative analyses and experiments to try:\n\n1. Analyze the quality of the generated world states. Are they accurately capturing the key information from each sentence? If not, the prompts may need to be improved to elicit more precise and complete world states from the model.\n\n2. Break down the results by the type of reasoning required (e.g., tracking location, existence, attributes of entities). This can help identify which types of reasoning the proposed method is helping with or struggling with, which can inform further prompt engineering.\n\n3. Experiment with alternative world state representations, such as graph or table structures, to see if they are more effective than the current logical predicate format.\n\n4. Conduct a few-shot experiment where the model is given a small number of annotated examples of world states for each dataset. This can help the model learn the desired format and granularity of the world states.\n\n5. Collect human annotations of world states on a subset of each dataset. Analyze how well the generated world states match the human annotations, and use them to finetune the model to generate more human-like world states."
    },
    "novelty_queries": [
        "KeywordQuery(\"language models world state tracking\")",
        "KeywordQuery(\"language models consistency world representation\")",
        "KeywordQuery(\"language models prompting procedural reasoning\")",
        "KeywordQuery(\"World Representation Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
            "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
            "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
            "year": 2022,
            "citationCount": 3517,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve factuality and consistency in language model generation by prompting LLMs to generate explicit world state representations during the generation process. The paper abstract explores how generating a chain of thought (a series of intermediate reasoning steps) improves the reasoning abilities of large language models.\n\nThe project focuses on using world state tracking to improve consistency in generated text, while the paper focuses on using chain of thought prompting to elicit reasoning abilities in language models. Although both involve prompting strategies, the project addresses factuality and consistency, while the paper addresses complex reasoning tasks.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "62176de125738e3b95850d1227bac81fd646b78e",
            "paperId": "62176de125738e3b95850d1227bac81fd646b78e",
            "title": "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models",
            "abstract": "Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, Few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual efforts, Zero-shot-CoT concatenates the target problem statement with \u201cLet\u2019s think step by step\u201d as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.",
            "year": 2023,
            "citationCount": 115,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results over GPT-3 show that the proposed zero-shot prompting consistently outperforms Zero- shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the factuality and consistency of language model outputs by prompting the model to generate explicit world state representations during the generation process. The proposed approach is called world representation prompting.\n\nThe paper abstract proposes Plan-and-Solve (PS) prompting, which consists of devising a plan to divide the entire task into smaller subtasks and then carrying out the subtasks according to the plan. The goal is to address missing-step errors and improve the quality of generated reasoning steps in zero-shot chain-of-thought reasoning.\n\nWhile both the project proposal and the paper abstract aim to improve the reasoning capabilities of language models, they focus on different aspects. The project proposal targets factuality and consistency by explicitly tracking world states, while the paper abstract addresses missing-step errors and the quality of generated reasoning steps through task decomposition and planning.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "261549439aebdda72b648ecc462448fd24857ac1",
            "paperId": "261549439aebdda72b648ecc462448fd24857ac1",
            "title": "Progressive-Hint Prompting Improves Reasoning in Large Language Models",
            "abstract": "The performance of Large Language Models (LLMs) in reasoning tasks depends heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency being critical methods that enhance this ability. However, these methods do not fully exploit the answers generated by the LLM to guide subsequent responses. This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers. PHP is orthogonal to CoT and self-consistency, making it easy to combine with state-of-the-art techniques to further improve performance. We conducted extensive and comprehensive experiments on seven benchmarks. The results show that PHP significantly improves accuracy while remaining highly efficient. For instance, with text-davinci-003, we observed a 4.2% improvement on GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction in sample paths with self-consistency. With GPT-4 and PHP, we achieve state-of-the-art performances on SVAMP (89.1% ->91.9%), GSM8K (92% ->95.5%), AQuA (76.4% ->79.9%) and MATH (50.3% ->53.9%).",
            "year": 2023,
            "citationCount": 64,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve factuality and consistency in language model generation by prompting LLMs to generate explicit world state representations during the generation process. The paper abstract proposes a new prompting method called Progressive-Hint Prompting (PHP) that uses previously generated answers as hints to guide LLMs towards correct answers in reasoning tasks.\n\nThe project focuses on improving consistency with the real world in open-ended generation, while the paper focuses on improving accuracy in specific reasoning benchmarks. The methods are also different, with the project using world state tracking and the paper using progressive hinting based on previous answers.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "5437e8adab596d7294124c0e798708e050e25321",
            "paperId": "5437e8adab596d7294124c0e798708e050e25321",
            "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
            "abstract": "Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99% using just 14 exemplars, compared to only 16% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.",
            "year": 2022,
            "citationCount": 581,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve factuality and consistency in language model generation by prompting LLMs to generate explicit world state representations during the generation process. The proposed approach is world representation prompting, where the LLM generates a structured world state after each sentence and uses it to guide future generation.\n\nThe paper abstract proposes a novel prompting strategy called least-to-most prompting, which breaks down complex problems into simpler subproblems and solves them in sequence. This approach aims to enable complex reasoning and generalization to harder problems in large language models.\n\nWhile both works aim to improve the reasoning capabilities of language models, the project proposal focuses specifically on improving factuality and consistency by tracking world states, while the paper abstract proposes a general problem-solving strategy for complex reasoning tasks. The approaches are quite different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "be9447ccc05a0e8a07321272778c7574173cf00e",
            "paperId": "be9447ccc05a0e8a07321272778c7574173cf00e",
            "title": "Resprompt: Residual Connection Prompting Advances Multi-Step Reasoning in Large Language Models",
            "abstract": "Chain-of-thought (CoT) prompting, which offers step-by-step problem-solving rationales, has impressively unlocked the reasoning potential of large language models (LLMs). Yet, the standard CoT is less effective in problems demanding multiple reasoning steps. This limitation arises from the complex reasoning process in multi-step problems: later stages often depend on the results of several steps earlier, not just the results of the immediately preceding step. Such complexities suggest the reasoning process is naturally represented as a graph. The almost linear and straightforward structure of CoT prompting, however, struggles to capture this complex reasoning graph. To address this challenge, we propose Residual Connection Prompting (RESPROMPT), a new prompting strategy that advances multi-step reasoning in LLMs. Our key idea is to reconstruct the reasoning graph within prompts. We achieve this by integrating necessary connections-links present in the reasoning graph but missing in the linear CoT flow-into the prompts. Termed\"residual connections\", these links are pivotal in morphing the linear CoT structure into a graph representation, effectively capturing the complex reasoning graphs inherent in multi-step problems. We evaluate RESPROMPT on six benchmarks across three diverse domains: math, sequential, and commonsense reasoning. For the open-sourced LLaMA family of models, RESPROMPT yields a significant average reasoning accuracy improvement of 12.5% on LLaMA-65B and 6.8% on LLaMA2-70B. Breakdown analysis further highlights RESPROMPT particularly excels in complex multi-step reasoning: for questions demanding at least five reasoning steps, RESPROMPT outperforms the best CoT based benchmarks by a remarkable average improvement of 21.1% on LLaMA-65B and 14.3% on LLaMA2-70B. Through extensive ablation studies and analyses, we pinpoint how to most effectively build residual connections.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes Residual Connection Prompting (RESPROMPT), a new prompting strategy that advances multi-step reasoning in LLMs by integrating necessary connections-links present in the reasoning graph but missing in the linear CoT flow-into the prompts."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving factuality and consistency in language model outputs by explicitly tracking the world state during generation. The proposed approach is to prompt the language model to generate a structured world state representation after each sentence and use it to guide the next sentence generation.\n\nThe research problem in the paper is improving multi-step reasoning in language models on complex problems where later steps depend on earlier steps. The proposed approach is to use residual connection prompting to integrate links capturing the reasoning graph structure into the prompts.\n\nThe two works have different research problems (factuality/consistency vs. multi-step reasoning) and different proposed approaches (world state tracking vs. residual connection prompting). Therefore, they are not directly relevant.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
            "paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
            "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "abstract": "Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).",
            "year": 2022,
            "citationCount": 1396,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting that first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve factuality and consistency in language model generation by prompting LLMs to generate explicit world state representations during the generation process. The paper focuses on improving chain-of-thought reasoning in language models by using a self-consistency decoding strategy that samples multiple reasoning paths and selects the most consistent answer.\n\nThe project proposal and the paper have different research problems and approaches:\n- Proposal: Improving factuality and consistency in generation via world state tracking\n- Paper: Improving chain-of-thought reasoning via self-consistency decoding\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "033dd6cf61a6017e9aa9b46068d3c89082849cf3",
            "paperId": "033dd6cf61a6017e9aa9b46068d3c89082849cf3",
            "title": "Tracking the World State with Recurrent Entity Networks",
            "abstract": "We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped with a dynamic long-term memory which allows it to maintain and update a representation of the state of the world as it receives new data. For language understanding tasks, it can reason on-the-fly as it reads text, not just when it is required to answer a question or respond as is the case for a Memory Network (Sukhbaatar et al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer (Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to perform location and content-based read and write operations. However, unlike those models it has a simple parallel architecture in which several memory locations can be updated simultaneously. The EntNet sets a new state-of-the-art on the bAbI tasks, and is the first method to solve all the tasks in the 10k training examples setting. We also demonstrate that it can solve a reasoning task which requires a large number of supporting facts, which other methods are not able to solve, and can generalize past its training horizon. It can also be practically used on large scale datasets such as Children's Book Test, where it obtains competitive performance, reading the story in a single pass.",
            "year": 2016,
            "citationCount": 219,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The EntNet sets a new state-of-the-art on the bAbI tasks, and is the first method to solve all the tasks in the 10k training examples setting, and can generalize past its training horizon."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve factuality and consistency in language model generation by prompting the model to generate explicit world state representations during the generation process. The approach involves using the generated world states to guide future generation steps.\n\nThe paper proposes the Recurrent Entity Network (EntNet), which maintains a dynamic long-term memory to represent and update the state of the world as it receives new information. The model can reason on-the-fly for language understanding tasks.\n\nWhile both works involve tracking world states, the project proposal focuses on using world state representations to guide language model generation, whereas the paper introduces a new model architecture for reasoning in language understanding tasks. The methods and goals are quite different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "b8ca7dfe49501c0dbeeeadbdc94737a5b4fea313",
            "paperId": "b8ca7dfe49501c0dbeeeadbdc94737a5b4fea313",
            "title": "OpenPI2.0: An Improved Dataset for Entity Tracking in Texts",
            "abstract": "Much texts describe a changing world (e.g., procedures, stories, newswires), and understanding them requires tracking how entities change. An earlier dataset, OpenPI, provided crowdsourced annotations of entity state changes in text. However, a major limitation was that those annotations were free-form and did not identify salient changes, hampering model evaluation. To overcome these limitations, we present an improved dataset, OpenPI2.0, where entities and attributes are fully canonicalized and additional entity salience annotations are added. On our fairer evaluation setting, we find that current state-of-the-art language models are far from competent. We also show that using state changes of salient entities as a chain-of-thought prompt, downstream performance is improved on tasks such as question answering and classical planning, outperforming the setting involving all related entities indiscriminately. We offer OpenPI2.0 for the continued development of models that can understand the dynamics of entities in text.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An improved dataset, OpenPI2.0, is presented, where entities and attributes are fully canonicalized and additional entity salience annotations are added, and it is shown that using state changes of salient entities as a chain-of-thought prompt, downstream performance is improved on tasks such as question answering and classical planning."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve factuality and consistency in language models by prompting them to generate explicit world state representations during text generation. The paper presents an improved dataset, OpenPI2.0, for evaluating models on tracking entity state changes in text.\n\nWhile both works involve tracking entities and their states in text, the project proposal focuses on using world state tracking as a way to improve language model generation, while the paper focuses on creating a better dataset for evaluating models on the entity state tracking task itself. The project proposal does not directly aim to create a new dataset.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "1a5a30c055bb6f76abbd559e9bdb836740f2d64c",
            "paperId": "1a5a30c055bb6f76abbd559e9bdb836740f2d64c",
            "title": "Infusing Commonsense World Models with Graph Knowledge",
            "abstract": "While language models have become more capable of producing compelling language, we find there are still gaps in maintaining consistency, especially when describing events in a dynamically changing world. We study the setting of generating narratives in an open world text adventure game, where a graph representation of the underlying game state can be used to train models that consume and output both grounded graph representations and natural language descriptions and actions. We build a large set of tasks by combining crowdsourced and simulated gameplays with a novel dataset of complex actions in order to to construct such models. We find it is possible to improve the consistency of action narration models by training on graph contexts and targets, even if graphs are not present at test time. This is shown both in automatic metrics and human evaluations. We plan to release our code, the new set of tasks, and best performing models.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is possible to improve the consistency of action narration models by training on graph contexts and targets, even if graphs are not present at test time, as shown both in automatic metrics and human evaluations."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve the factuality and consistency of language model outputs by prompting the model to generate explicit world state representations during the generation process. The paper abstract proposes to improve the consistency of action narration models in text adventure games by training on graph contexts and targets.\n\nWhile both works aim to improve the consistency of language model outputs, the project proposal focuses on general language generation and uses world state prompts, while the paper abstract is specific to text adventure games and uses graph representations of the game state.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "748a2700ec11f51560a69ec05c67ca9f97014be7",
            "paperId": "748a2700ec11f51560a69ec05c67ca9f97014be7",
            "title": "EvEntS ReaLM: Event Reasoning of Entity States via Language Models",
            "abstract": "This paper investigates models of event implications. Specifically, how well models predict entity state-changes, by targeting their understanding of physical attributes. Nominally, Large Language models (LLM) have been exposed to procedural knowledge about how objects interact, yet our benchmarking shows they fail to reason about the world. Conversely, we also demonstrate that existing approaches often misrepresent the surprising abilities of LLMs via improper task encodings and that proper model prompting can dramatically improve performance of reported baseline results across multiple tasks. In particular, our results indicate that our prompting technique is especially useful for unseen attributes (out-of-domain) or when only limited data is available.",
            "year": 2022,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results indicate that the prompting technique is especially useful for unseen attributes (out-of-domain) or when only limited data is available and that proper model prompting can dramatically improve performance of reported baseline results across multiple tasks."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve factuality and consistency in language models by prompting them to generate explicit world state representations during text generation. The paper investigates how well language models predict entity state changes by targeting their understanding of physical attributes, and proposes prompting techniques to improve performance on such tasks.\n\nWhile both works involve prompting language models, the project proposal focuses on using world state prompts to guide language models to generate more consistent and factual text, whereas the paper focuses on evaluating and improving language models' ability to reason about entity state changes. The high-level research problems and approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "71fd336f1ca337a638dffb236b432c29cdd19f3d",
            "paperId": "71fd336f1ca337a638dffb236b432c29cdd19f3d",
            "title": "Neuro-Symbolic Causal Language Planning with Commonsense Prompting",
            "abstract": "Language planning aims to implement complex high-level goals by decomposition into sequential simpler low-level steps. Such procedural reasoning ability is essential for applications such as household robots and virtual assistants. Although language planning is a basic skill set for humans in daily life, it remains a challenge for large language models (LLMs) that lack deep-level commonsense knowledge in the real world. Previous methods require either manual exemplars or annotated programs to acquire such ability from LLMs. In contrast, this paper proposes Neuro-Symbolic C ausal LA nguage P lanner (CLAP) that elicits procedural knowledge from the LLMs with commonsense-infused prompting. Pre-trained knowledge in LLMs is essentially an unobserved confounder that causes spurious correlations between tasks and action plans. Through the lens of a Structural Causal Model (SCM), we propose an effective strategy in CLAP to construct prompts as a causal intervention toward our SCM. Using graph sampling techniques and symbolic program executors, our strategy formalizes the structured causal prompts from commonsense knowledge bases. CLAP obtains state-of-the-art performance on WikiHow and RobotHow, achieving a relative improvement of 5 . 28% in human evaluations under the counterfactual setting. This indicates the superiority of CLAP in causal language planning semantically and sequentially.",
            "year": 2022,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes Neuro-Symbolic CLAP that elicits procedural knowledge from the LLMs with commonsense-infused prompting that indicates the superiority of CLAP in causal language planning semantically and sequentially."
            },
            "score": 6
        },
        {
            "id": "9efa81ec4954b0859c47dad8f42edfaf8bced69b",
            "paperId": "9efa81ec4954b0859c47dad8f42edfaf8bced69b",
            "title": "Boosting Language Models Reasoning with Chain-of-Knowledge Prompting",
            "abstract": "Recently, Chain-of-Thought (CoT) prompting has delivered success on complex reasoning tasks, which aims at designing a simple prompt like ``Let's think step by step'' or multiple in-context exemplars with well-designed rationales to elicit Large Language Models (LLMs) to generate intermediate reasoning steps. However, the generated rationales often come with mistakes, making unfactual and unfaithful reasoning chains. To mitigate this brittleness, we propose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple. This is inspired by our human behaviors, i.e., we can draw a mind map or knowledge map as the reasoning evidence in the brain before answering a complex question. Benefiting from CoK, we additionally introduce a F^2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness. For the unreliable response, the wrong evidence can be indicated to prompt the LLM to rethink. Extensive experiments demonstrate that our method can further improve the performance of commonsense, factual, symbolic, and arithmetic reasoning tasks.",
            "year": 2023,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel Chain-of-Knowledge prompting, where it aims at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple, and introduces a F^2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness."
            },
            "score": 6
        },
        {
            "id": "f8b8f926bbfa327c86c40796131fe2695db81126",
            "paperId": "f8b8f926bbfa327c86c40796131fe2695db81126",
            "title": "DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models",
            "abstract": "A long-standing goal of AI systems is to perform complex multimodal reasoning like humans. Recently, large language models (LLMs) have made remarkable strides in such multi-step reasoning on the language modality solely by leveraging the chain of thought (CoT) to mimic human thinking. However, the transfer of these advancements to multimodal contexts introduces heightened challenges, including but not limited to the impractical need for labor-intensive annotation and the limitations in terms of flexibility, generalizability, and explainability. To evoke CoT reasoning in multimodality, this work first conducts an in-depth analysis of these challenges posed by multimodality and presents two key insights:\"keeping critical thinking\"and\"letting everyone do their jobs\"in multimodal CoT reasoning. Furthermore, this study proposes a novel DDCoT prompting that maintains a critical attitude through negative-space prompting and incorporates multimodality into reasoning by first dividing the reasoning responsibility of LLMs into reasoning and recognition and then integrating the visual recognition capability of visual models into the joint reasoning process. The rationales generated by DDCoT not only improve the reasoning abilities of both large and small language models in zero-shot prompting and fine-tuning learning, significantly outperforming state-of-the-art methods but also exhibit impressive generalizability and explainability.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study proposes a novel DDCoT prompting that maintains a critical attitude through negative-space prompting and incorporates multimodality into reasoning by first dividing the reasoning responsibility of LLMs into reasoning and recognition and then integrating the visual recognition capability of visual models into the joint reasoning process."
            },
            "score": 6
        },
        {
            "id": "6ad26eb2d2aa6679d16d9c16fb75cd2cbe1127bc",
            "paperId": "6ad26eb2d2aa6679d16d9c16fb75cd2cbe1127bc",
            "title": "See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning",
            "abstract": "Large pre-trained vision and language models have demonstrated remarkable capacities for various tasks. However, solving the knowledge-based visual reasoning tasks remains challenging, which requires a model to comprehensively understand image content, connect the external world knowledge, and perform step-by-step reasoning to answer the questions correctly. To this end, we propose a novel framework named Interactive Prompting Visual Reasoner (IPVR) for few-shot knowledge-based visual reasoning. IPVR contains three stages, see, think and confirm. The see stage scans the image and grounds the visual concept candidates with a visual perception model. The think stage adopts a pre-trained large language model (LLM) to attend to the key concepts from candidates adaptively. It then transforms them into text context for prompting with a visual captioning model and adopts the LLM to generate the answer. The confirm stage further uses the LLM to generate the supporting rationale to the answer, verify the generated rationale with a cross-modality classifier and ensure that the rationale can infer the predicted output consistently. We conduct experiments on a range of knowledge-based visual reasoning datasets. We found our IPVR enjoys several benefits, 1). it achieves better performance than the previous few-shot learning baselines; 2). it enjoys the total transparency and trustworthiness of the whole reasoning process by providing rationales for each reasoning step; 3). it is computation-efficient compared with other fine-tuning baselines.",
            "year": 2023,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel framework named Interactive Prompting Visual Reasoner (IPVR), which achieves better performance than the previous few-shot learning baselines, and enjoys the total transparency and trustworthiness of the whole reasoning process by providing rationales for each reasoning step."
            },
            "score": 6
        },
        {
            "id": "0875651b68e6602d45ae08bee67cf63c02faa512",
            "paperId": "0875651b68e6602d45ae08bee67cf63c02faa512",
            "title": "Code Prompting: a Neural Symbolic Method for Complex Reasoning in Large Language Models",
            "abstract": "Large language models (LLMs) have scaled up to unlock a wide range of complex reasoning tasks with the aid of various prompting methods. However, current prompting methods generate natural language intermediate steps to help reasoning, which can cause imperfect task reduction and confusion. To mitigate such limitations, we explore code prompting, a neural symbolic prompting method with both zero-shot and few-shot versions which triggers code as intermediate steps. We conduct experiments on 7 widely-used benchmarks involving symbolic reasoning and arithmetic reasoning. Code prompting generally outperforms chain-of-thought (CoT) prompting. To further understand the performance and limitations of code prompting, we perform extensive ablation studies and error analyses, and identify several exclusive advantages of using symbolic promptings compared to natural language. We also consider the ensemble of code prompting and CoT prompting to combine the strengths of both. Finally, we show through experiments how code annotations and their locations affect code prompting.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores code prompting, a neural symbolic prompting method with both zero-shot and few-shot versions which triggers code as intermediate steps, and performs extensive ablation studies and error analyses and identifies several exclusive advantages of using symbolic promptings compared to natural language."
            },
            "score": 6
        },
        {
            "id": "c218cd1772999517b137bbbc9872c4f67e540b7f",
            "paperId": "c218cd1772999517b137bbbc9872c4f67e540b7f",
            "title": "OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models",
            "abstract": "We conduct a thorough investigation into the reasoning capabilities of Large Language Models (LLMs), focusing specifically on the Open Pretrained Transformers (OPT) models as a representative of such models. Our study entails finetuning three different sizes of OPT on a carefully curated reasoning corpus, resulting in two sets of finetuned models: OPT-R, finetuned without explanations, and OPT-RE, finetuned with explanations. We then evaluate all models on 57 out-of-domain tasks drawn from the Super-NaturalInstructions benchmark, covering 26 distinct reasoning skills, utilizing three prompting techniques. Through a comprehensive grid of 27 configurations and 6,156 test evaluations, we investigate the dimensions of finetuning, prompting, and scale to understand the role of explanations on different reasoning skills. Our findings reveal that having explanations in the fewshot exemplar has no significant impact on the model\u2019s performance when the model is finetuned, while positively affecting the non-finetuned counterpart. Moreover, we observe a slight yet consistent increase in classification accuracy as we incorporate explanations during prompting and finetuning, respectively. Finally, we offer insights on which reasoning skills benefit the most from incorporating explanations during finetuning and prompting, such as Numerical (+20.4%) and Analogical (+13.9%) reasoning, as well as skills that exhibit negligible or negative effects.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is revealed that having explanations in the fewshot exemplar has no significant impact on the model\u2019s performance when the model is finetuned, while positively affecting the non-finetuned counterpart, and a slight yet consistent increase in classification accuracy as the authors incorporate explanations during prompting and finetuning."
            },
            "score": 6
        },
        {
            "id": "780a7f5e8ba9b4b451e3dfee1bcfb0f68aba5050",
            "paperId": "780a7f5e8ba9b4b451e3dfee1bcfb0f68aba5050",
            "title": "Multimodal Chain-of-Thought Reasoning in Language Models",
            "abstract": "Large language models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer. However, existing CoT studies have focused on the language modality. We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference. In this way, answer inference can leverage better generated rationales that are based on multimodal information. With Multimodal-CoT, our model under 1 billion parameters outperforms the previous state-of-the-art LLM (GPT-3.5) by 16 percentage points (75.17%->91.68% accuracy) on the ScienceQA benchmark and even surpasses human performance. Code is publicly available available at https://github.com/amazon-science/mm-cot.",
            "year": 2023,
            "citationCount": 182,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference so that answer inference can leverage better generated rationales that are based on multimodal information."
            },
            "score": 6
        },
        {
            "id": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
            "paperId": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
            "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such\"society of minds\"approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.",
            "year": 2023,
            "citationCount": 206,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer is presented, indicating that this approach significantly enhances mathematical and strategic reasoning across a number of tasks."
            },
            "score": 6
        },
        {
            "id": "12c826f4195da172b212a529f8fcf10cc79e35da",
            "paperId": "12c826f4195da172b212a529f8fcf10cc79e35da",
            "title": "Context-faithful Prompting for Large Language Models",
            "abstract": "Large language models (LLMs) encode parametric knowledge about world facts and have shown remarkable performance in knowledge-driven NLP tasks. However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks). In this paper, we seek to assess and enhance LLMs' contextual faithfulness in two aspects: knowledge conflict and prediction with abstention. We demonstrate that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies. In particular, we identify opinion-based prompts and counterfactual demonstrations as the most effective methods. Opinion-based prompts reframe the context as a narrator's statement and inquire about the narrator's opinions, while counterfactual demonstrations use instances containing false facts to improve faithfulness in knowledge conflict situations. Neither technique requires additional training. We conduct experiments on three datasets of two standard NLP tasks, machine reading comprehension and relation extraction, and the results demonstrate significant improvement in faithfulness to contexts. Code and data are released at https://github.com/wzhouad/context-faithful-llm.",
            "year": 2023,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies, and opinion-based prompts and counterfactual demonstrations are identified as the most effective methods."
            },
            "score": 6
        },
        {
            "id": "16d730799318f7e621b835c075d75034a7fc6b8e",
            "paperId": "16d730799318f7e621b835c075d75034a7fc6b8e",
            "title": "Time-Stamped Language Model: Teaching Language Models to Understand The Flow of Events",
            "abstract": "Tracking entities throughout a procedure described in a text is challenging due to the dynamic nature of the world described in the process. Firstly, we propose to formulate this task as a question answering problem. This enables us to use pre-trained transformer-based language models on other QA benchmarks by adapting those to the procedural text understanding. Secondly, since the transformer-based language models cannot encode the flow of events by themselves, we propose a Time-Stamped Language Model (TSLM) to encode event information in LMs architecture by introducing the timestamp encoding. Our model evaluated on the Propara dataset shows improvements on the published state-of-the-art results with a 3.1% increase in F1 score. Moreover, our model yields better results on the location prediction task on the NPN-Cooking dataset. This result indicates that our approach is effective for procedural text understanding in general.",
            "year": 2021,
            "citationCount": 20,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Time-Stamped Language Model (TSLM) is proposed to encode event information in LMs architecture by introducing the timestamp encoding to enable pre-trained transformer-based language models to be used on other QA benchmarks by adapting those to the procedural text understanding."
            },
            "score": 5
        },
        {
            "id": "f8cbcb106a48524edc39df23e2a95f1e6d4c739a",
            "paperId": "f8cbcb106a48524edc39df23e2a95f1e6d4c739a",
            "title": "Examining Inter-Consistency of Large Language Models Collaboration: An In-depth Analysis via Debate",
            "abstract": "Large Language Models (LLMs) have shown impressive capabilities in various applications, but they still face various inconsistency issues. Existing works primarily focus on the inconsistency issues within a single LLM, while we complementarily explore the inter-consistency among multiple LLMs for collaboration. To examine whether LLMs can collaborate effectively to achieve a consensus for a shared goal, we focus on commonsense reasoning, and introduce a formal debate framework (FORD) to conduct a three-stage debate among LLMs with real-world scenarios alignment: fair debate, mismatched debate, and roundtable debate. Through extensive experiments on various datasets, LLMs can effectively collaborate to reach a consensus despite noticeable inter-inconsistencies, but imbalances in their abilities can lead to domination by superior LLMs. Leveraging a more advanced LLM like GPT-4 as an authoritative judge can boost collaboration performance. Our work contributes to understanding the inter-consistency among LLMs and lays the foundation for developing future collaboration methods. Codes and data are available at https://github.com/Waste-Wood/FORD",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a formal debate framework (FORD) to conduct a three-stage debate among LLMs with real-world scenarios alignment: fair debate, mismatched debate, and roundtable debate to examine whether LLMs can collaborate effectively to achieve a consensus for a shared goal."
            },
            "score": 5
        },
        {
            "id": "fbd4a876cee20eaf98f344aca597a55338f663f5",
            "paperId": "fbd4a876cee20eaf98f344aca597a55338f663f5",
            "title": "Examining the Inter-Consistency of Large Language Models: An In-depth Analysis via Debate",
            "abstract": "Large Language Models (LLMs) have demonstrated human-like intelligence and are widely used in various applications. However, LLMs still exhibit various kinds of inconsistency problems. Existing works mainly focus on the inconsistency issues within a single LLM, while we investigate the inter-consistency among multiple LLMs, which is critical for collaborating to solve a complex task. To examine whether LLMs can collaborate to ultimately achieve a consensus for the shared goal and whether LLMs easily change their viewpoints, we introduce a Formal Debate framework (FORD) With FORD, we conduct a three-stage debate aligned with real-world scenarios: fair debate, mismatched debate, and roundtable debate. Through extensive experiments on the commonsense reasoning task, LLMs not only become more inter-consistent but also achieve higher performance. More-over, we observe that stronger LLMs tend to dominate the debates by adhering to their perspectives, while weaker ones are more likely to change viewpoints. Additionally, we highlight the importance of a competent judge, such as GPT-4, to draw more proper conclusions. Our work contributes to understanding the inter-consistency among LLMs and lays the foundation for the development of future collaboration methods.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates the inter-consistency among multiple Large Language Models, and contributes to understanding the inter-consistency among LLMs and lays the foundation for the development of future collaboration methods."
            },
            "score": 5
        },
        {
            "id": "c43a4a7b7ea4f4889de051321cb0073fd577f843",
            "paperId": "c43a4a7b7ea4f4889de051321cb0073fd577f843",
            "title": "Causal Reasoning of Entities and Events in Procedural Texts",
            "abstract": "Entities and events are crucial to natural language reasoning and common in procedural texts. Existing work has focused either exclusively on entity state tracking (e.g., whether a pan is hot) or on event reasoning (e.g., whether one would burn themselves by touching the pan), while these two tasks are often causally related. We propose CREPE, the first benchmark on causal reasoning of event plausibility and entity states. We show that most language models, including GPT-3, perform close to chance at .35 F1, lagging far behind human at .87 F1. We boost model performance to .59 F1 by creatively representing events as programming languages while prompting language models pretrained on code. By injecting the causal relations between entities and events as intermediate reasoning steps in our representation, we further boost the performance to .67 F1. Our findings indicate not only the challenge that CREPE brings for language models, but also the efficacy of code-like prompting combined with chain-of-thought prompting for multihop event reasoning.",
            "year": 2023,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes CREPE, the first benchmark on causal reasoning of event plausibility and entity states, and boosts model performance to .59 F1 by creatively representing events as programming languages while prompting language models pretrained on code."
            },
            "score": 5
        },
        {
            "id": "4c9ea71ebca90fa902c718a40c19dcef5d6cdfbe",
            "paperId": "4c9ea71ebca90fa902c718a40c19dcef5d6cdfbe",
            "title": "PARADISE: Evaluating Implicit Planning Skills of Language Models with Procedural Warnings and Tips Dataset",
            "abstract": "Recently, there has been growing interest within the community regarding whether large language models are capable of planning or executing plans. However, most prior studies use LLMs to generate high-level plans for simplified scenarios lacking linguistic complexity and domain diversity, limiting analysis of their planning abilities. These setups constrain evaluation methods (e.g., predefined action space), architectural choices (e.g., only generative models), and overlook the linguistic nuances essential for realistic analysis. To tackle this, we present PARADISE, an abductive reasoning task using Q\\&A format on practical procedural text sourced from wikiHow. It involves warning and tip inference tasks directly associated with goals, excluding intermediary steps, with the aim of testing the ability of the models to infer implicit knowledge of the plan solely from the given goal. Our experiments, utilizing fine-tuned language models and zero-shot prompting, reveal the effectiveness of task-specific small models over large language models in most scenarios. Despite advancements, all models fall short of human performance. Notably, our analysis uncovers intriguing insights, such as variations in model behavior with dropped keywords, struggles of BERT-family and GPT-4 with physical and abstract goals, and the proposed tasks offering valuable prior knowledge for other unseen procedural tasks. The PARADISE dataset and associated resources are publicly available for further research exploration with https://github.com/GGLAB-KU/paradise.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experiments, utilizing fine-tuned language models and zero-shot prompting, reveal the effectiveness of task-specific small models over large language models in most scenarios and uncovers intriguing insights, such as variations in model behavior with dropped keywords, struggles of BERT-family and GPT-4 with physical and abstract goals, and the proposed tasks offering valuable prior knowledge for other unseen procedural tasks."
            },
            "score": 5
        },
        {
            "id": "3f8d2977a7753f9a3b0b5d18e6bee6afb0028ed8",
            "paperId": "3f8d2977a7753f9a3b0b5d18e6bee6afb0028ed8",
            "title": "PIVOT: Iterative Visual Prompting Elicits Actionable Knowledge for VLMs",
            "abstract": "Vision language models (VLMs) have shown impressive capabilities across a variety of tasks, from logical reasoning to visual understanding. This opens the door to richer interaction with the world, for example robotic control. However, VLMs produce only textual outputs, while robotic control and other spatial tasks require outputting continuous coordinates, actions, or trajectories. How can we enable VLMs to handle such settings without fine-tuning on task-specific data? In this paper, we propose a novel visual prompting approach for VLMs that we call Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as iterative visual question answering. In each iteration, the image is annotated with a visual representation of proposals that the VLM can refer to (e.g., candidate robot actions, localizations, or trajectories). The VLM then selects the best ones for the task. These proposals are iteratively refined, allowing the VLM to eventually zero in on the best available answer. We investigate PIVOT on real-world robotic navigation, real-world manipulation from images, instruction following in simulation, and additional spatial inference tasks such as localization. We find, perhaps surprisingly, that our approach enables zero-shot control of robotic systems without any robot training data, navigation in a variety of environments, and other capabilities. Although current performance is far from perfect, our work highlights potentials and limitations of this new regime and shows a promising approach for Internet-Scale VLMs in robotic and spatial reasoning domains. Website: pivot-prompt.github.io and HuggingFace: https://huggingface.co/spaces/pivot-prompt/pivot-prompt-demo.",
            "year": 2024,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel visual prompting approach for VLMs that is called Prompting with Iterative Visual Optimization (PIVOT), which casts tasks as iterative visual question answering, and finds that it enables zero-shot control of robotic systems without any robot training data, navigation in a variety of environments, and other capabilities."
            },
            "score": 5
        },
        {
            "id": "dd108d14f9ee57caeb3906c0efa2623595d5992c",
            "paperId": "dd108d14f9ee57caeb3906c0efa2623595d5992c",
            "title": "MOKA: Open-Vocabulary Robotic Manipulation through Mark-Based Visual Prompting",
            "abstract": "Open-vocabulary generalization requires robotic systems to perform tasks involving complex and diverse environments and task goals. While the recent advances in vision language models (VLMs) present unprecedented opportunities to solve unseen problems, how to utilize their emergent capabilities to control robots in the physical world remains an open question. In this paper, we present MOKA (Marking Open-vocabulary Keypoint Affordances), an approach that employs VLMs to solve robotic manipulation tasks specified by free-form language descriptions. At the heart of our approach is a compact point-based representation of affordance and motion that bridges the VLM's predictions on RGB images and the robot's motions in the physical world. By prompting a VLM pre-trained on Internet-scale data, our approach predicts the affordances and generates the corresponding motions by leveraging the concept understanding and commonsense knowledge from broad sources. To scaffold the VLM's reasoning in zero-shot, we propose a visual prompting technique that annotates marks on the images, converting the prediction of keypoints and waypoints into a series of visual question answering problems that are feasible for the VLM to solve. Using the robot experiences collected in this way, we further investigate ways to bootstrap the performance through in-context learning and policy distillation. We evaluate and analyze MOKA's performance on a variety of manipulation tasks specified by free-form language descriptions, such as tool use, deformable body manipulation, and object rearrangement.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents MOKA (Marking Open-vocabulary Keypoint Affordances), an approach that employs VLMs to solve robotic manipulation tasks specified by free-form language descriptions, and evaluates and analyze MOKA's performance on a variety of manipulation tasks specified by free-form language descriptions."
            },
            "score": 5
        },
        {
            "id": "288cb169619bde78604450adc8cb5df536ef20f1",
            "paperId": "288cb169619bde78604450adc8cb5df536ef20f1",
            "title": "Learning Chess Blindfolded: Evaluating Language Models on State Tracking",
            "abstract": "Transformer language models have made tremendous strides in natural language understanding tasks. However, the complexity of natural language makes it challenging to ascertain how accurately these models are tracking the world state underlying the text. Motivated by this issue, we consider the task of language modeling for the game of chess. Unlike natural language, chess notations describe a simple, constrained, and deterministic domain. Moreover, we observe that the appropriate choice of chess notation allows for directly probing the world state, without requiring any additional probing-related machinery. We find that: (a) With enough training data, transformer language models can learn to track pieces and predict legal moves with high accuracy when trained solely on move sequences. (b) For small training sets providing access to board state information during training can yield significant improvements. (c) The success of transformer language models is dependent on access to the entire game history i.e. \"full attention\". Approximating this full attention results in a significant performance drop. We propose this testbed as a benchmark for future work on the development and analysis of transformer language models.",
            "year": 2021,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that with enough training data, transformer language models can learn to track pieces and predict legal moves with high accuracy when trained solely on move sequences and for small training sets providing access to board state information during training can yield significant improvements."
            },
            "score": 4
        },
        {
            "id": "453fc588d97958c6fefad96e79edd896873b3e09",
            "paperId": "453fc588d97958c6fefad96e79edd896873b3e09",
            "title": "Chess as a Testbed for Language Model State Tracking",
            "abstract": "Transformer language models have made tremendous strides in natural language understanding tasks. However, the complexity of natural language makes it challenging to ascertain how accurately these models are tracking the world state underlying the text. Motivated by this issue, we consider the task of language modeling for the game of chess. Unlike natural language, chess notations describe a simple, constrained, and deterministic domain. Moreover, we observe that the appropriate choice of chess notation allows for directly probing the world state, without requiring any additional probing-related machinery. We find that: (a) With enough training data, transformer language models can learn to track pieces and predict legal moves with high accuracy when trained solely on move sequences. (b) For small training sets providing access to board state information during training can yield significant improvements. (c) The success of transformer language models is dependent on access to the entire game history i.e. \u201cfull attention\u201d. Approximating this full attention results in a significant performance drop. We propose this testbed as a benchmark for future work on the development and analysis of transformer language models.",
            "year": 2021,
            "citationCount": 19,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that with enough training data, transformer language models can learn to track pieces and predict legal moves with high accuracy when trained solely on move sequences and for small training sets providing access to board state information during training can yield significant improvements."
            },
            "score": 4
        },
        {
            "id": "8cdf08abce73b61bc700bcb3458cab27b26cdd03",
            "paperId": "8cdf08abce73b61bc700bcb3458cab27b26cdd03",
            "title": "DiSTRICT: Dialogue State Tracking with Retriever Driven In-Context Tuning",
            "abstract": "Dialogue State Tracking (DST), a key component of task-oriented conversation systems, represents user intentions by determining the values of pre-defined slots in an ongoing dialogue. Existing approaches use hand-crafted templates and additional slot information to fine-tune and prompt large pre-trained language models and elicit slot values from the dialogue context. Significant manual effort and domain knowledge is required to design effective prompts, limiting the generalizability of these approaches to new domains and tasks. In this work, we propose DiSTRICT, a generalizable in-context tuning approach for DST that retrieves highly relevant training examples for a given dialogue to fine-tune the model without any hand-crafted templates. Experiments with the MultiWOZ benchmark datasets show that DiSTRICT outperforms existing approaches in various zero-shot and few-shot settings using a much smaller model, thereby providing an important advantage for real-world deployments that often have limited resource availability.",
            "year": 2022,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "DiSTRICT is proposed, a generalizable in-context tuning approach for DST that retrieves highly relevant training examples for a given dialogue to fine-tune the model without any hand-crafted templates, thereby providing an important advantage for real-world deployments that often have limited resource availability."
            },
            "score": 4
        },
        {
            "id": "ad4e02784491f9794f6abb76b8982c980f51a6ee",
            "paperId": "ad4e02784491f9794f6abb76b8982c980f51a6ee",
            "title": "OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models",
            "abstract": "Neural Theory-of-Mind (N-ToM), machine's ability to understand and keep track of the mental states of others, is pivotal in developing socially intelligent agents. However, prevalent N-ToM benchmarks have several shortcomings, including the presence of ambiguous and artificial narratives, absence of personality traits and preferences, a lack of questions addressing characters' psychological mental states, and limited diversity in the questions posed. In response to these issues, we construct OpenToM, a new benchmark for assessing N-ToM with (1) longer and clearer narrative stories, (2) characters with explicit personality traits, (3) actions that are triggered by character intentions, and (4) questions designed to challenge LLMs' capabilities of modeling characters' mental states of both the physical and psychological world. Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling certain aspects of mental states in the physical world but fall short when tracking characters' mental states in the psychological world.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "OpenToM is constructed, a new benchmark for assessing N-ToM with longer and clearer narrative stories, characters with explicit personality traits, and questions designed to challenge LLMs' capabilities of modeling characters' mental states of both the physical and psychological world."
            },
            "score": 4
        },
        {
            "id": "1cbdccf8060cb5fb08ddef60242a7e96fc473cd3",
            "paperId": "1cbdccf8060cb5fb08ddef60242a7e96fc473cd3",
            "title": "Localizing Active Objects from Egocentric Vision with Symbolic World Knowledge",
            "abstract": "The ability to actively ground task instructions from an egocentric view is crucial for AI agents to accomplish tasks or assist humans virtually. One important step towards this goal is to localize and track key active objects that undergo major state change as a consequence of human actions/interactions to the environment without being told exactly what/where to ground (e.g., localizing and tracking the `sponge` in video from the instruction\"Dip the `sponge` into the bucket.\"). While existing works approach this problem from a pure vision perspective, we investigate to which extent the textual modality (i.e., task instructions) and their interaction with visual modality can be beneficial. Specifically, we propose to improve phrase grounding models' ability on localizing the active objects by: (1) learning the role of `objects undergoing change` and extracting them accurately from the instructions, (2) leveraging pre- and post-conditions of the objects during actions, and (3) recognizing the objects more robustly with descriptional knowledge. We leverage large language models (LLMs) to extract the aforementioned action-object knowledge, and design a per-object aggregation masking technique to effectively perform joint inference on object phrases and symbolic knowledge. We evaluate our framework on Ego4D and Epic-Kitchens datasets. Extensive experiments demonstrate the effectiveness of our proposed framework, which leads to>54% improvements in all standard metrics on the TREK-150-OPE-Det localization + tracking task,>7% improvements in all standard metrics on the TREK-150-OPE tracking task, and>3% improvements in average precision (AP) on the Ego4D SCOD task.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes to improve phrase grounding models' ability on localizing the active objects by learning the role of `objects undergoing change` and extracting them accurately from the instructions, and leveraging pre- and post-conditions of the objects during actions, and recognizing the objects more robustly with descriptional knowledge."
            },
            "score": 4
        },
        {
            "id": "2cbdd4ef5a14dbf3446e1d5b0ecf4b222ded820d",
            "paperId": "2cbdd4ef5a14dbf3446e1d5b0ecf4b222ded820d",
            "title": "Semantic Parsing by Large Language Models for Intricate Updating Strategies of Zero-Shot Dialogue State Tracking",
            "abstract": "Zero-shot Dialogue State Tracking (DST) addresses the challenge of acquiring and annotating task-oriented dialogues, which can be time-consuming and costly. However, DST extends beyond simple slot-filling and requires effective updating strategies for tracking dialogue state as conversations progress. In this paper, we propose ParsingDST, a new In-Context Learning (ICL) method, to introduce additional intricate updating strategies in zero-shot DST. Our approach reformulates the DST task by leveraging powerful Large Language Models (LLMs) and translating the original dialogue text to JSON through semantic parsing as an intermediate state. We also design a novel framework that includes more modules to ensure the effectiveness of updating strategies in the text-to-JSON process. Experimental results demonstrate that our approach outperforms existing zero-shot DST methods on MultiWOZ, exhibiting significant improvements in Joint Goal Accuracy (JGA) and slot accuracy compared to existing ICL methods. Our code has been released.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ParsingDST, a new In-Context Learning (ICL) method, is proposed to introduce additional intricate updating strategies in zero-shot DST, reformulates the DST task by leveraging powerful Large Language Models (LLMs) and translating the original dialogue text to JSON through semantic parsing as an intermediate state."
            },
            "score": 4
        },
        {
            "id": "158893cb39604383d18db673a08c64575600495b",
            "paperId": "158893cb39604383d18db673a08c64575600495b",
            "title": "NEURAL-SYMBOLIC MODELING FOR NATURAL LANGUAGE DISCOURSE",
            "abstract": "Language \u201cin the wild\u201d is complex and ambiguous and relies on a shared understanding of the world for its inter-pretation. Most current NLP methods represent language by learning word co-occurrence patterns from massive amounts of linguistic data. This representation can be very powerful, but it is insufficient to capture the meaning behind written and spoken communication. In this talk, I will motivate neural-symbolic representations for dealing with these challenges. On the one hand, symbols have inherent explanatory power, and they can help us express domain knowledge and enforce consistency across different decisions. On the other hand, expressive distributed representations allow us to leverage the strengths of statistical language models to make sense of large amounts of linguistic data. I will in-troduce a holistic framework that covers all stages of the neural-symbolic pipeline: modeling, learning, inference, and its application for analyzing discourse in real-world scenarios and show its advantages with respect to end-to-end neural approaches and traditional statistical relational learning methods.",
            "year": 2020,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A holistic framework that covers all stages of the neural-symbolic pipeline: modeling, learning, inference, and its application for analyzing discourse in real-world scenarios is presented and its advantages are shown with respect to end-to-end neural approaches and traditional statistical relational learning methods."
            },
            "score": 4
        },
        {
            "id": "96de6983be2d1bf2d947b29e320fde4e946a6dca",
            "paperId": "96de6983be2d1bf2d947b29e320fde4e946a6dca",
            "title": "Improving language models fine-tuning with representation consistency targets",
            "abstract": "Fine-tuning contextualized representations 001 learned by pre-trained language models has 002 become a standard practice in the NLP field. 003 However, pre-trained representations are prone 004 to degradation (also known as representation 005 collapse) during fine-tuning, which leads to in-006 stability, sub-optimal performance, and weak 007 generalization. In this paper, we propose a 008 novel fine-tuning method that avoids represen-009 tation collapse during fine-tuning by discourag-010 ing undesirable changes of the representations. 011 We show that our approach matches or exceeds 012 the performance of the existing regularization-013 based fine-tuning methods across 13 language 014 understanding tasks (GLUE benchmark and six 015 additional datasets). We also demonstrate its 016 effectiveness in low-data settings and robust-017 ness to label perturbation. Furthermore, we 018 extend previous studies of representation col-019 lapse and propose several metrics to quantify it. 020 Using these metrics and previously proposed 021 experiments, we show that our approach ob-022 tains significant improvements in retaining the 023 expressive power of representations. 024",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel fine-tuning method that avoids represen-009 collapse during fine-tuning by discourag-010 ing undesirable changes of the representations and shows significant improvements in retaining the expressive power of representations."
            },
            "score": 4
        },
        {
            "id": "01a5d0ed2300ec86aa82d0e56222932f200ad692",
            "paperId": "01a5d0ed2300ec86aa82d0e56222932f200ad692",
            "title": "LLaMA Rider: Spurring Large Language Models to Explore the Open World",
            "abstract": "Recently, various studies have leveraged Large Language Models (LLMs) to help decision-making and planning in environments, and try to align the LLMs' knowledge with the world conditions. Nonetheless, the capacity of LLMs to continuously acquire environmental knowledge and adapt in an open world remains uncertain. In this paper, we propose an approach to spur LLMs to explore the open world, gather experiences, and learn to improve their task-solving capabilities. In this approach, a multi-round feedback-revision mechanism is utilized to encourage LLMs to actively select appropriate revision actions guided by feedback information from the environment. This facilitates exploration and enhances the model's performance. Besides, we integrate sub-task relabeling to assist LLMs in maintaining consistency in sub-task planning and help the model learn the combinatorial nature between tasks, enabling it to complete a wider range of tasks through training based on the acquired exploration experiences. By evaluation in Minecraft, an open-ended sandbox world, we demonstrate that our approach LLaMA-Rider enhances the efficiency of the LLM in exploring the environment, and effectively improves the LLM's ability to accomplish more tasks through fine-tuning with merely 1.3k instances of collected data, showing minimal training costs compared to the baseline using reinforcement learning.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes an approach to spur Large Language Models to explore the open world, gather experiences, and learn to improve their task-solving capabilities, using a multi-round feedback-revision mechanism to encourage LLMs to actively select appropriate revision actions guided by feedback information from the environment."
            },
            "score": 4
        },
        {
            "id": "61889eb1cb94c66fc8782882b01c2cdd7e3a41ed",
            "paperId": "61889eb1cb94c66fc8782882b01c2cdd7e3a41ed",
            "title": "Understanding the Inner Workings of Language Models Through Representation Dissimilarity",
            "abstract": "As language models are applied to an increasing number of real-world applications, understanding their inner workings has become an important issue in model trust, interpretability, and transparency. In this work we show that representation dissimilarity measures, which are functions that measure the extent to which two model's internal representations differ, can be a valuable tool for gaining insight into the mechanics of language models. Among our insights are: (i) an apparent asymmetry in the internal representations of model using SoLU and GeLU activation functions, (ii) evidence that dissimilarity measures can identify and locate generalization properties of models that are invisible via in-distribution test set performance, and (iii) new evaluations of how language model features vary as width and depth are increased. Our results suggest that dissimilarity measures are a promising set of tools for shedding light on the inner workings of language models.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work shows that representation dissimilarity measures, which are functions that measure the extent to which two model's internal representations differ, can be a valuable tool for gaining insight into the mechanics of language models."
            },
            "score": 4
        },
        {
            "id": "b908824639d18f11883abcab21efeb22e315ab9c",
            "paperId": "b908824639d18f11883abcab21efeb22e315ab9c",
            "title": "Multimodal Procedural Planning via Dual Text-Image Prompting",
            "abstract": "Embodied agents have achieved prominent performance in following human instructions to complete tasks. However, the potential of providing instructions informed by texts and images to assist humans in completing tasks remains underexplored. To uncover this capability, we present the multimodal procedural planning (MPP) task, in which models are given a high-level goal and generate plans of paired text-image steps, providing more complementary and informative guidance than unimodal plans. The key challenges of MPP are to ensure the informativeness, temporal coherence,and accuracy of plans across modalities. To tackle this, we propose Text-Image Prompting (TIP), a dual-modality prompting method that jointly leverages zero-shot reasoning ability in large language models (LLMs) and compelling text-to-image generation ability from diffusion-based models. TIP improves the interaction in the dual modalities using Text-to-Image Bridge and Image-to-Text Bridge, allowing LLMs to guide the textual-grounded image plan generation and leveraging the descriptions of image plans to ground the textual plan reversely. To address the lack of relevant datasets, we collect WIKIPLAN and RECIPEPLAN as a testbed for MPP. Our results show compelling human preferences and automatic scores against unimodal and multimodal baselines on WIKIPLAN and RECIPEPLAN in terms of informativeness, temporal coherence, and plan accuracy. Our code and data: https://github.com/YujieLu10/MPP.",
            "year": 2023,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Text-Image Prompting (TIP) is proposed, a dual-modality prompting method that jointly leverages zero-shot reasoning ability in large language models (LLMs) and compelling text-to-image generation ability from diffusion-based models to tackle the key challenges of MPP."
            },
            "score": 4
        },
        {
            "id": "c2408a3a8da4f12d3eb156fe359a96b428e5aff1",
            "paperId": "c2408a3a8da4f12d3eb156fe359a96b428e5aff1",
            "title": "Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models",
            "abstract": "In this paper, we take the initiative to investigate the performance of LLMs on complex planning tasks that require LLMs to understand a virtual spatial environment simulated via natural language and act correspondingly in text. We propose a benchmark named Natural Language Planning (NLP) composed of a set of novel tasks: Brick World, NLVR-based Manipulations, and Natural Language Navigation. We found that current popular LLMs such as ChatGPT still lack abilities in complex planning. This arises a question \u2013 do the LLMs have a good understanding of the environments described in natural language, or maybe other alternatives such as symbolic representations are neater and hence better to be understood by LLMs? To this end, we propose a novel method called C O S ( C hain- o f- S ymbol Prompting) that represents the complex environments with condensed symbolic spatial representations during the chained intermediate thinking steps. C O S is easy to use and does not need additional training on LLMs. Extensive experiments indicate that C O S clearly surpasses the performance of the Chain-of-Thought (CoT) Prompting in all three planning tasks with even fewer tokens used in the inputs compared with CoT on ChatGPT and InstructGPT. The performance gain is strong, by up to 60.8% accuracy (from 31.8% to 92.6%) on Brick World for ChatGPT. C O S also reduces the number of tokens in the prompt obviously, by up to 65.8% of the tokens (from 407 to 139) for the intermediate steps from demonstrations on Brick World.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel method called C O S is proposed that represents the complex environments with condensed symbolic spatial representations during the chained intermediate thinking steps that surpasses the performance of the Chain-of-Thought Prompting in all three planning tasks with even fewer tokens used in the inputs."
            },
            "score": 4
        },
        {
            "id": "e1bc150d5d9e745a4920881c414ac9df0ea024a3",
            "paperId": "e1bc150d5d9e745a4920881c414ac9df0ea024a3",
            "title": "ChatGPT Prompting Cannot Estimate Predictive Uncertainty in High-Resource Languages",
            "abstract": "ChatGPT took the world by storm for its impressive abilities. Due to its release without documentation, scientists immediately attempted to identify its limits, mainly through its performance in natural language processing (NLP) tasks. This paper aims to join the growing literature regarding ChatGPT's abilities by focusing on its performance in high-resource languages and on its capacity to predict its answers' accuracy by giving a confidence level. The analysis of high-resource languages is of interest as studies have shown that low-resource languages perform worse than English in NLP tasks, but no study so far has analysed whether high-resource languages perform as well as English. The analysis of ChatGPT's confidence calibration has not been carried out before either and is critical to learn about ChatGPT's trustworthiness. In order to study these two aspects, five high-resource languages and two NLP tasks were chosen. ChatGPT was asked to perform both tasks in the five languages and to give a numerical confidence value for each answer. The results show that all the selected high-resource languages perform similarly and that ChatGPT does not have a good confidence calibration, often being overconfident and never giving low confidence values.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper aims to join the growing literature regarding ChatGPT's abilities by focusing on its performance in high-resource languages and on its capacity to predict its answers' accuracy by giving a confidence level."
            },
            "score": 4
        },
        {
            "id": "b6499bcc10d4a70c3ca8b84995270cfd0d29de4c",
            "paperId": "b6499bcc10d4a70c3ca8b84995270cfd0d29de4c",
            "title": "Model-tuning Via Prompts Makes NLP Models Adversarially Robust",
            "abstract": "In recent years, NLP practitioners have converged on the following practice: (i) import an off-the-shelf pretrained (masked) language model; (ii) append a multilayer perceptron atop the CLS token's hidden representation (with randomly initialized weights); and (iii) fine-tune the entire model on a downstream task (MLP-FT). This procedure has produced massive gains on standard NLP benchmarks, but these models remain brittle, even to mild adversarial perturbations. In this work, we demonstrate surprising gains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an alternative method of adapting to downstream tasks. Rather than appending an MLP head to make output prediction, MVP appends a prompt template to the input, and makes prediction via text infilling/completion. Across 5 NLP datasets, 4 adversarial attacks, and 3 different models, MVP improves performance against adversarial substitutions by an average of 8% over standard methods and even outperforms adversarial training-based state-of-art defenses by 3.5%. By combining MVP with adversarial training, we achieve further improvements in adversarial robustness while maintaining performance on unperturbed examples. Finally, we conduct ablations to investigate the mechanism underlying these gains. Notably, we find that the main causes of vulnerability of MLP-FT can be attributed to the misalignment between pre-training and fine-tuning tasks, and the randomly initialized MLP parameters.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work demonstrates surprising gains in adversarial robustness enjoyed by Model-tuning Via Prompts (MVP), an alternative method of adapting to downstream tasks that improves performance against adversarial substitutions and outperforms adversarial training-based state-of-art defenses by 3.5%."
            },
            "score": 4
        },
        {
            "id": "91806ece16c0e5cfde5a653277a219ddd9bfe5a7",
            "paperId": "91806ece16c0e5cfde5a653277a219ddd9bfe5a7",
            "title": "CheckDST: Measuring Real-World Generalization of Dialogue State Tracking Performance",
            "abstract": "Recent neural models that extend the pretrain-then-\ufb01netune paradigm continue to achieve new state-of-the-art results on joint goal accuracy (JGA) for dialogue state tracking (DST) benchmarks. However, we call into question their robustness as they show sharp drops in JGA for conversations containing utterances or dialog \ufb02ows with realistic perturbations. Inspired by CheckList (Ribeiro et al., 2020), we design a collection of metrics called CheckDST that facilitate comparisons of DST models on comprehensive dimensions of robustness by testing well-known weaknesses with augmented test sets. We evaluate recent DST models with CheckDST and argue that models should be assessed more holistically rather than pursuing state-of-the-art on JGA since a higher JGA does not guarantee better overall robustness. We \ufb01nd that span-based classi\ufb01cation models are resilient to unseen named entities but not robust to language variety, whereas those based on autoregressive language models generalize better to language variety but tend to memorize named entities and often hallucinate. Due to their respective weaknesses, neither approach is yet suitable for real-world deployment. We believe CheckDST is a useful guide for future research to develop task-oriented dialogue models that embody the strengths of various methods.",
            "year": 2021,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is argued that models should be assessed more holistically rather than pursuing state-of-the-art on JGA since a higher JGA does not guarantee better overall robustness, and a collection of metrics called CheckDST is a useful guide for future research to develop task-oriented dialogue models that embody the strengths of various methods."
            },
            "score": 3
        },
        {
            "id": "9997abf668f5ef6e95d9f71ccc8332a8f29c50fd",
            "paperId": "9997abf668f5ef6e95d9f71ccc8332a8f29c50fd",
            "title": "Adaptive Multi-Domain Dialogue State Tracking on Spoken Conversations",
            "abstract": "The main objective of the task-oriented dialogue system is to identify the intent and needs of human dialogue. Many existing studies are conducted under the setting of written dialogue, but there always exists a difficulty in coping with real-world spoken dialogues. To this end, DSTC10 challenge organizers propose the task of building robust dialogue state tracking (DST) models on spoken dialogues. With the powerful existing DST model (i.e., MinTL), this article suggests integral components for building a dialogue state tracker; 1) Data augmentation effectively enhances the capability of the model to catch the entities that exist in the evaluation dataset. 2) Levenshtein post-processing aims to prevent the distortion in model prediction caused by automatic speech recognition errors. To validate the effectiveness of our methods, we evaluate our model on DSTC10 datasets and conduct qualitative analysis by ablating each component of the model. Experimental results show that our model significantly outperforms baselines in all evaluation metrics and took 3rd place in the challenge.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This article suggests integral components for building a dialogue state tracker and suggests data augmentation effectively enhances the capability of the model to catch the entities that exist in the evaluation dataset and Levenshtein post-processing aims to prevent the distortion in model prediction caused by automatic speech recognition errors."
            },
            "score": 3
        },
        {
            "id": "7140be4f8f973c3a6a9503c76227e8d752c41065",
            "paperId": "7140be4f8f973c3a6a9503c76227e8d752c41065",
            "title": "Domain-Lifelong Learning for Dialogue State Tracking via Knowledge Preservation Networks",
            "abstract": "Dialogue state tracking (DST), which estimates user goals given a dialogue context, is an essential component of task-oriented dialogue systems. Conventional DST models are usually trained offline, which requires a fixed dataset prepared in advance. This paradigm is often impractical in real-world applications since online dialogue systems usually involve continually emerging new data and domains. Therefore, this paper explores Domain-Lifelong Learning for Dialogue State Tracking (DLL-DST), which aims to continually train a DST model on new data to learn incessantly emerging new domains while avoiding catastrophically forgetting old learned domains. To this end, we propose a novel domain-lifelong learning method, called Knowledge Preservation Networks (KPN), which consists of multi-prototype enhanced retrospection and multi-strategy knowledge distillation, to solve the problems of expression diversity and combinatorial explosion in the DLL-DST task. Experimental results show that KPN effectively alleviates catastrophic forgetting and outperforms previous state-of-the-art lifelong learning methods by 4.25% and 8.27% of whole joint goal accuracy on the MultiWOZ benchmark and the SGD benchmark, respectively.",
            "year": 2021,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel domain-lifelong learning method, called Knowledge Preservation Networks (KPN), which consists of multi-prototype enhanced retrospection and multi-strategy knowledge distillation to solve the problems of expression diversity and combinatorial explosion in the DLL-DST task."
            },
            "score": 3
        },
        {
            "id": "bbabf70bba1a8960966c4c679cd0dad0dea05c1c",
            "paperId": "bbabf70bba1a8960966c4c679cd0dad0dea05c1c",
            "title": "The SPPD System for Schema Guided Dialogue State Tracking Challenge",
            "abstract": "This paper introduces one of our group's work on the Dialog System Technology Challenges 8 (DSTC8), the SPPD system for Schema Guided dialogue state tracking challenge. This challenge, named as Track 4 in DSTC8, provides a brand new and challenging dataset for developing scalable multi-domain dialogue state tracking algorithms for real world dialogue systems. We propose a zero-shot dialogue state tracking system for this task. The key components of the system is a number of BERT based zero-shot NLU models that can effectively capture semantic relations between natural language descriptions of services' schemas and utterances from dialogue turns. We also propose some strategies to make the system better to exploit information from longer dialogue history and to overcome the slot carryover problem for multi-domain dialogues. The experimental results show that the proposed system achieves a significant improvement compared with the baseline system.",
            "year": 2020,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces one of the group's work on the Dialog System Technology Challenges 8, the SPPD system for Schema Guided dialogue state tracking challenge, which provides a brand new and challenging dataset for developing scalable multi-domain Dialogue state tracking algorithms for real world dialogue systems."
            },
            "score": 3
        },
        {
            "id": "d3342687757dc574a2cc1cfdcfc9568ccd6ececd",
            "paperId": "d3342687757dc574a2cc1cfdcfc9568ccd6ececd",
            "title": "A Multi-Task Approach to Incremental Dialogue State Tracking",
            "abstract": "Incrementality is a fundamental feature of language in real world use. To this point, however, the vast majority of work in automated dialogue processing has focused on language as turn based. In this paper we explore the challenge of incremental dialogue state tracking through the development and analysis of a multi-task approach to incremental dialogue state tracking. We present the design of our incremental dialogue state tracker in detail and provide evaluation against the well known Dialogue State Tracking Challenge 2 (DSTC2) dataset. In addition to a standard evaluation of the tracker, we also provide an analysis of the Incrementality phenomenon in our model\u2019s performance by analyzing how early our models can produce correct predictions and how stable those predictions are. We \ufb01nd that the Multi-Task Learning-based model achieves state-of-the-art results for incremental processing.",
            "year": 2018,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents the design of the incremental dialogue state tracker in detail and provides evaluation against the well known Dialogue State Tracking Challenge 2 (DSTC2) dataset and concludes that the Multi-Task Learning-based model achieves state-of-the-art results for incremental processing."
            },
            "score": 3
        },
        {
            "id": "490c03c084eb5ef08d959e6cb1aceb0646d33b17",
            "paperId": "490c03c084eb5ef08d959e6cb1aceb0646d33b17",
            "title": "SILG: The Multi-environment Symbolic Interactive Language Grounding Benchmark",
            "abstract": "Existing work in language grounding typically study single environments. How do we build unified models that apply across multiple environments? We propose the multi-environment Symbolic Interactive Language Grounding benchmark (SILG), which unifies a collection of diverse grounded language learning environments under a common interface. SILG consists of grid-world environments that require generalization to new dynamics, entities, and partially observed worlds (RTFM, Messenger, NetHack), as well as symbolic counterparts of visual worlds that require interpreting rich natural language with respect to complex scenes (ALFWorld, Touchdown). Together, these environments provide diverse grounding challenges in richness of observation space, action space, language specification, and plan complexity. In addition, we propose the first shared model architecture for RL on these environments, and evaluate recent advances such as egocentric local convolution, recurrent state-tracking, entity-centric attention, and pretrained LM using SILG. Our shared architecture achieves comparable performance to environment-specific architectures. Moreover, we find that many recent modelling advances do not result in significant gains on environments other than the one they were designed for. This highlights the need for a multi-environment benchmark. Finally, the best models significantly underperform humans on SILG, which suggests ample room for future work. We hope SILG enables the community to quickly identify new methodologies for language grounding that generalize to a diverse set of environments and their associated challenges.",
            "year": 2021,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The multi-environment Symbolic Interactive Language Grounding benchmark (SILG) is proposed, which unifies a collection of diverse grounded language learning environments under a common interface and enables the community to quickly identify new methodologies for language grounding that generalize to a diverse set of environments and their associated challenges."
            },
            "score": 3
        },
        {
            "id": "64cdc2602eb0c4968ba57d3cfef974cf765389a5",
            "paperId": "64cdc2602eb0c4968ba57d3cfef974cf765389a5",
            "title": "Editing Knowledge Representation of Language Lodel via Rephrased Prefix Prompts",
            "abstract": "Neural language models (LMs) have been extensively trained on vast corpora to store factual knowledge about various aspects of the world described in texts. Current technologies typically employ knowledge editing methods or specific prompts to modify LM outputs. However, existing knowledge editing methods are costly and inefficient, struggling to produce appropriate text. Additionally, prompt engineering is opaque and requires significant effort to find suitable prompts. To address these issues, we introduce a new method called PSPEM (Prefix Soft Prompt Editing Method), that can be used for a lifetime with just one training. It resolves the inefficiencies and generalizability issues in knowledge editing methods and overcomes the opacity of prompt engineering by automatically seeking optimal soft prompts. Specifically, PSPEM utilizes a prompt encoder and an encoding converter to refine key information in prompts and uses prompt alignment techniques to guide model generation, ensuring text consistency and adherence to the intended structure and content, thereby maintaining an optimal balance between efficiency and accuracy. We have validated the effectiveness of PSPEM through knowledge editing and attribute inserting. On the COUNTERFACT dataset, PSPEM achieved nearly 100\\% editing accuracy and demonstrated the highest level of fluency. We further analyzed the similarities between PSPEM and original prompts and their impact on the model's internals. The results indicate that PSPEM can serve as an alternative to original prompts, supporting the model in effective editing.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results indicate that PSPEM can serve as an alternative to original prompts, supporting the model in effective editing, and validated the effectiveness of PSPEM through knowledge editing and attribute inserting."
            },
            "score": 3
        },
        {
            "id": "3f5dfe5cbc1cb116f73d99af520a498fe878cc20",
            "paperId": "3f5dfe5cbc1cb116f73d99af520a498fe878cc20",
            "title": "FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D Scene Understanding",
            "abstract": "Precisely perceiving the geometric and semantic properties of real-world 3D objects is crucial for the continued evolution of augmented reality and robotic applications. To this end, we present \\algfull{} (\\algname{}), which incorporates vision-language embeddings of foundation models into 3D Gaussian Splatting (GS). The key contribution of this work is an efficient method to reconstruct and represent 3D vision-language models. This is achieved by distilling feature maps generated from image-based foundation models into those rendered from our 3D model. To ensure high-quality rendering and fast training, we introduce a novel scene representation by integrating strengths from both GS and multi-resolution hash encodings (MHE). Our effective training procedure also introduces a pixel alignment loss that makes the rendered feature distance of same semantic entities close, following the pixel-level semantic boundaries. Our results demonstrate remarkable multi-view semantic consistency, facilitating diverse downstream tasks, beating state-of-the-art methods by $\\mathbf{10.2}$ percent on open-vocabulary language-based object detection, despite that we are $\\mathbf{851\\times}$ faster for inference. This research explores the intersection of vision, language, and 3D scene representation, paving the way for enhanced scene understanding in uncontrolled real-world environments. We plan to release the code upon paper acceptance.",
            "year": 2024,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This research explores the intersection of vision, language, and 3D scene representation, paving the way for enhanced scene understanding in uncontrolled real-world environments."
            },
            "score": 3
        },
        {
            "id": "9f859726b3d8dffd96a1f55de4122617751cc1b4",
            "paperId": "9f859726b3d8dffd96a1f55de4122617751cc1b4",
            "title": "Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment",
            "abstract": "Larger language models (LLMs) have taken the world by storm with their massive multi-tasking capabilities simply by optimizing over a next-word prediction objective. With the emergence of their properties and encoded knowledge, the risk of LLMs producing harmful outputs increases, making them unfit for scalable deployment for the public. In this work, we propose a new safety evaluation benchmark RED-EVAL that carries out red-teaming. We show that even widely deployed models are susceptible to the Chain of Utterances-based (CoU) prompting, jailbreaking closed source LLM-based systems such as GPT-4 and ChatGPT to unethically respond to more than 65% and 73% of harmful queries. We also demonstrate the consistency of the RED-EVAL across 8 open-source LLMs in generating harmful responses in more than 86% of the red-teaming attempts. Next, we propose RED-INSTRUCT--An approach for the safety alignment of LLMs. It constitutes two phases: 1) HARMFULQA data collection: Leveraging CoU prompting, we collect a dataset that consists of 1.9K harmful questions covering a wide range of topics, 9.5K safe and 7.3K harmful conversations from ChatGPT; 2) SAFE-ALIGN: We demonstrate how the conversational dataset can be used for the safety alignment of LLMs by minimizing the negative log-likelihood over helpful responses and penalizing over harmful responses by gradient accent over sample loss. Our model STARLING, a fine-tuned Vicuna-7B, is observed to be more safely aligned when evaluated on RED-EVAL and HHH benchmarks while preserving the utility of the baseline models (TruthfulQA, MMLU, and BBH).",
            "year": 2023,
            "citationCount": 48,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that even widely deployed models are susceptible to the Chain of Utterances-based (CoU) prompting, jailbreaking closed source LLM-based systems such as GPT-4 and ChatGPT to unethically respond to more than 65% and 73% of harmful queries."
            },
            "score": 3
        },
        {
            "id": "c7c56ca5453b37e68b3bbc38112fb0306a175037",
            "paperId": "c7c56ca5453b37e68b3bbc38112fb0306a175037",
            "title": "Representation, Learning and Reasoning on Spatial Language for Downstream NLP Tasks",
            "abstract": "Understating spatial semantics expressed in natural language can become highly complex in real-world applications. This includes applications of language grounding, navigation, visual question answering, and more generic human-machine interaction and dialogue systems. In many of such downstream tasks, explicit representation of spatial concepts and relationships can improve the capabilities of machine learning models in reasoning and deep language understanding. In this tutorial, we overview the cutting-edge research results and existing challenges related to spatial language understanding including semantic annotations, existing corpora, symbolic and sub-symbolic representations, qualitative spatial reasoning, spatial common sense, deep and structured learning models. We discuss the recent results on the above-mentioned applications \u2013that need spatial language learning and reasoning \u2013 and highlight the research gaps and future directions.",
            "year": 2020,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This tutorial overviews the cutting-edge research results and existing challenges related to spatial language understanding including semantic annotations, existing corpora, symbolic and sub-symbolic representations, qualitative spatial reasoning, spatial common sense, deep and structured learning models."
            },
            "score": 3
        },
        {
            "id": "4201ae26e604d46fff2aa64709dc334fde85ac66",
            "paperId": "4201ae26e604d46fff2aa64709dc334fde85ac66",
            "title": "Lifelong Representation Learning for NLP Applications",
            "abstract": "Representation learning lives at the heart of deep learning for natural language processing (NLP). \nTraditional representation learning (such as softmax-based classification, pre-trained word embeddings, and language models, graph representations) focuses on learning general or static representations with the hope to help any end task. As the world keeps evolving, emerging knowledge (such as new tasks, domains, entities or relations) typically come with a small amount of data with shifted distributions that challenge the existing representations to be effective. As a result, how to effectively learn representations for new knowledge becomes crucial. Lifelong learning is a machine learning paradigm that aims to build an AI agent that keeps learning from the evolving world, like humans' learning from the world. This dissertation focuses on improving representations on different types of new knowledge (classification, word-level, contextual-level, and knowledge graph) for a myriad of NLP end tasks, ranging from text classification, sentiment analysis, entity recognition, question answering to the more complex dialog system. With the help of lifelong representation learning, models' performance on tasks is greatly improved beyond existing general representation learning.",
            "year": 2020,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This dissertation focuses on improving representations on different types of new knowledge for a myriad of NLP end tasks, ranging from text classification, sentiment analysis, entity recognition, question answering to the more complex dialog system."
            },
            "score": 3
        },
        {
            "id": "5aa49fb8ec1c2b24d7e317cbfb4b9d7a14165d09",
            "paperId": "5aa49fb8ec1c2b24d7e317cbfb4b9d7a14165d09",
            "title": "Comparing Trajectory and Vision Modalities for Verb Representation",
            "abstract": "Three-dimensional trajectories, or the 3D position and rotation of objects over time, have been shown to encode key aspects of verb semantics (e.g., the meanings of roll vs. slide). However, most multimodal models in NLP use 2D images as representations of the world. Given the importance of 3D space in formal models of verb semantics, we expect that these 2D images would result in impoverished representations that fail to capture nuanced differences in meaning. This paper tests this hypothesis directly in controlled experiments. We train self-supervised image and trajectory encoders, and then evaluate them on the extent to which each learns to differentiate verb concepts. Contrary to our initial expectations, we find that 2D visual modalities perform similarly well to 3D trajectories. While further work should be conducted on this question, our initial findings challenge the conventional wisdom that richer environment representations necessarily translate into better representation learning for language.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The initial findings challenge the conventional wisdom that richer environment representations necessarily translate into better representation learning for language and suggest that 2D visual modalities perform similarly well to 3D trajectories."
            },
            "score": 3
        },
        {
            "id": "807825a2687e77bd446e0a9ee4e2ba239774b57f",
            "paperId": "807825a2687e77bd446e0a9ee4e2ba239774b57f",
            "title": "Comparing the performance of ChatGPT and state-of-the-art climate NLP models on climate-related text classification tasks",
            "abstract": "Recently, there has been a surge in general-purpose language models, with ChatGPT being the most advanced model to date. These models are primarily used for generating text in response to user prompts on various topics. It needs to be validated how accurate and relevant the generated text from ChatGPT is on the specific topics, as it is designed for general conversation and not for context-specific purposes. This study explores how ChatGPT, as a general-purpose model, performs in the context of a real-world challenge such as climate change compared to ClimateBert, a state-of-the-art language model specifically trained on climate-related data from various sources, including texts, news, and papers. ClimateBert is fine-tuned on five different NLP classification tasks, making it a valuable benchmark for comparison with the ChatGPT on various NLP tasks. The main results show that for climate-specific NLP tasks, ClimateBert outperforms ChatGPT.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study explores how ChatGPT performs in the context of a real-world challenge such as climate change compared to ClimateBert, a state-of-the-art language model specifically trained on climate-related data from various sources, including texts, news, and papers."
            },
            "score": 3
        },
        {
            "id": "b2de5bd44ed8eb9cfd46d2e7e5a0fff61e68cfee",
            "paperId": "b2de5bd44ed8eb9cfd46d2e7e5a0fff61e68cfee",
            "title": "Connecting Language and Vision for Natural Language-Based Vehicle Retrieval",
            "abstract": "Vehicle search is one basic task for the efficient traffic management in terms of the AI City. Most existing prac-tices focus on the image-based vehicle matching, including vehicle re-identification and vehicle tracking. In this paper, we apply one new modality, i.e., the language description, to search the vehicle of interest and explore the potential of this task in the real-world scenario. The natural language-based vehicle search poses one new challenge of fine-grained understanding of both vision and language modalities. To connect language and vision, we propose to jointly train the state-of-the-art vision models with the transformer-based language model in an end-to-end manner. Except for the network structure design and the training strategy, several optimization objectives are also revisited in this work. The qualitative and quantitative experiments verify the effectiveness of the proposed method. Our proposed method has achieved the 1st place on the 5th AI City Challenge, yielding competitive performance 18.69% MRR accuracy on the private test set. We hope this work can pave the way for the future study on using language description effectively and efficiently for real-world vehicle retrieval systems. The code will be available at https://github.com/ShuaiBai623/AIC2021-T5-CLV.",
            "year": 2021,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes to jointly train the state-of-the-art vision models with the transformer-based language model in an end-to-end manner to connect language and vision, and achieves the 1st place on the 5th AI City Challenge."
            },
            "score": 2
        },
        {
            "id": "b2d7c81fb010777dacb59fa14f7f15e8a5d5a83c",
            "paperId": "b2d7c81fb010777dacb59fa14f7f15e8a5d5a83c",
            "title": "NADAQ: Natural Language Database Querying Based on Deep Learning",
            "abstract": "The high complexity behind SQL language and database schemas has made database querying a challenging task to human programmers. In this paper, we present our new natural language database querying (NADAQ) system as an alternative solution, by designing new translation models smoothly fusing deep learning and traditional database parsing techniques. On top of the popular encoder-decoder model for machine translation, NADAQ injects new dimensions of schema-aware bits associated with the input words into encoder phase and adds new hidden memory neurons controlled by the finite state machine for grammatical state tracking into the decoder phase. We further develop new techniques to enable the augmented neural network to reject queries irrelevant to the contents of the target database and recommend candidate queries reversely transformed into natural language. NADAQ performs well on real-world database systems over human labeled workload, returning query results at 90% accuracy.",
            "year": 2019,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper designs new translation models smoothly fusing deep learning and traditional database parsing techniques, and develops new techniques to enable the augmented neural network to reject queries irrelevant to the contents of the target database and recommend candidate queries reversely transformed into natural language."
            },
            "score": 2
        },
        {
            "id": "9262e934ed8bacd2fed26be0397107d5d5579af6",
            "paperId": "9262e934ed8bacd2fed26be0397107d5d5579af6",
            "title": "Embodied AI with Two Arms: Zero-shot Learning, Safety and Modularity",
            "abstract": "We present an embodied AI system which receives open-ended natural language instructions from a human, and controls two arms to collaboratively accomplish potentially long-horizon tasks over a large workspace. Our system is modular: it deploys state of the art Large Language Models for task planning,Vision-Language models for semantic perception, and Point Cloud transformers for grasping. With semantic and physical safety in mind, these modules are interfaced with a real-time trajectory optimizer and a compliant tracking controller to enable human-robot proximity. We demonstrate performance for the following tasks: bi-arm sorting, bottle opening, and trash disposal tasks. These are done zero-shot where the models used have not been trained with any real world data from this bi-arm robot, scenes or workspace.Composing both learning- and non-learning-based components in a modular fashion with interpretable inputs and outputs allows the user to easily debug points of failures and fragilities. One may also in-place swap modules to improve the robustness of the overall platform, for instance with imitation-learned policies.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 2
        },
        {
            "id": "71ad947616fb8d33c4896bb3c468269a2cfa35b1",
            "paperId": "71ad947616fb8d33c4896bb3c468269a2cfa35b1",
            "title": "Gaze-based Multimodal Meaning Recovery for Noisy\u00a0/\u00a0Complex Environments",
            "abstract": "Reference resolution is an important problem that has enormous practical implications in daily life, for example in recovering the intended meaning in communication when the environment is noisy (acoustic noise in the spoken channel, or clutter / occlusion in the visual world). Recent literature indicates that cross-modal processing of all the contributive modalities improves the reference resolution in such settings. In this paper, we investigate the contribution of the eye-tracking methodology, a substantial but underrepresented component of face-to-face communication in NLP systems, to recover the meaning in noisy settings. We integrate gaze features into state-of-the-art language models and test the model on data where parts of the sentences are masked, mimicking noise in the acoustic channel. The results indicate that eye movements can compensate for the missing information in the situation and support communication when language and visual modality fail.",
            "year": 2021,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The contribution of the eye-tracking methodology, a substantial but underrepresented component of face-to-face communication in NLP systems, to recover the meaning in noisy settings is investigated."
            },
            "score": 2
        },
        {
            "id": "5b0807ef8a00c1ab4cca7d32f0b3cd6f6fb42848",
            "paperId": "5b0807ef8a00c1ab4cca7d32f0b3cd6f6fb42848",
            "title": "Learning action models from plan examples with incomplete knowledge",
            "abstract": "AI planning requires the definition of an action model using a language such as PDDL as input. However, building an action model from scratch is a difficult and time-consuming task even for experts. In this paper, we develop an algorithm called ARMS for automatically discovering action models from a set of successful plan examples. Unlike the previous work in action-model learning, we do not assume complete knowledge of states in the middle of the example plans; that is, we assume that no intermediate states are given. This requirement is motivated by a variety of applications, including object tracking and plan monitoring where the knowledge about intermediate states is either minimal or unavailable to the observing agent. In a real world application, the cost is prohibitively high in labelling the training examples by manually annotating every state in a plan example from snapshots of an environment. To learn action models, our ARMS algorithm gathers knowledge on the statistical distribution of frequent sets of actions in the example plans. It then builds a propositional satisfiability (SAT) problem and solves it using a SAT solver. We lay the theoretical foundations of the learning problem and evaluate the effectiveness of ARMS empirically.",
            "year": 2005,
            "citationCount": 48,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An algorithm for automatically discovering action models from a set of successful plan examples called ARMS, which builds a propositional satisfiability (SAT) problem and solves it using a SAT solver and is evaluated empirically."
            },
            "score": 2
        },
        {
            "id": "33f3ce6b5d6f57774e42c602cad0c960dbf97a67",
            "paperId": "33f3ce6b5d6f57774e42c602cad0c960dbf97a67",
            "title": "OpenDlign: Enhancing Open-World 3D Learning with Depth-Aligned Images",
            "abstract": "Recent advances in Vision and Language Models (VLMs) have improved open-world 3D representation, facilitating 3D zero-shot capability in unseen categories. Existing open-world methods pre-train an extra 3D encoder to align features from 3D data (e.g., depth maps or point clouds) with CAD-rendered images and corresponding texts. However, the limited color and texture variations in CAD images can compromise the alignment robustness. Furthermore, the volume discrepancy between pre-training datasets of the 3D encoder and VLM leads to sub-optimal 2D to 3D knowledge transfer. To overcome these issues, we propose OpenDlign, a novel framework for learning open-world 3D representations, that leverages depth-aligned images generated from point cloud-projected depth maps. Unlike CAD-rendered images, our generated images provide rich, realistic color and texture diversity while preserving geometric and semantic consistency with the depth maps. OpenDlign also optimizes depth map projection and integrates depth-specific text prompts, improving 2D VLM knowledge adaptation for 3D learning efficient fine-tuning. Experimental results show that OpenDlign significantly outperforms existing benchmarks in zero-shot and few-shot 3D tasks, exceeding prior scores by 8.0% on ModelNet40 and 16.4% on OmniObject3D with just 6 million tuned parameters. Moreover, integrating generated depth-aligned images into existing 3D learning pipelines consistently improves their performance.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "OpenDlign is a novel framework for learning open-world 3D representations, that leverages depth-aligned images generated from point cloud-projected depth maps that provide rich, realistic color and texture diversity while preserving geometric and semantic consistency with the depth maps."
            },
            "score": 2
        },
        {
            "id": "9138e8a21663ad3f7b9d8717bd73d36fe422e004",
            "paperId": "9138e8a21663ad3f7b9d8717bd73d36fe422e004",
            "title": "Enhancing Text-to-Image Model Evaluation: SVCS and UCICM",
            "abstract": "As the world gradually embraces the integration of AI into various applications, there is a growing need for methods to enhance and evaluate AI-based technologies. One such a remarkable innovation is text to image models, which are currently revolutionizing the content generation, computer vision, design industries etc. Among the existing deep learning techniques utilized for text to image synthesis, stable diffusion model set apart due to its ability to generate high quality images. It transforms images into their latent space representation, mitigating computational memory and time requirements while minimizing instability and complexity. In this paper, we propose two evaluation metrics Semantic Visual Consistency Score (SVCS) and User-Centric Image Coherence Metric (UCICM) to analyze the coherence of the image with respect to the user prompt. SVCS assesses the model's capability to interpret human language, while UCICM is tailored to enhance application-specific image generation through personalization. This paper primarily focuses on the processes entailed in a transfer learning-based stable diffusion model and subsequently evaluates its performance using the newly proposed metrics.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Two evaluation metrics Semantic Visual Consistency Score (SVCS) and User-Centric Image Coherence Metric (UCICM) are proposed to analyze the coherence of the image with respect to the user prompt to assess the model's capability to interpret human language."
            },
            "score": 2
        },
        {
            "id": "8a389fd7b1897fc9175a11123156e08fa0d04050",
            "paperId": "8a389fd7b1897fc9175a11123156e08fa0d04050",
            "title": "Detection of Fake Users in Twitter Using Network Representation and NLP",
            "abstract": "Social Media Platforms like Facebook, Twitter, Instagram, etc. have large user base all around the world that generates huge amounts of data every second. This includes a lot of posts by fake and spam users, typically used by many organizations around the globe to gain a competitive edge over others. In this work, we aim at detecting such user accounts on Twitter. We show how to distinguish between Genuine and Spam accounts in Twitter using a novel combination of feature engineering, network representation, and natural language processing techniques.",
            "year": 2021,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work shows how to distinguish between Genuine and Spam accounts in Twitter using a novel combination of feature engineering, network representation, and natural language processing techniques."
            },
            "score": 2
        },
        {
            "id": "0e141942fa265142f41a2a26eb17b6005d3af29e",
            "paperId": "0e141942fa265142f41a2a26eb17b6005d3af29e",
            "title": "The State and Fate of Linguistic Diversity and Inclusion in the NLP World",
            "abstract": "Language technologies contribute to promoting multilingualism and linguistic diversity around the world. However, only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications. In this paper we look at the relation between the types of languages, resources, and their representation in NLP conferences to understand the trajectory that different languages have followed over time. Our quantitative investigation underlines the disparity between languages, especially in terms of their resources, and calls into question the \u201clanguage agnostic\u201d status of current models and systems. Through this paper, we attempt to convince the ACL community to prioritise the resolution of the predicaments highlighted here, so that no language is left behind.",
            "year": 2020,
            "citationCount": 548,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The relation between the types of languages, resources, and their representation in NLP conferences is looked at to understand the trajectory that different languages have followed over time and underlines the disparity between languages."
            },
            "score": 2
        },
        {
            "id": "b7fbef998c1792d4b3d05041aac0fc7564177efe",
            "paperId": "b7fbef998c1792d4b3d05041aac0fc7564177efe",
            "title": "EventBind: Learning a Unified Representation to Bind Them All for Event-based Open-world Understanding",
            "abstract": "In this paper, we propose EventBind, a novel and effective framework that unleashes the potential of vision-language models (VLMs) for event-based recognition to compensate for the lack of large-scale event-based datasets. In particular, due to the distinct modality gap with the image-text data and the lack of large-scale datasets, learning a common representation space for images, texts, and events is non-trivial.Intuitively, we need to address two key challenges: 1) how to generalize CLIP's visual encoder to event data while fully leveraging events' unique properties, e.g., sparsity and high temporal resolution; 2) how to effectively align the multi-modal embeddings, i.e., image, text, and events. Accordingly, we first introduce a novel event encoder that subtly models the temporal information from events and meanwhile, generates event prompts for modality bridging. We then design a text encoder that generates content prompts and utilizes hybrid text prompts to enhance EventBind's generalization ability across diverse datasets.With the proposed event encoder, text encoder, and image encoder, a novel Hierarchical Triple Contrastive Alignment (HTCA) module is introduced to jointly optimize the correlation and enable efficient knowledge transfer among the three modalities. We evaluate various settings, including fine-tuning and few-shot on three benchmarks, and our EventBind achieves new state-of-the-art accuracy compared with the previous methods, such as on N-Caltech101 (+5.34% and +1.70%) and N-Imagenet (+5.65% and +1.99%) with fine-tuning and 20-shot settings, respectively. Moreover, our EventBind can be flexibly extended to the event retrieval task using text or image queries, showing plausible performance. Our project code will be made publicly available.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel event encoder that subtly models the temporal information from events and meanwhile, generates event prompts for modality bridging is introduced, and a text encoder that generates content prompts and utilizes hybrid text prompts to enhance EventBind's generalization ability across diverse datasets."
            },
            "score": 2
        },
        {
            "id": "9eb5df546f9b35a42faa337da14a2167a67426f0",
            "paperId": "9eb5df546f9b35a42faa337da14a2167a67426f0",
            "title": "MARML: Motif-Aware Deep Representation Learning in Multilayer Networks.",
            "abstract": "The rapid increase in high-throughput, complex, and heterogeneous data has led to the adoption of network-structured models and analyses for interpretation. However, these data are inherently complex and challenging to understand, prompting researchers to turn to graph embedding methods to facilitate analysis. While general network embedding techniques have shown promise in improving downstream prediction and classification tasks, real-world data are complicated due to cross-domain interactions between different types of entities. Multilayered networks have been successful in integrating biological data to represent biological systems' hierarchy, but embedding nodes based on different types of interactions remains an unsolved problem. To address this challenge, we propose the Motif-aware deep representation learning in multilayer (MARML) networks for learning network representations. Our method considers recurring motif patterns, topological information, and attributive information from other sources as node features. We validated the MARML method using various multilayer network datasets. In addition, by incorporating motif information, MARML considers higher order connections across different hierarchies. The learned features exhibited excellent accuracy in tasks related to link prediction and link differentiation, enabling us to distinguish between existing and disconnected triplets. Through the integration of both intrinsic node attributes and topological network structures, we enhance our understanding of complex biological systems.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Motif-aware deep representation learning in multilayer (MARML) networks for learning network representations enhances the understanding of complex biological systems through the integration of both intrinsic node attributes and topological network structures."
            },
            "score": 2
        },
        {
            "id": "c2487ea07178462247d54be5801537c47ef0e718",
            "paperId": "c2487ea07178462247d54be5801537c47ef0e718",
            "title": "Internal Representation, Frame and Context in the Dynamics of Business Interactions: An Nlp Perspective",
            "abstract": "1. IntroductionOur inner representation of reality is unique for us, as our perception of the world is only an interpretation. It is not an exact representation of reality, because each person has his/her own way of thinking and takes information through his/her eyes, ears, taste and so on.People's internal representation acts as a filter on the world and gives us an indication of their different thoughts and responses to particular external stimuli. This filter produces states that are specific to each person's experience.From the communication model perspective, certain states are associated with certain behaviours, i.e. what we do and say depends on the state we are in. Our internal representation is a reflection of our state, consisting of images, sounds and feelings which are interrelated to make up a mental map and which interconnect to form behaviour.A way of changing the quality of our internal representations is, according to Molden (2001: 67), through the use of critical submodalities, which are likely to help us manage our states more easily: \"imagine you have a control panel for changing the qualities of your internal representations, called submodalities\". In other words, the three types of submodalities, i.e. visual, auditory and kinesthetic, enable us to develop our ability to work with our internal representations. This is particularly evident when using an internal representation of a pleasurable past experience. Starting from this point, one could make a mental exercise and bring back the feelings, images and sounds experienced at the time of the event and intensify them by using submodalities. That is why the role of submodalities is closely related to our internal representation, since images (black and white, bright and dim, larger and smaller, framed and panoramic), sounds (mono or stereo, loud or soft, cleared or muffled), and feelings (rough or smooth, hard or soft, hot or cold) are associated with the representation, and make up what the NLP Model of Communication calls a \"mind programming unit\".2. The internal representationOur functional model of communication also provides a new way of accessing information and representing the world internally. This method of recognizing which of the three communication channels is used to form the representation is known as the eye movement pattern. It provides cues for whether a person unconsciously prefers to represent the world visually, auditorily, or kinesthetically. Each eye accessing cue indicates how we represent and process information.Visual pattern-oriented people recall experiences by seeing a lot of pictures in their mind's eye. Strong, well-defined images, frequent upward eye movements characterize those with a visual representation of the world. Linguistically, persons who prefer the visual channel are more prone to use specific visual phrases like: It looks great, I can get the picture, I can picture what you're saying, She paints a completely different picture, We might consider this on a broader canvas.Kinesthetic pattern-based people process and represent information internally by moving their eyes down and to the right. During a conversation, this eye position accesses kinesthetic sensations of internal feelings and is a frequent cause for pauses and speech interruptions in mid-sentence. Kinesthetics' language cues are reflected in expressions such as Do you get a handle on things now?, Let's touch upon this point., He must brush up on his writing reports.Patterns of eye accessing cues include lateral and leftward movements for remembered sounds, and lateral and rightward movements for constructed sounds. Auditory pattern-oriented people use language structures that match their representational style: Listen to Mike's proposal, That's clear as a bell, Don't breathe a word.The filters on our experience determine the way we make sense of reality. As Sue Knight (2002: 47) put it, \"by learning to recognize filters you begin to build bridges of communication. \u2026",
            "year": 2016,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The functional model of communication provides a new way of accessing information and representing the world internally, and the three types of submodalities, i.e. visual, auditory and kinesthetic, enable us to develop the authors' ability to work with their internal representations."
            },
            "score": 2
        },
        {
            "id": "4601f459cc2baee8edd7dec1bd1f40771012630a",
            "paperId": "4601f459cc2baee8edd7dec1bd1f40771012630a",
            "title": "ZePT: Zero-Shot Pan-Tumor Segmentation via Query-Disentangling and Self-Prompting",
            "abstract": "The long-tailed distribution problem in medical image analysis reflects a high prevalence of common conditions and a low prevalence of rare ones, which poses a significant challenge in developing a unified model capable of identifying rare or novel tumor categories not encountered during training. In this paper, we propose a new zero-shot pan-tumor segmentation framework (ZePT) based on query-disentangling and self-prompting to segment unseen tumor categories beyond the training set. ZePT disentangles the object queries into two subsets and trains them in two stages. Initially, it learns a set of fundamental queries for organ segmentation through an object-aware feature grouping strategy, which gathers organ-level visual features. Subsequently, it refines the other set of advanced queries that focus on the auto-generated visual prompts for unseen tumor segmentation. Moreover, we introduce query-knowledge alignment at the feature level to enhance each query's discriminative representation and generalizability. Extensive experiments on various tumor segmentation tasks demonstrate the performance superiority of ZePT, which surpasses the previous counterparts and evidence the promising ability for zero-shot tumor segmentation in real-world settings.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new zero-shot pan-tumor segmentation framework based on query-disentangling and self-prompting to segment unseen tumor categories beyond the training set is proposed, which surpasses the previous counterparts and evidence the promising ability for zero-shot tumor segmentation in real-world settings."
            },
            "score": 2
        },
        {
            "id": "984c60e2cffa633d5f7478725a7e9c3e3abd0566",
            "paperId": "984c60e2cffa633d5f7478725a7e9c3e3abd0566",
            "title": "MEKER: Memory Efficient Knowledge Embedding Representation for Link Prediction and Question Answering",
            "abstract": "Knowledge Graphs (KGs) are symbolically structured storages of facts. The KG embedding contains concise data used in NLP tasks requiring implicit information about the real world. Furthermore, the size of KGs that may be useful in actual NLP assignments is enormous, and creating embedding over it has memory cost issues. We represent KG as a 3rd-order binary tensor and move beyond the standard CP decomposition (CITATION) by using a data-specific generalized version of it (CITATION). The generalization of the standard CP-ALS algorithm allows obtaining optimization gradients without a backpropagation mechanism. It reduces the memory needed in training while providing computational benefits. We propose a MEKER, a memory-efficient KG embedding model, which yields SOTA-comparable performance on link prediction tasks and KG-based Question Answering.",
            "year": 2022,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A MEKER is proposed, a memory-efficient KG embedding model, which yields SOTA-comparable performance on link prediction tasks and KG-based Question Answering and reduces the memory needed in training while providing computational benefits."
            },
            "score": 2
        },
        {
            "id": "e64ceb10230431e9aa90338b188c4d2adc720c3e",
            "paperId": "e64ceb10230431e9aa90338b188c4d2adc720c3e",
            "title": "BERT-Trip: Effective and Scalable Trip Representation using Attentive Contrast Learning",
            "abstract": "Trip recommendation has drawn considerable attention over the past decade. In trip recommendation, a sequence of point-of-interests (POIs) are recommended for a given query which includes an origin and a destination. Recently the emergence of the attention mechanism and many attention-incorporated models have achieved great success in various fields. Trip recommendation problems demonstrate similar characteristics that can potentially benefit from the attention mechanism. However, applying the attention mechanism for trip recommendation is non-trivial. We are motivated to answer the following two research questions. (1) How can we learn trip representation effectively without labels? Unlike most of the natural language processing tasks, there are no ground-truth labels available for trip recommendation. (2) How can we learn trip representation effectively without handcrafting negative samples? In this paper, we cast the trip representation learning into a natural language processing (NLP) task. We propose BERT-Trip, a self-supervised contrast learning framework, to learn effective and scalable trip representation in support of time-sensitive and user-personalized trip recommendation. BERT-Trip builds on a Siamese network to maximize the similarity between the augmentations of trips with BERT as the backbone encoder. We utilize the masking strategy for generating augmented views (positive sample pairs) of trips in the Siamese network and employ the stop-gradient on one side of the Siamese network to eliminate the need to use any negative sample pairs or momentum encoders. Extensive experiments on real-world datasets demonstrate that BERT-Trip consistently outperformed the state-of-the-art methods in terms of all effectiveness metrics. Compared with the state-of-the-art methods, BERT-Trip is able to yield up to 24 percent and 40 percent increases in F1 score on the Flickr and the Weeplaces datasets, respectively. A rigorous performance evaluation of BERT-Trip on scalability up to 12800 POIs is also provided.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes BERT-Trip, a self-supervised contrast learning framework, to learn effective and scalable trip representation in support of time-sensitive and user-personalized trip recommendation and casts the trip representation learning into a natural language processing (NLP) task."
            },
            "score": 2
        },
        {
            "id": "d75010b4a317f6ea5a9a26bae72505b22cb8d134",
            "paperId": "d75010b4a317f6ea5a9a26bae72505b22cb8d134",
            "title": "Contrastive Pre-training with Adversarial Perturbations for Check-In Sequence Representation Learning",
            "abstract": "A core step of mining human mobility data is to learn accurate representations for user-generated check-in sequences. The learned representations should be able to fully describe the spatial-temporal mobility patterns of users and the high-level semantics of traveling. However, existing check-in sequence representation learning is usually implicitly achieved by end-to-end models designed for specific downstream tasks, resulting in unsatisfactory generalizable abilities and poor performance. Besides, although the sequence representation learning models that follow the contrastive learning pre-training paradigm have achieved breakthroughs in many fields like NLP, they fail to simultaneously consider the unique spatial-temporal characteristics of check-in sequences and need manual adjustments on the data augmentation strategies. So, directly applying them to check-in sequences cannot yield a meaningful pretext task. To this end, in this paper we propose a contrastive pre-training model with adversarial perturbations for check-in sequence representation learning (CACSR). Firstly, we design a novel spatial-temporal augmentation block for disturbing the spatial-temporal features of check-in sequences in the latent space to relieve the stress of designing manual data augmentation strategies. Secondly, to construct an effective contrastive pretext task, we generate \u201chard\u201d positive and negative pairs for the check-in sequence by adversarial training. These two designs encourage the model to capture the high-level spatial-temporal patterns and semantics of check-in sequences while ignoring the noisy and unimportant details. We demonstrate the effectiveness and versatility of CACSR on two kinds of downstream tasks using three real-world datasets. The results show that our model outperforms both the state-of-the-art pre-training methods and the end-to-end models.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A contrastive pre-training model with adversarial perturbations for check-in sequence representation learning (CACSR) is proposed and the results show that the model outperforms both the state-of-the-art pre- training methods and the end-to-end models."
            },
            "score": 2
        },
        {
            "id": "6354666241b0dad77b4661f222506ce9997b8b1b",
            "paperId": "6354666241b0dad77b4661f222506ce9997b8b1b",
            "title": "Ontology Definition Metamodel based Consistency Checking of UML Models",
            "abstract": "Ontology definition metamodel (ODM) is the currently undergoing standardization through the object management group (OMG). It is a metamodel defined using meta-object facility (MOF), and enables using model driven architecture (MDA) standards and semantic Web together. In this modeling language (UML) models, we first derive a mapping for MOF to World Wide Web Consortium (W3C) 's Web ontology language (OWL) from ODM, and then provide a representation of UML metamodel in OWL based on this mapping. After transforming UML models to OWL instances, we can check consistency of these models by using logical reasoning and query techniques. An example of such a checking process is also given in this paper",
            "year": 2006,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "After transforming UML models to OWL instances, one can check consistency of these models by using logical reasoning and query techniques, and an example of such a checking process is given."
            },
            "score": 1
        },
        {
            "id": "d27942cb70b373dd6fe53895b82ff3ba23dceb5a",
            "paperId": "d27942cb70b373dd6fe53895b82ff3ba23dceb5a",
            "title": "Expression of a domain ontology model in unified modeling language for the World Health Organization International classification of impairment, disability, and handicap, version 2",
            "abstract": "The International Classification of Impairment, Disability, and Handicap Version 2(ICIDH-2), an anticipated addition to the World Health Organization suite of terminologies, has been put forth as a means for standardized representation of generic health and/or functional status data. In an attempt to make explicit the ontology upon which ICIDH-2 is based the authors derived a concept model expressed as a Unified Modeling Language static class diagram through abstraction of concept-terms in the documentation provided with the Full Version Pre-Final Draft of ICIDH-2 (December 2000). ICIDH-2's semantic structure is analyzed and evaluated for its semantic consistency. Discussion is presented on the utility of domain ontology models in terminology development and potential roles ICIDH-2 might play, as it undergoes refinement towards a representational standard. It is intended that the proposed UML rendering will stimulate domain discourse and consensus that will lead to enhancement of conceptual clarity in the ICIDH-2 ontological hierarchy and further enable its study and development as a healthcare classification.",
            "year": 2001,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is intended that the proposed UML rendering will stimulate domain discourse and consensus that will lead to enhancement of conceptual clarity in the ICIDH-2 ontological hierarchy and further enable its study and development as a healthcare classification."
            },
            "score": 1
        },
        {
            "id": "8a2b969a7455b5c269a7d1fa46172996b038a877",
            "paperId": "8a2b969a7455b5c269a7d1fa46172996b038a877",
            "title": "Flexible Invariants through Semantic Collaboration",
            "abstract": null,
            "year": 2013,
            "citationCount": 37,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 1
        },
        {
            "id": "af0e25ad6cdabd02ca7f4b633441102aa8cf5b53",
            "paperId": "af0e25ad6cdabd02ca7f4b633441102aa8cf5b53",
            "title": "\u042d\u043c\u043e\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u044b\u0439 \u0430\u0441\u043f\u0435\u043a\u0442 \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u043d\u0438\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0438\u043d\u043e\u0441\u0442\u0440\u0430\u043d\u043d\u044b\u043c \u044f\u0437\u044b\u043a\u0430\u043c",
            "abstract": "The article analyses the models of the content of foreign language teaching presented in the works by N.D. Galskova, A.N. Shchukin, R.K. Minyar-Beloruchev and others from the point of view of reflection of the emotional aspect. By results of the analyses the author proposes to include into the structure of the content of foreign language teaching emotional concepts as didactic units, ensuring completeness and consistency of representation of the linguistic emotional picture of the world, on the one hand, and sustainable motivation of the students to learn foreign languages, on the other hand.",
            "year": 2014,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The article analyses the models of the content of foreign language teaching presented in the works by N.N. Shchukin, R.K. Minyar-Beloruchev and others from the point of view of reflection of the emotional aspect to ensure completeness and consistency of representation of the linguistic emotional picture of the world and sustainable motivation of the students to learn foreign languages."
            },
            "score": 1
        },
        {
            "id": "cc84f7a7875eaaa681610eacc060863095de33ab",
            "paperId": "cc84f7a7875eaaa681610eacc060863095de33ab",
            "title": "Integrating Model-Based Monitoring and Diagnosis of Complex Dynamic Systems",
            "abstract": "We present a new approach to model-based monitoring and diagnosis of dynamic systems. The presented DIAMON algorithm uses hierarchical models to monitor and diagnose dynamic systems. DIAMON is based on the integration of teleological parameter-based monitoring models and repair-oriented device-based diagnosis models. It combines consistency-based diagnosis with model-based monitoring and uses an extension of the QSIM-language for the representation of qualitative system models. Furthermore, DIAMON is able to detect and localize a broad range of nonpermanent faults and thus extends traditional diagnosis which exclusively deals with permanent faulty behavior. The operation of DIAMON will be demonstrated on a real-world example in a multiple-faults scenario.",
            "year": 1991,
            "citationCount": 37,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "DIAMON is based on the integration of teleological parameter-based monitoring models and repair-oriented device-based diagnosis models and uses an extension of the QSIM-language for the representation of qualitative system models to monitor and diagnose dynamic systems."
            },
            "score": 1
        },
        {
            "id": "144adce2259ff5be0a1e2a685c4d6b45341c97c4",
            "paperId": "144adce2259ff5be0a1e2a685c4d6b45341c97c4",
            "title": "IMPLICATIONS OF SPEECH RECOGNITION THROUGH NLP: AN OVERVIEW",
            "abstract": "In the current state of digitalization researchers have found a leading-edge technology that delivers information to users from the digital machine and vice-versa and i.e. Natural Language Processing (NLP), one of an AI branch that made a difference. NLP is a computational representation and analysis of human language, which globally aims to facilitate naturally occurring interaction between computers and individuals whether phonically, calligraphically or textual. NLP has various booming sides in advanced world and has expands its applications in different domains. One of its application Speech recognition is a large field of research for modern-day researchers. Speech recognition, also known as automatic speech recognition (ASR) or speech-to-text, is capable of handling human speech in a written format. The main focus is on development of recognition system which enhances human-to-human communication by permitting man-machine communication through the processing of texts or speeches. It is a technology that makes it possible for people to communicate with devices. Several applications of voice recognition systems are in place and all understand diverse scrutiny challenges.This paper captures the necessity of how speech recognition actually functions, types of model, resolving challenges of ASR. Deep study on functioning of Alexa and Siri.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper captures the necessity of how speech recognition actually functions, types of model, resolving challenges of ASR, and a deep study on functioning of Alexa and Siri."
            },
            "score": 1
        }
    ],
    "novelty": "yes"
}