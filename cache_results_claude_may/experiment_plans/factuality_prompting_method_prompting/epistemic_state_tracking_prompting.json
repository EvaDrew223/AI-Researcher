{
    "topic_description": "novel prompting methods that can improve factuality and reduce hallucination of large language models",
    "idea_name": "Epistemic State Tracking Prompting",
    "raw_idea": {
        "Problem": "Large language models can easily lose track of what information is factual vs. hypothetical or uncertain when generating long sequences, leading to subtle inconsistencies and hallucinations. This is especially challenging for tasks like story generation, open-ended QA, and dialogues.",
        "Existing Methods": "Most existing methods focus on grounding the generation with retrieved evidence. However, even with grounding, models can still hallucinate by confusing the epistemic status of information over a long context.",
        "Motivation": "We take inspiration from the cognitive science concept of epistemic state tracking, which posits that humans maintain a mental state of the epistemic status of information (e.g., factual, uncertain, hypothetical, false) and use it to guide reasoning and communication. For example, when telling a story, humans keep track of what events definitely happened vs. might have happened. Explicitly tracking epistemic state helps maintain consistency.",
        "Proposed Method": "We propose augmenting prompts with annotations of epistemic state, and having the model generate its own epistemic state annotations during the generation process. For example, the prompt could include tags like <fact>, <uncertain>, <hypothesis>, <counterfactual> to mark the epistemic state of information, which the model must generate when producing its own response. This forces the model to more explicitly consider the epistemic state of the information it's generating and use that to maintain consistency. To guide the model to track epistemic state, we include a few-shot chain-of-thought prompt that steps through the reasoning of determining what information to mark with each epistemic state.",
        "Experiment Plan": "We will evaluate Epistemic State Tracking Prompting on story generation (e.g. WritingPrompts, WorldFlaws), open-ended QA (e.g. NarrativeQA, OpenBookQA), and dialogue (e.g. SocialIQA, DialogueCOPA) tasks. Baselines will include standard prompting, Chain-of-Thought prompting, and knowledge-augmented generation. We will measure automatic metrics like perplexity, GED, and FactCC, and also do human evaluation studies of coherence, consistency, factual accuracy, and level of hallucination."
    },
    "full_experiment_plan": {
        "Title": "Epistemic State Tracking Prompting Improves Factuality and Reduces Hallucination in Large Language Models",
        "Problem Statement": "Large language models can easily lose track of what information is factual vs. hypothetical or uncertain when generating long sequences, leading to subtle inconsistencies and hallucinations. This is especially challenging for tasks like story generation, open-ended QA, and dialogues.",
        "Motivation": "Most existing methods focus on grounding the generation with retrieved evidence. However, even with grounding, models can still hallucinate by confusing the epistemic status of information over a long context. We take inspiration from the cognitive science concept of epistemic state tracking, which posits that humans maintain a mental state of the epistemic status of information (e.g., factual, uncertain, hypothetical, false) and use it to guide reasoning and communication. For example, when telling a story, humans keep track of what events definitely happened vs. might have happened. Explicitly tracking epistemic state helps maintain consistency.",
        "Proposed Method": "We propose augmenting prompts with annotations of epistemic state, and having the model generate its own epistemic state annotations during the generation process. For example, the prompt could include tags like <fact>, <uncertain>, <hypothesis>, <counterfactual> to mark the epistemic state of information, which the model must generate when producing its own response. This forces the model to more explicitly consider the epistemic state of the information it's generating and use that to maintain consistency. To guide the model to track epistemic state, we include a few-shot chain-of-thought prompt that steps through the reasoning of determining what information to mark with each epistemic state.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "Evaluate Epistemic State Tracking Prompting on story generation (e.g. WritingPrompts, WorldFlaws), open-ended QA (e.g. NarrativeQA, OpenBookQA), and dialogue (e.g. SocialIQA, DialogueCOPA) tasks.",
            "Step 2: Construct Prompts": "Baselines include standard prompting, Chain-of-Thought prompting, and knowledge-augmented generation. For Epistemic State Tracking Prompting, augment prompts with epistemic state annotations (e.g. <fact>, <uncertain>, <hypothesis>, <counterfactual>) and include a few-shot chain-of-thought prompt that demonstrates reasoning about epistemic state.",
            "Step 3: Select Models": "Evaluate GPT-3.5 (text-davinci-003), GPT-4, and open-source models like Llama and MPT.",
            "Step 4: Get Results": "Generate outputs from the models on the selected datasets using the baseline and Epistemic State Tracking prompts.",
            "Step 5: Analyze Results": "Measure automatic metrics like perplexity, GED, and FactCC. Also conduct human evaluation studies of coherence, consistency, factual accuracy, and level of hallucination. Compare performance of Epistemic State Tracking Prompting to the baselines."
        },
        "Test Case Examples": {
            "Baseline Prompt Input (Standard Prompting)": "<prompt>John woke up excited for his big hiking trip. He had been planning it for weeks. The weather forecast predicted a small chance of rain. John hoped it would stay dry, but he packed his rain gear just in case. As he drove to the mountain, he noticed some dark clouds on the horizon.</prompt> What happened next in the story?",
            "Baseline Prompt Expected Output (Standard Prompting)": "John arrived at the trailhead and started hiking under overcast skies. A light rain began to fall, but John's rain jacket kept him dry. However, the rain got heavier as he climbed higher. The trail became muddy and slippery. John lost his footing and tumbled down a steep slope, badly injuring his leg. Unable to walk, John had to activate the emergency beacon on his satellite messenger. A rescue helicopter eventually spotted him and airlifted him to a hospital. John's hiking trip was ruined and he faced a long recovery.",
            "Proposed Prompt Input (Epistemic State Tracking Prompting)": "<prompt>John woke up excited for his big hiking trip. <fact>He had been planning it for weeks.</fact> <uncertain>The weather forecast predicted a small chance of rain.</uncertain> <hypothesis>John hoped it would stay dry,</hypothesis> <fact>but he packed his rain gear just in case.</fact> <fact>As he drove to the mountain, he noticed some dark clouds on the horizon.</fact></prompt> What happened next in the story? Annotate each sentence you generate with its epistemic state (<fact>, <uncertain>, <hypothesis>, <counterfactual>).",
            "Proposed Prompt Expected Output (Epistemic State Tracking Prompting)": "<fact>John arrived at the trailhead and started hiking under overcast skies.</fact> <fact>A light rain began to fall, but John's rain jacket kept him dry.</fact> <uncertain>However, the rain got heavier as he climbed higher.</uncertain> <fact>The trail became muddy and slippery.</fact> <hypothesis>John worried he might lose his footing.</hypothesis> <counterfactual>If he hadn't packed his rain gear, he would have been soaked.</counterfactual> <fact>John slowed his pace and stepped carefully.</fact> <fact>The rain eased off after an hour, and the sun peeked out from behind the clouds.</fact> <fact>John reached the summit safely and enjoyed the view.</fact> <hypothesis>He was glad he had persevered through the bad weather.</hypothesis>",
            "Explanation": "The standard prompting baseline generates a story that starts out reasonable but then hallucinates an injury and rescue that are inconsistent with the initial prompt. The Epistemic State Tracking prompt forces the model to explicitly annotate and track the epistemic state of the information, resulting in a story that is coherent and consistent with the initial facts, while clearly distinguishing hypotheticals and counterfactuals."
        },
        "Fallback Plan": "If Epistemic State Tracking Prompting does not improve factuality and reduce hallucination compared to the baselines, analyze the generated epistemic state annotations to see if they are accurate and relevant. It may be that the model struggles to correctly identify the epistemic state of information. In that case, the project could pivot to developing better few-shot prompts or fine-tuning techniques to teach the model epistemic state tracking. Alternatively, the project could analyze whether the model is able to use the epistemic state annotations correctly during generation to maintain consistency, even if it can recognize them. This could lead to insights about the model's reasoning abilities and point to architectural changes. If Epistemic State Tracking Prompting works well for some tasks but not others, comparisons between the tasks could yield insights about what kinds of consistency and factuality challenges are most important for different applications."
    },
    "novelty_queries": [
        "KeywordQuery(\"epistemic state tracking language models\")",
        "KeywordQuery(\"language models consistency factuality hallucination\")",
        "KeywordQuery(\"prompting language models factuality hallucination\")",
        "KeywordQuery(\"Epistemic State Tracking Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "035d7fce7690d61df76d7479d294e697a4319b7a",
            "paperId": "035d7fce7690d61df76d7479d294e697a4319b7a",
            "title": "Entity Tracking in Language Models",
            "abstract": "Keeping track of how states of entities change as a text or dialog unfolds is a key prerequisite to discourse understanding. Yet, there have been few systematic investigations into the ability of large language models (LLMs) to track discourse entities. In this work, we present a task probing to what extent a language model can infer the final state of an entity given an English description of the initial state and a series of state-changing operations. We use this task to first investigate whether Flan-T5, GPT-3 and GPT-3.5 can track the state of entities, and find that only GPT-3.5 models, which have been pretrained on large amounts of code, exhibit this ability. We then investigate whether smaller models pretrained primarily on text can learn to track entities, through finetuning T5 on several training/evaluation splits. While performance degrades for more complex splits, we find that even when evaluated on a different set of entities from training or longer operation sequences, a finetuned model can perform non-trivial entity tracking. Taken together, these results suggest that language models can learn to track entities but pretraining on text corpora alone does not make this capacity surface.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents a task probing to what extent a language model can infer the final state of an entity given an English description of the initial state and a series of state-changing operations, and finds that only GPT-3.5 models, which have been pretrained on large amounts of code, exhibit this ability."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving factuality and reducing hallucination in large language models for tasks like story generation, open-ended QA, and dialogues. The proposed approach is augmenting prompts with annotations of epistemic state and having the model generate its own epistemic state annotations during the generation process.\n\nThe research problem in the paper is investigating the ability of large language models to track discourse entities, i.e., keep track of how states of entities change as a text or dialog unfolds. The approach is probing language models with a task to infer the final state of an entity given an initial state and a series of state-changing operations.\n\nWhile both works involve tracking states in language models, the proposal focuses on epistemic states to improve factuality, whereas the paper focuses on entity states for discourse understanding. The methods are also quite different - prompt engineering vs. probing tasks.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "620a1a585a5e433a47103c112de17553a81fcbe6",
            "paperId": "620a1a585a5e433a47103c112de17553a81fcbe6",
            "title": "Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks",
            "abstract": "Although remarkable progress has been achieved in preventing large language model (LLM) hallucinations using instruction tuning and retrieval augmentation, it remains challenging to measure the reliability of LLMs using human-crafted evaluation data which is not available for many tasks and domains and could suffer from data leakage. Inspired by adversarial machine learning, this paper aims to develop a method of automatically generating evaluation data by appropriately modifying existing data on which LLMs behave faithfully. Specifically, this paper presents AutoDebug, an LLM-based framework to use prompting chaining to generate transferable adversarial attacks in the form of question-answering examples. We seek to understand the extent to which these examples trigger the hallucination behaviors of LLMs. We implement AutoDebug using ChatGPT and evaluate the resulting two variants of a popular open-domain question-answering dataset, Natural Questions (NQ), on a collection of open-source and proprietary LLMs under various prompting settings. Our generated evaluation data is human-readable and, as we show, humans can answer these modified questions well. Nevertheless, we observe pronounced accuracy drops across multiple LLMs including GPT-4. Our experimental results show that LLMs are likely to hallucinate in two categories of question-answering scenarios where (1) there are conflicts between knowledge given in the prompt and their parametric knowledge, or (2) the knowledge expressed in the prompt is complex. Finally, we find that the adversarial examples generated by our method are transferable across all considered LLMs. The examples generated by a small model can be used to debug a much larger model, making our approach cost-effective.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents AutoDebug, an LLM-based framework to use prompting chaining to generate transferable adversarial attacks in the form of question-answering examples, and finds that the adversarial examples generated by the method are transferable across all considered LLMs."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is reducing hallucination and improving factuality in large language models, and the proposed approach is using epistemic state tracking prompting. The research problem in the paper is automatically generating evaluation data to measure the reliability of large language models in terms of hallucination, and the proposed approach is using prompting chaining to generate transferable adversarial attacks.\n\nAlthough both works aim to address the hallucination problem in large language models, the proposal focuses on a prompting method to reduce hallucination during generation, while the paper focuses on automatically generating evaluation data to assess hallucination. The approaches are quite different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "288cb169619bde78604450adc8cb5df536ef20f1",
            "paperId": "288cb169619bde78604450adc8cb5df536ef20f1",
            "title": "Learning Chess Blindfolded: Evaluating Language Models on State Tracking",
            "abstract": "Transformer language models have made tremendous strides in natural language understanding tasks. However, the complexity of natural language makes it challenging to ascertain how accurately these models are tracking the world state underlying the text. Motivated by this issue, we consider the task of language modeling for the game of chess. Unlike natural language, chess notations describe a simple, constrained, and deterministic domain. Moreover, we observe that the appropriate choice of chess notation allows for directly probing the world state, without requiring any additional probing-related machinery. We find that: (a) With enough training data, transformer language models can learn to track pieces and predict legal moves with high accuracy when trained solely on move sequences. (b) For small training sets providing access to board state information during training can yield significant improvements. (c) The success of transformer language models is dependent on access to the entire game history i.e. \"full attention\". Approximating this full attention results in a significant performance drop. We propose this testbed as a benchmark for future work on the development and analysis of transformer language models.",
            "year": 2021,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that with enough training data, transformer language models can learn to track pieces and predict legal moves with high accuracy when trained solely on move sequences and for small training sets providing access to board state information during training can yield significant improvements."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving factuality and reducing hallucination in large language models for open-ended text generation tasks. The proposed approach is to augment prompts with epistemic state annotations and have the model generate its own annotations to better track the epistemic status of information.\n\nThe research problem in the paper is evaluating the ability of language models to accurately track world state from text alone, using the constrained domain of chess as a testbed. The approach is to train transformer language models on chess move sequences and measure their accuracy at predicting legal moves and tracking piece positions.\n\nThe key differences are:\n1) The proposal focuses on open-ended natural language tasks while the paper focuses on the narrow domain of chess. \n2) The proposal aims to improve generation quality while the paper aims to analyze language model capabilities.\n3) The proposal introduces a new prompting technique while the paper simply trains on chess notation data.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "e7ad08848d5d7c5c47673ffe0da06af443643bda",
            "paperId": "e7ad08848d5d7c5c47673ffe0da06af443643bda",
            "title": "Large Language Models are Zero-Shot Reasoners",
            "abstract": "Pretrained large language models (LLMs) are widely used in many sub-fields of natural language processing (NLP) and generally known as excellent few-shot learners with task-specific exemplars. Notably, chain of thought (CoT) prompting, a recent technique for eliciting complex multi-step reasoning through step-by-step answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difficult system-2 tasks that do not follow the standard scaling laws for LLMs. While these successes are often attributed to LLMs' ability for few-shot learning, we show that LLMs are decent zero-shot reasoners by simply adding\"Let's think step by step\"before each answer. Experimental results demonstrate that our Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics (MultiArith, GSM8K, AQUA-RAT, SVAMP), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shuffled Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on MultiArith from 17.7% to 78.7% and GSM8K from 10.4% to 40.7% with large InstructGPT model (text-davinci-002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter PaLM. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of LLMs, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside LLMs before crafting finetuning datasets or few-shot exemplars.",
            "year": 2022,
            "citationCount": 1862,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results demonstrate that the Zero-shot-CoT, using the same single prompt template, significantly outperforms zero-shot LLM performances on diverse benchmark reasoning tasks including arithmetics, symbolic reasoning, and other logical reasoning tasks, without any hand-crafted few-shot examples."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving factuality and reducing hallucination in large language models, and the proposed approach is using epistemic state tracking prompting.\n\nThe research problem in the paper is improving the zero-shot reasoning capabilities of large language models, and the proposed approach is using a simple \"Let's think step by step\" prompt without any few-shot examples.\n\nThe two works have different research problems (factuality and hallucination vs. zero-shot reasoning) and different approaches (epistemic state tracking prompting vs. a simple \"Let's think step by step\" prompt). Therefore, they are not directly relevant to each other.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "03251361c1d67c6b5badffc7059fdd7fbfea1fed",
            "paperId": "03251361c1d67c6b5badffc7059fdd7fbfea1fed",
            "title": "Statler: State-Maintaining Language Models for Embodied Reasoning",
            "abstract": "There has been a significant research interest in employing large language models to empower intelligent robots with complex reasoning. Existing work focuses on harnessing their abilities to reason about the histories of their actions and observations. In this paper, we explore a new dimension in which large language models may benefit robotics planning. In particular, we propose Statler, a framework in which large language models are prompted to maintain an estimate of the world state, which are often unobservable, and track its transition as new actions are taken. Our framework then conditions each action on the estimate of the current world state. Despite being conceptually simple, our Statler framework significantly outperforms strong competing methods (e.g., Code-as-Policies) on several robot planning tasks. Additionally, it has the potential advantage of scaling up to more challenging long-horizon planning tasks. We release our code at https://github.com/ripl/statler",
            "year": 2023,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Despite being conceptually simple, the Statler framework significantly outperforms strong competing methods (e.g., Code-as-Policies) on several robot planning tasks and has the potential advantage of scaling up to more challenging long-horizon planning tasks."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving factuality and reducing hallucination in large language models for tasks like story generation, open-ended QA, and dialogues. The proposed approach is epistemic state tracking prompting, which augments prompts with annotations of epistemic state and has the model generate its own epistemic state annotations during the generation process.\n\nThe research problem in the paper is employing large language models to empower intelligent robots with complex reasoning. The proposed approach is Statler, a framework in which large language models are prompted to maintain an estimate of the world state and track its transition as new actions are taken.\n\nThe proposal focuses on improving language models' ability to maintain consistency in generated text, while the paper focuses on using language models for robot planning. The methods are also different: epistemic state tracking prompting vs. state-maintaining prompting for robot actions.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "99832586d55f540f603637e458a292406a0ed75d",
            "paperId": "99832586d55f540f603637e458a292406a0ed75d",
            "title": "LANGUAGE MODELS",
            "abstract": "While large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. Furthermore, on two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.",
            "year": 2023,
            "citationCount": 600,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve factuality and reduce hallucination in large language models by augmenting prompts with epistemic state annotations and having the model generate its own annotations during the generation process. The paper explores using large language models to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for synergy between reasoning and acting.\n\nThe project focuses on epistemic state tracking to maintain consistency in generated text, while the paper focuses on combining reasoning traces with actions to interface with external sources. Although both aim to improve language model performance, the specific problems and approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "453fc588d97958c6fefad96e79edd896873b3e09",
            "paperId": "453fc588d97958c6fefad96e79edd896873b3e09",
            "title": "Chess as a Testbed for Language Model State Tracking",
            "abstract": "Transformer language models have made tremendous strides in natural language understanding tasks. However, the complexity of natural language makes it challenging to ascertain how accurately these models are tracking the world state underlying the text. Motivated by this issue, we consider the task of language modeling for the game of chess. Unlike natural language, chess notations describe a simple, constrained, and deterministic domain. Moreover, we observe that the appropriate choice of chess notation allows for directly probing the world state, without requiring any additional probing-related machinery. We find that: (a) With enough training data, transformer language models can learn to track pieces and predict legal moves with high accuracy when trained solely on move sequences. (b) For small training sets providing access to board state information during training can yield significant improvements. (c) The success of transformer language models is dependent on access to the entire game history i.e. \u201cfull attention\u201d. Approximating this full attention results in a significant performance drop. We propose this testbed as a benchmark for future work on the development and analysis of transformer language models.",
            "year": 2021,
            "citationCount": 19,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that with enough training data, transformer language models can learn to track pieces and predict legal moves with high accuracy when trained solely on move sequences and for small training sets providing access to board state information during training can yield significant improvements."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving factuality and reducing hallucination in large language models by explicitly tracking the epistemic state of information during generation. The approach is to augment prompts with epistemic state annotations and use chain-of-thought prompting to guide the model's reasoning about epistemic states.\n\nThe research problem in the paper is ascertaining how accurately transformer language models can track the world state underlying text, using the constrained domain of chess as a testbed. The approach is to train language models on chess move sequences and measure their ability to track piece positions and predict legal moves.\n\nThe proposal focuses on tracking epistemic states to improve language model factuality, while the paper focuses on tracking chess world states to probe language model reasoning. The domains (open-ended text vs. chess notation) and goals (reducing hallucination vs. probing reasoning abilities) are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "cd2e04598909158494e556823d9de8baa692cee2",
            "paperId": "cd2e04598909158494e556823d9de8baa692cee2",
            "title": "Towards Mitigating Hallucination in Large Language Models via Self-Reflection",
            "abstract": "Large language models (LLMs) have shown promise for generative and knowledge-intensive tasks including question-answering (QA) tasks. However, the practical deployment still faces challenges, notably the issue of\"hallucination\", where models generate plausible-sounding but unfaithful or nonsensical information. This issue becomes particularly critical in the medical domain due to the uncommon professional concepts and potential social risks involved. This paper analyses the phenomenon of hallucination in medical generative QA systems using widely adopted LLMs and datasets. Our investigation centers on the identification and comprehension of common problematic answers, with a specific emphasis on hallucination. To tackle this challenge, we present an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation. Through this feedback process, our approach steadily enhances the factuality, consistency, and entailment of the generated answers. Consequently, we harness the interactivity and multitasking ability of LLMs and produce progressively more precise and accurate answers. Experimental results on both automatic and human evaluation demonstrate the superiority of our approach in hallucination reduction compared to baselines.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper analyses the phenomenon of hallucination in medical generative QA systems using widely adopted LLMs and datasets and presents an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation that steadily enhances the factuality, consistency, and entailment of the generated answers."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve factuality and reduce hallucination in large language models by augmenting prompts with epistemic state annotations and having the model generate its own annotations during the generation process.\n\nThe paper abstract proposes an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation to enhance the factuality, consistency, and entailment of generated answers in medical question-answering systems.\n\nWhile both the project proposal and paper abstract focus on reducing hallucination in large language models, their approaches differ. The project proposal uses epistemic state tracking prompting, while the paper abstract employs an interactive self-reflection methodology.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "2751de08d6dbec07f53808231c016e96b075b06c",
            "paperId": "2751de08d6dbec07f53808231c016e96b075b06c",
            "title": "Learning to Trust Your Feelings: Leveraging Self-awareness in LLMs for Hallucination Mitigation",
            "abstract": "We evaluate the ability of Large Language Models (LLMs) to discern and express their internal knowledge state, a key factor in countering factual hallucination and ensuring reliable application of LLMs. We observe a robust self-awareness of internal knowledge state in LLMs, evidenced by over 85% accuracy in knowledge probing. However, LLMs often fail to express their internal knowledge during generation, leading to factual hallucinations. We develop an automated hallucination annotation tool, Dreamcatcher, which merges knowledge probing and consistency checking methods to rank factual preference data. Using knowledge preference as reward, We propose a Reinforcement Learning from Knowledge Feedback (RLKF) training framework, leveraging reinforcement learning to enhance the factuality and honesty of LLMs. Our experiments across multiple models show that RLKF training effectively enhances the ability of models to utilize their internal knowledge state, boosting performance in a variety of knowledge-based and honesty-related tasks.",
            "year": 2024,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Reinforcement Learning from Knowledge Feedback (RLKF) training framework is proposed, leveraging reinforcement learning to enhance the factuality and honesty of LLMs and shows that RLKF training effectively enhances the ability of models to utilize their internal knowledge state, boosting performance in a variety of knowledge-based and honesty-related tasks."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve factuality and reduce hallucination in large language models by augmenting prompts with epistemic state annotations and having the model generate its own epistemic state annotations during the generation process.\n\nThe paper abstract proposes using knowledge probing to assess the internal knowledge state of large language models and developing a reinforcement learning framework to enhance the factuality and honesty of the models by leveraging knowledge preference as a reward.\n\nWhile both works address the problem of factual hallucination in large language models, the project proposal focuses on explicit epistemic state tracking through prompt augmentation, while the paper abstract proposes using knowledge probing and reinforcement learning to enhance factuality. The approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "1b387e3fbec0447c8bf2dcee21f6db59cdddf698",
            "paperId": "1b387e3fbec0447c8bf2dcee21f6db59cdddf698",
            "title": "The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models",
            "abstract": "In the era of large language models (LLMs), hallucination (i.e., the tendency to generate factually incorrect content) poses great challenge to trustworthy and reliable deployment of LLMs in real-world applications. To tackle the LLM hallucination, three key questions should be well studied: how to detect hallucinations (detection), why do LLMs hallucinate (source), and what can be done to mitigate them (mitigation). To address these challenges, this work presents a systematic empirical study on LLM hallucination, focused on the the three aspects of hallucination detection, source and mitigation. Specially, we construct a new hallucination benchmark HaluEval 2.0, and designs a simple yet effective detection method for LLM hallucination. Furthermore, we zoom into the different training or utilization stages of LLMs and extensively analyze the potential factors that lead to the LLM hallucination. Finally, we implement and examine a series of widely used techniques to mitigate the hallucinations in LLMs. Our work has led to several important findings to understand the hallucination origin and mitigate the hallucinations in LLMs. Our code and data can be accessed at https://github.com/RUCAIBox/HaluEval-2.0.",
            "year": 2024,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work constructs a new hallucination benchmark HaluEval 2.0, designs a simple yet effective detection method for LLM hallucination, and implements and examines a series of widely used techniques to mitigate the hallucinations in LLMs."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve factuality and reduce hallucination in large language models by augmenting prompts with epistemic state annotations and having the model generate its own annotations during the generation process. The paper focuses on studying hallucination in large language models, including detecting hallucinations, analyzing their sources, and examining techniques to mitigate them.\n\nWhile both the project proposal and the paper are concerned with the problem of hallucination in large language models, their approaches differ. The project proposal suggests a specific method of using epistemic state tracking prompting to mitigate hallucinations, while the paper takes a broader approach of studying various aspects of hallucination, including detection, source analysis, and mitigation techniques.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "3a89e289e2dd29f5e52a2bf354a637762b661257",
            "paperId": "3a89e289e2dd29f5e52a2bf354a637762b661257",
            "title": "Fine-tuning Language Models for Factuality",
            "abstract": "The fluency and creativity of large pre-trained language models (LLMs) have led to their widespread use, sometimes even as a replacement for traditional search engines. Yet language models are prone to making convincing but factually inaccurate claims, often referred to as 'hallucinations.' These errors can inadvertently spread misinformation or harmfully perpetuate misconceptions. Further, manual fact-checking of model responses is a time-consuming process, making human factuality labels expensive to acquire. In this work, we fine-tune language models to be more factual, without human labeling and targeting more open-ended generation settings than past work. We leverage two key recent innovations in NLP to do so. First, several recent works have proposed methods for judging the factuality of open-ended text by measuring consistency with an external knowledge base or simply a large model's confidence scores. Second, the direct preference optimization algorithm enables straightforward fine-tuning of language models on objectives other than supervised imitation, using a preference ranking over possible model responses. We show that learning from automatically generated factuality preference rankings, generated either through existing retrieval systems or our novel retrieval-free approach, significantly improves the factuality (percent of generated claims that are correct) of Llama-2 on held-out topics compared with RLHF or decoding strategies targeted at factuality. At 7B scale, compared to Llama-2-chat, we observe 58% and 40% reduction in factual error rate when generating biographies and answering medical questions, respectively.",
            "year": 2023,
            "citationCount": 56,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that learning from automatically generated factuality preference rankings, generated either through existing retrieval systems or the novel retrieval-free approach, significantly improves the factuality of Llama-2 on held-out topics compared with RLHF or decoding strategies targeted at factuality."
            },
            "score": 6
        },
        {
            "id": "234dad3daec8db281882c25bb704eefa8970bdc7",
            "paperId": "234dad3daec8db281882c25bb704eefa8970bdc7",
            "title": "Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs",
            "abstract": "In recent years, large language models (LLMs) have drawn significant attention due to their impressive emergent capabilities that were not observed in earlier language models. One emerging area where LLMs have been widely used in recent times is the utilization of LLMs as the evaluator of the texts generated by various generative models. In this paper, we also explore the possibility of whether LLMs are reliable in assessing the factual consistency of summaries generated by text generation models. We first propose a new approach to evaluate the factuality score using LLMs by utilizing the same LLM to perform all steps in the question-answering-based factuality scoring pipeline. Subsequently, we study the performance of various LLMs to directly score the factuality. Our evaluation is conducted in traditional benchmarks by comparing their correlation with human annotations. Contrary to expectations, our findings revealed that none of the factuality metrics showed any significant correlations (e.g., coefficient scores greater than 0.3) to human evaluations of factuality for GPT-4, PaLM-2, and Claude-2, with the only exception being GPT-3.5 in two subcategories of factuality. Nonetheless, our findings are consistent across almost all factual error types, suggesting a fundamental limitation in the ability of current LLMs to assess factuality.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new approach to evaluate the factuality score using LLMs is proposed by utilizing the same LLM to perform all steps in the question-answering-based factuality scoring pipeline, and it is revealed that none of the factuality metrics showed any significant correlations to human evaluations of factuality."
            },
            "score": 6
        },
        {
            "id": "f543f1d36e4556b50b160c68fd18da3d7db40867",
            "paperId": "f543f1d36e4556b50b160c68fd18da3d7db40867",
            "title": "Hallucination Augmented Recitations for Language Models",
            "abstract": "Attribution is a key concept in large language models (LLMs) as it enables control over information sources and enhances the factuality of LLMs. While existing approaches utilize open book question answering to improve attribution, factual datasets may reward language models to recall facts that they already know from their pretraining data, not attribution. In contrast, counterfactual open book QA datasets would further improve attribution because the answer could only be grounded in the given text. We propose Hallucination Augmented Recitations (HAR) for creating counterfactual datasets by utilizing hallucination in LLMs to improve attribution. For open book QA as a case study, we demonstrate that models finetuned with our counterfactual datasets improve text grounding, leading to better open book QA performance, with up to an 8.0% increase in F1 score. Our counterfactual dataset leads to significantly better performance than using humanannotated factual datasets, even with 4x smaller datasets and 4x smaller models. We observe that improvements are consistent across various model sizes and datasets, including multi-hop, biomedical, and adversarial QA datasets.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes Hallucination Augmented Recitations (HAR) for creating counterfactual datasets by utilizing hallucination in LLMs to improve attribution and demonstrates that improvements are consistent across various model sizes and datasets, including multi-hop, biomedical, and adversarial QA datasets."
            },
            "score": 6
        },
        {
            "id": "7c1707db9aafd209aa93db3251e7ebd593d55876",
            "paperId": "7c1707db9aafd209aa93db3251e7ebd593d55876",
            "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
            "abstract": "Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose\"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods.",
            "year": 2023,
            "citationCount": 155,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes \"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database, and demonstrates that it can detect non-factual and factual sentences and rank passages in terms of factuality."
            },
            "score": 6
        },
        {
            "id": "41a41c75ba336dec98d58c563605f261019e5df0",
            "paperId": "41a41c75ba336dec98d58c563605f261019e5df0",
            "title": "\u201cAccording to . . . \u201d: Prompting Language Models Improves Quoting from Pre-Training Data",
            "abstract": "Large Language Models (LLMs) may hallucinate and generate fake information, despite pre-training on factual data. Inspired by the journalistic device of \u201caccording to sources\u201d, we propose according-to prompting: directing LLMs to ground responses against previously observed text. To quantify this grounding, we propose a novel evaluation metric (QUIP-Score) that measures the extent to which model-produced answers are directly found in underlying text corpora. We illustrate with experiments on three corpora (Wikipedia, PubMed, and the U.S. legal tax code) that these prompts improve grounding under our metrics, with the additional benefit of often improving end-task performance. Furthermore, prompts that ask the model to decrease grounding (or to ground to other corpora) indeed decrease QUIP-Score, indicating the ability of LLMs to increase or decrease grounded generations on request.",
            "year": 2023,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "According-to prompting is proposed: directing LLMs to ground responses against previously observed text, to quantify this grounding, and proposes a novel evaluation metric (QUIP-Score) that measures the extent to which model-produced answers are directly found in underlying text corpora."
            },
            "score": 6
        },
        {
            "id": "2fae69cea48d332c5788537a0b5e9e76c10e3baf",
            "paperId": "2fae69cea48d332c5788537a0b5e9e76c10e3baf",
            "title": "Enhancing Uncertainty-Based Hallucination Detection with Stronger Focus",
            "abstract": "Large Language Models (LLMs) have gained significant popularity for their impressive performance across diverse fields. However, LLMs are prone to hallucinate untruthful or nonsensical outputs that fail to meet user expectations in many real-world applications. Existing works for detecting hallucinations in LLMs either rely on external knowledge for reference retrieval or require sampling multiple responses from the LLM for consistency verification, making these methods costly and inefficient. In this paper, we propose a novel reference-free, uncertainty-based method for detecting hallucinations in LLMs. Our approach imitates human focus in factuality checking from three aspects: 1) focus on the most informative and important keywords in the given text; 2) focus on the unreliable tokens in historical context which may lead to a cascade of hallucinations; and 3) focus on the token properties such as token type and token frequency. Experimental results on relevant datasets demonstrate the effectiveness of our proposed method, which achieves state-of-the-art performance across all the evaluation metrics and eliminates the need for additional information.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel reference-free, uncertainty-based method for detecting hallucinations in LLMs that achieves state-of-the-art performance across all the evaluation metrics and eliminates the need for additional information."
            },
            "score": 5
        },
        {
            "id": "8936192d69bb78afd09d83f3ceafd18dea32a8fd",
            "paperId": "8936192d69bb78afd09d83f3ceafd18dea32a8fd",
            "title": "Self-Consistent Decoding for More Factual Open Responses",
            "abstract": "Self-consistency has emerged as a powerful method for improving the accuracy of short answers generated by large language models. As previously defined, it only concerns the accuracy of a final answer parsed from generated text. In this work, we extend the idea to open response generation, by integrating voting into the decoding method. Each output sentence is selected from among multiple samples, conditioning on the previous selections, based on a simple token overlap score. We compare this\"Sample&Select\"method to greedy decoding, beam search, nucleus sampling, and the recently introduced hallucination avoiding decoders of DoLA, P-CRR, and S-CRR. We show that Sample&Select improves factuality by a 30% relative margin against these decoders in NLI-based evaluation on the subsets of CNN/DM and XSum used in the FRANK benchmark, while maintaining comparable ROUGE-1 F1 scores against reference summaries. We collect human verifications of the generated summaries, confirming the factual superiority of our method.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work extends the idea of self-consistency to open response generation, by integrating voting into the decoding method, and shows that Sample&Select improves factuality by a 30% relative margin against these decoders in NLI-based evaluation on the subsets of CNN/DM and XSum used in the FRANK benchmark."
            },
            "score": 5
        },
        {
            "id": "6af460d34bfc8e955e43fbe15cedcf329b48bc19",
            "paperId": "6af460d34bfc8e955e43fbe15cedcf329b48bc19",
            "title": "SAC3: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency",
            "abstract": "Hallucination detection is a critical step toward understanding the trustworthiness of modern language models (LMs). To achieve this goal, we re-examine existing detection approaches based on the self-consistency of LMs and uncover two types of hallucinations resulting from 1) question-level and 2) model-level, which cannot be effectively identified through self-consistency check alone. Building upon this discovery, we propose a novel sampling-based method, i.e., semantic-aware cross-check consistency (SAC3) that expands on the principle of self-consistency checking. Our SAC3 approach incorporates additional mechanisms to detect both question-level and model-level hallucinations by leveraging advances including semantically equivalent question perturbation and cross-model response consistency checking. Through extensive and systematic empirical analysis, we demonstrate that SAC3 outperforms the state of the art in detecting both non-factual and factual statements across multiple question-answering and open-domain generation benchmarks.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel sampling-based method, i.e., semantic-aware cross-check consistency (SAC3) that expands on the principle of self-consistency checking and demonstrates that SAC3 outperforms the state of the art in detecting both non-factual and factual statements across multiple question-answering and open-domain generation benchmarks."
            },
            "score": 5
        },
        {
            "id": "152d9a231c00d4495c9bc4a466f42165ce2e2164",
            "paperId": "152d9a231c00d4495c9bc4a466f42165ce2e2164",
            "title": "Evaluating Factual Consistency of Summaries with Large Language Models",
            "abstract": "Detecting factual errors in summaries has been an important and challenging subject in summarization research. Inspired by the emergent ability of large language models (LLMs), we explore evaluating factual consistency of summaries by directly prompting LLMs. We present a comprehensive empirical study to assess the ability of LLMs as factual consistency evaluators, which consists of (1) analyzing different LLMs such as the GPT model series and Flan-T5; (2) investigating a variety of prompting methods including vanilla prompting, chain-of-thought prompting, and a sentence-by-sentence prompting method to tackle long summaries; and (3) evaluating on diverse summaries generated by multiple summarization systems, ranging from pre-transformer methods to SOTA pretrained models. Our experiments demonstrate that prompting LLMs is able to outperform the previous best factuality systems in all settings, by up to 12.2 absolute points in terms of the binary classification accuracy on inconsistency detection.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A comprehensive empirical study to assess the ability of LLMs as factual consistency evaluators, which consists of analyzing different LLMs such as the GPT model series and Flan-T5 and investigating a variety of prompting methods including vanilla prompting, chain-of-thought prompting, and a sentence-by-sentence prompting method to tackle long summaries."
            },
            "score": 5
        },
        {
            "id": "be177300487b6d0f25e6cade9a31900454b13281",
            "paperId": "be177300487b6d0f25e6cade9a31900454b13281",
            "title": "FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation",
            "abstract": "Most large language models (LLMs) are trained once and never updated; thus, they lack the ability to dynamically adapt to our ever-changing world. In this work, we perform a detailed study of the factuality of LLM-generated text in the context of answering questions that test current world knowledge. Specifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a diverse range of question and answer types, including questions that require fast-changing world knowledge as well as questions with false premises that need to be debunked. We benchmark a diverse array of both closed and open-source LLMs under a two-mode evaluation procedure that allows us to measure both correctness and hallucination. Through human evaluations involving more than 50K judgments, we shed light on limitations of these models and demonstrate significant room for improvement: for instance, all models (regardless of model size) struggle on questions that involve fast-changing knowledge and false premises. Motivated by these results, we present FreshPrompt, a simple few-shot prompting method that substantially boosts the performance of an LLM on FreshQA by incorporating relevant and up-to-date information retrieved from a search engine into the prompt. Our experiments show that FreshPrompt outperforms both competing search engine-augmented prompting methods such as Self-Ask (Press et al., 2022) as well as commercial systems such as Perplexity.AI. Further analysis of FreshPrompt reveals that both the number of retrieved evidences and their order play a key role in influencing the correctness of LLM-generated answers. Additionally, instructing the LLM to generate concise and direct answers helps reduce hallucination compared to encouraging more verbose answers. To facilitate future work, we release FreshQA at github.com/freshllms/freshqa and commit to updating it at regular intervals.",
            "year": 2023,
            "citationCount": 51,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A detailed study of the factuality of LLM-generated text in the context of answering questions that test current world knowledge and introduces FreshQA, a novel dynamic QA benchmark encompassing a diverse range of question and answer types, including questions that require fast-changing world knowledge as well as questions with false premises that need to be debunked."
            },
            "score": 5
        },
        {
            "id": "c18e13ba65c7247774301314d181c87ee5ebc847",
            "paperId": "c18e13ba65c7247774301314d181c87ee5ebc847",
            "title": "Do Language Models Know When They\u2019re Hallucinating References?",
            "abstract": "State-of-the-art language models (LMs) are notoriously susceptible to generating hallucinated information. Such inaccurate outputs not only undermine the reliability of these models but also limit their use and raise serious concerns about misinformation and propaganda. In this work, we focus on hallucinated book and article references and present them as the \u201cmodel organism\u201d of language model hallucination research, due to their frequent and easy-to-discern nature. We posit that if a language model cites a particular reference in its output, then it should ideally possess sufficient information about its authors and content, among other relevant details. Using this basic insight, we illustrate that one can identify hallucinated references without ever consulting any external resources, by asking a set of direct or indirect queries to the language model about the references. These queries can be considered as \u201cconsistency checks.\u201d Our findings highlight that while LMs, including GPT-4, often produce inconsistent author lists for hallucinated references, they also often accurately recall the authors of real references. In this sense, the LM can be said to \u201cknow\u201d when it is hallucinating references. Furthermore, these findings show how hallucinated references can be dissected to shed light on their nature.",
            "year": 2023,
            "citationCount": 44,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work highlights that while LMs, including GPT-4, often produce inconsistent author lists for hallucinated references, they also often accurately recall the authors of real references and shows how hallucinated references can be dissected to shed light on their nature."
            },
            "score": 5
        },
        {
            "id": "8a9ede4be8458629d54451d241a8d0ae318d471b",
            "paperId": "8a9ede4be8458629d54451d241a8d0ae318d471b",
            "title": "Prompting with Divide-and-Conquer Program Makes Large Language Models Discerning to Hallucination and Deception",
            "abstract": "Foundation models, such as Large language Models (LLMs), have attracted significant amount of interest due to their large number of applications. Existing works show that appropriate prompt design, such as Chain-of-Thoughts, can unlock LLM's powerful capacity in diverse areas. However, when handling tasks involving repetitive sub-tasks and/or deceptive contents, such as arithmetic calculation and article-level fake news detection, existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination. To make LLM more discerning to such intermediate errors, we propose to guide LLM with a Divide-and-Conquer program that simultaneously ensures superior expressive power and disentangles task decomposition, sub-task resolution, and resolution assembly process. Theoretic analysis reveals that our strategy can guide LLM to extend the expressive power of fixed-depth Transformer. Experiments indicate that our proposed method can achieve better performance than typical prompting strategies in tasks bothered by intermediate errors and deceptive contents, such as large integer multiplication, hallucination detection and misinformation detection.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes to guide LLM with a Divide-and-Conquer program that simultaneously ensures superior expressive power and disentangles task decomposition, sub-task resolution, and resolution assembly process andoretic analysis reveals that this strategy can guide LLM to extend the expressive power of fixed-depth Transformer."
            },
            "score": 5
        },
        {
            "id": "9efa81ec4954b0859c47dad8f42edfaf8bced69b",
            "paperId": "9efa81ec4954b0859c47dad8f42edfaf8bced69b",
            "title": "Boosting Language Models Reasoning with Chain-of-Knowledge Prompting",
            "abstract": "Recently, Chain-of-Thought (CoT) prompting has delivered success on complex reasoning tasks, which aims at designing a simple prompt like ``Let's think step by step'' or multiple in-context exemplars with well-designed rationales to elicit Large Language Models (LLMs) to generate intermediate reasoning steps. However, the generated rationales often come with mistakes, making unfactual and unfaithful reasoning chains. To mitigate this brittleness, we propose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple. This is inspired by our human behaviors, i.e., we can draw a mind map or knowledge map as the reasoning evidence in the brain before answering a complex question. Benefiting from CoK, we additionally introduce a F^2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness. For the unreliable response, the wrong evidence can be indicated to prompt the LLM to rethink. Extensive experiments demonstrate that our method can further improve the performance of commonsense, factual, symbolic, and arithmetic reasoning tasks.",
            "year": 2023,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel Chain-of-Knowledge prompting, where it aims at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple, and introduces a F^2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness."
            },
            "score": 5
        },
        {
            "id": "0f6fe87afd1a3571f77c790893b03717e5d0422a",
            "paperId": "0f6fe87afd1a3571f77c790893b03717e5d0422a",
            "title": "Beyond Factuality: A Comprehensive Evaluation of Large Language Models as Knowledge Generators",
            "abstract": "Large language models (LLMs) outperform information retrieval techniques for downstream knowledge-intensive tasks when being prompted to generate world knowledge. However, community concerns abound regarding the factuality and potential implications of using this uncensored knowledge. In light of this, we introduce CONNER, a COmpreheNsive kNowledge Evaluation fRamework, designed to systematically and automatically evaluate generated knowledge from six important perspectives -- Factuality, Relevance, Coherence, Informativeness, Helpfulness and Validity. We conduct an extensive empirical analysis of the generated knowledge from three different types of LLMs on two widely studied knowledge-intensive tasks, i.e., open-domain question answering and knowledge-grounded dialogue. Surprisingly, our study reveals that the factuality of generated knowledge, even if lower, does not significantly hinder downstream tasks. Instead, the relevance and coherence of the outputs are more important than small factual mistakes. Further, we show how to use CONNER to improve knowledge-intensive tasks by designing two strategies: Prompt Engineering and Knowledge Selection. Our evaluation code and LLM-generated knowledge with human annotations will be released to facilitate future research.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Surprisingly, the study reveals that the factuality of generated knowledge, even if lower, does not significantly hinder downstream tasks and the relevance and coherence of the outputs are more important than small factual mistakes."
            },
            "score": 5
        },
        {
            "id": "2cbdd4ef5a14dbf3446e1d5b0ecf4b222ded820d",
            "paperId": "2cbdd4ef5a14dbf3446e1d5b0ecf4b222ded820d",
            "title": "Semantic Parsing by Large Language Models for Intricate Updating Strategies of Zero-Shot Dialogue State Tracking",
            "abstract": "Zero-shot Dialogue State Tracking (DST) addresses the challenge of acquiring and annotating task-oriented dialogues, which can be time-consuming and costly. However, DST extends beyond simple slot-filling and requires effective updating strategies for tracking dialogue state as conversations progress. In this paper, we propose ParsingDST, a new In-Context Learning (ICL) method, to introduce additional intricate updating strategies in zero-shot DST. Our approach reformulates the DST task by leveraging powerful Large Language Models (LLMs) and translating the original dialogue text to JSON through semantic parsing as an intermediate state. We also design a novel framework that includes more modules to ensure the effectiveness of updating strategies in the text-to-JSON process. Experimental results demonstrate that our approach outperforms existing zero-shot DST methods on MultiWOZ, exhibiting significant improvements in Joint Goal Accuracy (JGA) and slot accuracy compared to existing ICL methods. Our code has been released.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ParsingDST, a new In-Context Learning (ICL) method, is proposed to introduce additional intricate updating strategies in zero-shot DST, reformulates the DST task by leveraging powerful Large Language Models (LLMs) and translating the original dialogue text to JSON through semantic parsing as an intermediate state."
            },
            "score": 4
        },
        {
            "id": "59ef1b67c5f238d5d6d175d84fb6b239b4221a97",
            "paperId": "59ef1b67c5f238d5d6d175d84fb6b239b4221a97",
            "title": "Stabilized In-Context Learning with Pre-trained Language Models for Few Shot Dialogue State Tracking",
            "abstract": "Prompt-based methods with large pre-trained language models (PLMs) have shown impressive unaided performance across many NLP tasks. These models improve even further with the addition of a few labeled in-context exemplars to guide output generation. However, for more complex tasks such as dialogue state tracking (DST), designing prompts that reliably convey the desired intent is nontrivial, leading to unstable results. Furthermore, building in-context exemplars for dialogue tasks is difficult because conversational contexts are long while model input lengths are relatively short.To overcome these issues we first adapt a meta-learning scheme to the dialogue domain which stabilizes the ability of the model to perform well under various prompts. We additionally design a novel training method to improve upon vanilla retrieval mechanisms to find ideal in-context examples. Finally, we introduce a saliency model to limit dialogue text length, allowing us to include more exemplars per query. In effect, we are able to achieve highly competitive results for few-shot DST on MultiWOZ.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A meta-learning scheme is adapted to the dialogue domain which stabilizes the ability of the model to perform well under various prompts and introduces a saliency model to limit dialogue text length, allowing for highly competitive results for few-shot DST on MultiWOZ."
            },
            "score": 4
        },
        {
            "id": "00a48c76e123ab77f301bf4dfd88b9b376b234c6",
            "paperId": "00a48c76e123ab77f301bf4dfd88b9b376b234c6",
            "title": "Chat with the Environment: Interactive Multimodal Perception Using Large Language Models",
            "abstract": "Programming robot behavior in a complex world faces challenges on multiple levels, from dextrous low-level skills to high-level planning and reasoning. Recent pre-trained Large Language Models (LLMs) have shown remarkable reasoning ability in few-shot robotic planning. However, it remains challenging to ground LLMs in multimodal sensory input and continuous action output, while enabling a robot to interact with its environment and acquire novel information as its policies unfold. We develop a robot interaction scenario with a partially observable state, which necessitates a robot to decide on a range of epistemic actions in order to sample sensory information among multiple modalities, before being able to execute the task correctly. An interactive perception framework is therefore proposed with an LLM as its backbone, whose ability is exploited to instruct epistemic actions and to reason over the resulting multimodal sensations (vision, sound, haptics, proprioception), as well as to plan an entire task execution based on the interactively acquired information. Our study demonstrates that LLMs can provide high-level planning and reasoning skills and control interactive robot behavior in a multimodal environment, while multimodal modules with the context of the environmental state help ground the LLMs and extend their processing ability. The project website can be found at https://matcha-model.github.io/.",
            "year": 2023,
            "citationCount": 21,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study develops a robot interaction scenario with a partially observable state, which necessitates a robot to decide on a range of epistemic actions in order to sample sensory information among multiple modalities, before being able to execute the task correctly."
            },
            "score": 4
        },
        {
            "id": "9c5a5e932139621da37c93d48f8a8df40a9c61d4",
            "paperId": "9c5a5e932139621da37c93d48f8a8df40a9c61d4",
            "title": "Dialogue State Tracking with a Language Model using Schema-Driven Prompting",
            "abstract": "Task-oriented conversational systems often use dialogue state tracking to represent the user\u2019s intentions, which involves filling in values of pre-defined slots. Many approaches have been proposed, often using task-specific architectures with special-purpose classifiers. Recently, good results have been obtained using more general architectures based on pretrained language models. Here, we introduce a new variation of the language modeling approach that uses schema-driven prompting to provide task-aware history encoding that is used for both categorical and non-categorical slots. We further improve performance by augmenting the prompting with schema descriptions, a naturally occurring source of in-domain knowledge. Our purely generative system achieves state-of-the-art performance on MultiWOZ 2.2 and achieves competitive performance on two other benchmarks: MultiWOZ 2.1 and M2M. The data and code will be available at https://github.com/chiahsuan156/DST-as-Prompting.",
            "year": 2021,
            "citationCount": 102,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new variation of the language modeling approach that uses schema-driven prompting to provide task-aware history encoding that is used for both categorical and non-categorical slots is introduced."
            },
            "score": 4
        },
        {
            "id": "dce4e4cf1cd8c42b8a400280a48283234ad7aafb",
            "paperId": "dce4e4cf1cd8c42b8a400280a48283234ad7aafb",
            "title": "Are Large Language Models All You Need for Task-Oriented Dialogue?",
            "abstract": "Instruction-finetuned large language models (LLMs) gained a huge popularity recently, thanks to their ability to interact with users through conversation. In this work, we aim to evaluate their ability to complete multi-turn tasks and interact with external databases in the context of established task-oriented dialogue benchmarks. We show that in explicit belief state tracking, LLMs underperform compared to specialized task-specific models. Nevertheless, they show some ability to guide the dialogue to a successful ending through their generated responses if they are provided with correct slot values. Furthermore, this ability improves with few-shot in-domain examples.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that in explicit belief state tracking, LLMs underperform compared to specialized task-specific models, but they show some ability to guide the dialogue to a successful ending through their generated responses if they are provided with correct slot values."
            },
            "score": 4
        },
        {
            "id": "b645e706651391eca1f692e7f560051c21b3dea4",
            "paperId": "b645e706651391eca1f692e7f560051c21b3dea4",
            "title": "In-Context Learning for Few-Shot Dialogue State Tracking",
            "abstract": "Collecting and annotating task-oriented dialogues is time-consuming and costly; thus, zero and few shot learning could greatly benefit dialogue state tracking (DST). In this work, we propose an in-context learning (ICL) framework for zero-shot and few-shot learning DST, where a large pre-trained language model (LM) takes a test instance and a few exemplars as input, and directly decodes the dialogue state without any parameter updates. To better leverage a tabular domain description in the LM prompt, we reformulate DST into a text-to-SQL problem. We also propose a novel approach to retrieve annotated dialogues as exemplars. Empirical results on MultiWOZ show that our method IC-DST substantially outperforms previous fine-tuned state-of-the-art models in few-shot settings. In addition, we test IC-DST in zero-shot settings, in which the model only takes a fixed task instruction as input, finding that it outperforms previous zero-shot methods by a large margin.",
            "year": 2022,
            "citationCount": 38,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes an in-context learning (ICL) framework for zero-shot and few-shot learning DST, where a large pre-trained language model takes a test instance and a few exemplars as input, and directly decodes the dialogue state without any parameter updates."
            },
            "score": 4
        },
        {
            "id": "723a6340ee641e190b22bb47455d05e3b4237179",
            "paperId": "723a6340ee641e190b22bb47455d05e3b4237179",
            "title": "Large Language Models as Zero-shot Dialogue State Tracker through Function Calling",
            "abstract": "Large language models (LLMs) are increasingly prevalent in conversational systems due to their advanced understanding and generative capabilities in general contexts. However, their effectiveness in task-oriented dialogues (TOD), which requires not only response generation but also effective dialogue state tracking (DST) within specific tasks and domains, remains less satisfying. In this work, we propose a novel approach FnCTOD for solving DST with LLMs through function calling. This method improves zero-shot DST, allowing adaptation to diverse domains without extensive data collection or model tuning. Our experimental results demonstrate that our approach achieves exceptional performance with both modestly sized open-source and also proprietary LLMs: with in-context prompting it enables various 7B or 13B parameter models to surpass the previous state-of-the-art (SOTA) achieved by ChatGPT, and improves ChatGPT's performance beating the SOTA by 5.6% average joint goal accuracy (JGA). Individual model results for GPT-3.5 and GPT-4 are boosted by 4.8% and 14%, respectively. We also show that by fine-tuning on a small collection of diverse task-oriented dialogues, we can equip modest at https://github.com/facebookresearch/FnCTOD",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel approach FnCTOD for solving DST with LLMs through function calling that improves zero-shot DST, allowing adaptation to diverse domains without extensive data collection or model tuning."
            },
            "score": 4
        },
        {
            "id": "d38af39275524068e7aab12fa8d54d342eff7dfe",
            "paperId": "d38af39275524068e7aab12fa8d54d342eff7dfe",
            "title": "UFO: a Unified and Flexible Framework for Evaluating Factuality of Large Language Models",
            "abstract": "Large language models (LLMs) may generate text that lacks consistency with human knowledge, leading to factual inaccuracies or \\textit{hallucination}. Existing research for evaluating the factuality of LLMs involves extracting fact claims using an LLM and verifying them against a predefined fact source. However, these evaluation metrics are task-specific, and not scalable, and the substitutability of fact sources in different tasks is under-explored. To address these challenges, we categorize four available fact sources: human-written evidence, reference documents, search engine results, and LLM knowledge, along with five text generation tasks containing six representative datasets. Then, we propose \\texttt{UFO}, an LLM-based unified and flexible evaluation framework to verify facts against plug-and-play fact sources. We implement five evaluation scenarios based on this framework. Experimental results show that for most QA tasks, human-written evidence and reference documents are crucial, and they can substitute for each other in retrieval-augmented QA tasks. In news fact generation tasks, search engine results and LLM knowledge are essential. Our dataset and code are available at \\url{https://github.com/WaldenRUC/UFO}.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes \\texttt{UFO}, an LLM-based unified and flexible evaluation framework to verify facts against plug-and-play fact sources, and implements five evaluation scenarios based on this framework."
            },
            "score": 4
        },
        {
            "id": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
            "paperId": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
            "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such\"society of minds\"approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.",
            "year": 2023,
            "citationCount": 206,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer is presented, indicating that this approach significantly enhances mathematical and strategic reasoning across a number of tasks."
            },
            "score": 4
        },
        {
            "id": "663d743272e9ab04f54d9105a3c3a3f6e22dd1dd",
            "paperId": "663d743272e9ab04f54d9105a3c3a3f6e22dd1dd",
            "title": "FactKB: Generalizable Factuality Evaluation using Language Models Enhanced with Factual Knowledge",
            "abstract": "Evaluating the factual consistency of automatically generated summaries is essential for the progress and adoption of reliable summarization systems. Despite recent advances, existing factuality evaluation models are not robust, being especially prone to entity and relation errors in new domains. We propose FactKB, a simple new approach to factuality evaluation that is generalizable across domains, in particular with respect to entities and relations. FactKB is based on language models pretrained using facts extracted from external knowledge bases. We introduce three types of complementary factuality pretraining objectives based on direct entity facts, facts grounded in auxiliary knowledge about entities, and facts constructed compositionally through knowledge base walks. The resulting factuality evaluation model achieves state-of-the-art performance on two in-domain news summarization benchmarks as well as on three out-of-domain scientific literature datasets. Further analysis of FactKB shows improved ability to detect erroneous entities and relations in summaries and is robust and generalizable across domains.",
            "year": 2023,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "FactKB is a simple new approach to factuality evaluation that is generalizable across domains, in particular with respect to entities and relations and shows improved ability to detect erroneous entities and relation in summaries."
            },
            "score": 4
        },
        {
            "id": "6a637773923c39d172f31beca9afb3e086f57d55",
            "paperId": "6a637773923c39d172f31beca9afb3e086f57d55",
            "title": "Evaluation of Hallucination and Robustness for Large Language Models",
            "abstract": "As large language models (LLMs) rapidly advance, rigorous testing and evaluation of these models grows increasingly crucial. To address this need, we have developed three types of questions: Chinese contextual, English contextual, and language context-independent. Testing in both Chinese and English probes the LLMs' hallucination tendencies. We investigate the impact of language on hallucinations from two perspectives: the type of language used in the input prompt and the cultural context underlying the prompt's content. Additionally, 52 multi-domain single-choice questions from C-EVAL are presented in original and randomized order to assess robustness to perturbations. Among the five LLMs, the tests demonstrate GPT -4 has the strongest anti-hallucination and robustness capabilities, answering with greater accuracy, consistency, and reliability. ChatGLM ranks second and outperforms GPT -4 on Chinese context-dependent questions. Emergent testing phenomena are analyzed from the user's perspective. Hallucinated responses are categorized and potential causal factors leading to hallucination and fragility are examined. Based on these findings, viable avenues for improvement are proposed.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Three types of questions are developed: Chinese contextual, English contextual, and language context-independent, which demonstrate GPT -4 has the strongest anti-hallucination and robustness capabilities, answering with greater accuracy, consistency, and reliability."
            },
            "score": 4
        },
        {
            "id": "ed5020eeda1fbe8c29b1282d654b34abee22d90f",
            "paperId": "ed5020eeda1fbe8c29b1282d654b34abee22d90f",
            "title": "DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models",
            "abstract": "Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained LLMs that does not require conditioning on retrieved external knowledge nor additional fine-tuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an LLMs has generally been shown to be localized to particular transformer layers. We find that this Decoding by Contrasting Layers (DoLa) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA by 12-17% absolute points, demonstrating its potential in making LLMs reliably generate truthful facts.",
            "year": 2023,
            "citationCount": 35,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA by 12-17% absolute points, demonstrating its potential in making LLMs reliably generate truthful facts."
            },
            "score": 4
        },
        {
            "id": "e0384ba36555232c587d4a80d527895a095a9001",
            "paperId": "e0384ba36555232c587d4a80d527895a095a9001",
            "title": "HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models",
            "abstract": "Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the Hallucination Evaluation benchmark for Large Language Models (HaluEval), a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing hallucination. To generate these samples, we propose a ChatGPT-based two-step framework, i.e., sampling-then-filtering. Besides, we also hire some human labelers to annotate the hallucinations in ChatGPT responses. The empirical results suggest that ChatGPT is likely to generate hallucinated content in specific topics by fabricating unverifiable information (i.e., about $19.5\\%$ responses). Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts. However, our experiments also prove that providing external knowledge or adding reasoning steps can help LLMs recognize hallucinations. Our benchmark can be accessed at https://github.com/RUCAIBox/HaluEval.",
            "year": 2023,
            "citationCount": 57,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Hallucination Evaluation benchmark for Large Language Models (HaluEval), a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing hallucination, is introduced and it is proved that providing external knowledge or adding reasoning steps can help LLMs recognize hallucinations."
            },
            "score": 4
        },
        {
            "id": "ca261cb681b082e90ca6c7a9d325b4265ed1dc28",
            "paperId": "ca261cb681b082e90ca6c7a9d325b4265ed1dc28",
            "title": "MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models",
            "abstract": "Large language models (LLMs) have achieved remarkable performance in natural language understanding and generation tasks. However, they often suffer from limitations such as difficulty in incorporating new knowledge, generating hallucinations, and explaining their reasoning process. To address these challenges, we propose a novel prompting pipeline, named \\method, that leverages knowledge graphs (KGs) to enhance LLMs' inference and transparency. Our method enables LLMs to comprehend KG inputs and infer with a combination of implicit and external knowledge. Moreover, our method elicits the mind map of LLMs, which reveals their reasoning pathways based on the ontology of knowledge. We evaluate our method on diverse question \\&answering tasks, especially in medical domains, and show significant improvements over baselines. We also introduce a new hallucination evaluation benchmark and analyze the effects of different components of our method. Our results demonstrate the effectiveness and robustness of our method in merging knowledge from LLMs and KGs for combined inference. To reproduce our results and extend the framework further, we make our codebase available at https://github.com/wyl-willing/MindMap.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel prompting pipeline is proposed that leverages knowledge graphs (KGs) to enhance LLMs' inference and transparency and elicits the mind map of LLMs, which reveals their reasoning pathways based on the ontology of knowledge."
            },
            "score": 4
        },
        {
            "id": "3aa200f562346a5e312767e5e9c1333a4f2c951b",
            "paperId": "3aa200f562346a5e312767e5e9c1333a4f2c951b",
            "title": "Large Language Models, scientific knowledge and factuality: A systematic analysis in antibiotic discovery",
            "abstract": "Inferring over and extracting information from Large Language Models (LLMs) trained on a large corpus of scientific literature can potentially drive a new era in biomedical research, reducing the barriers for accessing existing medical evidence. This work examines the potential of LLMs for dialoguing with biomedical background knowledge, using the context of antibiotic discovery. The systematic analysis is applied to ten state-of-the-art models, from models specialised on biomedical scientific corpora to general models such as ChatGPT, GPT-4 and Llama 2 in two prompting-based tasks: chemical compound definition generation and chemical compound-fungus relation determination. The work provides a systematic assessment on the ability of LLMs to encode and express these relations, verifying for fluency, prompt-alignment, semantic coherence, factual knowledge and specificity of generated responses. Results show that while recent models have improved in fluency, factual accuracy is still low and models are biased towards over-represented entities. The ability of LLMs to serve as biomedical knowledge bases is questioned, and the need for additional systematic evaluation frameworks is highlighted. The best performing GPT-4 produced a factual definition for 70% of chemical compounds and 43.6% factual relations to fungi, whereas the best open source model BioGPT-large 30% of the compounds and 30% of the relations for the best-performing prompt. The results show that while LLMs are currently not fit for purpose to be used as biomedical factual knowledge bases, there is a promising emerging property in the direction of factuality as the models become domain specialised, scale-up in size and level of human feedback.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work examines the potential of LLMs for dialoguing with biomedical background knowledge, using the context of antibiotic discovery, and shows a promising emerging property in the direction of factuality as the models become domain specialised, scale-up in size and level of human feedback."
            },
            "score": 4
        },
        {
            "id": "786e935da2bebaf6401a7cab8029b5048bc91521",
            "paperId": "786e935da2bebaf6401a7cab8029b5048bc91521",
            "title": "Magenta: Metrics and Evaluation Framework for Generative Agents based on LLMs",
            "abstract": "Large Language Models (LLMs) have emerged as a driving force in the field of Natural Language Processing (NLP) with applications spanning various domains, including the development of Autonomous Generative Agents. Generative Agents are computational software programs designed to believably simulate human behavior by harnessing the capabilities of large language models. Through repetitive prompts against the large language model, these agents operate based on a system architecture consisting of memory streams, reflection, and planning, allowing them to store experiences, learn from them, and translate insights into high-level action plans to interact with their environment. This paper discusses the current landscape of language models and autonomous agents, their advantages and challenges, and the current state of evaluation, and proposes an innovative evaluation benchmark designed to provide a holistic perspective on their performance. Additionally, we see the impact of fine-tuning such an LLM, evaluate using our benchmark, and then propose a framework for evaluation of both the agents and their underlying LLMs. The existing frameworks for evaluating LLMs and autonomous agents focus on single tasks and are limited in capturing their capabilities. We outline the methodology for evaluating autonomous agents' performance in responding to single and multi-step prompts. The process consists of three key stages: Preparation of the data, Preparation of the Gold Answers, and Evaluations. We use the meticulously crafted 20 unique prompts to challenge the agents, covering simple and complex questions. Using GPT-4, a state-of-the-art model, we generate the initial responses, which undergo rigorous verification to produce gold answers, indicating correctness and revealing the minimum steps required for task completion. Our evaluation framework relies on two critical metrics: the effort metrics, quantifying the steps taken by autonomous agents, and the success rate, measuring their accuracy in achieving task objectives and also keeping track of hallucinations of the model. We conduct experiments with ten different models, representing the current landscape of natural language processing models, presenting each with 20 unique prompts. Their responses are meticulously compared to our gold answers and gold steps (optimal number of steps) to generate the evaluation metrics. Similarly, a fine-tuned model is also evaluated with ten different questions, which test the agent's decision-making process by selecting the correct tool and then the ability of the model to reach the correct conclusion to the question asked by the user in this process.This comprehensive approach ensures a thorough assessment of autonomous agents' capabilities. It demonstrates the utility of these metrics, revealing how they can shed light on the strengths and weaknesses of various autonomous agents. As a step toward standardization, we propose transforming the evaluation process of LLMs into an automated framework that accommodates all types of language models, agents, and LLM-based applications. Such an approach promises to establish a unified and comprehensive evaluation methodology, empowering users to make informed decisions when selecting, fine-tuning, and assessing the accuracy of underlying language models and their applications for different domains.In summary, this paper contributes to the ongoing research on evaluating LLMs and autonomous agents by introducing a novel benchmark and proposing a framework, focusing on evaluating the language models while keeping different knowledge domains in mind. Our framework will enhance our understanding of these technologies and serve as a valuable resource for researchers, engineers, and practitioners working in the ever-evolving landscape of NLP and autonomous systems.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper discusses the current landscape of language models and autonomous agents, their advantages and challenges, and proposes an innovative evaluation benchmark designed to provide a holistic perspective on their performance, and introduces a framework for evaluation of both the agents and their underlying LLMs."
            },
            "score": 4
        },
        {
            "id": "0f5245e3a53f69f66b876173affe9309ee31e7d6",
            "paperId": "0f5245e3a53f69f66b876173affe9309ee31e7d6",
            "title": "OrchestraLLM: Efficient Orchestration of Language Models for Dialogue State Tracking",
            "abstract": "Large language models (LLMs) have revolutionized the landscape of Natural Language Processing systems, but are computationally expensive. To reduce the cost without sacrificing performance, previous studies have explored various approaches to harness the potential of Small Language Models (SLMs) as cost-effective alternatives to their larger counterparts. Driven by findings that SLMs and LLMs exhibit complementary strengths in a structured knowledge extraction task, this work presents a novel SLM/LLM routing framework designed to improve computational efficiency and enhance task performance. First, exemplar pools are created to represent the types of contexts where each LM provides a more reliable answer, leveraging a sentence embedding fine-tuned so that context similarity is close to dialogue state similarity. Then, during inference, the k-nearest exemplars to the testing instance are retrieved, and the instance is routed according to majority vote. In dialogue state tracking tasks, the proposed routing framework enhances performance substantially compared to relying solely on LLMs, while reducing the computational costs by over 50%.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents a novel SLM/LLM routing framework designed to improve computational efficiency and enhance task performance and enhances performance substantially compared to relying solely on LLMs, while reducing the computational costs by over 50%."
            },
            "score": 3
        },
        {
            "id": "9f0f4c95806a6924727ee169eb9cd7da27159441",
            "paperId": "9f0f4c95806a6924727ee169eb9cd7da27159441",
            "title": "Domain-Slot Relationship Modeling Using a Pre-Trained Language Encoder for Multi-Domain Dialogue State Tracking",
            "abstract": "Dialogue state tracking for multi-domain dialogues is challenging because the model should be able to track dialogue states across multiple domains and slots. As using pre-trained language models is the de facto standard for natural language processing tasks, many recent studies use them to encode the dialogue context for predicting the dialogue states. Model architectures that have certain inductive biases for modeling the relationship among different domain-slot pairs are also emerging. Our work is based on these research approaches on multi-domain dialogue state tracking. We propose a model architecture that effectively models the relationship among domain-slot pairs using a pre-trained language encoder. Inspired by the way the special $[CLS]$ token in BERT is used to aggregate the information of the whole sequence, we use multiple special tokens for each domain-slot pair that encodes information corresponding to its domain and slot. The special tokens are run together with the dialogue context through the pre-trained language encoder, which effectively models the relationship among different domain-slot pairs. Our experimental results on the datasets MultiWOZ-2.0 and MultiWOZ-2.1 show that our model outperforms other models with the same setting. Our ablation studies incorporate three main parts. The first component shows the effectiveness of our approach exploiting the relationship modeling. The second component compares the effect of using different pre-trained language encoders. The final component involves comparing different initialization methods that could be used for the special tokens. Qualitative analysis of the attention map of the pre-trained language encoder shows that our special tokens encode relevant information through the encoding process by attending to each other.",
            "year": 2022,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a model architecture that effectively models the relationship among domain-slot pairs using a pre-trained language encoder inspired by the way the special $[CLS]$ token in BERT is used to aggregate the information of the whole sequence, and shows that the model outperforms other models with the same setting."
            },
            "score": 3
        },
        {
            "id": "bc604b501ede16121a87666e05860e49989ef8fc",
            "paperId": "bc604b501ede16121a87666e05860e49989ef8fc",
            "title": "More Robust Schema-Guided Dialogue State Tracking via Tree-Based Paraphrase Ranking",
            "abstract": "The schema-guided paradigm overcomes scalability issues inherent in building task-oriented dialogue (TOD) agents with static ontologies. Rather than operating on dialogue context alone, agents have access to hierarchical schemas containing task-relevant natural language descriptions. Fine-tuned language models excel at schema-guided dialogue state tracking (DST) but are sensitive to the writing style of the schemas. We explore methods for improving the robustness of DST models. We propose a framework for generating synthetic schemas which uses tree-based ranking to jointly optimise lexical diversity and semantic faithfulness. The robust generalisation of strong baselines is improved when augmenting their training data with prompts generated by our framework, as demonstrated by marked improvements in average Joint Goal Accuracy (JGA) and schema sensitivity (SS) on the SGD-X benchmark.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A framework for generating synthetic schemas which uses tree-based ranking to jointly optimise lexical diversity and semantic faithfulness is proposed and demonstrated by marked improvements in average Joint Goal Accuracy (JGA) and schema sensitivity (SS) on the SGD-X benchmark."
            },
            "score": 3
        },
        {
            "id": "d860b056403a4098f31fffbacc35f322b9bcec5b",
            "paperId": "d860b056403a4098f31fffbacc35f322b9bcec5b",
            "title": "Diable: Efficient Dialogue State Tracking as Operations on Tables",
            "abstract": "Sequence-to-sequence state-of-the-art systems for dialogue state tracking (DST) use the full dialogue history as input, represent the current state as a list with all the slots, and generate the entire state from scratch at each dialogue turn. This approach is inefficient, especially when the number of slots is large and the conversation is long. We propose Diable, a new task formalisation that simplifies the design and implementation of efficient DST systems and allows one to easily plug and play large language models. We represent the dialogue state as a table and formalise DST as a table manipulation task. At each turn, the system updates the previous state by generating table operations based on the dialogue context. Extensive experimentation on the MultiWoz datasets demonstrates that Diable (i) outperforms strong efficient DST baselines, (ii) is 2.4x more time efficient than current state-of-the-art methods while retaining competitive Joint Goal Accuracy, and (iii) is robust to noisy data annotations due to the table operations approach.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Diable is proposed, a new task formalisation that simplifies the design and implementation of efficient DST systems and allows one to easily plug and play large language models and robust to noisy data annotations due to the table operations approach."
            },
            "score": 3
        },
        {
            "id": "75300b35eee9e5e013400a163cb91b4950e5dcb9",
            "paperId": "75300b35eee9e5e013400a163cb91b4950e5dcb9",
            "title": "Schema Graph-Guided Prompt for Multi-Domain Dialogue State Tracking",
            "abstract": "Tracking dialogue states is an essential topic in task-oriented dialogue systems, which involve filling in the necessary information in pre-defined slots corresponding to a schema. While general pre-trained language models have been shown effective in slot-filling, their performance is limited when applied to specific domains. We propose a graph-based framework that learns domain-specific prompts by incorporating the dialogue schema. Specifically, we embed domain-specific schema encoded by a graph neural network into the pre-trained language model, which allows for relations in the schema to guide the model for better adaptation to the specific domain. Our experiments demonstrate that the proposed graph-based method outperforms other multi-domain DST approaches while using similar or fewer trainable parameters. We also conduct a comprehensive study of schema graph architectures, parameter usage, and module ablation that demonstrate the effectiveness of our model on multi-domain dialogue state tracking.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A graph-based framework that learns domain-specific prompts by incorporating the dialogue schema into the pre-trained language model, which allows for relations in the schema to guide the model for better adaptation to the specific domain."
            },
            "score": 3
        },
        {
            "id": "e7c97e953849f1a8e5d85ceb4cfcc0a5d54d2365",
            "paperId": "e7c97e953849f1a8e5d85ceb4cfcc0a5d54d2365",
            "title": "Enabling Large Language Models to Generate Text with Citations",
            "abstract": "Large language models (LLMs) have emerged as a widely-used tool for information seeking, but their generated outputs are prone to hallucination. In this work, our aim is to allow LLMs to generate text with citations, improving their factual correctness and verifiability. Existing work mainly relies on commercial search engines and human evaluation, making it challenging to reproduce and compare different modeling approaches. We propose ALCE, the first benchmark for Automatic LLMs' Citation Evaluation. ALCE collects a diverse set of questions and retrieval corpora and requires building end-to-end systems to retrieve supporting evidence and generate answers with citations. We develop automatic metrics along three dimensions -- fluency, correctness, and citation quality -- and demonstrate their strong correlation with human judgements. Our experiments with state-of-the-art LLMs and novel prompting strategies show that current systems have considerable room for improvement -- For example, on the ELI5 dataset, even the best models lack complete citation support 50% of the time. Our analyses further highlight promising future directions, including developing better retrievers, advancing long-context LLMs, and improving the ability to synthesize information from multiple sources.",
            "year": 2023,
            "citationCount": 102,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes ALCE, the first benchmark for Automatic LLMs' Citation Evaluation, and develops automatic metrics along three dimensions -- fluency, correctness, and citation quality -- and demonstrates their strong correlation with human judgements."
            },
            "score": 3
        },
        {
            "id": "6052486bc9144dc1730c12bf35323af3792a1fd0",
            "paperId": "6052486bc9144dc1730c12bf35323af3792a1fd0",
            "title": "Large language models encode clinical knowledge",
            "abstract": null,
            "year": 2022,
            "citationCount": 825,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "MultiMedQA, a benchmark combining six existing medical question answering datasets spanning professional medicine, research and consumer queries and\u00a0a new dataset of medical questions searched online, is presented and a human evaluation framework for model answers is proposed, suggesting the potential utility of LLMs in medicine."
            },
            "score": 3
        },
        {
            "id": "3e14227862ae21ec691794b8586d6c695c5feb75",
            "paperId": "3e14227862ae21ec691794b8586d6c695c5feb75",
            "title": "RE-MOVE: An Adaptive Policy Design for Robotic Navigation Tasks in Dynamic Environments via Language-Based Feedback",
            "abstract": "Reinforcement learning-based policies for continuous control robotic navigation tasks often fail to adapt to changes in the environment during real-time deployment, which may result in catastrophic failures. To address this limitation, we propose a novel approach called RE-MOVE (REquest help and MOVE on) to adapt already trained policy to real-time changes in the environment without re-training via utilizing a language-based feedback. The proposed approach essentially boils down to addressing two main challenges of (1) when to ask for feedback and, if received, (2) how to incorporate feedback into trained policies. RE-MOVE incorporates an epistemic uncertainty-based framework to determine the optimal time to request instructions-based feedback. For the second challenge, we employ a zero-shot learning natural language processing (NLP) paradigm with efficient, prompt design and leverage state-of-the-art GPT-3.5, Llama-2 language models. To show the efficacy of the proposed approach, we performed extensive synthetic and real-world evaluations in several test-time dynamic navigation scenarios. Utilizing RE-MOVE result in up to 80% enhancement in the attainment of successful goals, coupled with a reduction of 13.50% in the normalized trajectory length, as compared to alternative approaches, particularly in demanding real-world environments with perceptual challenges.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 3
        },
        {
            "id": "00d9655ac219f71712a57ba262249f0cf83e71b8",
            "paperId": "00d9655ac219f71712a57ba262249f0cf83e71b8",
            "title": "Voluminous yet Vacuous? Semantic Capital in an Age of Large Language Models",
            "abstract": "Large Language Models (LLMs) have emerged as transformative forces in the realm of natural language processing, wielding the power to generate human-like text. However, despite their potential for content creation, they carry the risk of eroding our Semantic Capital (SC) - the collective knowledge within our digital ecosystem - thereby posing diverse social epistemic challenges. This paper explores the evolution, capabilities, and limitations of these models, while highlighting ethical concerns they raise. The study contribution is two-fold: first, it is acknowledged that, withstanding the challenges of tracking and controlling LLM impacts, it is necessary to reconsider our interaction with these AI technologies and the narratives that form public perception of them. It is argued that before achieving this goal, it is essential to confront a potential deontological tipping point in an increasing AI-driven infosphere. This goes beyond just adhering to AI ethical norms or regulations and requires understanding the spectrum of social epistemic risks LLMs might bring to our collective SC. Secondly, building on Luciano Floridi's taxonomy for SC risks, those are mapped within the functionality and constraints of LLMs. By this outlook, we aim to protect and enrich our SC while fostering a collaborative environment between humans and AI that augments human intelligence rather than replacing it.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is argued that before achieving this goal, it is essential to confront a potential deontological tipping point in an increasing AI-driven infosphere and protect and enrich the authors' SC while fostering a collaborative environment between humans and AI that augments human intelligence rather than replacing it."
            },
            "score": 2
        },
        {
            "id": "0b395ed1c8b284e551172b728e83cf257e33729a",
            "paperId": "0b395ed1c8b284e551172b728e83cf257e33729a",
            "title": "HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models",
            "abstract": "We introduce HallusionBench, a comprehensive benchmark designed for the evaluation of image-context reasoning. This benchmark presents significant challenges to advanced large visual-language models (LVLMs), such as GPT-4V(Vision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, by emphasizing nuanced understanding and interpretation of visual data. The benchmark comprises 346 images paired with 1129 questions, all meticulously crafted by human experts. We introduce a novel structure for these visual questions designed to establish control groups. This structure enables us to conduct a quantitative analysis of the models' response tendencies, logical consistency, and various failure modes. In our evaluation on HallusionBench, we benchmarked 15 different models, highlighting a 31.42% question-pair accuracy achieved by the state-of-the-art GPT-4V. Notably, all other evaluated models achieve accuracy below 16%. Moreover, our analysis not only highlights the observed failure modes, including language hallucination and visual illusion, but also deepens an understanding of these pitfalls. Our comprehensive case studies within HallusionBench shed light on the challenges of hallucination and illusion in LVLMs. Based on these insights, we suggest potential pathways for their future improvement. The benchmark and codebase can be accessed at https://github.com/tianyi-lab/HallusionBench.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This benchmark presents significant challenges to advanced large visual-language models (LVLMs), such as GPT-4V(Vision), Gemini Pro Vision, Claude 3, and LLaVA-1.5, by emphasizing nuanced understanding and interpretation of visual data."
            },
            "score": 2
        },
        {
            "id": "7c9f69848d28e0a7cbb00942ee83dab9773c23e4",
            "paperId": "7c9f69848d28e0a7cbb00942ee83dab9773c23e4",
            "title": "GPT-NER: Named Entity Recognition via Large Language Models",
            "abstract": "Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model. In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text\"Columbus is a city\"is transformed to generate the text sequence\"@@Columbus## is a city\", where special tokens @@## marks the entity to extract. To efficiently address the\"hallucination\"issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a self-verification strategy by prompting LLMs to ask itself whether the extracted entities belong to a labeled entity tag. We conduct experiments on five widely adopted NER datasets, and GPT-NER achieves comparable performances to fully supervised baselines, which is the first time as far as we are concerned. More importantly, we find that GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, GPT-NER performs significantly better than supervised models. This demonstrates the capabilities of GPT-NER in real-world NER applications where the number of labeled examples is limited.",
            "year": 2023,
            "citationCount": 58,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, and performs significantly better than supervised models, which demonstrates the capabilities of GPT-ner in real-world NER applications where the number of labeled examples is limited."
            },
            "score": 2
        },
        {
            "id": "b9df0d4631f9fab1432c152765e243ae4cd667f4",
            "paperId": "b9df0d4631f9fab1432c152765e243ae4cd667f4",
            "title": "Effective Prompt Extraction from Language Models",
            "abstract": "The text generated by large language models is commonly controlled by prompting, where a prompt prepended to a user's query guides the model's output. The prompts used by companies to guide their models are often treated as secrets, to be hidden from the user making the query. They have even been treated as commodities to be bought and sold. However, anecdotal reports have shown adversarial users employing prompt extraction attacks to recover these prompts. In this paper, we present a framework for systematically measuring the effectiveness of these attacks. In experiments with 3 different sources of prompts and 11 underlying large language models, we find that simple text-based attacks can in fact reveal prompts with high probability. Our framework determines with high precision whether an extracted prompt is the actual secret prompt, rather than a model hallucination. Prompt extraction experiments on real systems such as Bing Chat and ChatGPT suggest that system prompts can be revealed by an adversary despite existing defenses in place.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents a framework for systematically measuring the effectiveness of prompt extraction attacks and determines with high precision whether an extracted prompt is the actual secret prompt, rather than a model hallucination."
            },
            "score": 2
        },
        {
            "id": "f9dd3efb45b4a3aaa72ac0719ea56344cd7d25eb",
            "paperId": "f9dd3efb45b4a3aaa72ac0719ea56344cd7d25eb",
            "title": "Encode Once and Decode in Parallel: Efficient Transformer Decoding",
            "abstract": "Transformer-based NLP models are powerful but have high computational costs that limit deployment scenarios. Finetuned encoder-decoder models are popular in specialized domains and can outperform larger more generalized decoder-only models, such as GPT-4. We introduce a new configuration for encoder-decoder models that improves efficiency on structured output and question-answering tasks where multiple outputs are required of a single input. Our method, prompt-in-decoder (PiD), encodes the input once and decodes output in parallel, boosting both training and inference efficiency by avoiding duplicate input encoding, thereby reducing the decoder's memory footprint. We achieve computation reduction that roughly scales with the number of subtasks, gaining up to 4.6x speed-up over state-of-the-art models for dialogue state tracking, summarization, and question-answering tasks with comparable or better performance. We release our training/inference code and checkpoints.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a new configuration for encoder-decoder models that improves efficiency on structured output and question-answering tasks where multiple outputs are required of a single input, and achieves computation reduction that roughly scales with the number of subtasks."
            },
            "score": 2
        },
        {
            "id": "36b7dbfb2abe2fb5e9f0d0f35fdf3d0f38b1cede",
            "paperId": "36b7dbfb2abe2fb5e9f0d0f35fdf3d0f38b1cede",
            "title": "GIS Investigation of Crime Prediction with an Operationalized Tweet Corpus",
            "abstract": "Social media as the de facto communication channel is being used to disseminate one\u2019s diurnal self-revelations. This profound discovery often contains double-talk, peculiar insights, or contextual information about real-world events. Natural language processing is regularly used to uncover both obvious and latent knowledge claims within disclosures published amid the complex environment. For example, a perpetrator with first-hand knowledge of their criminal incident uses social media to post critical information about it. A geographic information system (GIS) is capable of large-scale point data analysis and possesses methods that enable dataset processing, evaluation, and automatic spatial visualization. Such an artifactfused with traditional environmental criminology theory and social mediaerects guidelines, tools, and models for substantive construction and evaluation of GIS crime analysis solutions. Provided the social media stream is timely and correctly processed, corrective action can be taken. The construction of a natural language processing social media annotation pipe identifies latent indicators extracted from a social media corpus and is an integral part of societal mishap prediction. Spatial visualizations and regression analyses were used to describe and evaluate project artifacts. As a result, a social media corpus was operationalized, and subsequently used as a proxy for a traditional environmental criminology risk layer in construction of a social media GIS crime analysis artifact. Using such multi-domain collaboration, the artifact was able to increase the predictive crime incident outcome with an overall R-squared increase of 21.94%. This result is the state-of-the-art; there are no other results to compare it to. Poster Download: http://scholarworks.umass.edu/foss4g/vol17/iss1/21 \u2217Corresponding author Email address: acorso@calbaptist.edu (Anthony J. Corso) Submitted to FOSS4G 2017 Conference Proceedings, Boston, USA. September 20, 2017 Social media (e.g., tweets) are the de facto communication channel to disseminate one\u2019s diurnal self-revelations. This profound phenomenon contains doubletalk, peculiar insight, and contextual data or information about real-world events. Amid such complex and personal expose, natural language processing (NLP) techniques uncover both obvious and latent knowledge claims published within. A geographical information system is capable of large-scale data analysis and possesses methods that enable dataset processing, evaluation, and spatial visualization. When fused with traditional research theory\u2014such an artifact defines guidelines, algorithms, and models for substantive and predictive investigation. Introduction With a novel NLP pipeline tweets were processed and used to measure the change in performance of an ArcGIS 10.4.1 artifact. A 1,000 tweet sample was hand tagged and compared to a baseline model, and to an innovative social media grammar applied by a rule-based social media NLP pipeline. GIS evaluation tools answer the question, prior to content analysis of a tweet, does a method exist to support identifying a tweet as \u201cuseful\u201d for subsequent GIS processing? Indeed, \u201cuseful\u201d tweet identification via NLP returned precision of 0.9256, recall of 0.6590, and F-measure of 0.7699; consequently, exploratory GIS processing of a social media variable increased 0.2194 over baseline. Predictive capability potential of a GIS artifact implementing social media\u2019s latent behavior attributes is vast. Yes, preliminary results are encouraging but future research is important and needs to identify its value. References Alonso, O., Marshall, C. C., & Najork, M. (2013). Are some tweets more interesting than others?# hardquestion. Paper presented at the Proceedings of the Symposium on Human-Computer Interaction and Information Retrieval. Andr\u00e9, P., Bernstein, M., & Luther, K. (2012). Who gives a tweet?: evaluating microblog content value. Paper presented at the Proceedings of the ACM 2012 conference on Computer Supported Cooperative Work. Bendler, J., Ratku, A., & Neumann, D. (2014). Crime Mapping through Geo-Spatial Social Media Activity. [1] Bontcheva, K., Derczynski, L., Funk, A., Greenwood, M. A., Maynard, D., & Aswani, N. (2013). TwitIE: An Open-Source Information Extraction Pipeline for Microblog Text. Paper presented at the RANLP. Bramsen, P., Escobar-Molano, M., Patel, A., & Alonso, R. (2011). Extracting social power relationships from natural language. Paper presented at the Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Caplan, J. M., Kennedy, L. W., & Miller, J. (2011). Risk Terrain Modeling: Brokering Criminological Theory and GIS Methods for Crime Forecasting. Justice Quarterly, 28(2), 360-381. doi: 10.1080/07418825.2010.486037 [3] City of Chicago. (2014) Retrieved December 31, 2014, from https://data.cityofchicago.org/ Corso, A., Alsudais, K., & Hilton, B. (2016). Big Social Data and GIS: Visualize Predictive Crime. AMCIS Conference 2016 Proceedings. Corso, A. J., & Alsudais, A. (2015). GIS, Big Data, and a Tweet Corpus Operationalized via Natural Language Processing. AMCIS Conference 2015 Proceedings. Drawve, G. (2014). A Metric Comparison of Predictive Hot Spot Techniques and RTM. Justice Quarterly, 1-29. doi: 10.1080/07418825.2014.904393 [5] ESRI 2011. ArcGIS Desktop: Release 10. Redlands, CA: Environmental Systems Research Institute. Frantzi, K., Ananiadou, S., & Mima, H. (2000). Automatic recognition of multi-word terms: the C-value/NC-value method. International Journal on Digital Libraries, 3(2), 115-130. Gerber, M. S. (2014). Predicting crime using Twitter and kernel density estimation. Decision Support Systems, 61, 115-125. Hirst, G., & Feiguina, O. g. (2007). Bigrams of syntactic labels for authorship discrimination of short texts. Literary and Linguistic Computing, 22(4), 405-417. Hiruta, S., Yonezawa, T., Jurmu, M., & Tokuda, H. (2012). Detection, Classification and Visualization of Place-triggerd Geotagged Tweets. Hurlock, J., & Wilson, M. L. (2011). Searching Twitter: Separating the Tweet from the Chaff. Paper presented at the ICWSM. Jurafsky, D., & Martin, J. H. (2009). Speech and Language Processing. Kennedy, L. W., Caplan, J. M., & Piza, E. (2011). Risk Clusters, Hotspots, and Spatial Intelligence: Risk Terrain Modeling as an Algorithm for Police Resource Allocation Strategies. Journal of Quantitative Criminology, 27(3), 339-362. doi: 10.1007/s10940-010-9126-2 Leroy, G. (2011). Designing User Studies in Informatics: Springer Science & Business Media. Phan, X.-H., Nguyen, L.-M., & Horiguchi, S. (2008). Learning to classify short and sparse text & web with hidden topics from large-scale data collections. Paper presented at the Proceedings of the 17th international conference on World Wide Web. Piao, S., & Whittle, J. (2011). A Feasibility Study on Extracting Twitter Users' Interests Using NLP Tools for Serendipitous Connections. Paper presented at the Privacy, security, risk and trust (passat), 2011 ieee third international conference on and 2011 ieee third international conference on social computing (socialcom). Quattrone, G., Proserpio, D., Quercia, D., Capra, L., & Musolesi, M. (2016). Who Benefits from the \"Sharing\" Economy of Airbnb? Paper presented at the Proceedings of the 25th International Conference on World Wide Web, Montr&#233;al, Qu&#233;bec, Canada. Sriram, B., Fuhry, D., Demir, E., Ferhatosmanoglu, H., & Demirbas, M. (2010). Short text classification in twitter to improve information filtering. Paper presented at the Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval, Geneva, Switzerland. Stefanidis, A., Crooks, A., & Radzikowski, J. (2013). Harvesting ambient geospatial information from social media feeds. GeoJournal, 78(2), 319-338. doi: 10.1007/s10708-011-9438-2 Torres-Moreno, J.-M. (2014). Three Statistical Summarizers at CLEF-INEX 2013 Tweet Contextualization Track. Paper presented at the CLEF (Working Notes). [2] Twitter. (2014) Retrieved December 31, 2014, from http://www.twitter.com [4] United States Department of Agriculture. (2014) Retrieved December 31, 2014, from http://www.fns.usda.gov/snap/retailerlocator Zingla, M. A., Chiraz, L., Slimani, Y., & Berrut, C. (2015). Statistical and Semantic Approaches for Tweet Contextualization. Procedia Computer Science, 60, 498-507. Zubiaga, A., & Ji, H. (2014). Tweet, but verify: epistemic study of information verification on Twitter. [journal article]. Social Network Analysis and Mining, 4(1), 1-12. Contact Dr. Corso holds Ph.D. in Information Systems and Technology from Claremont Graduate University. Since 2007, he is an Associate Professor in the Gordon and Jill Bourns College of Engineering at California Baptist University. -----------E-mail: acorso@calbaptist.edu Despite a tweet\u2019s sparse content, NLP makes their use in a predictive GIS artifact feasible. For example, subsequent to processing, useful tweets are able to: \u2022 Predict the validity of a real-world event only recorded by observation of social media eyewitness; or \u2022 Predict real-time trends by amalgamating social media with traditional social behavior variables. California Baptist University Anthony J. Corso, Ph.D. GIS Investigation of Crime Prediction with an Operationalized Tweet Corpus",
            "year": 2017,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "GIS evaluation tools answer the question, prior to content analysis of a tweet, does a method exist to support identifying a tweet as \u201cuseful\u201d for subsequent GIS processing?"
            },
            "score": 2
        },
        {
            "id": "8c3a8967920cae323c1bac9c20c75e7007e4262a",
            "paperId": "8c3a8967920cae323c1bac9c20c75e7007e4262a",
            "title": "What \u2019 s in a planet ? \u21e4",
            "abstract": "This Festschrift piece is the occasion to put forth some reflections on lexical meaning and on the internalism-externalism debate. I discuss, based on some of Frank Veltman\u2019s considerations about the pragmatic meaning of vague terms, the dynamic view whereby speaker and hearer jointly elaborate the meaning of general terms in the course of their transactions, and the question of whether general terms have a core meaning. 1 What\u2019s in three names? I cannot think of celebrating Jeroen Groenendijk, Martin Stokhof and Frank Veltman without thereby celebrating the community to which their names are attached. I am sure all three of them will understand what I mean, but let me explain. Back in 2001, during the first year of my PhD in Paris, I went to my first ESSLLI summer school in Helsinki, encouraged by Philippe Schlenker whom I had met just a year before. In Helsinki Philippe introduced me to Paul Dekker, who asked me about my interests, gave me invaluable encouragements, and sent me by surface mail, just a few weeks later, two hard copies of two dissertations, one by Maria Aloni on the semantics of belief reports, the other by Jelle Gerbrandy on epistemic logic (the mysterious title of which: \u201cBisimulations on Planet Kripke\u201d, bears a rather serendipitous connection to the question I am asking in this paper). For me this was the beginning of a wonderfully fruitful intellectual and human relation with the Amsterdam community and the ILLC people. I gave one of my first PhD presentations at the ILLC in 2002, a few months later, at one of the Logic Teas, invited by Boudewijn de Bruijn and Balter ten Cate, where I also met Robert van Rooij for the first time, and where Dick de Jongh kindly gave me an o\u21b5print of one of his papers on provability logic after my talk. The following summer I went to the first NASSLLI summer school at Stanford, encouraged by Paul Dekker and Patrick Blackburn. This is where I met Frank Veltman, who gave a memorable course on conditionals, and Johan van Benthem, who was hosting the event. (Although this Festschift is not the place to celebrate Johan, Johan was later to play an equally important part in the development of one direction of my PhD). Only a few years later did I first shake hands with Jeroen Groenendijk and Martin Stokhof: if I remember correctly, this was back in 2005, when I also met Maria Aloni, and was kindly hosted by Henk Zeevat for the first Paris-Amsterdam Logic Meeting of Young Researchers. But their work on questions had already guided me during all of my PhD years (along with Maria\u2019s thesis). Back in 2002, Balder ten Cate and I were sharing the same interest for their work on questions, although prompted by di\u21b5erent problems. My own interest was in their work on embedded \u21e4This paper is a Festschrit piece written in honor of Jeroen Groenendijk, Martin Stokhof and Frank Veltman on the occasion of their upcoming retirement. On the topic of planets, I am indebted to the organizers of the Elbereth 2013 conference held at the Institut d\u2019Astrophysique de Paris for the invitation made to a philosopher to speak about vagueness to an audience of astrophysicists. Thanks in particular to Mathieu Languer, Agn\u00e8s Fert\u00e9 and to the audience for prompting me to think about the scientific and unscientific uses of the word \u201cplanet\u201d in relation to vagueness. questions, and on the features that could explain why some verbs are question-selecting and others not. Twelve years later, I marvel at how much my work owes to these many figures. I can\u2019t think of any of the research projects that have mattered to me the most, not to mention the exciting collaborations I have had with Maria Aloni and Robert van Rooij, without thinking of the extraordinary inspiration received from the ILLC as a community. Like several Parisian friends of my generation (including Isidora Stojanovic, Benjamin Spector, Denis Bonnay, Mikael Cozic, Emmanuel Chemla), all with interests at the intersection of logic, language and philosophy, I think with gratefulness that we were a lucky group of French students, at the beginning of the 21st century, to be associated to the vibrant environment of the ILLC, and to be able to walk in the steps of some of its leading figures, like Frank, Jeroen or Martin. But let me save more souvenirs for another occasion, and turn to my main tribute. 2 What\u2019s in a general term? This piece is about the vexed question of what determines the meaning of a general term in language, and about the dynamics of lexical meaning. By a general term, I mean a nonlogical expression whose extension is not reduced to a singleton (it includes adjectives like \u201cblue\u201d or \u201cprime\u201d, nouns like \u201cplanet\u201d or \u201cnumber\u201d, verbs like \u201cboil\u201d or \u201cdivide\u201d). I was prompted to write this paper after reading (an approximation to) Frank Veltman\u2019s address on the di\u21b5erence between vagueness and imprecision (\u201cHet verschil tussen vaag en niet precies\u201d),1 and based on an interest for the relation between the scientific and the unscientific use of general terms. An overarching theme of the work of Groenendijk, Stokhof and Veltman is the idea, shared with other theorists of language such as Stalnaker, Lewis, Heim and Kamp, that meaning is not statically encoded in the truth-conditions of an expression, but lies in the di\u21b5erence the use of that expression makes to the context. In his address, Veltman summarizes the idea as follows: De betekenis van een zin wordt in zo\u2019n opzet niet geidentificeerd met zijn waarheidscondities, maar met het e\u21b5ect dat die zin heeft op de cognitieve toestand van degenen die de informatie die in die zin vervat is moeten verwerken. \u201cIn this approach, the meaning of a sentence is not identified with its truthconditions, but rather with the e\u21b5ect that the sentence has on the cognitive state of those who have to process the information that is embodied by the sentence.\u201d On the picture Veltman proposes, this view of meaning is meant to apply not only to full sentences, but to smaller units, namely words such as common nouns, adjectives and verbs. Their meaning is not rigidly fixed ahead of discourse, but it is negotiated between speakers of a community during their verbal transactions. Prima facie, this may seem to convey a rather internalist picture of lexical meaning, because the primary meaning attached to a word might then appear to be either what the hearer mentally represents, or more likely, what the speaker intends the hearer to represent. But of course, the lesson of this pragmatic approach can\u2019t be so simple, since on the one hand, speaker and hearer have to coordinate their internal meanings with each other for communication to be successful, and on the other hand, because the use of a word by a speaker is also based on properties of his or her external environment that the speaker is intending to refer to. 1Not being competent to read Dutch, I had access to that paper first by testimonies, and then with the help of Google Translate, which gave me a valuable but still rough approximation to a proper translation. I thank Floris Roelofsen for sending me the proper English translation of the citation which follows. A particularly acute form of this tension between internalism and externalism about lexical meaning is indeed revealed when considering the meaning of vague general terms (like \u201cblue\u201d, \u201ctiger\u201d, \u201cchair\u201d or \u201cplanet\u201d), which are the focus of Veltman\u2019s paper. On one end of the spectrum, we find externalist pictures of lexical meaning, including for vague expressions, according to which general terms, whether for natural kinds of artifacts, essentially track properties in the world (See Putnam 1975 for the general view, and Williamson 1992 on vagueness specifically).2 On Williamson\u2019s version of the view, vague expressions, like precise ones, must have sharp underlying lines, because they purport to designate such external properties, which themselves must have precise contours. At another end of the spectrum, we find internalist pictures of vague meanings, on which speakers need only have \u201cclose enough\u201d lexical representations in their head in order to successfully communicate (see Parikh 1994). Veltman\u2019s view of lexical meaning is obviously more in sympathy with the second picture, and this is also where my sympathy lies. I would like to question that second view, however, to see what amount of externalist residue it is likely to leave behind. One of the examples Parikh discusses is that of color words, for which speakers of a community may have very di\u21b5erent inner representations. What Parikh concedes, however, is the requirement of overlap. If my use of \u201cblue\u201d and your use of \u201cblue\u201d really have no extensional overlap, then we are likely to end in miscommunication. So let us consider two speakers who use \u201cblue\u201d with some overlap. Should we say that the overlap between your representation and my representation is where the objective meaning of the expression lies? It is tempting to think so, if we think of the overlap as consisting of a common set of referential values (prototypical values such as good or true blues). But that may still remain an idealized view of meaning, on which speakers all share something like the \u201csemantic core\u201d of an expression. What if you and I have learnt the meaning of \u201cblue\u201d by association to distinct exemplars (you saw only dark blues, and I only light blues)? What if my perception of colors is severely limited and yours isn\u2019t? Maybe, to borrow a structuralist view of meaning, what really matters then is that your inner representation of \u201cblue\u201d be related to your inner representation of other color words in a way that is itself close enough to the relation between my representation of \u201cblue\u201d and my representation of other color words. On that view, your use of \u201cblue\u201d may have no extensional overlap with mine, but it might still be functionally congruent with my use of \u201cblue\u201d. Maybe, but even so the structuralist view would fail to explain something. When we communicate, w",
            "year": 2013,
            "citationCount": 1,
            "tldr": null,
            "score": 1
        }
    ],
    "novelty": "yes"
}