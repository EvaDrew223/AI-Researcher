{
    "topic_description": "novel prompting methods that can improve factuality and reduce hallucination of large language models",
    "idea_name": "Fact-Aware Prompting",
    "raw_idea": {
        "Problem": "Large language models tend to generate factually incorrect information when answering questions or generating text. Existing methods often rely on fact-checking against knowledge bases or human feedback, which can be expensive and time-consuming.",
        "Existing Methods": "Current methods for reducing hallucination include using retrieved evidence to guide generation, training models to be more calibrated in their outputs, or using reinforcement learning with factual consistency rewards.",
        "Motivation": "Instead of relying on post-hoc fact-checking or additional training, we propose to make the model more aware of factual information during the generation process itself. By conditioning the model on relevant facts and training it to attend to factual evidence, we can guide it to generate more factually consistent outputs.",
        "Proposed Method": "We propose Fact-Aware Prompting (FAP), a method that incorporates factual information into the prompting process to guide the model's generation. Given an input query, FAP first retrieves relevant facts from a trusted knowledge source (e.g., Wikipedia, Wikidata). It then constructs a prompt that includes both the original query and the retrieved facts, using special tokens to distinguish the facts from the query. During generation, the model is trained to attend to the factual evidence provided in the prompt, using techniques such as attention masking or fact-aware decoding. The model is also trained to generate outputs that are consistent with the provided facts, using techniques such as factual consistency loss or fact-aware beam search.",
        "Experiment Plan": "We will evaluate FAP on a range of factual generation tasks, such as open-domain question answering, fact-grounded dialog, and fact-guided summarization. We will compare FAP to baseline methods such as direct prompting and retrieved-augmented generation, as well as state-of-the-art methods that use post-hoc fact-checking or reinforcement learning. We will measure factuality using both automatic metrics (e.g., factual F1, consistency score) and human evaluation of the generated outputs' correctness and consistency with the provided facts."
    },
    "full_experiment_plan": {
        "Title": "Fact-Aware Prompting: Incorporating Factual Evidence to Guide Language Model Generation",
        "Problem Statement": "Large language models tend to generate factually incorrect information when answering questions or generating text. Existing methods often rely on fact-checking against knowledge bases or human feedback, which can be expensive and time-consuming.",
        "Motivation": "Instead of relying on post-hoc fact-checking or additional training, we propose to make the model more aware of factual information during the generation process itself. By conditioning the model on relevant facts and training it to attend to factual evidence, we can guide it to generate more factually consistent outputs. This approach is inspired by two key ideas: (1) retrieving relevant facts can provide useful context for the model to generate more accurate outputs, and (2) explicitly training the model to attend to and incorporate the retrieved facts can help mitigate the issue of hallucination.",
        "Proposed Method": "We propose Fact-Aware Prompting (FAP), a method that incorporates factual information into the prompting process to guide the model's generation. Given an input query, FAP first retrieves relevant facts from a trusted knowledge source (e.g., Wikipedia, Wikidata). It then constructs a prompt that includes both the original query and the retrieved facts, using special tokens to distinguish the facts from the query. During generation, the model is trained to attend to the factual evidence provided in the prompt, using techniques such as attention masking or fact-aware decoding. The model is also trained to generate outputs that are consistent with the provided facts, using techniques such as factual consistency loss or fact-aware beam search.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Data Preparation": "We will use a diverse set of factual generation tasks, such as open-domain question answering (e.g., Natural Questions, TriviaQA), fact-grounded dialog (e.g., Wizard of Wikipedia), and fact-guided summarization (e.g., XSum). For each task, we will use the standard train/dev/test splits. We will also use a trusted knowledge source (e.g., Wikipedia) to retrieve relevant facts for each input query.",
            "Step 2: Fact Retrieval": "For each input query, we will retrieve the top-k most relevant facts from the knowledge source using a retrieval model (e.g., BM25, DPR). We will experiment with different values of k (e.g., 1, 3, 5) to find the optimal number of facts to retrieve. We will also experiment with different retrieval models to find the most effective one for each task.",
            "Step 3: Prompt Construction": "We will construct prompts that include both the original query and the retrieved facts. We will use special tokens (e.g., <fact>) to distinguish the facts from the query. For example, a prompt for a question answering task may look like: 'Question: What is the capital of France? <fact> Paris is the capital and most populous city of France. </fact> Answer:'. We will experiment with different prompt formats to find the most effective one for each task.",
            "Step 4: Model Training": "We will train the language model to generate outputs that are consistent with the provided facts. We will use techniques such as attention masking (i.e., forcing the model to attend only to the retrieved facts) and fact-aware decoding (i.e., penalizing the model for generating tokens that are inconsistent with the facts). We will also experiment with different loss functions, such as factual consistency loss (i.e., penalizing the model for generating outputs that are inconsistent with the facts) and fact-aware beam search (i.e., favoring hypotheses that are consistent with the facts during beam search). We will use pre-trained language models (e.g., T5, BART) as the base models and fine-tune them on each task using the constructed prompts.",
            "Step 5: Evaluation": "We will evaluate the trained models on the test set of each task using both automatic metrics (e.g., BLEU, ROUGE, F1) and human evaluation. For human evaluation, we will ask annotators to rate the generated outputs in terms of factual consistency (i.e., whether the output is consistent with the provided facts), relevance (i.e., whether the output answers the input query), and fluency (i.e., whether the output is grammatically correct and easy to understand). We will compare FAP with baseline methods such as direct prompting (i.e., prompting the model with the input query only) and retrieval-augmented generation (i.e., concatenating the retrieved facts with the input query). We will also conduct ablation studies to understand the contribution of each component of FAP (e.g., fact retrieval, prompt construction, model training)."
        },
        "Test Case Examples": {
            "Test Case 1": {
                "Input Query": "What is the capital of France?",
                "Retrieved Facts": "Paris is the capital and most populous city of France.",
                "Baseline Prompt": "What is the capital of France?",
                "Baseline Output": "The capital of France is Paris, but some people may mistakenly think it is Lyon or Marseille.",
                "FAP Prompt": "Question: What is the capital of France? <fact> Paris is the capital and most populous city of France. </fact> Answer:",
                "FAP Output": "Paris is the capital of France.",
                "Explanation": "The baseline model generates a factually incorrect output by mentioning Lyon and Marseille as potential capitals of France. In contrast, FAP retrieves the relevant fact that Paris is the capital of France and incorporates it into the prompt. This guides the model to generate a factually consistent output that correctly answers the question."
            },
            "Test Case 2": {
                "Input Query": "What is the largest planet in our solar system?",
                "Retrieved Facts": "Jupiter is the largest planet in the solar system. It has a mass of 1.898 \u00d7 10^27 kg and a radius of 69,911 km.",
                "Baseline Prompt": "What is the largest planet in our solar system?",
                "Baseline Output": "The largest planet in our solar system is Saturn, with a radius of about 58,232 km.",
                "FAP Prompt": "Question: What is the largest planet in our solar system? <fact> Jupiter is the largest planet in the solar system. It has a mass of 1.898 \u00d7 10^27 kg and a radius of 69,911 km. </fact> Answer:",
                "FAP Output": "Jupiter is the largest planet in our solar system, with a radius of 69,911 km.",
                "Explanation": "The baseline model generates a factually incorrect output by stating that Saturn is the largest planet in our solar system. In contrast, FAP retrieves the relevant fact that Jupiter is the largest planet and provides its mass and radius. This guides the model to generate a factually consistent output that correctly answers the question and provides additional relevant information."
            }
        },
        "Fallback Plan": "If the proposed FAP method does not outperform the baseline methods, we can conduct additional analysis to understand why. Some potential directions include: (1) Analyzing the retrieved facts to see if they are relevant and informative for answering the input queries. If the retrieved facts are not relevant or informative, we can experiment with different retrieval models or knowledge sources. (2) Analyzing the constructed prompts to see if they effectively incorporate the retrieved facts. If the prompts are not effective, we can experiment with different prompt formats or ways of distinguishing the facts from the queries. (3) Analyzing the generated outputs to see if they are consistent with the retrieved facts. If the outputs are not consistent, we can experiment with different model training techniques or loss functions to better enforce factual consistency. If FAP still does not outperform the baselines after these analyses and modifications, we can turn the project into an analysis paper that provides insights into the challenges and limitations of incorporating factual evidence into language model prompting."
    },
    "novelty_queries": [
        "KeywordQuery(\"fact-aware prompting language models\")",
        "KeywordQuery(\"factual consistency language models\")",
        "KeywordQuery(\"retrieval augmented generation factual consistency\")",
        "KeywordQuery(\"incorporating knowledge language model generation\")",
        "KeywordQuery(\"Fact-Aware Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "56373d3fd0f1354a61f9e577db039cdb187d8d43",
            "paperId": "56373d3fd0f1354a61f9e577db039cdb187d8d43",
            "title": "Evaluating the Factual Consistency of Large Language Models Through News Summarization",
            "abstract": "While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information. To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB(Factual Inconsistency Benchmark) that focuses on the task of summarization. Specifically, our benchmark involves comparing the scores an LLM assigns to a factually consistent versus a factually inconsistent summary for an input news article. For factually consistent summaries, we use human-written reference summaries that we manually verify as factually consistent. To generate summaries that are factually inconsistent, we generate summaries from a suite of summarization models that we have manually annotated as factually inconsistent. A model's factual consistency is then measured according to its accuracy, i.e.\\ the proportion of documents where it assigns a higher score to the factually consistent summary. To validate the usefulness of FIB, we evaluate 23 large language models ranging from 1B to 176B parameters from six different model families including BLOOM and OPT. We find that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries. However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries. We validate design choices in our benchmark including the scoring method and source of distractor summaries. Our code and benchmark data can be found at https://github.com/r-three/fib.",
            "year": 2022,
            "citationCount": 51,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new benchmark called FIB(Factual Inconsistency Benchmark) that focuses on the task of summarization, and finds that existing LLMs generally assign a higher score to factually consistent summaries than tofactually inconsistent summaries."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the factual consistency of language model outputs by incorporating relevant facts into the prompting process during generation. The paper evaluates the factual consistency of existing large language models on a news summarization benchmark.\n\nThe project focuses on fact-aware prompting to guide the model to generate factually consistent outputs, while the paper measures the preference of language models for factually consistent summaries over inconsistent ones. The approaches and goals are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "152d9a231c00d4495c9bc4a466f42165ce2e2164",
            "paperId": "152d9a231c00d4495c9bc4a466f42165ce2e2164",
            "title": "Evaluating Factual Consistency of Summaries with Large Language Models",
            "abstract": "Detecting factual errors in summaries has been an important and challenging subject in summarization research. Inspired by the emergent ability of large language models (LLMs), we explore evaluating factual consistency of summaries by directly prompting LLMs. We present a comprehensive empirical study to assess the ability of LLMs as factual consistency evaluators, which consists of (1) analyzing different LLMs such as the GPT model series and Flan-T5; (2) investigating a variety of prompting methods including vanilla prompting, chain-of-thought prompting, and a sentence-by-sentence prompting method to tackle long summaries; and (3) evaluating on diverse summaries generated by multiple summarization systems, ranging from pre-transformer methods to SOTA pretrained models. Our experiments demonstrate that prompting LLMs is able to outperform the previous best factuality systems in all settings, by up to 12.2 absolute points in terms of the binary classification accuracy on inconsistency detection.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A comprehensive empirical study to assess the ability of LLMs as factual consistency evaluators, which consists of analyzing different LLMs such as the GPT model series and Flan-T5 and investigating a variety of prompting methods including vanilla prompting, chain-of-thought prompting, and a sentence-by-sentence prompting method to tackle long summaries."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the factual consistency of language model outputs by incorporating relevant facts into the prompting process during generation. The paper focuses on evaluating the factual consistency of summaries using large language models.\n\nThe project proposes a new method called Fact-Aware Prompting (FAP) to guide the model to generate more factually consistent outputs by conditioning on relevant facts. The paper explores using large language models to directly evaluate the factual consistency of summaries through prompting.\n\nWhile both works are related to factual consistency, the project is about improving factual consistency during generation, while the paper is about evaluating factual consistency of generated summaries. The methods proposed are different: the project uses fact-aware prompting during generation, while the paper uses prompting for evaluation.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "f33d19614d78f4a6e107e768ccd0a2d4244fc89a",
            "paperId": "f33d19614d78f4a6e107e768ccd0a2d4244fc89a",
            "title": "Evaluating the Factual Consistency of Large Language Models Through Summarization",
            "abstract": "While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information. To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB ( F actual I nconsistency B enchmark) that focuses on the task of summarization. Specifically, our benchmark involves comparing the scores an LLM assigns to a factually consistent versus a factually inconsistent summary for an input news article. For factually consistent summaries, we use human-written reference summaries that we manually verify as fac-tually consistent. To generate summaries that are factually inconsistent, we generate summaries from a suite of summarization models that we have manually annotated as fac-tually inconsistent. A model\u2019s factual consistency is then measured according to its accuracy, i.e. the proportion of documents where it assigns a higher score to the factually consistent summary. To validate the usefulness of FIB , we evaluate 23 large language models ranging from 1B to 176B parameters from six different model families including BLOOM and OPT. We \ufb01nd that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries. However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries. We validate design choices in our benchmark including the scoring method and source of distractor summaries. Our code and",
            "year": 2022,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new benchmark called FIB is proposed that focuses on the task of summarization and shows that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries, and that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the factual correctness of language model outputs by incorporating factual evidence during prompting and generation. The approach is to retrieve relevant facts, construct prompts with the facts, and train the model to attend to and be consistent with the provided facts.\n\nThe research problem in the paper is evaluating the factual consistency of language models in summarization. The approach is to compare the scores assigned by language models to factually consistent and inconsistent summaries.\n\nThe proposal focuses on improving factual correctness during generation, while the paper focuses on evaluating factual consistency in generated summaries. The methods are also different: the proposal uses fact-aware prompting and training, while the paper uses score comparison between consistent and inconsistent summaries.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "659df3de2c4f21b17811b70e45a04702adbf8bfc",
            "paperId": "659df3de2c4f21b17811b70e45a04702adbf8bfc",
            "title": "Ask, Assess, and Refine: Rectifying Factual Consistency and Hallucination in LLMs with Metric-Guided Feedback Learning",
            "abstract": "Recent advancements in Large Language Models (LLMs) have heralded unprecedented capabilities in information-seeking and text generation, as evidenced by applications like Bing Chat and perplexity.ai. Despite these strides, challenges on hallucination and factual inconsistency continue to impede their wider real-world adoption. Contemporary methods, including retrieval-augmented LLMs and feedback-based learning, serve as alternatives to mitigate these challenges. However, challenges remain, particularly regarding referencing erroneous evidence (citation errors) and generating information not present in the evidence (hallucination). In this paper, we introduce the \\mathsf{A}^2\\mathsf{R} framework: Ask, Assess, and Refine. Our approach utilizes an explicit evaluation paradigm, incorporating metrics specifically tailored to assess citation errors and hallucination, aiming to address these prevalent challenges robustly. Capitalizing on these evaluations, we devise a strategy to formulate actionable natural language feedback, enabling iterative refinements that yield improved factual consistency and reduced hallucinations in responses. Our experiments on ASQA, ELI5, and QAMPARI datasets demonstrate our method\u2019s superiority in enhancing correctness, fluency, and citation quality.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces the \\mathsf{A}^2\\mathsf{R} framework: Ask, Assess, and Refine, and utilizes an explicit evaluation paradigm, incorporating metrics specifically tailored to assess citation errors and hallucination, aiming to address these prevalent challenges robustly."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the factual consistency of language model outputs by incorporating relevant facts into the prompting process and training the model to attend to and incorporate these facts during generation.\n\nThe paper proposes a framework called A^2R (Ask, Assess, and Refine) to address factual inconsistency and hallucination in large language models by using metrics to assess citation errors and hallucination, and then providing actionable natural language feedback for iterative refinement.\n\nWhile both the project proposal and the paper aim to improve factual consistency in language model outputs, their approaches differ. The project proposal focuses on fact-aware prompting and training, while the paper proposes an explicit evaluation and feedback loop using tailored metrics.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "88884b8806262a4095036041e3567d450dba39f7",
            "paperId": "88884b8806262a4095036041e3567d450dba39f7",
            "title": "Active Retrieval Augmented Generation",
            "abstract": "Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.",
            "year": 2023,
            "citationCount": 59,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the factual correctness of language model outputs by incorporating relevant facts into the prompting process and training the model to attend to and be consistent with the provided facts. The paper proposes an active retrieval augmented generation approach that iteratively retrieves relevant information throughout the generation process to improve the factual accuracy of the generated text.\n\nWhile both works aim to improve the factual correctness of language model outputs, the project proposal focuses on incorporating facts into the prompting process, while the paper proposes an active retrieval approach that retrieves information iteratively during generation. The methods differ in their approach to addressing the problem.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "6fcdad7b8d6b60b23bc51859e736c29f913b249a",
            "paperId": "6fcdad7b8d6b60b23bc51859e736c29f913b249a",
            "title": "Improving the Domain Adaptation of Retrieval Augmented Generation (RAG) Models for Open Domain Question Answering",
            "abstract": "Retrieval Augment Generation (RAG) is a recent advancement in Open-Domain Question Answering (ODQA). RAG has only been trained and explored with a Wikipedia-based external knowledge base and is not optimized for use in other specialized domains such as healthcare and news. In this paper, we evaluate the impact of joint training of the retriever and generator components of RAG for the task of domain adaptation in ODQA. We propose RAG-end2end, an extension to RAG that can adapt to a domain-specific knowledge base by updating all components of the external knowledge base during training. In addition, we introduce an auxiliary training signal to inject more domain-specific knowledge. This auxiliary signal forces RAG-end2end to reconstruct a given sentence by accessing the relevant information from the external knowledge base. Our novel contribution is that, unlike RAG, RAG-end2end does joint training of the retriever and generator for the end QA task and domain adaptation. We evaluate our approach with datasets from three domains: COVID-19, News, and Conversations, and achieve significant performance improvements compared to the original RAG model. Our work has been open-sourced through the HuggingFace Transformers library, attesting to our work\u2019s credibility and technical consistency.",
            "year": 2022,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes RAG-end2end, an extension to RAG that can adapt to a domain- specific knowledge base by updating all components of the external knowledge base during training and introduces an auxiliary training signal to inject more domain-specific knowledge."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the factual correctness of language model outputs by incorporating relevant facts into the prompting process and training the model to attend to the factual evidence. The paper focuses on adapting Retrieval Augmented Generation (RAG) models to different domains for open-domain question answering by jointly training the retriever and generator components.\n\nProject Proposal: Improving factual correctness of language model outputs using fact-aware prompting.\nPaper: Adapting RAG models to different domains for open-domain question answering.\n\nWhile both involve retrieval augmentation, the project proposal focuses on factual correctness in general language model outputs, while the paper specifically targets domain adaptation for question answering.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "dfab0f3ee6f47e36cccee145794cd117773e6f73",
            "paperId": "dfab0f3ee6f47e36cccee145794cd117773e6f73",
            "title": "Towards LLM-based Fact Verification on News Claims with a Hierarchical Step-by-Step Prompting Method",
            "abstract": "While large pre-trained language models (LLMs) have shown their impressive capabilities in various NLP tasks, they are still under-explored in the misinformation domain. In this paper, we examine LLMs with in-context learning (ICL) for news claim verification, and find that only with 4-shot demonstration examples, the performance of several prompting methods can be comparable with previous supervised models. To further boost performance, we introduce a Hierarchical Step-by-Step (HiSS) prompting method which directs LLMs to separate a claim into several subclaims and then verify each of them via multiple questions-answering steps progressively. Experiment results on two public misinformation datasets show that HiSS prompting outperforms state-of-the-art fully-supervised approach and strong few-shot ICL-enabled baselines.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Hierarchical Step-by-Step (HiSS) prompting method is introduced which directs LLMs to separate a claim into several subclaims and then verify each of them via multiple questions-answering steps progressively."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the factual correctness of language model outputs by incorporating relevant facts into the prompting process and training the model to attend to and be consistent with the provided facts. The paper focuses on using large language models with in-context learning and a hierarchical step-by-step prompting method for news claim verification.\n\nThe project proposal and the paper have different research problems and approaches. The project proposal tackles the issue of factual incorrectness in language model outputs and proposes fact-aware prompting and training techniques. The paper explores the use of large language models for news claim verification and introduces a hierarchical step-by-step prompting method.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "449257147fe40e0016f3ef89a62f20db8ff29039",
            "paperId": "449257147fe40e0016f3ef89a62f20db8ff29039",
            "title": "Pattern-Aware Chain-of-Thought Prompting in Large Language Models",
            "abstract": "Chain-of-thought (CoT) prompting can guide language models to engage in complex multi-step reasoning. The quality of provided demonstrations significantly impacts the success of downstream inference tasks. While existing automated methods prioritize accuracy and semantics in these demonstrations, we show that the underlying reasoning patterns play a more crucial role in such tasks. In this paper, we propose Pattern-Aware CoT, a prompting method that considers the diversity of demonstration patterns. By incorporating patterns such as step length and reasoning process within intermediate steps, PA-CoT effectively mitigates the issue of bias induced by demonstrations and enables better generalization to diverse scenarios. We conduct experiments on nine reasoning benchmark tasks using two open-source LLMs. The results show that our method substantially enhances reasoning performance and exhibits robustness to errors. The code will be made publicly available.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "By incorporating patterns such as step length and reasoning process within intermediate steps, PA-CoT effectively mitigates the issue of bias induced by demonstrations and enables better generalization to diverse scenarios."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve the factual correctness of language model outputs by incorporating relevant facts into the prompting process and training the model to attend to these facts during generation.\n\nThe paper focuses on enhancing the reasoning performance of language models in chain-of-thought prompting by considering the diversity of reasoning patterns in the demonstrations.\n\nWhile both works involve prompting methods for language models, the project proposal targets factual correctness, while the paper addresses reasoning performance and pattern diversity. The research problems and approaches are not directly aligned.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "85b5068d3e1364b44ec9f46b0930b521b4089df6",
            "paperId": "85b5068d3e1364b44ec9f46b0930b521b4089df6",
            "title": "Self-Checker: Plug-and-Play Modules for Fact-Checking with Large Language Models",
            "abstract": "Fact-checking is an essential task in NLP that is commonly utilized for validating the factual accuracy of claims. Prior work has mainly focused on fine-tuning pre-trained languages models on specific datasets, which can be computationally intensive and time-consuming. With the rapid development of large language models (LLMs), such as ChatGPT and GPT-3, researchers are now exploring their in-context learning capabilities for a wide range of tasks. In this paper, we aim to assess the capacity of LLMs for fact-checking by introducing Self-Checker, a framework comprising a set of plug-and-play modules that facilitate fact-checking by purely prompting LLMs in an almost zero-shot setting. This framework provides a fast and efficient way to construct fact-checking systems in low-resource environments. Empirical results demonstrate the potential of Self-Checker in utilizing LLMs for fact-checking. However, there is still significant room for improvement compared to SOTA fine-tuned models, which suggests that LLM adoption could be a promising approach for future fact-checking research.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Self-Checker, a framework comprising a set of plug-and-play modules that facilitate fact-checking by purely prompting LLMs in an almost zero-shot setting, is introduced, which provides a fast and efficient way to construct fact- checking systems in low-resource environments."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to incorporate factual evidence into the prompting process to guide language models to generate more factually consistent outputs. The proposed approach, Fact-Aware Prompting (FAP), retrieves relevant facts from a knowledge source and includes them in the prompt to the language model, training the model to attend to and incorporate the facts during generation.\n\nThe paper abstract proposes Self-Checker, a framework of plug-and-play modules that enables fact-checking using large language models in a nearly zero-shot setting by prompting them. The goal is to assess the capacity of LLMs for fact-checking and provide an efficient way to construct fact-checking systems in low-resource environments.\n\nWhile both the project proposal and the paper abstract deal with factual accuracy and large language models, their research problems and approaches differ. The project proposal focuses on improving the factual consistency of language model outputs during generation, while the paper abstract aims to assess the fact-checking capabilities of LLMs using prompting. The project proposal incorporates facts into the prompting process, whereas the paper abstract uses plug-and-play modules for fact-checking.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "12c826f4195da172b212a529f8fcf10cc79e35da",
            "paperId": "12c826f4195da172b212a529f8fcf10cc79e35da",
            "title": "Context-faithful Prompting for Large Language Models",
            "abstract": "Large language models (LLMs) encode parametric knowledge about world facts and have shown remarkable performance in knowledge-driven NLP tasks. However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks). In this paper, we seek to assess and enhance LLMs' contextual faithfulness in two aspects: knowledge conflict and prediction with abstention. We demonstrate that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies. In particular, we identify opinion-based prompts and counterfactual demonstrations as the most effective methods. Opinion-based prompts reframe the context as a narrator's statement and inquire about the narrator's opinions, while counterfactual demonstrations use instances containing false facts to improve faithfulness in knowledge conflict situations. Neither technique requires additional training. We conduct experiments on three datasets of two standard NLP tasks, machine reading comprehension and relation extraction, and the results demonstrate significant improvement in faithfulness to contexts. Code and data are released at https://github.com/wzhouad/context-faithful-llm.",
            "year": 2023,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies, and opinion-based prompts and counterfactual demonstrations are identified as the most effective methods."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the factual correctness of language model outputs by incorporating factual evidence during prompting. The approach is to retrieve relevant facts, construct prompts that include the facts, and train the model to attend to and incorporate the facts during generation.\n\nThe research problem in the paper is improving the contextual faithfulness of language models in context-sensitive tasks. The approach is to use carefully designed prompting strategies, such as opinion-based prompts and counterfactual demonstrations, without additional training.\n\nWhile both works aim to improve the output quality of language models, the proposal focuses specifically on factual correctness and uses fact retrieval and incorporation during prompting, while the paper focuses on contextual faithfulness and uses prompting strategies without fact retrieval or additional training.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "d2d16333a4b0dc7e3463b280b9945e5ee6c53396",
            "paperId": "d2d16333a4b0dc7e3463b280b9945e5ee6c53396",
            "title": "TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models",
            "abstract": "Factual consistency evaluation is often conducted using Natural Language Inference (NLI) models, yet these models exhibit limited success in evaluating summaries. Previous work improved such models with synthetic training data. However, the data is typically based on perturbed human-written summaries, which often differ in their characteristics from real model-generated summaries and have limited coverage of possible factual errors. Alternatively, large language models (LLMs) have recently shown promising results in directly evaluating generative tasks, but are too computationally expensive for practical use. Motivated by these limitations, we introduce TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM. Unlike prior work, TrueTeacher does not rely on human-written summaries, and is multilingual by nature. Experiments on the TRUE benchmark show that a student model trained using our data, substantially outperforms both the state-of-the-art model with similar capacity, and the LLM teacher. In a systematic study, we compare TrueTeacher to existing synthetic data generation methods and demonstrate its superiority and robustness to domain-shift. We also show that our method generalizes to multilingual scenarios. Lastly, we release our large scale synthetic dataset (1.4M examples), generated using TrueTeacher, and a checkpoint trained on this data.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM, which does not rely on human-written summaries, and is multilingual by nature."
            },
            "score": 6
        },
        {
            "id": "d998ff93b2e58c9b219a9103f8c2ad714a41e4b9",
            "paperId": "d998ff93b2e58c9b219a9103f8c2ad714a41e4b9",
            "title": "The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency of Language Models",
            "abstract": "Large Language Models (LLMs) make natural interfaces to factual knowledge, but their usefulness is limited by their tendency to deliver inconsistent answers to semantically equivalent questions. For example, a model might predict both\"Anne Redpath passed away in Edinburgh.\"and\"Anne Redpath's life ended in London.\"In this work, we identify potential causes of inconsistency and evaluate the effectiveness of two mitigation strategies: up-scaling and augmenting the LM with a retrieval corpus. Our results on the LLaMA and Atlas models show that both strategies reduce inconsistency while retrieval augmentation is considerably more efficient. We further consider and disentangle the consistency contributions of different components of Atlas. For all LMs evaluated we find that syntactical form and other evaluation task artifacts impact consistency. Taken together, our results provide a better understanding of the factors affecting the factual consistency of language models.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work identifies potential causes of inconsistency and evaluates the effectiveness of two mitigation strategies: up-scaling and augmenting the LM with a retrieval corpus, showing that both strategies reduce inconsistency while retrieval augmentation is considerably more efficient."
            },
            "score": 6
        },
        {
            "id": "f3cd3a0e1a0a29193fd88178ef75946dfe756622",
            "paperId": "f3cd3a0e1a0a29193fd88178ef75946dfe756622",
            "title": "Factual Consistency Evaluation of Summarisation in the Era of Large Language Models",
            "abstract": "Factual inconsistency with source documents in automatically generated summaries can lead to misinformation or pose risks. Existing factual consistency(FC) metrics are constrained by their performance, efficiency, and explainability. Recent advances in Large language models (LLMs) have demonstrated remarkable potential in text evaluation but their effectiveness in assessing FC in summarisation remains underexplored. Prior research has mostly focused on proprietary LLMs, leaving essential factors that affect their assessment capabilities unexplored. Additionally, current FC evaluation benchmarks are restricted to news articles, casting doubt on the generality of the FC methods tested on them. In this paper, we first address the gap by introducing TreatFact a dataset of LLM-generated summaries of clinical texts, annotated for FC by domain experts. Moreover, we benchmark 11 LLMs for FC evaluation across news and clinical domains and analyse the impact of model size, prompts, pre-training and fine-tuning data. Our findings reveal that despite proprietary models prevailing on the task, open-source LLMs lag behind. Nevertheless, there is potential for enhancing the performance of open-source LLMs through increasing model size, expanding pre-training data, and developing well-curated fine-tuning data. Experiments on TreatFact suggest that both previous methods and LLM-based evaluators are unable to capture factual inconsistencies in clinical summaries, posing a new challenge for FC evaluation.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "TreatFact, a dataset of LLM-generated summaries of clinical texts, annotated for FC by domain experts, is introduced and it is revealed that despite proprietary models prevailing on the task, open-source LLMs lag behind."
            },
            "score": 6
        },
        {
            "id": "4ea413e5a21a743d68c92e7f169535d0543f6051",
            "paperId": "4ea413e5a21a743d68c92e7f169535d0543f6051",
            "title": "On Improving Summarization Factual Consistency from Natural Language Feedback",
            "abstract": "Despite the recent progress in language generation models, their outputs may not always meet user expectations. In this work, we study whether informational feedback in natural language can be leveraged to improve generation quality and user preference alignment. To this end, we consider factual consistency in summarization, the quality that the summary should only contain information supported by the input documents, as the user-expected preference. We collect a high-quality dataset, DeFacto, containing human demonstrations and informational natural language feedback consisting of corrective instructions, edited summaries, and explanations with respect to the factual consistency of the summary. Using our dataset, we study three natural language generation tasks: (1) editing a summary by following the human feedback, (2) generating human feedback for editing the original summary, and (3) revising the initial summary to correct factual errors by generating both the human feedback and edited summary. We show that DeFacto can provide factually consistent human-edited summaries and further insights into summarization factual consistency thanks to its informational natural language feedback. We further demonstrate that fine-tuned language models can leverage our dataset to improve the summary factual consistency, while large language models lack the zero-shot learning ability in our proposed tasks that require controllable text generation.",
            "year": 2022,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "DeFacto can provide factually consistent human-edited summaries and further insights into summarization factual consistency thanks to its informational natural language feedback, and it is demonstrated that fine-tuned language models can leverage the dataset to improve the summary factual consistency."
            },
            "score": 6
        },
        {
            "id": "663d743272e9ab04f54d9105a3c3a3f6e22dd1dd",
            "paperId": "663d743272e9ab04f54d9105a3c3a3f6e22dd1dd",
            "title": "FactKB: Generalizable Factuality Evaluation using Language Models Enhanced with Factual Knowledge",
            "abstract": "Evaluating the factual consistency of automatically generated summaries is essential for the progress and adoption of reliable summarization systems. Despite recent advances, existing factuality evaluation models are not robust, being especially prone to entity and relation errors in new domains. We propose FactKB, a simple new approach to factuality evaluation that is generalizable across domains, in particular with respect to entities and relations. FactKB is based on language models pretrained using facts extracted from external knowledge bases. We introduce three types of complementary factuality pretraining objectives based on direct entity facts, facts grounded in auxiliary knowledge about entities, and facts constructed compositionally through knowledge base walks. The resulting factuality evaluation model achieves state-of-the-art performance on two in-domain news summarization benchmarks as well as on three out-of-domain scientific literature datasets. Further analysis of FactKB shows improved ability to detect erroneous entities and relations in summaries and is robust and generalizable across domains.",
            "year": 2023,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "FactKB is a simple new approach to factuality evaluation that is generalizable across domains, in particular with respect to entities and relations and shows improved ability to detect erroneous entities and relation in summaries."
            },
            "score": 6
        },
        {
            "id": "f5d581e916613838cbadc05ab8c35ee4ea78da32",
            "paperId": "f5d581e916613838cbadc05ab8c35ee4ea78da32",
            "title": "Fine-grained Factual Consistency Assessment for Abstractive Summarization Models",
            "abstract": "Factual inconsistencies existed in the output of abstractive summarization models with original documents are frequently presented. Fact consistency assessment requires the reasoning capability to find subtle clues to identify whether a model-generated summary is consistent with the original document. This paper proposes a fine-grained two-stage Fact Consistency assessment framework for Summarization models (SumFC). Given a document and a summary sentence, in the first stage, SumFC selects the top-K most relevant sentences with the summary sentence from the document. In the second stage, the model performs fine-grained consistency reasoning at the sentence level, and then aggregates all sentences\u2019 consistency scores to obtain the final assessment result. We get the training data pairs by data synthesis and adopt contrastive loss of data pairs to help the model identify subtle cues. Experiment results show that SumFC has made a significant improvement over the previous state-of-the-art methods. Our experiments also indicate that SumFC distinguishes detailed differences better.",
            "year": 2021,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A fine-grained two-stage Fact Consistency assessment framework for Summarization models (SumFC), which selects the top-K most relevant sentences with the summary sentence from the document and aggregates all sentences\u2019 consistency scores to obtain the final assessment result."
            },
            "score": 6
        },
        {
            "id": "3a89e289e2dd29f5e52a2bf354a637762b661257",
            "paperId": "3a89e289e2dd29f5e52a2bf354a637762b661257",
            "title": "Fine-tuning Language Models for Factuality",
            "abstract": "The fluency and creativity of large pre-trained language models (LLMs) have led to their widespread use, sometimes even as a replacement for traditional search engines. Yet language models are prone to making convincing but factually inaccurate claims, often referred to as 'hallucinations.' These errors can inadvertently spread misinformation or harmfully perpetuate misconceptions. Further, manual fact-checking of model responses is a time-consuming process, making human factuality labels expensive to acquire. In this work, we fine-tune language models to be more factual, without human labeling and targeting more open-ended generation settings than past work. We leverage two key recent innovations in NLP to do so. First, several recent works have proposed methods for judging the factuality of open-ended text by measuring consistency with an external knowledge base or simply a large model's confidence scores. Second, the direct preference optimization algorithm enables straightforward fine-tuning of language models on objectives other than supervised imitation, using a preference ranking over possible model responses. We show that learning from automatically generated factuality preference rankings, generated either through existing retrieval systems or our novel retrieval-free approach, significantly improves the factuality (percent of generated claims that are correct) of Llama-2 on held-out topics compared with RLHF or decoding strategies targeted at factuality. At 7B scale, compared to Llama-2-chat, we observe 58% and 40% reduction in factual error rate when generating biographies and answering medical questions, respectively.",
            "year": 2023,
            "citationCount": 55,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that learning from automatically generated factuality preference rankings, generated either through existing retrieval systems or the novel retrieval-free approach, significantly improves the factuality of Llama-2 on held-out topics compared with RLHF or decoding strategies targeted at factuality."
            },
            "score": 6
        },
        {
            "id": "f70bf522a90c09ed06c32c9bf36b7ee14b8a9856",
            "paperId": "f70bf522a90c09ed06c32c9bf36b7ee14b8a9856",
            "title": "Knowledge Graph-Augmented Language Models for Knowledge-Grounded Dialogue Generation",
            "abstract": "Language models have achieved impressive performances on dialogue generation tasks. However, when generating responses for a conversation that requires factual knowledge, they are far from perfect, due to an absence of mechanisms to retrieve, encode, and reflect the knowledge in the generated responses. Some knowledge-grounded dialogue generation methods tackle this problem by leveraging facts from Knowledge Graphs (KGs); however, they do not guarantee that the model utilizes a relevant piece of knowledge from the KG. To overcome this limitation, we propose SUbgraph Retrieval-augmented GEneration (SURGE), a framework for generating context-relevant and knowledge-grounded dialogues with the KG. Specifically, our SURGE framework first retrieves the relevant subgraph from the KG, and then enforces consistency across facts by perturbing their word embeddings conditioned by the retrieved subgraph. Then, we utilize contrastive learning to ensure that the generated texts have high similarity to the retrieved subgraphs. We validate our SURGE framework on OpendialKG and KOMODIS datasets, showing that it generates high-quality dialogues that faithfully reflect the knowledge from KG.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The proposed SUbgraph Retrieval-augmented GEneration (SURGE), a framework for generating context-relevant and knowledge-grounded dialogues with the KG, first retrieves the relevant subgraph from the KGs, and then enforces consistency across facts by perturbing their word embeddings conditioned by the retrieved subgraph."
            },
            "score": 6
        },
        {
            "id": "58ed1fbaabe027345f7bb3a6312d41c5aac63e22",
            "paperId": "58ed1fbaabe027345f7bb3a6312d41c5aac63e22",
            "title": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks",
            "abstract": "Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.",
            "year": 2020,
            "citationCount": 1841,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation, and finds that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline."
            },
            "score": 6
        },
        {
            "id": "b462417b40852a5a1e2d8862e5d5b464242ff902",
            "paperId": "b462417b40852a5a1e2d8862e5d5b464242ff902",
            "title": "Dynamic Retrieval-Augmented Generation",
            "abstract": "Current state-of-the-art large language models are effective in generating high-quality text and encapsulating a broad spectrum of world knowledge. These models, however, often hallucinate and lack locally relevant factual data. Retrieval-augmented approaches were introduced to overcome these problems and provide more accurate responses. Typically, the retrieved information is simply appended to the main request, restricting the context window size of the model. We propose a novel approach for the Dynamic Retrieval-Augmented Generation (DRAG), based on the entity-augmented generation, which injects compressed embeddings of the retrieved entities into the generative model. The proposed pipeline was developed for code-generation tasks, yet can be transferred to some domains of natural language processing. To train the model, we collect and publish a new project-level code generation dataset. We use it for the evaluation along with publicly available datasets. Our approach achieves several targets: (1) lifting the length limitations of the context window, saving on the prompt size; (2) allowing huge expansion of the number of retrieval entities available for the context; (3) alleviating the problem of misspelling or failing to find relevant entity names. This allows the model to beat all baselines (except GPT-3.5) with a strong margin.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel approach for the Dynamic Retrieval-Augmented Generation (DRAG), based on the entity-augmented generation, which injects compressed embeddings of the retrieved entities into the generative model, which allows the model to beat all baselines with a strong margin."
            },
            "score": 6
        },
        {
            "id": "b3876204758c54f63c64cd82d5b21b7f4983736d",
            "paperId": "b3876204758c54f63c64cd82d5b21b7f4983736d",
            "title": "Prompt Generate Train (PGT): Few-shot Domain Adaption of Retrieval Augmented Generation Models for Open Book Question-Answering",
            "abstract": "We propose a framework - Prompt, Generate, Train (PGT) - to efficiently develop a generative question-answering model for open-book question-answering over a proprietary collection of text documents. The framework adapts a retriever augmented generation (RAG) model to the target domain using supervised fine-tuning and reinforcement learning with synthetic feedback in a few-shot setting. This, we hypothesize, will yield an aligned, uncertainty calibrated model that is competitive with GPT-4 based in-context retrieval augmented generation in generating relevant answers at lower serving costs. The framework's synthetic generation pipeline will generate synthetic training data comprisingtuples using an open-source LLM and a novel consistency filtering scheme. The pipeline will be designed to generate both abstractive and extractive questions that span the entire corpus. The framework proposes to fine-tune a smaller RAG model comprising a dense retriever (ColBERTv2) and a smaller sized LLM on the synthetic dataset. In parallel, the framework will train a Reward model to score domain grounded answers higher than hallucinated answers using an a priori relevance ordering of synthetically assembled samples. In the next phase, the framework will align the RAG model with the target domain using reinforcement learning (Proximal Policy Optimization). This step may improve the RAG model's ability to generate grounded answers and ignore out of domain questions. In the final phase, the framework will calibrate the model's uncertainty for extractive question-answers.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A framework adapts a retriever augmented generation model to the target domain using supervised fine-tuning and reinforcement learning with synthetic feedback in a few-shot setting to yield an aligned, uncertainty calibrated model that is competitive with GPT-4 based in-context retrieval augmented generation in generating relevant answers at lower serving costs."
            },
            "score": 6
        },
        {
            "id": "9111d6632e3ad648e65c57c52fd945641ccbdac2",
            "paperId": "9111d6632e3ad648e65c57c52fd945641ccbdac2",
            "title": "Fine Tuning vs. Retrieval Augmented Generation for Less Popular Knowledge",
            "abstract": "Large language models (LLMs) memorize a vast amount of factual knowledge, exhibiting strong performance across diverse tasks and domains. However, it has been observed that the performance diminishes when dealing with less-popular or low-frequency concepts and entities, for example in domain specific applications. The two prominent approaches to enhance the performance of LLMs on low-frequent topics are: Retrieval Augmented Generation (RAG) and fine-tuning (FT) over synthetic data. This paper explores and evaluates the impact of RAG and FT on customizing LLMs in handling low-frequency entities on question answering task. Our findings indicate that FT significantly boosts the performance across entities of varying popularity, especially in the most and least popular groups, while RAG surpasses other methods. Additionally, the success of both RAG and FT approaches is amplified by advancements in retrieval and data augmentation techniques. We release our data and code at https://github.com/informagi/RAGvsFT.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Findings indicate that FT significantly boosts the performance across entities of varying popularity, especially in the most and least popular groups, while RAG surpasses other methods."
            },
            "score": 6
        },
        {
            "id": "bd5deadc58ee45b5e004378ba1d54a96bc947b4a",
            "paperId": "bd5deadc58ee45b5e004378ba1d54a96bc947b4a",
            "title": "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation",
            "abstract": "Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly. In this paper, we introduce FACTSCORE, a new evaluation that breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. We conduct an extensive human evaluation to obtain FACTSCOREs of people biographies generated by several state-of-the-art commercial LMs -- InstructGPT, ChatGPT, and the retrieval-augmented PerplexityAI -- and report new analysis demonstrating the need for such a fine-grained score (e.g., ChatGPT only achieves 58%). Since human evaluation is costly, we also introduce an automated model that estimates FACTSCORE using retrieval and a strong language model, with less than a 2% error rate. Finally, we use this automated metric to evaluate 6,500 generations from a new set of 13 recent LMs that would have cost $26K if evaluated by humans, with various findings: GPT-4 and ChatGPT are more factual than public models, and Vicuna and Alpaca are some of the best public models. FACTSCORE is available for public use via `pip install factscore`.",
            "year": 2023,
            "citationCount": 188,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An automated model is introduced that estimates FACTSCORE using retrieval and a strong language model, and is used to evaluate 6,500 generations from a new set of 13 recent LMs that would have cost $26K if evaluated by humans, with various findings."
            },
            "score": 6
        },
        {
            "id": "27cb586fcea5ec076b984750e9c77f0d7fc976e5",
            "paperId": "27cb586fcea5ec076b984750e9c77f0d7fc976e5",
            "title": "AlignScore: Evaluating Factual Consistency with A Unified Alignment Function",
            "abstract": "Many text generation applications require the generated text to be factually consistent with input information. Automatic evaluation of factual consistency is challenging. Previous work has developed various metrics that often depend on specific functions, such as natural language inference (NLI) or question answering (QA), trained on limited data. Those metrics thus can hardly assess diverse factual inconsistencies (e.g., contradictions, hallucinations) that occur in varying inputs/outputs (e.g., sentences, documents) from different tasks. In this paper, we propose AlignScore, a new holistic metric that applies to a variety of factual inconsistency scenarios as above. AlignScore is based on a general function of information alignment between two arbitrary text pieces. Crucially, we develop a unified training framework of the alignment function by integrating a large diversity of data sources, resulting in 4.7M training examples from 7 well-established tasks (NLI, QA, paraphrasing, fact verification, information retrieval, semantic similarity, and summarization). We conduct extensive experiments on large-scale benchmarks including 22 evaluation datasets, where 19 of the datasets were never seen in the alignment training. AlignScore achieves substantial improvement over a wide range of previous metrics. Moreover, AlignScore (355M parameters) matches or even outperforms metrics based on ChatGPT and GPT-4 that are orders of magnitude larger.",
            "year": 2023,
            "citationCount": 37,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "AlignScore, a new holistic metric that applies to a variety of factual inconsistency scenarios, is proposed, based on a general function of information alignment between two arbitrary text pieces that achieves substantial improvement over a wide range of previous metrics."
            },
            "score": 6
        },
        {
            "id": "a8b66565cdb2b8c90556bb98a7fc58ac679c2cec",
            "paperId": "a8b66565cdb2b8c90556bb98a7fc58ac679c2cec",
            "title": "Accelerating Retrieval-Augmented Language Model Serving with Speculation",
            "abstract": "Retrieval-augmented language models (RaLM) have demonstrated the potential to solve knowledge-intensive natural language processing (NLP) tasks by combining a non-parametric knowledge base with a parametric language model. Instead of fine-tuning a fully parametric model, RaLM excels at its low-cost adaptation to the latest data and better source attribution mechanisms. Among various RaLM approaches, iterative RaLM delivers a better generation quality due to a more frequent interaction between the retriever and the language model. Despite the benefits, iterative RaLM usually encounters high overheads due to the frequent retrieval step. To this end, we propose RaLMSpec, a speculation-inspired framework that provides generic speed-up over iterative RaLM while preserving the same model outputs through speculative retrieval and batched verification. By further incorporating prefetching, optimal speculation stride scheduler, and asynchronous verification, RaLMSpec can automatically exploit the acceleration potential to the fullest. For naive iterative RaLM serving, extensive evaluations over three language models on four downstream QA datasets demonstrate that RaLMSpec can achieve a speed-up ratio of 1.75-2.39x, 1.04-1.39x, and 1.31-1.77x when the retriever is an exact dense retriever, approximate dense retriever, and sparse retriever respectively compared with the baseline. For KNN-LM serving, RaLMSpec can achieve a speed-up ratio up to 7.59x and 2.45x when the retriever is an exact dense retriever and approximate dense retriever, respectively, compared with the baseline.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes RaLMSpec, a speculation-inspired framework that provides generic speed-up over iterative RaLM while preserving the same model outputs through speculative retrieval and batched verification, and can automatically exploit the acceleration potential to the fullest."
            },
            "score": 6
        },
        {
            "id": "369167166539257852eb89d0ebb47ea703dfdb8c",
            "paperId": "369167166539257852eb89d0ebb47ea703dfdb8c",
            "title": "KG-CTG: Citation Generation Through Knowledge Graph-Guided Large Language Models",
            "abstract": null,
            "year": 2024,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents a framework, and a comparative study to demonstrate the use of Large Language Models (LLMs) for the task of citation generation, and shows the improvement in the results of citation generation by incorporating the knowledge graph relations of the papers in the prompt for the LLM to better learn the relationship between the papers."
            },
            "score": 6
        },
        {
            "id": "896ca0a68e4d33d76a7366bcab85eb7d2605a8c4",
            "paperId": "896ca0a68e4d33d76a7366bcab85eb7d2605a8c4",
            "title": "Metacognitive Prompting Improves Understanding in Large Language Models",
            "abstract": "In Large Language Models (LLMs), there have been consistent advancements in task-specific performance, largely influenced by effective prompt design. Recent advancements in prompting have enhanced reasoning in logic-intensive tasks for LLMs, yet the nuanced understanding abilities of these models, crucial for processing and interpreting complex information, remain underexplored. In this study, we introduce Metacognitive Prompting (MP), a strategy inspired by human introspective reasoning processes. Using MP, LLMs undergo a systematic series of structured, self-aware evaluations, drawing on both their vast inherent knowledge and new insights. We conduct extensive experiments on four prevalent LLMs: Llama2, PaLM2, GPT-3.5, and GPT-4, across ten natural language understanding (NLU) datasets from GLUE, SuperGLUE, BLUE, and LexGLUE benchmarks. Additionally, we compare our method with chain-of-thought prompting and its advanced versions. The results show that GPT-4 consistently excels across all tasks, while other models have shown significant progress in some tasks when used in conjunction with MP. Furthermore, MP consistently outperforms existing prompting methods in both general and domain-specific NLU tasks. This study underscores the potential to amplify the understanding abilities of LLMs and highlights the benefits of mirroring human introspective reasoning in NLU tasks.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study introduces Metacognitive Prompting (MP), a strategy inspired by human introspective reasoning processes that consistently outperforms existing prompting methods in both general and domain-specific NLU tasks."
            },
            "score": 5
        },
        {
            "id": "acf90b4d165690fe27c62c4af1a28d540c784000",
            "paperId": "acf90b4d165690fe27c62c4af1a28d540c784000",
            "title": "Automatic Evaluation of Attribution by Large Language Models",
            "abstract": "A recent focus of large language model (LLM) development, as exemplified by generative search engines, is to incorporate external references to generate and support its claims. However, evaluating the attribution, i.e., verifying whether the generated statement is fully supported by the cited reference, remains an open problem. Although human evaluation is common practice, it is costly and time-consuming. In this paper, we investigate the automatic evaluation of attribution given by LLMs. We begin by defining different types of attribution errors, and then explore two approaches for automatic evaluation: prompting LLMs and fine-tuning smaller LMs. The fine-tuning data is repurposed from related tasks such as question answering, fact-checking, natural language inference, and summarization. We manually curate a set of test examples covering 12 domains from a generative search engine, New Bing. Our results on this curated test set and simulated examples from existing benchmarks highlight both promising signals and challenges. We hope our problem formulation, testbeds, and findings will help lay the foundation for future studies on this important problem.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper investigates the automatic evaluation of attribution given by large language model (LLMs), defining different types of attribution errors, and exploring two approaches for automatic evaluation: prompting LLMs and fine-tuning smaller LMs."
            },
            "score": 5
        },
        {
            "id": "e96348576b682e709b2ee06ea28cd81f8bfa102b",
            "paperId": "e96348576b682e709b2ee06ea28cd81f8bfa102b",
            "title": "Exploring the Factual Consistency in Dialogue Comprehension of Large Language Models",
            "abstract": "LLMs (Large Language Models) usually interact with users in the form of dialogue and generate responses following their instructions, which naturally require dialogue comprehension abilities. However, dialogue comprehension is a general language ability which is hard to be evaluated directly. In this work, we propose to perform the evaluation focusing on the factual consistency issue with the help of the dialogue summarization task. Besides evaluating and analyzing the dialogue summarization performance (DIAC-Sum) of different LLMs, we also derive factual questions from the generated summaries and use them as a more flexible measurement of dialogue comprehension (DIAC-QA). Our evaluation shows that, on average, 26.8% of the summaries generated by LLMs contain factual inconsistency. Even ChatGPT, the strongest model evaluated, has such errors in 16% of its summaries. For answering the factual questions, which is more challenging, the average error rate of all evaluated LLMs is 36.1%. Both results indicate serious deficiencies. Detailed analysis shows that the understanding of subject/object of the conversation is still challenging for LLMs. Furthermore, to stimulate and enhance the dialogue comprehension ability of LLMs, we propose a fine-tuning paradigm with auto-constructed multi-task data, which achieved a relative error rate reduction of 11% on DIAC-QA.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "To stimulate and enhance the dialogue comprehension ability of LLMs, a fine-tuning paradigm with auto-constructed multi-task data is proposed, which achieved a relative error rate reduction of 11% on DIAC-QA."
            },
            "score": 5
        },
        {
            "id": "36870ac7332b45edf59e5c111dc5bd24139dc1ce",
            "paperId": "36870ac7332b45edf59e5c111dc5bd24139dc1ce",
            "title": "Factual Consistency of Multilingual Pretrained Language Models",
            "abstract": "Pretrained language models can be queried for factual knowledge, with potential applications in knowledge base acquisition and tasks that require inference. However, for that, we need to know how reliable this knowledge is, and recent work has shown that monolingual English language models lack consistency when predicting factual knowledge, that is, they fill-in-the-blank differently for paraphrases describing the same fact. In this paper, we extend the analysis of consistency to a multilingual setting. We introduce a resource, mParaRel, and investigate (i) whether multilingual language models such as mBERT and XLM-R are more consistent than their monolingual counterparts;and (ii) if such models are equally consistent across languages.We find that mBERT is as inconsistent as English BERT in English paraphrases, but that both mBERT and XLM-R exhibit a high degree of inconsistency in English and even more so for all the other 45 languages.",
            "year": 2022,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "MBERT is as inconsistent as English BERT in English paraphrases, but that both mBERT and XLM-R exhibit a high degree of inconsistency in English and even more so for all the other 45 languages."
            },
            "score": 5
        },
        {
            "id": "f727f928e7e179307d8d4a1da2387393f2bd7915",
            "paperId": "f727f928e7e179307d8d4a1da2387393f2bd7915",
            "title": "Methods for Measuring, Updating, and Visualizing Factual Beliefs in Language Models",
            "abstract": "Language models can memorize a considerable amount of factual information during pretraining that can be elicited through prompting or finetuning models on tasks like question answering. In this paper, we discuss approaches to measuring model factual beliefs, updating incorrect factual beliefs in models, and visualizing graphical relationships between factual beliefs. Our main contributions include: (1) new metrics for evaluating belief-updating methods focusing on the logical consistency of beliefs, (2) a training objective for Sequential, Local, and Generalizing updates (SLAG) that improves the performance of existing hypernetwork approaches, and (3) the introduction of the belief graph, a new form of visualization for language models that shows relationships between stored model beliefs. Our experiments suggest that models show only limited consistency between factual beliefs, but update methods can both fix incorrect model beliefs and greatly improve their consistency. Although off-the-shelf optimizers are surprisingly strong belief-updating baselines, our learned optimizers can outperform them in more difficult settings than have been considered in past work.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experiments suggest that models show only limited consistency between factual beliefs, but update methods can both fix incorrect model beliefs and greatly improve their consistency, and off-the-shelf optimizers can outperform them in more difficult settings than have been considered in past work."
            },
            "score": 5
        },
        {
            "id": "6af460d34bfc8e955e43fbe15cedcf329b48bc19",
            "paperId": "6af460d34bfc8e955e43fbe15cedcf329b48bc19",
            "title": "SAC3: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency",
            "abstract": "Hallucination detection is a critical step toward understanding the trustworthiness of modern language models (LMs). To achieve this goal, we re-examine existing detection approaches based on the self-consistency of LMs and uncover two types of hallucinations resulting from 1) question-level and 2) model-level, which cannot be effectively identified through self-consistency check alone. Building upon this discovery, we propose a novel sampling-based method, i.e., semantic-aware cross-check consistency (SAC3) that expands on the principle of self-consistency checking. Our SAC3 approach incorporates additional mechanisms to detect both question-level and model-level hallucinations by leveraging advances including semantically equivalent question perturbation and cross-model response consistency checking. Through extensive and systematic empirical analysis, we demonstrate that SAC3 outperforms the state of the art in detecting both non-factual and factual statements across multiple question-answering and open-domain generation benchmarks.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel sampling-based method, i.e., semantic-aware cross-check consistency (SAC3) that expands on the principle of self-consistency checking and demonstrates that SAC3 outperforms the state of the art in detecting both non-factual and factual statements across multiple question-answering and open-domain generation benchmarks."
            },
            "score": 5
        },
        {
            "id": "73b6de24eb0e5f6ff4f9c3bdd9257f4554faca19",
            "paperId": "73b6de24eb0e5f6ff4f9c3bdd9257f4554faca19",
            "title": "Measuring and Improving Consistency in Pretrained Language Models",
            "abstract": "Abstract Consistency of a model\u2014that is, the invariance of its behavior under meaning-preserving alternations in its input\u2014is a highly desirable property in natural language processing. In this paper we study the question: Are Pretrained Language Models (PLMs) consistent with respect to factual knowledge? To this end, we create ParaRel\ud83e\udd18, a high-quality resource of cloze-style query English paraphrases. It contains a total of 328 paraphrases for 38 relations. Using ParaRel\ud83e\udd18, we show that the consistency of all PLMs we experiment with is poor\u2014 though with high variance between relations. Our analysis of the representational spaces of PLMs suggests that they have a poor structure and are currently not suitable for representing knowledge robustly. Finally, we propose a method for improving model consistency and experimentally demonstrate its effectiveness.1",
            "year": 2021,
            "citationCount": 226,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The creation of PARAREL, a high-quality resource of cloze-style query English paraphrases, and analysis of the representational spaces of PLMs suggest that they have a poor structure and are currently not suitable for representing knowledge in a robust way."
            },
            "score": 5
        },
        {
            "id": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
            "paperId": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
            "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such\"society of minds\"approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.",
            "year": 2023,
            "citationCount": 206,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer is presented, indicating that this approach significantly enhances mathematical and strategic reasoning across a number of tasks."
            },
            "score": 5
        },
        {
            "id": "06c94251e4502c483dc918f189f788e57172c14c",
            "paperId": "06c94251e4502c483dc918f189f788e57172c14c",
            "title": "NoMIRACL: Knowing When You Don't Know for Robust Multilingual Retrieval-Augmented Generation",
            "abstract": "Retrieval-augmented generation (RAG) grounds large language model (LLM) output by leveraging external knowledge sources to reduce factual hallucinations. However, prior works lack a comprehensive evaluation of different language families, making it challenging to evaluate LLM robustness against errors in external retrieved knowledge. To overcome this, we establish NoMIRACL, a human-annotated dataset for evaluating LLM robustness in RAG across 18 typologically diverse languages. NoMIRACL includes both a non-relevant and a relevant subset. Queries in the non-relevant subset contain passages judged as non-relevant, whereas queries in the relevant subset include at least a single judged relevant passage. We measure LLM robustness using two metrics: (i) hallucination rate, measuring model tendency to hallucinate an answer, when the answer is not present in passages in the non-relevant subset, and (ii) error rate, measuring model inaccuracy to recognize relevant passages in the relevant subset. In our work, we measure robustness for a wide variety of multilingual-focused LLMs and observe that most of the models struggle to balance the two capacities. Models such as LLAMA-2, Orca-2, and FLAN-T5 observe more than an 88% hallucination rate on the non-relevant subset, whereas, Mistral overall hallucinates less, but can achieve up to a 74.9% error rate on the relevant subset. Overall, GPT-4 is observed to provide the best tradeoff on both subsets, highlighting future work necessary to improve LLM robustness.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work establishes NoMIRACL, a human-annotated dataset for evaluating LLM robustness in RAG across 18 typologically diverse languages and measures robustness for a wide variety of multilingual-focused LLMs and observes that most of the models struggle to balance the two capacities."
            },
            "score": 5
        },
        {
            "id": "622380527d218729a3daf74d00afee10f5e8ebc4",
            "paperId": "622380527d218729a3daf74d00afee10f5e8ebc4",
            "title": "Retrieval Augmented Generation using Engineering Design Knowledge",
            "abstract": "Large-language Models (LLMs) need to adopt Retrieval-Augmented Generation (RAG) to generate factual responses that are better suited to knowledge-based applications in the design process. We present a data-driven method to identify explicit facts of the form - head entity :: relationship :: tail entity from patented artefact descriptions. We train roBERTa Transformer-based sequence classification models using our proprietary dataset of 44,227 sentences. Upon classifying tokens in a sentence as entities or relationships, our method uses another classifier to identify specific relationship tokens for a given pair of entities. We compare the performances against linear classifiers and Graph Neural Networks (GNNs) that both incorporate BERT Transformer-based token embeddings to predict associations among the entities and relationships. We apply our method to 4,870 fan system related patents and populate a knowledge base that constitutes around 3 million facts. Using the knowledge base, we demonstrate retrieving generalisable and specific domain knowledge for contextualising LLMs.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A data-driven method to identify explicit facts of the form - head entity :: relationship :: tail entity from patented artefact descriptions is presented and retrieving generalisable and specific domain knowledge for contextualising LLMs is demonstrated."
            },
            "score": 5
        },
        {
            "id": "810b3f4475f22f6ca0f1bded3b8523f3cdebee8d",
            "paperId": "810b3f4475f22f6ca0f1bded3b8523f3cdebee8d",
            "title": "UniMS-RAG: A Unified Multi-source Retrieval-Augmented Generation for Personalized Dialogue Systems",
            "abstract": "Large Language Models (LLMs) has shown exceptional capabilities in many natual language understanding and generation tasks. However, the personalization issue still remains a much-coveted property, especially when it comes to the multiple sources involved in the dialogue system. To better plan and incorporate the use of multiple sources in generating personalized response, we firstly decompose it into three sub-tasks: Knowledge Source Selection, Knowledge Retrieval, and Response Generation. We then propose a novel Unified Multi-Source Retrieval-Augmented Generation system (UniMS-RAG) Specifically, we unify these three sub-tasks with different formulations into the same sequence-to-sequence paradigm during the training, to adaptively retrieve evidences and evaluate the relevance on-demand using special tokens, called acting tokens and evaluation tokens. Enabling language models to generate acting tokens facilitates interaction with various knowledge sources, allowing them to adapt their behavior to diverse task requirements. Meanwhile, evaluation tokens gauge the relevance score between the dialogue context and the retrieved evidence. In addition, we carefully design a self-refinement mechanism to iteratively refine the generated response considering 1) the consistency scores between the generated response and retrieved evidence; and 2) the relevance scores. Experiments on two personalized datasets (DuLeMon and KBP) show that UniMS-RAG achieves state-of-the-art performance on the knowledge source selection and response generation task with itself as a retriever in a unified manner. Extensive analyses and discussions are provided for shedding some new perspectives for personalized dialogue systems.",
            "year": 2024,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experiments show that UniMS-RAG achieves state-of-the-art performance on the knowledge source selection and response generation task with itself as a retriever in a unified manner."
            },
            "score": 5
        },
        {
            "id": "9b7854829ae4d4653a56ba04880aff848d70fc42",
            "paperId": "9b7854829ae4d4653a56ba04880aff848d70fc42",
            "title": "Prompt Perturbation in Retrieval-Augmented Generation based Large Language Models",
            "abstract": "The robustness of large language models (LLMs) becomes increasingly important as their use rapidly grows in a wide range of domains. Retrieval-Augmented Generation (RAG) is considered as a means to improve the trustworthiness of text generation from LLMs. However, how the outputs from RAG-based LLMs are affected by slightly different inputs is not well studied. In this work, we find that the insertion of even a short prefix to the prompt leads to the generation of outputs far away from factually correct answers. We systematically evaluate the effect of such prefixes on RAG by introducing a novel optimization technique called Gradient Guided Prompt Perturbation (GGPP). GGPP achieves a high success rate in steering outputs of RAG-based LLMs to targeted wrong answers. It can also cope with instructions in the prompts requesting to ignore irrelevant context. We also exploit LLMs' neuron activation difference between prompts with and without GGPP perturbations to give a method that improves the robustness of RAG-based LLMs through a highly effective detector trained on neuron activation triggered by GGPP generated prompts. Our evaluation on open-sourced LLMs demonstrates the effectiveness of our methods.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work systematically evaluates the effect of short prefixes on RAG by introducing a novel optimization technique called Gradient Guided Prompt Perturbation (GGPP), which achieves a high success rate in steering outputs of RAG-based LLMs to targeted wrong answers."
            },
            "score": 5
        },
        {
            "id": "37e6b5df9d7253cfc1f164f48692e69792107071",
            "paperId": "37e6b5df9d7253cfc1f164f48692e69792107071",
            "title": "KINet: Incorporating Relevant Facts Into Knowledge-Grounded Dialog Generation",
            "abstract": "Knowledge-grounded conversation has led to great progress in producing informative dialog responses by leveraging external knowledge. This work focuses on two affiliated knowledge grounded conversation tasks: Knowledge Selection and Response Generation. Previous work followed the paradigm of selecting the most optimal knowledge piece to guide the conversation towards generating the proper response. However, some knowledge pieces, which are not recognized as optimal, may still benefit response generation. How to effectively leverage these relevant knowledge pieces for response generation still remain a tricky issue. To address this problem, we propose KINet, a Knowledge Incorporation Network, which deals with the problem by boosting both the knowledge selection and the response generation. The proposed model contains a negative enhanced knowledge approximator which improves knowledge selection by enhancing the dense representation of knowledge pieces, and a curriculum knowledge sampler which improves generated responses by incorporating more relevant knowledge pieces in an easy-to-hard manner. We conduct the experiment on two datasets of knowledge-grounded conversations, the results show that the proposed model significantly outperforms state-of-the-art methods in terms of both automatic and human evaluations.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes KINet, a Knowledge Incorporation Network, which deals with the problem by boosting both the knowledge selection and the response generation, and shows that the proposed model significantly outperforms state-of-the-art methods in terms of both automatic and human evaluations."
            },
            "score": 5
        },
        {
            "id": "3d6b094f439ceae770ad1ca5cb322421debf3ba8",
            "paperId": "3d6b094f439ceae770ad1ca5cb322421debf3ba8",
            "title": "DialoKG: Knowledge-Structure Aware Task-Oriented Dialogue Generation",
            "abstract": "Task-oriented dialogue generation is challenging since the underlying knowledge is often dynamic and effectively incorporating knowledge into the learning process is hard. It is particularly challenging to generate both human-like and informative responses in this setting. Recent research primarily focused on various knowledge distillation methods where the underlying relationship between the facts in a knowledge base is not effectively captured. In this paper, we go one step further and demonstrate how the structural information of a knowledge graph can improve the system's inference capabilities. Specifically, we propose DialoKG, a novel task-oriented dialogue system that effectively incorporates knowledge into a language model. Our proposed system views relational knowledge as a knowledge graph and introduces (1) a structure-aware knowledge embedding technique, and (2) a knowledge graph-weighted attention masking strategy to facilitate the system selecting relevant information during the dialogue generation. An empirical evaluation demonstrates the effectiveness of DialoKG over state-of-the-art methods on several standard benchmark datasets.",
            "year": 2022,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes DialoKG, a novel task-oriented dialogue system that effectively incorporates knowledge into a language model and introduces (1) a structure-aware knowledge embedding technique, and (2) a knowledge graph-weighted attention masking strategy to facilitate the system selecting relevant information during the dialogue generation."
            },
            "score": 5
        },
        {
            "id": "8a6fd3477a024c76ef2e0e0b2807a27f694aeed4",
            "paperId": "8a6fd3477a024c76ef2e0e0b2807a27f694aeed4",
            "title": "Unleashing Potential of Evidence in Knowledge-Intensive Dialogue Generation",
            "abstract": "Incorporating external knowledge into dialogue generation (KIDG) is crucial for improving the correctness of response, where evidence fragments serve as knowledgeable snippets supporting the factual dialogue replies. However, introducing irrelevant content often adversely impacts reply quality and easily leads to hallucinated responses. Prior work on evidence retrieval and integration in dialogue systems falls short of fully leveraging existing evidence since the model fails to locate useful fragments accurately and overlooks hidden evidence labels within the KIDG dataset. To fully Unleash the potential of evidence, we propose a framework to effectively incorporate Evidence in knowledge-Intensive Dialogue Generation (u-EIDG). Specifically, we introduce an automatic evidence generation framework that harnesses the power of Large Language Models (LLMs) to mine reliable evidence veracity labels from unlabeled data. By utilizing these evidence labels, we train a reliable evidence indicator to effectively identify relevant evidence from retrieved passages. Furthermore, we propose an evidence-augmented generator with an evidence-focused attention mechanism, which allows the model to concentrate on evidenced segments. Experimental results on MultiDoc2Dial demonstrate the efficacy of evidential label augmentation and refined attention mechanisms in improving model performance. Further analysis confirms that the proposed method outperforms other baselines (+3~+5 points) regarding coherence and factual consistency.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An automatic evidence generation framework that harnesses the power of Large Language Models to mine reliable evidence veracity labels from unlabeled data and trains a reliable evidence indicator to effectively identify relevant evidence from retrieved passages is introduced."
            },
            "score": 5
        },
        {
            "id": "03532123ccffae8d411264320e8a5ae2b6eddea0",
            "paperId": "03532123ccffae8d411264320e8a5ae2b6eddea0",
            "title": "Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP",
            "abstract": "Retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). Existing work has combined these in simple\"retrieve-then-read\"pipelines in which the RM retrieves passages that are inserted into the LM prompt. To begin to fully realize the potential of frozen LMs and RMs, we propose Demonstrate-Search-Predict (DSP), a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LM and RM can handle more reliably. We have written novel DSP programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art in-context learning results and delivering 37-120%, 8-39%, and 80-290% relative gains against the vanilla LM (GPT-3.5), a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline, respectively. We release DSP at https://github.com/stanfordnlp/dsp",
            "year": 2022,
            "citationCount": 133,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Demonstrate-Search-Predict (DSP) is proposed, a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM and can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions."
            },
            "score": 5
        },
        {
            "id": "33ac04c55ebcfa7a6dbc47514922bfb5cfeecbab",
            "paperId": "33ac04c55ebcfa7a6dbc47514922bfb5cfeecbab",
            "title": "Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales",
            "abstract": "Machine reasoning has made great progress in recent years owing to large language models (LLMs). In the clinical domain, however, most NLP-driven projects mainly focus on clinical classification or reading comprehension, and under-explore clinical reasoning for disease diagnosis due to the expensive rationale annotation with clinicians. In this work, we present a \"reasoning-aware\" diagnosis framework that rationalizes the diagnostic process via prompt-based learning in a time- and labor-efficient manner, and learns to reason over the prompt-generated rationales. Specifically, we address the clinical reasoning for disease diagnosis, where the LLM generates diagnostic rationales providing its insight on presented patient data and the reasoning path towards the diagnosis, namely Clinical Chain-of-Thought (Clinical CoT). We empirically demonstrate LLMs/LMs' ability of clinical reasoning via extensive experiments and analyses on both rationale generation and disease diagnosis in various settings. We further propose a novel set of criteria for evaluating machine-generated rationales' potential for real-world clinical settings, facilitating and benefiting future research in this area.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents a \"reasoning-aware\" diagnosis framework that rationalizes the diagnostic process via prompt-based learning in a time- and labor-efficient manner, and learns to reason over the prompt-generated rationales."
            },
            "score": 5
        },
        {
            "id": "99832586d55f540f603637e458a292406a0ed75d",
            "paperId": "99832586d55f540f603637e458a292406a0ed75d",
            "title": "LANGUAGE MODELS",
            "abstract": "While large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. Furthermore, on two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.",
            "year": 2023,
            "citationCount": 600,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces."
            },
            "score": 4
        },
        {
            "id": "d7d8c21bb385b701687f220466e519fc898502a7",
            "paperId": "d7d8c21bb385b701687f220466e519fc898502a7",
            "title": "Language-Aware Soft Prompting for Vision & Language Foundation Models",
            "abstract": "This paper is on soft prompt learning for Vision & Language (V&L) models. Similarly to their NLP counterparts, V&L models can be adapted to a downstream task by learning soft continuous prompts using a few training examples. Current methods learn the soft prompts by minimizing a cross-entropy loss using as class weights the features obtained by passing the prompts plus the class names through the text encoder. Such methods, however, signi\ufb01cantly over\ufb01t the training data suffering from large accuracy degradation when tested on unseen classes from the same domain. Our main contribution, in this paper, is a surprisingly simple approach to alleviate this problem: we use a second cross entropy loss to minimize the distance between the learned soft prompts and a set of hand-engineered manual prompts (obtained by prompt engineering). The proposed loss can be interpreted in multiple ways including as a regularizer, as a means for language-based augmentation, and as a way of learning more discriminative class centroids. Importantly, our formulation is inherently amenable to including, during training, virtual classes, i.e. class names for which no visual samples are available, further increasing the robustness of the learned prompts. Through extensive evaluations on 11 datasets, we show that our approach (a) signi\ufb01cantly outperforms all prior works on soft prompting, and (b) matches and surpasses, for the \ufb01rst time, the accuracy on novel classes obtained by hand-crafted prompts and CLIP for the majority of the test datasets. Code will be made available.",
            "year": 2022,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work uses a second cross entropy loss to minimize the distance between the learned soft prompts and a set of hand-engineered manual prompts (obtained by prompt engineering), and can be interpreted in multiple ways including as a regularizer, as a means for language-based augmentation, and as a way of learning more discriminating class centroids."
            },
            "score": 4
        },
        {
            "id": "b626560f19f815808a289ef5c24a17c57320da70",
            "paperId": "b626560f19f815808a289ef5c24a17c57320da70",
            "title": "MathPrompter: Mathematical Reasoning using Large Language Models",
            "abstract": "Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs. To the best of our knowledge, we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption. To address this deficiency, we propose \u2018MathPrompter\u2019, a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions. MathPrompter uses the Zero-shot chain-of-thought prompting technique to generate multiple algebraic expressions or python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results. This is in contrast to other prompt based CoT methods, where there is no check on the validity of the intermediate steps followed. Our technique improves over state-of-the-art on the \u2018MultiArith\u2019 dataset (78.7% - 92.5%) evaluated using 175B parameter GPT-based LLM.",
            "year": 2023,
            "citationCount": 89,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes \u2018MathPrompter\u2019, a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions, using the Zero-shot chain-of-thought prompting technique to generate multiple algebraic expressions or python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results."
            },
            "score": 4
        },
        {
            "id": "7c9f69848d28e0a7cbb00942ee83dab9773c23e4",
            "paperId": "7c9f69848d28e0a7cbb00942ee83dab9773c23e4",
            "title": "GPT-NER: Named Entity Recognition via Large Language Models",
            "abstract": "Despite the fact that large-scale Language Models (LLM) have achieved SOTA performances on a variety of NLP tasks, its performance on NER is still significantly below supervised baselines. This is due to the gap between the two tasks the NER and LLMs: the former is a sequence labeling task in nature while the latter is a text-generation model. In this paper, we propose GPT-NER to resolve this issue. GPT-NER bridges the gap by transforming the sequence labeling task to a generation task that can be easily adapted by LLMs e.g., the task of finding location entities in the input text\"Columbus is a city\"is transformed to generate the text sequence\"@@Columbus## is a city\", where special tokens @@## marks the entity to extract. To efficiently address the\"hallucination\"issue of LLMs, where LLMs have a strong inclination to over-confidently label NULL inputs as entities, we propose a self-verification strategy by prompting LLMs to ask itself whether the extracted entities belong to a labeled entity tag. We conduct experiments on five widely adopted NER datasets, and GPT-NER achieves comparable performances to fully supervised baselines, which is the first time as far as we are concerned. More importantly, we find that GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, GPT-NER performs significantly better than supervised models. This demonstrates the capabilities of GPT-NER in real-world NER applications where the number of labeled examples is limited.",
            "year": 2023,
            "citationCount": 58,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "GPT-NER exhibits a greater ability in the low-resource and few-shot setups, when the amount of training data is extremely scarce, and performs significantly better than supervised models, which demonstrates the capabilities of GPT-ner in real-world NER applications where the number of labeled examples is limited."
            },
            "score": 4
        },
        {
            "id": "a206a0c96d6076c6ab081288b0c2c95d3c7efd64",
            "paperId": "a206a0c96d6076c6ab081288b0c2c95d3c7efd64",
            "title": "Inspecting and Editing Knowledge Representations in Language Models",
            "abstract": "Neural language models (LMs) represent facts about the world described by text. Sometimes these facts derive from training data (in most LMs, a representation of the word\"banana\"encodes the fact that bananas are fruits). Sometimes facts derive from input text itself (a representation of the sentence\"I poured out the bottle\"encodes the fact that the bottle became empty). We describe REMEDI, a method for learning to map statements in natural language to fact encodings in an LM's internal representation system. REMEDI encodings can be used as knowledge editors: when added to LM hidden representations, they modify downstream generation to be consistent with new facts. REMEDI encodings may also be used as probes: when compared to LM representations, they reveal which properties LMs already attribute to mentioned entities, in some cases making it possible to predict when LMs will generate outputs that conflict with background knowledge or input text. REMEDI thus links work on probing, prompting, and LM editing, and offers steps toward general tools for fine-grained inspection and control of knowledge in LMs.",
            "year": 2023,
            "citationCount": 32,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "REMEDI is described, a method for learning to map statements in natural language to fact encodings in an LM's internal representation system, and offers steps toward general tools for fine-grained inspection and control of knowledge in LMs."
            },
            "score": 4
        },
        {
            "id": "468d1e2d75a23fecaf96fe65d8b01ff35ea5d0bd",
            "paperId": "468d1e2d75a23fecaf96fe65d8b01ff35ea5d0bd",
            "title": "Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models",
            "abstract": "Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts inserted in English reveal a clear pattern whereby the new piece of knowledge transfers only to languages with which English has a high RankC score.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy, and conducts an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level."
            },
            "score": 4
        },
        {
            "id": "3867d999b550b57e6762f9d4b0114ee7551b2e2f",
            "paperId": "3867d999b550b57e6762f9d4b0114ee7551b2e2f",
            "title": "Predicting Question-Answering Performance of Large Language Models through Semantic Consistency",
            "abstract": "Semantic consistency of a language model is broadly defined as the model\u2019s ability to produce semantically-equivalent outputs, given semantically-equivalent inputs. We address the task of assessing question-answering (QA) semantic consistency of contemporary large language models (LLMs) by manually creating a benchmark dataset with high-quality paraphrases for factual questions, and release the dataset to the community.We further combine the semantic consistency metric with additional measurements suggested in prior work as correlating with LLM QA accuracy, for building and evaluating a framework for factual QA reference-less performance prediction \u2013 predicting the likelihood of a language model to accurately answer a question. Evaluating the framework on five contemporary LLMs, we demonstrate encouraging, significantly outperforming baselines, results.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work addresses the task of assessing question-answering (QA) semantic consistency of contemporary large language models (LLMs) by manually creating a benchmark dataset with high-quality paraphrases for factual questions, and releases the dataset to the community."
            },
            "score": 4
        },
        {
            "id": "99ad11764421bfff6261c9df8526c8b5af82d794",
            "paperId": "99ad11764421bfff6261c9df8526c8b5af82d794",
            "title": "Improving Logical Consistency in Pre-Trained Language Models using Natural Language Inference",
            "abstract": "Current state-of-the-art pre-trained language models (PTLMs) contain rich and vast amounts of world knowledge, demonstrating an ability to extrapolate information from contextual texts and to accurately answer questions [1]. However, the latent factual understanding captured by PTLMs can be irrational and inconsistent, causing PTLMs to be prone to generating contradictory statements [2]. We demonstrate that natural language inference (NLI) can provide additional signal about contradictory statements output by a PTLM. We explore several approaches for aggregating the entailment and contradiction probabilities acquired through NLI on a batch of PTLM predicted answers and define a scoring heuristic that balances between the NLI output and the PTLM\u2019s confidence in its answers. Predictions whose scores are below a tuned threshold are revised before outputting final answers. In addition, we investigate methods for using these NLI probabilities to define a MaxSAT problem that, when optimized, yields corrected predictions. Our results demonstrate that a system that uses either of our approaches to revise PTLM answers has better accuracy and logical consistency than a vanilla PTLM.",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that natural language inference (NLI) can provide additional signal about contradictory statements output by a PTLM and that a system that uses either of these approaches to revise PTLM answers has better accuracy and logical consistency than a vanilla PTLM."
            },
            "score": 4
        },
        {
            "id": "c8defe1745206ed14949182739e6e970852c8027",
            "paperId": "c8defe1745206ed14949182739e6e970852c8027",
            "title": "IITK at SemEval-2024 Task 2: Exploring the Capabilities of LLMs for Safe Biomedical Natural Language Inference for Clinical Trials",
            "abstract": "Large Language models (LLMs) have demonstrated state-of-the-art performance in various natural language processing (NLP) tasks across multiple domains, yet they are prone to shortcut learning and factual inconsistencies. This research investigates LLMs' robustness, consistency, and faithful reasoning when performing Natural Language Inference (NLI) on breast cancer Clinical Trial Reports (CTRs) in the context of SemEval 2024 Task 2: Safe Biomedical Natural Language Inference for Clinical Trials. We examine the reasoning capabilities of LLMs and their adeptness at logical problem-solving. A comparative analysis is conducted on pre-trained language models (PLMs), GPT-3.5, and Gemini Pro under zero-shot settings using Retrieval-Augmented Generation (RAG) framework, integrating various reasoning chains. The evaluation yields an F1 score of 0.69, consistency of 0.71, and a faithfulness score of 0.90 on the test dataset.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This research investigates LLMs' robustness, consistency, and faithful reasoning when performing Natural Language Inference on breast cancer Clinical Trial Reports (CTRs) in the context of SemEval 2024 Task 2: Safe Biomedical Natural Language Inference for Clinical Trials."
            },
            "score": 4
        },
        {
            "id": "a698b4218024098b1adbf6c731e71d6e23e8b434",
            "paperId": "a698b4218024098b1adbf6c731e71d6e23e8b434",
            "title": "RAUCG: Retrieval-Augmented Unsupervised Counter Narrative Generation for Hate Speech",
            "abstract": "The Counter Narrative (CN) is a promising approach to combat online hate speech (HS) without infringing on freedom of speech. In recent years, there has been a growing interest in automatically generating CNs using natural language generation techniques. However, current automatic CN generation methods mainly rely on expert-authored datasets for training, which are time-consuming and labor-intensive to acquire. Furthermore, these methods cannot directly obtain and extend counter-knowledge from external statistics, facts, or examples. To address these limitations, we propose Retrieval-Augmented Unsupervised Counter Narrative Generation (RAUCG) to automatically expand external counter-knowledge and map it into CNs in an unsupervised paradigm. Specifically, we first introduce an SSF retrieval method to retrieve counter-knowledge from the multiple perspectives of stance consistency, semantic overlap rate, and fitness for HS. Then we design an energy-based decoding mechanism by quantizing knowledge injection, countering and fluency constraints into differentiable functions, to enable the model to build mappings from counter-knowledge to CNs without expert-authored CN data. Lastly, we comprehensively evaluate model performance in terms of language quality, toxicity, persuasiveness, relevance, and success rate of countering HS, etc. Experimental results show that RAUCG outperforms strong baselines on all metrics and exhibits stronger generalization capabilities, achieving significant improvements of +2.0% in relevance and +4.5% in success rate of countering metrics. Moreover, RAUCG enabled GPT2 to outperform T0 in all metrics, despite the latter being approximately eight times larger than the former. Warning: This paper may contain offensive or upsetting content!",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Retrieval-Augmented Unsupervised Counter Narrative Generation (RAUCG) is proposed to automatically expand external counter-knowledge and map it into CNs in an unsupervised paradigm and comprehensively evaluates model performance in terms of language quality, toxicity, persuasion, relevance, and success rate of countering HS, etc."
            },
            "score": 4
        },
        {
            "id": "e17109b4ca2688753948c02e37912834b5c76fad",
            "paperId": "e17109b4ca2688753948c02e37912834b5c76fad",
            "title": "Diversify Question Generation with Retrieval-Augmented Style Transfer",
            "abstract": "Given a textual passage and an answer, humans are able to ask questions with various expressions, but this ability is still challenging for most question generation (QG) systems. Existing solutions mainly focus on the internal knowledge within the given passage or the semantic word space for diverse content planning. These methods, however, have not considered the potential of external knowledge for expression diversity. To bridge this gap, we propose RAST, a framework for Retrieval-Augmented Style Transfer, where the objective is to utilize the style of diverse templates for question generation. For training RAST, we develop a novel Reinforcement Learning (RL) based approach that maximizes a weighted combination of diversity reward and consistency reward. Here, the consistency reward is computed by a Question-Answering (QA) model, whereas the diversity reward measures how much the final output mimics the retrieved template. Experimental results show that our method outperforms previous diversity-driven baselines on diversity while being comparable in terms of consistency scores. Our code is available at https://github.com/gouqi666/RAST.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes RAST, a framework for Retrieval-Augmented Style Transfer, where the objective is to utilize the style of diverse templates for question generation, and develops a novel Reinforcement Learning (RL) based approach that maximizes a weighted combination of diversity reward and consistency reward."
            },
            "score": 4
        },
        {
            "id": "d7e70b562784659721e9c12e162c3b0b1a023b6d",
            "paperId": "d7e70b562784659721e9c12e162c3b0b1a023b6d",
            "title": "Integrating Retrieval-Augmented Generation with Large Language Models in Nephrology: Advancing Practical Applications",
            "abstract": "The integration of large language models (LLMs) into healthcare, particularly in nephrology, represents a significant advancement in applying advanced technology to patient care, medical research, and education. These advanced models have progressed from simple text processors to tools capable of deep language understanding, offering innovative ways to handle health-related data, thus improving medical practice efficiency and effectiveness. A significant challenge in medical applications of LLMs is their imperfect accuracy and/or tendency to produce hallucinations\u2014outputs that are factually incorrect or irrelevant. This issue is particularly critical in healthcare, where precision is essential, as inaccuracies can undermine the reliability of these models in crucial decision-making processes. To overcome these challenges, various strategies have been developed. One such strategy is prompt engineering, like the chain-of-thought approach, which directs LLMs towards more accurate responses by breaking down the problem into intermediate steps or reasoning sequences. Another one is the retrieval-augmented generation (RAG) strategy, which helps address hallucinations by integrating external data, enhancing output accuracy and relevance. Hence, RAG is favored for tasks requiring up-to-date, comprehensive information, such as in clinical decision making or educational applications. In this article, we showcase the creation of a specialized ChatGPT model integrated with a RAG system, tailored to align with the KDIGO 2023 guidelines for chronic kidney disease. This example demonstrates its potential in providing specialized, accurate medical advice, marking a step towards more reliable and efficient nephrology practices.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A specialized ChatGPT model integrated with a RAG system, tailored to align with the KDIGO 2023 guidelines for chronic kidney disease is created, demonstrating its potential in providing specialized, accurate medical advice, marking a step towards more reliable and efficient nephrology practices."
            },
            "score": 4
        },
        {
            "id": "717d4cc5188e06791ef2043045e6e570ae764091",
            "paperId": "717d4cc5188e06791ef2043045e6e570ae764091",
            "title": "Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning",
            "abstract": "Augmenting pretrained language models (LMs) with a vision encoder (e.g., Flamingo) has obtained the state-of-the-art results in image-to-text generation. However, these models store all the knowledge within their parameters, thus often requiring enormous model parameters to model the abundant visual concepts and very rich textual descriptions. Additionally, they are inefficient in incorporating new data, requiring a computational-expensive fine-tuning process. In this work, we introduce a Retrieval-augmented Visual Language Model, Re-ViLM, built upon the Flamingo, that supports retrieving the relevant knowledge from the external database for zero and in-context few-shot image-to-text generations. By storing certain knowledge explicitly in the external database, our approach reduces the number of model parameters and can easily accommodate new data during evaluation by simply updating the database. We also construct an interleaved image and text data that facilitates in-context few-shot learning capabilities. We demonstrate that Re-ViLM significantly boosts performance for image-to-text generation tasks, especially for zero-shot and few-shot generation in out-of-domain settings with 4 times less parameters compared with baseline methods.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a Retrieval-augmented Visual Language Model, Re-ViLM, built upon the Flamingo, that supports retrieving the relevant knowledge from the external database for zero and in-context few-shot image-to-text generations."
            },
            "score": 4
        },
        {
            "id": "718ed96f853b89ae948f4688cc96ed97458141e5",
            "paperId": "718ed96f853b89ae948f4688cc96ed97458141e5",
            "title": "Incorporating Knowledge into Document Summarization: an Application of Prefix-Tuning on GPT-2",
            "abstract": "Despite the great development of document summarization techniques nowadays, factual inconsistencies between the generated summaries and the original text still occur from time to time. This paper proposes a pre\ufb01x-tuning-based approach that uses a set of trainable continuous pre\ufb01x prompt together with discrete prompts to aid model generation, which makes a signi\ufb01cant impact on both CNN/Daily Mail and XSum summaries generated using GPT-2. The improvements on fact preservation in the generated summaries indicates the effectiveness of adopting this pre\ufb01x-tuning-based method in knowledge-enhanced document summarization, and also shows a great potential on other natural language processing tasks.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The improvements on fact preservation in the generated summaries indicates the effectiveness of adopting this pre\ufb01x-tuning-based method in knowledge-enhanced document summarization, and also shows a great potential on other natural language processing tasks."
            },
            "score": 4
        },
        {
            "id": "6959dd12b49fa425d98a29cd7dfaaf53b59dcca1",
            "paperId": "6959dd12b49fa425d98a29cd7dfaaf53b59dcca1",
            "title": "Vision-Language Generative Model for View-Specific Chest X-ray Generation",
            "abstract": "Synthetic medical data generation has opened up new possibilities in the healthcare domain, offering a powerful tool for simulating clinical scenarios, enhancing diagnostic and treatment quality, gaining granular medical knowledge, and accelerating the development of unbiased algorithms. In this context, we present a novel approach called ViewXGen, designed to overcome the limitations of existing methods that rely on general domain pipelines using only radiology reports to generate frontal-view chest X-rays. Our approach takes into consideration the diverse view positions found in the dataset, enabling the generation of chest X-rays with specific views, which marks a significant advancement in the field. To achieve this, we introduce a set of specially designed tokens for each view position, tailoring the generation process to the user's preferences. Furthermore, we leverage multi-view chest X-rays as input, incorporating valuable information from different views within the same study. This integration rectifies potential errors and contributes to faithfully capturing abnormal findings in chest X-ray generation. To validate the effectiveness of our approach, we conducted statistical analyses, evaluating its performance in a clinical efficacy metric on the MIMIC-CXR dataset. Also, human evaluation demonstrates the remarkable capabilities of ViewXGen, particularly in producing realistic view-specific X-rays that closely resemble the original images.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents a novel approach called ViewXGen, designed to overcome the limitations of existing methods that rely on general domain pipelines using only radiology reports to generate frontal-view chest X-rays, taking into consideration the diverse view positions found in the dataset."
            },
            "score": 4
        },
        {
            "id": "bf731e4a763e2c389609ec5b1cfc28c98a4132d3",
            "paperId": "bf731e4a763e2c389609ec5b1cfc28c98a4132d3",
            "title": "Incorporating Background Knowledge into Dialogue Generation Using Multi-task Transformer Learning",
            "abstract": "Knowledge plays a very important role in the dialogue systems. Inspired by how humans use unstructured background knowledge in the conversations, this paper proposes a dialogue generation model based on multi-task learning. The model divides the conversation generation task into two tasks, a knowledge selection task and a response prediction task, which are regard as a reading comprehension task and a text generation task separately. Specifically, in the task of knowledge selection, a language pre-training model Bidirectional Encoder Representations from Transformers (BERT) is applied to solve the problem of selecting the knowledge from the background knowledge documents in the current context. And in the task of response prediction, a transformer version of pointer-generator network, being composed of an encoder using the shared BERT mentioned in knowledge selection and a decoder using the left-context-only transformer, is applied to copy tokens from the background knowledge via pointing and produce tokens in the vocabulary through a generator. Our experiments on the HOLL-E dataset show that our model achieves better results than the strong baseline models and the related recent work.",
            "year": 2021,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a dialogue generation model based on multi-task learning that achieves better results than the strong baseline models and the related recent work."
            },
            "score": 4
        },
        {
            "id": "ea4fa9e7155cf064de19862e41a6e63738770012",
            "paperId": "ea4fa9e7155cf064de19862e41a6e63738770012",
            "title": "Enhancing Dialogue Generation via Dynamic Graph Knowledge Aggregation",
            "abstract": "Incorporating external graph knowledge into neural chatbot models has been proven effective for enhancing dialogue generation. However, in conventional graph neural networks (GNNs), message passing on a graph is independent from text, resulting in the graph representation hidden space differing from that of the text. This training regime of existing models therefore leads to a semantic gap between graph knowledge and text. In this study, we propose a novel framework for knowledge graph enhanced dialogue generation. We dynamically construct a multi-hop knowledge graph with pseudo nodes to involve the language model in feature aggregation within the graph at all steps. To avoid the semantic biases caused by learning on vanilla subgraphs, the proposed framework applies hierarchical graph attention to aggregate graph features on pseudo nodes and then attains a global feature. Therefore, the framework can better utilise the heterogeneous features from both the post and external graph knowledge. Extensive experiments demonstrate that our framework outperforms state-of-the-art (SOTA) baselines on dialogue generation. Further analysis also shows that our representation learning framework can fill the semantic gap by coagulating representations of both text and graph knowledge. Moreover, the language model also learns how to better select knowledge triples for a more informative response via exploiting subgraph patterns within our feature aggregation process. Our code and resources are available at https://github.com/tangg555/SaBART.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel framework for knowledge graph enhanced dialogue generation that outperforms state-of-the-art (SOTA) baselines on dialogue generation and can fill the semantic gap by coagulating representations of both text and graph knowledge."
            },
            "score": 4
        },
        {
            "id": "7d884b40ef5892f61e0f6f358b8e29983f64a178",
            "paperId": "7d884b40ef5892f61e0f6f358b8e29983f64a178",
            "title": "Controllable Story Generation with External Knowledge Using Large-Scale Language Models",
            "abstract": "Existing pre-trained large language models have shown unparalleled generative capabilities. However, they are not controllable. In this paper, we propose MEGATRON-CNTRL, a novel framework that uses large-scale language models and adds control to text generation by incorporating an external knowledge base. Our framework consists of a keyword predictor, a knowledge retriever, a contextual knowledge ranker, and a conditional text generator. As we do not have access to ground-truth supervision for the knowledge ranker, we make use of weak supervision from sentence embedding. The empirical results show that our model generates more fluent, consistent, and coherent stories with less repetition and higher diversity compared to prior work on the ROC story dataset. We showcase the controllability of our model by replacing the keywords used to generate stories and re-running the generation process. Human evaluation results show that 77.5% of these stories are successfully controlled by the new keywords. Furthermore, by scaling our model from 124 million to 8.3 billion parameters we demonstrate that larger models improve both the quality of generation (from 74.5% to 93.0% for consistency) and controllability (from 77.5% to 91.5%).",
            "year": 2020,
            "citationCount": 107,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "MEGATRON-CNTRL is a novel framework that uses large-scale language models and adds control to text generation by incorporating an external knowledge base and showcases the controllability of the model by replacing the keywords used to generate stories and re-running the generation process."
            },
            "score": 4
        },
        {
            "id": "3768f2294e6f7407124de95ddf1c97989ba3c55e",
            "paperId": "3768f2294e6f7407124de95ddf1c97989ba3c55e",
            "title": "Multitask Pretraining with Structured Knowledge for Text-to-SQL Generation",
            "abstract": "Many machine learning-based low-code or no-code applications involve generating code that interacts with structured knowledge. For example, one of the most studied tasks in this area is generating SQL code from a natural language statement. Prior work shows that incorporating context information from the database schema, such as table and column names, is beneficial to model performance on this task. In this work we present a large pretraining dataset and strategy for learning representations of text, tables, and SQL code that leverages the entire context of the problem. Specifically, we build on existing encoder-decoder architecture by introducing a multitask pretraining framework that complements the unique attributes of our diverse pretraining data. Our work represents the first study on large-scale pretraining of encoder-decoder models for interacting with structured knowledge, and offers a new state-of-the-art foundation model in text-to-SQL generation. We validate our approach with experiments on two SQL tasks, showing improvement over existing methods, including a 1.7 and 2.2 percentage point improvement over prior state-of-the-arts on Spider and CoSQL.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents a large pretraining dataset and strategy for learning representations of text, tables, and SQL code that leverages the entire context of the problem, and offers a new state-of-the-art foundation model in text-to-SQL generation."
            },
            "score": 4
        },
        {
            "id": "c7de1e95e7f130fcbab0dea763869ff2244523e8",
            "paperId": "c7de1e95e7f130fcbab0dea763869ff2244523e8",
            "title": "Incorporating Background Knowledge into Video Description Generation",
            "abstract": "Most previous efforts toward video captioning focus on generating generic descriptions, such as, \u201cA man is talking.\u201d We collect a news video dataset to generate enriched descriptions that include important background knowledge, such as named entities and related events, which allows the user to fully understand the video content. We develop an approach that uses video meta-data to retrieve topically related news documents for a video and extracts the events and named entities from these documents. Then, given the video as well as the extracted events and entities, we generate a description using a Knowledge-aware Video Description network. The model learns to incorporate entities found in the topically related documents into the description via an entity pointer network and the generation procedure is guided by the event and entity types from the topically related documents through a knowledge gate, which is a gating mechanism added to the model\u2019s decoder that takes a one-hot vector of these types. We evaluate our approach on the new dataset of news videos we have collected, establishing the first benchmark for this dataset as well as proposing a new metric to evaluate these descriptions.",
            "year": 2018,
            "citationCount": 25,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work develops an approach that uses video meta-data to retrieve topically related news documents for a video and extracts the events and named entities from these documents and generates a description using a Knowledge-aware Video Description network."
            },
            "score": 4
        },
        {
            "id": "ad27059af0c92da5deb854874f7c9e89214b992a",
            "paperId": "ad27059af0c92da5deb854874f7c9e89214b992a",
            "title": "PHALM: Building a Knowledge Graph from Scratch by Prompting Humans and a Language Model",
            "abstract": "Despite the remarkable progress in natural language understanding with pretrained Transformers, neural language models often do not handle commonsense knowledge well. Toward commonsense-aware models, there have been attempts to obtain knowledge, ranging from automatic acquisition to crowdsourcing. However, it is difficult to obtain a high-quality knowledge base at a low cost, especially from scratch. In this paper, we propose PHALM, a method of building a knowledge graph from scratch, by prompting both crowdworkers and a large language model (LLM). We used this method to build a Japanese event knowledge graph and trained Japanese commonsense generation models. Experimental results revealed the acceptability of the built graph and inferences generated by the trained models. We also report the difference in prompting humans and an LLM. Our code, data, and models are available at github.com/nlp-waseda/comet-atomic-ja.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes PHALM, a method of building a knowledge graph from scratch, by prompting both crowdworkers and a large language model (LLM), and used this method to build a Japanese event knowledge graph and trained Japanese commonsense generation models."
            },
            "score": 4
        },
        {
            "id": "1640bda76a52f8b29e012b4e98b785882fb011c2",
            "paperId": "1640bda76a52f8b29e012b4e98b785882fb011c2",
            "title": "Legal Syllogism Prompting: Teaching Large Language Models for Legal Judgment Prediction",
            "abstract": "Legal syllogism is a form of deductive reasoning commonly used by legal professionals to analyze cases. In this paper, we propose legal syllogism prompting (LoT), a simple prompting method to teach large language models (LLMs) for legal judgment prediction. LoT teaches only that in the legal syllogism the major premise is law, the minor premise is the fact, and the conclusion is judgment. Then the models can produce a syllogism reasoning of the case and give the judgment without any learning, fine-tuning, or examples. On CAIL2018, a Chinese criminal case dataset, we performed zero-shot judgment prediction experiments with GPT-3 models. Our results show that LLMs with LoT achieve better performance than the baseline and chain of thought prompting, the state-of-art prompting method on diverse reasoning tasks. LoT enables the model to concentrate on the key information relevant to the judgment and to correctly understand the legal meaning of acts, as compared to other methods. Our method enables LLMs to predict judgment along with law articles and justification, which significantly enhances the explainability of models.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results show that LLMs with LoT achieve better performance than the baseline and chain of thought prompting, the state-of-art prompting method on diverse reasoning tasks."
            },
            "score": 3
        },
        {
            "id": "6d1ef4436904de111c8b1975bbf25d3fe2f165f7",
            "paperId": "6d1ef4436904de111c8b1975bbf25d3fe2f165f7",
            "title": "DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting",
            "abstract": "Recent progress has shown that large-scale pre-training using contrastive image-text pairs can be a promising alternative for high-quality visual representation learning from natural language supervision. Benefiting from a broader source of supervision, this new paradigm exhibits impressive transferability to downstream classification tasks and datasets. However, the problem of transferring the knowledge learned from image-text pairs to more complex dense prediction tasks has barely been visited. In this work, we present a new framework for dense prediction by implicitly and explicitly leveraging the pre-trained knowledge from CLIP. Specifically, we convert the original image-text matching problem in CLIP to a pixel-text matching problem and use the pixel-text score maps to guide the learning of dense prediction models. By further using the contextual information from the image to prompt the language model, we are able to facilitate our model to better exploit the pretrained knowledge. Our method is model-agnostic, which can be applied to arbitrary dense prediction systems and various pre-trained visual backbones including both CLIP models and ImageNet pre-trained models. Extensive experiments demonstrate the superior performance of our methods on semantic segmentation, object detection, and instance segmentation tasks. Code is available at https://github.com/raoyongming/DenseCLIP.",
            "year": 2021,
            "citationCount": 336,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents a new framework for dense prediction by implicitly and explicitly leveraging the pre-trained knowledge from CLIP and converts the original image-text matching problem in CLIP to a pixel- text matching problem and uses the pixel-text score maps to guide the learning of dense prediction models."
            },
            "score": 3
        },
        {
            "id": "97992c13baa6185c03d9e672f53185bc59822596",
            "paperId": "97992c13baa6185c03d9e672f53185bc59822596",
            "title": "Chain-of-Dictionary Prompting Elicits Translation in Large Language Models",
            "abstract": "Large language models (LLMs) have shown surprisingly good performance in multilingual neural machine translation (MNMT) even when trained without parallel data. Yet, despite the fact that the amount of training data is gigantic, they still struggle with translating rare words, particularly for low-resource languages. Even worse, it is usually unrealistic to retrieve relevant demonstrations for in-context learning with low-resource languages on LLMs, which restricts the practical use of LLMs for translation -- how should we mitigate this problem? To this end, we present a novel method, CoD, which augments LLMs with prior knowledge with the chains of multilingual dictionaries for a subset of input words to elicit translation abilities for LLMs. Extensive experiments indicate that augmenting ChatGPT with CoD elicits large gains by up to 13x chrF++ points for MNMT (3.08 to 42.63 for English to Serbian written in Cyrillic script) on FLORES-200 full devtest set. We further demonstrate the importance of chaining the multilingual dictionaries, as well as the superiority of CoD to few-shot demonstration for low-resource languages.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel method, CoD, is presented, which augments LLMs with prior knowledge with the chains of multilingual dictionaries for a subset of input words to elicit translation abilities for LLMs, and indicates that augmenting ChatGPT with CoD elicits large gains for MNMT."
            },
            "score": 3
        },
        {
            "id": "bbc1c2c96c24b564d34c4a861eb99300af9ba0e2",
            "paperId": "bbc1c2c96c24b564d34c4a861eb99300af9ba0e2",
            "title": "Cache me if you Can: an Online Cost-aware Teacher-Student framework to Reduce the Calls to Large Language Models",
            "abstract": "Prompting Large Language Models (LLMs) performs impressively in zero- and few-shot settings. Hence, small and medium-sized enterprises (SMEs) that cannot afford the cost of creating large task-specific training datasets, but also the cost of pretraining their own LLMs, are increasingly turning to third-party services that allow them to prompt LLMs. However, such services currently require a payment per call, which becomes a significant operating expense (OpEx). Furthermore, customer inputs are often very similar over time, hence SMEs end-up prompting LLMs with very similar instances. We propose a framework that allows reducing the calls to LLMs by caching previous LLM responses and using them to train a local inexpensive model on the SME side. The framework includes criteria for deciding when to trust the local model or call the LLM, and a methodology to tune the criteria and measure the tradeoff between performance and cost. For experimental purposes, we instantiate our framework with two LLMs, GPT-3.5 or GPT-4, and two inexpensive students, a k-NN classifier or a Multi-Layer Perceptron, using two common business tasks, intent recognition and sentiment analysis. Experimental results indicate that significant OpEx savings can be obtained with only slightly lower performance.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a framework that allows reducing the calls to LLMs by caching previous LLM responses and using them to train a local inexpensive model on the SME side, and results indicate that significant OpEx savings can be obtained with only slightly lower performance."
            },
            "score": 3
        },
        {
            "id": "c871377b208814713c18e25633866323a2982136",
            "paperId": "c871377b208814713c18e25633866323a2982136",
            "title": "Proving Test Set Contamination in Black Box Language Models",
            "abstract": "Large language models are trained on vast amounts of internet data, prompting concerns and speculation that they have memorized public benchmarks. Going from speculation to proof of contamination is challenging, as the pretraining data used by proprietary models are often not publicly accessible. We show that it is possible to provide provable guarantees of test set contamination in language models without access to pretraining data or model weights. Our approach leverages the fact that when there is no data contamination, all orderings of an exchangeable benchmark should be equally likely. In contrast, the tendency for language models to memorize example order means that a contaminated language model will find certain canonical orderings to be much more likely than others. Our test flags potential contamination whenever the likelihood of a canonically ordered benchmark dataset is significantly higher than the likelihood after shuffling the examples. We demonstrate that our procedure is sensitive enough to reliably prove test set contamination in challenging situations, including models as small as 1.4 billion parameters, on small test sets of only 1000 examples, and datasets that appear only a few times in the pretraining corpus. Using our test, we audit five popular publicly accessible language models for test set contamination and find little evidence for pervasive contamination.",
            "year": 2023,
            "citationCount": 34,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work shows that it is possible to provide provable guarantees of test set contamination in language models without access to pretraining data or model weights, and demonstrates that the procedure is sensitive enough to reliably prove testSet contamination in challenging situations."
            },
            "score": 3
        },
        {
            "id": "77040969110fab39a55699cb06f9edf68789445a",
            "paperId": "77040969110fab39a55699cb06f9edf68789445a",
            "title": "Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation",
            "abstract": "Generating videos for visual storytelling can be a tedious and complex process that typically requires either live-action filming or graphics animation rendering. To bypass these challenges, our key idea is to utilize the abundance of existing video clips and synthesize a coherent storytelling video by customizing their appearances. We achieve this by developing a framework comprised of two functional modules: (i) Motion Structure Retrieval, which provides video candidates with desired scene or motion context described by query texts, and (ii) Structure-Guided Text-to-Video Synthesis, which generates plot-aligned videos under the guidance of motion structure and text prompts. For the first module, we leverage an off-the-shelf video retrieval system and extract video depths as motion structure. For the second module, we propose a controllable video generation model that offers flexible controls over structure and characters. The videos are synthesized by following the structural guidance and appearance instruction. To ensure visual consistency across clips, we propose an effective concept personalization approach, which allows the specification of the desired character identities through text prompts. Extensive experiments demonstrate that our approach exhibits significant advantages over various existing baselines.",
            "year": 2023,
            "citationCount": 20,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work develops a framework comprised of two functional modules, which provides video candidates with desired scene or motion context described by query texts, and generates plot-aligned videos under the guidance of motion structure and text prompts."
            },
            "score": 3
        },
        {
            "id": "ad113d8b6f8f6dace7abd12dc88af520aaaf3fc7",
            "paperId": "ad113d8b6f8f6dace7abd12dc88af520aaaf3fc7",
            "title": "LISA: Reasoning Segmentation via Large Language Model",
            "abstract": "Although perception systems have made remarkable advancements in recent years, they still rely on explicit human instruction or pre-defined categories to identify the target objects before executing visual recognition tasks. Such systems cannot actively reason and comprehend implicit user intention. In this work, we propose a new segmentation task -- reasoning segmentation. The task is designed to output a segmentation mask given a complex and implicit query text. Furthermore, we establish a benchmark comprising over one thousand image-instruction-mask data samples, incorporating intricate reasoning and world knowledge for evaluation purposes. Finally, we present LISA: large Language Instructed Segmentation Assistant, which inherits the language generation capabilities of multimodal Large Language Models (LLMs) while also possessing the ability to produce segmentation masks. We expand the original vocabulary with atoken and propose the embedding-as-mask paradigm to unlock the segmentation capability. Remarkably, LISA can handle cases involving complex reasoning and world knowledge. Also, it demonstrates robust zero-shot capability when trained exclusively on reasoning-free datasets. In addition, fine-tuning the model with merely 239 reasoning segmentation data samples results in further performance enhancement. Both quantitative and qualitative experiments show our method effectively unlocks new reasoning segmentation capabilities for multimodal LLMs. Code, models, and data are available at https://github.com/dvlab-research/LISA.",
            "year": 2023,
            "citationCount": 107,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a new segmentation task -- reasoning segmentation and presents LISA: large Language Instructed Segmentation Assistant, which inherits the language generation capabilities of multimodal Large Language Models (LLMs) while also possessing the ability to produce segmentation masks."
            },
            "score": 3
        },
        {
            "id": "6e53a4e0f38d4be2dcaa2625a2a2a9332d338d05",
            "paperId": "6e53a4e0f38d4be2dcaa2625a2a2a9332d338d05",
            "title": "Incorporating domain knowledge through task augmentation for front-end JavaScript code generation",
            "abstract": "Code generation aims to generate a code snippet automatically from natural language descriptions. Generally, the mainstream code generation methods rely on a large amount of paired training data, including both the natural language description and the code. However, in some domain-specific scenarios, building such a large paired corpus for code generation is difficult because there is no directly available pairing data, and a lot of effort is required to manually write the code descriptions to construct a high-quality training dataset. Due to the limited training data, the generation model cannot be well trained and is likely to be overfitting, making the model's performance unsatisfactory for real-world use. To this end, in this paper, we propose a task augmentation method that incorporates domain knowledge into code generation models through auxiliary tasks and a Subtoken-TranX model by extending the original TranX model to support subtoken-level code generation. To verify our proposed approach, we collect a real-world code generation dataset and conduct experiments on it. Our experimental results demonstrate that the subtoken-level TranX model outperforms the original TranX model and the Transformer model on our dataset, and the exact match accuracy of Subtoken-TranX improves significantly by 12.75% with the help of our task augmentation method. The model performance on several code categories has satisfied the requirements for application in industrial systems. Our proposed approach has been adopted by Alibaba's BizCook platform. To the best of our knowledge, this is the first domain code generation system adopted in industrial development environments.",
            "year": 2022,
            "citationCount": 20,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A task augmentation method that incorporates domain knowledge into code generation models through auxiliary tasks and a Subtoken-TranX model by extending the original TranX model to support subtoken-level code generation is proposed and adopted by Alibaba's BizCook platform."
            },
            "score": 3
        },
        {
            "id": "9c16be614fb8470c27b62b12ccf142841dc40eef",
            "paperId": "9c16be614fb8470c27b62b12ccf142841dc40eef",
            "title": "APIContext2Com: Code Comment Generation by Incorporating Pre-Defined API Documentation",
            "abstract": "Code comments are significantly helpful in comprehending software programs and also aid developers to save a great deal of time in software maintenance. Code comment generation aims to automatically predict comments in natural language given a code snippet. Several works investigate the effect of integrating external knowledge on the quality of generated comments. In this study, we propose a solution, namely APIContext2Com, to improve the effectiveness of generated comments by incorporating the pre-defined Application Programming Interface (API) context. The API context includes the definition and description of the pre-defined APIs that are used within the code snippets. As the detailed API information expresses the functionality of a code snippet, it can be helpful in better generating the code summary. We introduce a seq-2-seq encoder-decoder neural network model with different sets of multiple encoders to effectively transform distinct inputs into target comments. A ranking mechanism is also developed to exclude non-informative APIs, so that we can filter out unrelated APIs. We evaluate our approach using the Java dataset from CodeSearchNet. The findings reveal that the proposed model improves the best baseline by 1.88 (8.24%), 2.16 (17.58% 1.38 (18.3%), 0.73 (14.17%), 1.58 (14.98 %) and 1.9 (6.92 %) for BLEU1, BLEU2, BLEU3, BLEU4, METEOR, ROUGE-L respectively. Human evaluation and ablation studies confirm the quality of the generated comments and the effect of architecture and ranking APIs.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A seq-2-seq encoder-decoder neural network model with different sets of multiple encoders to effectively transform distinct inputs into target comments is introduced to improve the effectiveness of generated comments by incorporating the pre-defined Application Programming Interface (API) context."
            },
            "score": 3
        },
        {
            "id": "ee83fd551110eb335a8769af80c98fc84de4e7da",
            "paperId": "ee83fd551110eb335a8769af80c98fc84de4e7da",
            "title": "Information Retrieval Performance in Text Generation using Knowledge from Generative Pre-trained Transformer (GPT-3)",
            "abstract": "The rise of advanced language models like GPT-3 and text generation has witnessed remarkable progress. However, leveraging the vast amount of knowledge within these models to enhance information retrieval performance remains an area that needs to be explored. This research used Artificial Intelligence, specifically the OpenAI GPT-3 language model, to create an application to help make written content. This research investigates the impact of incorporating GPT-3's knowledge into text generation processes and evaluates its influence on information retrieval tasks. Several features in text generation generate text that requires exact information, such as specifications for a product and accurate descriptions of a job or product, which are included in the concept of information retrieval in text creation by language models. The research used the few-shot learning method in the GPT-3 language model. The generated responses are then evaluated using established information retrieval metrics such as precision, recall, and F1-score. The findings of this research reveal the effectiveness of utilizing GPT-3's knowledge in enhancing information retrieval performance. The generated responses demonstrate improved relevance to user queries, resulting in the same performance precision and recall scores compared to other paid text generator websites. Application results are testing in capabilities of retrieving some information. Application capabilities tested on other commercial text generator engines. The test results obtained BERTscore 86\\% (precision), 88\\% (recall), and 87\\% (F1-Score).\u00a0",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The findings of this research reveal the effectiveness of utilizing GPT-3's knowledge in enhancing information retrieval performance, with improved relevance to user queries, resulting in the same performance precision and recall scores compared to other paid text generator websites."
            },
            "score": 3
        },
        {
            "id": "5f86c548675f54299a0a1f7abe2b6ca9d3a3296c",
            "paperId": "5f86c548675f54299a0a1f7abe2b6ca9d3a3296c",
            "title": "TeleQnA: A Benchmark Dataset to Assess Large Language Models Telecommunications Knowledge",
            "abstract": "We introduce TeleQnA, the first benchmark dataset designed to evaluate the knowledge of Large Language Models (LLMs) in telecommunications. Comprising 10,000 questions and answers, this dataset draws from diverse sources, including standards and research articles. This paper outlines the automated question generation framework responsible for creating this dataset, along with how human input was integrated at various stages to ensure the quality of the questions. Afterwards, using the provided dataset, an evaluation is conducted to assess the capabilities of LLMs, including GPT-3.5 and GPT-4. The results highlight that these models struggle with complex standards related questions but exhibit proficiency in addressing general telecom-related inquiries. Additionally, our results showcase how incorporating telecom knowledge context significantly enhances their performance, thus shedding light on the need for a specialized telecom foundation model. Finally, the dataset is shared with active telecom professionals, whose performance is subsequently benchmarked against that of the LLMs. The findings illustrate that LLMs can rival the performance of active professionals in telecom knowledge, thanks to their capacity to process vast amounts of information, underscoring the potential of LLMs within this domain. The dataset has been made publicly accessible on GitHub.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The automated question generation framework responsible for creating TeleQnA, the first benchmark dataset designed to evaluate the knowledge of Large Language Models (LLMs) in telecommunications, is outlined, illustrating that LLMs can rival the performance of active professionals in telecom knowledge, thanks to their capacity to process vast amounts of information."
            },
            "score": 3
        },
        {
            "id": "5311db0b04b95fa43b886387fb1f484055638660",
            "paperId": "5311db0b04b95fa43b886387fb1f484055638660",
            "title": "Modal-aware Visual Prompting for Incomplete Multi-modal Brain Tumor Segmentation",
            "abstract": "In the realm of medical imaging, distinct magnetic resonance imaging (MRI) modalities can provide complementary medical insights. However, it is not uncommon for one or more modalities to be absent due to image corruption, artifacts, acquisition protocols, allergies to contrast agents, or cost constraints, posing a significant challenge for perceiving the modality-absent state in incomplete modality segmentation.In this work, we introduce a novel incomplete multi-modal segmentation framework called Modal-aware Visual Prompting (MAVP), which draws inspiration from the widely used pre-training and prompt adjustment protocol employed in natural language processing (NLP). In contrast to previous prompts that typically use textual network embeddings, we utilize embeddings as the prompts generated by a modality state classifier that focuses on the missing modality states. Additionally, we integrate modality state prompts into both the extraction stage of each modality and the modality fusion stage to facilitate intra/inter-modal adaptation. Our approach achieves state-of-the-art performance in various modality-incomplete scenarios compared to incomplete modality-specific solutions.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a novel incomplete multi-modal segmentation framework called Modal-aware Visual Prompting (MAVP), which draws inspiration from the widely used pre-training and prompt adjustment protocol employed in natural language processing (NLP), and utilizes embeddings as the prompts generated by a modality state classifier that focuses on the missing modality states."
            },
            "score": 3
        },
        {
            "id": "5c5ab276b00c1f19fbb0a3d2c38d532becac9442",
            "paperId": "5c5ab276b00c1f19fbb0a3d2c38d532becac9442",
            "title": "A Brief History of Prompt: Leveraging Language Models. (Through Advanced Prompting)",
            "abstract": "This paper presents a comprehensive exploration of the evolution of prompt engineering and generation in the field of natural language processing (NLP). Starting from the early language models and information retrieval systems, we trace the key developments that have shaped prompt engineering over the years. The introduction of attention mechanisms in 2015 revolutionized language understanding, leading to advancements in controllability and context-awareness. Subsequent breakthroughs in reinforcement learning techniques further enhanced prompt engineering, addressing issues like exposure bias and biases in generated text. We examine the significant contributions in 2018 and 2019, focusing on fine-tuning strategies, control codes, and template-based generation. The paper also discusses the growing importance of fairness, human-AI collaboration, and low-resource adaptation. In 2020 and 2021, contextual prompting and transfer learning gained prominence, while 2022 and 2023 witnessed the emergence of advanced techniques like unsupervised pre-training and novel reward shaping. Throughout the paper, we reference specific research studies that exemplify the impact of various developments on prompt engineering. The journey of prompt engineering continues, with ethical considerations being paramount for the responsible and inclusive future of AI systems.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The journey of prompt engineering continues, with ethical considerations being paramount for the responsible and inclusive future of AI systems."
            },
            "score": 3
        },
        {
            "id": "4acba4215a22be3a56c879cc33e8f974ba64d449",
            "paperId": "4acba4215a22be3a56c879cc33e8f974ba64d449",
            "title": "Hard Sample Aware Prompt-Tuning",
            "abstract": "Prompt-tuning based few-shot learning has garnered increasing attention in recent years due to its efficiency and promising capability. To achieve the best performance for NLP tasks with just a few samples, it is vital to include as many informative samples as possible and to avoid misleading ones. However, there is no work in prompt-tuning literature addressing the problem of differentiating informative hard samples from misleading ones in model training, which is challenging due to the lack of supervision signals about the quality of the samples to train a well-performed model. We propose a Hard Sample Aware Prompt-Tuning framework (i.e. HardPT) to solve the non-differentiable problem in hard sample identification with reinforcement learning, and to strengthen the discrimination of the feature space without changing the original data distribution via an adaptive contrastive learning method. An extensive empirical study on a series of NLP tasks demonstrates the capability of HardPT in few-shot scenarios. HardPT obtains new SOTA results on all evaluated NLP tasks, including pushing the SST-5 accuracy to 49.5% (1.1% point absolute improvement), QNLI accuracy to 74.6% (1.9% absolute improvement), NMLI accuracy to 71.5 (0.7% absolute improvement), TACREV F_1-score to 28.2 (1.0 absolute improvement), and i2b2/VA F_1-score to 41.2 (1.3 absolute improvement).",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a Hard Sample Aware Prompt-Tuning framework (i.e. HardPT) to solve the non-differentiable problem in hard sample identification with reinforcement learning, and to strengthen the discrimination of the feature space without changing the original data distribution via an adaptive contrastive learning method."
            },
            "score": 3
        },
        {
            "id": "96328033cd5fba1973c81fefc69a4f9f956985d2",
            "paperId": "96328033cd5fba1973c81fefc69a4f9f956985d2",
            "title": "LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision & Language Models",
            "abstract": "Soft prompt learning has recently emerged as one of the methods of choice for adapting V&L models to a downstream task using a few training examples. However, current methods significantly overfit the training data, suffering from large accuracy degradation when tested on unseen classes from the same domain. To this end, in this paper, we make the following 4 contributions: (1) To alleviate base class overfitting, we propose a novel Language- Aware Soft Prompting (LASP) learning method by means of a text-to-text cross-entropy loss that maximizes the probability of the learned prompts to be correctly classified with respect to pre-defined hand-crafted textual prompts. (2) To increase the representation capacity of the prompts, we propose grouped LASP where each group of prompts is optimized with respect to a separate subset of textual prompts. (3) We identify a visual-language misalignment introduced by prompt learning and LASP, and more importantly, propose a re-calibration mechanism to address it. (4) We show that LASP is inherently amenable to including, during training, virtual classes, i.e. class names for which no visual samples are available, further increasing the robustness of the learned prompts. Through evaluations on 11 datasets, we show that our approach (a) significantly outperforms all prior works on soft prompting, and (b) matches and surpasses, for the first time, the accuracy on novel classes obtained by hand-crafted prompts and CLIP for 8 out of 11 test datasets. Code will be made available here.",
            "year": 2022,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel Language- Aware Soft Prompting (LASP) learning method by means of a text-to-text cross-entropy loss that maximizes the probability of the learned prompts to be correctly classified with respect to pre-defined hand-crafted textual prompts and shows that LASP is inherently amenable to including, during training, virtual classes."
            },
            "score": 2
        },
        {
            "id": "fccf8776d7525627c518a56a1f4db367a4d7120b",
            "paperId": "fccf8776d7525627c518a56a1f4db367a4d7120b",
            "title": "Choice Over Control: How Users Write with Large Language Models using Diegetic and Non-Diegetic Prompting",
            "abstract": "We propose a conceptual perspective on prompts for Large Language Models (LLMs) that distinguishes between (1) diegetic prompts (part of the narrative, e.g. \u201cOnce upon a time, I saw a fox...\u201d), and (2) non-diegetic prompts (external, e.g. \u201cWrite about the adventures of the fox.\u201d). With this lens, we study how 129 crowd workers on Prolific write short texts with different user interfaces (1 vs 3 suggestions, with/out non-diegetic prompts; implemented with GPT-3): When the interface offered multiple suggestions and provided an option for non-diegetic prompting, participants preferred choosing from multiple suggestions over controlling them via non-diegetic prompts. When participants provided non-diegetic prompts it was to ask for inspiration, topics or facts. Single suggestions in particular were guided both with diegetic and non-diegetic information. This work informs human-AI interaction with generative models by revealing that (1) writing non-diegetic prompts requires effort, (2) people combine diegetic and non-diegetic prompting, and (3) they use their draft (i.e. diegetic information) and suggestion timing to strategically guide LLMs.",
            "year": 2023,
            "citationCount": 30,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work informs human-AI interaction with generative models by revealing that (1) writing non-diegetic prompts requires effort, (2) people combine diegetic and non- diegetic prompting, and (3) they use their draft (i.e. diegetic information) and suggestion timing to strategically guide LLMs."
            },
            "score": 2
        },
        {
            "id": "3675ae068acb1bd60e9c880c763150253ad1daef",
            "paperId": "3675ae068acb1bd60e9c880c763150253ad1daef",
            "title": "Retrieval-Augmented Score Distillation for Text-to-3D Generation",
            "abstract": "Text-to-3D generation has achieved significant success by incorporating powerful 2D diffusion models, but insufficient 3D prior knowledge also leads to the inconsistency of 3D geometry. Recently, since large-scale multi-view datasets have been released, fine-tuning the diffusion model on the multi-view datasets becomes a mainstream to solve the 3D inconsistency problem. However, it has confronted with fundamental difficulties regarding the limited quality and diversity of 3D data, compared with 2D data. To sidestep these trade-offs, we explore a retrieval-augmented approach tailored for score distillation, dubbed RetDream. We postulate that both expressiveness of 2D diffusion models and geometric consistency of 3D assets can be fully leveraged by employing the semantically relevant assets directly within the optimization process. To this end, we introduce novel framework for retrieval-based quality enhancement in text-to-3D generation. We leverage the retrieved asset to incorporate its geometric prior in the variational objective and adapt the diffusion model's 2D prior toward view consistency, achieving drastic improvements in both geometry and fidelity of generated scenes. We conduct extensive experiments to demonstrate that RetDream exhibits superior quality with increased geometric consistency. Project page is available at https://ku-cvlab.github.io/RetDream/.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work postulates that both expressiveness of 2D diffusion models and geometric consistency of 3D assets can be fully leveraged by employing the semantically relevant assets directly within the optimization process, and introduces novel framework for retrieval-based quality enhancement in text-to-3D generation."
            },
            "score": 2
        },
        {
            "id": "a8ed2f4d8e8117d4f05d2cb85af7fafa7dee14bc",
            "paperId": "a8ed2f4d8e8117d4f05d2cb85af7fafa7dee14bc",
            "title": "Transferring Knowledge from Structure-aware Self-attention Language Model to Sequence-to-Sequence Semantic Parsing",
            "abstract": "Semantic parsing considers the task of mapping a natural language sentence into a target formal representation, where various sophisticated sequence-to-sequence (seq2seq) models have been applied with promising results. Generally, these target representations follow a syntax formalism that limits permitted forms. However, it is neither easy nor flexible to explicitly integrate this syntax formalism into a neural seq2seq model. In this paper, we present a structure-aware self-attention language model to capture structural information of target representations and propose a knowledge distillation based approach to incorporating the target language model into a seq2seq model, where grammar rules or sketches are not required in the training process. An ablation study shows that the proposed language model can notably improve the performance of the baseline model. The experiments show that our method achieves new state-of-the-art performance among neural approaches on four semantic parsing (ATIS, GEO) and Python code generation (Django, CoNaLa) tasks.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A structure-aware self-attention language model to capture structural information of target representations and a knowledge distillation based approach to incorporating the target language model into a seq2seq model, where grammar rules or sketches are not required in the training process are proposed."
            },
            "score": 2
        },
        {
            "id": "ada91d93c135b3b889f47b1ef1410a0b29ff4b03",
            "paperId": "ada91d93c135b3b889f47b1ef1410a0b29ff4b03",
            "title": "Enhancing Scene Graph Generation with Hierarchical Relationships and Commonsense Knowledge",
            "abstract": "This work presents an enhanced approach to generating scene graphs by incorporating a relationship hierarchy and commonsense knowledge. Specifically, we propose a Bayesian classification head that exploits an informative hierarchical structure. It jointly predicts the super-category or type of relationship between the two objects, along with the detailed relationship under each super-category. We design a commonsense validation pipeline that uses a large language model to critique the results from the scene graph prediction system and then use that feedback to enhance the model performance. The system requires no external large language model assistance at test time, making it more convenient for practical applications. Experiments on the Visual Genome and the OpenImage V6 datasets demonstrate that harnessing hierarchical relationships enhances the model performance by a large margin. The proposed Bayesian head can also be incorporated as a portable module in existing scene graph generation algorithms to improve their results. In addition, the commonsense validation enables the model to generate an extensive set of reasonable predictions beyond dataset annotations.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a Bayesian classification head that exploits an informative hierarchical structure that jointly predicts the super-category or type of relationship between the two objects, along with the detailed relationship under each super- category."
            },
            "score": 2
        },
        {
            "id": "119bea315698f5de7acd6a8049da72711006a2a3",
            "paperId": "119bea315698f5de7acd6a8049da72711006a2a3",
            "title": "PCROD: Context Aware Role based Offensive Detection using NLP/ DL Approaches",
            "abstract": "With the increased use of social media many people misuse online platforms by uploading offensive content and sharing the same with vast audience. Here comes controlling of such offensive contents. In this work we concentrate on the issue of finding offensive text in social media. Existing offensive text detection systems treat weak pejoratives like \u2018idiot\u2018 and extremely indecent pejoratives like \u2018f***\u2018 as same as offensive irrespective of formal and informal contexts . In fact the weakly pejoratives in informal discussions among friends are casual and common which are not offensive but the same can be offensive when expressed in formal discussions. Crucial challenges to accomplish the task of role based offensive detection in text are i) considering the roles while classifying the text as offensive or not i) creating a contextual datasets including both formal and informal roles. To tackle the above mentioned challenges we develop deep neural network based model known as context aware role based offensive detection(CROD). We examine CROD on the manually created dataset that is collected from social networking sites. Results show that CROD gives better performance with RoBERTa with an accuracy of 94% while considering the context and role in data specifics.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Deep neural network based model known as context aware role based offensive detection(CROD) is developed and results show that CROD gives better performance with RoBERTa with an accuracy of 94% while considering the context and role in data specifics."
            },
            "score": 2
        },
        {
            "id": "da8c3729c01efe017b7fb9189363c2b088c18647",
            "paperId": "da8c3729c01efe017b7fb9189363c2b088c18647",
            "title": "On the definition of toxicity in NLP",
            "abstract": "The fundamental problem in toxicity detection task lies in the fact that the toxicity is ill-defined. This causes us to rely on subjective and vague data in models' training, which results in non-robust and non-accurate results: garbage in - garbage out. This work suggests a new, stress-level-based definition of toxicity designed to be objective and context-aware. On par with it, we also describe possible ways of applying this new definition to dataset creation and model training.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work suggests a new, stress-level-based definition of toxicity designed to be objective and context-aware, and describes possible ways of applying this new definition to dataset creation and model training."
            },
            "score": 2
        },
        {
            "id": "52047b38532ccc94c3c88cd9ccea5666ecc40009",
            "paperId": "52047b38532ccc94c3c88cd9ccea5666ecc40009",
            "title": "An Efficient Transformer with Distance-aware Attention",
            "abstract": "In recent years, the transformer model has become one of the main highlights of advances in natural language processing (NLP). The attention mechanism of the transformer model makes it possible to track the relations between words across very long text sequences in both forward and reverse directions. However, the complexity of the attention mechanism is quadratic and introduces a performance bottleneck in the transformer. We propose a distance-aware attention mechanism which integrates the locality information by assigning different weights to query-key pairs according to the distance between the query and the key. By doing so, we in fact shrink the dimension of the matrix in the vector matrix multiplication and reduce the complexity of the attention to O(n2/m). Experiments show that the distance-aware attention is superior to or close to the original model and other variants in various NLP tasks.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A distance-aware attention mechanism which integrates the locality information by assigning different weights to query-key pairs according to the distance between the query and the key is proposed."
            },
            "score": 2
        },
        {
            "id": "c2e8afc58853914e87d48a9064152838a6c2ac46",
            "paperId": "c2e8afc58853914e87d48a9064152838a6c2ac46",
            "title": "Relation Classification based on Selective Entity-Aware Attention",
            "abstract": "Relation classification aims to classify the entity pairs into a certain relation, which is an important task of natural language processing. The latest end-to-end models based on attention mechanism still have shortcomings, i.e., the attention will be gradually weakened when processing long sequences, and they cannot make use of the hidden type information of the entities. To solve these problems, we propose a relation classification model based on the selective entity-aware attention mechanism, which consists of context encoder and entity-aware attention network. In the context encoder, contextual word semantics are learned through self-attention. Entity selection is applied to adapt the fact that different words can determine each other\u2019s importance. Latent types of entities are taken as auxiliary information to make full use of the entities\u2019 hidden features. Experiments on the SemEval-2010 Task 8 dataset and TACRED show that our model outperforms the baselines without implementing any external resources or NLP tools, and the entity-aware attention indeed improve the model\u2019s performance.",
            "year": 2022,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experiments show that the relation classification model based on the selective entity-aware attention mechanism outperforms the baselines without implementing any external resources or NLP tools, and the entity- aware attention indeed improve the model\u2019s performance."
            },
            "score": 2
        },
        {
            "id": "2961c78dea48c0626e2ff5ea5e500f63c6c973dd",
            "paperId": "2961c78dea48c0626e2ff5ea5e500f63c6c973dd",
            "title": "Neuralign: A Context-Aware, Cross-Lingual and Fully-Neural Sentence Alignment System for Long Texts",
            "abstract": "Sentence alignment \u2013 establishing links between corresponding sentences in two related documents \u2013 is an important NLP task with several downstream applications, such as machine translation (MT).Despite the fact that existing sentence alignment systems have achieved promising results, their effectiveness is based on auxiliary information such as document metadata or machine-generated translations, as well as hyperparameter-sensitive techniques. Moreover, these systems often overlook the crucial role that context plays in the alignment process.In this paper, we address the aforementioned issues and propose Neuralign: the first context-aware, end-to-end and fully-neural architecture for sentence alignment. Our system maps source and target sentences in long documents by contextualizing their sentence embeddings with respect to the other sentences in the document. We extensively evaluate Neuralign on a multilingual dataset consisting of 20 language pairs derived from the Opus project, and demonstrate that our model achieves state-of-the-art performance. To ensure reproducibility, we release our code and model checkpoints at https://github.com/Babelscape/Neuralign.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes Neuralign: the first context-aware, end-to-end and fully-neural architecture for sentence alignment, and extensively evaluates Neuralign on a multilingual dataset consisting of 20 language pairs derived from the Opus project, and demonstrates that the model achieves state-of-the-art performance."
            },
            "score": 2
        },
        {
            "id": "7cc2aac18db1493142240ae3982dce688e856f58",
            "paperId": "7cc2aac18db1493142240ae3982dce688e856f58",
            "title": "Discourse-Aware Prompt for Argument Impact Classification",
            "abstract": "Discourse information behind the arguments attracts a lot of attention from the field of Natural Language Processing (NLP) and computational argumentation. Durmus et al. [10] launched a new study on the influence of discourse contexts on determining argument impact. Argument Impact Classification is an intriguing but challenging task to classify whether the argumentative unit or an argument is impactful in a conversation. This paper empirically demonstrates that the discourse marker (e.g., \"for example,\" \"in other words\") can be represented by the learnable continuous prompt to align with discourse information existing in Pre-trained Language Model (PLM). This discourse information helps the Pre-trained Language Model understand the input template and elicit the discourse information to improve the performance on this task. Therefore, based on this intuition, we propose a prompt model DAPA and surpass the previous state-of-the-art model with a 2.5% F1 score.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper empirically demonstrates that the discourse marker can be represented by the learnable continuous prompt to align with discourse information existing in Pre-trained Language Model (PLM) and proposes a prompt model DAPA to surpass the previous state-of-the-art model."
            },
            "score": 2
        },
        {
            "id": "15cda7c4604c983fce3f037b14791e0fa629d355",
            "paperId": "15cda7c4604c983fce3f037b14791e0fa629d355",
            "title": "Forgery-aware Adaptive Transformer for Generalizable Synthetic Image Detection",
            "abstract": "In this paper, we study the problem of generalizable synthetic image detection, aiming to detect forgery images from diverse generative methods, e.g., GANs and diffusion models. Cutting-edge solutions start to explore the benefits of pre-trained models, and mainly follow the fixed paradigm of solely training an attached classifier, e.g., combining frozen CLIP-ViT with a learnable linear layer in UniFD. However, our analysis shows that such a fixed paradigm is prone to yield detectors with insufficient learning regarding forgery representations. We attribute the key challenge to the lack of forgery adaptation, and present a novel forgery-aware adaptive transformer approach, namely FatFormer. Based on the pre-trained vision-language spaces of CLIP, FatFormer introduces two core designs for the adaption to build generalized forgery representations. First, motivated by the fact that both image and frequency analysis are essential for synthetic image detection, we develop a forgery-aware adapter to adapt image features to discern and integrate local forgery traces within image and frequency domains. Second, we find that considering the contrastive objectives between adapted image features and text prompt embeddings, a previously overlooked aspect, results in a nontrivial generalization improvement. Accordingly, we introduce language-guided alignment to supervise the forgery adaptation with image and text prompts in FatFormer. Experiments show that, by coupling these two designs, our approach tuned on 4-class ProGAN data attains a remarkable detection performance, achieving an average of 98% accuracy to unseen GANs, and surprisingly generalizes to unseen diffusion models with 95% accuracy.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A forgery-aware adapter to adapt image features to discern and integrate local forgery traces within image and frequency domains is developed, and considering the contrastive objectives between adapted image features and text prompt embeddings results in a nontrivial generalization improvement."
            },
            "score": 2
        },
        {
            "id": "ddd49e6fae7c8e081b76865a841d217008b8f3b9",
            "paperId": "ddd49e6fae7c8e081b76865a841d217008b8f3b9",
            "title": "Fill in the Blank: Context-aware Automated Text Input Generation for Mobile GUI Testing",
            "abstract": "Automated GUI testing is widely used to help ensure the quality of mobile apps. However, many GUIs require appropriate text inputs to proceed to the next page, which remains a prominent obstacle for testing coverage. Considering the diversity and semantic requirement of valid inputs (e.g., flight departure, movie name), it is challenging to automate the text input generation. Inspired by the fact that the pre-trained Large Language Model (LLM) has made outstanding progress in text generation, we propose an approach named QTypist based on LLM for intelligently generating semantic input text according to the GUI context. To boost the performance of LLM in the mobile testing scenario, we develop a prompt-based data construction and tuning method which automatically extracts the prompts and answers for model tuning. We evaluate QTypist on 106 apps from Google Play, and the result shows that the passing rate of QTypist is 87%, which is 93% higher than the best baseline. We also integrate QTypist with the automated GUI testing tools and it can cover 42% more app activities, 52% more pages, and subsequently help reveal 122% more bugs compared with the raw tool.",
            "year": 2022,
            "citationCount": 35,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes an approach named QTypist based on LLM for intelligently generating semantic input text according to the GUI context which can cover 42% more app activities, 52% more pages, and subsequently help reveal 122% more bugs compared with the raw tool."
            },
            "score": 2
        },
        {
            "id": "e441f22769447578177b956e8f064dc426f91752",
            "paperId": "e441f22769447578177b956e8f064dc426f91752",
            "title": "Edit-A-Video: Single Video Editing with Object-Aware Consistency",
            "abstract": "Despite the fact that text-to-video (TTV) model has recently achieved remarkable success, there have been few approaches on TTV for its extension to video editing. Motivated by approaches on TTV models adapting from diffusion-based text-to-image (TTI) models, we suggest the video editing framework given only a pretrained TTI model and a singlepair, which we term Edit-A-Video. The framework consists of two stages: (1) inflating the 2D model into the 3D model by appending temporal modules and tuning on the source video (2) inverting the source video into the noise and editing with target text prompt and attention map injection. Each stage enables the temporal modeling and preservation of semantic attributes of the source video. One of the key challenges for video editing include a background inconsistency problem, where the regions not included for the edit suffer from undesirable and inconsistent temporal alterations. To mitigate this issue, we also introduce a novel mask blending method, termed as sparse-causal blending (SC Blending). We improve previous mask blending methods to reflect the temporal consistency so that the area where the editing is applied exhibits smooth transition while also achieving spatio-temporal consistency of the unedited regions. We present extensive experimental results over various types of text and videos, and demonstrate the superiority of the proposed method compared to baselines in terms of background consistency, text alignment, and video editing quality.",
            "year": 2023,
            "citationCount": 33,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel mask blending method, termed as sparse-causal blending (SC Blending), is introduced to reflect the temporal consistency so that the area where the editing is applied exhibits smooth transition while also achieving spatio-temporal consistency of the unedited regions."
            },
            "score": 1
        },
        {
            "id": "5db270f86b42193812f7cecb943167ce7aa45aaa",
            "paperId": "5db270f86b42193812f7cecb943167ce7aa45aaa",
            "title": "PrefixMol: Target- and Chemistry-aware Molecule Design via Prefix Embedding",
            "abstract": "Is there a unified model for generating molecules considering different conditions, such as binding pockets and chemical properties? Although target-aware generative models have made significant advances in drug design, they do not consider chemistry conditions and cannot guarantee the desired chemical properties. Unfortunately, merging the target-aware and chemical-aware models into a unified model to meet customized requirements may lead to the problem of negative transfer. Inspired by the success of multi-task learning in the NLP area, we use prefix embeddings to provide a novel generative model that considers both the targeted pocket's circumstances and a variety of chemical properties. All conditional information is represented as learnable features, which the generative model subsequently employs as a contextual prompt. Experiments show that our model exhibits good controllability in both single and multi-conditional molecular generation. The controllability enables us to outperform previous structure-based drug design methods. More interestingly, we open up the attention mechanism and reveal coupling relationships between conditions, providing guidance for multi-conditional molecule generation.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Inspired by the success of multi-task learning in the NLP area, prefix embeddings are used to provide a novel generative model that considers both the targeted pocket's circumstances and a variety of chemical properties, providing guidance for multi-conditional molecule generation."
            },
            "score": 1
        }
    ],
    "novelty": "yes"
}