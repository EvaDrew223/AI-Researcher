{
    "topic_description": "novel prompting methods that can improve factuality and reduce hallucination of large language models",
    "idea_name": "Meta-Cognitive Prompting",
    "raw_idea": {
        "Problem": "Large language models often generate overconfident responses that lack self-awareness and meta-cognitive reasoning, leading to hallucinations and factual inconsistencies.",
        "Existing Methods": "Existing methods for improving the meta-cognitive abilities of language models mainly focus on incorporating human feedback and using reinforcement learning.",
        "Motivation": "We observe that humans are able to monitor their own thought processes and regulate their behavior based on meta-cognitive reasoning. We hypothesize that by explicitly modeling the meta-cognitive processes in the prompt, we can guide the model to generate more self-aware and calibrated responses.",
        "Proposed Method": "We propose Meta-Cognitive Prompting (MCP), a novel prompting method that incorporates meta-cognitive reasoning into the generation process. MCP consists of three main steps: 1) Meta-Cognitive Query Generation: Given an input prompt, we first generate a set of meta-cognitive queries that probe the model's understanding of its own knowledge and reasoning process, such as \"What do you know about this topic?\" and \"How confident are you in your response?\" 2) Meta-Cognitive Prompt Augmentation: We then augment the original prompt with the generated meta-cognitive queries, and ask the model to generate responses that are self-aware and calibrated. 3) Meta-Cognitive Reflection: After generating the initial response, we ask the model to reflect on its own response and generate a meta-cognitive assessment, such as \"I'm not very confident in this response because I don't have enough information about the topic.\" We then use this assessment to guide the model to revise its response if needed.",
        "Experiment Plan": "We plan to evaluate MCP on a range of tasks that require meta-cognitive reasoning and self-awareness, such as open-domain question answering, language model calibration, and conversational agents. We will compare MCP with state-of-the-art baselines, including methods that incorporate human feedback and reinforcement learning. We will also conduct human evaluation to assess the self-awareness and calibration of the generated responses."
    },
    "full_experiment_plan": {
        "Title": "Meta-Cognitive Prompting: Improving Language Model Factuality and Reducing Hallucination through Self-Awareness and Reflection",
        "Problem Statement": "Large language models often generate overconfident responses that lack self-awareness and meta-cognitive reasoning, leading to hallucinations and factual inconsistencies.",
        "Motivation": "Existing methods for improving the meta-cognitive abilities of language models mainly focus on incorporating human feedback and using reinforcement learning. However, these approaches can be time-consuming, expensive, and may not always be feasible. We observe that humans are able to monitor their own thought processes and regulate their behavior based on meta-cognitive reasoning. We hypothesize that by explicitly modeling the meta-cognitive processes in the prompt, we can guide the model to generate more self-aware and calibrated responses.",
        "Proposed Method": "We propose Meta-Cognitive Prompting (MCP), a novel prompting method that incorporates meta-cognitive reasoning into the generation process. MCP consists of three main steps:\n1. Meta-Cognitive Query Generation: Given an input prompt, we first generate a set of meta-cognitive queries that probe the model's understanding of its own knowledge and reasoning process, such as \"What do you know about this topic?\" and \"How confident are you in your response?\"\n2. Meta-Cognitive Prompt Augmentation: We then augment the original prompt with the generated meta-cognitive queries, and ask the model to generate responses that are self-aware and calibrated.\n3. Meta-Cognitive Reflection: After generating the initial response, we ask the model to reflect on its own response and generate a meta-cognitive assessment, such as \"I'm not very confident in this response because I don't have enough information about the topic.\" We then use this assessment to guide the model to revise its response if needed.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We evaluate MCP on a range of tasks that require meta-cognitive reasoning and self-awareness, such as open-domain question answering (Natural Questions, TriviaQA), language model calibration (MMLU), and conversational agents (Wizard of Wikipedia, BlendedSkillTalk).",
            "Step 2: Construct Prompts": "For each task, we construct the following prompts:\n1. Baseline: The original input prompt without any meta-cognitive queries.\n2. MCP-Query: The input prompt augmented with meta-cognitive queries generated by the model.\n3. MCP-Reflect: The input prompt augmented with both meta-cognitive queries and reflections generated by the model.\nFor the meta-cognitive query generation, we use the following prompt:\n\"Given the following input prompt, generate 3 meta-cognitive queries that probe your understanding and reasoning process:\\n[INPUT_PROMPT]\\n\\nMeta-Cognitive Queries:\\n1.\"\nFor the meta-cognitive reflection, we use the following prompt:\n\"Reflect on your response and assess your confidence and reasoning:\\n[RESPONSE]\\n\\nMeta-Cognitive Reflection:\"",
            "Step 3: Select Models": "We evaluate MCP on GPT-3.5 (text-davinci-003), GPT-4, and ChatGPT.",
            "Step 4: Get Results": "For each task and model combination, we generate responses using the baseline, MCP-Query, and MCP-Reflect prompts. We evaluate the responses using both automatic metrics (e.g., F1 score, accuracy, perplexity) and human evaluation (e.g., factuality, consistency, calibration).",
            "Step 5: Analyze Results": "We compare the performance of MCP-Query and MCP-Reflect with the baseline to assess the effectiveness of meta-cognitive prompting. We also analyze the generated meta-cognitive queries and reflections to gain insights into the model's reasoning process and self-awareness."
        },
        "Test Case Examples": {
            "Test Case 1": {
                "Input": "What is the capital of France?",
                "Baseline Output": "The capital of France is Paris.",
                "MCP-Query Output": "Meta-Cognitive Queries:\n1. What do I know about the geography and political system of France?\n2. How confident am I in my knowledge of French cities?\n3. Is there any additional information I need to answer this question accurately?\n\nResponse: Based on my knowledge of French geography and politics, I am quite confident that the capital of France is Paris. Paris is the largest city in France and has been the country's capital since the 12th century. It is also the seat of the French government and the official residence of the President of France. I don't believe I need any additional information to answer this question accurately.",
                "MCP-Reflect Output": "Meta-Cognitive Reflection: I am very confident in my response that Paris is the capital of France. This is a well-known fact that I have encountered many times before, and I don't have any doubts about its accuracy. My reasoning is based on my knowledge of French geography, history, and politics, which all point to Paris being the capital city. I don't feel the need to revise my response."
            },
            "Test Case 2": {
                "Input": "What is the largest planet in our solar system?",
                "Baseline Output": "The largest planet in our solar system is Jupiter.",
                "MCP-Query Output": "Meta-Cognitive Queries:\n1. How familiar am I with the planets in our solar system and their relative sizes?\n2. Do I have any doubts or uncertainties about which planet is the largest?\n3. Can I think of any evidence or facts to support my answer?\n\nResponse: I am fairly knowledgeable about the planets in our solar system and their sizes relative to each other. I believe that Jupiter is the largest planet, but I have a slight uncertainty because I can't remember the exact sizes of all the planets. To support my answer, I know that Jupiter is a gas giant and is much larger than the inner rocky planets like Earth and Mars. However, I'm not entirely confident in my response and feel I may need to double-check some facts about planetary sizes to be sure.",
                "MCP-Reflect Output": "Meta-Cognitive Reflection: Upon reflection, I realize that I am not as confident in my response as I initially thought. While I do believe that Jupiter is likely the largest planet, I have some doubts because I can't recall the precise sizes of the other gas giants like Saturn, Uranus, and Neptune. My reasoning is based more on a general sense that Jupiter is very large, rather than specific evidence. To improve my response, I should look up the diameters or masses of the planets to confirm that Jupiter is indeed the largest. I may need to revise my response if I find contradictory information."
            }
        },
        "Fallback Plan": "If the proposed Meta-Cognitive Prompting method does not significantly improve the factuality and reduce the hallucination of the language models compared to the baselines, we can consider the following alternative approaches:\n1. Analyze the generated meta-cognitive queries and reflections to identify potential issues, such as irrelevant or superficial questions and assessments. Based on this analysis, refine the prompts for generating meta-cognitive queries and reflections to elicit more meaningful and informative responses.\n2. Experiment with different prompt formats and wordings to optimize the effectiveness of the meta-cognitive prompts. For example, try using more specific or targeted questions, or provide examples of high-quality meta-cognitive queries and reflections to guide the model.\n3. Investigate the impact of different model sizes and architectures on the effectiveness of Meta-Cognitive Prompting. It's possible that certain models may be more amenable to this approach than others.\n4. Conduct a more in-depth error analysis to understand the types of mistakes and hallucinations that the models are making, even with Meta-Cognitive Prompting. This can provide insights into the limitations of the approach and potential areas for improvement.\n5. Consider combining Meta-Cognitive Prompting with other techniques, such as retrieval-augmented generation or adversarial filtering, to further improve the factuality and reduce hallucination.\nIf the Meta-Cognitive Prompting approach does not yield the desired results after these modifications, we can pivot the project to focus on analyzing the limitations and challenges of improving meta-cognitive reasoning in language models. This can involve conducting more extensive experiments to understand the factors that influence the model's self-awareness and calibration, and proposing new research directions based on the insights gained from this analysis."
    },
    "novelty_queries": [
        "KeywordQuery(\"meta-cognitive prompting language models\")",
        "KeywordQuery(\"self-awareness reflection language models\")",
        "KeywordQuery(\"reducing hallucination language models\")",
        "KeywordQuery(\"improving factuality language models\")",
        "KeywordQuery(\"Meta-Cognitive Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "cd2e04598909158494e556823d9de8baa692cee2",
            "paperId": "cd2e04598909158494e556823d9de8baa692cee2",
            "title": "Towards Mitigating Hallucination in Large Language Models via Self-Reflection",
            "abstract": "Large language models (LLMs) have shown promise for generative and knowledge-intensive tasks including question-answering (QA) tasks. However, the practical deployment still faces challenges, notably the issue of\"hallucination\", where models generate plausible-sounding but unfaithful or nonsensical information. This issue becomes particularly critical in the medical domain due to the uncommon professional concepts and potential social risks involved. This paper analyses the phenomenon of hallucination in medical generative QA systems using widely adopted LLMs and datasets. Our investigation centers on the identification and comprehension of common problematic answers, with a specific emphasis on hallucination. To tackle this challenge, we present an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation. Through this feedback process, our approach steadily enhances the factuality, consistency, and entailment of the generated answers. Consequently, we harness the interactivity and multitasking ability of LLMs and produce progressively more precise and accurate answers. Experimental results on both automatic and human evaluation demonstrate the superiority of our approach in hallucination reduction compared to baselines.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper analyses the phenomenon of hallucination in medical generative QA systems using widely adopted LLMs and datasets and presents an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation that steadily enhances the factuality, consistency, and entailment of the generated answers."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving language model factuality and reducing hallucination through self-awareness and reflection, while the approach is Meta-Cognitive Prompting (MCP) which incorporates meta-cognitive reasoning into the generation process. The research problem in the paper is also mitigating hallucination in large language models, and the approach is an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation.\n\nThe proposal and paper are both about reducing hallucination in language models through self-reflection and meta-cognitive reasoning, although the specific methods (Meta-Cognitive Prompting vs. interactive self-reflection) differ in details.\n\nYes",
            "novelty_judgment": "yes"
        },
        {
            "id": "32c5b515cab893e5e4bf3f90c8b6c8262bd7ac09",
            "paperId": "32c5b515cab893e5e4bf3f90c8b6c8262bd7ac09",
            "title": "Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation",
            "abstract": "Despite showing increasingly human-like abilities, large language models (LLMs) often struggle with factual inaccuracies, i.e.\"hallucinations\", even when they hold relevant knowledge. To address these hallucinations, current approaches typically necessitate high-quality human factuality annotations. In this work, we explore Self-Alignment for Factuality, where we leverage the self-evaluation capability of an LLM to provide training signals that steer the model towards factuality. Specifically, we incorporate Self-Eval, a self-evaluation component, to prompt an LLM to validate the factuality of its own generated responses solely based on its internal knowledge. Additionally, we design Self-Knowledge Tuning (SK-Tuning) to augment the LLM's self-evaluation ability by improving the model's confidence estimation and calibration. We then utilize these self-annotated responses to fine-tune the model via Direct Preference Optimization algorithm. We show that the proposed self-alignment approach substantially enhances factual accuracy over Llama family models across three key knowledge-intensive tasks on TruthfulQA and BioGEN.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The proposed self-alignment approach substantially enhances factual accuracy over Llama family models across three key knowledge-intensive tasks on TruthfulQA and BioGEN."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving language model factuality and reducing hallucination through self-awareness and reflection, while the approach is Meta-Cognitive Prompting (MCP) which incorporates meta-cognitive reasoning into the generation process.\n\nThe research problem in the paper is also mitigating hallucinations in LLMs to improve factuality, and the approach is Self-Alignment for Factuality which leverages the self-evaluation capability of an LLM to provide training signals that steer the model towards factuality.\n\nBoth the proposal and the paper aim to address the problem of factual inaccuracies and hallucinations in language models, and both propose to use the model's self-evaluation and self-awareness capabilities to improve factuality. Although the specific methods (MCP vs. Self-Alignment) differ, the high-level research problem and approach are directly relevant.\n\nYes",
            "novelty_judgment": "yes"
        },
        {
            "id": "59eeb8a259ef2a9c981868470480f53b67854060",
            "paperId": "59eeb8a259ef2a9c981868470480f53b67854060",
            "title": "Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding",
            "abstract": "We introduce meta-prompting, an effective scaffolding technique designed to enhance the functionality of language models (LMs). This approach transforms a single LM into a multi-faceted conductor, adept at managing and integrating multiple independent LM queries. By employing high-level instructions, meta-prompting guides the LM to break down complex tasks into smaller, more manageable subtasks. These subtasks are then handled by distinct\"expert\"instances of the same LM, each operating under specific, tailored instructions. Central to this process is the LM itself, in its role as the conductor, which ensures seamless communication and effective integration of the outputs from these expert models. It additionally employs its inherent critical thinking and robust verification processes to refine and authenticate the end result. This collaborative prompting approach empowers a single LM to simultaneously act as a comprehensive orchestrator and a panel of diverse experts, significantly enhancing its performance across a wide array of tasks. The zero-shot, task-agnostic nature of meta-prompting greatly simplifies user interaction by obviating the need for detailed, task-specific instructions. Furthermore, our research demonstrates the seamless integration of external tools, such as a Python interpreter, into the meta-prompting framework, thereby broadening its applicability and utility. Through rigorous experimentation with GPT-4, we establish the superiority of meta-prompting over conventional scaffolding methods: When averaged across all tasks, including the Game of 24, Checkmate-in-One, and Python Programming Puzzles, meta-prompting, augmented with a Python interpreter functionality, surpasses standard prompting by 17.1%, expert (dynamic) prompting by 17.3%, and multipersona prompting by 15.2%.",
            "year": 2024,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Meta-prompting is introduced, an effective scaffolding technique designed to enhance the functionality of language models (LMs) and the seamless integration of external tools, such as a Python interpreter, into the meta-prompting framework, thereby broadening its applicability and utility."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving language model factuality and reducing hallucination through meta-cognitive prompting, while the paper focuses on enhancing language model functionality through task-agnostic scaffolding and integrating multiple LM queries. The proposal aims to guide the model to generate more self-aware and calibrated responses, while the paper aims to transform a single LM into a multi-faceted conductor that manages and integrates multiple independent LM queries.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "f48b95576bd50a69937d8bbff0cc42bdb20e49f5",
            "paperId": "f48b95576bd50a69937d8bbff0cc42bdb20e49f5",
            "title": "MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception",
            "abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated exceptional capabilities in visual perception and understanding. However, these models also suffer from hallucinations, which limit their reliability as AI systems. We believe that these hallucinations are partially due to the models' struggle with understanding what they can and cannot perceive from images, a capability we refer to as self-awareness in perception. Despite its importance, this aspect of MLLMs has been overlooked in prior studies. In this paper, we aim to define and evaluate the self-awareness of MLLMs in perception. To do this, we first introduce the knowledge quadrant in perception, which helps define what MLLMs know and do not know about images. Using this framework, we propose a novel benchmark, the Self-Awareness in Perception for MLLMs (MM-SAP), specifically designed to assess this capability. We apply MM-SAP to a variety of popular MLLMs, offering a comprehensive analysis of their self-awareness and providing detailed insights. The experiment results reveal that current MLLMs possess limited self-awareness capabilities, pointing to a crucial area for future advancement in the development of trustworthy MLLMs. Code and data are available at https://github.com/YHWmz/MM-SAP.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel benchmark, the Self-Awareness in Perception for MLLMs (MM-SAP), is proposed, specifically designed to assess this capability, and applied to a variety of popular MLLMs, offering a comprehensive analysis of their self-awareness and providing detailed insights."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving language model factuality and reducing hallucination through meta-cognitive prompting. The approach involves generating meta-cognitive queries and reflections to guide the model to generate more self-aware and calibrated responses.\n\nThe research problem in the paper is assessing the self-awareness of multimodal language models in perception. The approach involves defining a knowledge quadrant framework and proposing a benchmark to evaluate the models' understanding of what they can and cannot perceive from images.\n\nWhile both works aim to address the issue of hallucination in language models, the proposal focuses on improving factuality through meta-cognitive prompting, while the paper focuses on assessing self-awareness in perception using a benchmark. The modalities and methods used are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "ddbd8fe782ac98e9c64dd98710687a962195dd9b",
            "paperId": "ddbd8fe782ac98e9c64dd98710687a962195dd9b",
            "title": "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",
            "abstract": "Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.",
            "year": 2023,
            "citationCount": 98,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) is introduced that enhances an LM's quality and factuality through retrieval and self-reflection and shows significant gains in improving factuality and citation accuracy for long-form generations relative to state-of-the-art models."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve language model factuality and reduce hallucination through meta-cognitive prompting, which involves generating self-aware queries and reflections to guide the model's responses. The paper, on the other hand, proposes a framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection using special tokens called reflection tokens.\n\nWhile both the project proposal and the paper focus on improving language model factuality, their approaches differ. The project proposal uses meta-cognitive prompting to generate self-aware queries and reflections, whereas the paper uses retrieval-augmented generation and reflection tokens to enhance the model's performance.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "6489640b1d30a8a3e7cb906bb6557f1ccd0d799d",
            "paperId": "6489640b1d30a8a3e7cb906bb6557f1ccd0d799d",
            "title": "Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models",
            "abstract": "Retrieval-augmented language models (RALMs) represent a substantial advancement in the capabilities of large language models, notably in reducing factual hallucination by leveraging external knowledge sources. However, the reliability of the retrieved information is not always guaranteed. The retrieval of irrelevant data can lead to misguided responses, and potentially causing the model to overlook its inherent knowledge, even when it possesses adequate information to address the query. Moreover, standard RALMs often struggle to assess whether they possess adequate knowledge, both intrinsic and retrieved, to provide an accurate answer. In situations where knowledge is lacking, these systems should ideally respond with\"unknown\"when the answer is unattainable. In response to these challenges, we introduces Chain-of-Noting (CoN), a novel approach aimed at improving the robustness of RALMs in facing noisy, irrelevant documents and in handling unknown scenarios. The core idea of CoN is to generate sequential reading notes for retrieved documents, enabling a thorough evaluation of their relevance to the given question and integrating this information to formulate the final answer. We employed ChatGPT to create training data for CoN, which was subsequently trained on an LLaMa-2 7B model. Our experiments across four open-domain QA benchmarks show that RALMs equipped with CoN significantly outperform standard RALMs. Notably, CoN achieves an average improvement of +7.9 in EM score given entirely noisy retrieved documents and +10.5 in rejection rates for real-time questions that fall outside the pre-training knowledge scope.",
            "year": 2023,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Chain-of-Noting (CoN) is introduced, a novel approach aimed at improving the robustness of RALMs in facing noisy, irrelevant documents and in handling unknown scenarios, and achieves an average improvement of +7.9 in EM score given entirely noisy retrieved documents and +10.5 in rejection rates for real-time questions that fall outside the pre-training knowledge scope."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving language model factuality and reducing hallucination through meta-cognitive prompting, while the paper focuses on enhancing the robustness of retrieval-augmented language models in handling noisy documents and unknown scenarios.\n\nThe proposed approach in the paper is Chain-of-Noting (CoN), which generates sequential reading notes for retrieved documents to evaluate their relevance and integrate the information for the final answer. In contrast, the proposal suggests Meta-Cognitive Prompting (MCP), which incorporates meta-cognitive queries and reflections into the generation process to guide the model towards more self-aware and calibrated responses.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "03764434729b83d4f04a8bd02f99f2500cd5bbae",
            "paperId": "03764434729b83d4f04a8bd02f99f2500cd5bbae",
            "title": "Teaching Language Models to Hallucinate Less with Synthetic Tasks",
            "abstract": "Large language models (LLMs) frequently hallucinate on abstractive summarization tasks such as document-based question-answering, meeting summarization, and clinical report generation, even though all necessary information is included in context. However, optimizing LLMs to hallucinate less on these tasks is challenging, as hallucination is hard to efficiently evaluate at each optimization step. In this work, we show that reducing hallucination on a synthetic task can also reduce hallucination on real-world downstream tasks. Our method, SynTra, first designs a synthetic task where hallucinations are easy to elicit and measure. It next optimizes the LLM's system message via prefix-tuning on the synthetic task, and finally transfers the system message to realistic, hard-to-optimize tasks. Across three realistic abstractive summarization tasks, SynTra reduces hallucination for two 13B-parameter LLMs using only a synthetic retrieval task for supervision. We also find that optimizing the system message rather than the model weights can be critical; fine-tuning the entire model on the synthetic task can counterintuitively increase hallucination. Overall, SynTra demonstrates that the extra flexibility of working with synthetic data can help mitigate undesired behaviors in practice.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Across three realistic abstractive summarization tasks, SynTra reduces hallucination for two 13B-parameter LLMs using only a synthetic retrieval task for supervision, demonstrating that the extra flexibility of working with synthetic data can help mitigate undesired behaviors in practice."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving language model factuality and reducing hallucination through meta-cognitive prompting, while the approach in the paper is reducing hallucination on real-world tasks by optimizing the model on a synthetic task where hallucinations are easy to measure.\n\nProposal: Improving language model factuality and reducing hallucination through self-awareness and reflection using meta-cognitive prompting.\nPaper: Reducing hallucination on real-world tasks by optimizing the model on a synthetic task where hallucinations are easy to measure.\n\nThe key difference is that the proposal focuses on using meta-cognitive prompting to improve factuality, while the paper focuses on using synthetic tasks to optimize the model and transfer the learned behavior to real-world tasks.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "696bc5ba0d023822bbee6b878a71ea2e4a4b0e5a",
            "paperId": "696bc5ba0d023822bbee6b878a71ea2e4a4b0e5a",
            "title": "N-Critics: Self-Refinement of Large Language Models with Ensemble of Critics",
            "abstract": "We propose a self-correction mechanism for Large Language Models (LLMs) to mitigate issues such as toxicity and fact hallucination. This method involves refining model outputs through an ensemble of critics and the model's own feedback. Drawing inspiration from human behavior, we explore whether LLMs can emulate the self-correction process observed in humans who often engage in self-reflection and seek input from others to refine their understanding of complex topics. Our approach is model-agnostic and can be applied across various domains to enhance trustworthiness by addressing fairness, bias, and robustness concerns. We consistently observe performance improvements in LLMs for reducing toxicity and correcting factual errors.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a self-correction mechanism for Large Language Models (LLMs) to mitigate issues such as toxicity and fact hallucination and consistently observe performance improvements in LLMs for reducing toxicity and correcting factual errors."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving language model factuality and reducing hallucination through self-awareness and reflection, while the approach is meta-cognitive prompting. The research problem in the paper is mitigating issues such as toxicity and fact hallucination in language models, and the approach is refining model outputs through an ensemble of critics and the model's own feedback.\n\nAlthough both works aim to address the issue of factual inconsistencies and hallucinations in language models, the proposed approaches differ. The proposal focuses on incorporating meta-cognitive reasoning into the prompting process, while the paper explores a self-correction mechanism using an ensemble of critics and the model's own feedback.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "e5c72b92c48d68594b290c84a8904da7c8335554",
            "paperId": "e5c72b92c48d68594b290c84a8904da7c8335554",
            "title": "Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback",
            "abstract": "Large language models (LLMs), such as ChatGPT, are able to generate human-like, fluent responses for many downstream tasks, e.g., task-oriented dialog and question answering. However, applying LLMs to real-world, mission-critical applications remains challenging mainly due to their tendency to generate hallucinations and their inability to use external knowledge. This paper proposes a LLM-Augmenter system, which augments a black-box LLM with a set of plug-and-play modules. Our system makes the LLM generate responses grounded in external knowledge, e.g., stored in task-specific databases. It also iteratively revises LLM prompts to improve model responses using feedback generated by utility functions, e.g., the factuality score of a LLM-generated response. The effectiveness of LLM-Augmenter is empirically validated on two types of scenarios, task-oriented dialog and open-domain question answering. LLM-Augmenter significantly reduces ChatGPT's hallucinations without sacrificing the fluency and informativeness of its responses. We make the source code and models publicly available.",
            "year": 2023,
            "citationCount": 222,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A LLM-Augmenter system, which augments a black-box LLM with a set of plug-and-play modules to significantly reduces ChatGPT's hallucinations without sacrificing the fluency and informativeness of its responses."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving language model factuality and reducing hallucination through meta-cognitive prompting, while the paper aims to address the same problem using external knowledge and automated feedback.\n\nThe proposed approach in the paper is to augment a black-box LLM with plug-and-play modules for grounding responses in external knowledge and iteratively revising prompts using utility function feedback. In contrast, the proposal suggests using meta-cognitive prompting to generate self-aware and reflective responses.\n\nWhile both the paper and the proposal tackle the issue of improving language model factuality and reducing hallucination, their approaches differ significantly. The paper relies on external knowledge and automated feedback, whereas the proposal focuses on meta-cognitive reasoning within the model itself.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "16f55d843b1aed18cab7464580f229c07f9bd188",
            "paperId": "16f55d843b1aed18cab7464580f229c07f9bd188",
            "title": "Prompting Techniques for Reducing Social Bias in LLMs through System 1 and System 2 Cognitive Processes",
            "abstract": "Dual process theory posits that human cognition arises via two systems. System 1, which is a quick, emotional, and intuitive process, which is subject to cognitive biases, and System 2, a slow, onerous, and deliberate process. NLP researchers often compare zero-shot prompting in LLMs to System 1 reasoning and chain-of-thought (CoT) prompting to System 2. In line with this interpretation, prior research has found that using CoT prompting in LLMs leads to reduced gender bias. We investigate the relationship between bias, CoT prompting, and dual process theory in LLMs directly. We compare zero-shot, CoT, and a variety of dual process theory-based prompting strategies on two bias datasets spanning nine different social bias categories. We also use human and machine personas to determine whether the effects of dual process theory in LLMs are based on modeling human cognition or inherent to the system. We find that a human persona, System 2, and CoT prompting all tend to reduce social biases in LLMs, though the best combination of features depends on the exact model and bias category -- resulting in up to a 13 percent drop in stereotypical judgments by an LLM.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that a human persona, System 2, and CoT prompting all tend to reduce social biases in LLMs, though the best combination of features depends on the exact model and bias category -- resulting in up to a 13 percent drop in stereotypical judgments by an LLM."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving language model factuality and reducing hallucination through meta-cognitive prompting, while the paper focuses on reducing social biases in language models using prompting techniques based on dual process theory. Although both involve prompting methods to improve certain aspects of language models, the specific problems (factuality vs. social bias) and approaches (meta-cognitive prompting vs. dual process theory-based prompting) are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "21f20493959db16995f971c90548809642c55d25",
            "paperId": "21f20493959db16995f971c90548809642c55d25",
            "title": "Meta-Cognitive Analysis: Evaluating Declarative and Procedural Knowledge in Datasets and Large Language Models",
            "abstract": "Declarative knowledge and procedural knowledge are two key parts in meta-cognitive theory, and these two hold significant importance in pre-training and inference of LLMs. However, a comprehensive analysis comparing these two types of knowledge is lacking, primarily due to challenges in definition, probing and quantitative assessment. In this paper, we explore from a new perspective by providing ground-truth knowledge for LLMs and evaluating the effective score. Through extensive experiments with widely-used datasets and models, we get conclusions: (1) In most tasks, benefits from declarative knowledge are greater than those from procedural knowledge. (2) Profits of procedural knowledge are larger than declarative knowledge only in reasoning tasks with simple logic. (3) As pre-training progresses and size increases, model ability to utilize both kinds of knowledge significantly improves, but in different speed. We do detailed analysis for the findings and this can provide primary guidance for evaluation and enhancement of large language models.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper provides ground-truth knowledge for LLMs and evaluates the effective score, finding that in most tasks, benefits from declarative knowledge are greater than those from procedural knowledge."
            },
            "score": 5
        },
        {
            "id": "46299fee72ca833337b3882ae1d8316f44b32b3c",
            "paperId": "46299fee72ca833337b3882ae1d8316f44b32b3c",
            "title": "Reflexion: an autonomous agent with dynamic memory and self-reflection",
            "abstract": "Recent advancements in decision-making large language model (LLM) agents have demonstrated impressive performance across various benchmarks. However, these state-of-the-art approaches typically necessitate internal model fine-tuning, external model fine-tuning, or policy optimization over a defined state space. Implementing these methods can prove challenging due to the scarcity of high-quality training data or the lack of well-defined state space. Moreover, these agents do not possess certain qualities inherent to human decision-making processes, specifically the ability to learn from mistakes. Self-reflection allows humans to efficiently solve novel problems through a process of trial and error. Building on recent research, we propose Reflexion, an approach that endows an agent with dynamic memory and self-reflection capabilities to enhance its existing reasoning trace and task-specific action choice abilities. To achieve full automation, we introduce a straightforward yet effective heuristic that enables the agent to pinpoint hallucination instances, avoid repetition in action sequences, and, in some environments, construct an internal memory map of the given environment. To assess our approach, we evaluate the agent\u2019s ability to complete decision-making tasks in AlfWorld environments and knowledge-intensive, search-based question-and-answer tasks in HotPotQA environments. We observe success rates of 97% and 51%, respectively, and provide a discussion on the emergent property of self-reflection.",
            "year": 2023,
            "citationCount": 166,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 5
        },
        {
            "id": "798963674902f741c3ea9298403eb8384c099a42",
            "paperId": "798963674902f741c3ea9298403eb8384c099a42",
            "title": "Factored Verification: Detecting and Reducing Hallucination in Summaries of Academic Papers",
            "abstract": "Hallucination plagues even frontier LLMs--but how bad is it really for summarizing academic papers? We evaluate Factored Verification, a simple automated method for detecting hallucinations in abstractive summaries. This method sets a new SotA on hallucination detection in the summarization task of the HaluEval benchmark, achieving 76.2% accuracy. We then use this method to estimate how often language models hallucinate when summarizing across multiple academic papers and find 0.62 hallucinations in the average ChatGPT (16k) summary, 0.84 for GPT-4, and 1.55 for Claude 2. We ask models to self-correct using Factored Critiques and find that this lowers the number of hallucinations to 0.49 for ChatGPT, 0.46 for GPT-4, and 0.95 for Claude 2. The hallucinations we find are often subtle, so we advise caution when using models to synthesize academic papers.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Factored Verification, a simple automated method for detecting hallucinations in abstractive summaries, is evaluated and sets a new SotA on hallucination detection in the summarization task of the HaluEval benchmark, achieving 76.2% accuracy."
            },
            "score": 5
        },
        {
            "id": "79429814fd4d967b9277af2805c53f370e52ebb5",
            "paperId": "79429814fd4d967b9277af2805c53f370e52ebb5",
            "title": "Chain of Natural Language Inference for Reducing Large Language Model Ungrounded Hallucinations",
            "abstract": "Large language models (LLMs) can generate fluent natural language texts when given relevant documents as background context. This ability has attracted considerable interest in developing industry applications of LLMs. However, LLMs are prone to generate hallucinations that are not supported by the provided sources. In this paper, we propose a hierarchical framework to detect and mitigate such ungrounded hallucination. Our framework uses Chain of Natural Language Inference (CoNLI) for hallucination detection and hallucination reduction via post-editing. Our approach achieves state-of-the-art performance on hallucination detection and enhances text quality through rewrite, using LLMs without any fine-tuning or domain-specific prompt engineering. We show that this simple plug-and-play framework can serve as an effective choice for hallucination detection and reduction, achieving competitive performance across various contexts.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a hierarchical framework to detect and mitigate ungrounded hallucination, using Chain of Natural Language Inference (CoNLI) for hallucination detection and hallucination reduction via post-editing and shows that this simple plug-and-play framework can serve as an effective choice for hallucinations detection and reduction, achieving competitive performance across various contexts."
            },
            "score": 5
        },
        {
            "id": "4115a24474ef5f184f5cbae3f43aca4d3bb07bea",
            "paperId": "4115a24474ef5f184f5cbae3f43aca4d3bb07bea",
            "title": "Improving Language Models via Plug-and-Play Retrieval Feedback",
            "abstract": "Large language models (LLMs) exhibit remarkable performance across various NLP tasks. However, they often generate incorrect or hallucinated information, which hinders their practical applicability in real-world scenarios. Human feedback has been shown to effectively enhance the factuality and quality of generated content, addressing some of these limitations. However, this approach is resource-intensive, involving manual input and supervision, which can be time-consuming and expensive. Moreover, it cannot be provided during inference, further limiting its practical utility in dynamic and interactive applications. In this paper, we introduce ReFeed, a novel pipeline designed to enhance LLMs by providing automatic retrieval feedback in a plug-and-play framework without the need for expensive fine-tuning. ReFeed first generates initial outputs, then utilizes a retrieval model to acquire relevant information from large document collections, and finally incorporates the retrieved information into the in-context demonstration for output refinement, thereby addressing the limitations of LLMs in a more efficient and cost-effective manner. Experiments on four knowledge-intensive benchmark datasets demonstrate our proposed ReFeed could improve over +6.0% under zero-shot setting and +2.5% under few-shot setting, compared to baselines without using retrieval feedback.",
            "year": 2023,
            "citationCount": 34,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces ReFeed, a novel pipeline designed to enhance LLMs by providing automatic retrieval feedback in a plug-and-play framework without the need for expensive fine-tuning, thereby addressing the limitations of LLMs in a more efficient and cost-effective manner."
            },
            "score": 5
        },
        {
            "id": "6920de816acd201aadc0de51cf0fa62fa92bb0cc",
            "paperId": "6920de816acd201aadc0de51cf0fa62fa92bb0cc",
            "title": "On the Calibration of Large Language Models and Alignment",
            "abstract": "As large language models attract increasing attention and find widespread application, concurrent challenges of reliability also arise at the same time. Confidence calibration, an effective analysis method for gauging the reliability of deep models, serves as a crucial tool for assessing and improving their reliability. However, such investigation has been comparatively underexplored. In this work, we conduct a systematic examination of the calibration of aligned language models throughout the entire construction process, including pretraining and alignment training. At each stage, we investigate how different training settings, such as parameter scales and training data, affect model calibration. To thoroughly assess model calibration, we evaluate models on three most concerned aspects: generation, factuality and understanding. Our work sheds light on whether popular LLMs are well-calibrated and how the training process influences model calibration.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work sheds light on whether popular LLMs are well-calibrated and how the training process influences model calibration, as well as how different training settings affect model calibration."
            },
            "score": 5
        },
        {
            "id": "94617fc3593e87c4ff72ea0a3f5456fde5896801",
            "paperId": "94617fc3593e87c4ff72ea0a3f5456fde5896801",
            "title": "Deductive Closure Training of Language Models for Coherence, Accuracy, and Updatability",
            "abstract": "While language models (LMs) can sometimes generate factually correct text and estimate truth values of individual claims, these generally do not reflect a globally coherent, manipulable model of the world. As a consequence, current LMs also generate incorrect or nonsensical content, and are difficult to edit and bring up to date. We present a method called Deductive Closure Training (DCT) that uses LMs themselves to identify implications of (and contradictions within) the text that they generate, yielding an efficient self-supervised procedure for improving LM factuality. Given a collection of seed documents, DCT prompts LMs to generate additional text implied by these documents, reason globally about the correctness of this generated text, and finally fine-tune on text inferred to be correct. Given seed documents from a trusted source, DCT provides a tool for supervised model updating; if seed documents are sampled from the LM itself, DCT enables fully unsupervised fine-tuning for improved coherence and accuracy. Across the CREAK, MQUaKE, and Reversal Curse datasets, supervised DCT improves LM fact verification and text generation accuracy by 3-26%; on CREAK fully unsupervised DCT improves verification accuracy by 12%. These results show that LMs' reasoning capabilities during inference can be leveraged during training to improve their reliability.",
            "year": 2024,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "DCT shows that LMs' reasoning capabilities during inference can be leveraged during training to improve their reliability, and shows that LMs' reasoning capabilities during inference can be leveraged during training to improve their reliability."
            },
            "score": 5
        },
        {
            "id": "8936192d69bb78afd09d83f3ceafd18dea32a8fd",
            "paperId": "8936192d69bb78afd09d83f3ceafd18dea32a8fd",
            "title": "Self-Consistent Decoding for More Factual Open Responses",
            "abstract": "Self-consistency has emerged as a powerful method for improving the accuracy of short answers generated by large language models. As previously defined, it only concerns the accuracy of a final answer parsed from generated text. In this work, we extend the idea to open response generation, by integrating voting into the decoding method. Each output sentence is selected from among multiple samples, conditioning on the previous selections, based on a simple token overlap score. We compare this\"Sample&Select\"method to greedy decoding, beam search, nucleus sampling, and the recently introduced hallucination avoiding decoders of DoLA, P-CRR, and S-CRR. We show that Sample&Select improves factuality by a 30% relative margin against these decoders in NLI-based evaluation on the subsets of CNN/DM and XSum used in the FRANK benchmark, while maintaining comparable ROUGE-1 F1 scores against reference summaries. We collect human verifications of the generated summaries, confirming the factual superiority of our method.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work extends the idea of self-consistency to open response generation, by integrating voting into the decoding method, and shows that Sample&Select improves factuality by a 30% relative margin against these decoders in NLI-based evaluation on the subsets of CNN/DM and XSum used in the FRANK benchmark."
            },
            "score": 5
        },
        {
            "id": "83712698cee4627b0aeb23c4b949fb317b753107",
            "paperId": "83712698cee4627b0aeb23c4b949fb317b753107",
            "title": "Prompting Internal Self-conversation Enhances Learning by Discussion to Train Meta-cognitive Skills",
            "abstract": null,
            "year": 2012,
            "citationCount": 0,
            "tldr": null,
            "score": 5
        },
        {
            "id": "4e3e064e6613e2c40c1fa5dc2bd8fd934b410ccb",
            "paperId": "4e3e064e6613e2c40c1fa5dc2bd8fd934b410ccb",
            "title": "Meta-CoT: Generalizable Chain-of-Thought Prompting in Mixed-task Scenarios with Large Language Models",
            "abstract": "Large language models (LLMs) have unveiled remarkable reasoning capabilities by exploiting chain-of-thought (CoT) prompting, which generates intermediate reasoning chains to serve as the rationale for deriving the answer. However, current CoT methods either simply employ general prompts such as Let's think step by step, or heavily rely on pre-defined task-specific demonstrations to attain preferable performances, thereby engendering an inescapable gap between performance and generalization. To bridge this gap, we propose GeM-CoT, a Generalizable CoT prompting mechanism in Mixed-task scenarios where the type of input questions is unknown. GeM-CoT first categorizes the question type and subsequently samples or constructs demonstrations from the corresponding data pool in an automatic pattern. With this technical design, GeM-CoT simultaneously enjoys superior generalization capabilities and remarkable performances on 10 public reasoning tasks and 23 BBH tasks.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "GeM-CoT is proposed, a Generalizable CoT prompting mechanism in Mixed-task scenarios where the type of input questions is unknown, which enjoys superior generalization capabilities and remarkable performances on 10 public reasoning tasks and 23 BBH tasks."
            },
            "score": 4
        },
        {
            "id": "434b9f9bc71c935e4a46a1aff36a8cc4c22d9afa",
            "paperId": "434b9f9bc71c935e4a46a1aff36a8cc4c22d9afa",
            "title": "Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration",
            "abstract": "Human intelligence thrives on cognitive synergy, where collaboration among different minds yield superior outcomes compared to isolated individuals. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist is an intelligent agent that collaboratively combines multiple minds' strengths and knowledge to enhance problem-solving in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. Our in-depth analysis shows that assigning multiple fine-grained personas in LLMs improves problem-solving abilities compared to using a single or fixed number of personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing, Codenames Collaborative, and Logic Grid Puzzle, encompassing both knowledge-intensive and reasoning-intensive types. Unlike previous works, such as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, experimental results demonstrate that SPP effectively reduces factual hallucination, and maintains strong reasoning capabilities. Additionally, comparative experiments show that cognitive synergy only emerges in GPT-4 and does not appear in less capable models, such as GPT-3.5-turbo and Llama2-13b-chat, which draws an interesting analogy to human development. Code, data, and prompts can be found at: https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.",
            "year": 2023,
            "citationCount": 42,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas by dynamically identifying and simulating different personas based on task inputs, which unleashes the potential of cognitive synergy in LLMs."
            },
            "score": 4
        },
        {
            "id": "d55ed10e6a77e8f0a2359eb92221915f56481843",
            "paperId": "d55ed10e6a77e8f0a2359eb92221915f56481843",
            "title": "Benchmarking Cognitive Biases in Large Language Models as Evaluators",
            "abstract": "Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 15 LLMs of four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators, such as System Star is better than System Square. We then evaluate the quality of ranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the Egocentric bias where a model prefers to rank its own outputs highly in evaluation. We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (average of 40% of comparisons across all models) within each of their evaluations that question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the average Rank-Biased Overlap (RBO) score to be 49.6%, indicating that machine preferences are misaligned with humans. According to our findings, LLMs may still be unable to be utilized for automatic annotation aligned with human preferences. Our project page is at: https://minnesotanlp.github.io/cobbler.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work assembles 15 LLMs of four different size ranges and finds that LLMs are biased text quality evaluators, exhibiting strong indications on the authors' bias benchmark within each of their evaluations that question their robustness as evaluator."
            },
            "score": 4
        },
        {
            "id": "d4bf36cbc5855ea87235d7a64f406717ac6aa3c9",
            "paperId": "d4bf36cbc5855ea87235d7a64f406717ac6aa3c9",
            "title": "Large Language Models as Analogical Reasoners",
            "abstract": "Chain-of-thought (CoT) prompting for language models demonstrates impressive performance across reasoning tasks, but typically needs labeled exemplars of the reasoning process. In this work, we introduce a new prompting approach, analogical prompting, designed to automatically guide the reasoning process of large language models. Inspired by analogical reasoning, a cognitive process in which humans draw from relevant past experiences to tackle new problems, our approach prompts language models to self-generate relevant exemplars or knowledge in the context, before proceeding to solve the given problem. This method presents several advantages: it obviates the need for labeling or retrieving exemplars, offering generality and convenience; it can also tailor the generated exemplars and knowledge to each problem, offering adaptability. Experimental results show that our approach outperforms 0-shot CoT and manual few-shot CoT in a variety of reasoning tasks, including math problem solving in GSM8K and MATH, code generation in Codeforces, and other reasoning tasks in BIG-Bench.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results show that this approach outperforms 0-shot CoT and manual few-shot CoT in a variety of reasoning tasks, including math problem solving in GSM8K and MATH, code generation in Codeforces, and other reasoning tasks in BIG-Bench."
            },
            "score": 4
        },
        {
            "id": "0334987f094121c094d5043ab38f14ebf5852c05",
            "paperId": "0334987f094121c094d5043ab38f14ebf5852c05",
            "title": "DyVal 2: Dynamic Evaluation of Large Language Models by Meta Probing Agents",
            "abstract": "Evaluation of large language models (LLMs) has raised great concerns in the community due to the issue of data contamination. Existing work designed evaluation protocols using well-defined algorithms for specific tasks, which cannot be easily extended to diverse scenarios. Moreover, current evaluation benchmarks can only provide the overall benchmark results and cannot support a fine-grained and multifaceted analysis of LLMs' abilities. In this paper, we propose meta probing agents (MPA), a general dynamic evaluation protocol inspired by psychometrics to evaluate LLMs. MPA is the key component of DyVal 2, which naturally extends the previous DyVal~\\citep{zhu2023dyval}. MPA designs the probing and judging agents to automatically transform an original evaluation problem into a new one following psychometric theory on three basic cognitive abilities: language understanding, problem solving, and domain knowledge. These basic abilities are also dynamically configurable, allowing multifaceted analysis. We conducted extensive evaluations using MPA and found that most LLMs achieve poorer performance, indicating room for improvement. Our multifaceted analysis demonstrated the strong correlation between the basic abilities and an implicit Matthew effect on model size, i.e., larger models possess stronger correlations of the abilities. MPA can also be used as a data augmentation approach to enhance LLMs.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes meta probing agents (MPA), a general dynamic evaluation protocol inspired by psychometrics to evaluate LLMs and demonstrates the strong correlation between the basic abilities and an implicit Matthew effect on model size, i.e., larger models possess stronger correlations of the abilities."
            },
            "score": 4
        },
        {
            "id": "385c74957858e7d6856d48e72b5a902b4c1aa28c",
            "paperId": "385c74957858e7d6856d48e72b5a902b4c1aa28c",
            "title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
            "abstract": "Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of\"tit for tat\"and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counter-intuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of\"tit for tat\"state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents. Codes: https://github.com/Skytliang/Multi-Agents-Debate",
            "year": 2023,
            "citationCount": 125,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Multi-Agent Debate (MAD) framework is proposed, in which multiple agents express their arguments in the state of\"tit for tat\"and a judge manages the debate process to obtain a final solution."
            },
            "score": 4
        },
        {
            "id": "f41977c497c96c1da2e9e945315e9be6d6ad472e",
            "paperId": "f41977c497c96c1da2e9e945315e9be6d6ad472e",
            "title": "Towards reducing hallucination in extracting information from financial reports using Large Language Models",
            "abstract": "For a financial analyst, the question and answer (Q\\&A) segment of the company financial report is a crucial piece of information for various analysis and investment decisions. However, extracting valuable insights from the Q\\&A section has posed considerable challenges as the conventional methods such as detailed reading and note-taking lack scalability and are susceptible to human errors, and Optical Character Recognition (OCR) and similar techniques encounter difficulties in accurately processing unstructured transcript text, often missing subtle linguistic nuances that drive investor decisions. Here, we demonstrate the utilization of Large Language Models (LLMs) to efficiently and rapidly extract information from earnings report transcripts while ensuring high accuracy transforming the extraction process as well as reducing hallucination by combining retrieval-augmented generation technique as well as metadata. We evaluate the outcomes of various LLMs with and without using our proposed approach based on various objective metrics for evaluating Q\\&A systems, and empirically demonstrate superiority of our method.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work demonstrates the utilization of Large Language Models (LLMs) to efficiently and rapidly extract information from earnings report transcripts while ensuring high accuracy transforming the extraction process as well as reducing hallucination by combining retrieval-augmented generation technique aswell as metadata."
            },
            "score": 4
        },
        {
            "id": "4e53b481beabba42aac027e5a8c69fed26ab4062",
            "paperId": "4e53b481beabba42aac027e5a8c69fed26ab4062",
            "title": "RHO ($\u03c1$): Reducing Hallucination in Open-domain Dialogues with Knowledge Grounding",
            "abstract": "Dialogue systems can leverage large pre-trained language models and knowledge to generate fluent and informative responses. However, these models are still prone to produce hallucinated responses not supported by the input source, which greatly hinders their application. The heterogeneity between external knowledge and dialogue context challenges representation learning and source integration, and further contributes to unfaithfulness. To handle this challenge and generate more faithful responses, this paper presents RHO ($\\rho$) utilizing the representations of linked entities and relation predicates from a knowledge graph (KG). We propose (1) local knowledge grounding to combine textual embeddings with the corresponding KG embeddings; and (2) global knowledge grounding to equip RHO with multi-hop reasoning abilities via the attention mechanism. In addition, we devise a response re-ranking technique based on walks over KG sub-graphs for better conversational reasoning. Experimental results on OpenDialKG show that our approach significantly outperforms state-of-the-art methods on both automatic and human evaluation by a large margin, especially in hallucination reduction (17.54% in FeQA).",
            "year": 2022,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "RHO is presented utilizing the representations of linked entities and relation predicates from a knowledge graph (KG) to equip RHO with multi-hop reasoning abilities via the attention mechanism and devise a response re-ranking technique based on walks over KG sub-graphs for better conversational reasoning."
            },
            "score": 4
        },
        {
            "id": "889feabe31ba0d24c093ac94d54a06eecb87e3f4",
            "paperId": "889feabe31ba0d24c093ac94d54a06eecb87e3f4",
            "title": "Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding",
            "abstract": "Dialogue systems powered by large pre-trained language models exhibit an innate ability to deliver fluent and natural-sounding responses. Despite their impressive performance, these models are fitful and can often generate factually incorrect statements impeding their widespread adoption. In this paper, we focus on the task of improving faithfulness and reducing hallucination of neural dialogue systems to known facts supplied by a Knowledge Graph (KG). We propose Neural Path Hunter which follows a generate-then-refine strategy whereby a generated response is amended using the KG. Neural Path Hunter leverages a separate token-level fact critic to identify plausible sources of hallucination followed by a refinement stage that retrieves correct entities by crafting a query signal that is propagated over a k-hop subgraph. We empirically validate our proposed approach on the OpenDialKG dataset (Moon et al., 2019) against a suite of metrics and report a relative improvement of faithfulness over dialogue responses by 20.35% based on FeQA (Durmus et al., 2020). The code is available at https://github.com/nouhadziri/Neural-Path-Hunter.",
            "year": 2021,
            "citationCount": 80,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes Neural Path Hunter which follows a generate-then-refine strategy whereby a generated response is amended using the KG, and leverages a separate token-level fact critic to identify plausible sources of hallucination and retrieves correct entities by crafting a query signal that is propagated over a k-hop subgraph."
            },
            "score": 4
        },
        {
            "id": "2986b2b06173e065c94bae49c7a9a3718dad486c",
            "paperId": "2986b2b06173e065c94bae49c7a9a3718dad486c",
            "title": "Reducing hallucination in structured outputs via Retrieval-Augmented Generation",
            "abstract": "A common and fundamental limitation of Generative AI (GenAI) is its propensity to hallucinate. While large language models (LLM) have taken the world by storm, without eliminating or at least reducing hallucinations, real-world GenAI systems may face challenges in user adoption. In the process of deploying an enterprise application that produces workflows based on natural language requirements, we devised a system leveraging Retrieval Augmented Generation (RAG) to greatly improve the quality of the structured output that represents such workflows. Thanks to our implementation of RAG, our proposed system significantly reduces hallucinations in the output and improves the generalization of our LLM in out-of-domain settings. In addition, we show that using a small, well-trained retriever encoder can reduce the size of the accompanying LLM, thereby making deployments of LLM-based systems less resource-intensive.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This proposed system significantly reduces hallucinations in the output and improves the generalization of the LLM in out-of-domain settings, and it is shown that using a small, well-trained retriever encoder can reduce the size of the accompanying LLM, thereby making deployments of LLM-based systems less resource-intensive."
            },
            "score": 4
        },
        {
            "id": "60988a0ebad89af503f17de977785814fb864635",
            "paperId": "60988a0ebad89af503f17de977785814fb864635",
            "title": "Correction with Backtracking Reduces Hallucination in Summarization",
            "abstract": "Abstractive summarization aims at generating natural language summaries of a source document that are succinct while preserving the important elements. Despite recent advances, neural text summarization models are known to be susceptible to hallucinating (or more correctly confabulating), that is to produce summaries with details that are not grounded in the source document. In this paper, we introduce a simple yet efficient technique, CoBa, to reduce hallucination in abstractive summarization. The approach is based on two steps: hallucination detection and mitigation. We show that the former can be achieved through measuring simple statistics about conditional word probabilities and distance to context words. Further, we demonstrate that straight-forward backtracking is surprisingly effective at mitigation. We thoroughly evaluate the proposed method with prior art on three benchmark datasets for text summarization. The results show that CoBa is effective and efficient in reducing hallucination, and offers great adaptability and flexibility.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a simple yet efficient technique, CoBa, to reduce hallucination in abstractive summarization, and shows that the former can be achieved through measuring simple statistics about conditional word probabilities and distance to context words."
            },
            "score": 4
        },
        {
            "id": "fcee1c19e12f3b7e3595aeba702416d055bdbc3f",
            "paperId": "fcee1c19e12f3b7e3595aeba702416d055bdbc3f",
            "title": "Knowledge Verification to Nip Hallucination in the Bud",
            "abstract": "While large language models (LLMs) have demonstrated exceptional performance across various tasks following human alignment, they may still generate responses that sound plausible but contradict factual knowledge, a phenomenon known as \\emph{hallucination}. In this paper, we demonstrate the feasibility of mitigating hallucinations by verifying and minimizing the inconsistency between external knowledge present in the alignment data and the intrinsic knowledge embedded within foundation LLMs. Specifically, we propose a novel approach called Knowledge Consistent Alignment (KCA), which employs a well-aligned LLM to automatically formulate assessments based on external knowledge to evaluate the knowledge boundaries of foundation LLMs. To address knowledge inconsistencies in the alignment data, KCA implements several specific strategies to deal with these data instances. We demonstrate the superior efficacy of KCA in reducing hallucinations across six benchmarks, utilizing foundation LLMs of varying backbones and scales. This confirms the effectiveness of mitigating hallucinations by reducing knowledge inconsistency. Our code, model weights, and data are openly accessible at \\url{https://github.com/fanqiwan/KCA}.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The superior efficacy of KCA is demonstrated in reducing hallucinations across six benchmarks, utilizing foundation LLMs of varying backbones and scales, which confirms the effectiveness of mitigating hallucinations by reducing knowledge inconsistency."
            },
            "score": 4
        },
        {
            "id": "f767f0a883c8fc70de03fb8b65ed87e1fef5f415",
            "paperId": "f767f0a883c8fc70de03fb8b65ed87e1fef5f415",
            "title": "Hallucination Diversity-Aware Active Learning for Text Summarization",
            "abstract": "Large Language Models (LLMs) have shown propensity to generate hallucinated outputs, i.e., texts that are factually incorrect or unsupported. Existing methods for alleviating hallucinations typically require costly human annotations to identify and correct hallucinations in LLM outputs. Moreover, most of these methods focus on a specific type of hallucination, e.g., entity or token errors, which limits their effectiveness in addressing various types of hallucinations exhibited in LLM outputs. To our best knowledge, in this paper we propose the first active learning framework to alleviate LLM hallucinations, reducing costly human annotations of hallucination needed. By measuring fine-grained hallucinations from errors in semantic frame, discourse and content verifiability in text summarization, we propose HAllucination Diversity-Aware Sampling (HADAS) to select diverse hallucinations for annotations in active learning for LLM finetuning. Extensive experiments on three datasets and different backbone models demonstrate advantages of our method in effectively and efficiently mitigating LLM hallucinations.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes the first active learning framework to alleviate LLM hallucinations, reducing costly human annotations of hallucination needed, and proposes HAllucination Diversity-Aware Sampling (HADAS) to select diverse hallucinations for annotations in active learning for LLM finetuning."
            },
            "score": 4
        },
        {
            "id": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
            "paperId": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
            "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such\"society of minds\"approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.",
            "year": 2023,
            "citationCount": 206,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer is presented, indicating that this approach significantly enhances mathematical and strategic reasoning across a number of tasks."
            },
            "score": 4
        },
        {
            "id": "d2d3960bd1e4e84cbb0051945ea1419005e75e07",
            "paperId": "d2d3960bd1e4e84cbb0051945ea1419005e75e07",
            "title": "Dial BeInfo for Faithfulness: Improving Factuality of Information-Seeking Dialogue via Behavioural Fine-Tuning",
            "abstract": "Factuality is a crucial requirement in information seeking dialogue: the system should respond to the user's queries so that the responses are meaningful and aligned with the knowledge provided to the system. However, most modern large language models suffer from hallucinations, that is, they generate responses not supported by or contradicting the knowledge source. To mitigate the issue and increase faithfulness of information-seeking dialogue systems, we introduce BeInfo, a simple yet effective method that applies behavioural tuning to aid information-seeking dialogue. Relying on three standard datasets, we show that models tuned with BeInfo} become considerably more faithful to the knowledge source both for datasets and domains seen during BeInfo-tuning, as well as on unseen domains, when applied in a zero-shot manner. In addition, we show that the models with 3B parameters (e.g., Flan-T5) tuned with BeInfo demonstrate strong performance on data from real `production' conversations and outperform GPT4 when tuned on a limited amount of such realistic in-domain dialogues.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces BeInfo, a simple yet effective method that applies behavioural tuning to aid information-seeking dialogue and shows that models tuned with BeInfo become considerably more faithful to the knowledge source both for datasets and domains seen during BeInfo-tuning, as well as on unseen domains, when applied in a zero-shot manner."
            },
            "score": 4
        },
        {
            "id": "3fa9f7aa5e8050b54e1aca41407c12922a6e4867",
            "paperId": "3fa9f7aa5e8050b54e1aca41407c12922a6e4867",
            "title": "Supervised Knowledge Makes Large Language Models Better In-context Learners",
            "abstract": "Large Language Models (LLMs) exhibit emerging in-context learning abilities through prompt engineering. The recent progress in large-scale generative models has further expanded their use in real-world language applications. However, the critical challenge of improving the generalizability and factuality of LLMs in natural language understanding and question answering remains under-explored. While previous in-context learning research has focused on enhancing models to adhere to users' specific instructions and quality expectations, and to avoid undesired outputs, little to no work has explored the use of task-Specific fine-tuned Language Models (SLMs) to improve LLMs' in-context learning during the inference stage. Our primary contribution is the establishment of a simple yet effective framework that enhances the reliability of LLMs as it: 1) generalizes out-of-distribution data, 2) elucidates how LLMs benefit from discriminative models, and 3) minimizes hallucinations in generative tasks. Using our proposed plug-in method, enhanced versions of Llama 2 and ChatGPT surpass their original versions regarding generalizability and factuality. We offer a comprehensive suite of resources, including 16 curated datasets, prompts, model checkpoints, and LLM outputs across 9 distinct tasks. The code and data are released at: https://github.com/YangLinyi/Supervised-Knowledge-Makes-Large-Language-Models-Better-In-context-Learners. Our empirical analysis sheds light on the advantages of incorporating discriminative models into LLMs and highlights the potential of our methodology in fostering more reliable LLMs.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Using the proposed plug-in method, enhanced versions of Llama 2 and ChatGPT surpass their original versions regarding generalizability and factuality and the potential of the methodology in fostering more reliable LLMs is highlighted."
            },
            "score": 4
        },
        {
            "id": "3cebb93c399db7e1434741338b0a24db19786b15",
            "paperId": "3cebb93c399db7e1434741338b0a24db19786b15",
            "title": "Prompt to be Consistent is Better than Self-Consistent? Few-Shot and Zero-Shot Fact Verification with Pre-trained Language Models",
            "abstract": "Few-shot or zero-shot fact verification only relies on a few or no labeled training examples. In this paper, we propose a novel method called ProToCo, to \\underline{Pro}mpt pre-trained language models (PLMs) \\underline{To} be \\underline{Co}nsistent, for improving the factuality assessment capability of PLMs in the few-shot and zero-shot settings. Given a claim-evidence pair, ProToCo generates multiple variants of the claim with different relations and frames a simple consistency mechanism as constraints for making compatible predictions across these variants. We update PLMs by using parameter-efficient fine-tuning (PEFT), leading to more accurate predictions in few-shot and zero-shot fact verification tasks. Our experiments on three public verification datasets show that ProToCo significantly outperforms state-of-the-art few-shot fact verification baselines. With a small number of unlabeled instances, ProToCo also outperforms the strong zero-shot learner T0 on zero-shot verification. Compared to large PLMs using in-context learning (ICL) method, ProToCo outperforms OPT-30B and the Self-Consistency-enabled OPT-6.7B model in both few- and zero-shot settings.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel method called ProToCo, to improve the factuality assessment capability of pre-trained language models (PLMs) by using parameter-efficient fine-tuning (PEFT), leading to more accurate predictions in few-shot and zero-shot fact verification tasks."
            },
            "score": 4
        },
        {
            "id": "3cf51f5f36bac0cbafdb7581d8713979e0d1c17e",
            "paperId": "3cf51f5f36bac0cbafdb7581d8713979e0d1c17e",
            "title": "CLIFF: Contrastive Learning for Improving Faithfulness and Factuality in Abstractive Summarization",
            "abstract": "We study generating abstractive summaries that are faithful and factually consistent with the given articles. A novel contrastive learning formulation is presented, which leverages both reference summaries, as positive training data, and automatically generated erroneous summaries, as negative training data, to train summarization systems that are better at distinguishing between them. We further design four types of strategies for creating negative samples, to resemble errors made commonly by two state-of-the-art models, BART and PEGASUS, found in our new human annotations of summary errors. Experiments on XSum and CNN/Daily Mail show that our contrastive learning framework is robust across datasets and models. It consistently produces more factual summaries than strong comparisons with post error correction, entailment-based reranking, and unlikelihood training, according to QA-based factuality evaluation. Human judges echo the observation and find that our model summaries correct more errors.",
            "year": 2021,
            "citationCount": 135,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that the contrastive learning framework consistently produces more factual summaries than strong comparisons with post error correction, entailment-based reranking, and unlikelihood training, according to QA-based factuality evaluation."
            },
            "score": 4
        },
        {
            "id": "ae1e48a74cb2f313e8e99c82f0aa4487b0805002",
            "paperId": "ae1e48a74cb2f313e8e99c82f0aa4487b0805002",
            "title": "Faithful to the Document or to the World? Mitigating Hallucinations via Entity-linked Knowledge in Abstractive Summarization",
            "abstract": "Despite recent advances in abstractive summarization, current summarization systems still suffer from content hallucinations where models generate text that is either irrelevant or contradictory to the source document. However, prior work has been predicated on the assumption that any generated facts not appearing explicitly in the source are undesired hallucinations. Methods have been proposed to address this scenario by ultimately improving `faithfulness' to the source document, but in reality, there is a large portion of entities in the gold reference targets that are not directly in the source. In this work, we show that these entities are not aberrations, but they instead require utilizing external world knowledge to infer reasoning paths from entities in the source. We show that by utilizing an external knowledge base, we can improve the faithfulness of summaries without simply making them more extractive, and additionally, we show that external knowledge bases linked from the source can benefit the factuality of generated summaries.",
            "year": 2022,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that by utilizing an external knowledge base, one can improve the faithfulness of summaries without simply making them more extractive, and additionally, that external knowledge bases linked from the source can benefit the factuality of generated summaries."
            },
            "score": 4
        },
        {
            "id": "f11d7ff00ac0dacc4248559dc3e14479123d9cbb",
            "paperId": "f11d7ff00ac0dacc4248559dc3e14479123d9cbb",
            "title": "Enhancing Early Detection of Cognitive Decline in the Elderly through Ensemble of NLP Techniques: A Comparative Study Utilizing Large Language Models in Clinical Notes",
            "abstract": "Background: Early detection of cognitive decline in elderly individuals facilitates clinical trial enrollment and timely medical interventions. This study aims to apply, evaluate, and compare advanced natural language processing techniques for identifying signs of cognitive decline in clinical notes. Methods: This study, conducted at Mass General Brigham (MGB), Boston, MA, included clinical notes from the 4 years prior to initial mild cognitive impairment (MCI) diagnosis in 2019 for patients [\u2265] 50 years. Note sections regarding cognitive decline were labeled manually. A random sample of 4,949 note sections filtered with cognitive functions-related keywords were used for traditional AI model development, and 200 random subset were used for LLM and prompt development; another random sample of 1996 note sections without keyword filtering were used for testing. Prompt templates for large language models (LLM), Llama 2 on Amazon Web Service and GPT-4 on Microsoft Azure, were developed with multiple prompting approaches to select the optimal LLM-based method. Baseline comparisons were made with XGBoost and a hierarchical attention-based deep neural network model. An ensemble of the three models was then constructed using majority vote. Results: GPT-4 demonstrated superior accuracy and efficiency to Llama 2. The ensemble model outperformed individual models, achieving a precision of 90.3%, recall of 94.2%, and F1-score of 92.2%. Notably, the ensemble model demonstrated a marked improvement in precision (from a 70%-79% range to above 90%) compared to the best performing single model. Error analysis revealed 63 samples were wrongly predicted by at least one model; however, only 2 cases (3.2%) were mutual errors across all models, indicating diverse error profiles among them. Conclusion: Our findings indicate that LLMs and traditional models exhibit diverse error profiles. The ensemble of LLMs and locally trained machine learning models on EHR data was found to be complementary, enhancing performance and improving diagnostic accuracy.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The ensemble of LLMs and locally trained machine learning models on EHR data was found to be complementary, enhancing performance and improving diagnostic accuracy, indicating that LLMs and traditional models exhibit diverse error profiles."
            },
            "score": 4
        },
        {
            "id": "80c698688bb4488beaceaab5c64f701a946cb7ae",
            "paperId": "80c698688bb4488beaceaab5c64f701a946cb7ae",
            "title": "All in One: Multi-Task Prompting for Graph Neural Networks",
            "abstract": "Recently, \"pre-training and fine-tuning'' has been adopted as a standard workflow for many graph tasks since it can take general graph knowledge to relieve the lack of graph annotations from each application. However, graph tasks with node level, edge level, and graph level are far diversified, making the pre-training pretext often incompatible with these multiple tasks. This gap may even cause a \"negative transfer'' to the specific application, leading to poor results. Inspired by the prompt learning in natural language processing (NLP), which has presented significant effectiveness in leveraging prior knowledge for various NLP tasks, we study the prompting topic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models. Specifically, we first unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern. In this way, the prompting idea from NLP can be seamlessly introduced to the graph area. Then, to further narrow the gap between various graph tasks and state-of-the-art pre-training strategies, we further study the task space of various graph applications and reformulate downstream problems to the graph-level task. Afterward, we introduce meta-learning to efficiently learn a better initialization for the multi-task prompt of graphs so that our prompting framework can be more reliable and general for different tasks. We conduct extensive experiments, results from which demonstrate the superiority of our method.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel multi-task prompting method for graph models that unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern, and introduces meta-learning to efficiently learn a better initialization for the multi- task prompt of graphs so that the prompting framework can be more reliable and general for different tasks."
            },
            "score": 4
        },
        {
            "id": "ccfdea3722ec43e45fc33e170d1fe908dca003a5",
            "paperId": "ccfdea3722ec43e45fc33e170d1fe908dca003a5",
            "title": "TransPrompt v2: A Transferable Prompting Framework for Cross-task Text Classification",
            "abstract": "Text classification is one of the most imperative tasks in natural language processing (NLP). Recent advances with pre-trained language models (PLMs) have shown remarkable success on this task. However, the satisfying results obtained by PLMs heavily depend on the large amounts of task-specific labeled data, which may not be feasible in many application scenarios due to data access and privacy constraints. The recently-proposed prompt-based fine-tuning paradigm improves the performance of PLMs for few-shot text classification with task-specific templates. Yet, it is unclear how the prompting knowledge can be transferred across tasks, for the purpose of mutual reinforcement. We propose TransPrompt v2, a novel transferable prompting framework for few-shot learning across similar or distant text classification tasks. For learning across similar tasks, we employ a multi-task meta-knowledge acquisition (MMA) procedure to train a meta-learner that captures the cross-task transferable knowledge. For learning across distant tasks, we further inject the task type descriptions into the prompt, and capture the intra-type and inter-type prompt embeddings among multiple distant tasks. Additionally, two de-biasing techniques are further designed to make the trained meta-learner more task-agnostic and unbiased towards any tasks. After that, the meta-learner can be adapted to each specific task with better parameters initialization. Extensive experiments show that TransPrompt v2 outperforms single-task and cross-task strong baselines over multiple NLP tasks and datasets. We further show that the meta-learner can effectively improve the performance of PLMs on previously unseen tasks. In addition, TransPrompt v2 also outperforms strong fine-tuning baselines when learning with full training sets.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Extensive experiments show that TransPrompt v2 outperforms single-task and cross-task strong baselines over multiple NLP tasks and datasets, and shows that the meta-learner can effectively improve the performance of PLMs on previously unseen tasks."
            },
            "score": 4
        },
        {
            "id": "4f8ca3ccb77ff3c60ce90910854175eb8ab21a57",
            "paperId": "4f8ca3ccb77ff3c60ce90910854175eb8ab21a57",
            "title": "Empowering Psychotherapy with Large Language Models: Cognitive Distortion Detection through Diagnosis of Thought Prompting",
            "abstract": "Mental illness remains one of the most critical public health issues of our time, due to the severe scarcity and accessibility limit of professionals. Psychotherapy requires high-level expertise to conduct deep, complex reasoning and analysis on the cognition modeling of the patients. In the era of Large Language Models, we believe it is the right time to develop AI assistance for computational psychotherapy. We study the task of cognitive distortion detection and propose the Diagnosis of Thought (DoT) prompting. DoT performs diagnosis on the patient's speech via three stages: subjectivity assessment to separate the facts and the thoughts; contrastive reasoning to elicit the reasoning processes supporting and contradicting the thoughts; and schema analysis to summarize the cognition schemas. The generated diagnosis rationales through the three stages are essential for assisting the professionals. Experiments demonstrate that DoT obtains significant improvements over ChatGPT for cognitive distortion detection, while generating high-quality rationales approved by human experts.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The task of cognitive distortion detection is studied and the Diagnosis of Thought (DoT) prompting is proposed, demonstrating that DoT obtains significant improvements over ChatGPT for Cognitive distortion detection, while generating high-quality rationales approved by human experts."
            },
            "score": 3
        },
        {
            "id": "e53e03f8d12e031d714a07af3b7930deb5351db4",
            "paperId": "e53e03f8d12e031d714a07af3b7930deb5351db4",
            "title": "Meta-Task Prompting Elicits Embedding from Large Language Models",
            "abstract": "In this work, we introduce a new unsupervised embedding method, Meta-Task Prompting with Explicit One-Word Limitation (MetaEOL), for generating high-quality sentence embeddings from Large Language Models (LLMs) without the need for model fine-tuning or task-specific engineering. Leveraging meta-task prompting, MetaEOL guides LLMs to produce embeddings through a series of carefully designed prompts that address multiple representational aspects. Our comprehensive experiments demonstrate that embeddings averaged from various meta-tasks yield competitive performance on Semantic Textual Similarity (STS) benchmarks and excel in downstream tasks, surpassing contrastive-trained models. Our findings suggest a new scaling law for embedding generation, offering a versatile, resource-efficient approach for embedding extraction across diverse sentence-centric scenarios.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 3
        },
        {
            "id": "0815c5a05f50fc3405299abdb97cf1343ad63ac9",
            "paperId": "0815c5a05f50fc3405299abdb97cf1343ad63ac9",
            "title": "Synergistic Integration of Large Language Models and Cognitive Architectures for Robust AI: An Exploratory Analysis",
            "abstract": "This paper explores the integration of two AI subdisciplines employed in the development of artificial agents that exhibit intelligent behavior: Large Language Models (LLMs) and Cognitive Architectures (CAs). We present three integration approaches, each grounded in theoretical models and supported by preliminary empirical evidence. The modular approach, which introduces four models with varying degrees of integration, makes use of chain-of-thought prompting, and draws inspiration from augmented LLMs, the Common Model of Cognition, and the simulation theory of cognition. The agency approach, motivated by the Society of Mind theory and the LIDA cognitive architecture, proposes the formation of agent collections that interact at micro and macro cognitive levels, driven by either LLMs or symbolic components. The neuro-symbolic approach, which takes inspiration from the CLARION cognitive architecture, proposes a model where bottom-up learning extracts symbolic representations from an LLM layer and top-down guidance utilizes symbolic representations to direct prompt engineering in the LLM layer. These approaches aim to harness the strengths of both LLMs and CAs, while mitigating their weaknesses, thereby advancing the development of more robust AI systems. We discuss the tradeoffs and challenges associated with each approach.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Three integration approaches are presented, each grounded in theoretical models and supported by preliminary empirical evidence, that aim to harness the strengths of both LLMs and CAs, while mitigating their weaknesses, thereby advancing the development of more robust AI systems."
            },
            "score": 3
        },
        {
            "id": "9fe222cb8464e8157b3654ab96ef719331dd2357",
            "paperId": "9fe222cb8464e8157b3654ab96ef719331dd2357",
            "title": "Meta-Learning the Difference: Preparing Large Language Models for Efficient Adaptation",
            "abstract": "Abstract Large pretrained language models (PLMs) are often domain- or task-adapted via finetuning or prompting. Finetuning requires modifying all of the parameters and having enough data to avoid overfitting while prompting requires no training and few examples but limits performance. Instead, we prepare PLMs for data- and parameter-efficient adaptation by learning to learn the difference between general and adapted PLMs. This difference is expressed in terms of model weights and sublayer structure through our proposed dynamic low-rank reparameterization and learned architecture controller. Experiments on few-shot dialogue completion, low-resource abstractive summarization, and multi-domain language modeling show improvements in adaptation time and performance over direct finetuning or preparation via domain-adaptive pretraining. Ablations show our task-adaptive reparameterization (TARP) and model search (TAMS) components individually improve on other parameter-efficient transfer like adapters and structure-learning methods like learned sparsification.",
            "year": 2022,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experiments on few-shot dialogue completion, low-resource abstractive summarization, and multi-domain language modeling show improvements in adaptation time and performance over direct finetuning or preparation via domain-adaptive pretraining."
            },
            "score": 3
        },
        {
            "id": "96d6bb5d6abdeda9b2db9af6296527200ba7aa32",
            "paperId": "96d6bb5d6abdeda9b2db9af6296527200ba7aa32",
            "title": "Boosting Theory-of-Mind Performance in Large Language Models via Prompting",
            "abstract": "Large language models (LLMs) excel in many tasks in 2023, but they still face challenges in complex reasoning. Theory-of-mind (ToM) tasks, which require understanding agents' beliefs, goals, and mental states, are essential for common-sense reasoning involving humans, making it crucial to enhance LLM performance in this area. This study measures the ToM performance of GPT-4 and three GPT-3.5 variants (Davinci-2, Davinci-3, GPT-3.5-Turbo), and investigates the effectiveness of in-context learning in improving their ToM comprehension. We evaluated prompts featuring two-shot chain of thought reasoning and step-by-step thinking instructions. We found that LLMs trained with Reinforcement Learning from Human Feedback (RLHF) (all models excluding Davinci-2) improved their ToM accuracy via in-context learning. GPT-4 performed best in zero-shot settings, reaching nearly 80% ToM accuracy, but still fell short of the 87% human accuracy on the test set. However, when supplied with prompts for in-context learning, all RLHF-trained LLMs exceeded 80% ToM accuracy, with GPT-4 reaching 100%. These results demonstrate that appropriate prompting enhances LLM ToM reasoning, and they underscore the context-dependent nature of LLM cognitive capacities.",
            "year": 2023,
            "citationCount": 43,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that appropriate prompting enhances LLM ToM reasoning, and the context-dependent nature of LLM cognitive capacities is underscored, as it is found that LLMs trained with Reinforcement Learning from Human Feedback improved their ToM accuracy via in-context learning."
            },
            "score": 3
        },
        {
            "id": "ce913026f693101e54d3ab9152e107034d81fce1",
            "paperId": "ce913026f693101e54d3ab9152e107034d81fce1",
            "title": "Holistic Evaluation of Language Models",
            "abstract": "Language models (LMs) like GPT\u20103, PaLM, and ChatGPT are the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of LMs. LMs can serve many purposes and their behavior should satisfy many desiderata. To navigate the vast space of potential scenarios and metrics, we taxonomize the space and select representative subsets. We evaluate models on 16 core scenarios and 7 metrics, exposing important trade\u2010offs. We supplement our core evaluation with seven targeted evaluations to deeply analyze specific aspects (including world knowledge, reasoning, regurgitation of copyrighted content, and generation of disinformation). We benchmark 30 LMs, from OpenAI, Microsoft, Google, Meta, Cohere, AI21 Labs, and others. Prior to HELM, models were evaluated on just 17.9% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0%: all 30 models are now benchmarked under the same standardized conditions. Our evaluation surfaces 25 top\u2010level findings. For full transparency, we release all raw model prompts and completions publicly. HELM is a living benchmark for the community, continuously updated with new scenarios, metrics, and models https://crfm.stanford.edu/helm/latest/.",
            "year": 2023,
            "citationCount": 485,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "HELM is a living benchmark for the community, continuously updated with new scenarios, metrics, and models https://crfm.stanford.edu/helm/latest/."
            },
            "score": 3
        },
        {
            "id": "d32764d479f338e0a1897cc3c35630f4ed0a39bf",
            "paperId": "d32764d479f338e0a1897cc3c35630f4ed0a39bf",
            "title": "SelectIT: Selective Instruction Tuning for Large Language Models via Uncertainty-Aware Self-Reflection",
            "abstract": "Instruction tuning (IT) is crucial to tailoring large language models (LLMs) towards human-centric interactions. Recent advancements have shown that the careful selection of a small, high-quality subset of IT data can significantly enhance the performance of LLMs. Despite this, common approaches often rely on additional models or data sets, which increases costs and limits widespread adoption. In this work, we propose a novel approach, termed SelectIT, that capitalizes on the foundational capabilities of the LLM itself. Specifically, we exploit the intrinsic uncertainty present in LLMs to more effectively select high-quality IT data, without the need for extra resources. Furthermore, we introduce a novel IT dataset, the Selective Alpaca, created by applying SelectIT to the Alpaca-GPT4 dataset. Empirical results demonstrate that IT using Selective Alpaca leads to substantial model ability enhancement. The robustness of SelectIT has also been corroborated in various foundation models and domain-specific tasks. Our findings suggest that longer and more computationally intensive IT data may serve as superior sources of IT, offering valuable insights for future research in this area. Data, code, and scripts are freely available at https://github.com/Blue-Raincoat/SelectIT.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work exploits the intrinsic uncertainty present in LLMs to more effectively select high-quality IT data, without the need for extra resources, and introduces a novel IT dataset, the Selective Alpaca, created by applying SelectIT to the Alpaca-GPT4 dataset."
            },
            "score": 3
        },
        {
            "id": "3214cf1edc0955ade27d34032ae5576fd4944257",
            "paperId": "3214cf1edc0955ade27d34032ae5576fd4944257",
            "title": "Rethinking Large Language Models in Mental Health Applications",
            "abstract": "Large Language Models (LLMs) have become valuable assets in mental health, showing promise in both classification tasks and counseling applications. This paper offers a perspective on using LLMs in mental health applications. It discusses the instability of generative models for prediction and the potential for generating hallucinatory outputs, underscoring the need for ongoing audits and evaluations to maintain their reliability and dependability. The paper also distinguishes between the often interchangeable terms ``explainability'' and ``interpretability'', advocating for developing inherently interpretable methods instead of relying on potentially hallucinated self-explanations generated by LLMs. Despite the advancements in LLMs, human counselors' empathetic understanding, nuanced interpretation, and contextual awareness remain irreplaceable in the sensitive and complex realm of mental health counseling. The use of LLMs should be approached with a judicious and considerate mindset, viewing them as tools that complement human expertise rather than seeking to replace it.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The instability of generative models for prediction and the potential for generating hallucinatory outputs are discussed, underscoring the need for ongoing audits and evaluations to maintain their reliability and dependability."
            },
            "score": 3
        },
        {
            "id": "33560c52a5a90e1074a9c341b752bd9e8ac86f7d",
            "paperId": "33560c52a5a90e1074a9c341b752bd9e8ac86f7d",
            "title": "AcTune: Uncertainty-Based Active Self-Training for Active Fine-Tuning of Pretrained Language Models",
            "abstract": "Although fine-tuning pre-trained language models (PLMs) renders strong performance in many NLP tasks, it relies on excessive labeled data. Recently, researchers have resorted to active fine-tuning for enhancing the label efficiency of PLM fine-tuning, but existing methods of this type usually ignore the potential of unlabeled data. We develop AcTune, a new framework that improves the label efficiency of active PLM fine-tuning by unleashing the power of unlabeled data via self-training. AcTune switches between data annotation and model self-training based on uncertainty: the unlabeled samples of high-uncertainty are selected for annotation, while the ones from low-uncertainty regions are used for model self-training. Additionally, we design (1) a region-aware sampling strategy to avoid redundant samples when querying annotations and (2) a momentum-based memory bank to dynamically aggregate the model\u2019s pseudo labels to suppress label noise in self-training. Experiments on 6 text classification datasets show that AcTune outperforms the strongest active learning and self-training baselines and improves the label efficiency of PLM fine-tuning by 56.2% on average. Our implementation is available at https://github.com/yueyu1030/actune.",
            "year": 2022,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "AcTune is developed, a new framework that improves the label efficiency of active PLM fine-tuning by unleashing the power of unlabeled data via self-training by switching between data annotation and model self- training based on uncertainty."
            },
            "score": 3
        },
        {
            "id": "06b2ac5153e3d8d05c13c82f93d7f4e13eee6d0f",
            "paperId": "06b2ac5153e3d8d05c13c82f93d7f4e13eee6d0f",
            "title": "Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites",
            "abstract": "Large language models (LLMs) have shown remarkable performance in natural language processing (NLP) tasks. To comprehend and execute diverse human instructions over image data, instruction-tuned large vision-language models (LVLMs) have been introduced. However, LVLMs may suffer from different types of object hallucinations. Nevertheless, LVLMs are evaluated for coarse-grained object hallucinations only (i.e., generated objects non-existent in the input image). The fine-grained object attributes and behaviors non-existent in the image may still be generated but not measured by the current evaluation methods. In this paper, we thus focus on reducing fine-grained hallucinations of LVLMs. We propose \\textit{ReCaption}, a framework that consists of two components: rewriting captions using ChatGPT and fine-tuning the instruction-tuned LVLMs on the rewritten captions. We also propose a fine-grained probing-based evaluation method named \\textit{Fine-Grained Object Hallucination Evaluation} (\\textit{FGHE}). Our experiment results demonstrate that ReCaption effectively reduces fine-grained object hallucination for different LVLM options and improves their text generation quality. The code can be found at https://github.com/Anonymousanoy/FOHE.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ReCaption is proposed, a framework that consists of two components: rewriting captions using ChatGPT and fine-tuning the instruction-tuned LVLMs on the rewritten captions, and a fine-grained probing-based evaluation method named \\textit{Fine-Grained Object Hallucination Evaluation} (FGHE)."
            },
            "score": 3
        },
        {
            "id": "f99116659c7522941c2353f23bddd07251adaccc",
            "paperId": "f99116659c7522941c2353f23bddd07251adaccc",
            "title": "BTR: Binary Token Representations for Efficient Retrieval Augmented Language Models",
            "abstract": "Retrieval augmentation addresses many critical problems in large language models such as hallucination, staleness, and privacy leaks. However, running retrieval-augmented language models (LMs) is slow and difficult to scale due to processing large amounts of retrieved text. We introduce binary token representations (BTR), which use 1-bit vectors to precompute every token in passages, significantly reducing computation during inference. Despite the potential loss of accuracy, our new calibration techniques and training objectives restore performance. Combined with offline and runtime compression, this only requires 127GB of disk space for encoding 3 billion tokens in Wikipedia. Our experiments show that on five knowledge-intensive NLP tasks, BTR accelerates state-of-the-art inference by up to 4x and reduces storage by over 100x while maintaining over 95% task performance.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "BTR is introduced, which use 1-bit vectors to precompute every token in passages, significantly reducing computation during inference, and accelerates state-of-the-art inference by up to 4x and reduces storage by over 100x while maintaining over 95% task performance."
            },
            "score": 3
        },
        {
            "id": "7cfbd36c0043098589cbaf18dca2b41d8dc24abe",
            "paperId": "7cfbd36c0043098589cbaf18dca2b41d8dc24abe",
            "title": "Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre-training",
            "abstract": "Large-scale vision-language pre-trained (VLP) models are prone to hallucinate non-existent visual objects when generating text based on visual information. In this paper, we systematically study the object hallucination problem from three aspects. First, we examine recent state-of-the-art VLP models, showing that they still hallucinate frequently and models achieving better scores on standard metrics (e.g., CIDEr) could be more unfaithful. Second, we investigate how different types of image encoding in VLP influence hallucination, including region-based, grid-based, and patch-based. Surprisingly, we find that patch-based features perform the best and smaller patch resolution yields a non-trivial reduction in object hallucination. Third, we decouple various VLP objectives and demonstrate that token-level image-text alignment and controlled generation are crucial to reducing hallucination. Based on that, we propose a simple yet effective VLP loss named ObjMLM to further mitigate object hallucination. Results show that it reduces object hallucination by up to 17.4% when tested on two benchmarks (COCO Caption for in-domain and NoCaps for out-of-domain evaluation).",
            "year": 2022,
            "citationCount": 35,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a simple yet effective VLP loss named ObjMLM to further mitigate object hallucination and decouple various VLP objectives and demonstrates that token-level image-text alignment and controlled generation are crucial to reducing hallucination."
            },
            "score": 3
        },
        {
            "id": "6df5ba162b38d2853cc8431ff6f878d085c03693",
            "paperId": "6df5ba162b38d2853cc8431ff6f878d085c03693",
            "title": "SERPENT-VLM : Self-Refining Radiology Report Generation Using Vision Language Models",
            "abstract": "Radiology Report Generation (R2Gen) demonstrates how Multi-modal Large Language Models (MLLMs) can automate the creation of accurate and coherent radiological reports. Existing methods often hallucinate details in text-based reports that don't accurately reflect the image content. To mitigate this, we introduce a novel strategy, SERPENT-VLM (SElf Refining Radiology RePort GENeraTion using Vision Language Models), which improves the R2Gen task by integrating a self-refining mechanism into the MLLM framework. We employ a unique self-supervised loss that leverages similarity between pooled image representations and the contextual representations of the generated radiological text, alongside the standard Causal Language Modeling objective, to refine image-text representations. This allows the model to scrutinize and align the generated text through dynamic interaction between a given image and the generated text, therefore reducing hallucination and continuously enhancing nuanced report generation. SERPENT-VLM outperforms existing baselines such as LLaVA-Med, BiomedGPT, etc., achieving SoTA performance on the IU X-ray and Radiology Objects in COntext (ROCO) datasets, and also proves to be robust against noisy images. A qualitative case study emphasizes the significant advancements towards more sophisticated MLLM frameworks for R2Gen, opening paths for further research into self-supervised refinement in the medical imaging domain.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel strategy, SERPENT-VLM (SElf Refining Radiology RePort GENeraTion using Vision Language Models), which improves the R2Gen task by integrating a self-refining mechanism into the MLLM framework, and proves to be robust against noisy images."
            },
            "score": 3
        },
        {
            "id": "7751f6cdec0f4473c1733eec91699744a7d5176f",
            "paperId": "7751f6cdec0f4473c1733eec91699744a7d5176f",
            "title": "HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding",
            "abstract": "While large vision-language models (LVLMs) have demonstrated impressive capabilities in interpreting multi-modal contexts, they invariably suffer from object hallucinations (OH). We introduce HALC, a novel decoding algorithm designed to mitigate OH in LVLMs. HALC leverages distinct fine-grained optimal visual information in vision-language tasks and operates on both local and global contexts simultaneously. Specifically, HALC integrates a robust auto-focal grounding mechanism (locally) to correct hallucinated tokens on the fly, and a specialized beam search algorithm (globally) to significantly reduce OH while preserving text generation quality. Additionally, HALC can be integrated into any LVLMs as a plug-and-play module without extra training. Extensive experimental studies demonstrate the effectiveness of HALC in reducing OH, outperforming state-of-the-arts across four benchmarks.",
            "year": 2024,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "HALC, a novel decoding algorithm designed to mitigate OH in LVLMs, is introduced, which leverages distinct fine-grained optimal visual information in vision-language tasks and operates on both local and global contexts simultaneously."
            },
            "score": 3
        },
        {
            "id": "afdeef9585232642d18e7c6a7942b2395e94ede1",
            "paperId": "afdeef9585232642d18e7c6a7942b2395e94ede1",
            "title": "A Cause-Effect Look at Alleviating Hallucination of Knowledge-grounded Dialogue Generation",
            "abstract": "Empowered by the large-scale pretrained language models, existing dialogue systems have demonstrated impressive performance conducting fluent and natural-sounding conversations. However, they are still plagued by the hallucination problem, causing unpredictable factual errors in the generated responses. Recently, knowledge-grounded dialogue generation models, that intentionally invoke external knowledge resources to more informative responses, are also proven to be effective in reducing hallucination. Following the idea of getting high-quality knowledge, a few efforts have achieved pretty good performance on this issue. As some inevitable knowledge noises may also lead to hallucinations, it is emergent to investigate the reason and future directions for building noise-tolerant methods in KGD tasks. In this paper, we analyze the causal story behind this problem with counterfactual reasoning methods. Based on the causal effect analysis, we propose a possible solution for alleviating the hallucination in KGD by exploiting the dialogue-knowledge interaction. Experimental results of our example implementation show that this method can reduce hallucination without disrupting other dialogue performance, while keeping adaptive to different generation models. We hope our efforts can support and call for more attention to developing lightweight techniques towards robust and trusty dialogue systems.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Based on the causal effect analysis, a possible solution for alleviating the hallucination in KGD by exploiting the dialogue-knowledge interaction is proposed and results show this method can reduce hallucination without disrupting other dialogue performance, while keeping adaptive to different generation models."
            },
            "score": 3
        },
        {
            "id": "ed5020eeda1fbe8c29b1282d654b34abee22d90f",
            "paperId": "ed5020eeda1fbe8c29b1282d654b34abee22d90f",
            "title": "DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models",
            "abstract": "Despite their impressive capabilities, large language models (LLMs) are prone to hallucinations, i.e., generating content that deviates from facts seen during pretraining. We propose a simple decoding strategy for reducing hallucinations with pretrained LLMs that does not require conditioning on retrieved external knowledge nor additional fine-tuning. Our approach obtains the next-token distribution by contrasting the differences in logits obtained from projecting the later layers versus earlier layers to the vocabulary space, exploiting the fact that factual knowledge in an LLMs has generally been shown to be localized to particular transformer layers. We find that this Decoding by Contrasting Layers (DoLa) approach is able to better surface factual knowledge and reduce the generation of incorrect facts. DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA by 12-17% absolute points, demonstrating its potential in making LLMs reliably generate truthful facts.",
            "year": 2023,
            "citationCount": 35,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "DoLa consistently improves the truthfulness across multiple choices tasks and open-ended generation tasks, for example improving the performance of LLaMA family models on TruthfulQA by 12-17% absolute points, demonstrating its potential in making LLMs reliably generate truthful facts."
            },
            "score": 3
        },
        {
            "id": "7808f614d993cae3ae2cfd3afc32f8d9191f7126",
            "paperId": "7808f614d993cae3ae2cfd3afc32f8d9191f7126",
            "title": "Model Editing Can Hurt General Abilities of Large Language Models",
            "abstract": "One critical challenge that has emerged is the presence of hallucinations in the output of large language models (LLMs) due to false or outdated knowledge. Since retraining LLMs with updated information is resource-intensive, there has been a growing interest in model editing. However, current model editing methods, while effective in improving editing performance in various scenarios, often overlook potential side effects on the general abilities of LLMs. In this paper, we raise concerns that model editing inherently improves the factuality of the model, but may come at the cost of a significant degradation of these general abilities. Systematically, we analyze side effects by evaluating four popular editing methods on three LLMs across eight representative task categories. Extensive empirical research reveals that current model editing methods are difficult to couple well with LLMs to simultaneously improve the factuality and maintain the general abilities such as reasoning, question answering, etc. Strikingly, the use of a specific method to edit LLaMA-1 (7B) resulted in a drastic performance degradation to nearly 0 on all selected tasks with just a single edit. Therefore, we advocate for more research efforts to minimize the loss of general abilities acquired during LLM pre-training and to ultimately preserve them during model editing.",
            "year": 2024,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Concerns are raised that model editing inherently improves the factuality of the model, but may come at the cost of a significant degradation of these general abilities, which should be preserved during model editing."
            },
            "score": 3
        },
        {
            "id": "dbed9b9a616d7525583b58b56ee103b4d45beea3",
            "paperId": "dbed9b9a616d7525583b58b56ee103b4d45beea3",
            "title": "PROM: A Phrase-level Copying Mechanism with Pre-training for Abstractive Summarization",
            "abstract": "Based on the remarkable achievements of pre-trained language models in abstractive summarization, the copying mechanism has proved helpful by improving the factuality, stability, and overall performance. This work proposes PROM, a new PhRase-level cOpying Mechanism that enhances attention on n-grams, which can be applied to zero-shot summarization with pre-training. PROM adds an indicator layer to explicitly pick up tokens in n-gram that can be copied from the source, and calculates an auxiliary loss for the copying prediction. Empirical studies show that PROM makes significant improvements in fine-tuning on benchmarks. In zero-shot setting, PROM is utilized in the self-supervised pre-training on raw corpora and provides new general baselines on a wide range of summarization datasets. Further analysis shows that PROM performs more reasonable copying and contributes to faithfulness.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes PROM, a new PhRase-level cOpying Mechanism that enhances attention on n-grams that can be applied to zero-shot summarization with pre-training, which performs more reasonable copying and contributes to faithfulness."
            },
            "score": 3
        },
        {
            "id": "a3488f44051d21a6c8bcdcb37feadab17fb134e5",
            "paperId": "a3488f44051d21a6c8bcdcb37feadab17fb134e5",
            "title": "The Difference between Mathematical Reasoning Ability Improvement by Learning with Meta Cognitive Approach Aided Probing and Prompting Techniques in SMP Negeri 4 SeiSuka",
            "abstract": "The purpose of this study were: (1) analyzed the differences in students\u2019 mathematical reasoning ability improvement taught by metacognition approach aided probing technique (PMT-probing) and metacognition approach aided prompting technique (PMT-prompting); and (2) described the process of the students\u2019 responses in solving mathematical reasoning abilities. This study was a quasi experimentalresearch. The population in this study were all students of class VIII SMP Negeri 4 SeiSuka, with a purposive sampling techniques, the obtained sample was VIII-1 and VIII-2. The research instrument used a test of mathematical reasoning ability, and had qualified the criteria of content validity, and reliability coefficient of 0.819. Anova two ways was used to analyze the difference of mathematical reasoning ability improvement, while descriptive analysis was used to analyze students\u2019 answers process. The results showed that: (1) There were differences in students\u2019 mathematical reasoning skills improvement which were taught by metacognition approach aided probing techniques and the students taught by prompting technical approach; and (2) The process of the students\u2019 responses on students\u2019 mathematical reasoning abilitythrough learning with metacognition approach aided by prompting techniques was better than metacognition approach aided by probing techniques.",
            "year": 2016,
            "citationCount": 6,
            "tldr": null,
            "score": 3
        },
        {
            "id": "482afe3ac13e4db20ab262f969481a24d3d4ec5c",
            "paperId": "482afe3ac13e4db20ab262f969481a24d3d4ec5c",
            "title": "Learning technology-enabled ( meta )-cognitive scaffolding to support learning aspects of written argumentation",
            "abstract": "This paper reports on an AI-informed and NLP-based work in progress. It shares the technology, educational and cognitive approaches for enabling science students to engage with automated (AI) personalized (meta)-cognitive scaffolding to learn aspects of written scientific argumentation. We briefly report on the features and functionalities of MindWare technology and preliminary and brief results of a small-scale pilot to gauge the impact of technology-mediated scaffolding on students\u2019 learning of how to argue (in written form). CCS Concepts \u2022Computing methodologies \u279d Cognitive",
            "year": 2018,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper reports on an AI-informed and NLP-based work in progress for enabling science students to engage with automated (AI) personalized (meta)-cognitive scaffolding to learn aspects of written scientific argumentation."
            },
            "score": 3
        },
        {
            "id": "d41d97617a20427fda4fd899a3004793f981de74",
            "paperId": "d41d97617a20427fda4fd899a3004793f981de74",
            "title": "ProMISe: A Proactive Multi-turn Dialogue Dataset for Information-seeking Intent Resolution",
            "abstract": "Users of AI-based virtual assistants and search systems encounter challenges in articulating their intents while seeking information on unfamiliar topics, possibly due to complexity of the user\u2019s intent or the lack of meta-information on the topic. We posit that an iterative suggested question-answering (SQA) conversation can improve the trade-off between the satisfaction of the user\u2019s intent while keeping the information exchange natural and cognitive load of the interaction minimal on the users. In this paper, we evaluate a novel setting ProMISe by means of a sequence of interactions between a user, having a predefined information-seeking intent, and an agent that generates a set of SQA pairs at each step to aid the user to get closer to their intent. We simulate this two-player setting to create a multi-turn conversational dataset of SQAs and user choices (1025 dialogues comprising 4453 turns and 17812 SQAs) using human-feedback, chain-of-thought prompting and web-retrieval augmented large language models. We evaluate the quality of the SQs in the dataset on attributes such as diversity, specificity, grounding, etc, and benchmark the performance of different language models for the task of replicating user behavior.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper evaluates a novel setting ProMISe by means of a sequence of interactions between a user, having a predefined information-seeking intent, and an agent that generates a set of SQA pairs at each step to aid the user to get closer to their intent."
            },
            "score": 2
        },
        {
            "id": "b1bb8d2c996740a12163db2eaaca351bbf87dbbb",
            "paperId": "b1bb8d2c996740a12163db2eaaca351bbf87dbbb",
            "title": "Generating Better Items for Cognitive Assessments Using Large Language Models",
            "abstract": "Writing high-quality test questions (items) is critical to building educational measures but has traditionally also been a time-consuming process. One promising avenue for alleviating this is automated item generation, whereby methods from artificial intelligence (AI) are used to generate new items with minimal human intervention. Researchers have explored using large language models (LLMs) to generate new items with equivalent psychometric properties to human-written ones. But can LLMs generate items with improved psychometric properties, even when existing items have poor validity evidence? We investigate this using items from a natural language inference (NLI) dataset. We develop a novel prompting strategy based on selecting items with both the best and worst properties to use in the prompt and use GPT-3 to generate new NLI items. We find that the GPT-3 items show improved psychometric properties in many cases, whilst also possessing good content, convergent and discriminant validity evidence. Collectively, our results demonstrate the potential of employing LLMs to ease the item development process and suggest that the careful use of prompting may allow for iterative improvement of item quality.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel prompting strategy based on selecting items with both the best and worst properties to use in the prompt is developed and the GPT-3 items show improved psychometric properties in many cases, whilst also possessing good content, convergent and discriminant validity evidence."
            },
            "score": 2
        },
        {
            "id": "136d968598ab14715cec3393153355c3b535201e",
            "paperId": "136d968598ab14715cec3393153355c3b535201e",
            "title": "Meta Learning to Bridge Vision and Language Models for Multimodal Few-Shot Learning",
            "abstract": "Multimodal few-shot learning is challenging due to the large domain gap between vision and language modalities. Existing methods are trying to communicate visual concepts as prompts to frozen language models, but rely on hand-engineered task induction to reduce the hypothesis space. To make the whole process learnable, we introduce a multimodal meta-learning approach. Specifically, our approach decomposes the training of the model into a set of related multimodal few-shot tasks. We define a meta-mapper network, acting as a meta-learner, to efficiently bridge frozen large-scale vision and language models and leverage their already learned capacity. By updating the learnable parameters only of the meta-mapper, it learns to accrue shared meta-knowledge among these tasks. Thus, it can rapidly adapt to newly presented samples with only a few gradient updates. Importantly, it induces the task in a completely data-driven manner, with no need for a hand-engineered task induction. We evaluate our approach on recently proposed multimodal few-shot benchmarks, measuring how rapidly the model can bind novel visual concepts to words and answer visual questions by observing only a limited set of labeled examples. The experimental results show that our meta-learning approach outperforms the baseline across multiple datasets and various training settings while being computationally more efficient.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results show that the multimodal meta-learning approach outperforms the baseline across multiple datasets and various training settings while being computationally more efficient."
            },
            "score": 2
        },
        {
            "id": "9977fee41d9cce1b2ed924da966140ac8120762b",
            "paperId": "9977fee41d9cce1b2ed924da966140ac8120762b",
            "title": "Evaluating Cognitive Maps and Planning in Large Language Models with CogEval",
            "abstract": "Recently an influx of studies claim emergent cognitive abilities in large language models (LLMs). Yet, most rely on anecdotes, overlook contamination of training sets, or lack systematic Evaluation involving multiple tasks, control conditions, multiple iterations, and statistical robustness tests. Here we make two major contributions. First, we propose CogEval, a cognitive science-inspired protocol for the systematic evaluation of cognitive capacities in Large Language Models. The CogEval protocol can be followed for the evaluation of various abilities. Second, here we follow CogEval to systematically evaluate cognitive maps and planning ability across eight LLMs (OpenAI GPT-4, GPT-3.5-turbo-175B, davinci-003-175B, Google Bard, Cohere-xlarge-52.4B, Anthropic Claude-1-52B, LLaMA-13B, and Alpaca-7B). We base our task prompts on human experiments, which offer both established construct validity for evaluating planning, and are absent from LLM training sets. We find that, while LLMs show apparent competence in a few planning tasks with simpler structures, systematic evaluation reveals striking failure modes in planning tasks, including hallucinations of invalid trajectories and getting trapped in loops. These findings do not support the idea of emergent out-of-the-box planning ability in LLMs. This could be because LLMs do not understand the latent relational structures underlying planning problems, known as cognitive maps, and fail at unrolling goal-directed trajectories based on the underlying structure. Implications for application and future directions are discussed.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that, while LLMs show apparent competence in a few planning tasks with simpler structures, systematic evaluation reveals striking failure modes in planning tasks, which do not support the idea of emergent out-of-the-box planning ability in LLMs."
            },
            "score": 2
        },
        {
            "id": "b62c1229b130218d3cfe868ee06ee68ec3c44d0c",
            "paperId": "b62c1229b130218d3cfe868ee06ee68ec3c44d0c",
            "title": "Exploring the Use of Large Language Models for Improving the Awareness of Mindfulness",
            "abstract": "Teachable self-help techniques, such as mindfulness, can reduce anxiety and improve mental well-being outcomes. However, people lack proper awareness of such techniques. In this work, we explore the design space of using online dissemination channels to help people learn about mindfulness. We investigate the potential benefits of using Large Language Models (LLMs) to improve awareness and willingness to practice these techniques, building on a video-based intervention to introduce mindfulness. We designed a pilot between subjects randomized factorial experiment of 2 (Informational Chatbot: present vs. absent) x 2 (Tutorial Video: present vs. absent) x 2 (Reflection Chatbot: present vs. absent). Our preliminary findings suggest that interaction with either of the chatbots improved the participants\u2019 intent to practice Mindfulness again, and the tutorial video improved the participants\u2019 reported overall experience of the exercise. This highlights the potential promise and outlines the directions for exploring the use of LLM-based chatbots for awareness-related interventions.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The potential benefits of using Large Language Models to improve awareness and willingness to practice mindfulness techniques and the directions for exploring the use of LLM-based chatbots for awareness-related interventions are outlined."
            },
            "score": 2
        },
        {
            "id": "b6513f37d51ef3dcdb77aa47a22e9836139bce34",
            "paperId": "b6513f37d51ef3dcdb77aa47a22e9836139bce34",
            "title": "Where It Really Matters: Few-Shot Environmental Conservation Media Monitoring for Low-Resource Languages",
            "abstract": "Environmental conservation organizations routinely monitor news content on conservation in protected areas to maintain situational awareness of developments that can have an environmental impact. Existing automated media monitoring systems require large amounts of data labeled by domain experts, which is only feasible at scale for high-resource languages like English. However, such tools are most needed in the global south where the news of interest is mainly in local low-resource languages, and far fewer experts are available to annotate datasets on a sustainable basis. In this paper, we propose NewsSerow, a method to automatically recognize environmental conservation content in low-resource languages. NewsSerow is a pipeline of summarization, in-context few-shot classification, and self-reflection using large language models (LLMs). Using at most 10 demonstration example news articles in Nepali, NewsSerow significantly outperforms other few-shot methods and can achieve comparable performance with models fully fine-tuned using thousands of examples. With NewsSerow, Organization X has been able to deploy the media monitoring tool in Nepal, significantly reducing their operational burden, and ensuring that AI tools for conservation actually reach the communities that need them the most. NewsSerow has also been deployed for countries with other languages like Colombia.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "NewsSerow is a pipeline of summarization, in-context few-shot classification, and self-reflection using large language models (LLMs) that significantly outperforms other few-shot methods and can achieve comparable performance with models fully fine-tuned using thousands of examples."
            },
            "score": 2
        },
        {
            "id": "8e0dbc206db278c29c4b70eae2060db2818f72dd",
            "paperId": "8e0dbc206db278c29c4b70eae2060db2818f72dd",
            "title": "Generalization and Hallucination of Large Vision-Language Models through a Camouflaged Lens",
            "abstract": "Large Vision-Language Model (LVLM) has seen burgeoning development and increasing attention recently. In this paper, we propose a novel framework, camo-perceptive vision-language framework (CPVLF), to explore whether LVLM can generalize to the challenging camouflaged object detection (COD) scenario in a training-free manner. During the process of generalization, we find that due to hallucination issues within LVLM, it can erroneously perceive objects in camouflaged scenes, producing counterfactual concepts. Moreover, as LVLM is not specifically trained for the precise localization of camouflaged objects, it exhibits a degree of uncertainty in accurately pinpointing these objects. Therefore, we propose chain of visual perception, which enhances LVLM's perception of camouflaged scenes from both linguistic and visual perspectives, reducing the hallucination issue and improving its capability in accurately locating camouflaged objects. We validate the effectiveness of CPVLF on three widely used COD datasets, and the experiments show the potential of LVLM in the COD task.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel framework, camo-perceptive vision-language framework (CPVLF), is proposed, which enhances LVLM's perception of camouflaged scenes from both linguistic and visual perspectives, reducing the hallucination issue and improving its capability in accurately locating camouflaged objects."
            },
            "score": 2
        },
        {
            "id": "d16f8d624ab16c8bb35dde676f522b66a771d271",
            "paperId": "d16f8d624ab16c8bb35dde676f522b66a771d271",
            "title": "Large Language Models are Null-Shot Learners",
            "abstract": "This paper presents null-shot prompting. Null-shot prompting exploits hallucination in large language models (LLMs) by instructing LLMs to utilize information from the\"Examples\"section that never exists within the provided context to perform a task. While reducing hallucination is crucial and non-negligible for daily and critical uses of LLMs, we propose that in the current landscape in which these LLMs still hallucinate, it is possible, in fact, to exploit hallucination to increase performance in performing tasks compared to standard zero-shot prompting. Experiments with eight LLMs show improvements in performance across the majority of eight datasets, including reading comprehension, arithmetic reasoning, and closed-book question answering. The observed inconsistency in increased relative performance across the LLMs also potentially indicates a different degree of inherent hallucination in each model. These differences show that it is possible to utilize null-shot prompting as a way to detect degrees of hallucination in LLMs using existing benchmarking datasets. We also perform ablation studies, including experimenting with a modified version of null-shot prompting that incorporates ideas from zero-shot chain-of-thought prompting, which shows different trends of results.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experiments with eight LLMs show improvements in performance across the majority of eight datasets, including reading comprehension, arithmetic reasoning, and closed-book question answering, and differences show that it is possible to utilize null-shot prompting as a way to detect degrees of hallucination in LLMs using existing benchmarking datasets."
            },
            "score": 2
        },
        {
            "id": "793d69ecc8fe11e0a41aad5e34e5764227552e24",
            "paperId": "793d69ecc8fe11e0a41aad5e34e5764227552e24",
            "title": "Summary Cycles: Exploring the Impact of Prompt Engineering on Large Language Models\u2019 Interaction with Interaction Log Information",
            "abstract": "With the aim of improving work efficiency, we examine how Large Language Models (LLMs) can better support the handoff of information by summarizing user interactions in collaborative intelligence analysis communication. We experiment with interaction logs, or a record of user interactions with a system. Inspired by chain-of-thought prompting, we describe a technique to avoid API token limits with recursive summarization requests. We then apply ChatGPT over multiple iterations to extract named entities, topics, and summaries, combined with interaction sequence sentences, to generate summaries of critical events and results of analysis sessions. We quantitatively evaluate the generated summaries against human-generated ones using common accuracy metrics (e.g., ROUGE-L, BLEU, BLEURT, and TER). We also report qualitative trends and the factuality of the output. We find that manipulating the audience feature or providing single-shot examples minimally influences the model\u2019s accuracy. While our methodology successfully summarizes interaction logs, the lack of significant results raises questions about prompt engineering and summarization effectiveness generally. We call on explainable artificial intelligence research to better understand how terms and their placement may change LLM outputs, striving for more consistent prompt engineering guidelines.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work applies ChatGPT over multiple iterations to extract named entities, topics, and summaries, combined with interaction sequence sentences, to generate summaries of critical events and results of analysis sessions, finding that manipulating the audience feature or providing single-shot examples minimally influences the model\u2019s accuracy."
            },
            "score": 2
        },
        {
            "id": "01bc871c0ecb7f586e54b9f67d2e50e08cbedf3d",
            "paperId": "01bc871c0ecb7f586e54b9f67d2e50e08cbedf3d",
            "title": "Models See Hallucinations: Evaluating the Factuality in Video Captioning",
            "abstract": "Video captioning aims to describe events in a video with natural language. In recent years, many works have focused on improving captioning models' performance. However, like other text generation tasks, it risks introducing factual errors not supported by the input video. These factual errors can seriously affect the quality of the generated text, sometimes making it completely unusable. Although factual consistency has received much research attention in text-to-text tasks (e.g., summarization), it is less studied in the context of vision-based text generation. In this work, we conduct a detailed human evaluation of the factuality in video captioning and collect two annotated factuality datasets. We find that 57.0% of the model-generated sentences have factual errors, indicating it is a severe problem in this field. However, existing evaluation metrics are mainly based on n-gram matching and show little correlation with human factuality annotation. We further propose a weakly-supervised, model-based factuality metric FactVC, which outperforms previous metrics on factuality evaluation of video captioning. The datasets and metrics will be released to promote future research for video captioning.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work conducts a detailed human evaluation of the factuality in video captioning and collects two annotated factuality datasets and proposes a weakly-supervised, model-based factuality metric FactVC, which outperforms previous metrics on factuality evaluation ofVideo captioning."
            },
            "score": 2
        },
        {
            "id": "6abd2c18bc5c9441465ae094eb1e93c25b3732c3",
            "paperId": "6abd2c18bc5c9441465ae094eb1e93c25b3732c3",
            "title": "Emptying the Ocean with a Spoon: Should We Edit Models?",
            "abstract": "We call into question the recently popularized method of direct model editing as a means of correcting factual errors in LLM generations. We contrast model editing with three similar but distinct approaches that pursue better defined objectives: (1) retrieval-based architectures, which decouple factual memory from inference and linguistic capabilities embodied in LLMs; (2) concept erasure methods, which aim at preventing systemic bias in generated text; and (3) attribution methods, which aim at grounding generations into identified textual sources. We argue that direct model editing cannot be trusted as a systematic remedy for the disadvantages inherent to LLMs, and while it has proven potential in improving model explainability, it opens risks by reinforcing the notion that models can be trusted for factuality. We call for cautious promotion and application of model editing as part of the LLM deployment process, and for responsibly limiting the use cases of LLMs to those not relying on editing as a critical component.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is argued that direct model editing cannot be trusted as a systematic remedy for the disadvantages inherent to LLMs, and while it has proven potential in improving model explainability, it opens risks by reinforcing the notion that models can be trusted for factuality."
            },
            "score": 2
        },
        {
            "id": "606868548ad266351ad631d3a14d7b612f82bc7f",
            "paperId": "606868548ad266351ad631d3a14d7b612f82bc7f",
            "title": "Factuality Levels of Diagnoses in Swedish Clinical Text",
            "abstract": "Different levels of knowledge certainty, or factuality levels, are expressed in clinical health record documentation. This information is currently not fully exploited, as the subtleties expressed in natural language cannot easily be machine analyzed. Extracting relevant information from knowledge-intensive resources such as electronic health records can be used for improving health care in general by e.g. building automated information access systems. We present an annotation model of six factuality levels linked to diagnoses in Swedish clinical assessments from an emergency ward. Our main findings are that overall agreement is fairly high (0.7/0.58 F-measure, 0.73/0.6 Cohen's \u03ba, Intra/Inter). These distinctions are important for knowledge models, since only approx. 50% of the diagnoses are affirmed with certainty. Moreover, our results indicate that there are patterns inherent in the diagnosis expressions themselves conveying factuality levels, showing that certainty is not only dependent on context cues.",
            "year": 2011,
            "citationCount": 26,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An annotation model of six factuality levels linked to diagnoses in Swedish clinical assessments from an emergency ward is presented, showing that certainty is not only dependent on context cues."
            },
            "score": 2
        },
        {
            "id": "9447df9a75687d4bcd0f31bc99cb199bab271766",
            "paperId": "9447df9a75687d4bcd0f31bc99cb199bab271766",
            "title": "Cognitive reappraisal of food craving and emotions: a coordinate-based meta-analysis of fMRI studies",
            "abstract": "Abstract Growing evidence supports the effectiveness of cognitive reappraisal in down-regulating food desire. Still, the neural bases of food craving down-regulation via reappraisal, as well as their degree of overlap vs specificity compared with emotion down-regulation, remain unclear. We addressed this gap through activation likelihood estimation meta-analyses of neuroimaging studies on the neural bases of (i) food craving down-regulation and (ii) emotion down-regulation, alongside conjunction and subtraction analyses among the resulting maps. Exploratory meta-analyses on activations related to food viewing compared with active regulation and up-regulation of food craving have also been performed. Food and emotion down-regulation via reappraisal consistently engaged overlapping activations in dorsolateral and ventrolateral prefrontal, posterior parietal, pre-supplementary motor and lateral posterior temporal cortices, mainly in the left hemisphere. Its distinctive association with the right anterior/posterior insula and left inferior frontal gyrus suggests that food craving down-regulation entails a more extensive integration of interoceptive information about bodily states and greater inhibitory control over the appetitive urge towards food compared with emotion down-regulation. This evidence is suggestive of unique interoceptive and motivational components elicited by food craving reappraisal, associated with distinctive patterns of fronto-insular activity. These results might inform theoretical models of food craving regulation and prompt novel therapeutic interventions for obesity and eating disorders.",
            "year": 2023,
            "citationCount": 0,
            "tldr": null,
            "score": 2
        },
        {
            "id": "25b296bdfdaabe7d30f4c02138a569fb19d278bc",
            "paperId": "25b296bdfdaabe7d30f4c02138a569fb19d278bc",
            "title": "Meta-analysis of the Correlation Between Cognitive Function and Imaging Features in Hepatic Encephalopathy",
            "abstract": "Objectives: This study aimed to systematically evaluate the correlation between cognitive function and brain imaging findings in patients with hepatic encephalopathy (HE). Methods: We searched PubMed, China National Knowledge Infrastructure, and Wanfang databases to obtain literature on cognitive function and the diagnosis of HE via brain imaging from the establishment of the databases to March 20, 2023. Two researchers independently screened the literature, extracted data, and evaluated the risk of bias in the included studies. RevMan v. 5.3 software was used in the meta-analysis. Results: A total of 14 articles were included. The meta-analysis showed that digit symbol test (DST) scores and psychometric HE scores (PHES) were positively correlated with the imaging findings of the frontal lobe in patients with HE, with correlation coefficients of 0.49 (Z = 0.53; 95% confidence interval [CI]: 0.23 ~ 0.83) and 0.52 (Fisher\u2019s Z: 0.58; 95% CI: 0.41 ~ 0.76), respectively. The number connection test-A reaction times were negatively correlated with the signal in the globus pallidus, with a correlation coefficient of -0.23 (Z = - 0.23; 95% CI: - 0.43~ - 0.03); DST scores were positively correlated with the signal in the globus pallidus, with a correlation coefficient of 0.45 (Z = 0.49; 95% CI: 0.29 ~ 0.69); and PHES were positively correlated with the mean peak of grey matter, with a correlation coefficient of 0.52 (Z = 0.57; 95% CI: 0.46 ~ 0.73). Conclusions: Imaging findings are related to the cognitive function of patients with HE. Therefore, they can be used to evaluate the cognitive function of these patients and promptly intervene in and prevent the progression of the disease.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Imaging findings are related to the cognitive function of patients with hepatic encephalopathy and can be used to evaluate the Cognitive function of these patients and promptly intervene in and prevent the progression of the disease."
            },
            "score": 2
        },
        {
            "id": "5fe2df884c0bb48137b14a8a00ce169aaad392ad",
            "paperId": "5fe2df884c0bb48137b14a8a00ce169aaad392ad",
            "title": "Meta-analysis of Controlled Oral Word Association Test (COWAT) FAS performance in amnestic mild cognitive impairment and cognitively unimpaired older adults",
            "abstract": "Abstract Findings from several studies have shown that amnestic mild cognitive impairment (aMCI) older adults have significantly lower performance on phonemic fluency tasks relative to cognitively unimpaired (CU) older adults. These findings suggest that nonmemory domains, such as executive function, are impacted in aMCI. As Alzheimer\u2019s disease (AD) research has shifted toward identifying and characterizing preclinical AD, there is a need to identify subtle but significant cognitive changes that are below the threshold for clinical impairment. The aim of this meta-analysis was to examine phonemic fluency differences between aMCI and CU older adults. Data from 18 studies were included in this analysis that found that aMCI individuals\u2019 phonemic fluency performance was approximately seven points lower than CU individuals (\u0394\u2009=\u2009\u22127.31, 95% CI [\u22129.10, \u22125.52], z\u2009=\u2009\u22128.01, p\u2009<\u20090.001), which represents a medium effect size of (g\u2009=\u20090.61, 95% CI [0.46, 0.76], z\u2009=\u20097.90, p\u2009<\u20090.001). Normative conversion of the aMCI groups\u2019 raw scores showed that all were in the normal range of performance. The findings of this meta-analysis demonstrate that significant subclinical deficits in phonemic fluency can be present in aMCI. This should prompt greater use of phonemic fluency tasks in outcome measures for observational and intervention studies.",
            "year": 2021,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The findings of this meta-analysis demonstrate that significant subclinical deficits in phonemic fluency can be present in aMCI, and should prompt greater use of phonemicfluency tasks in outcome measures for observational and intervention studies."
            },
            "score": 2
        },
        {
            "id": "d9b57ab98d289565f535d23495c19f1e4b2259fc",
            "paperId": "d9b57ab98d289565f535d23495c19f1e4b2259fc",
            "title": "A critical look at meta\u2010analytic evidence for the cognitive approach to lie detection: A re\u2010examination of Vrij, Fisher, and Blank (2017)",
            "abstract": "Vrij, Fisher, and Blank (2017) published the first meta-analysis assessing the effectiveness of Vrij\u2019s (2015) new cognitive approach to lie detection. Their basic premise is that verbal andnonverbal deception cues canbe amplified bymaking lyingmore cognitively effortful. Cues can be amplified in threeways: (1) instilling additional cognitive load, (2) prompting additional information, and (3) employing unexpected questions. Amplified cues lead to more accurate deception detection. Vrij et al. (2017) reported that their cognitive approach showed a 15-point advantage over the traditional standard approach. They claimed 71% correct classification for their cognitive approach compared to 56% for the standard approach in head-to-head experimental comparisons. This essay provides a critical look at Vrij et al.\u2019s (2017) claims, methods, and data. Vrij et al.\u2019s meta-analysis averaged across two conceptually, empirically, and pragmatically different types of outcomes. As a result of conflated outcomes, evidence for the cognitive approach was exaggerated and important patterns of findings were hidden. Specifically, accuracy in the controls and reliability in assessment correlate negativelywith support for the cognitive approach.",
            "year": 2018,
            "citationCount": 22,
            "tldr": null,
            "score": 2
        },
        {
            "id": "77766ae342ebfd4eed46e7452e36499c2a5f252c",
            "paperId": "77766ae342ebfd4eed46e7452e36499c2a5f252c",
            "title": "Leveraging Meta Data Management: Powering Cognitive Automation in Clinical Trial Processes",
            "abstract": "Today, arti\uf001cial intelligence (AI) is transforming every area of business, including life sciences, particularly in the clinical trials space. Numerous companies are already well on their way to using cognitive automation for clinical trial set up, fraud detection, pharmacovigilance, recommendation engines, crop classi\uf001cation, and so on. Systems such as natural language processing (NLP), machine learning (ML), and neural networks are becoming commercially viable solutions for deciphering clinical content and simplifying reporting for supporting business decisions and ensuring compliance. As a welcome consequence to all the clinical standard initiatives which were mainly targeting standards governance, this paper highlights how these initiatives in combination with AI can provide a real shot at optimizing clinical development processes. We explore how metadata management can play a foundational role in standardizing governance, while being a key enabler for cognitive process automation. The Evolving Compliance Landscape The pharmaceutical business is a highly process-driven space, governed by stringent local and international regulations pertaining to drug discovery, development, and testing. Companies therefore are seeking ways to increase trial ef\uf001ciency and improve outcomes while reducing overall cycle time. However, the diverse nature of trials across therapeutic areas is a primary obstacle towards optimizing the companywide process. By early last year, there were over 200,000 registered clinical 1 research studies. Depending on the therapy area and the way the organization is structured, there are many variations in terms of governance frameworks and protocols followed even within an organization. Without a standard technology-driven process for setting up clinical trials, these studies will run the risk of costly delays or fail to comply with regulatory guidelines for submissions. Over the last decade or so, the industry has been launching various standardizations such as the Clinical Data Interchange Standards Consortium (CDISC), and simpli\uf001cation such as the Transcelerate initiatives. These are aimed towards harmonizing and standardizing electronic data submissions. Drive for Cognitive Process Automation We have seen companies taking steps towards applying AI based solution across the entire scope of clinical activities. 2 GlaxoSmithKline, for example, uses AI for data management mapping raw clinical information per study data tabulation (STDM) standards. The company met initial success with 5060% accuracy and has been steadily improving. 3 Proprietary technologies such as those from Innoplexus condenses critical life sciences information to create a repository of business use cases, which make information more actionable for the end-user. WHITE PAPER",
            "year": 2018,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper explores how metadata management can play a foundational role in standardizing governance, while being a key enabler for cognitive process automation in clinical development processes."
            },
            "score": 2
        },
        {
            "id": "c152836dd29c7b2027575bc5ea8c46dc2a030e8f",
            "paperId": "c152836dd29c7b2027575bc5ea8c46dc2a030e8f",
            "title": "[Meta-Cognitive Ability and Brain Damage in Autoimmune Encephalitis: A Case Report].",
            "abstract": "Autoimmune encephalitis is an important contributor to rapidly progressive cognitive and behavioral decline. The purpose of this work was to evaluate the effects of cognitive rehabilitation in a patient with autoimmune encephalitis. We also wanted to evaluate the effectiveness of rehabilitative treatment by monitoring the cognitive and metacognitive outcomes over a time interval. We reported a case of 22 year-old female patient with autoimmune encephalitis, cognitive behavioral impairments, and severe reduction in metarepresentational capacity. We performed an assessment of personality, neuropsychological, and meta-cognitive functions at the beginning of the rehabilitative training. The last evaluation was performed six months after the discharge from the rehabilitation unit. We applied a combination of remediation, psycho-educational treatment, and psychotherapy to improve the knowledge and the empathy of the patient, to promote the selfcontrol strategies, and to prompt better behavioral management. Our findings revealed an improvement in the performance of the individual tests after rehabilitative training.",
            "year": 2018,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A combination of remediation, psycho-educational treatment, and psychotherapy is applied to improve the knowledge and the empathy of the patient, to promote the selfcontrol strategies, and to prompt better behavioral management."
            },
            "score": 2
        },
        {
            "id": "5a85761d32442b6eef8de353d356d777eb72c3eb",
            "paperId": "5a85761d32442b6eef8de353d356d777eb72c3eb",
            "title": "\u201cThe less I type, the better\u201d: How AI Language Models can Enhance or Impede Communication for AAC Users",
            "abstract": "Users of augmentative and alternative communication (AAC) devices sometimes find it difficult to communicate in real time with others due to the time it takes to compose messages. AI technologies such as large language models (LLMs) provide an opportunity to support AAC users by improving the quality and variety of text suggestions. However, these technologies may fundamentally change how users interact with AAC devices as users transition from typing their own phrases to prompting and selecting AI-generated phrases. We conducted a study in which 12 AAC users tested live suggestions from a language model across three usage scenarios: extending short replies, answering biographical questions, and requesting assistance. Our study participants believed that AI-generated phrases could save time, physical and cognitive effort when communicating, but felt it was important that these phrases reflect their own communication style and preferences. This work identifies opportunities and challenges for future AI-enhanced AAC devices.",
            "year": 2023,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work identifies opportunities and challenges for future AI-enhanced AAC devices by testing live suggestions from a language model across three usage scenarios: extending short replies, answering biographical questions, and requesting assistance."
            },
            "score": 1
        },
        {
            "id": "6589da60a2f577d315ed929517e11510d4f2ae32",
            "paperId": "6589da60a2f577d315ed929517e11510d4f2ae32",
            "title": "Introducing Meta-analysis in the Evaluation of Computational Models of Infant Language Development",
            "abstract": "Computational models of child language development can help us understand the cognitive underpinnings of the language learning process, which occurs along several linguistic levels at once (e.g., prosodic and phonological). However, in light of the replication crisis, modelers face the challenge of selecting representative and consolidated infant data. Thus, it is desirable to have evaluation methodologies that could account for robust empirical reference data, across multiple infant capabilities. Moreover, there is a need for practices that can compare developmental trajectories of infants to those of models as a function of language experience and development. The present study aims to take concrete steps to address these needs by introducing the concept of comparing models with large-scale cumulative empirical data from infants, as quantified by meta-analyses conducted across a large number of individual behavioral studies. We formalize the connection between measurable model and human behavior, and then present a conceptual framework for meta-analytic evaluation of computational models. We exemplify the meta-analytic model evaluation approach with two modeling experiments on infant-directed speech preference and native/non-native vowel discrimination.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The concept of comparing models with large-scale cumulative empirical data from infants, as quantified by meta-analyses conducted across a large number of individual behavioral studies is introduced."
            },
            "score": 1
        },
        {
            "id": "9073ff59cfc1a3d6f3f50a2723e80ee50767176b",
            "paperId": "9073ff59cfc1a3d6f3f50a2723e80ee50767176b",
            "title": "A model and method for \"making\" a Combined-Integrated psychologist: equilintegration (EI) theory and the beliefs, events, and values inventory (BEVI).",
            "abstract": "Although the Consensus Conference on Combined and Integrated Doctoral Training in Psychology (e.g., Bailey, 2003) generated much content of relevance to the structure and commitments of Combined-Integrated (C-I) programs, faculty, and students-and Competencies 2002: Future Directions in Education and Credentialing in Professional Psychology (www.appic.org) developed language and guidelines regarding the knowledge areas, skills, and values that students in professional psychology programs should acquire and demonstrate-specific models and methods are necessary to translate these professional guidelines and aspirations into reality. This article offers one such model, Equilintegration (EI) Theory, and method, the Beliefs, Events, and Values Inventory (BEVI), that can be used by faculty, training staff, supervisors, and students in C-I programs to operationalize, assess, and cultivate basic values of education and training from a C-I perspective (e.g., self-awareness, self-assessment, and self-reflection). In addition to this model and method, relevant background information, theory, and research are presented along with attendant implications, hypotheses, and principles.",
            "year": 2004,
            "citationCount": 39,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "One such model, Equilintegration (EI) Theory, and method, that can be used by faculty, training staff, supervisors, and students in C-I programs to operationalize, assess, and cultivate basic values of education and training from a C- I perspective are offered."
            },
            "score": 1
        },
        {
            "id": "5eb6979460b2d246a01f3fdc22c4c07530fb619f",
            "paperId": "5eb6979460b2d246a01f3fdc22c4c07530fb619f",
            "title": "University American Sign Language Learners: Self-Awareness and Error Analysis within a Narrative Context",
            "abstract": "Abstract:Second-language learners of American Sign Language (ASL) often struggle in the acquisition of more complex ASL aspects, such as role shift, constructed action, and eye gaze to represent characters and their actions with narratives. These learners also often overestimate their ASL skill level. This study investigated errors in second modality, second language (M2L2) university learners' ASL narrative retellings through rating comparisons (i.e., agreements and disagreements) between learners themselves and their instructor. Learners engaged in repeated viewings of ASL models of two Signing Naturally stories, Timber and Gum, paired with instructor analysis, modeling, and feedback on learners' productions within a sociocultural framework. Learners' self-evaluations were compared to instructor evaluations using an itemized list of narrative episode details and options for how they were produced to identify where these evaluations were in agreement or not. Overall agreement was high (i.e., M = 92%), however, agreement across specific ASL aspects, including role shift, constructed action, and accurate sign production, varied to a greater extent (i.e., 89\u201394%). Two types of disagreements occurred: those in which learners rated a detail as absent while the instructor rated it as present, and those in which learners rated a detail as present while the instructor rated it as absent. Omissions occurred infrequently, in which both the learner and instructor agreed that a detail was absent. The specific instructional framework used in this study and implications for university-level ASL instruction are discussed in this article.",
            "year": 2022,
            "citationCount": 1,
            "tldr": null,
            "score": 1
        },
        {
            "id": "b60d3726cff30ff41714887edf517f6aed074e45",
            "paperId": "b60d3726cff30ff41714887edf517f6aed074e45",
            "title": "Clinical Hypnosis as an Adjunct to Cognitive Behavior Therapy: An Updated Meta-Analysis",
            "abstract": "ABSTRACT In 1995, Kirsch and colleagues published an influential meta-analysis (k = 20, N = 577) which found that CBT enhanced with hypnosis (CBTH) was superior to CBT alone by at least d = .53. However, a lack of full replication and the emergence of new empirical studies prompted this updated analysis. A total of 48 post- (N = 1,928) and 25 follow-up treatments (N = 1,165) were meta-analyzed. CBTH achieved small to medium but statistically significant advantages over CBT at posttreatment (dIGPP/d = .25 to .41), and specifically in the management of depressed mood and pain. At follow-up, there was a medium sized advantage for CBTH (dIGPP/d = .54 to .59), and specifically for the treatment of obesity. These results further support the adjunctive use of hypnosis as an enhancer of CBT\u2019s efficaciousness and endurance as a treatment.",
            "year": 2021,
            "citationCount": 29,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results of this updated analysis support the adjunctive use of hypnosis as an enhancer of CBT\u2019s efficaciousness and endurance as a treatment."
            },
            "score": 1
        }
    ],
    "novelty": "no"
}