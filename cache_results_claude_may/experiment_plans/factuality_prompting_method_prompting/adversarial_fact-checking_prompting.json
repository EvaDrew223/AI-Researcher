{
    "topic_description": "novel prompting methods that can improve factuality and reduce hallucination of large language models",
    "idea_name": "Adversarial Fact-Checking Prompting",
    "raw_idea": {
        "Problem": "Large language models can generate hallucinated content that is not supported by the input context or external knowledge, leading to factual inaccuracies in the generated output.",
        "Existing Methods": "Existing methods for reducing hallucination include using retrieval-augmented generation, incorporating external knowledge bases, or using reinforcement learning with factual consistency rewards.",
        "Motivation": "We hypothesize that large language models can be prompted to fact-check their own generated output, if framed as an adversarial game between a generator and a fact-checker. By explicitly prompting the model to generate both the initial output and a critique of its own output from an adversarial fact-checking perspective, we can encourage the model to generate more factually consistent output and catch its own mistakes.",
        "Proposed Method": "We propose Adversarial Fact-Checking Prompting, a novel prompting approach to reduce hallucination and improve factual accuracy. First, we prompt the model to generate an initial output given the input context. Second, we prompt the model to critique its own generated output from an adversarial fact-checking perspective, identifying any factual inaccuracies, inconsistencies, or unsupported claims. Third, we prompt the model to revise its initial output to address the critiques and generate a more factually consistent final output. We can repeat steps 2-3 for multiple rounds of adversarial fact-checking and revision.",
        "Experiment Plan": "We will evaluate our proposed method on factual consistency benchmarks such as the Fact Extraction and VERification (FEVER) dataset and the Fact-Checking Assistant task. We will compare our method with baseline prompting approaches without adversarial fact-checking, as well as state-of-the-art methods for reducing hallucination. We will also analyze the quality and accuracy of the generated fact-checking critiques across multiple rounds of interaction."
    },
    "full_experiment_plan": {
        "Title": "Adversarial Fact-Checking Prompting for Reducing Hallucination in Large Language Models",
        "Problem Statement": "Large language models can generate hallucinated content that is not supported by the input context or external knowledge, leading to factual inaccuracies in the generated output. This is a significant problem that hinders the reliability and trustworthiness of these models in real-world applications.",
        "Motivation": "Existing methods for reducing hallucination, such as retrieval-augmented generation, incorporating external knowledge bases, or using reinforcement learning with factual consistency rewards, have shown promising results but still face challenges in effectively mitigating the issue. We hypothesize that large language models can be prompted to fact-check their own generated output, if framed as an adversarial game between a generator and a fact-checker. By explicitly prompting the model to generate both the initial output and a critique of its own output from an adversarial fact-checking perspective, we can encourage the model to generate more factually consistent output and catch its own mistakes.",
        "Proposed Method": "We propose Adversarial Fact-Checking Prompting, a novel prompting approach to reduce hallucination and improve factual accuracy. The method consists of three main steps:\n1. Initial Generation: Prompt the model to generate an initial output given the input context.\n2. Adversarial Fact-Checking: Prompt the model to critique its own generated output from an adversarial fact-checking perspective, identifying any factual inaccuracies, inconsistencies, or unsupported claims.\n3. Revision: Prompt the model to revise its initial output to address the critiques and generate a more factually consistent final output.\nWe can repeat steps 2-3 for multiple rounds of adversarial fact-checking and revision to further refine the output.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Dataset Selection": "Evaluate the proposed method on factual consistency benchmarks such as the Fact Extraction and VERification (FEVER) dataset and the Fact-Checking Assistant task. These datasets provide a diverse set of claims and corresponding evidence to assess the factual accuracy of generated outputs.",
            "Step 2: Baseline Methods": "Compare the proposed method with the following baseline prompting approaches:\n1. Direct Prompting: Prompt the model to generate the output directly without any fact-checking or revision.\n2. Self-Consistency Prompting: Prompt the model to generate multiple outputs and select the most consistent one.\n3. Knowledge-Augmented Prompting: Incorporate relevant facts from external knowledge bases into the prompt to guide the generation process.",
            "Step 3: Prompt Engineering": "Design prompts for each step of the proposed method:\n1. Initial Generation Prompt: 'Given the following context, generate a response: [context]'\n2. Adversarial Fact-Checking Prompt: 'Critically analyze the following generated output for any factual inaccuracies, inconsistencies, or unsupported claims: [generated_output]'\n3. Revision Prompt: 'Revise the following generated output to address the identified factual issues: [generated_output] [fact_checking_critique]'",
            "Step 4: Model Selection": "Experiment with state-of-the-art large language models such as GPT-3.5 (text-davinci-002) and GPT-4 to evaluate the effectiveness of the proposed method across different model sizes and capabilities.",
            "Step 5: Evaluation Metrics": "Assess the factual consistency of the generated outputs using the following metrics:\n1. Accuracy: Percentage of generated outputs that are factually consistent with the input context and external knowledge.\n2. F1 Score: Harmonic mean of precision and recall, considering both the correctness and completeness of the generated facts.\n3. Human Evaluation: Conduct a human evaluation to assess the factual consistency, coherence, and overall quality of the generated outputs.",
            "Step 6: Iterative Refinement": "Investigate the impact of multiple rounds of adversarial fact-checking and revision on the factual consistency of the generated outputs. Vary the number of iterations (e.g., 1, 2, 3) and analyze the trade-off between factual accuracy and computational cost.",
            "Step 7: Error Analysis": "Perform a qualitative analysis of the generated outputs to identify common types of factual errors and limitations of the proposed method. Use these insights to guide future improvements and research directions.",
            "Step 8: Ablation Study": "Conduct an ablation study to understand the contribution of each component of the proposed method. Evaluate the performance of the model with and without adversarial fact-checking and revision steps to assess their individual impact on factual consistency."
        },
        "Test Case Examples": {
            "Example 1": {
                "Input": "The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower.",
                "Baseline Output (Direct Prompting)": "The Eiffel Tower is a famous landmark located in Paris, France. It was designed by the French architect Gustave Eiffel and completed in 1889. The tower stands at a height of 324 meters and is made of wrought-iron. It was originally built as the entrance arch for the 1889 World's Fair and has since become a global cultural icon of France.",
                "Proposed Method Output": "Initial Generation: The Eiffel Tower is an iconic landmark in Paris, France. It was designed and built by the company of engineer Gustave Eiffel, after whom the tower is named. The tower is constructed using wrought-iron lattice and stands on the Champ de Mars. It was completed in 1889 and has a height of 324 meters, making it the tallest structure in Paris.\n\nAdversarial Fact-Checking: The generated output is mostly factually consistent with the given context. However, there are a few unsupported claims:\n1. The output mentions that the tower was completed in 1889, but this information is not provided in the input context.\n2. The output states that the Eiffel Tower is the tallest structure in Paris, but this claim is not supported by the given context.\n\nRevised Output: The Eiffel Tower is an iconic landmark located on the Champ de Mars in Paris, France. It is a wrought-iron lattice tower designed and built by the company of engineer Gustave Eiffel, after whom the tower is named. The tower stands at a height of 324 meters.",
                "Explanation": "The proposed method generates an initial output that contains some unsupported claims not present in the input context. The adversarial fact-checking step identifies these unsupported claims, and the revision step generates a more factually consistent output by removing the unsupported information."
            },
            "Example 2": {
                "Input": "The Great Wall of China is a series of fortifications and walls built across the historical northern borders of ancient Chinese states and Imperial China to protect against nomadic invasions from the Eurasian Steppe.",
                "Baseline Output (Knowledge-Augmented Prompting)": "The Great Wall of China is an ancient series of walls and fortifications built along the northern borders of Imperial China. It was constructed to protect the Chinese Empire and its territories against nomadic invasions from the Eurasian Steppe. The wall stretches over 21,000 kilometers from the Shanhaiguan district in the east to the Lop Lake in the west. It is made of stone, brick, tamped earth, and other materials. The Great Wall is not a single continuous wall, but rather a series of multiple walls and fortifications built by various dynasties over centuries. Construction of the first sections of the wall began during the Qin Dynasty (221-206 BCE) and continued through the Ming Dynasty (1368-1644 CE). Today, the Great Wall is a UNESCO World Heritage Site and a popular tourist attraction, attracting millions of visitors each year.",
                "Proposed Method Output": "Initial Generation: The Great Wall of China is a series of fortifications and walls constructed along the historical northern borders of ancient Chinese states and Imperial China. Its primary purpose was to protect against nomadic invasions originating from the Eurasian Steppe. The wall spans over 21,000 kilometers from the eastern Shanhaiguan district to the western Lop Lake. It is composed of various materials, including stone, brick, and tamped earth. The Great Wall is not a single continuous structure but rather a collection of multiple walls and fortifications built by different dynasties over several centuries. The initial construction began during the Qin Dynasty (221-206 BCE) and continued through the Ming Dynasty (1368-1644 CE). In modern times, the Great Wall has been designated as a UNESCO World Heritage Site and attracts millions of tourists annually.\n\nAdversarial Fact-Checking: The generated output provides additional information that is not directly supported by the given input context:\n1. The output mentions specific details about the length and endpoints of the Great Wall (21,000 kilometers from Shanhaiguan to Lop Lake), but these facts are not present in the input.\n2. The output states that the Great Wall is a UNESCO World Heritage Site and a popular tourist attraction, attracting millions of visitors each year. While these claims may be true, they are not supported by the provided context.\n\nRevised Output: The Great Wall of China is a series of fortifications and walls constructed along the historical northern borders of ancient Chinese states and Imperial China. Its main purpose was to protect against nomadic invasions from the Eurasian Steppe. The Great Wall is not a single continuous structure but rather a collection of multiple walls and fortifications built by different dynasties over several centuries. The initial construction began during the Qin Dynasty (221-206 BCE) and continued through the Ming Dynasty (1368-1644 CE).",
                "Explanation": "The knowledge-augmented prompting baseline generates an output that contains several additional facts not present in the input context, such as the specific length and endpoints of the Great Wall and its status as a UNESCO World Heritage Site. The proposed method identifies these unsupported claims during the adversarial fact-checking step and generates a revised output that omits the unsupported information, resulting in a more factually consistent response."
            }
        },
        "Fallback Plan": "If the proposed Adversarial Fact-Checking Prompting method does not significantly improve the factual consistency of the generated outputs compared to the baselines, consider the following alternative approaches:\n1. Analyze the quality and accuracy of the generated fact-checking critiques to identify potential weaknesses in the adversarial fact-checking step. Investigate whether the model struggles to identify certain types of factual errors or generates critiques that are not sufficiently informative for the revision step.\n2. Experiment with different prompt formulations and instructions to guide the model in generating more effective fact-checking critiques and revisions. Explore alternative prompting strategies, such as providing more explicit instructions or using few-shot examples to demonstrate the desired behavior.\n3. Investigate the impact of domain-specific knowledge on the factual consistency of the generated outputs. Incorporate domain-specific facts or knowledge bases into the prompts to provide additional context and constraints for the generation process.\n4. Conduct a detailed error analysis to identify common patterns and sources of factual inconsistencies in the generated outputs. Use these insights to develop targeted strategies for addressing specific types of errors, such as incorporating additional fact-checking steps or using domain-specific fact-checking models.\n5. If the proposed method does not yield significant improvements, consider pivoting the project to focus on analyzing the limitations and challenges of fact-checking in large language models. Investigate the factors that contribute to the generation of hallucinated content and explore alternative approaches for mitigating this issue, such as combining multiple fact-checking techniques or leveraging human feedback to guide the model towards more factually consistent outputs."
    },
    "novelty_queries": [
        "KeywordQuery(\"adversarial fact-checking language models\")",
        "KeywordQuery(\"reducing hallucination language models\")",
        "KeywordQuery(\"self-consistency prompting language models\")",
        "KeywordQuery(\"factual consistency large language models\")",
        "KeywordQuery(\"Adversarial Fact-Checking Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "85b5068d3e1364b44ec9f46b0930b521b4089df6",
            "paperId": "85b5068d3e1364b44ec9f46b0930b521b4089df6",
            "title": "Self-Checker: Plug-and-Play Modules for Fact-Checking with Large Language Models",
            "abstract": "Fact-checking is an essential task in NLP that is commonly utilized for validating the factual accuracy of claims. Prior work has mainly focused on fine-tuning pre-trained languages models on specific datasets, which can be computationally intensive and time-consuming. With the rapid development of large language models (LLMs), such as ChatGPT and GPT-3, researchers are now exploring their in-context learning capabilities for a wide range of tasks. In this paper, we aim to assess the capacity of LLMs for fact-checking by introducing Self-Checker, a framework comprising a set of plug-and-play modules that facilitate fact-checking by purely prompting LLMs in an almost zero-shot setting. This framework provides a fast and efficient way to construct fact-checking systems in low-resource environments. Empirical results demonstrate the potential of Self-Checker in utilizing LLMs for fact-checking. However, there is still significant room for improvement compared to SOTA fine-tuned models, which suggests that LLM adoption could be a promising approach for future fact-checking research.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Self-Checker, a framework comprising a set of plug-and-play modules that facilitate fact-checking by purely prompting LLMs in an almost zero-shot setting, is introduced, which provides a fast and efficient way to construct fact- checking systems in low-resource environments."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is reducing hallucination in large language models, and the proposed approach is adversarial fact-checking prompting.\n\nThe research problem in the paper is assessing the capacity of large language models for fact-checking, and the proposed approach is introducing Self-Checker, a framework of plug-and-play modules for fact-checking by prompting large language models in an almost zero-shot setting.\n\nWhile both the proposal and the paper focus on fact-checking and large language models, the specific research problems and approaches are different. The proposal aims to reduce hallucination in large language models, while the paper assesses the capacity of large language models for fact-checking. The proposal uses adversarial fact-checking prompting, while the paper introduces a framework of plug-and-play modules for prompting.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "6489640b1d30a8a3e7cb906bb6557f1ccd0d799d",
            "paperId": "6489640b1d30a8a3e7cb906bb6557f1ccd0d799d",
            "title": "Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models",
            "abstract": "Retrieval-augmented language models (RALMs) represent a substantial advancement in the capabilities of large language models, notably in reducing factual hallucination by leveraging external knowledge sources. However, the reliability of the retrieved information is not always guaranteed. The retrieval of irrelevant data can lead to misguided responses, and potentially causing the model to overlook its inherent knowledge, even when it possesses adequate information to address the query. Moreover, standard RALMs often struggle to assess whether they possess adequate knowledge, both intrinsic and retrieved, to provide an accurate answer. In situations where knowledge is lacking, these systems should ideally respond with\"unknown\"when the answer is unattainable. In response to these challenges, we introduces Chain-of-Noting (CoN), a novel approach aimed at improving the robustness of RALMs in facing noisy, irrelevant documents and in handling unknown scenarios. The core idea of CoN is to generate sequential reading notes for retrieved documents, enabling a thorough evaluation of their relevance to the given question and integrating this information to formulate the final answer. We employed ChatGPT to create training data for CoN, which was subsequently trained on an LLaMa-2 7B model. Our experiments across four open-domain QA benchmarks show that RALMs equipped with CoN significantly outperform standard RALMs. Notably, CoN achieves an average improvement of +7.9 in EM score given entirely noisy retrieved documents and +10.5 in rejection rates for real-time questions that fall outside the pre-training knowledge scope.",
            "year": 2023,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Chain-of-Noting (CoN) is introduced, a novel approach aimed at improving the robustness of RALMs in facing noisy, irrelevant documents and in handling unknown scenarios, and achieves an average improvement of +7.9 in EM score given entirely noisy retrieved documents and +10.5 in rejection rates for real-time questions that fall outside the pre-training knowledge scope."
            },
            "score": 7,
            "novelty_score": "The research problem in the project proposal is reducing hallucination in large language models, and the proposed approach is adversarial fact-checking prompting.\n\nThe research problem in the paper is enhancing robustness in retrieval-augmented language models, and the proposed approach is Chain-of-Note, which generates sequential reading notes for retrieved documents to evaluate their relevance and integrate the information for the final answer.\n\nWhile both works aim to improve the factual accuracy of language models, the project proposal focuses on reducing hallucination in standalone models through adversarial prompting, while the paper focuses on enhancing robustness in retrieval-augmented models by generating reading notes for retrieved documents. The approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "03764434729b83d4f04a8bd02f99f2500cd5bbae",
            "paperId": "03764434729b83d4f04a8bd02f99f2500cd5bbae",
            "title": "Teaching Language Models to Hallucinate Less with Synthetic Tasks",
            "abstract": "Large language models (LLMs) frequently hallucinate on abstractive summarization tasks such as document-based question-answering, meeting summarization, and clinical report generation, even though all necessary information is included in context. However, optimizing LLMs to hallucinate less on these tasks is challenging, as hallucination is hard to efficiently evaluate at each optimization step. In this work, we show that reducing hallucination on a synthetic task can also reduce hallucination on real-world downstream tasks. Our method, SynTra, first designs a synthetic task where hallucinations are easy to elicit and measure. It next optimizes the LLM's system message via prefix-tuning on the synthetic task, and finally transfers the system message to realistic, hard-to-optimize tasks. Across three realistic abstractive summarization tasks, SynTra reduces hallucination for two 13B-parameter LLMs using only a synthetic retrieval task for supervision. We also find that optimizing the system message rather than the model weights can be critical; fine-tuning the entire model on the synthetic task can counterintuitively increase hallucination. Overall, SynTra demonstrates that the extra flexibility of working with synthetic data can help mitigate undesired behaviors in practice.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Across three realistic abstractive summarization tasks, SynTra reduces hallucination for two 13B-parameter LLMs using only a synthetic retrieval task for supervision, demonstrating that the extra flexibility of working with synthetic data can help mitigate undesired behaviors in practice."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is reducing hallucination in large language models, and the proposed approach is adversarial fact-checking prompting.\n\nThe research problem in the paper is also reducing hallucination in large language models, but the proposed approach is using synthetic tasks to optimize the model's system message.\n\nWhile both works aim to address the same problem of reducing hallucination, they propose different methods: adversarial fact-checking prompting vs. synthetic task optimization.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "696bc5ba0d023822bbee6b878a71ea2e4a4b0e5a",
            "paperId": "696bc5ba0d023822bbee6b878a71ea2e4a4b0e5a",
            "title": "N-Critics: Self-Refinement of Large Language Models with Ensemble of Critics",
            "abstract": "We propose a self-correction mechanism for Large Language Models (LLMs) to mitigate issues such as toxicity and fact hallucination. This method involves refining model outputs through an ensemble of critics and the model's own feedback. Drawing inspiration from human behavior, we explore whether LLMs can emulate the self-correction process observed in humans who often engage in self-reflection and seek input from others to refine their understanding of complex topics. Our approach is model-agnostic and can be applied across various domains to enhance trustworthiness by addressing fairness, bias, and robustness concerns. We consistently observe performance improvements in LLMs for reducing toxicity and correcting factual errors.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a self-correction mechanism for Large Language Models (LLMs) to mitigate issues such as toxicity and fact hallucination and consistently observe performance improvements in LLMs for reducing toxicity and correcting factual errors."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is reducing hallucination in large language models, and the proposed approach is adversarial fact-checking prompting.\n\nThe research problem in the paper is mitigating issues such as toxicity and fact hallucination in large language models, and the proposed approach is self-refinement using an ensemble of critics and the model's own feedback.\n\nWhile both the proposal and the paper aim to address the issue of fact hallucination in large language models, the proposed approaches are different. The proposal focuses on adversarial fact-checking prompting, while the paper explores self-refinement using an ensemble of critics and the model's own feedback.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "7c1707db9aafd209aa93db3251e7ebd593d55876",
            "paperId": "7c1707db9aafd209aa93db3251e7ebd593d55876",
            "title": "SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models",
            "abstract": "Generative Large Language Models (LLMs) such as GPT-3 are capable of generating highly fluent responses to a wide variety of user prompts. However, LLMs are known to hallucinate facts and make non-factual statements which can undermine trust in their output. Existing fact-checking approaches either require access to the output probability distribution (which may not be available for systems such as ChatGPT) or external databases that are interfaced via separate, often complex, modules. In this work, we propose\"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database. SelfCheckGPT leverages the simple idea that if an LLM has knowledge of a given concept, sampled responses are likely to be similar and contain consistent facts. However, for hallucinated facts, stochastically sampled responses are likely to diverge and contradict one another. We investigate this approach by using GPT-3 to generate passages about individuals from the WikiBio dataset, and manually annotate the factuality of the generated passages. We demonstrate that SelfCheckGPT can: i) detect non-factual and factual sentences; and ii) rank passages in terms of factuality. We compare our approach to several baselines and show that our approach has considerably higher AUC-PR scores in sentence-level hallucination detection and higher correlation scores in passage-level factuality assessment compared to grey-box methods.",
            "year": 2023,
            "citationCount": 155,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes \"SelfCheckGPT\", a simple sampling-based approach that can be used to fact-check the responses of black-box models in a zero-resource fashion, i.e. without an external database, and demonstrates that it can detect non-factual and factual sentences and rank passages in terms of factuality."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is reducing hallucination in large language models, and the proposed approach is adversarial fact-checking prompting. The research problem in the paper is also hallucination detection in large language models, but the proposed approach is different - it uses a sampling-based method called SelfCheckGPT that does not require external resources.\n\nThe proposal focuses on prompting the model to fact-check its own generated output in an adversarial manner to encourage more factually consistent generation. In contrast, the paper proposes a method to detect hallucinations by comparing sampled responses and checking for inconsistencies, without actively trying to reduce hallucinations during generation.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "bcdaf6c98ddbd6809cf6241aa77200d7394db163",
            "paperId": "bcdaf6c98ddbd6809cf6241aa77200d7394db163",
            "title": "CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing",
            "abstract": "Recent developments in large language models (LLMs) have been impressive. However, these models sometimes show inconsistencies and problematic behavior, such as hallucinating facts, generating flawed code, or creating offensive and toxic content. Unlike these models, humans typically utilize external tools to cross-check and refine their initial content, like using a search engine for fact-checking, or a code interpreter for debugging. Inspired by this observation, we introduce a framework called CRITIC that allows LLMs, which are essentially\"black boxes\"to validate and progressively amend their own outputs in a manner similar to human interaction with tools. More specifically, starting with an initial output, CRITIC interacts with appropriate tools to evaluate certain aspects of the text, and then revises the output based on the feedback obtained during this validation process. Comprehensive evaluations involving free-form question answering, mathematical program synthesis, and toxicity reduction demonstrate that CRITIC consistently enhances the performance of LLMs. Meanwhile, our research highlights the crucial importance of external feedback in promoting the ongoing self-improvement of LLMs.",
            "year": 2023,
            "citationCount": 120,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A framework called CRITIC is introduced that allows LLMs, which are essentially\"black boxes\" to validate and progressively amend their own outputs in a manner similar to human interaction with tools."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is reducing hallucination in large language models, and the proposed approach is adversarial fact-checking prompting. The research problem in the paper is also reducing inconsistencies and problematic behavior (including hallucination) in large language models, and the proposed approach is also using external tools to validate and amend the model's own outputs.\n\nBoth the proposal and the paper aim to address the issue of hallucination and inconsistencies in large language models by utilizing external feedback or validation to refine the model's outputs.\n\nYes",
            "novelty_judgment": "yes"
        },
        {
            "id": "f41977c497c96c1da2e9e945315e9be6d6ad472e",
            "paperId": "f41977c497c96c1da2e9e945315e9be6d6ad472e",
            "title": "Towards reducing hallucination in extracting information from financial reports using Large Language Models",
            "abstract": "For a financial analyst, the question and answer (Q\\&A) segment of the company financial report is a crucial piece of information for various analysis and investment decisions. However, extracting valuable insights from the Q\\&A section has posed considerable challenges as the conventional methods such as detailed reading and note-taking lack scalability and are susceptible to human errors, and Optical Character Recognition (OCR) and similar techniques encounter difficulties in accurately processing unstructured transcript text, often missing subtle linguistic nuances that drive investor decisions. Here, we demonstrate the utilization of Large Language Models (LLMs) to efficiently and rapidly extract information from earnings report transcripts while ensuring high accuracy transforming the extraction process as well as reducing hallucination by combining retrieval-augmented generation technique as well as metadata. We evaluate the outcomes of various LLMs with and without using our proposed approach based on various objective metrics for evaluating Q\\&A systems, and empirically demonstrate superiority of our method.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work demonstrates the utilization of Large Language Models (LLMs) to efficiently and rapidly extract information from earnings report transcripts while ensuring high accuracy transforming the extraction process as well as reducing hallucination by combining retrieval-augmented generation technique aswell as metadata."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to reduce hallucination in large language models by using adversarial fact-checking prompting, while the paper focuses on reducing hallucination in extracting information from financial reports using retrieval-augmented generation and metadata.\n\nProject Proposal: Reduce hallucination in large language models using adversarial fact-checking prompting.\nPaper: Reduce hallucination in extracting information from financial reports using retrieval-augmented generation and metadata.\n\nAlthough both works aim to reduce hallucination, they target different domains (general language models vs. financial report information extraction) and propose different approaches (adversarial fact-checking prompting vs. retrieval-augmented generation with metadata).\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "798963674902f741c3ea9298403eb8384c099a42",
            "paperId": "798963674902f741c3ea9298403eb8384c099a42",
            "title": "Factored Verification: Detecting and Reducing Hallucination in Summaries of Academic Papers",
            "abstract": "Hallucination plagues even frontier LLMs--but how bad is it really for summarizing academic papers? We evaluate Factored Verification, a simple automated method for detecting hallucinations in abstractive summaries. This method sets a new SotA on hallucination detection in the summarization task of the HaluEval benchmark, achieving 76.2% accuracy. We then use this method to estimate how often language models hallucinate when summarizing across multiple academic papers and find 0.62 hallucinations in the average ChatGPT (16k) summary, 0.84 for GPT-4, and 1.55 for Claude 2. We ask models to self-correct using Factored Critiques and find that this lowers the number of hallucinations to 0.49 for ChatGPT, 0.46 for GPT-4, and 0.95 for Claude 2. The hallucinations we find are often subtle, so we advise caution when using models to synthesize academic papers.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Factored Verification, a simple automated method for detecting hallucinations in abstractive summaries, is evaluated and sets a new SotA on hallucination detection in the summarization task of the HaluEval benchmark, achieving 76.2% accuracy."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is reducing hallucination in large language models, and the proposed approach is adversarial fact-checking prompting. The research problem in the paper is also detecting and reducing hallucination, but the proposed approach is factored verification.\n\nProposal: Reducing hallucination in large language models using adversarial fact-checking prompting.\nPaper: Detecting and reducing hallucination in summaries of academic papers using factored verification.\n\nWhile both works aim to address the problem of hallucination, they propose different methods: adversarial fact-checking prompting vs. factored verification. The proposal focuses on prompting techniques, while the paper uses an automated detection method.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "889feabe31ba0d24c093ac94d54a06eecb87e3f4",
            "paperId": "889feabe31ba0d24c093ac94d54a06eecb87e3f4",
            "title": "Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding",
            "abstract": "Dialogue systems powered by large pre-trained language models exhibit an innate ability to deliver fluent and natural-sounding responses. Despite their impressive performance, these models are fitful and can often generate factually incorrect statements impeding their widespread adoption. In this paper, we focus on the task of improving faithfulness and reducing hallucination of neural dialogue systems to known facts supplied by a Knowledge Graph (KG). We propose Neural Path Hunter which follows a generate-then-refine strategy whereby a generated response is amended using the KG. Neural Path Hunter leverages a separate token-level fact critic to identify plausible sources of hallucination followed by a refinement stage that retrieves correct entities by crafting a query signal that is propagated over a k-hop subgraph. We empirically validate our proposed approach on the OpenDialKG dataset (Moon et al., 2019) against a suite of metrics and report a relative improvement of faithfulness over dialogue responses by 20.35% based on FeQA (Durmus et al., 2020). The code is available at https://github.com/nouhadziri/Neural-Path-Hunter.",
            "year": 2021,
            "citationCount": 80,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes Neural Path Hunter which follows a generate-then-refine strategy whereby a generated response is amended using the KG, and leverages a separate token-level fact critic to identify plausible sources of hallucination and retrieves correct entities by crafting a query signal that is propagated over a k-hop subgraph."
            },
            "score": 6,
            "novelty_score": "The research problem in the project proposal is reducing hallucination in large language models, and the proposed approach is adversarial fact-checking prompting.\n\nThe research problem in the paper is improving faithfulness and reducing hallucination in neural dialogue systems, and the proposed approach is a generate-then-refine strategy using a knowledge graph.\n\nWhile both the project proposal and the paper aim to reduce hallucination, the project focuses on large language models in general, while the paper specifically targets dialogue systems. Additionally, the proposed approaches differ: the project uses adversarial fact-checking prompting, while the paper employs a generate-then-refine strategy with a knowledge graph.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "79429814fd4d967b9277af2805c53f370e52ebb5",
            "paperId": "79429814fd4d967b9277af2805c53f370e52ebb5",
            "title": "Chain of Natural Language Inference for Reducing Large Language Model Ungrounded Hallucinations",
            "abstract": "Large language models (LLMs) can generate fluent natural language texts when given relevant documents as background context. This ability has attracted considerable interest in developing industry applications of LLMs. However, LLMs are prone to generate hallucinations that are not supported by the provided sources. In this paper, we propose a hierarchical framework to detect and mitigate such ungrounded hallucination. Our framework uses Chain of Natural Language Inference (CoNLI) for hallucination detection and hallucination reduction via post-editing. Our approach achieves state-of-the-art performance on hallucination detection and enhances text quality through rewrite, using LLMs without any fine-tuning or domain-specific prompt engineering. We show that this simple plug-and-play framework can serve as an effective choice for hallucination detection and reduction, achieving competitive performance across various contexts.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a hierarchical framework to detect and mitigate ungrounded hallucination, using Chain of Natural Language Inference (CoNLI) for hallucination detection and hallucination reduction via post-editing and shows that this simple plug-and-play framework can serve as an effective choice for hallucinations detection and reduction, achieving competitive performance across various contexts."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to reduce hallucination in large language models by using adversarial fact-checking prompting, where the model critiques its own generated output and revises it for better factual consistency.\n\nThe paper proposes a hierarchical framework using Chain of Natural Language Inference (CoNLI) for hallucination detection and reduction via post-editing, without fine-tuning or domain-specific prompt engineering.\n\nWhile both the project proposal and the paper address the problem of reducing hallucination in large language models, their approaches differ. The project proposal focuses on adversarial fact-checking prompting, while the paper uses a hierarchical framework based on natural language inference for detection and post-editing.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
            "paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
            "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "abstract": "Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).",
            "year": 2022,
            "citationCount": 1396,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting that first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths."
            },
            "score": 6
        },
        {
            "id": "261549439aebdda72b648ecc462448fd24857ac1",
            "paperId": "261549439aebdda72b648ecc462448fd24857ac1",
            "title": "Progressive-Hint Prompting Improves Reasoning in Large Language Models",
            "abstract": "The performance of Large Language Models (LLMs) in reasoning tasks depends heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency being critical methods that enhance this ability. However, these methods do not fully exploit the answers generated by the LLM to guide subsequent responses. This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers. PHP is orthogonal to CoT and self-consistency, making it easy to combine with state-of-the-art techniques to further improve performance. We conducted extensive and comprehensive experiments on seven benchmarks. The results show that PHP significantly improves accuracy while remaining highly efficient. For instance, with text-davinci-003, we observed a 4.2% improvement on GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction in sample paths with self-consistency. With GPT-4 and PHP, we achieve state-of-the-art performances on SVAMP (89.1% ->91.9%), GSM8K (92% ->95.5%), AQuA (76.4% ->79.9%) and MATH (50.3% ->53.9%).",
            "year": 2023,
            "citationCount": 64,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers."
            },
            "score": 6
        },
        {
            "id": "5f66d1a667eec13b5d337c3fc5619bcef95092bd",
            "paperId": "5f66d1a667eec13b5d337c3fc5619bcef95092bd",
            "title": "Universal Self-Consistency for Large Language Model Generation",
            "abstract": "Self-consistency with chain-of-thought prompting (CoT) has demonstrated remarkable performance gains on various challenging tasks, by utilizing multiple reasoning paths sampled from large language models (LLMs). However, self-consistency relies on the answer extraction process to aggregate multiple solutions, which is not applicable to free-form answers. In this work, we propose Universal Self-Consistency (USC), which leverages LLMs themselves to select the most consistent answer among multiple candidates. We evaluate USC on a variety of benchmarks, including mathematical reasoning, code generation, long-context summarization, and open-ended question answering. On open-ended generation tasks where the original self-consistency method is not applicable, USC effectively utilizes multiple samples and improves the performance. For mathematical reasoning, USC matches the standard self-consistency performance without requiring the answer formats to be similar. Finally, without access to execution results, USC also matches the execution-based voting performance on code generation.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes Universal Self-Consistency (USC), which leverages LLMs themselves to select the most consistent answer among multiple candidates and effectively utilizes multiple samples and improves the performance on open-ended generation tasks where the original self-consistency method is not applicable."
            },
            "score": 6
        },
        {
            "id": "08a3e2c94707926fe543df69bdf6c3a9b71aab52",
            "paperId": "08a3e2c94707926fe543df69bdf6c3a9b71aab52",
            "title": "Ask One More Time: Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios",
            "abstract": "Although chain-of-thought (CoT) prompting combined with language models has achieved encouraging results on complex reasoning tasks, the naive greedy decoding used in CoT prompting usually causes the repetitiveness and local optimality. To address this shortcoming, ensemble-optimization tries to obtain multiple reasoning paths to get the final answer assembly. However, current ensemble-optimization methods either simply employ rule-based post-processing such as \\textit{self-consistency}, or train an additional model based on several task-related human annotations to select the best one among multiple reasoning paths, yet fail to generalize to realistic settings where the type of input questions is unknown or the answer format of reasoning paths is unknown. To avoid their limitations, we propose \\textbf{Self-Agreement}, a generalizable ensemble-optimization method applying in almost all scenarios where the type of input questions and the answer format of reasoning paths may be known or unknown. Self-agreement firstly samples from language model's decoder to generate a \\textit{diverse} set of reasoning paths, and subsequently prompts the language model \\textit{one more time} to determine the optimal answer by selecting the most \\textit{agreed} answer among the sampled reasoning paths. Self-agreement simultaneously achieves remarkable performance on six public reasoning benchmarks and superior generalization capabilities.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Self-agreement is proposed, a generalizable ensemble-optimization method applying in almost all scenarios where the type of input questions and the answer format of reasoning paths may be known or unknown, and achieves remarkable performance on six public reasoning benchmarks and superior generalization capabilities."
            },
            "score": 6
        },
        {
            "id": "717392dac099d1506b766787382d61b277863163",
            "paperId": "717392dac099d1506b766787382d61b277863163",
            "title": "Better Zero-Shot Reasoning with Self-Adaptive Prompting",
            "abstract": "Modern large language models (LLMs) have demonstrated impressive capabilities at sophisticated tasks, often through step-by-step reasoning similar to humans. This is made possible by their strong few and zero-shot abilities -- they can effectively learn from a handful of handcrafted, completed responses (\"in-context examples\"), or are prompted to reason spontaneously through specially designed triggers. Nonetheless, some limitations have been observed. First, performance in the few-shot setting is sensitive to the choice of examples, whose design requires significant human effort. Moreover, given the diverse downstream tasks of LLMs, it may be difficult or laborious to handcraft per-task labels. Second, while the zero-shot setting does not require handcrafting, its performance is limited due to the lack of guidance to the LLMs. To address these limitations, we propose Consistency-based Self-adaptive Prompting (COSP), a novel prompt design method for LLMs. Requiring neither handcrafted responses nor ground-truth labels, COSP selects and builds the set of examples from the LLM zero-shot outputs via carefully designed criteria that combine consistency, diversity and repetition. In the zero-shot setting for three different LLMs, we show that using only LLM predictions, COSP improves performance up to 15% compared to zero-shot baselines and matches or exceeds few-shot baselines for a range of reasoning tasks.",
            "year": 2023,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Consistency-based Self-adaptive Prompting (COSP), a novel prompt design method for LLMs that selects and builds the set of examples from the LLM zero-shot outputs via carefully designed criteria that combine consistency, diversity and repetition."
            },
            "score": 6
        },
        {
            "id": "d2d16333a4b0dc7e3463b280b9945e5ee6c53396",
            "paperId": "d2d16333a4b0dc7e3463b280b9945e5ee6c53396",
            "title": "TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models",
            "abstract": "Factual consistency evaluation is often conducted using Natural Language Inference (NLI) models, yet these models exhibit limited success in evaluating summaries. Previous work improved such models with synthetic training data. However, the data is typically based on perturbed human-written summaries, which often differ in their characteristics from real model-generated summaries and have limited coverage of possible factual errors. Alternatively, large language models (LLMs) have recently shown promising results in directly evaluating generative tasks, but are too computationally expensive for practical use. Motivated by these limitations, we introduce TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM. Unlike prior work, TrueTeacher does not rely on human-written summaries, and is multilingual by nature. Experiments on the TRUE benchmark show that a student model trained using our data, substantially outperforms both the state-of-the-art model with similar capacity, and the LLM teacher. In a systematic study, we compare TrueTeacher to existing synthetic data generation methods and demonstrate its superiority and robustness to domain-shift. We also show that our method generalizes to multilingual scenarios. Lastly, we release our large scale synthetic dataset (1.4M examples), generated using TrueTeacher, and a checkpoint trained on this data.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM, which does not rely on human-written summaries, and is multilingual by nature."
            },
            "score": 6
        },
        {
            "id": "152d9a231c00d4495c9bc4a466f42165ce2e2164",
            "paperId": "152d9a231c00d4495c9bc4a466f42165ce2e2164",
            "title": "Evaluating Factual Consistency of Summaries with Large Language Models",
            "abstract": "Detecting factual errors in summaries has been an important and challenging subject in summarization research. Inspired by the emergent ability of large language models (LLMs), we explore evaluating factual consistency of summaries by directly prompting LLMs. We present a comprehensive empirical study to assess the ability of LLMs as factual consistency evaluators, which consists of (1) analyzing different LLMs such as the GPT model series and Flan-T5; (2) investigating a variety of prompting methods including vanilla prompting, chain-of-thought prompting, and a sentence-by-sentence prompting method to tackle long summaries; and (3) evaluating on diverse summaries generated by multiple summarization systems, ranging from pre-transformer methods to SOTA pretrained models. Our experiments demonstrate that prompting LLMs is able to outperform the previous best factuality systems in all settings, by up to 12.2 absolute points in terms of the binary classification accuracy on inconsistency detection.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A comprehensive empirical study to assess the ability of LLMs as factual consistency evaluators, which consists of analyzing different LLMs such as the GPT model series and Flan-T5 and investigating a variety of prompting methods including vanilla prompting, chain-of-thought prompting, and a sentence-by-sentence prompting method to tackle long summaries."
            },
            "score": 6
        },
        {
            "id": "234dad3daec8db281882c25bb704eefa8970bdc7",
            "paperId": "234dad3daec8db281882c25bb704eefa8970bdc7",
            "title": "Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs",
            "abstract": "In recent years, large language models (LLMs) have drawn significant attention due to their impressive emergent capabilities that were not observed in earlier language models. One emerging area where LLMs have been widely used in recent times is the utilization of LLMs as the evaluator of the texts generated by various generative models. In this paper, we also explore the possibility of whether LLMs are reliable in assessing the factual consistency of summaries generated by text generation models. We first propose a new approach to evaluate the factuality score using LLMs by utilizing the same LLM to perform all steps in the question-answering-based factuality scoring pipeline. Subsequently, we study the performance of various LLMs to directly score the factuality. Our evaluation is conducted in traditional benchmarks by comparing their correlation with human annotations. Contrary to expectations, our findings revealed that none of the factuality metrics showed any significant correlations (e.g., coefficient scores greater than 0.3) to human evaluations of factuality for GPT-4, PaLM-2, and Claude-2, with the only exception being GPT-3.5 in two subcategories of factuality. Nonetheless, our findings are consistent across almost all factual error types, suggesting a fundamental limitation in the ability of current LLMs to assess factuality.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new approach to evaluate the factuality score using LLMs is proposed by utilizing the same LLM to perform all steps in the question-answering-based factuality scoring pipeline, and it is revealed that none of the factuality metrics showed any significant correlations to human evaluations of factuality."
            },
            "score": 6
        },
        {
            "id": "536dd3e54ad83e64d27217485db230ea09b19f51",
            "paperId": "536dd3e54ad83e64d27217485db230ea09b19f51",
            "title": "Synthetic Disinformation Attacks on Automated Fact Verification Systems",
            "abstract": "Automated fact-checking is a needed technology to curtail the spread of online misinformation. One current framework for such solutions proposes to verify claims by retrieving supporting or refuting evidence from related textual sources. However, the realistic use cases for fact-checkers will require verifying claims against evidence sources that could be affected by the same misinformation. Furthermore, the development of modern NLP tools that can produce coherent, fabricated content would allow malicious actors to systematically generate adversarial disinformation for fact-checkers.\n \nIn this work, we explore the sensitivity of automated fact-checkers to synthetic adversarial evidence in two simulated settings: ADVERSARIAL ADDITION, where we fabricate documents and add them to the evidence repository available to the fact-checking system, and ADVERSARIAL MODIFICATION, where existing evidence source documents in the repository are automatically altered. Our study across multiple models on three benchmarks demonstrates that these systems suffer significant performance drops against these attacks. Finally, we discuss the growing threat of modern NLG systems as generators of disinformation in the context of the challenges they pose to automated fact-checkers.",
            "year": 2022,
            "citationCount": 19,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores the sensitivity of automated fact-checkers to synthetic adversarial evidence in two simulated settings: ADVERSARIAL ADDITION, where documents are fabricate and added to the evidence repository available to the fact-checking system, and ADVERSarIAL MODIFICATION, where existing evidence source documents in the repository are automatically altered."
            },
            "score": 6
        },
        {
            "id": "f72b6f2fee42c3f46bc3b7f8f3dbcdc32c38f119",
            "paperId": "f72b6f2fee42c3f46bc3b7f8f3dbcdc32c38f119",
            "title": "The perils and promises of fact-checking with large language models",
            "abstract": "Automated fact-checking, using machine learning to verify claims, has grown vital as misinformation spreads beyond human fact-checking capacity. Large language models (LLMs) like GPT-4 are increasingly trusted to write academic papers, lawsuits, and news articles and to verify information, emphasizing their role in discerning truth from falsehood and the importance of being able to verify their outputs. Understanding the capacities and limitations of LLMs in fact-checking tasks is therefore essential for ensuring the health of our information ecosystem. Here, we evaluate the use of LLM agents in fact-checking by having them phrase queries, retrieve contextual data, and make decisions. Importantly, in our framework, agents explain their reasoning and cite the relevant sources from the retrieved context. Our results show the enhanced prowess of LLMs when equipped with contextual information. GPT-4 outperforms GPT-3, but accuracy varies based on query language and claim veracity. While LLMs show promise in fact-checking, caution is essential due to inconsistent accuracy. Our investigation calls for further research, fostering a deeper comprehension of when agents succeed and when they fail.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work evaluates the use of LLM agents in fact-checking by having them phrase queries, retrieve contextual data, and make decisions, and results show the enhanced prowess of LLMs when equipped with contextual information."
            },
            "score": 5
        },
        {
            "id": "8c5acaafe43e710d55b08c63d567550ad26ec437",
            "paperId": "8c5acaafe43e710d55b08c63d567550ad26ec437",
            "title": "Fact-Checking the Output of Large Language Models via Token-Level Uncertainty Quantification",
            "abstract": "Large language models (LLMs) are notorious for hallucinating, i.e., producing erroneous claims in their output. Such hallucinations can be dangerous, as occasional factual inaccuracies in the generated text might be obscured by the rest of the output being generally factual, making it extremely hard for the users to spot them. Current services that leverage LLMs usually do not provide any means for detecting unreliable generations. Here, we aim to bridge this gap. In particular, we propose a novel fact-checking and hallucination detection pipeline based on token-level uncertainty quantification. Uncertainty scores leverage information encapsulated in the output of a neural network or its layers to detect unreliable predictions, and we show that they can be used to fact-check the atomic claims in the LLM output. Moreover, we present a novel token-level uncertainty quantification method that removes the impact of uncertainty about what claim to generate on the current step and what surface form to use. Our method Claim Conditioned Probability (CCP) measures only the uncertainty of particular claim value expressed by the model. Experiments on the task of biography generation demonstrate strong improvements for CCP compared to the baselines for six different LLMs and three languages. Human evaluation reveals that the fact-checking pipeline based on uncertainty quantification is competitive with a fact-checking tool that leverages external knowledge.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel fact-checking and hallucination detection pipeline based on token-level uncertainty quantification that is competitive with a fact-checking tool that leverages external knowledge is proposed."
            },
            "score": 5
        },
        {
            "id": "4e53b481beabba42aac027e5a8c69fed26ab4062",
            "paperId": "4e53b481beabba42aac027e5a8c69fed26ab4062",
            "title": "RHO ($\u03c1$): Reducing Hallucination in Open-domain Dialogues with Knowledge Grounding",
            "abstract": "Dialogue systems can leverage large pre-trained language models and knowledge to generate fluent and informative responses. However, these models are still prone to produce hallucinated responses not supported by the input source, which greatly hinders their application. The heterogeneity between external knowledge and dialogue context challenges representation learning and source integration, and further contributes to unfaithfulness. To handle this challenge and generate more faithful responses, this paper presents RHO ($\\rho$) utilizing the representations of linked entities and relation predicates from a knowledge graph (KG). We propose (1) local knowledge grounding to combine textual embeddings with the corresponding KG embeddings; and (2) global knowledge grounding to equip RHO with multi-hop reasoning abilities via the attention mechanism. In addition, we devise a response re-ranking technique based on walks over KG sub-graphs for better conversational reasoning. Experimental results on OpenDialKG show that our approach significantly outperforms state-of-the-art methods on both automatic and human evaluation by a large margin, especially in hallucination reduction (17.54% in FeQA).",
            "year": 2022,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "RHO is presented utilizing the representations of linked entities and relation predicates from a knowledge graph (KG) to equip RHO with multi-hop reasoning abilities via the attention mechanism and devise a response re-ranking technique based on walks over KG sub-graphs for better conversational reasoning."
            },
            "score": 5
        },
        {
            "id": "2986b2b06173e065c94bae49c7a9a3718dad486c",
            "paperId": "2986b2b06173e065c94bae49c7a9a3718dad486c",
            "title": "Reducing hallucination in structured outputs via Retrieval-Augmented Generation",
            "abstract": "A common and fundamental limitation of Generative AI (GenAI) is its propensity to hallucinate. While large language models (LLM) have taken the world by storm, without eliminating or at least reducing hallucinations, real-world GenAI systems may face challenges in user adoption. In the process of deploying an enterprise application that produces workflows based on natural language requirements, we devised a system leveraging Retrieval Augmented Generation (RAG) to greatly improve the quality of the structured output that represents such workflows. Thanks to our implementation of RAG, our proposed system significantly reduces hallucinations in the output and improves the generalization of our LLM in out-of-domain settings. In addition, we show that using a small, well-trained retriever encoder can reduce the size of the accompanying LLM, thereby making deployments of LLM-based systems less resource-intensive.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This proposed system significantly reduces hallucinations in the output and improves the generalization of the LLM in out-of-domain settings, and it is shown that using a small, well-trained retriever encoder can reduce the size of the accompanying LLM, thereby making deployments of LLM-based systems less resource-intensive."
            },
            "score": 5
        },
        {
            "id": "60988a0ebad89af503f17de977785814fb864635",
            "paperId": "60988a0ebad89af503f17de977785814fb864635",
            "title": "Correction with Backtracking Reduces Hallucination in Summarization",
            "abstract": "Abstractive summarization aims at generating natural language summaries of a source document that are succinct while preserving the important elements. Despite recent advances, neural text summarization models are known to be susceptible to hallucinating (or more correctly confabulating), that is to produce summaries with details that are not grounded in the source document. In this paper, we introduce a simple yet efficient technique, CoBa, to reduce hallucination in abstractive summarization. The approach is based on two steps: hallucination detection and mitigation. We show that the former can be achieved through measuring simple statistics about conditional word probabilities and distance to context words. Further, we demonstrate that straight-forward backtracking is surprisingly effective at mitigation. We thoroughly evaluate the proposed method with prior art on three benchmark datasets for text summarization. The results show that CoBa is effective and efficient in reducing hallucination, and offers great adaptability and flexibility.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a simple yet efficient technique, CoBa, to reduce hallucination in abstractive summarization, and shows that the former can be achieved through measuring simple statistics about conditional word probabilities and distance to context words."
            },
            "score": 5
        },
        {
            "id": "fcee1c19e12f3b7e3595aeba702416d055bdbc3f",
            "paperId": "fcee1c19e12f3b7e3595aeba702416d055bdbc3f",
            "title": "Knowledge Verification to Nip Hallucination in the Bud",
            "abstract": "While large language models (LLMs) have demonstrated exceptional performance across various tasks following human alignment, they may still generate responses that sound plausible but contradict factual knowledge, a phenomenon known as \\emph{hallucination}. In this paper, we demonstrate the feasibility of mitigating hallucinations by verifying and minimizing the inconsistency between external knowledge present in the alignment data and the intrinsic knowledge embedded within foundation LLMs. Specifically, we propose a novel approach called Knowledge Consistent Alignment (KCA), which employs a well-aligned LLM to automatically formulate assessments based on external knowledge to evaluate the knowledge boundaries of foundation LLMs. To address knowledge inconsistencies in the alignment data, KCA implements several specific strategies to deal with these data instances. We demonstrate the superior efficacy of KCA in reducing hallucinations across six benchmarks, utilizing foundation LLMs of varying backbones and scales. This confirms the effectiveness of mitigating hallucinations by reducing knowledge inconsistency. Our code, model weights, and data are openly accessible at \\url{https://github.com/fanqiwan/KCA}.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The superior efficacy of KCA is demonstrated in reducing hallucinations across six benchmarks, utilizing foundation LLMs of varying backbones and scales, which confirms the effectiveness of mitigating hallucinations by reducing knowledge inconsistency."
            },
            "score": 5
        },
        {
            "id": "dee83ee947ea95f54b40a3c101552b4eb447bbf6",
            "paperId": "dee83ee947ea95f54b40a3c101552b4eb447bbf6",
            "title": "Self-Consistency of Large Language Models under Ambiguity",
            "abstract": "Large language models (LLMs) that do not give consistent answers across contexts are problematic when used for tasks with expectations of consistency\u2013e.g. question-answering, explanations, etc. Our work presents an evaluation benchmark for self-consistency in cases of under-specification where two or more answers can be correct. We conduct a series of behavioral experiments on the OpenAI model suite using an ambiguous integer sequence completion task. We find that average consistency ranges from 67% to 82%, far higher than would be predicted if a model\u2019s consistency was random, and increases as model capability improves. Furthermore, we show that models tend to maintain self-consistency across a series of robustness checks, including prompting speaker changes and sequence length changes. These results suggest that self-consistency arises as an emergent capability without specifically training for it. Despite this, we find that models are uncalibrated when judging their own consistency, with models displaying both over- and under-confidence. We also propose a nonparametric test for determining from token output distribution whether a model assigns non-trivial probability to alternative answers. Using this test, we find that despite increases in self-consistency, models usually place significant weight on alternative, inconsistent answers. This distribution of probability mass provides evidence that even highly self-consistent models internally compute multiple possible responses.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents an evaluation benchmark for self-consistency in cases of under-specification where two or more answers can be correct, and proposes a nonparametric test for determining from token output distribution whether a model assigns non-trivial probability to alternative answers."
            },
            "score": 5
        },
        {
            "id": "3fa70115248377c3d1517c9f978791a296fbc1dd",
            "paperId": "3fa70115248377c3d1517c9f978791a296fbc1dd",
            "title": "Large Language Models Can Self-Improve",
            "abstract": "Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate\"high-confidence\"rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4%->82.1% on GSM8K, 78.2%->83.0% on DROP, 90.0%->94.4% on OpenBookQA, and 63.4%->67.9% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.",
            "year": 2022,
            "citationCount": 265,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work uses a pre-trained LLM to generate \"high-confidence\"rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs to improve the general reasoning ability."
            },
            "score": 5
        },
        {
            "id": "dca6c3927ade6481a1ae080f5c24decbfeced1be",
            "paperId": "dca6c3927ade6481a1ae080f5c24decbfeced1be",
            "title": "Boosted Prompt Ensembles for Large Language Models",
            "abstract": "Methods such as chain-of-thought prompting and self-consistency have pushed the frontier of language model reasoning performance with no additional training. To further improve performance, we propose a prompt ensembling method for large language models, which uses a small dataset to construct a set of few shot prompts that together comprise a ``boosted prompt ensemble''. The few shot examples for each prompt are chosen in a stepwise fashion to be ``hard'' examples on which the previous step's ensemble is uncertain. We show that this outperforms single-prompt output-space ensembles and bagged prompt-space ensembles on the GSM8k and AQuA datasets, among others. We propose both train-time and test-time versions of boosted prompting that use different levels of available annotation and conduct a detailed empirical study of our algorithm.",
            "year": 2023,
            "citationCount": 21,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A prompt ensembling method for large language models, which uses a small dataset to construct a set of few shot prompts that together comprise a ``boosted prompt ensemble'' that outperforms single-prompt output-space ensembles and bagged prompt-spaceEnsemble on the GSM8k and AQuA datasets, among others."
            },
            "score": 5
        },
        {
            "id": "cb4f6823e25896d5919046696066d0d498cc4397",
            "paperId": "cb4f6823e25896d5919046696066d0d498cc4397",
            "title": "Forward-Backward Reasoning in Large Language Models for Verification",
            "abstract": "Chain-of-Though (CoT) prompting has shown promising performance in various reasoning tasks. Recently, Self-Consistency (Wang et al., 2023) proposes to sample a diverse set of reasoning chains which may lead to different answers while the answer that receives the most votes is selected. In this paper, we propose a novel method to use backward reasoning in verifying candidate answers. We mask a token in the question by x and ask the LLM to predict the masked token when a candidate answer is provided by a simple template, i.e., \u201cIf we know the answer of the above question is {a candidate answer}, what is the value of unknown variable x?\u201d Intuitively, the LLM is expected to predict the masked token successfully if the provided candidate answer is correct. We further propose FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers. We conduct extensive experiments on six data sets and three LLMs. Experimental results demonstrate that FOBAR achieves state-of-the-art performance on various reasoning benchmarks.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel method to use backward reasoning in verifying candidate answers inChain-of-Though prompting and proposes FOBAR to combine forward and backward reasoning for estimating the probability of candidate answers."
            },
            "score": 5
        },
        {
            "id": "d2a01f9b2a565070ce64ff38eb7cdc26f3ed992a",
            "paperId": "d2a01f9b2a565070ce64ff38eb7cdc26f3ed992a",
            "title": "Self-Discover: Large Language Models Self-Compose Reasoning Structures",
            "abstract": "We introduce SELF-DISCOVER, a general framework for LLMs to self-discover the task-intrinsic reasoning structures to tackle complex reasoning problems that are challenging for typical prompting methods. Core to the framework is a self-discovery process where LLMs select multiple atomic reasoning modules such as critical thinking and step-by-step thinking, and compose them into an explicit reasoning structure for LLMs to follow during decoding. SELF-DISCOVER substantially improves GPT-4 and PaLM 2's performance on challenging reasoning benchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, by as much as 32% compared to Chain of Thought (CoT). Furthermore, SELF-DISCOVER outperforms inference-intensive methods such as CoT-Self-Consistency by more than 20%, while requiring 10-40x fewer inference compute. Finally, we show that the self-discovered reasoning structures are universally applicable across model families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2, and share commonalities with human reasoning patterns.",
            "year": 2024,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "SELF-DISCOVER substantially improves GPT-4 and PaLM 2's performance on challenging reasoning benchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, and outperforms inference-intensive methods such as CoT-Self-Consistency by more than 20%, while requiring 10-40x fewer inference compute."
            },
            "score": 5
        },
        {
            "id": "56373d3fd0f1354a61f9e577db039cdb187d8d43",
            "paperId": "56373d3fd0f1354a61f9e577db039cdb187d8d43",
            "title": "Evaluating the Factual Consistency of Large Language Models Through News Summarization",
            "abstract": "While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information. To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB(Factual Inconsistency Benchmark) that focuses on the task of summarization. Specifically, our benchmark involves comparing the scores an LLM assigns to a factually consistent versus a factually inconsistent summary for an input news article. For factually consistent summaries, we use human-written reference summaries that we manually verify as factually consistent. To generate summaries that are factually inconsistent, we generate summaries from a suite of summarization models that we have manually annotated as factually inconsistent. A model's factual consistency is then measured according to its accuracy, i.e.\\ the proportion of documents where it assigns a higher score to the factually consistent summary. To validate the usefulness of FIB, we evaluate 23 large language models ranging from 1B to 176B parameters from six different model families including BLOOM and OPT. We find that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries. However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries. We validate design choices in our benchmark including the scoring method and source of distractor summaries. Our code and benchmark data can be found at https://github.com/r-three/fib.",
            "year": 2022,
            "citationCount": 51,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new benchmark called FIB(Factual Inconsistency Benchmark) that focuses on the task of summarization, and finds that existing LLMs generally assign a higher score to factually consistent summaries than tofactually inconsistent summaries."
            },
            "score": 5
        },
        {
            "id": "f33d19614d78f4a6e107e768ccd0a2d4244fc89a",
            "paperId": "f33d19614d78f4a6e107e768ccd0a2d4244fc89a",
            "title": "Evaluating the Factual Consistency of Large Language Models Through Summarization",
            "abstract": "While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information. To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB ( F actual I nconsistency B enchmark) that focuses on the task of summarization. Specifically, our benchmark involves comparing the scores an LLM assigns to a factually consistent versus a factually inconsistent summary for an input news article. For factually consistent summaries, we use human-written reference summaries that we manually verify as fac-tually consistent. To generate summaries that are factually inconsistent, we generate summaries from a suite of summarization models that we have manually annotated as fac-tually inconsistent. A model\u2019s factual consistency is then measured according to its accuracy, i.e. the proportion of documents where it assigns a higher score to the factually consistent summary. To validate the usefulness of FIB , we evaluate 23 large language models ranging from 1B to 176B parameters from six different model families including BLOOM and OPT. We \ufb01nd that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries. However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries. We validate design choices in our benchmark including the scoring method and source of distractor summaries. Our code and",
            "year": 2022,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new benchmark called FIB is proposed that focuses on the task of summarization and shows that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries, and that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries."
            },
            "score": 5
        },
        {
            "id": "f3cd3a0e1a0a29193fd88178ef75946dfe756622",
            "paperId": "f3cd3a0e1a0a29193fd88178ef75946dfe756622",
            "title": "Factual Consistency Evaluation of Summarisation in the Era of Large Language Models",
            "abstract": "Factual inconsistency with source documents in automatically generated summaries can lead to misinformation or pose risks. Existing factual consistency(FC) metrics are constrained by their performance, efficiency, and explainability. Recent advances in Large language models (LLMs) have demonstrated remarkable potential in text evaluation but their effectiveness in assessing FC in summarisation remains underexplored. Prior research has mostly focused on proprietary LLMs, leaving essential factors that affect their assessment capabilities unexplored. Additionally, current FC evaluation benchmarks are restricted to news articles, casting doubt on the generality of the FC methods tested on them. In this paper, we first address the gap by introducing TreatFact a dataset of LLM-generated summaries of clinical texts, annotated for FC by domain experts. Moreover, we benchmark 11 LLMs for FC evaluation across news and clinical domains and analyse the impact of model size, prompts, pre-training and fine-tuning data. Our findings reveal that despite proprietary models prevailing on the task, open-source LLMs lag behind. Nevertheless, there is potential for enhancing the performance of open-source LLMs through increasing model size, expanding pre-training data, and developing well-curated fine-tuning data. Experiments on TreatFact suggest that both previous methods and LLM-based evaluators are unable to capture factual inconsistencies in clinical summaries, posing a new challenge for FC evaluation.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "TreatFact, a dataset of LLM-generated summaries of clinical texts, annotated for FC by domain experts, is introduced and it is revealed that despite proprietary models prevailing on the task, open-source LLMs lag behind."
            },
            "score": 5
        },
        {
            "id": "d38af39275524068e7aab12fa8d54d342eff7dfe",
            "paperId": "d38af39275524068e7aab12fa8d54d342eff7dfe",
            "title": "UFO: a Unified and Flexible Framework for Evaluating Factuality of Large Language Models",
            "abstract": "Large language models (LLMs) may generate text that lacks consistency with human knowledge, leading to factual inaccuracies or \\textit{hallucination}. Existing research for evaluating the factuality of LLMs involves extracting fact claims using an LLM and verifying them against a predefined fact source. However, these evaluation metrics are task-specific, and not scalable, and the substitutability of fact sources in different tasks is under-explored. To address these challenges, we categorize four available fact sources: human-written evidence, reference documents, search engine results, and LLM knowledge, along with five text generation tasks containing six representative datasets. Then, we propose \\texttt{UFO}, an LLM-based unified and flexible evaluation framework to verify facts against plug-and-play fact sources. We implement five evaluation scenarios based on this framework. Experimental results show that for most QA tasks, human-written evidence and reference documents are crucial, and they can substitute for each other in retrieval-augmented QA tasks. In news fact generation tasks, search engine results and LLM knowledge are essential. Our dataset and code are available at \\url{https://github.com/WaldenRUC/UFO}.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes \\texttt{UFO}, an LLM-based unified and flexible evaluation framework to verify facts against plug-and-play fact sources, and implements five evaluation scenarios based on this framework."
            },
            "score": 5
        },
        {
            "id": "3a89e289e2dd29f5e52a2bf354a637762b661257",
            "paperId": "3a89e289e2dd29f5e52a2bf354a637762b661257",
            "title": "Fine-tuning Language Models for Factuality",
            "abstract": "The fluency and creativity of large pre-trained language models (LLMs) have led to their widespread use, sometimes even as a replacement for traditional search engines. Yet language models are prone to making convincing but factually inaccurate claims, often referred to as 'hallucinations.' These errors can inadvertently spread misinformation or harmfully perpetuate misconceptions. Further, manual fact-checking of model responses is a time-consuming process, making human factuality labels expensive to acquire. In this work, we fine-tune language models to be more factual, without human labeling and targeting more open-ended generation settings than past work. We leverage two key recent innovations in NLP to do so. First, several recent works have proposed methods for judging the factuality of open-ended text by measuring consistency with an external knowledge base or simply a large model's confidence scores. Second, the direct preference optimization algorithm enables straightforward fine-tuning of language models on objectives other than supervised imitation, using a preference ranking over possible model responses. We show that learning from automatically generated factuality preference rankings, generated either through existing retrieval systems or our novel retrieval-free approach, significantly improves the factuality (percent of generated claims that are correct) of Llama-2 on held-out topics compared with RLHF or decoding strategies targeted at factuality. At 7B scale, compared to Llama-2-chat, we observe 58% and 40% reduction in factual error rate when generating biographies and answering medical questions, respectively.",
            "year": 2023,
            "citationCount": 56,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that learning from automatically generated factuality preference rankings, generated either through existing retrieval systems or the novel retrieval-free approach, significantly improves the factuality of Llama-2 on held-out topics compared with RLHF or decoding strategies targeted at factuality."
            },
            "score": 5
        },
        {
            "id": "05d2ced6a4fb7efb8d527a228ad792526a202235",
            "paperId": "05d2ced6a4fb7efb8d527a228ad792526a202235",
            "title": "How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities",
            "abstract": "The rapid progress in open-source Large Language Models (LLMs) is significantly driving AI development forward. However, there is still a limited understanding of their trustworthiness. Deploying these models at scale without sufficient trustworthiness can pose significant risks, highlighting the need to uncover these issues promptly. In this work, we conduct an adversarial assessment of open-source LLMs on trustworthiness, scrutinizing them across eight different aspects including toxicity, stereotypes, ethics, hallucination, fairness, sycophancy, privacy, and robustness against adversarial demonstrations. We propose advCoU, an extended Chain of Utterances-based (CoU) prompting strategy by incorporating carefully crafted malicious demonstrations for trustworthiness attack. Our extensive experiments encompass recent and representative series of open-source LLMs, including Vicuna, MPT, Falcon, Mistral, and Llama 2. The empirical outcomes underscore the efficacy of our attack strategy across diverse aspects. More interestingly, our result analysis reveals that models with superior performance in general NLP tasks do not always have greater trustworthiness; in fact, larger models can be more vulnerable to attacks. Additionally, models that have undergone instruction tuning, focusing on instruction following, tend to be more susceptible, although fine-tuning LLMs for safety alignment proves effective in mitigating adversarial trustworthiness attacks.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An adversarial assessment of open-source LLMs on trustworthiness is conducted, scrutinizing them across eight different aspects including toxicity, stereotypes, ethics, hallucination, fairness, sycophancy, privacy, and robustness against adversarial demonstrations, revealing that models with superior performance in general NLP tasks do not always have greater trustworthiness."
            },
            "score": 5
        },
        {
            "id": "4b3e2fe609840998075e548e67b0589661467f44",
            "paperId": "4b3e2fe609840998075e548e67b0589661467f44",
            "title": "Generating Label Cohesive and Well-Formed Adversarial Claims",
            "abstract": "Adversarial attacks reveal important vulnerabilities and flaws of trained models. One potent type of attack are universal adversarial triggers, which are individual n-grams that, when appended to instances of a class under attack, can trick a model into predicting a target class. However, for inference tasks such as fact checking, these triggers often inadvertently invert the meaning of instances they are inserted in. In addition, such attacks produce semantically nonsensical inputs, as they simply concatenate triggers to existing samples. Here, we investigate how to generate adversarial attacks against fact checking systems that preserve the ground truth meaning and are semantically valid. We extend the HotFlip attack algorithm used for universal trigger generation by jointly minimising the target class loss of a fact checking model and the entailment class loss of an auxiliary natural language inference model. We then train a conditional language model to generate semantically valid statements, which include the found universal triggers. We find that the generated attacks maintain the directionality and semantic validity of the claim better than previous work.",
            "year": 2020,
            "citationCount": 33,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work extends the HotFlip attack algorithm used for universal trigger generation by jointly minimising the target class loss of a fact checking model and the entailment class Loss of an auxiliary natural language inference model, and trains a conditional language model to generate semantically valid statements, which include the found universal triggers."
            },
            "score": 4
        },
        {
            "id": "8d34de18ce1c2345e3fa1bff786a2410c2783e6a",
            "paperId": "8d34de18ce1c2345e3fa1bff786a2410c2783e6a",
            "title": "FactLLaMA: Optimizing Instruction-Following Language Models with External Knowledge for Automated Fact-Checking",
            "abstract": "Automatic fact-checking plays a crucial role in combating the spread of misinformation. Large Language Models (LLMs) and Instruction-Following variants, such as InstructGPT and Alpaca, have shown remarkable performance in various natural language processing tasks. However, their knowledge may not always be up-to-date or sufficient, potentially leading to inaccuracies in fact-checking. To address this limitation, we propose combining the power of instruction-following language models with external evidence retrieval to enhance fact-checking performance. Our approach involves leveraging search engines to retrieve relevant evidence for a given input claim. This external evidence serves as valuable supplementary information to augment the knowledge of the pretrained language model. Then, we instruct-tune an open-sourced language model, called LLaMA, using this evidence, enabling it to predict the veracity of the input claim more accurately. To evaluate our method, we conducted experiments on two widely used fact-checking datasets: RAWFC and LIAR. The results demonstrate that our approach achieves state-of-the-art performance in fact-checking tasks. By integrating external evidence, we bridge the gap between the model's knowledge and the most up-to-date and sufficient context available, leading to improved fact-checking outcomes. Our findings have implications for combating misinformation and promoting the dissemination of accurate information on online platforms. Our released materials are accessible at: https://thcheung.github.io/factllama.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes combining the power of instruction-following language models with external evidence retrieval to enhance fact-checking performance by leveraging search engines to retrieve relevant evidence for a given input claim and instruct-tune an open-sourced language model, called LLaMA, using this evidence."
            },
            "score": 4
        },
        {
            "id": "2744c617017c113bec09093962c66e061ccd9909",
            "paperId": "2744c617017c113bec09093962c66e061ccd9909",
            "title": "Evidence-based Interpretable Open-domain Fact-checking with Large Language Models",
            "abstract": "Universal fact-checking systems for real-world claims face significant challenges in gathering valid and sufficient real-time evidence and making reasoned decisions. In this work, we introduce the Open-domain Explainable Fact-checking (OE-Fact) system for claim-checking in real-world scenarios. The OE-Fact system can leverage the powerful understanding and reasoning capabilities of large language models (LLMs) to validate claims and generate causal explanations for fact-checking decisions. To adapt the traditional three-module fact-checking framework to the open domain setting, we first retrieve claim-related information as relevant evidence from open websites. After that, we retain the evidence relevant to the claim through LLM and similarity calculation for subsequent verification. We evaluate the performance of our adapted three-module OE-Fact system on the Fact Extraction and Verification (FEVER) dataset. Experimental results show that our OE-Fact system outperforms general fact-checking baseline systems in both closed- and open-domain scenarios, ensuring stable and accurate verdicts while providing concise and convincing real-time explanations for fact-checking decisions.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results show that the Open-domain Explainable Fact-checking (OE-Fact) system outperforms general fact-checking baseline systems in both closed- and open-domain scenarios, ensuring stable and accurate verdicts while providing concise and convincing real-time explanations for fact- checking decisions."
            },
            "score": 4
        },
        {
            "id": "3414be052766667fe375221804e48f4a9c815ba5",
            "paperId": "3414be052766667fe375221804e48f4a9c815ba5",
            "title": "Multimodal Large Language Models to Support Real-World Fact-Checking",
            "abstract": "Multimodal large language models (MLLMs) carry the potential to support humans in processing vast amounts of information. While MLLMs are already being used as a fact-checking tool, their abilities and limitations in this regard are understudied. Here is aim to bridge this gap. In particular, we propose a framework for systematically assessing the capacity of current multimodal models to facilitate real-world fact-checking. Our methodology is evidence-free, leveraging only these models' intrinsic knowledge and reasoning capabilities. By designing prompts that extract models' predictions, explanations, and confidence levels, we delve into research questions concerning model accuracy, robustness, and reasons for failure. We empirically find that (1) GPT-4V exhibits superior performance in identifying malicious and misleading multimodal claims, with the ability to explain the unreasonable aspects and underlying motives, and (2) existing open-source models exhibit strong biases and are highly sensitive to the prompt. Our study offers insights into combating false multimodal information and building secure, trustworthy multimodal models. To the best of our knowledge, we are the first to evaluate MLLMs for real-world fact-checking.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study empirically finds that GPT-4V exhibits superior performance in identifying malicious and misleading multimodal claims, with the ability to explain the unreasonable aspects and underlying motives, and existing open-source models exhibit strong biases and are highly sensitive to the prompt."
            },
            "score": 4
        },
        {
            "id": "f89ed27318cb930ae884af0c62be37f0355571b5",
            "paperId": "f89ed27318cb930ae884af0c62be37f0355571b5",
            "title": "RAGAR, Your Falsehood RADAR: RAG-Augmented Reasoning for Political Fact-Checking using Multimodal Large Language Models",
            "abstract": "The escalating challenge of misinformation, particularly in the context of political discourse, necessitates advanced solutions for fact-checking. We introduce innovative approaches to enhance the reliability and efficiency of multimodal fact-checking through the integration of Large Language Models (LLMs) with Retrieval-augmented Generation (RAG)- based advanced reasoning techniques. This work proposes two novel methodologies, Chain of RAG (CoRAG) and Tree of RAG (ToRAG). The approaches are designed to handle multimodal claims by reasoning the next questions that need to be answered based on previous evidence. Our approaches improve the accuracy of veracity predictions and the generation of explanations over the traditional fact-checking approach of sub-question generation with chain of thought veracity prediction. By employing multimodal LLMs adept at analyzing both text and images, this research advances the capability of automated systems in identifying and countering misinformation.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes two novel methodologies, Chain of RAG (CoRAG) and Tree of RAG (ToRAG), designed to handle multimodal claims by reasoning the next questions that need to be answered based on previous evidence."
            },
            "score": 4
        },
        {
            "id": "06b2ac5153e3d8d05c13c82f93d7f4e13eee6d0f",
            "paperId": "06b2ac5153e3d8d05c13c82f93d7f4e13eee6d0f",
            "title": "Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites",
            "abstract": "Large language models (LLMs) have shown remarkable performance in natural language processing (NLP) tasks. To comprehend and execute diverse human instructions over image data, instruction-tuned large vision-language models (LVLMs) have been introduced. However, LVLMs may suffer from different types of object hallucinations. Nevertheless, LVLMs are evaluated for coarse-grained object hallucinations only (i.e., generated objects non-existent in the input image). The fine-grained object attributes and behaviors non-existent in the image may still be generated but not measured by the current evaluation methods. In this paper, we thus focus on reducing fine-grained hallucinations of LVLMs. We propose \\textit{ReCaption}, a framework that consists of two components: rewriting captions using ChatGPT and fine-tuning the instruction-tuned LVLMs on the rewritten captions. We also propose a fine-grained probing-based evaluation method named \\textit{Fine-Grained Object Hallucination Evaluation} (\\textit{FGHE}). Our experiment results demonstrate that ReCaption effectively reduces fine-grained object hallucination for different LVLM options and improves their text generation quality. The code can be found at https://github.com/Anonymousanoy/FOHE.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ReCaption is proposed, a framework that consists of two components: rewriting captions using ChatGPT and fine-tuning the instruction-tuned LVLMs on the rewritten captions, and a fine-grained probing-based evaluation method named \\textit{Fine-Grained Object Hallucination Evaluation} (FGHE)."
            },
            "score": 4
        },
        {
            "id": "f99116659c7522941c2353f23bddd07251adaccc",
            "paperId": "f99116659c7522941c2353f23bddd07251adaccc",
            "title": "BTR: Binary Token Representations for Efficient Retrieval Augmented Language Models",
            "abstract": "Retrieval augmentation addresses many critical problems in large language models such as hallucination, staleness, and privacy leaks. However, running retrieval-augmented language models (LMs) is slow and difficult to scale due to processing large amounts of retrieved text. We introduce binary token representations (BTR), which use 1-bit vectors to precompute every token in passages, significantly reducing computation during inference. Despite the potential loss of accuracy, our new calibration techniques and training objectives restore performance. Combined with offline and runtime compression, this only requires 127GB of disk space for encoding 3 billion tokens in Wikipedia. Our experiments show that on five knowledge-intensive NLP tasks, BTR accelerates state-of-the-art inference by up to 4x and reduces storage by over 100x while maintaining over 95% task performance.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "BTR is introduced, which use 1-bit vectors to precompute every token in passages, significantly reducing computation during inference, and accelerates state-of-the-art inference by up to 4x and reduces storage by over 100x while maintaining over 95% task performance."
            },
            "score": 4
        },
        {
            "id": "7cfbd36c0043098589cbaf18dca2b41d8dc24abe",
            "paperId": "7cfbd36c0043098589cbaf18dca2b41d8dc24abe",
            "title": "Plausible May Not Be Faithful: Probing Object Hallucination in Vision-Language Pre-training",
            "abstract": "Large-scale vision-language pre-trained (VLP) models are prone to hallucinate non-existent visual objects when generating text based on visual information. In this paper, we systematically study the object hallucination problem from three aspects. First, we examine recent state-of-the-art VLP models, showing that they still hallucinate frequently and models achieving better scores on standard metrics (e.g., CIDEr) could be more unfaithful. Second, we investigate how different types of image encoding in VLP influence hallucination, including region-based, grid-based, and patch-based. Surprisingly, we find that patch-based features perform the best and smaller patch resolution yields a non-trivial reduction in object hallucination. Third, we decouple various VLP objectives and demonstrate that token-level image-text alignment and controlled generation are crucial to reducing hallucination. Based on that, we propose a simple yet effective VLP loss named ObjMLM to further mitigate object hallucination. Results show that it reduces object hallucination by up to 17.4% when tested on two benchmarks (COCO Caption for in-domain and NoCaps for out-of-domain evaluation).",
            "year": 2022,
            "citationCount": 35,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a simple yet effective VLP loss named ObjMLM to further mitigate object hallucination and decouple various VLP objectives and demonstrates that token-level image-text alignment and controlled generation are crucial to reducing hallucination."
            },
            "score": 4
        },
        {
            "id": "6df5ba162b38d2853cc8431ff6f878d085c03693",
            "paperId": "6df5ba162b38d2853cc8431ff6f878d085c03693",
            "title": "SERPENT-VLM : Self-Refining Radiology Report Generation Using Vision Language Models",
            "abstract": "Radiology Report Generation (R2Gen) demonstrates how Multi-modal Large Language Models (MLLMs) can automate the creation of accurate and coherent radiological reports. Existing methods often hallucinate details in text-based reports that don't accurately reflect the image content. To mitigate this, we introduce a novel strategy, SERPENT-VLM (SElf Refining Radiology RePort GENeraTion using Vision Language Models), which improves the R2Gen task by integrating a self-refining mechanism into the MLLM framework. We employ a unique self-supervised loss that leverages similarity between pooled image representations and the contextual representations of the generated radiological text, alongside the standard Causal Language Modeling objective, to refine image-text representations. This allows the model to scrutinize and align the generated text through dynamic interaction between a given image and the generated text, therefore reducing hallucination and continuously enhancing nuanced report generation. SERPENT-VLM outperforms existing baselines such as LLaVA-Med, BiomedGPT, etc., achieving SoTA performance on the IU X-ray and Radiology Objects in COntext (ROCO) datasets, and also proves to be robust against noisy images. A qualitative case study emphasizes the significant advancements towards more sophisticated MLLM frameworks for R2Gen, opening paths for further research into self-supervised refinement in the medical imaging domain.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel strategy, SERPENT-VLM (SElf Refining Radiology RePort GENeraTion using Vision Language Models), which improves the R2Gen task by integrating a self-refining mechanism into the MLLM framework, and proves to be robust against noisy images."
            },
            "score": 4
        },
        {
            "id": "7751f6cdec0f4473c1733eec91699744a7d5176f",
            "paperId": "7751f6cdec0f4473c1733eec91699744a7d5176f",
            "title": "HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding",
            "abstract": "While large vision-language models (LVLMs) have demonstrated impressive capabilities in interpreting multi-modal contexts, they invariably suffer from object hallucinations (OH). We introduce HALC, a novel decoding algorithm designed to mitigate OH in LVLMs. HALC leverages distinct fine-grained optimal visual information in vision-language tasks and operates on both local and global contexts simultaneously. Specifically, HALC integrates a robust auto-focal grounding mechanism (locally) to correct hallucinated tokens on the fly, and a specialized beam search algorithm (globally) to significantly reduce OH while preserving text generation quality. Additionally, HALC can be integrated into any LVLMs as a plug-and-play module without extra training. Extensive experimental studies demonstrate the effectiveness of HALC in reducing OH, outperforming state-of-the-arts across four benchmarks.",
            "year": 2024,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "HALC, a novel decoding algorithm designed to mitigate OH in LVLMs, is introduced, which leverages distinct fine-grained optimal visual information in vision-language tasks and operates on both local and global contexts simultaneously."
            },
            "score": 4
        },
        {
            "id": "afdeef9585232642d18e7c6a7942b2395e94ede1",
            "paperId": "afdeef9585232642d18e7c6a7942b2395e94ede1",
            "title": "A Cause-Effect Look at Alleviating Hallucination of Knowledge-grounded Dialogue Generation",
            "abstract": "Empowered by the large-scale pretrained language models, existing dialogue systems have demonstrated impressive performance conducting fluent and natural-sounding conversations. However, they are still plagued by the hallucination problem, causing unpredictable factual errors in the generated responses. Recently, knowledge-grounded dialogue generation models, that intentionally invoke external knowledge resources to more informative responses, are also proven to be effective in reducing hallucination. Following the idea of getting high-quality knowledge, a few efforts have achieved pretty good performance on this issue. As some inevitable knowledge noises may also lead to hallucinations, it is emergent to investigate the reason and future directions for building noise-tolerant methods in KGD tasks. In this paper, we analyze the causal story behind this problem with counterfactual reasoning methods. Based on the causal effect analysis, we propose a possible solution for alleviating the hallucination in KGD by exploiting the dialogue-knowledge interaction. Experimental results of our example implementation show that this method can reduce hallucination without disrupting other dialogue performance, while keeping adaptive to different generation models. We hope our efforts can support and call for more attention to developing lightweight techniques towards robust and trusty dialogue systems.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Based on the causal effect analysis, a possible solution for alleviating the hallucination in KGD by exploiting the dialogue-knowledge interaction is proposed and results show this method can reduce hallucination without disrupting other dialogue performance, while keeping adaptive to different generation models."
            },
            "score": 4
        },
        {
            "id": "3d68522abfadfc8ee6b7ec9edaaf91f1b2f38e5e",
            "paperId": "3d68522abfadfc8ee6b7ec9edaaf91f1b2f38e5e",
            "title": "Large Language Models Can Be Easily Distracted by Irrelevant Context",
            "abstract": "Large language models have achieved impressive performance on various natural language processing tasks. However, so far they have been evaluated primarily on benchmarks where all information in the input context is relevant for solving the task. In this work, we investigate the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context. In particular, we introduce Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description. We use this benchmark to measure the distractibility of cutting-edge prompting techniques for large language models, and find that the model performance is dramatically decreased when irrelevant information is included. We also identify several approaches for mitigating this deficiency, such as decoding with self-consistency and adding to the prompt an instruction that tells the language model to ignore the irrelevant information.",
            "year": 2023,
            "citationCount": 212,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates the distractibility of large language models, i.e., how the model problem-solving accuracy can be influenced by irrelevant context, and introduces Grade-School Math with Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant information in the problem description."
            },
            "score": 4
        },
        {
            "id": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
            "paperId": "4780d0a027c5c5a8e01d7cf697f6296880ffc945",
            "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such\"society of minds\"approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.",
            "year": 2023,
            "citationCount": 206,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer is presented, indicating that this approach significantly enhances mathematical and strategic reasoning across a number of tasks."
            },
            "score": 4
        },
        {
            "id": "32426b96ff3c680125bde3b835bfa931288b8ade",
            "paperId": "32426b96ff3c680125bde3b835bfa931288b8ade",
            "title": "Better Patching Using LLM Prompting, via Self-Consistency",
            "abstract": "Large Language models (LLMs) can be induced to solve non-trivial problems with \u201cfew-shot\u201d prompts including illustrative problem-solution examples. Now if the few-shots also include \u201cchain of thought\u201d ($\\mathcal{C}oT$) explanations, which are of the form problem-explanation-solution, LLMs will generate a \u201cexplained\u201d solution, and perform even better. Recently an exciting, substantially better technique, self-consistency [1] ($\\mathcal{S}-C$) has emerged, based on the intuition that there are many plausible explanations for the right solution; when the LLM is sampled repeatedly to generate a pool of explanation-solution pairs, for a given problem, the most frequently occurring solutions in the pool (ignoring the explanations) tend to be even more likely to be correct! Unfortunately, the use of this highly-performant $\\mathcal{S}-C$ (or even $\\mathcal{C}oT$) approach in software engineering settings is hampered by the lack of explanations; most software datasets lack explanations. In this paper, we describe an application of the $\\mathcal{S}-C$ approach to program repair, using the commit log on the fix as the explanation, only in the illustrative few-shots. We achieve state-of-the art results, beating previous approaches to prompting-based program repair, on the MODIT dataset; we also find evidence suggesting that the correct commit messages are helping the LLM learn to produce better patches.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper describes an application of the $\\mathcal{S}-C$ approach to program repair, using the commit log on the fix as the explanation, only in the illustrative few-shots, on the MODIT dataset."
            },
            "score": 4
        },
        {
            "id": "5e5f6e3620a9b472cfa8cb81b264f350cd77d5d4",
            "paperId": "5e5f6e3620a9b472cfa8cb81b264f350cd77d5d4",
            "title": "Look Before You Leap: Problem Elaboration Prompting Improves Mathematical Reasoning in Large Language Models",
            "abstract": "Large language models (LLMs) still grapple with complex tasks like mathematical reasoning. Despite significant efforts invested in improving prefix prompts or reasoning process, the crucial role of problem context might have been neglected. Accurate recognition of inputs is fundamental for solving mathematical tasks, as ill-formed problems could potentially mislead LLM's reasoning. In this study, we propose a new approach named Problem Elaboration Prompting (PEP) to enhance the mathematical capacities of LLMs. Specifically, PEP decomposes and elucidates the problem context before reasoning, therefore enhancing the context modeling and parsing efficiency. Experiments across datasets and models demonstrate promising performances: (1) PEP demonstrates an overall enhancement in various mathematical tasks. For instance, with the GPT-3.5 model, PEP exhibits improvements of 9.93% and 8.80% on GSM8k through greedy decoding and self-consistency, respectively. (2) PEP can be easily implemented and integrated with other prompting methods. (3) PEP shows particular strength in handling distraction problems.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new approach named Problem Elaboration Prompting (PEP) is proposed to enhance the mathematical capacities of LLMs by decomposing and elucidates the problem context before reasoning, therefore enhancing the context modeling and parsing efficiency."
            },
            "score": 4
        },
        {
            "id": "cd9f3efbe5995855e646ea1cfd368a3ed067d1a3",
            "paperId": "cd9f3efbe5995855e646ea1cfd368a3ed067d1a3",
            "title": "It's Not Easy Being Wrong: Large Language Models Struggle with Process of Elimination Reasoning",
            "abstract": "Chain-of-thought (COT) prompting can help large language models (LLMs) reason toward correct answers, but its efficacy in reasoning toward incorrect answers is unexplored. This process of elimination (PoE), when used with COT, can enhance self-consistency, interpretability, and tasks such as medical diagnoses of exclusion. Thus, we propose PoE with COT, where LLMs must reason toward incorrect options on multiple-choice questions. We evaluate the ability of GPT-3.5, LLaMA-2, and Falcon to perform PoE with COT on a total of four commonsense and scientific reasoning datasets. We find that the strategy of PoE always underperforms the strategy of choosing the correct answer. The agreement of these strategies is also lower than the self-consistency of each strategy. To study these issues further, we conduct error analyses and give suggestions for future work.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that the strategy of PoE always underperforms the strategy of choosing the correct answer, and the agreement of these strategies is also lower than the self-consistency of each strategy."
            },
            "score": 4
        },
        {
            "id": "7d305b87b37487136f0f96f451f9e95e09b12b46",
            "paperId": "7d305b87b37487136f0f96f451f9e95e09b12b46",
            "title": "It's Not Easy Being Wrong: Evaluating Process of Elimination Reasoning in Large Language Models",
            "abstract": "Chain-of-thought (COT) prompting can help large language models (LLMs) reason toward correct answers, but its efficacy in reasoning toward incorrect answers is unexplored. This strategy of process of elimination (PoE), when used with COT, has the potential to enhance interpretability in tasks like medical diagnoses of exclusion. Thus, we propose PoE with COT, a new task where LLMs must reason toward incorrect options on multiple-choice questions. We evaluate the ability of GPT-3.5, LLaMA-2, and Falcon to perform PoE with COT on 2-choice commonsense and scientific reasoning datasets. We show that PoE consistently underperforms directly choosing the correct answer. The agreement of these strategies is also lower than the self-consistency of each strategy. To study these issues further, we conduct an error analysis and give suggestions for future work.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes PoE with COT, a new task where LLMs must reason toward incorrect options on multiple-choice questions, and shows that PoE consistently underperforms directly choosing the correct answer."
            },
            "score": 4
        },
        {
            "id": "dbeee4f77aefc944e8daa30d9c907f1413f830d8",
            "paperId": "dbeee4f77aefc944e8daa30d9c907f1413f830d8",
            "title": "Beyond Self-Consistency: Ensemble Reasoning Boosts Consistency and Accuracy of LLMs in Cancer Staging",
            "abstract": "Advances in large language models (LLMs) have encouraged their adoption in the healthcare domain where vital clinical information is often contained in unstructured notes. Cancer staging status is available in clinical reports, but it requires natural language processing to extract the status from the unstructured text. With the advance in clinical-oriented LLMs, it is promising to extract such status without extensive efforts in training the algorithms. Prompting approaches of the pre-trained LLMs that elicit a model's reasoning process, such as chain-of-thought, may help to improve the trustworthiness of the generated responses. Using self-consistency further improves model performance, but often results in inconsistent generations across the multiple reasoning paths. In this study, we propose an ensemble reasoning approach with the aim of improving the consistency of the model generations. Using an open access clinical large language model to determine the pathologic cancer stage from real-world pathology reports, we show that the ensemble reasoning approach is able to improve both the consistency and performance of the LLM in determining cancer stage, thereby demonstrating the potential to use these models in clinical or other domains where reliability and trustworthiness are critical.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The ensemble reasoning approach is able to improve both the consistency and performance of the LLM in determining cancer stage, thereby demonstrating the potential to use these models in clinical or other domains where reliability and trustworthiness are critical."
            },
            "score": 4
        },
        {
            "id": "e96348576b682e709b2ee06ea28cd81f8bfa102b",
            "paperId": "e96348576b682e709b2ee06ea28cd81f8bfa102b",
            "title": "Exploring the Factual Consistency in Dialogue Comprehension of Large Language Models",
            "abstract": "LLMs (Large Language Models) usually interact with users in the form of dialogue and generate responses following their instructions, which naturally require dialogue comprehension abilities. However, dialogue comprehension is a general language ability which is hard to be evaluated directly. In this work, we propose to perform the evaluation focusing on the factual consistency issue with the help of the dialogue summarization task. Besides evaluating and analyzing the dialogue summarization performance (DIAC-Sum) of different LLMs, we also derive factual questions from the generated summaries and use them as a more flexible measurement of dialogue comprehension (DIAC-QA). Our evaluation shows that, on average, 26.8% of the summaries generated by LLMs contain factual inconsistency. Even ChatGPT, the strongest model evaluated, has such errors in 16% of its summaries. For answering the factual questions, which is more challenging, the average error rate of all evaluated LLMs is 36.1%. Both results indicate serious deficiencies. Detailed analysis shows that the understanding of subject/object of the conversation is still challenging for LLMs. Furthermore, to stimulate and enhance the dialogue comprehension ability of LLMs, we propose a fine-tuning paradigm with auto-constructed multi-task data, which achieved a relative error rate reduction of 11% on DIAC-QA.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "To stimulate and enhance the dialogue comprehension ability of LLMs, a fine-tuning paradigm with auto-constructed multi-task data is proposed, which achieved a relative error rate reduction of 11% on DIAC-QA."
            },
            "score": 4
        },
        {
            "id": "d998ff93b2e58c9b219a9103f8c2ad714a41e4b9",
            "paperId": "d998ff93b2e58c9b219a9103f8c2ad714a41e4b9",
            "title": "The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency of Language Models",
            "abstract": "Large Language Models (LLMs) make natural interfaces to factual knowledge, but their usefulness is limited by their tendency to deliver inconsistent answers to semantically equivalent questions. For example, a model might predict both\"Anne Redpath passed away in Edinburgh.\"and\"Anne Redpath's life ended in London.\"In this work, we identify potential causes of inconsistency and evaluate the effectiveness of two mitigation strategies: up-scaling and augmenting the LM with a retrieval corpus. Our results on the LLaMA and Atlas models show that both strategies reduce inconsistency while retrieval augmentation is considerably more efficient. We further consider and disentangle the consistency contributions of different components of Atlas. For all LMs evaluated we find that syntactical form and other evaluation task artifacts impact consistency. Taken together, our results provide a better understanding of the factors affecting the factual consistency of language models.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work identifies potential causes of inconsistency and evaluates the effectiveness of two mitigation strategies: up-scaling and augmenting the LM with a retrieval corpus, showing that both strategies reduce inconsistency while retrieval augmentation is considerably more efficient."
            },
            "score": 4
        },
        {
            "id": "6674e254f343cdb2511b84d4bf120182fb112b67",
            "paperId": "6674e254f343cdb2511b84d4bf120182fb112b67",
            "title": "Evaluating Large Language Models for Health-related Queries with Presuppositions",
            "abstract": "As corporations rush to integrate large language models (LLMs) to their search offerings, it is critical that they provide factually accurate information that is robust to any presuppositions that a user may express. In this work, we introduce UPHILL, a dataset consisting of health-related queries with varying degrees of presuppositions. Using UPHILL, we evaluate the factual accuracy and consistency of InstructGPT, ChatGPT, and BingChat models. We find that while model responses rarely disagree with true health claims (posed as questions), they often fail to challenge false claims: responses from InstructGPT agree with 32% of the false claims, ChatGPT 26% and BingChat 23%. As we increase the extent of presupposition in input queries, the responses from InstructGPT and ChatGPT agree with the claim considerably more often, regardless of its veracity. Responses from BingChat, which rely on retrieved webpages, are not as susceptible. Given the moderate factual accuracy, and the inability of models to consistently correct false assumptions, our work calls for a careful assessment of current LLMs for use in high-stakes scenarios.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "UPHILL, a dataset consisting of health-related queries with varying degrees of presuppositions, is introduced and it is found that while model responses rarely disagree with true health claims, they often fail to challenge false claims."
            },
            "score": 4
        },
        {
            "id": "25243632a6159c19db280e2f0064aa59562a518a",
            "paperId": "25243632a6159c19db280e2f0064aa59562a518a",
            "title": "Retrieve Only When It Needs: Adaptive Retrieval Augmentation for Hallucination Mitigation in Large Language Models",
            "abstract": "Hallucinations pose a significant challenge for the practical implementation of large language models (LLMs). The utilization of parametric knowledge in generating factual content is constrained by the limited knowledge of LLMs, potentially resulting in internal hallucinations. While incorporating external information can help fill knowledge gaps, it also introduces the risk of irrelevant information, thereby increasing the likelihood of external hallucinations. A careful and balanced integration of the parametric knowledge within LLMs with external information is crucial to alleviate hallucinations. In this study, we present Rowen, a novel approach that enhances LLMs with a selective retrieval augmentation process tailored to address hallucinated outputs. This process is governed by a multilingual semantic-aware detection module, which evaluates the consistency of the perturbed responses across various languages for the same queries. Upon detecting inconsistencies indicative of hallucinations, Rowen activates the retrieval of external information to rectify the model outputs. Rowen adeptly harmonizes the intrinsic parameters in LLMs with external knowledge sources, effectively mitigating hallucinations by ensuring a balanced integration of internal reasoning and external evidence. Through a comprehensive empirical analysis, we demonstrate that Rowen surpasses the current state-of-the-art in both detecting and mitigating hallucinated content within the outputs of LLMs.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study presents Rowen, a novel approach that enhances LLMs with a selective retrieval augmentation process tailored to address hallucinated outputs, which surpasses the current state-of-the-art in both detecting and mitigating hallucinated content within the outputs of LLMs."
            },
            "score": 4
        },
        {
            "id": "50daeb780c4cc7be9f71bb5c412e460daf4b2c29",
            "paperId": "50daeb780c4cc7be9f71bb5c412e460daf4b2c29",
            "title": "Fake News Detection via NLP is Vulnerable to Adversarial Attacks",
            "abstract": "News plays a significant role in shaping people's beliefs and opinions. Fake news has always been a problem, which wasn't exposed to the mass public until the past election cycle for the 45th President of the United States. While quite a few detection methods have been proposed to combat fake news since 2015, they focus mainly on linguistic aspects of an article without any fact checking. In this paper, we argue that these models have the potential to misclassify fact-tampering fake news as well as under-written real news. Through experiments on Fakebox, a state-of-the-art fake news detector, we show that fact tampering attacks can be effective. To address these weaknesses, we argue that fact checking should be adopted in conjunction with linguistic characteristics analysis, so as to truly separate fake news from real news. A crowdsourced knowledge graph is proposed as a straw man solution to collecting timely facts about news events.",
            "year": 2019,
            "citationCount": 89,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is argued that fact checking should be adopted in conjunction with linguistic characteristics analysis, so as to truly separate fake news from real news."
            },
            "score": 4
        },
        {
            "id": "300b01dc726fe8acbededd805501811d427920bd",
            "paperId": "300b01dc726fe8acbededd805501811d427920bd",
            "title": "X-PARADE: Cross-Lingual Textual Entailment and Information Divergence across Paragraphs",
            "abstract": "Understanding when two pieces of text convey the same information is a goal touching many subproblems in NLP, including textual entailment and fact-checking. This problem becomes more complex when those two pieces of text are in different languages. Here, we introduce X-PARADE (Cross-lingual Paragraph-level Analysis of Divergences and Entailments), the first cross-lingual dataset of paragraph-level information divergences. Annotators label a paragraph in a target language at the span level and evaluate it with respect to a corresponding paragraph in a source language, indicating whether a given piece of information is the same, new, or new but can be inferred. This last notion establishes a link with cross-language NLI. Aligned paragraphs are sourced from Wikipedia pages in different languages, reflecting real information divergences observed in the wild. Armed with our dataset, we investigate a diverse set of approaches for this problem, including token alignment from machine translation, textual entailment methods that localize their decisions, and prompting LLMs. Our results show that these methods vary in their capability to handle inferable information, but they all fall short of human performance.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "X-PARADE (Cross-lingual Paragraph-level Analysis of Divergences and Entailments), the first cross-lingual dataset of paragraph-level information divergences, is introduced and a diverse set of approaches for this problem are investigated, including token alignment from machine translation, textual entailment methods that localize their decisions, and prompting LLMs."
            },
            "score": 4
        },
        {
            "id": "99f53c35aaeec3c7c4a02eb3ca30a303a3d8291b",
            "paperId": "99f53c35aaeec3c7c4a02eb3ca30a303a3d8291b",
            "title": "Do We Need Language-Specific Fact-Checking Models? The Case of Chinese",
            "abstract": "This paper investigates the potential benefits of language-specific fact-checking models, focusing on the case of Chinese. We first demonstrate the limitations of translation-based methods and multilingual large language models (e.g., GPT-4), highlighting the need for language-specific systems. We further propose a Chinese fact-checking system that can better retrieve evidence from a document by incorporating context information. To better analyze token-level biases in different systems, we construct an adversarial dataset based on the CHEF dataset, where each instance has large word overlap with the original one but holds the opposite veracity label. Experimental results on the CHEF dataset and our adversarial dataset show that our proposed method outperforms translation-based methods and multilingual LLMs and is more robust toward biases, while there is still large room for improvement, emphasizing the importance of language-specific fact-checking systems.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Chinese fact-checking system that can better retrieve evidence from a document by incorporating context information and is more robust toward biases is proposed, emphasizing the importance of language-specific fact-checking systems."
            },
            "score": 3
        },
        {
            "id": "c789ba25da6372b55743a0073f56ffb434b066fc",
            "paperId": "c789ba25da6372b55743a0073f56ffb434b066fc",
            "title": "Automated Claim Detection for Fact-checking: A Case Study using Norwegian Pre-trained Language Models",
            "abstract": "We investigate to what extent pre-trained language models can be used for automated claim detection for fact-checking in a low resource setting. We explore this idea by fine-tuning four Norwegian pre-trained language models to perform the binary classification task of determining if a claim should be discarded or upheld to be further processed by human fact-checkers. We conduct a set of experiments to compare the performance of the language models, and provide a simple baseline model using SVM with tf-idf features. Since we are focusing on claim detection, the recall score for the upheld class is to be emphasized over other performance measures. Our experiments indicate that the language models are superior to the baseline system in terms of F1, while the baseline model results in the highest precision. However, the two Norwegian models, NorBERT2 and NB-BERT_large, give respectively superior F1 and recall values. We argue that large language models could be successfully employed to solve the automated claim detection problem. The choice of the model depends on the desired end-goal. Moreover, our error analysis shows that language models are generally less sensitive to the changes in claim length and source than the SVM model.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is argued that large language models could be successfully employed to solve the automated claim detection problem and are generally less sensitive to the changes in claim length and source than the SVM model."
            },
            "score": 3
        },
        {
            "id": "fbcbba5c5bd921e009c615046df562247b45a32a",
            "paperId": "fbcbba5c5bd921e009c615046df562247b45a32a",
            "title": "Fact-checking benchmark for the Russian Large Language Models",
            "abstract": "Modern text-generative language models are rapidly developing. They produce text of high quality and are used in many real-world applications. However, they still have several limitations, for instance, the length of the context, degeneration processes, lack of logical structure, and facts consistency. In this work, we focus on the fact-checking problem applied to the output of the generative models on classical downstream tasks, such as paraphrasing, summarization, text style transfer, etc. We define the task of internal fact-checking, set the criteria for factual consistency, and present the novel dataset for this task for the Russian language. The benchmark for internal fact-checking and several baselines are also provided. We research data augmentation approaches to extend the training set and compare classification methods on different augmented data sets.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work defines the task of internal fact-checking, set the criteria for factual consistency, and presents the novel dataset for this task for the Russian language, along with the benchmark for internal fact-checking and several baselines."
            },
            "score": 3
        },
        {
            "id": "86d3fc220453e91076eeac6374e784b1bc75374c",
            "paperId": "86d3fc220453e91076eeac6374e784b1bc75374c",
            "title": "Surprising Efficacy of Fine-Tuned Transformers for Fact-Checking over Larger Language Models",
            "abstract": "In this paper, we explore the challenges associated with establishing an end-to-end fact-checking pipeline in a real-world context, covering over 90 languages. Our real-world experimental benchmarks demonstrate that fine-tuning Transformer models specifically for fact-checking tasks, such as claim detection and veracity prediction, provide superior performance over large language models (LLMs) like GPT-4, GPT-3.5-Turbo, and Mistral-7b. However, we illustrate that LLMs excel in generative tasks such as question decomposition for evidence retrieval. Through extensive evaluation, we show the efficacy of fine-tuned models for fact-checking in a multilingual setting and complex claims that include numerical quantities.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Through extensive evaluation, the efficacy of fine-tuned models for fact-checking in a multilingual setting and complex claims that include numerical quantities are shown."
            },
            "score": 3
        },
        {
            "id": "edfaf6c4189c33da487f303a4fb2c93ffff74951",
            "paperId": "edfaf6c4189c33da487f303a4fb2c93ffff74951",
            "title": "Ensemble learning with soft-prompted pretrained language models for fact checking",
            "abstract": null,
            "year": 2024,
            "citationCount": 0,
            "tldr": null,
            "score": 3
        },
        {
            "id": "3496d82c96095c3e4cebf1ef9aa7a7356c578660",
            "paperId": "3496d82c96095c3e4cebf1ef9aa7a7356c578660",
            "title": "Extracting, and not extracting, knowledge from language models for fact-checking",
            "abstract": "Fact-checking is a challenging and useful classification task where a model evaluates the truthfulness of a natural-language claim. Researchers have taken a variety of approaches to building automated fact-checking systems, but recent work has introduced a paradigm that queries a language model to extract factual knowledge. We implement and evaluate several extensions to that pipeline, including alternate strategies for masking the claim and selecting the tokens that the language models predicts for masks. Evaluating these alternatives on a well-known fact-checking dataset, we find that they find that they have minimal impact on overall performance. Motivated by this finding, we construct a drastically simplified version of the pipeline \u2014 removing the language model \u2014 and find that its accuracy changes little. While its performance remains below state-of-art, these surprising results highlight difficulties in extracting knowledge from language models and introduce a new (to our knowledge) kind of entailment-based fact-checking baseline that involves no language model, corpus or knowledge base.",
            "year": 2021,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A drastically simplified version of the pipeline is constructed \u2014 removing the language model \u2014 and it is found that its accuracy changes little, highlighting difficulties in extracting knowledge from language models and introducing a new kind of entailment-based fact-checking baseline that involves no language model, corpus or knowledge base."
            },
            "score": 3
        },
        {
            "id": "d91645280619f93f3ce1da975f569ee17f2fadbc",
            "paperId": "d91645280619f93f3ce1da975f569ee17f2fadbc",
            "title": "Applicability of Language Models to Fact Checking",
            "abstract": "Modern information platforms are becoming increasingly crowd-sourced and editable by the general public (e.g. Wikipedia) and are consequently easily affected by large quantities of false statements. The sheer volume of information makes it near impossible for humans to regulate, and thus it is becoming important to automatically identify inconsistent and factually false information. However traditional fact checking requires access to large, veri\ufb01ed information stores which is both slow and expensive. Since language models are trained on such data sets they potentially implicitly learn factual contexts that may be leveraged for passive automated fact-checking. We investigate the use of a Language Model based approach to fact-checking that is based on computing a heuristic likelihood of individual facts. We explore 3 state-of-the-art Language Models across different threshold models and evaluate the precision and recall over a small fact update data-set.",
            "year": 2019,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates the use of a Language Model based approach to fact-checking that is based on computing a heuristic likelihood of individual facts and explores 3 state-of-the-art Language Models across different threshold models and evaluates the precision and recall over a small fact update data-set."
            },
            "score": 3
        },
        {
            "id": "8e0dbc206db278c29c4b70eae2060db2818f72dd",
            "paperId": "8e0dbc206db278c29c4b70eae2060db2818f72dd",
            "title": "Generalization and Hallucination of Large Vision-Language Models through a Camouflaged Lens",
            "abstract": "Large Vision-Language Model (LVLM) has seen burgeoning development and increasing attention recently. In this paper, we propose a novel framework, camo-perceptive vision-language framework (CPVLF), to explore whether LVLM can generalize to the challenging camouflaged object detection (COD) scenario in a training-free manner. During the process of generalization, we find that due to hallucination issues within LVLM, it can erroneously perceive objects in camouflaged scenes, producing counterfactual concepts. Moreover, as LVLM is not specifically trained for the precise localization of camouflaged objects, it exhibits a degree of uncertainty in accurately pinpointing these objects. Therefore, we propose chain of visual perception, which enhances LVLM's perception of camouflaged scenes from both linguistic and visual perspectives, reducing the hallucination issue and improving its capability in accurately locating camouflaged objects. We validate the effectiveness of CPVLF on three widely used COD datasets, and the experiments show the potential of LVLM in the COD task.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel framework, camo-perceptive vision-language framework (CPVLF), is proposed, which enhances LVLM's perception of camouflaged scenes from both linguistic and visual perspectives, reducing the hallucination issue and improving its capability in accurately locating camouflaged objects."
            },
            "score": 3
        },
        {
            "id": "f767f0a883c8fc70de03fb8b65ed87e1fef5f415",
            "paperId": "f767f0a883c8fc70de03fb8b65ed87e1fef5f415",
            "title": "Hallucination Diversity-Aware Active Learning for Text Summarization",
            "abstract": "Large Language Models (LLMs) have shown propensity to generate hallucinated outputs, i.e., texts that are factually incorrect or unsupported. Existing methods for alleviating hallucinations typically require costly human annotations to identify and correct hallucinations in LLM outputs. Moreover, most of these methods focus on a specific type of hallucination, e.g., entity or token errors, which limits their effectiveness in addressing various types of hallucinations exhibited in LLM outputs. To our best knowledge, in this paper we propose the first active learning framework to alleviate LLM hallucinations, reducing costly human annotations of hallucination needed. By measuring fine-grained hallucinations from errors in semantic frame, discourse and content verifiability in text summarization, we propose HAllucination Diversity-Aware Sampling (HADAS) to select diverse hallucinations for annotations in active learning for LLM finetuning. Extensive experiments on three datasets and different backbone models demonstrate advantages of our method in effectively and efficiently mitigating LLM hallucinations.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes the first active learning framework to alleviate LLM hallucinations, reducing costly human annotations of hallucination needed, and proposes HAllucination Diversity-Aware Sampling (HADAS) to select diverse hallucinations for annotations in active learning for LLM finetuning."
            },
            "score": 3
        },
        {
            "id": "2cf1f6c723006f258599fd9f000bb616ae83387a",
            "paperId": "2cf1f6c723006f258599fd9f000bb616ae83387a",
            "title": "Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models",
            "abstract": "The performance of large language models (LLMs) on existing reasoning benchmarks has significantly improved over the past years. In response, we present JEEBench, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs. We curate 515 challenging pre-engineering mathematics, physics and chemistry problems from the highly competitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep in-domain knowledge is essential for solving problems in this benchmark. Our evaluation on various open-source and proprietary models reveals that the highest performance, even after using techniques like self-consistency, self-refinement and chain-of-thought prompting, is less than 40%. The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts. We also observe that by mere prompting, GPT-4 is unable to assess risk introduced by negative marking for incorrect answers. For this, we develop a post-hoc confidence-thresholding method over self-consistency, which enables effective response selection. We hope that our challenging benchmark will guide future re-search in problem-solving using LLMs.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "JEEBench is presented, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs, and a post-hoc confidence-thresholding method over self-consistency is developed, which enables effective response selection."
            },
            "score": 3
        },
        {
            "id": "45314de9beef18dcce99f0bc5e067446a0196505",
            "paperId": "45314de9beef18dcce99f0bc5e067446a0196505",
            "title": "OpenMedLM: Prompt engineering can out-perform fine-tuning in medical question-answering with open-source large language models",
            "abstract": "LLMs have become increasingly capable at accomplishing a range of specialized-tasks and can be utilized to expand equitable access to medical knowledge. Most medical LLMs have involved extensive fine-tuning, leveraging specialized medical data and significant, thus costly, amounts of computational power. Many of the top performing LLMs are proprietary and their access is limited to very few research groups. However, open-source (OS) models represent a key area of growth for medical LLMs due to significant improvements in performance and an inherent ability to provide the transparency and compliance required in healthcare. We present OpenMedLM, a prompting platform which delivers state-of-the-art (SOTA) performance for OS LLMs on medical benchmarks. We evaluated a range of OS foundation LLMs (7B-70B) on four medical benchmarks (MedQA, MedMCQA, PubMedQA, MMLU medical-subset). We employed a series of prompting strategies, including zero-shot, few-shot, chain-of-thought (random selection and kNN selection), and ensemble/self-consistency voting. We found that OpenMedLM delivers OS SOTA results on three common medical LLM benchmarks, surpassing the previous best performing OS models that leveraged computationally costly extensive fine-tuning. The model delivers a 72.6% accuracy on the MedQA benchmark, outperforming the previous SOTA by 2.4%, and achieves 81.7% accuracy on the MMLU medical-subset, establishing itself as the first OS LLM to surpass 80% accuracy on this benchmark. Our results highlight medical-specific emergent properties in OS LLMs which have not yet been documented to date elsewhere, and showcase the benefits of further leveraging prompt engineering to improve the performance of accessible LLMs for medical applications.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "OpenMedLM is presented, a prompting platform which delivers state-of-the-art (SOTA) performance for OS LLMs on medical benchmarks, and highlights medical-specific emergent properties in OS LLMs which have not yet been documented to date elsewhere."
            },
            "score": 3
        },
        {
            "id": "3867d999b550b57e6762f9d4b0114ee7551b2e2f",
            "paperId": "3867d999b550b57e6762f9d4b0114ee7551b2e2f",
            "title": "Predicting Question-Answering Performance of Large Language Models through Semantic Consistency",
            "abstract": "Semantic consistency of a language model is broadly defined as the model\u2019s ability to produce semantically-equivalent outputs, given semantically-equivalent inputs. We address the task of assessing question-answering (QA) semantic consistency of contemporary large language models (LLMs) by manually creating a benchmark dataset with high-quality paraphrases for factual questions, and release the dataset to the community.We further combine the semantic consistency metric with additional measurements suggested in prior work as correlating with LLM QA accuracy, for building and evaluating a framework for factual QA reference-less performance prediction \u2013 predicting the likelihood of a language model to accurately answer a question. Evaluating the framework on five contemporary LLMs, we demonstrate encouraging, significantly outperforming baselines, results.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work addresses the task of assessing question-answering (QA) semantic consistency of contemporary large language models (LLMs) by manually creating a benchmark dataset with high-quality paraphrases for factual questions, and releases the dataset to the community."
            },
            "score": 3
        },
        {
            "id": "468d1e2d75a23fecaf96fe65d8b01ff35ea5d0bd",
            "paperId": "468d1e2d75a23fecaf96fe65d8b01ff35ea5d0bd",
            "title": "Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models",
            "abstract": "Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts inserted in English reveal a clear pattern whereby the new piece of knowledge transfers only to languages with which English has a high RankC score.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy, and conducts an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level."
            },
            "score": 3
        },
        {
            "id": "d36bbbe2eb83981c4e714f1d4688334f2aae6369",
            "paperId": "d36bbbe2eb83981c4e714f1d4688334f2aae6369",
            "title": "GPT-4 as Evaluator: Evaluating Large Language Models on Pest Management in Agriculture",
            "abstract": "In the rapidly evolving field of artificial intelligence (AI), the application of large language models (LLMs) in agriculture, particularly in pest management, remains nascent. We aimed to prove the feasibility by evaluating the content of the pest management advice generated by LLMs, including the Generative Pre-trained Transformer (GPT) series from OpenAI and the FLAN series from Google. Considering the context-specific properties of agricultural advice, automatically measuring or quantifying the quality of text generated by LLMs becomes a significant challenge. We proposed an innovative approach, using GPT-4 as an evaluator, to score the generated content on Coherence, Logical Consistency, Fluency, Relevance, Comprehensibility, and Exhaustiveness. Additionally, we integrated an expert system based on crop threshold data as a baseline to obtain scores for Factual Accuracy on whether pests found in crop fields should take management action. Each model's score was weighted by percentage to obtain a final score. The results showed that GPT-3.4 and GPT-4 outperform the FLAN models in most evaluation categories. Furthermore, the use of instruction-based prompting containing domain-specific knowledge proved the feasibility of LLMs as an effective tool in agriculture, with an accuracy rate of 72%, demonstrating LLMs' effectiveness in providing pest management suggestions.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The use of instruction-based prompting containing domain-specific knowledge proved the feasibility of LLMs as an effective tool in agriculture, with an accuracy rate of 72%, demonstrating LLMs' effectiveness in providing pest management suggestions."
            },
            "score": 3
        },
        {
            "id": "70a75b05a9410198fd71c3a0fe937a77d15d6bc8",
            "paperId": "70a75b05a9410198fd71c3a0fe937a77d15d6bc8",
            "title": "Text2KGBench: A Benchmark for Ontology-Driven Knowledge Graph Generation from Text",
            "abstract": "The recent advances in large language models (LLM) and foundation models with emergent capabilities have been shown to improve the performance of many NLP tasks. LLMs and Knowledge Graphs (KG) can complement each other such that LLMs can be used for KG construction or completion while existing KGs can be used for different tasks such as making LLM outputs explainable or fact-checking in Neuro-Symbolic manner. In this paper, we present Text2KGBench, a benchmark to evaluate the capabilities of language models to generate KGs from natural language text guided by an ontology. Given an input ontology and a set of sentences, the task is to extract facts from the text while complying with the given ontology (concepts, relations, domain/range constraints) and being faithful to the input sentences. We provide two datasets (i) Wikidata-TekGen with 10 ontologies and 13,474 sentences and (ii) DBpedia-WebNLG with 19 ontologies and 4,860 sentences. We define seven evaluation metrics to measure fact extraction performance, ontology conformance, and hallucinations by LLMs. Furthermore, we provide results for two baseline models, Vicuna-13B and Alpaca-LoRA-13B using automatic prompt generation from test cases. The baseline results show that there is room for improvement using both Semantic Web and Natural Language Processing techniques.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents Text2KGBench, a benchmark to evaluate the capabilities of language models to generate KGs from natural language text guided by an ontology, and defines seven evaluation metrics to measure fact extraction performance, ontology conformance, and hallucinations by LLMs."
            },
            "score": 3
        },
        {
            "id": "cc23347d4a5c26f633103ef49bbf54019be9ee45",
            "paperId": "cc23347d4a5c26f633103ef49bbf54019be9ee45",
            "title": "Zero-Shot Fact-Checking with Semantic Triples and Knowledge Graphs",
            "abstract": "Despite progress in automated fact-checking, most systems require a significant amount of labeled training data, which is expensive. In this paper, we propose a novel zero-shot method, which instead of operating directly on the claim and evidence sentences, decomposes them into semantic triples augmented using external knowledge graphs, and uses large language models trained for natural language inference. This allows it to generalize to adversarial datasets and domains that supervised models require specific training data for. Our empirical results show that our approach outperforms previous zero-shot approaches on FEVER, FEVER-Symmetric, FEVER 2.0, and Climate-FEVER, while being comparable or better than supervised models on the adversarial and the out-of-domain datasets.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel zero-shot method, which instead of operating directly on the claim and evidence sentences, decomposes them into semantic triples augmented using external knowledge graphs, and uses large language models trained for natural language inference to generalize to adversarial datasets and domains that supervised models require specific training data for."
            },
            "score": 2
        },
        {
            "id": "ec2423e8a1092a099e222c727bd756d4a07f9113",
            "paperId": "ec2423e8a1092a099e222c727bd756d4a07f9113",
            "title": "MelAnalyze: Fact-Checking Melatonin claims using Large Language Models and Natural Language Inference",
            "abstract": "With the explosion of health related information in mainstream discourse, distinguishing accurate health-related claims from misinformation is important. Using computational tools and algorithms to help is key. Our focus in this paper is on the hormone Melatonin which is claimed to have broad health benefits and largely sold as a supplement. This paper introduces \u2018MelAnalyze,\u2019 a framework for using generative and transformer-based deep learning models adapted as a natural language inference (NLI) task, to semi-automate the fact-checking of general melatonin claims. MelAnalyze is built upon a comprehensive collection of melatonin-related scientific abstracts from PubMed for validation. The framework incorporates components for precise extraction of information from scientific literature, semantic similarity and NLI. At its core, MelAnalyze leverages pre trained NLI models that are fine-tuned on melatonin-specific claims along with semantic search based on vectorized representation of the articles. The best models, fine-tuned on LLaMA1 and RoBERTa, attain good precision, recall, and F1-scores of approximately 0.92. We also introduce a user-friendly web-based tool for fact-checking algorithm evaluation and use. In summary, we show MelAnalyze\u2019s role in empowering users and researchers to assess melatonin-related claims using evidence-based decision-making.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A framework for using generative and transformer-based deep learning models adapted as a natural language inference (NLI) task, to semi-automate the fact-checking of general melatonin claims and its role in empowering users and researchers to assess melatonin-related claims using evidence-based decision-making is shown."
            },
            "score": 2
        },
        {
            "id": "d16f8d624ab16c8bb35dde676f522b66a771d271",
            "paperId": "d16f8d624ab16c8bb35dde676f522b66a771d271",
            "title": "Large Language Models are Null-Shot Learners",
            "abstract": "This paper presents null-shot prompting. Null-shot prompting exploits hallucination in large language models (LLMs) by instructing LLMs to utilize information from the\"Examples\"section that never exists within the provided context to perform a task. While reducing hallucination is crucial and non-negligible for daily and critical uses of LLMs, we propose that in the current landscape in which these LLMs still hallucinate, it is possible, in fact, to exploit hallucination to increase performance in performing tasks compared to standard zero-shot prompting. Experiments with eight LLMs show improvements in performance across the majority of eight datasets, including reading comprehension, arithmetic reasoning, and closed-book question answering. The observed inconsistency in increased relative performance across the LLMs also potentially indicates a different degree of inherent hallucination in each model. These differences show that it is possible to utilize null-shot prompting as a way to detect degrees of hallucination in LLMs using existing benchmarking datasets. We also perform ablation studies, including experimenting with a modified version of null-shot prompting that incorporates ideas from zero-shot chain-of-thought prompting, which shows different trends of results.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experiments with eight LLMs show improvements in performance across the majority of eight datasets, including reading comprehension, arithmetic reasoning, and closed-book question answering, and differences show that it is possible to utilize null-shot prompting as a way to detect degrees of hallucination in LLMs using existing benchmarking datasets."
            },
            "score": 2
        },
        {
            "id": "5b9bf4a82da690e738821ac0460b96c2770ed5dd",
            "paperId": "5b9bf4a82da690e738821ac0460b96c2770ed5dd",
            "title": "Are Large Language Models Temporally Grounded?",
            "abstract": "Are Large language models (LLMs) temporally grounded? Since LLMs cannot perceive and interact with the environment, it is impossible to answer this question directly. Instead, we provide LLMs with textual narratives and probe them with respect to their common-sense knowledge of the structure and duration of events, their ability to order events along a timeline, and self-consistency within their temporal model (e.g., temporal relations such as after and before are mutually exclusive for any pair of events). We evaluate state-of-the-art LLMs (such as LLaMA 2 and GPT-4) on three tasks reflecting these abilities. Generally, we find that LLMs lag significantly behind both human performance as well as small-scale, specialised LMs. In-context learning, instruction tuning, and chain-of-thought prompting reduce this gap only to a limited degree. Crucially, LLMs struggle the most with self-consistency, displaying incoherent behaviour in at least 27.23% of their predictions. Contrary to expectations, we also find that scaling the model size does not guarantee positive gains in performance. To explain these results, we study the sources from which LLMs may gather temporal information: we find that sentence ordering in unlabelled texts, available during pre-training, is only weakly correlated with event ordering. Moreover, public instruction tuning mixtures contain few temporal tasks. Hence, we conclude that current LLMs lack a consistent temporal model of textual narratives. Code, datasets, and LLM outputs are available at https://github.com/yfqiu-nlp/temporal-llms.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is concluded that current LLMs lack a consistent temporal model of textual narratives, and study the sources from which LLMs may gather temporal information finds that sentence ordering in unlabelled texts is only weakly correlated with event ordering."
            },
            "score": 2
        },
        {
            "id": "2f33488d1e4d405f84b1f15021c0870aec8bf680",
            "paperId": "2f33488d1e4d405f84b1f15021c0870aec8bf680",
            "title": "Exploring the pitfalls of large language models: Inconsistency and inaccuracy in answering pathology board examination\u2010style questions",
            "abstract": "In the rapidly advancing field of artificial intelligence, large language models (LLMs) such as ChatGPT and Google Bard are making significant progress, with applications extending across various fields, including medicine. This study explores their potential utility and pitfalls by assessing the performance of these LLMs in answering 150 multiple-choice questions sourced from the PathologyOutlines.com Question Bank, a well-established resource for pathology examination preparation. The assessment, encompassing 15 subspecialties in pathology, evaluated the accuracy and consistency of responses by these LLMs. Overall, ChatGPT outperformed Google Bard, scoring 122 out of 150, while Google Bard achieved a score of 70. In addition to accuracy, we explored the consistency of these LLMs by applying a test-retest approach over a two-week interval. ChatGPT showed a consistency rate of 85%, while Google Bard exhibited a lower consistency rate of 61%. In-depth analysis of incorrect responses identified potential factual inaccuracies and interpretive errors, underscoring the need for ongoing model refinement and human oversight. In conclusion, while LLMs have potential to enhance medical education and assist clinical decision-making, their current limitations underscore the need for continued development and the critical role of human expertise in the application of such models.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "While LLMs have potential to enhance medical education and assist clinical decision-making, their current limitations underscore the need for continued development and the critical role of human expertise in the application of such models."
            },
            "score": 2
        },
        {
            "id": "3d3a55074000b375625c6233332f08648430d413",
            "paperId": "3d3a55074000b375625c6233332f08648430d413",
            "title": "Large Legal Fictions: Profiling Legal Hallucinations in Large Language Models",
            "abstract": "Large language models (LLMs) have the potential to transform the practice of law, but this potential is threatened by the presence of legal hallucinations -- responses from these models that are not consistent with legal facts. We investigate the extent of these hallucinations using an original suite of legal queries, comparing LLMs' responses to structured legal metadata and examining their consistency. Our work makes four key contributions: (1) We develop a typology of legal hallucinations, providing a conceptual framework for future research in this area. (2) We find that legal hallucinations are alarmingly prevalent, occurring between 69% of the time with ChatGPT 3.5 and 88% with Llama 2, when these models are asked specific, verifiable questions about random federal court cases. (3) We illustrate that LLMs often fail to correct a user's incorrect legal assumptions in a contra-factual question setup. (4) We provide evidence that LLMs cannot always predict, or do not always know, when they are producing legal hallucinations. Taken together, these findings caution against the rapid and unsupervised integration of popular LLMs into legal tasks. Even experienced lawyers must remain wary of legal hallucinations, and the risks are highest for those who stand to benefit from LLMs the most -- pro se litigants or those without access to traditional legal resources.",
            "year": 2024,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A typology of legal hallucinations is developed, providing a conceptual framework for future research in this area and providing evidence that LLMs cannot always predict, or do not always know, when they are producing legal hallucinations."
            },
            "score": 2
        },
        {
            "id": "e4ae6598b04aa73985145f388871af7937be787d",
            "paperId": "e4ae6598b04aa73985145f388871af7937be787d",
            "title": "Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning",
            "abstract": "Recently, increasing attention has been focused drawn on to improve the ability of Large Language Models (LLMs) to perform complex reasoning. However, previous methods, such as Chain-of-Thought and Self-Consistency, mainly follow Direct Reasoning (DR) frameworks, so they will meet difficulty in solving numerous real-world tasks which can hardly be solved via DR. Therefore, to strengthen the reasoning power of LLMs, this paper proposes a novel Indirect Reasoning (IR) method that employs the logic of contrapositives and contradictions to tackle IR tasks such as factual reasoning and mathematic proof. Specifically, our methodology comprises two steps. Firstly, we leverage the logical equivalence of contrapositive to augment the data and rules to enhance the comprehensibility of LLMs. Secondly, we design a set of prompt templates to trigger LLMs to conduct IR based on proof by contradiction that is logically equivalent to the original DR process. Our IR method is simple yet effective and can be straightforwardly integrated with existing DR methods to further boost the reasoning abilities of LLMs. The experimental results on popular LLMs, such as GPT-3.5-turbo and Gemini-pro, show that our IR method enhances the overall accuracy of factual reasoning by 27.33% and mathematical proof by 31.43%, when compared with traditional DR methods. Moreover, the methods combining IR and DR significantly outperform the methods solely using IR or DR, further demonstrating the effectiveness of our strategy.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel Indirect Reasoning (IR) method that employs the logic of contrapositives and contradictions to tackle IR tasks such as factual reasoning and mathematic proof to strengthen the reasoning power of LLMs is proposed."
            },
            "score": 2
        },
        {
            "id": "318d7da35307221267b6ce6ead995cc812245abb",
            "paperId": "318d7da35307221267b6ce6ead995cc812245abb",
            "title": "Fake News Detection on Social Media using Geometric Deep Learning",
            "abstract": "Social media are nowadays one of the main news sources for millions of people around the globe due to their low cost, easy access and rapid dissemination. This however comes at the cost of dubious trustworthiness and significant risk of exposure to 'fake news', intentionally written to mislead the readers. Automatically detecting fake news poses challenges that defy existing content-based analysis approaches. One of the main reasons is that often the interpretation of the news requires the knowledge of political or social context or 'common sense', which current NLP algorithms are still missing. Recent studies have shown that fake and real news spread differently on social media, forming propagation patterns that could be harnessed for the automatic fake news detection. Propagation-based approaches have multiple advantages compared to their content-based counterparts, among which is language independence and better resilience to adversarial attacks. In this paper we show a novel automatic fake news detection model based on geometric deep learning. The underlying core algorithms are a generalization of classical CNNs to graphs, allowing the fusion of heterogeneous data such as content, user profile and activity, social graph, and news propagation. Our model was trained and tested on news stories, verified by professional fact-checking organizations, that were spread on Twitter. Our experiments indicate that social network structure and propagation are important features allowing highly accurate (92.7% ROC AUC) fake news detection. Second, we observe that fake news can be reliably detected at an early stage, after just a few hours of propagation. Third, we test the aging of our model on training and testing data separated in time. Our results point to the promise of propagation-based approaches for fake news detection as an alternative or complementary strategy to content-based approaches.",
            "year": 2019,
            "citationCount": 380,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel automatic fake news detection model based on geometric deep learning that can be reliably detected at an early stage, after just a few hours of propagation, and the results point to the promise of propagation-based approaches forfake news detection as an alternative or complementary strategy to content-based approach."
            },
            "score": 2
        }
    ],
    "novelty": "no"
}