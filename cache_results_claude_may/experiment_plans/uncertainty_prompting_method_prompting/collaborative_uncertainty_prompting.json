{
    "topic_description": "novel prompting methods that can better quantify uncertainty or calibrate the confidence of large language models",
    "idea_name": "Collaborative Uncertainty Prompting",
    "raw_idea": {
        "Problem": "Large language models often produce overconfident predictions when dealing with ambiguous or subjective tasks, such as sentiment analysis or hate speech detection. This leads to poorly calibrated confidence scores and limits the models' ability to defer to human judgment in uncertain cases.",
        "Existing Methods": "Current approaches to handling subjectivity and ambiguity in language understanding tasks include using ensemble methods, calibrating confidence scores based on human annotations, and incorporating external knowledge sources. However, these methods often fail to capture the inherent uncertainty in subjective tasks and do not effectively leverage human expertise.",
        "Motivation": "By designing prompts that encourage collaboration between the language model and human experts, we can potentially improve the calibration of confidence scores and enable the model to defer to human judgment when faced with highly uncertain or subjective cases.",
        "Proposed Method": "We propose Collaborative Uncertainty Prompting (CUP), a novel prompting technique that facilitates collaboration between the language model and human experts in subjective language understanding tasks. The steps are as follows: 1) Given an input example, prompt the language model to predict the label and estimate its confidence score. 2) If the confidence score falls below a predefined threshold, indicating high uncertainty, generate a prompt that asks the model to identify the most ambiguous or subjective aspects of the input example. 3) Present the identified ambiguous aspects to a human expert and prompt them to provide their interpretation or judgment. 4) Incorporate the human expert's feedback into the model's prediction by updating the confidence score based on the expert's judgment and the model's initial uncertainty estimate. 5) If the updated confidence score still falls below the threshold, prompt the model to defer the final decision to the human expert.",
        "Experiment Plan": "Evaluate CUP on benchmark datasets for subjective language understanding tasks, such as sentiment analysis (e.g., SST-2) and hate speech detection (e.g., OffensEval). Compare the calibration performance (e.g., ECE, MCE) and human agreement scores of CUP with baseline methods such as direct prediction and human-in-the-loop calibration. Conduct user studies to assess the effectiveness of CUP in facilitating collaboration between the model and human experts, and measure the impact of human feedback on the model's performance and calibration. Analyze the trade-off between the model's autonomy and the reliance on human expertise in different uncertainty thresholds and task settings."
    },
    "full_experiment_plan": {
        "Title": "Collaborative Uncertainty Prompting: Leveraging Human Expertise for Improved Confidence Calibration in Subjective Language Understanding Tasks",
        "Problem Statement": "Large language models often produce overconfident predictions when dealing with ambiguous or subjective tasks, such as sentiment analysis or hate speech detection. This leads to poorly calibrated confidence scores and limits the models' ability to defer to human judgment in uncertain cases.",
        "Motivation": "Current approaches to handling subjectivity and ambiguity in language understanding tasks, such as ensemble methods, calibrating confidence scores based on human annotations, and incorporating external knowledge sources, often fail to capture the inherent uncertainty in subjective tasks and do not effectively leverage human expertise. By designing prompts that encourage collaboration between the language model and human experts, we can potentially improve the calibration of confidence scores and enable the model to defer to human judgment when faced with highly uncertain or subjective cases.",
        "Proposed Method": "Collaborative Uncertainty Prompting (CUP) is a novel prompting technique that facilitates collaboration between the language model and human experts in subjective language understanding tasks. The steps are as follows:\n1. Given an input example, prompt the language model to predict the label and estimate its confidence score.\n2. If the confidence score falls below a predefined threshold, indicating high uncertainty, generate a prompt that asks the model to identify the most ambiguous or subjective aspects of the input example.\n3. Present the identified ambiguous aspects to a human expert and prompt them to provide their interpretation or judgment.\n4. Incorporate the human expert's feedback into the model's prediction by updating the confidence score based on the expert's judgment and the model's initial uncertainty estimate.\n5. If the updated confidence score still falls below the threshold, prompt the model to defer the final decision to the human expert.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "Evaluate CUP on benchmark datasets for subjective language understanding tasks, such as sentiment analysis (e.g., SST-2) and hate speech detection (e.g., OffensEval).",
            "Step 2: Construct Prompts": "1. Initial prediction prompt: 'Given the input text: [input_text], predict the [task_name] label and provide a confidence score between 0 and 1.'\n2. Ambiguity identification prompt (if confidence score < threshold): 'Given the input text: [input_text], identify the most ambiguous or subjective aspects that make the [task_name] prediction uncertain.'\n3. Human expert prompt: 'Based on the input text: [input_text] and the identified ambiguous aspects: [ambiguous_aspects], provide your interpretation or judgment for the [task_name] task.'\n4. Confidence score update prompt: 'Given the initial confidence score: [initial_score], the human expert's judgment: [expert_judgment], and the model's initial uncertainty estimate: [uncertainty_estimate], update the confidence score for the [task_name] prediction.'\n5. Deferral prompt (if updated confidence score < threshold): 'Due to high uncertainty in the [task_name] prediction for the input text: [input_text], defer the final decision to the human expert.'",
            "Step 3: Set Hyperparameters": "1. Uncertainty threshold: Set a threshold (e.g., 0.7) below which the model is considered uncertain and requires human collaboration.\n2. Confidence score update function: Define a function that takes the initial confidence score, human expert's judgment, and the model's uncertainty estimate to update the confidence score (e.g., a weighted average).",
            "Step 4: Implement CUP": "1. For each input example, prompt the model for initial prediction and confidence score.\n2. If the confidence score is below the threshold, prompt the model to identify ambiguous aspects.\n3. Present the ambiguous aspects to a human expert and prompt for their judgment.\n4. Update the confidence score based on the expert's judgment and the model's uncertainty estimate.\n5. If the updated confidence score is still below the threshold, prompt the model to defer the final decision to the human expert.",
            "Step 5: Evaluate Performance": "1. Compare the calibration performance (e.g., ECE, MCE) of CUP with baseline methods such as direct prediction and human-in-the-loop calibration.\n2. Measure the impact of human feedback on the model's performance and calibration.\n3. Analyze the trade-off between the model's autonomy and the reliance on human expertise in different uncertainty thresholds and task settings."
        },
        "Test Case Examples": {
            "Test Case 1": {
                "Input": "I'm not sure if I like this movie. The acting was good, but the plot was confusing.",
                "Baseline Output": "Sentiment: Negative, Confidence: 0.85",
                "CUP Output": "Initial prediction: Sentiment: Negative, Confidence: 0.6\nAmbiguous aspects: The reviewer expresses mixed feelings, praising the acting but criticizing the plot.\nHuman expert judgment: The sentiment seems to be mixed, leaning slightly negative due to the confusion caused by the plot.\nUpdated prediction: Sentiment: Negative, Confidence: 0.7\nFinal decision: Defer to human expert for final judgment due to the mixed sentiment and subjective nature of the review.",
                "Explanation": "CUP identifies the ambiguity in the review and seeks human expert judgment to refine the sentiment prediction and confidence score. The model defers the final decision to the human expert due to the inherent subjectivity of the task."
            },
            "Test Case 2": {
                "Input": "This movie was an absolute masterpiece! The cinematography, acting, and storyline were all top-notch.",
                "Baseline Output": "Sentiment: Positive, Confidence: 0.95",
                "CUP Output": "Initial prediction: Sentiment: Positive, Confidence: 0.95\nFinal decision: Sentiment: Positive, Confidence: 0.95",
                "Explanation": "CUP does not seek human expert judgment for this example because the model's confidence score is above the uncertainty threshold, indicating a high degree of certainty in the sentiment prediction."
            }
        },
        "Fallback Plan": "If the proposed CUP method does not significantly improve the calibration performance or human agreement scores compared to the baseline methods, consider the following alternative approaches:\n1. Analyze the quality and relevance of the generated ambiguous aspects to ensure they accurately capture the sources of uncertainty in the input examples. Refine the ambiguity identification prompt if needed.\n2. Experiment with different confidence score update functions and uncertainty thresholds to optimize the trade-off between model autonomy and human intervention.\n3. Investigate the impact of human expert background and expertise on the effectiveness of CUP. Consider incorporating expert-specific weights or adapting the prompts to elicit more informative judgments.\n4. Explore alternative methods for integrating human feedback, such as fine-tuning the model on a subset of expert-annotated examples or using active learning techniques to selectively seek human judgment for the most informative examples.\n5. If CUP does not yield significant improvements, focus on analyzing the limitations of the proposed method and the challenges of calibrating confidence scores for subjective language understanding tasks. Use the insights gained from the experiments to inform future research directions and propose alternative approaches for handling uncertainty and subjectivity in these tasks."
    },
    "novelty_queries": [
        "KeywordQuery(\"language models uncertainty prompting\")",
        "KeywordQuery(\"language models confidence calibration subjective tasks\")",
        "KeywordQuery(\"human-in-the-loop language models uncertainty\")",
        "KeywordQuery(\"collaborative language models subjective tasks\")",
        "KeywordQuery(\"Collaborative Uncertainty Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "8f7297454d7f44365b9bcda5ebb9439a43daf5e6",
            "paperId": "8f7297454d7f44365b9bcda5ebb9439a43daf5e6",
            "title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
            "abstract": "Empowering large language models to accurately express confidence in their answers is essential for trustworthy decision-making. Previous confidence elicitation methods, which primarily rely on white-box access to internal model information or model fine-tuning, have become less suitable for LLMs, especially closed-source commercial APIs. This leads to a growing need to explore the untapped area of black-box approaches for LLM uncertainty estimation. To better break down the problem, we define a systematic framework with three components: prompting strategies for eliciting verbalized confidence, sampling methods for generating multiple responses, and aggregation techniques for computing consistency. We then benchmark these methods on two key tasks-confidence calibration and failure prediction-across five types of datasets (e.g., commonsense and arithmetic reasoning) and five widely-used LLMs including GPT-4 and LLaMA 2 Chat. Our analysis uncovers several key insights: 1) LLMs, when verbalizing their confidence, tend to be overconfident, potentially imitating human patterns of expressing confidence. 2) As model capability scales up, both calibration and failure prediction performance improve. 3) Employing our proposed strategies, such as human-inspired prompts, consistency among multiple responses, and better aggregation strategies can help mitigate this overconfidence from various perspectives. 4) Comparisons with white-box methods indicate that while white-box methods perform better, the gap is narrow, e.g., 0.522 to 0.605 in AUROC. Despite these advancements, none of these techniques consistently outperform others, and all investigated methods struggle in challenging tasks, such as those requiring professional knowledge, indicating significant scope for improvement. We believe this study can serve as a strong baseline and provide insights for eliciting confidence in black-box LLMs.",
            "year": 2023,
            "citationCount": 97,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study defines a systematic framework with three components: prompting strategies for eliciting verbalized confidence, sampling methods for generating multiple responses, and aggregation techniques for computing consistency and believes it can serve as a strong baseline and provide insights for eliciting confidence in black-box LLMs."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is improving confidence calibration of language models on subjective language understanding tasks by leveraging human expertise through collaborative prompting. The approach is to use prompts that encourage the model to identify ambiguous aspects of the input and seek human judgment when uncertain.\n\nThe research problem in the paper is evaluating the effectiveness of black-box methods for eliciting confidence estimates from language models. The approach is to benchmark various prompting strategies, sampling methods, and aggregation techniques for confidence elicitation on tasks like calibration and failure prediction.\n\nWhile both works aim to improve the confidence estimates of language models, the proposal focuses specifically on subjective language understanding tasks and proposes a human-in-the-loop approach. In contrast, the paper evaluates black-box methods for confidence elicitation on a broader range of tasks without explicitly involving human collaboration.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "ab4ce5dda7ad4d9032995c9c049a89d65723c6aa",
            "paperId": "ab4ce5dda7ad4d9032995c9c049a89d65723c6aa",
            "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
            "abstract": "A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated. However, the most widely-used LMs are fine-tuned with reinforcement learning from human feedback (RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated. In light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50%.",
            "year": 2023,
            "citationCount": 96,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "For RLHF-LMs such as ChatGPT, GPT-4, and Claude, it is found that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50%."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving confidence calibration of language models on subjective language understanding tasks by leveraging human expertise through collaborative prompting. The approach is to prompt the model to identify ambiguous aspects of the input when its confidence is low, then incorporate human judgments to update the confidence scores.\n\nThe research problem in the paper is improving confidence calibration of language models fine-tuned with human feedback. The approach is to use the model's verbalized confidences emitted as output tokens instead of its conditional probabilities.\n\nWhile both works aim to improve confidence calibration of language models, the proposal focuses on subjective language understanding tasks and leverages human expertise through collaborative prompting, whereas the paper focuses on models fine-tuned with human feedback and uses the model's verbalized confidences as an alternative to conditional probabilities. The tasks and approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "6920de816acd201aadc0de51cf0fa62fa92bb0cc",
            "paperId": "6920de816acd201aadc0de51cf0fa62fa92bb0cc",
            "title": "On the Calibration of Large Language Models and Alignment",
            "abstract": "As large language models attract increasing attention and find widespread application, concurrent challenges of reliability also arise at the same time. Confidence calibration, an effective analysis method for gauging the reliability of deep models, serves as a crucial tool for assessing and improving their reliability. However, such investigation has been comparatively underexplored. In this work, we conduct a systematic examination of the calibration of aligned language models throughout the entire construction process, including pretraining and alignment training. At each stage, we investigate how different training settings, such as parameter scales and training data, affect model calibration. To thoroughly assess model calibration, we evaluate models on three most concerned aspects: generation, factuality and understanding. Our work sheds light on whether popular LLMs are well-calibrated and how the training process influences model calibration.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work sheds light on whether popular LLMs are well-calibrated and how the training process influences model calibration, as well as how different training settings affect model calibration."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving confidence calibration of language models on subjective language understanding tasks by leveraging human expertise through collaborative prompting. The approach is to use prompts that encourage the model to identify ambiguous aspects of the input and seek human judgment when the model's confidence is low.\n\nThe research problem in the paper is investigating the calibration of aligned language models throughout the pretraining and alignment process. The approach is to evaluate how different training settings affect model calibration on generation, factuality, and understanding tasks.\n\nThe proposal focuses on a specific problem of calibration in subjective language tasks and proposes a human-in-the-loop prompting method, while the paper broadly investigates calibration of aligned language models during training and does not involve human collaboration.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "74c7343d91d5464c27ca407fd504b07e690363be",
            "paperId": "74c7343d91d5464c27ca407fd504b07e690363be",
            "title": "Combining Confidence Elicitation and Sample-based Methods for Uncertainty Quantification in Misinformation Mitigation",
            "abstract": "Large Language Models have emerged as prime candidates to tackle misinformation mitigation. However, existing approaches struggle with hallucinations and overconfident predictions. We propose an uncertainty quantification framework that leverages both direct confidence elicitation and sampled-based consistency methods to provide better calibration for NLP misinformation mitigation solutions. We first investigate the calibration of sample-based consistency methods that exploit distinct features of consistency across sample sizes and stochastic levels. Next, we evaluate the performance and distributional shift of a robust numeric verbalization prompt across single vs. two-step confidence elicitation procedure. We also compare the performance of the same prompt with different versions of GPT and different numerical scales. Finally, we combine the sample-based consistency and verbalized methods to propose a hybrid framework that yields a better uncertainty estimation for GPT models. Overall, our work proposes novel uncertainty quantification methods that will improve the reliability of Large Language Models in misinformation mitigation applications.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes an uncertainty quantification framework that leverages both direct confidence elicitation and sampled-based consistency methods to provide better calibration for NLP misinformation mitigation solutions to improve the reliability of Large Language Models in misinformation mitigation applications."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving confidence calibration of language models on subjective language understanding tasks by leveraging human expertise through collaborative prompting. The approach is to use prompts that encourage the model to identify ambiguous aspects of the input and seek human judgment when the model's confidence is low.\n\nThe research problem in the paper is improving uncertainty quantification for misinformation mitigation using language models. The approach is to combine direct confidence elicitation through prompting and sample-based consistency methods.\n\nWhile both works aim to improve the reliability of language models, the proposal focuses on subjective language understanding tasks and leverages human expertise, while the paper focuses on misinformation mitigation and combines confidence elicitation and sample-based methods. The research problems and approaches are not directly relevant.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "ad402080a4aa66ef3c57a46ce4685a47a3cc0a61",
            "paperId": "ad402080a4aa66ef3c57a46ce4685a47a3cc0a61",
            "title": "Quantifying Uncertainty in Natural Language Explanations of Large Language Models",
            "abstract": "Large Language Models (LLMs) are increasingly used as powerful tools for several high-stakes natural language processing (NLP) applications. Recent prompting works claim to elicit intermediate reasoning steps and key tokens that serve as proxy explanations for LLM predictions. However, there is no certainty whether these explanations are reliable and reflect the LLMs behavior. In this work, we make one of the first attempts at quantifying the uncertainty in explanations of LLMs. To this end, we propose two novel metrics -- $\\textit{Verbalized Uncertainty}$ and $\\textit{Probing Uncertainty}$ -- to quantify the uncertainty of generated explanations. While verbalized uncertainty involves prompting the LLM to express its confidence in its explanations, probing uncertainty leverages sample and model perturbations as a means to quantify the uncertainty. Our empirical analysis of benchmark datasets reveals that verbalized uncertainty is not a reliable estimate of explanation confidence. Further, we show that the probing uncertainty estimates are correlated with the faithfulness of an explanation, with lower uncertainty corresponding to explanations with higher faithfulness. Our study provides insights into the challenges and opportunities of quantifying uncertainty in LLM explanations, contributing to the broader discussion of the trustworthiness of foundation models.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes two novel metrics -- verbalized uncertainty and probing uncertainty -- to quantify the uncertainty of generated explanations of large Language Models, and shows that the probing uncertainty estimates are correlated with the faithfulness of an explanation, with lower uncertainty corresponding to explanations with higher faithfulness."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving confidence calibration of language models in subjective language understanding tasks by leveraging human expertise through collaborative prompting. The approach is to prompt the model to identify ambiguous aspects of the input, seek human judgment, and update the confidence scores accordingly.\n\nThe research problem in the paper is quantifying the uncertainty in explanations generated by language models. The approach is to propose two metrics, verbalized uncertainty and probing uncertainty, to measure the confidence and faithfulness of the explanations.\n\nWhile both works deal with uncertainty estimation in language models, the proposal focuses on improving confidence calibration in subjective tasks using human collaboration, while the paper aims to quantify the uncertainty in generated explanations without human involvement. The tasks and approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "ba63e1ab5b6e9d849982ae293ac0483053badaff",
            "paperId": "ba63e1ab5b6e9d849982ae293ac0483053badaff",
            "title": "Uncertainty in Language Models: Assessment through Rank-Calibration",
            "abstract": "Language Models (LMs) have shown promising performance in natural language generation. However, as LMs often generate incorrect or hallucinated responses, it is crucial to correctly quantify their uncertainty in responding to given inputs. In addition to verbalized confidence elicited via prompting, many uncertainty measures ($e.g.$, semantic entropy and affinity-graph-based measures) have been proposed. However, these measures can differ greatly, and it is unclear how to compare them, partly because they take values over different ranges ($e.g.$, $[0,\\infty)$ or $[0,1]$). In this work, we address this issue by developing a novel and practical framework, termed $Rank$-$Calibration$, to assess uncertainty and confidence measures for LMs. Our key tenet is that higher uncertainty (or lower confidence) should imply lower generation quality, on average. Rank-calibration quantifies deviations from this ideal relationship in a principled manner, without requiring ad hoc binary thresholding of the correctness score ($e.g.$, ROUGE or METEOR). The broad applicability and the granular interpretability of our methods are demonstrated empirically.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel and practical framework, termed $Rank$-$Calibration$ is developed, to assess uncertainty and confidence measures for LMs, with the key tenet that higher uncertainty should imply lower generation quality, on average."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving confidence calibration of language models on subjective language understanding tasks by leveraging human expertise through collaborative prompting. The approach is to prompt the model to identify ambiguous aspects of the input when its confidence is low, ask human experts for their judgment, and update the model's confidence score based on the expert feedback.\n\nThe research problem in the paper is assessing uncertainty measures for language models by developing a rank-calibration framework. The approach is based on the idea that higher uncertainty should imply lower generation quality on average, and the framework quantifies deviations from this ideal relationship without requiring binary thresholding of correctness scores.\n\nThe proposal focuses on improving confidence calibration in subjective tasks using human feedback, while the paper proposes a framework for assessing uncertainty measures in language model generations. The research problems and approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "5043a786a1db5d830dc6c51d62f58e172a96e479",
            "paperId": "5043a786a1db5d830dc6c51d62f58e172a96e479",
            "title": "Methods to Estimate Large Language Model Confidence",
            "abstract": "Large Language Models have difficulty communicating uncertainty, which is a significant obstacle to applying LLMs to complex medical tasks. This study evaluates methods to measure LLM confidence when suggesting a diagnosis for challenging clinical vignettes. GPT4 was asked a series of challenging case questions using Chain of Thought and Self Consistency prompting. Multiple methods were investigated to assess model confidence and evaluated on their ability to predict the models observed accuracy. The methods evaluated were Intrinsic Confidence, SC Agreement Frequency and CoT Response Length. SC Agreement Frequency correlated with observed accuracy, yielding a higher Area under the Receiver Operating Characteristic Curve compared to Intrinsic Confidence and CoT Length analysis. SC agreement is the most useful proxy for model confidence, especially for medical diagnosis. Model Intrinsic Confidence and CoT Response Length exhibit a weaker ability to differentiate between correct and incorrect answers, preventing them from being reliable and interpretable markers for model confidence. We conclude GPT4 has a limited ability to assess its own diagnostic accuracy. SC Agreement Frequency is the most useful method to measure GPT4 confidence.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study evaluates methods to measure LLM confidence when suggesting a diagnosis for challenging clinical vignettes and concludes SC Agreement Frequency is the most useful method to measure GPT4 confidence."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving confidence calibration of language models on subjective language understanding tasks by leveraging human expertise through collaborative prompting. The approach is to prompt the model to identify ambiguous aspects of the input, seek human judgment on those aspects, and update the model's confidence score based on the human feedback.\n\nThe research problem in the paper is estimating the confidence of large language models when suggesting diagnoses for challenging clinical cases. The approach is to evaluate different methods, such as intrinsic confidence, self-consistency agreement frequency, and chain-of-thought response length, to assess the model's confidence and compare their ability to predict the model's observed accuracy.\n\nThe proposal focuses on improving confidence calibration in subjective language tasks, while the paper focuses on estimating confidence in medical diagnosis. The proposal uses collaborative prompting with human feedback, while the paper evaluates different confidence estimation methods without human involvement.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "27dd800cb087f1575a65fba06c95ec8fd83a0fb4",
            "paperId": "27dd800cb087f1575a65fba06c95ec8fd83a0fb4",
            "title": "Fact-and-Reflection (FaR) Improves Confidence Calibration of Large Language Models",
            "abstract": "For a LLM to be trustworthy, its confidence level should be well-calibrated with its actual performance. While it is now common sense that LLM performances are greatly impacted by prompts, the confidence calibration in prompting LLMs has yet to be thoroughly explored. In this paper, we explore how different prompting strategies influence LLM confidence calibration and how it could be improved. We conduct extensive experiments on six prompting methods in the question-answering context and we observe that, while these methods help improve the expected LLM calibration, they also trigger LLMs to be over-confident when responding to some instances. Inspired by human cognition, we propose Fact-and-Reflection (FaR) prompting, which improves the LLM calibration in two steps. First, FaR elicits the known\"facts\"that are relevant to the input prompt from the LLM. And then it asks the model to\"reflect\"over them to generate the final answer. Experiments show that FaR prompting achieves significantly better calibration; it lowers the Expected Calibration Error by 23.5% on our multi-purpose QA tasks. Notably, FaR prompting even elicits the capability of verbally expressing concerns in less confident scenarios, which helps trigger retrieval augmentation for solving these harder instances.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Fact-and-Reflection prompting is proposed, which improves the LLM calibration in two steps, and even elicits the capability of verbally expressing concerns in less confident scenarios, which helps trigger retrieval augmentation for solving these harder instances."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving confidence calibration of language models in subjective language understanding tasks, and the proposed approach is collaborative uncertainty prompting that involves human experts.\n\nThe research problem in the paper is also improving confidence calibration of language models, but the proposed approach is fact-and-reflection prompting that elicits relevant facts and reflects over them to generate the final answer.\n\nWhile both works aim to improve confidence calibration, the proposal focuses on subjective language understanding tasks and involves human experts, whereas the paper focuses on question-answering tasks and does not involve human experts.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "4feb412574eb5d0b187276069fe6024c22629c0e",
            "paperId": "4feb412574eb5d0b187276069fe6024c22629c0e",
            "title": "The Calibration Gap between Model and Human Confidence in Large Language Models",
            "abstract": "For large language models (LLMs) to be trusted by humans they need to be well-calibrated in the sense that they can accurately assess and communicate how likely it is that their predictions are correct. Recent work has focused on the quality of internal LLM confidence assessments, but the question remains of how well LLMs can communicate this internal model confidence to human users. This paper explores the disparity between external human confidence in an LLM's responses and the internal confidence of the model. Through experiments involving multiple-choice questions, we systematically examine human users' ability to discern the reliability of LLM outputs. Our study focuses on two key areas: (1) assessing users' perception of true LLM confidence and (2) investigating the impact of tailored explanations on this perception. The research highlights that default explanations from LLMs often lead to user overestimation of both the model's confidence and its' accuracy. By modifying the explanations to more accurately reflect the LLM's internal confidence, we observe a significant shift in user perception, aligning it more closely with the model's actual confidence levels. This adjustment in explanatory approach demonstrates potential for enhancing user trust and accuracy in assessing LLM outputs. The findings underscore the importance of transparent communication of confidence levels in LLMs, particularly in high-stakes applications where understanding the reliability of AI-generated information is essential.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "By modifying the explanations of large language models to more accurately reflect the LLM's internal confidence, a significant shift in user perception is observed, aligning it more closely with the model's actual confidence levels."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving confidence calibration of language models on subjective language understanding tasks by leveraging human expertise through collaborative prompting. The approach is to prompt the model to identify ambiguous aspects of the input when its confidence is low, ask human experts for their judgment, and update the model's confidence score based on the expert feedback.\n\nThe research problem in the paper is studying the gap between the model's internal confidence and the human's perceived confidence in the model's outputs. The approach is to modify the model's explanations to better reflect its internal confidence and measure how this affects the user's perception of the model's accuracy and confidence.\n\nWhile both works deal with confidence calibration in language models, the proposal focuses on improving the model's confidence on subjective tasks by incorporating human feedback, whereas the paper studies how to better communicate the model's confidence to users through explanations. The research problems and approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "92746dfa09dcad92ecf1e6272ebb300c1112b7eb",
            "paperId": "92746dfa09dcad92ecf1e6272ebb300c1112b7eb",
            "title": "Automatic Calibration and Error Correction for Large Language Models via Pareto Optimal Self-Supervision",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities out of box for a wide range of applications, yet accuracy still remains a major growth area, especially in mission-critical domains such as biomedicine. An effective method to calibrate the con\ufb01dence level on LLM responses is essential to automatically detect errors and facilitate human-in-the-loop veri\ufb01cation. An important source of calibration signals stems from expert-stipulated programmatic super-vision, which is often available at low cost but has its own limitations such as noise and coverage. In this paper, we introduce a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every response, without any additional manual efforts. This is accomplished by learning a harmonizer model to align LLM output with other available supervision sources, which would assign higher risk scores to more uncertain LLM responses and facilitate error correction. Experiments on standard relation extraction tasks in biomedical and general domains demonstrate the promise of this approach, with our proposed risk scores highly correlated with the real error rate of LLMs. For the most uncertain test instances, dynamic prompting based on our proposed risk scores results in signi\ufb01cant accuracy improvement for off-the-shelf LLMs, boosting GPT-3 results past state-of-the-art (SOTA) weak supervision and GPT-4 results past SOTA supervised results on challenging evaluation datasets.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every response, without any additional manual efforts."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve confidence calibration in subjective language understanding tasks by leveraging human expertise through collaborative uncertainty prompting. The paper focuses on automatic calibration and error correction for large language models using Pareto optimal self-supervision, which aligns LLM output with available supervision sources to assign risk scores and facilitate error correction.\n\nThe project proposal and the paper address different research problems and propose different approaches. The project proposal tackles subjective language understanding tasks and involves human collaboration, while the paper focuses on automatic calibration and error correction for LLMs using programmatic supervision.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "c01c7c1f903dfaa78812fb20a6cb2db25e4712e3",
            "paperId": "c01c7c1f903dfaa78812fb20a6cb2db25e4712e3",
            "title": "Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness",
            "abstract": "We introduce BSDetector, a method for detecting bad and speculative answers from a pretrained Large Language Model by estimating a numeric confidence score for any output it generated. Our uncertainty quantification technique works for any LLM accessible only via a black-box API, whose training data remains unknown. By expending a bit of extra computation, users of any LLM API can now get the same response as they would ordinarily, as well as a confidence estimate that cautions when not to trust this response. Experiments on both closed and open-form Question-Answer benchmarks reveal that BSDetector more accurately identifies incorrect LLM responses than alternative uncertainty estimation procedures (for both GPT-3 and ChatGPT). By sampling multiple responses from the LLM and considering the one with the highest confidence score, we can additionally obtain more accurate responses from the same LLM, without any extra training steps. In applications involving automated evaluation with LLMs, accounting for our confidence scores leads to more reliable evaluation in both human-in-the-loop and fully-automated settings (across both GPT 3.5 and 4).",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "BSDetector is introduced, a method for detecting bad and speculative answers from a pretrained Large Language Model by estimating a numeric confidence score for any output it generated by sampling multiple responses from the LLM and considering the one with the highest confidence score."
            },
            "score": 6
        },
        {
            "id": "99b6406518edd5368f71e08a904d302bde29570f",
            "paperId": "99b6406518edd5368f71e08a904d302bde29570f",
            "title": "Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation",
            "abstract": "Humans are widely involved in the evaluation of open-ended natural language generation tasks (NLG) that demand creativity, as automatic metrics often exhibit weak correlations with human judgments. Large language models (LLMs) recently have emerged as a scalable and cost-effective alternative to human evaluations. However, both humans and LLMs have limitations, i.e., inherent subjectivity and unreliable judgments, particularly for open-ended tasks that require adaptable metrics tailored to diverse task requirements. To explore the synergy between humans and LLM-based evaluators and address the challenges of existing inconsistent evaluation criteria in open-ended NLG tasks, we propose a Collaborative Evaluation pipeline CoEval, involving the design of a checklist of task-specific criteria and the detailed evaluation of texts, in which LLM generates initial ideation, and then humans engage in scrutiny. We conducted a series of experiments to investigate the mutual effects between LLMs and humans in CoEval. Results show that, by utilizing LLMs, CoEval effectively evaluates lengthy texts, saving significant time and reducing human evaluation outliers. Human scrutiny still plays a role, revising around 20% of LLM evaluation scores for ultimate reliability.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Collaborative Evaluation pipeline CoEval is proposed, involving the design of a checklist of task-specific criteria and the detailed evaluation of texts, in which LLM generates initial ideation, and then humans engage in scrutiny."
            },
            "score": 6
        },
        {
            "id": "b1ec3002f4c80d721fc7d975cf469dce0833fed0",
            "paperId": "b1ec3002f4c80d721fc7d975cf469dce0833fed0",
            "title": "DeLLMa: A Framework for Decision Making Under Uncertainty with Large Language Models",
            "abstract": "Large language models (LLMs) are increasingly used across society, including in domains like business, engineering, and medicine. These fields often grapple with decision-making under uncertainty, a critical yet challenging task. In this paper, we show that directly prompting LLMs on these types of decision-making problems yields poor results, especially as the problem complexity increases. To overcome this limitation, we propose DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments. DeLLMa involves a multi-step scaffolding procedure, drawing upon principles from decision theory and utility theory, to provide an optimal and human-auditable decision-making process. We validate our framework on decision-making environments involving real agriculture and finance data. Our results show that DeLLMa can significantly improve LLM decision-making performance, achieving up to a 40% increase in accuracy over competing methods.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments, is proposed and validated on decision-making environments involving real agriculture and finance data."
            },
            "score": 5
        },
        {
            "id": "9ff92d31babb7bdaecf7220b0a81c701230d8b95",
            "paperId": "9ff92d31babb7bdaecf7220b0a81c701230d8b95",
            "title": "A Study on the Calibration of In-context Learning",
            "abstract": "Accurate uncertainty quantification is crucial for the safe deployment of machine learning models, and prior research has demonstrated improvements in the calibration of modern language models (LMs). We study in-context learning (ICL), a prevalent method for adapting static LMs through tailored prompts, and examine the balance between performance and calibration across a broad spectrum of natural language understanding and reasoning tasks. Through comprehensive experiments, we observe that, with an increasing number of ICL examples, models initially exhibit increased miscalibration before achieving better calibration and miscalibration tends to arise in low-shot settings. Moreover, we find that methods aimed at improving usability, such as fine-tuning and chain-of-thought (CoT) prompting, can lead to miscalibration and unreliable natural language explanations. Furthermore, we explore recalibration techniques and find that a scaling-binning calibrator can reduce calibration errors consistently.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that methods aimed at improving usability, such as fine-tuning and chain-of-thought (CoT) prompting, can lead to miscalibration and unreliable natural language explanations."
            },
            "score": 5
        },
        {
            "id": "288338c1ec81b259af5587677fdd10d5191baf71",
            "paperId": "288338c1ec81b259af5587677fdd10d5191baf71",
            "title": "Reasoning in Conversation: Solving Subjective Tasks through Dialogue Simulation for Large Language Models",
            "abstract": "Large Language Models (LLMs) have achieved remarkable performance in objective tasks such as open-domain question answering and mathematical reasoning, which can often be solved through recalling learned factual knowledge or chain-of-thought style reasoning. However, we find that the performance of LLMs in subjective tasks is still unsatisfactory, such as metaphor recognition, dark humor detection, etc. Compared to objective tasks, subjective tasks focus more on interpretation or emotional response rather than a universally accepted reasoning pathway. Based on the characteristics of the tasks and the strong dialogue-generation capabilities of LLMs, we propose RiC (Reasoning in Conversation), a method that focuses on solving subjective tasks through dialogue simulation. The motivation of RiC is to mine useful contextual information by simulating dialogues instead of supplying chain-of-thought style rationales, thereby offering potential useful knowledge behind dialogues for giving the final answers. We evaluate both API-based and open-source LLMs including GPT-4, ChatGPT, and OpenChat across twelve tasks. Experimental results show that RiC can yield significant improvement compared with various baselines.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes RiC (Reasoning in Conversation), a method that focuses on solving subjective tasks through dialogue simulation to mine useful contextual information by simulating dialogues instead of supplying chain-of-thought style rationales."
            },
            "score": 5
        },
        {
            "id": "385c74957858e7d6856d48e72b5a902b4c1aa28c",
            "paperId": "385c74957858e7d6856d48e72b5a902b4c1aa28c",
            "title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
            "abstract": "Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of\"tit for tat\"and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counter-intuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of\"tit for tat\"state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents. Codes: https://github.com/Skytliang/Multi-Agents-Debate",
            "year": 2023,
            "citationCount": 125,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Multi-Agent Debate (MAD) framework is proposed, in which multiple agents express their arguments in the state of\"tit for tat\"and a judge manages the debate process to obtain a final solution."
            },
            "score": 5
        },
        {
            "id": "bd0d6a6bd10f80726b870450f6275f0530c24afb",
            "paperId": "bd0d6a6bd10f80726b870450f6275f0530c24afb",
            "title": "Preserving Pre-trained Features Helps Calibrate Fine-tuned Language Models",
            "abstract": "Large pre-trained language models (PLMs) have demonstrated strong performance on natural language understanding (NLU) tasks through fine-tuning. However, fine-tuned models still suffer from overconfident predictions, especially in out-of-domain settings. In this paper, we tackle the problem of calibrating fine-tuned language models. We demonstrate that the PLMs are well-calibrated on the masked language modeling task with robust predictive confidence under domain shift, yet the fine-tuned models fail to retain such property due to catastrophic forgetting, which impacts the calibration on the downstream classification task. In light of these observations, we evaluate the calibration of several methods that preserve pre-trained features and show that preserving pre-trained features can improve the calibration of fine-tuned language models. Among these methods, our proposed method that encourages the fine-tuned model to learn generative representations with auxiliary language modeling objective achieves competitive accuracy and the lowest expected calibration error compared to several strong baselines under both in-domain and out-of-domain settings on three downstream NLU tasks.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A proposed method that encourages the fine-tuned model to learn generative representations with auxiliary language modeling objective achieves competitive accuracy and the lowest expected calibration error compared to several strong baselines under both in-domain and out-of-domain settings on three downstream NLU tasks."
            },
            "score": 5
        },
        {
            "id": "b195b707dc54fab51ae22a0cd7511e151b6533f4",
            "paperId": "b195b707dc54fab51ae22a0cd7511e151b6533f4",
            "title": "Selective-LAMA: Selective Prediction for Confidence-Aware Evaluation of Language Models",
            "abstract": "Recent studies have suggested that neural language models learn and store a large amount of facts and commonsense knowledge from training data. The ability of language models to restore such knowledge is often evaluated via zero-shot cloze-style QA tasks. However, such evaluations rely only on prediction accuracy without punishing the systems for their mistakes, e.g., simply guessing or hallucinating likely answers. Selective prediction is a more informative evaluation framework that takes the confidence of predictions into account. Under the selective prediction setting, a model is evaluated not only by the number of correct predictions, but also by the ability to filter out dubious predictions by estimating the confidence of individual predictions. Such confidence-aware evaluation is crucial for determining whether to trust zero-shot predictions of language models. In this paper, we apply the selective prediction setting to an existing benchmark, LAMA probe, and conduct extensive experiments with recent neural language models and different confidence functions. We empirically show that our Selective-LAMA evaluation is more robust to the effect of simple guesses than the conventional accuracy-based evaluation.Our evaluation reveals the importance of the choice of confidence functions by showing that simply relying on token probabilities is not always the best choice.Further analysis shows that various confidence functions exhibit different preferences over predicted tokens for a given context.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper empirically shows that the Selective-LAMA evaluation is more robust to the effect of simple guesses than the conventional accuracy-based evaluation, and reveals the importance of the choice of confidence functions by showing that simply relying on token probabilities is not always the best choice."
            },
            "score": 5
        },
        {
            "id": "99da2f941c0a606278463f1e5e60a2bdbaf8eb99",
            "paperId": "99da2f941c0a606278463f1e5e60a2bdbaf8eb99",
            "title": "Enabling Synergistic Knowledge Sharing and Reasoning in Large Language Models with Collaborative Multi-Agents",
            "abstract": "Despite the significant advancements in the field of Natural Language Processing (NLP), Large Language Models (LLMs) have shown limitations in performing complex tasks that require arithmetic, commonsense, and symbolic reasoning. Reasoning frameworks like ReAct, Chain-of-thought (CoT), Tree-of-thoughts (ToT), etc. have shown success but with limitations in solving long-form complex tasks. To address this, we pro-pose a knowledge-sharing and collaborative multi-agent assisted framework on LLMs that leverages the capabilities of existing reasoning frameworks and the collaborative skills of multi-agent systems (MASs). The objectives of the proposed framework are to overcome the limitations of LLMs, enhance their reasoning capabilities, and improve their performance in complex tasks. It involves generating natural language rationales and in-context few-shot learning via prompting, and integrates the reasoning techniques with efficient knowledge-sharing and communication-driven agent networks. The potential benefits of the proposed framework include saving time and money, improved efficiency for computationally intensive reasoning, and the ability to incor-porate multiple collaboration strategies for dynamically changing environments.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A knowledge-sharing and collaborative multi-agent assisted framework that leverages the capabilities of existing reasoning frameworks and the collaborative skills of multi-agent systems (MASs) to overcome the limitations of LLMs, enhance their reasoning capabilities, and improve their performance in complex tasks."
            },
            "score": 5
        },
        {
            "id": "8089b431b2e09c27967428fb542c0935fb95ec30",
            "paperId": "8089b431b2e09c27967428fb542c0935fb95ec30",
            "title": "STAR: Constraint LoRA with Dynamic Active Learning for Data-Efficient Fine-Tuning of Large Language Models",
            "abstract": "Though Large Language Models (LLMs) have demonstrated the powerful capabilities of few-shot learning through prompting methods, supervised training is still necessary for complex reasoning tasks. Because of their extensive parameters and memory consumption, both Parameter-Efficient Fine-Tuning (PEFT) methods and Memory-Efficient Fine-Tuning methods have been proposed for LLMs. Nevertheless, the issue of large annotated data consumption, the aim of Data-Efficient Fine-Tuning, remains unexplored. One obvious way is to combine the PEFT method with active learning. However, the experimental results show that such a combination is not trivial and yields inferior results. Through probe experiments, such observation might be explained by two main reasons: uncertainty gap and poor model calibration. Therefore, in this paper, we propose a novel approach to effectively integrate uncertainty-based active learning and LoRA. Specifically, for the uncertainty gap, we introduce a dynamic uncertainty measurement that combines the uncertainty of the base model and the uncertainty of the full model during the iteration of active learning. For poor model calibration, we incorporate the regularization method during LoRA training to keep the model from being over-confident, and the Monte-Carlo dropout mechanism is employed to enhance the uncertainty estimation. Experimental results show that the proposed approach outperforms existing baseline models on three complex reasoning tasks.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel approach to effectively integrate uncertainty-based active learning and LoRA is proposed, which incorporates the regularization method during LoRA training to keep the model from being over-confident, and the Monte-Carlo dropout mechanism is employed to enhance the uncertainty estimation."
            },
            "score": 4
        },
        {
            "id": "e3446ef313663e30d8251dee339bca52962e7bfd",
            "paperId": "e3446ef313663e30d8251dee339bca52962e7bfd",
            "title": "Towards Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4",
            "abstract": "Misinformation poses a critical societal challenge, and current approaches have yet to produce an effective solution. We propose focusing on generalization, uncertainty, and how to leverage recent large language models, in order to create more practical tools to evaluate information veracity in contexts where perfect classification is impossible. We first demonstrate that GPT-4 can outperform prior methods in multiple settings and languages. Next, we explore generalization, revealing that GPT-4 and RoBERTa-large exhibit differences in failure modes. Third, we propose techniques to handle uncertainty that can detect impossible examples and strongly improve outcomes. We also discuss results on other language models, temperature, prompting, versioning, explainability, and web retrieval, each one providing practical insights and directions for future research. Finally, we publish the LIAR-New dataset with novel paired English and French misinformation data and Possibility labels that indicate if there is sufficient context for veracity evaluation. Overall, this research lays the groundwork for future tools that can drive real-world progress to combat misinformation.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This research demonstrates that GPT-4 can outperform prior methods in multiple settings and languages, and proposes techniques to handle uncertainty that can detect impossible examples and strongly improve outcomes."
            },
            "score": 4
        },
        {
            "id": "3fc3460c4554a28e489a0ea6ef067b79b7d301d9",
            "paperId": "3fc3460c4554a28e489a0ea6ef067b79b7d301d9",
            "title": "Active Prompting with Chain-of-Thought for Large Language Models",
            "abstract": "The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks requiring reasoning, such as arithmetic and commonsense reasoning. It is known that the effective design of task-specific prompts is critical for LLMs' ability to produce high-quality answers. In particular, an effective approach for complex question-and-answer tasks is example-based prompting with chain-of-thought (CoT) reasoning, which significantly improves the performance of LLMs. However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks. This paper proposes a new method, Active-Prompt, to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning). For this purpose, we propose a solution to the key problem of determining which questions are the most important and helpful ones to annotate from a pool of task-specific queries. By borrowing ideas from the related problem of uncertainty-based active learning, we introduce several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation. Experimental results demonstrate the superiority of our proposed method, achieving state-of-the-art on eight complex reasoning tasks. Further analyses of different uncertainty metrics, pool sizes, zero-shot learning, and accuracy-uncertainty relationship demonstrate the effectiveness of our method. Our code will be available at https://github.com/shizhediao/active-prompt.",
            "year": 2023,
            "citationCount": 58,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new method to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning), and introduces several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation."
            },
            "score": 4
        },
        {
            "id": "05f6628948f79d0cce8664cc8146fd459d53e9d5",
            "paperId": "05f6628948f79d0cce8664cc8146fd459d53e9d5",
            "title": "On the Calibration of Pre-trained Language Models using Mixup Guided by Area Under the Margin and Saliency",
            "abstract": "A well-calibrated neural model produces confidence (probability outputs) closely approximated by the expected accuracy. While prior studies have shown that mixup training as a data augmentation technique can improve model calibration on image classification tasks, little is known about using mixup for model calibration on natural language understanding (NLU) tasks. In this paper, we explore mixup for model calibration on several NLU tasks and propose a novel mixup strategy for pre-trained language models that improves model calibration further. Our proposed mixup is guided by both the Area Under the Margin (AUM) statistic (Pleiss et al., 2020) and the saliency map of each sample (Simonyan et al., 2013). Moreover, we combine our mixup strategy with model miscalibration correction techniques (i.e., label smoothing and temperature scaling) and provide detailed analyses of their impact on our proposed mixup. We focus on systematically designing experiments on three NLU tasks: natural language inference, paraphrase detection, and commonsense reasoning. Our method achieves the lowest expected calibration error compared to strong baselines on both in-domain and out-of-domain test samples while maintaining competitive accuracy.",
            "year": 2022,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper systematically designs experiments on three NLU tasks and proposes a novel mixup strategy for pre-trained language models that improves model calibration further and achieves the lowest expected calibration error compared to strong baselines on both in-domain and out-of-domain test samples while maintaining competitive accuracy."
            },
            "score": 4
        },
        {
            "id": "a2b89d2196b4cc88797d4907ce7458bb7584f6b6",
            "paperId": "a2b89d2196b4cc88797d4907ce7458bb7584f6b6",
            "title": "On the Calibration of Massively Multilingual Language Models",
            "abstract": "Massively Multilingual Language Models (MMLMs) have recently gained popularity due to their surprising effectiveness in cross-lingual transfer. While there has been much work in evaluating these models for their performance on a variety of tasks and languages, little attention has been paid on how well calibrated these models are with respect to the confidence in their predictions. We first investigate the calibration of MMLMs in the zero-shot setting and observe a clear case of miscalibration in low-resource languages or those which are typologically diverse from English. Next, we empirically show that calibration methods like temperature scaling and label smoothing do reasonably well in improving calibration in the zero-shot scenario. We also find that few-shot examples in the language can further help reduce calibration errors, often substantially. Overall, our work contributes towards building more reliable multilingual models by highlighting the issue of their miscalibration, understanding what language and model-specific factors influence it, and pointing out the strategies to improve the same.",
            "year": 2022,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates the calibration of MMLMs in the zero-shot setting and observes a clear case of miscalibration in low-resource languages or those which are typologically diverse from English, and empirically shows that calibration methods like temperature scaling and label smoothing do reasonably well in improving calibration in thezero-shot scenario."
            },
            "score": 4
        },
        {
            "id": "c589ddc6c6fb07189af7c1212f6eb15c5ff72cde",
            "paperId": "c589ddc6c6fb07189af7c1212f6eb15c5ff72cde",
            "title": "Sentiment Analysis in the Era of Large Language Models: A Reality Check",
            "abstract": "Sentiment analysis (SA) has been a long-standing research area in natural language processing. It can offer rich insights into human sentiments and opinions and has thus seen considerable interest from both academia and industry. With the advent of large language models (LLMs) such as ChatGPT, there is a great potential for their employment on SA problems. However, the extent to which existing LLMs can be leveraged for different sentiment analysis tasks remains unclear. This paper aims to provide a comprehensive investigation into the capabilities of LLMs in performing various sentiment analysis tasks, from conventional sentiment classification to aspect-based sentiment analysis and multifaceted analysis of subjective texts. We evaluate performance across 13 tasks on 26 datasets and compare the results against small language models (SLMs) trained on domain-specific datasets. Our study reveals that while LLMs demonstrate satisfactory performance in simpler tasks, they lag behind in more complex tasks requiring deeper understanding or structured sentiment information. However, LLMs significantly outperform SLMs in few-shot learning settings, suggesting their potential when annotation resources are limited. We also highlight the limitations of current evaluation practices in assessing LLMs' SA abilities and propose a novel benchmark, \\textsc{SentiEval}, for a more comprehensive and realistic evaluation. Data and code during our investigations are available at \\url{https://github.com/DAMO-NLP-SG/LLM-Sentiment}.",
            "year": 2023,
            "citationCount": 70,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study reveals that while LLMs demonstrate satisfactory performance in simpler tasks, they lag behind in more complex tasks requiring deeper understanding or structured sentiment information, however, LLMs significantly outperform SLMs in few-shot learning settings, suggesting their potential when annotation resources are limited."
            },
            "score": 4
        },
        {
            "id": "465dcb2dc66bc1095f5750b09bd2908ccb4b9e1e",
            "paperId": "465dcb2dc66bc1095f5750b09bd2908ccb4b9e1e",
            "title": "Knowledge Acquisition for Human-In-The-Loop Image Captioning",
            "abstract": "Image captioning offers a computational process to understand the semantics of images and convey them using descriptive language. However, automated captioning models may not always generate satisfactory captions due to the complex nature of the images and the quality/size of the training data. We propose an interactive captioning framework to improve machine-generated cap-tions by keeping humans in the loop and performing an online-offline knowledge acquisition (KA) process. In particular, online KA accepts a list of keywords specified by human users and fuses them with the image features to generate a read-able sentence that captures the semantics of the image. It leverages a multimodal conditioned caption completion mechanism to ensure the appearance of all user-input keywords in the generated caption. Offline KA further learns from the user inputs to update the model and benefits caption generation for unseen images in the future. It is built upon a Bayesian transformer architecture that dynamically allocates neural resources and supports uncertainty-aware model updates to mitigate overfitting. Our theoretical analysis also proves that Offline KA automatically selects the best model capacity to accommodate the newly acquired knowledge. Experiments on real-world data demonstrate the effectiveness of the proposed framework.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An interactive captioning framework to improve machine-generated cap-tions by keeping humans in the loop and performing an online-offline knowledge acquisition (KA) process that is built upon a Bayesian transformer architecture that dynamically allocates neural resources and supports uncertainty-aware model updates to mitigate overfitting."
            },
            "score": 4
        },
        {
            "id": "635ffc4efa56cced3148932eb52bc2275255960e",
            "paperId": "635ffc4efa56cced3148932eb52bc2275255960e",
            "title": "Optimal Query Selection Using Multi-Armed Bandits",
            "abstract": "Query selection for latent variable estimation is conventionally performed by opting for observations with low noise or optimizing information-theoretic objectives related to reducing the level of estimated uncertainty based on the current best estimate. In these approaches, typically, the system makes a decision by leveraging the current available information about the state. However, trusting the current best estimate results in poor query selection when truth is far from the current estimate, and this negatively impacts the speed and accuracy of the latent variable estimation procedure. We introduce a novel sequential adaptive action value function for query selection using the multi-armed bandit framework, which allows us to find a tractable solution. For this adaptive-sequential query selection method, we analytically show: 1) performance improvement in the query selection for a dynamical system; and 2) the conditions where the model outperforms competitors. We also present favorable empirical assessments of the performance for this method, compared to alternative methods, both using Monte Carlo simulations and human-in-the-loop experiments with a brain\u2013computer interface typing system, where the language model provides the prior information.",
            "year": 2018,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a novel sequential adaptive action value function for query selection using the multi-armed bandit framework, which allows for performance improvement in the query selection for a dynamical system and the conditions where the model outperforms competitors."
            },
            "score": 4
        },
        {
            "id": "56e7bda25b83228f91962d3465fd587cfe8908e1",
            "paperId": "56e7bda25b83228f91962d3465fd587cfe8908e1",
            "title": "How Far Can We Extract Diverse Perspectives from Large Language Models? Criteria-Based Diversity Prompting!",
            "abstract": "Collecting diverse human opinions is costly and challenging. This leads to a recent trend in collaborative efforts between humans and Large Language Models (LLMs) for generating diverse data, offering potential scalable and efficient solutions. However, the extent of LLMs' capability to generate diverse perspectives on subjective topics remains an unexplored question. In this study, we investigate LLMs' capacity for generating diverse perspectives and rationales on subjective topics, such as social norms and argumentative texts. We formulate a new problem of maximum diversity extraction from LLMs. Motivated by how humans develop their opinions through their values, we propose a criteria-based prompting technique to ground diverse opinions. To see how far we can extract diverse perspectives from LLMs, or called diversity coverage, we employ a step-by-step recall prompting for generating more outputs from the model in an iterative manner. As we apply our methods to various tasks, indeed we find that LLMs can generate diverse opinions according to the degree of task subjectivity",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study investigates LLMs' capacity for generating diverse perspectives and rationales on subjective topics, such as social norms and argumentative texts, and proposes a criteria-based prompting technique to ground diverse opinions."
            },
            "score": 4
        },
        {
            "id": "1366b07120580eaf1badde105b9361806e8f9629",
            "paperId": "1366b07120580eaf1badde105b9361806e8f9629",
            "title": "Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models",
            "abstract": "The emotions we experience involve complex processes; besides physiological aspects, research in psychology has studied cognitive appraisals where people assess their situations subjectively, according to their own values (Scherer, 2005). Thus, the same situation can often result in different emotional experiences. While the detection of emotion is a well-established task, there is very limited work so far on the automatic prediction of cognitive appraisals. This work fills the gap by presenting CovidET-Appraisals, the most comprehensive dataset to-date that assesses 24 appraisal dimensions, each with a natural language rationale, across 241 Reddit posts. CovidET-Appraisals presents an ideal testbed to evaluate the ability of large language models -- excelling at a wide range of NLP tasks -- to automatically assess and explain cognitive appraisals. We found that while the best models are performant, open-sourced LLMs fall short at this task, presenting a new challenge in the future development of emotionally intelligent models. We release our dataset at https://github.com/honglizhan/CovidET-Appraisals-Public.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "CovidET-Appraisals is presented, the most comprehensive dataset to-date that assesses 24 appraisal dimensions, each with a natural language rationale, across 241 Reddit posts, and found that while the best models are performant, open-sourced LLMs fall short at this task, presenting a new challenge in the future development of emotionally intelligent models."
            },
            "score": 4
        },
        {
            "id": "169c51dc01bd0fd74b2ebc18c0ed94c760bd63f5",
            "paperId": "169c51dc01bd0fd74b2ebc18c0ed94c760bd63f5",
            "title": "Learning Personal Human Biases and Representations for Subjective Tasks in Natural Language Processing",
            "abstract": "Many tasks in natural language processing like offensive, toxic, or emotional text classification are subjective by nature. Humans tend to perceive textual content in their own individual way. Existing methods commonly rely on the agreed output values, the same for all consumers. Here, we propose personalized solutions to subjective tasks. Our four new deep learning models take into account not only the content but also the specificity of a given human. The models represent different approaches to learning the representation and processing data about text readers. The experiments were carried out on four datasets: Wikipedia discussion texts labelled with attack, aggression, and toxicity, as well as opinions annotated with ten numerical emotional categories. Emotional data was considered as multivariate regression (multitask), whereas Wikipedia data as independent classifications. All our models based on human biases and their representations significantly improve the prediction quality in subjective tasks evaluated from the individual\u2019s perspective.",
            "year": 2021,
            "citationCount": 25,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Four new deep learning models take into account not only the content but also the specificity of a given human to significantly improve the prediction quality in subjective tasks evaluated from the individual\u2019s perspective."
            },
            "score": 4
        },
        {
            "id": "e1bc150d5d9e745a4920881c414ac9df0ea024a3",
            "paperId": "e1bc150d5d9e745a4920881c414ac9df0ea024a3",
            "title": "ChatGPT Prompting Cannot Estimate Predictive Uncertainty in High-Resource Languages",
            "abstract": "ChatGPT took the world by storm for its impressive abilities. Due to its release without documentation, scientists immediately attempted to identify its limits, mainly through its performance in natural language processing (NLP) tasks. This paper aims to join the growing literature regarding ChatGPT's abilities by focusing on its performance in high-resource languages and on its capacity to predict its answers' accuracy by giving a confidence level. The analysis of high-resource languages is of interest as studies have shown that low-resource languages perform worse than English in NLP tasks, but no study so far has analysed whether high-resource languages perform as well as English. The analysis of ChatGPT's confidence calibration has not been carried out before either and is critical to learn about ChatGPT's trustworthiness. In order to study these two aspects, five high-resource languages and two NLP tasks were chosen. ChatGPT was asked to perform both tasks in the five languages and to give a numerical confidence value for each answer. The results show that all the selected high-resource languages perform similarly and that ChatGPT does not have a good confidence calibration, often being overconfident and never giving low confidence values.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper aims to join the growing literature regarding ChatGPT's abilities by focusing on its performance in high-resource languages and on its capacity to predict its answers' accuracy by giving a confidence level."
            },
            "score": 4
        },
        {
            "id": "39cf3e0deb4d76c931514aac32a0620f3ac7a176",
            "paperId": "39cf3e0deb4d76c931514aac32a0620f3ac7a176",
            "title": "Knowledge Construction and Uncertainty in Real World Argumentation: A Text Analysis Approach",
            "abstract": "Collaborative argumentation is key to promoting understanding of scientific issues. However, classroom structures may not always prepare students to engage in argumentation. To address this challenge, education researchers have examined the importance of social knowledge construction and managing uncertainty in group understanding. In this study, we explore these processes using data from /r/ChangeMyView, an online forum on Reddit where users present their opinions, engage others in critiquing ideas, and acknowledge when the discussion has modified their opinions. This unfacilitated environment can illuminate how argumentation evolves naturally towards refined opinions. We employ automated text analyses (LIWC) and discourse analyses to understand the features and discourse sequences of successful arguments. We find that argumentative threads are more likely to be successful if they focus on idea articulation, coherence, and semantic diversity. Findings highlight the role of uncertainty: threads with more certainty words are less likely to be successful. Furthermore, successful arguments are characterized by cycles of raising, managing, and reducing uncertainty, with more occurrences of evidence and idea incorporation. We discuss how learning environments can create norms for idea construction, coherence, and uncertainty, and the potential to provide adaptive prompts to maintain and reduce uncertainty when unproductive argumentative sequences are detected.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study uses data from /r/ChangeMyView, an online forum on Reddit where users present their opinions, engage others in critiquing ideas, and acknowledge when the discussion has modified their opinions to explore how argumentation evolves naturally towards refined opinions."
            },
            "score": 4
        },
        {
            "id": "1db56be01aeb44ca0f3fcebb45180cab1e4cd82e",
            "paperId": "1db56be01aeb44ca0f3fcebb45180cab1e4cd82e",
            "title": "AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation",
            "abstract": "The advancement of natural language processing (NLP) has been significantly boosted by the development of transformer-based large language models (LLMs). These models have revolutionized NLP tasks, particularly in code generation, aiding developers in creating software with enhanced efficiency. Despite their advancements, challenges in balancing code snippet generation with effective test case generation and execution persist. To address these issues, this paper introduces Multi-Agent Assistant Code Generation (AgentCoder), a novel solution comprising a multi-agent framework with specialized agents: the programmer agent, the test designer agent, and the test executor agent. During the coding procedure, the programmer agent will focus on the code generation and refinement based on the test executor agent's feedback. The test designer agent will generate test cases for the generated code, and the test executor agent will run the code with the test cases and write the feedback to the programmer. This collaborative system ensures robust code generation, surpassing the limitations of single-agent models and traditional methodologies. Our extensive experiments on 9 code generation models and 12 enhancement approaches showcase AgentCoder's superior performance over existing code generation models and prompt engineering techniques across various benchmarks. For example, AgentCoder achieves 77.4% and 89.1% pass@1 in HumanEval-ET and MBPP-ET with GPT-3.5, while SOTA baselines obtain only 69.5% and 63.0%.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces Multi-Agent Assistant Code Generation (AgentCoder), a novel solution comprising a multi-agent framework with specialized agents: the programmer agent, the test designer agent, and the test executor agent that ensures robust code generation, surpassing the limitations of single-agent models and traditional methodologies"
            },
            "score": 4
        },
        {
            "id": "72fb75f7c38a83424308c8205bb36cd88995494b",
            "paperId": "72fb75f7c38a83424308c8205bb36cd88995494b",
            "title": "Leveraging Large Language Models for Exploiting ASR Uncertainty",
            "abstract": "While large language models excel in a variety of natural language processing (NLP) tasks, to perform well on spoken language understanding (SLU) tasks, they must either rely on off-the-shelf automatic speech recognition (ASR) systems for transcription, or be equipped with an in-built speech modality. This work focuses on the former scenario, where LLM's accuracy on SLU tasks is constrained by the accuracy of a fixed ASR system on the spoken input. Specifically, we tackle speech-intent classification task, where a high word-error-rate can limit the LLM's ability to understand the spoken intent. Instead of chasing a high accuracy by designing complex or specialized architectures regardless of deployment costs, we seek to answer how far we can go without substantially changing the underlying ASR and LLM, which can potentially be shared by multiple unrelated tasks. To this end, we propose prompting the LLM with an n-best list of ASR hypotheses instead of only the error-prone 1-best hypothesis. We explore prompt-engineering to explain the concept of n-best lists to the LLM; followed by the finetuning of Low-Rank Adapters on the downstream tasks. Our approach using n-best lists proves to be effective on a device-directed speech detection task as well as on a keyword spotting task, where systems using n-best list prompts outperform those using 1-best ASR hypothesis; thus paving the way for an efficient method to exploit ASR uncertainty via LLMs for speech-based applications.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work tackles speech-intent classification task, where a high word-error-rate can limit the LLM's ability to understand the spoken intent, and proposes prompting theLLM with an n-best list of ASR hypotheses instead of only the error-prone 1-best hypothesis."
            },
            "score": 3
        },
        {
            "id": "754d5164e196ff231786d10a48594f3f27d8721f",
            "paperId": "754d5164e196ff231786d10a48594f3f27d8721f",
            "title": "A Comprehensive Study of Multimodal Large Language Models for Image Quality Assessment",
            "abstract": "While Multimodal Large Language Models (MLLMs) have experienced significant advancement on visual understanding and reasoning, their potentials to serve as powerful, flexible, interpretable, and text-driven models for Image Quality Assessment (IQA) remains largely unexplored. In this paper, we conduct a comprehensive and systematic study of prompting MLLMs for IQA. Specifically, we first investigate nine prompting systems for MLLMs as the combinations of three standardized testing procedures in psychophysics (i.e., the single-stimulus, double-stimulus, and multiple-stimulus methods) and three popular prompting strategies in natural language processing (i.e., the standard, in-context, and chain-of-thought prompting). We then present a difficult sample selection procedure, taking into account sample diversity and uncertainty, to further challenge MLLMs equipped with the respective optimal prompting systems. We assess three open-source and one close-source MLLMs on several visual attributes of image quality (e.g., structural and textural distortions, color differences, and geometric transformations) in both full-reference and no-reference scenarios. Experimental results show that only the close-source GPT-4V provides a reasonable account for human perception of image quality, but is weak at discriminating fine-grained quality variations (e.g., color differences) and at comparing visual quality of multiple images, tasks humans can perform effortlessly.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A comprehensive and systematic study of prompting MLLMs for IQA and presents a difficult sample selection procedure, taking into account sample diversity and uncertainty, to further challenge MLLMs equipped with the respective optimal prompting systems."
            },
            "score": 3
        },
        {
            "id": "5b37aa7ee3f517380f92fdac67753adbaa58514f",
            "paperId": "5b37aa7ee3f517380f92fdac67753adbaa58514f",
            "title": "Bayesian Optimization of Catalysts With In-context Learning",
            "abstract": "Large language models (LLMs) are able to do accurate classification with zero or only a few examples (in-context learning). We show a prompting system that enables regression with uncertainty for in-context learning with frozen LLM (GPT-3, GPT-3.5, and GPT-4) models, allowing predictions without features or architecture tuning. By incorporating uncertainty, our approach enables Bayesian optimization for catalyst or molecule optimization using natural language, eliminating the need for training or simulation. Here, we performed the optimization using the synthesis procedure of catalysts to predict properties. Working with natural language mitigates difficulty synthesizability since the literal synthesis procedure is the model's input. We showed that in-context learning could improve past a model context window (maximum number of tokens the model can process at once) as data is gathered via example selection, allowing the model to scale better. Although our method does not outperform all baselines, it requires zero training, feature selection, and minimal computing while maintaining satisfactory performance. We also find Gaussian Process Regression on text embeddings is strong at Bayesian optimization. The code is available in our GitHub repository: https://github.com/ur-whitelab/BO-LIFT",
            "year": 2023,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that in-context learning could improve past a model context window as data is gathered via example selection, allowing the model to scale better, and Gaussian Process Regression on text embeddings is strong at Bayesian optimization."
            },
            "score": 3
        },
        {
            "id": "7dc928f41e15f65f1267bd87b0fcfcc7e715cb56",
            "paperId": "7dc928f41e15f65f1267bd87b0fcfcc7e715cb56",
            "title": "Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting",
            "abstract": "Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. This level of transparency into LLMs' predictions would yield significant safety benefits. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs--e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always\"(A)\"--which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations rationalizing those answers. This causes accuracy to drop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. Building more transparent and explainable systems will require either improving CoT faithfulness through targeted efforts or abandoning CoT in favor of alternative methods.",
            "year": 2023,
            "citationCount": 137,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that CoT explanations can be plausible yet misleading, which risks increasing trust in LLMs without guaranteeing their safety, and building more transparent and explainable systems will require either improving CoT faithfulness through targeted efforts or abandoning CoT in favor of alternative methods."
            },
            "score": 3
        },
        {
            "id": "261549439aebdda72b648ecc462448fd24857ac1",
            "paperId": "261549439aebdda72b648ecc462448fd24857ac1",
            "title": "Progressive-Hint Prompting Improves Reasoning in Large Language Models",
            "abstract": "The performance of Large Language Models (LLMs) in reasoning tasks depends heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency being critical methods that enhance this ability. However, these methods do not fully exploit the answers generated by the LLM to guide subsequent responses. This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers. PHP is orthogonal to CoT and self-consistency, making it easy to combine with state-of-the-art techniques to further improve performance. We conducted extensive and comprehensive experiments on seven benchmarks. The results show that PHP significantly improves accuracy while remaining highly efficient. For instance, with text-davinci-003, we observed a 4.2% improvement on GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction in sample paths with self-consistency. With GPT-4 and PHP, we achieve state-of-the-art performances on SVAMP (89.1% ->91.9%), GSM8K (92% ->95.5%), AQuA (76.4% ->79.9%) and MATH (50.3% ->53.9%).",
            "year": 2023,
            "citationCount": 64,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers."
            },
            "score": 3
        },
        {
            "id": "69619a2a47faee7a29ec596db13172e2a42ff921",
            "paperId": "69619a2a47faee7a29ec596db13172e2a42ff921",
            "title": "Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models",
            "abstract": "Large language models can perform various reasoning tasks by using chain-of-thought prompting, which guides them to find answers through step-by-step demonstrations. However, the quality of the prompts depends on the demonstrations given to the models, and creating many of them by hand is costly. We introduce Synthetic prompting, a method that leverages a few handcrafted examples to prompt the model to generate more examples by itself, and selects effective demonstrations to elicit better reasoning. Our method alternates between a backward and forward process to generate new examples. The backward process generates a question that match a sampled reasoning chain, so that the question is solvable and clear. The forward process produces a more detailed reasoning chain for the question, improving the quality of the example. We evaluate our method on numerical, symbolic, and algorithmic reasoning tasks, and show that it outperforms existing prompting techniques.",
            "year": 2023,
            "citationCount": 41,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Synthetic prompting is introduced, a method that leverages a few handcrafted examples to prompt the model to generate more examples by itself, and selects effective demonstrations to elicit better reasoning."
            },
            "score": 3
        },
        {
            "id": "12c826f4195da172b212a529f8fcf10cc79e35da",
            "paperId": "12c826f4195da172b212a529f8fcf10cc79e35da",
            "title": "Context-faithful Prompting for Large Language Models",
            "abstract": "Large language models (LLMs) encode parametric knowledge about world facts and have shown remarkable performance in knowledge-driven NLP tasks. However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks). In this paper, we seek to assess and enhance LLMs' contextual faithfulness in two aspects: knowledge conflict and prediction with abstention. We demonstrate that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies. In particular, we identify opinion-based prompts and counterfactual demonstrations as the most effective methods. Opinion-based prompts reframe the context as a narrator's statement and inquire about the narrator's opinions, while counterfactual demonstrations use instances containing false facts to improve faithfulness in knowledge conflict situations. Neither technique requires additional training. We conduct experiments on three datasets of two standard NLP tasks, machine reading comprehension and relation extraction, and the results demonstrate significant improvement in faithfulness to contexts. Code and data are released at https://github.com/wzhouad/context-faithful-llm.",
            "year": 2023,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies, and opinion-based prompts and counterfactual demonstrations are identified as the most effective methods."
            },
            "score": 3
        },
        {
            "id": "f953dbea152b4160a4fc7463cc6dcd3214111117",
            "paperId": "f953dbea152b4160a4fc7463cc6dcd3214111117",
            "title": "Aligning with Whom? Large Language Models Have Gender and Racial Biases in Subjective NLP Tasks",
            "abstract": "Human perception of language depends on personal backgrounds like gender and ethnicity. While existing studies have shown that large language models (LLMs) hold values that are closer to certain societal groups, it is unclear whether their prediction behaviors on subjective NLP tasks also exhibit a similar bias. In this study, leveraging the POPQUORN dataset which contains annotations of diverse demographic backgrounds, we conduct a series of experiments on four popular LLMs to investigate their capability to understand group differences and potential biases in their predictions for politeness and offensiveness. We find that for both tasks, model predictions are closer to the labels from White and female participants. We further explore prompting with the target demographic labels and show that including the target demographic in the prompt actually worsens the model's performance. More specifically, when being prompted to respond from the perspective of\"Black\"and\"Asian\"individuals, models show lower performance in predicting both overall scores as well as the scores from corresponding groups. Our results suggest that LLMs hold gender and racial biases for subjective NLP tasks and that demographic-infused prompts alone may be insufficient to mitigate such effects. Code and data are available at https://github.com/Jiaxin-Pei/LLM-Group-Bias.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that for both tasks, model predictions are closer to the labels from White and female participants, and that demographic-infused prompts alone may be insufficient to mitigate such effects."
            },
            "score": 3
        },
        {
            "id": "b626560f19f815808a289ef5c24a17c57320da70",
            "paperId": "b626560f19f815808a289ef5c24a17c57320da70",
            "title": "MathPrompter: Mathematical Reasoning using Large Language Models",
            "abstract": "Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs. To the best of our knowledge, we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption. To address this deficiency, we propose \u2018MathPrompter\u2019, a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions. MathPrompter uses the Zero-shot chain-of-thought prompting technique to generate multiple algebraic expressions or python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results. This is in contrast to other prompt based CoT methods, where there is no check on the validity of the intermediate steps followed. Our technique improves over state-of-the-art on the \u2018MultiArith\u2019 dataset (78.7% - 92.5%) evaluated using 175B parameter GPT-based LLM.",
            "year": 2023,
            "citationCount": 89,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes \u2018MathPrompter\u2019, a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions, using the Zero-shot chain-of-thought prompting technique to generate multiple algebraic expressions or python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results."
            },
            "score": 3
        },
        {
            "id": "44f3b747e0912f7ced56b472531b574410a1d306",
            "paperId": "44f3b747e0912f7ced56b472531b574410a1d306",
            "title": "Collaborative autonomous sensing with Bayesians in the loop",
            "abstract": "There is a strong push to develop intelligent unmanned autonomy that complements human reasoning for applications as diverse as wilderness search and rescue, military surveillance, and robotic space exploration. More than just replacing humans for `dull, dirty and dangerous' work, autonomous agents are expected to cope with a whole host of uncertainties while working closely together with humans in new situations. The robotics revolution firmly established the primacy of Bayesian algorithms for tackling challenging perception, learning and decision-making problems. Since the next frontier of autonomy demands the ability to gather information across stretches of time and space that are beyond the reach of a single autonomous agent, the next generation of Bayesian algorithms must capitalize on opportunities to draw upon the sensing and perception abilities of humans-in/on-the-loop. This work summarizes our recent research toward harnessing `human sensors' for information gathering tasks. The basic idea behind is to allow human end users (i.e. non-experts in robotics, statistics, machine learning, etc.) to directly `talk to' the information fusion engine and perceptual processes aboard any autonomous agent. Our approach is grounded in rigorous Bayesian modeling and fusion of flexible semantic information derived from user-friendly interfaces, such as natural language chat and locative hand-drawn sketches. This naturally enables `plug and play' human sensing with existing probabilistic algorithms for planning and perception, and has been successfully demonstrated with human-robot teams in target localization applications.",
            "year": 2016,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The basic idea behind is to allow human end users to directly `talk to' the information fusion engine and perceptual processes aboard any autonomous agent, grounded in rigorous Bayesian modeling and fusion of flexible semantic information derived from user-friendly interfaces."
            },
            "score": 3
        },
        {
            "id": "30753bc1f583ba7ce71dd0186e109813cd658616",
            "paperId": "30753bc1f583ba7ce71dd0186e109813cd658616",
            "title": "Markov logic for machine reading",
            "abstract": "A long-standing goal of AI and natural language processing (NLP) is to harness human knowledge by automatically understanding text. Known as machine reading, it has become increasingly urgent with the rise of billions of web documents. However, progress in machine reading has been difficult, due to the combination of several key challenges: the complexity and uncertainty in representing and reasoning with knowledge, and the prohibitive cost in providing direct supervision (e.g., designing the meaning representation and labeling examples) for training a machine reading system. \nIn this dissertation, I propose a unifying approach for machine reading based on Markov logic. Markov logic defines a probabilistic model by weighted first-order logical formulas. It provides an ideal language for representing and reasoning with complex, probabilistic knowledge, and opens up new avenues for leveraging indirect supervision via joint inference, where the labels of some objects can be used to predict the labels of others. \nI will demonstrate the promise of this approach by presenting a series of works that applied Markov logic to increasingly challenging problems in machine reading. First, I will describe a joint approach for citation information extraction that combines information among different citations and processing stages. Using Markov logic as a representation language and the generic learning and inference algorithms available for it, our solution largely reduced to writing appropriate logical formulas and was able to achieve state-of-the-art accuracy with substantially less engineering effort compared to previous approaches. \nNext, I will describe an unsupervised coreference resolution system that builds on Markov logic to incorporate prior knowledge and conduct large-scale joint inference. This helps compensate for the lack of labeled examples, and our unsupervised system often ties or even outperforms previous state-of-the-art supervised systems. \nFinally, I will describe the USP system, the first unsupervised approach for jointly inducing a meaning representation and extracting detailed meanings from text. To resolve linguistic variations for the same meaning, USP recursively clusters expressions that are composed with or by similar expressions. USP can also induce ontological relations by creating abstractions to assimilate commonalities among non-synonymous meaning clusters. This results in a state-of-the-art end-to-end machine reading system that can read text, extract knowledge and answer questions, all without any labeled examples. Markov logic provides an extremely compact representation of the USP model, and enables future work to \"close the loop\" by incorporating the extracted knowledge into the model to aid further extraction.",
            "year": 2011,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This dissertation proposes a unifying approach for machine reading based on Markov logic, and describes the USP system, the first unsupervised approach for jointly inducing a meaning representation and extracting detailed meanings from text."
            },
            "score": 3
        },
        {
            "id": "39f0d1b894130852ee9f39a5df58905a09645c81",
            "paperId": "39f0d1b894130852ee9f39a5df58905a09645c81",
            "title": "Multistage Collaborative Knowledge Distillation from Large Language Models",
            "abstract": "We study semi-supervised sequence generation tasks where labeled data are too scarce to effectively finetune a model and at the same time few-shot prompting of a large language model (LLM) has suboptimal performance. This happens when a task, such as parsing, is expensive to annotate and also unfamiliar to a pretrained LLM. In this paper, we present a discovery that student models distilled from an in-context learned LLM can often generalize better than their teacher on such tasks. Leveraging this finding, we present a new method \u2013 multistage collaborative knowledge distillation from an LLM ( MCKD ) \u2013 for such tasks. MCKD first few-shot prompts an LLM to produce pseudola-bels for unlabeled data. At each intermediate knowledge distillation (KD) stage, a new pair of students is trained on disjoint partitions of the pseudolabeled data. Each student then produces new and improved pseudolabels for its unseen partition to be used in the next stage of distillation. We demonstrate the advantage of multistage cross-partition labeling on several syntactic and semantic parsing tasks. On CRAFT biomedical parsing, for example, 3-stage MCKD with 50 labeled examples outperforms the prompted LLM and vanilla KD by 7.5% and 3.7% parsing F1, respectively, and matches the performance of supervised finetuning with 500 examples.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is presented a discovery that student models distilled from an in-context learned LLM can often generalize better than their teacher on such tasks and a new method \u2013 multistage collaborative knowledge distillation from an LLM ( MCKD) \u2013 for such tasks."
            },
            "score": 3
        },
        {
            "id": "434b9f9bc71c935e4a46a1aff36a8cc4c22d9afa",
            "paperId": "434b9f9bc71c935e4a46a1aff36a8cc4c22d9afa",
            "title": "Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration",
            "abstract": "Human intelligence thrives on cognitive synergy, where collaboration among different minds yield superior outcomes compared to isolated individuals. In this work, we propose Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas. A cognitive synergist is an intelligent agent that collaboratively combines multiple minds' strengths and knowledge to enhance problem-solving in complex tasks. By dynamically identifying and simulating different personas based on task inputs, SPP unleashes the potential of cognitive synergy in LLMs. Our in-depth analysis shows that assigning multiple fine-grained personas in LLMs improves problem-solving abilities compared to using a single or fixed number of personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing, Codenames Collaborative, and Logic Grid Puzzle, encompassing both knowledge-intensive and reasoning-intensive types. Unlike previous works, such as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, experimental results demonstrate that SPP effectively reduces factual hallucination, and maintains strong reasoning capabilities. Additionally, comparative experiments show that cognitive synergy only emerges in GPT-4 and does not appear in less capable models, such as GPT-3.5-turbo and Llama2-13b-chat, which draws an interesting analogy to human development. Code, data, and prompts can be found at: https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.",
            "year": 2023,
            "citationCount": 42,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes Solo Performance Prompting (SPP), which transforms a single LLM into a cognitive synergist by engaging in multi-turn self-collaboration with multiple personas by dynamically identifying and simulating different personas based on task inputs, which unleashes the potential of cognitive synergy in LLMs."
            },
            "score": 3
        },
        {
            "id": "022229b056cbff623fd947f32da33c92f6bd954b",
            "paperId": "022229b056cbff623fd947f32da33c92f6bd954b",
            "title": "Cloud-Device Collaborative Adaptation to Continual Changing Environments in the Real-World",
            "abstract": "When facing changing environments in the real world, the lightweight model on client devices suffers from severe performance drops under distribution shifts. The main limitations of the existing device model lie in (1) unable to update due to the computation limit of the device, (2) the limited generalization ability of the lightweight model. Meanwhile, recent large models have shown strong generalization capability on the cloud while they can not be deployed on client devices due to poor computation constraints. To enable the device model to deal with changing environments, we propose a new learning paradigm of Cloud-Device Collaborative Continual Adaptation, which encourages collaboration between cloud and device and improves the generalization of the device model. Based on this paradigm, we further propose an Uncertainty-based Visual Prompt Adapted (U-VPA) teacher-student model to transfer the generalization capability of the large model on the cloud to the device model. Specifically, we first design the Uncertainty Guided Sampling (UGS) to screen out challenging data continuously and transmit the most out-of-distribution samples from the device to the cloud. Then we propose a Visual Prompt Learning Strategy with Uncertainty guided updating (VPLU) to specifically deal with the selected samples with more distribution shifts. We transmit the visual prompts to the device and concatenate them with the incoming data to pull the device testing distribution closer to the cloud training distribution. We conduct extensive experiments on two object detection datasets with continually changing environments. Our proposed U-VPA teacher-student framework outperforms previous state-of-the-art test time adaptation and device-cloud collaboration methods. The code and datasets will be released.",
            "year": 2022,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new learning paradigm of Cloud-Device Collaborative Continual Adaptation, which encourages collaboration between cloud and device and improves the generalization of the device model is proposed, which outperforms previous state-of-the-art test time adaptation and device-cloud collaboration methods."
            },
            "score": 3
        },
        {
            "id": "213a616590be543c5a2d7dee3206ecd816c58b4a",
            "paperId": "213a616590be543c5a2d7dee3206ecd816c58b4a",
            "title": "Macroeconomic Effects of Uncertainty: A Big Data Analysis for India",
            "abstract": "Uncertainty about the current state and near-term outlook of an economy as well as the likely course of future policy actions can prompt economic agents to alter their decisions to spend, save, invest and hire. In this paper, we construct three alternative indices to measure the level of uncertainty for the Indian economy. The first two uncertainty indices are constructed by applying text mining and natural language processing (NLP) techniques on a dataset compiled from leading Indian business newspapers. The third index is based on internet search intensity data available from Google Trends. Empirical findings from a Local Projections-based econometric framework suggest that uncertainty shocks influence financial markets as well as the real economy in India. Our results indicate that both investment activity and real GDP growth slow down when uncertainty increases in the economy. Such uncertainty indices can help strengthen policy simulation exercises to study the impact of low/high uncertainty scenarios and also improve near-term projection of macroeconomic variables which exhibit high degree of sensitivity to uncertainty.",
            "year": 2020,
            "citationCount": 7,
            "tldr": null,
            "score": 3
        },
        {
            "id": "83fd6233b1a3b27e8192917411c4f307ed9a376d",
            "paperId": "83fd6233b1a3b27e8192917411c4f307ed9a376d",
            "title": "Development of the Online Collaborative Summarizing Feature on Student-Centered E-Learning Environment",
            "abstract": "Universitas Indonesia has implemented a Learning Management System (LMS) known as Student-Centered E-Learning Environment (SCeLE). The SCeLE facilitates collaborative learning through an online discussion forum. Collaborative learning can promote critical thinking. However, students will have difficulties contributing and understanding discussions when the discussion process is fast, lengthy and when diverse ideas arise. This could lead to uncertainty and demotivation among the students. On the other hand, teachers have difficulties in monitoring students\u2019 understanding and the discussion progress. Consequently, providing constructive feedback promptly becomes challenging. This study proposed an online collaborative summarizing feature to be integrated with the online discussion forum to facilitate the construction of a discussion summary. The tool was developed using the User-Centered Design (UCD) approach and tested using the usability evaluation. The participants (both students and lecturers) found that the usability of the developed system was good. This was indicated by the fact that more than 80% of the students and more than 80% of the lecturers were able to finish the tasks given easily. Furthermore, the SUS score exceeded the average score, meaning that students and lecturers found that the developed system had a good usability.",
            "year": 2019,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study proposed an online collaborative summarizing feature to be integrated with the online discussion forum to facilitate the construction of a discussion summary that was developed using the User-Centered Design approach and tested using the usability evaluation."
            },
            "score": 3
        },
        {
            "id": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
            "paperId": "1b6e810ce0afd0dd093f789d2b2742d047e316d5",
            "title": "Chain of Thought Prompting Elicits Reasoning in Large Language Models",
            "abstract": "We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",
            "year": 2022,
            "citationCount": 3517,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks."
            },
            "score": 2
        },
        {
            "id": "62176de125738e3b95850d1227bac81fd646b78e",
            "paperId": "62176de125738e3b95850d1227bac81fd646b78e",
            "title": "Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models",
            "abstract": "Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, Few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual efforts, Zero-shot-CoT concatenates the target problem statement with \u201cLet\u2019s think step by step\u201d as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend PS prompting with more detailed instructions and derive PS+ prompting. We evaluate our proposed prompting strategy on ten datasets across three reasoning problems. The experimental results over GPT-3 show that our proposed zero-shot prompting consistently outperforms Zero-shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem. The code can be found at https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.",
            "year": 2023,
            "citationCount": 115,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results over GPT-3 show that the proposed zero-shot prompting consistently outperforms Zero- shot-CoT across all datasets by a large margin, is comparable to or exceeds Zero-shot-Program-of-Thought Prompting, and has comparable performance with 8-shot CoT prompting on the math reasoning problem."
            },
            "score": 2
        },
        {
            "id": "2d3bc530d8f1ed36932a70bc362ea94d988adec9",
            "paperId": "2d3bc530d8f1ed36932a70bc362ea94d988adec9",
            "title": "Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting",
            "abstract": "Ranking documents using Large Language Models (LLMs) by directly feeding the query and candidate documents into the prompt is an interesting and practical problem. However, researchers have found it difficult to outperform fine-tuned baseline rankers on benchmark datasets. We analyze pointwise and listwise ranking prompts used by existing methods and argue that off-the-shelf LLMs do not fully understand these challenging ranking formulations. In this paper, we propose to significantly reduce the burden on LLMs by using a new technique called Pairwise Ranking Prompting (PRP). Our results are the first in the literature to achieve state-of-the-art ranking performance on standard benchmarks using moderate-sized open-sourced LLMs. On TREC-DL 2019&2020, PRP based on the Flan-UL2 model with 20B parameters performs favorably with the previous best approach in the literature, which is based on the blackbox commercial GPT-4 that has 50x (estimated) model size, while outperforming other LLM-based solutions, such as InstructGPT which has 175B parameters, by over 10% for all ranking metrics. By using the same prompt template on seven BEIR tasks, PRP outperforms supervised baselines and outperforms the blackbox commercial ChatGPT solution by 4.2% and pointwise LLM-based solutions by more than 10% on average NDCG@10. Furthermore, we propose several variants of PRP to improve efficiency and show that it is possible to achieve competitive results even with linear complexity.",
            "year": 2023,
            "citationCount": 79,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results are the first in the literature to achieve state-of-the-art ranking performance on standard benchmarks using moderate-sized open-sourced LLMs and several variants of PRP are proposed to improve efficiency and show that it is possible to achieve competitive results even with linear complexity."
            },
            "score": 2
        },
        {
            "id": "fccf8776d7525627c518a56a1f4db367a4d7120b",
            "paperId": "fccf8776d7525627c518a56a1f4db367a4d7120b",
            "title": "Choice Over Control: How Users Write with Large Language Models using Diegetic and Non-Diegetic Prompting",
            "abstract": "We propose a conceptual perspective on prompts for Large Language Models (LLMs) that distinguishes between (1) diegetic prompts (part of the narrative, e.g. \u201cOnce upon a time, I saw a fox...\u201d), and (2) non-diegetic prompts (external, e.g. \u201cWrite about the adventures of the fox.\u201d). With this lens, we study how 129 crowd workers on Prolific write short texts with different user interfaces (1 vs 3 suggestions, with/out non-diegetic prompts; implemented with GPT-3): When the interface offered multiple suggestions and provided an option for non-diegetic prompting, participants preferred choosing from multiple suggestions over controlling them via non-diegetic prompts. When participants provided non-diegetic prompts it was to ask for inspiration, topics or facts. Single suggestions in particular were guided both with diegetic and non-diegetic information. This work informs human-AI interaction with generative models by revealing that (1) writing non-diegetic prompts requires effort, (2) people combine diegetic and non-diegetic prompting, and (3) they use their draft (i.e. diegetic information) and suggestion timing to strategically guide LLMs.",
            "year": 2023,
            "citationCount": 30,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work informs human-AI interaction with generative models by revealing that (1) writing non-diegetic prompts requires effort, (2) people combine diegetic and non- diegetic prompting, and (3) they use their draft (i.e. diegetic information) and suggestion timing to strategically guide LLMs."
            },
            "score": 2
        },
        {
            "id": "2eb0d00e5675582980245b95a48e40bd8e5f46a0",
            "paperId": "2eb0d00e5675582980245b95a48e40bd8e5f46a0",
            "title": "Vision-Language Models Performing Zero-Shot Tasks Exhibit Gender-based Disparities",
            "abstract": "We explore the extent to which zero-shot vision-language models exhibit gender bias for different vision tasks. Vision models traditionally required task-specific labels for representing concepts, as well as finetuning; zero-shot models like CLIP instead perform tasks with an open-vocabulary, meaning they do not need a fixed set of labels, by using text embeddings to represent concepts. With these capabilities in mind, we ask: Do vision-language models exhibit gender bias when performing zero-shot image classification, object detection and semantic segmentation? We evaluate different vision-language models with multiple datasets across a set of concepts and find (i) all models evaluated show distinct performance differences based on the perceived gender of the person co-occurring with a given concept in the image and that aggregating analyses over all concepts can mask these concerns; (ii) model calibration (i.e. the relationship between accuracy and confidence) also differs distinctly by perceived gender, even when evaluating on similar representations of concepts; and (iii) these observed disparities align with existing gender biases in word embeddings from language models. These findings suggest that, while language greatly expands the capability of vision tasks, it can also contribute to social biases in zero-shot vision settings. Furthermore, biases can further propagate when foundational models like CLIP are used by other models to enable zero-shot capabilities.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work evaluates different vision-language models with multiple datasets across a set of concepts and finds all models evaluated show distinct performance differences based on the perceived gender of the person co-occurring with a given concept in the image."
            },
            "score": 2
        },
        {
            "id": "9c1485ae4f96fd8942c124e7b1564fd929e71a42",
            "paperId": "9c1485ae4f96fd8942c124e7b1564fd929e71a42",
            "title": "Vision-Language Models Performing Zero-Shot Tasks Exhibit Disparities Between Gender Groups",
            "abstract": "We explore the extent to which zero-shot vision-language models exhibit gender bias for different vision tasks. Vision models traditionally required task-specific labels for representing concepts, as well as finetuning; zero-shot models like CLIP instead perform tasks with an open-vocabulary, meaning they do not need a fixed set of labels, by using text embeddings to represent concepts. With these capabilities in mind, we ask: Do vision-language models exhibit gender bias when performing zero-shot image classification, object detection and semantic segmentation? We evaluate different vision-language models with multiple datasets across a set of concepts and find (i) all models evaluated show distinct performance differences when identifying concepts based on the gender of the person co-occurring in the image (ii) model calibration (i.e., the relationship between accuracy and confidence) also differs distinctly by gender, even when evaluating on similar representations of concepts and (iii) these observed disparities align with existing gender biases in word embeddings from language models. These findings suggest that, while language greatly expands the capability of vision tasks, it can contribute to propagating social biases in zero-shot settings.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is suggested that, while language greatly expands the capability of vision tasks, it can contribute to propagating social biases in zero-shot settings."
            },
            "score": 2
        },
        {
            "id": "c18f2239a4bd8cc68db9a013416167357f5e1353",
            "paperId": "c18f2239a4bd8cc68db9a013416167357f5e1353",
            "title": "Evaluating the Performance of Large Language Models on GAOKAO Benchmark",
            "abstract": "Large Language Models(LLMs) have demonstrated remarkable performance across various natural language processing tasks; however, how to comprehensively and accurately assess their performance becomes an urgent issue to be addressed. This paper introduces GAOKAO-Bench, an intuitive benchmark that employs questions from the Chinese GAOKAO examination as test samples, including both subjective and objective questions. To align with human examination methods, we design a method based on zero-shot settings to evaluate the performance of LLMs. With human evaluation, we obtain the converted total score of LLMs, including GPT-4, ChatGPT and ERNIE-Bot.Our findings reveal that LLMs have achieved competitive scores in Chinese GAOKAO examination, while they exhibit significant performance disparities across various subjects. We also use LLMs to grade the subjective questions, and find that model scores achieve a moderate level of consistency with human scores. In conclusion, this research contributes a robust evaluation benchmark for future large language models and offers valuable insights into the advantages and limitations of such models.",
            "year": 2023,
            "citationCount": 42,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "GAOKAO-Bench is introduced, an intuitive benchmark that employs questions from the Chinese GAOKAO examination as test samples, including both subjective and objective questions that contribute a robust evaluation benchmark for future large language models and offers valuable insights into the advantages and limitations of such models."
            },
            "score": 2
        },
        {
            "id": "12db3efff4cc9e16822dd64bb1cad66f3f034f3b",
            "paperId": "12db3efff4cc9e16822dd64bb1cad66f3f034f3b",
            "title": "L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models",
            "abstract": "Recently, large language models (LLMs), especially those that are pretrained on code, have demonstrated strong capabilities in generating programs from natural language inputs in a few-shot or even zero-shot manner. Despite promising results, there is a notable lack of a comprehensive evaluation of these models language-to-code generation capabilities. Existing studies often focus on specific tasks, model architectures, or learning paradigms, leading to a fragmented understanding of the overall landscape. In this work, we present L2CEval, a systematic evaluation of the language-to-code generation capabilities of LLMs on 7 tasks across the domain spectrum of semantic parsing, math reasoning and Python programming, analyzing the factors that potentially affect their performance, such as model size, pretraining data, instruction tuning, and different prompting methods. In addition to assessing model performance, we measure confidence calibration for the models and conduct human evaluations of the output programs. This enables us to identify and analyze the typical failure modes across various tasks and models. L2CEval offers a comprehensive understanding of the capabilities and limitations of LLMs in language-to-code generation. We also release the evaluation framework and all model outputs, hoping to lay the groundwork for further future research in this domain.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents L2CEval, a systematic evaluation of the language-to-code generation capabilities of LLMs on 7 tasks across the domain spectrum of semantic parsing, math reasoning and Python programming, analyzing the factors that potentially affect their performance."
            },
            "score": 2
        },
        {
            "id": "993143b7cf13e8e6fa9243996c60aa9a29c721cc",
            "paperId": "993143b7cf13e8e6fa9243996c60aa9a29c721cc",
            "title": "OceanChat: Piloting Autonomous Underwater Vehicles in Natural Language",
            "abstract": "In the trending research of fusing Large Language Models (LLMs) and robotics, we aim to pave the way for innovative development of AI systems that can enable Autonomous Underwater Vehicles (AUVs) to seamlessly interact with humans in an intuitive manner. We propose OceanChat, a system that leverages a closed-loop LLM-guided task and motion planning framework to tackle AUV missions in the wild. LLMs translate an abstract human command into a high-level goal, while a task planner further grounds the goal into a task sequence with logical constraints. To assist the AUV with understanding the task sequence, we utilize a motion planner to incorporate real-time Lagrangian data streams received by the AUV, thus mapping the task sequence into an executable motion plan. Considering the highly dynamic and partially known nature of the underwater environment, an event-triggered replanning scheme is developed to enhance the system's robustness towards uncertainty. We also build a simulation platform HoloEco that generates photo-realistic simulation for a wide range of AUV applications. Experimental evaluation verifies that the proposed system can achieve improved performance in terms of both success rate and computation time. Project website: \\url{https://sites.google.com/view/oceanchat}",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes OceanChat, a system that leverages a closed-loop LLM-guided task and motion planning framework to tackle AUV missions in the wild, and builds a simulation platform HoloEco that generates photo-realistic simulation for a wide range of AUV applications."
            },
            "score": 2
        },
        {
            "id": "8fe7bdc62b68e5178b7a24950a3710765307e9f4",
            "paperId": "8fe7bdc62b68e5178b7a24950a3710765307e9f4",
            "title": "Advanced control sequences and FDD technology. Just shiny objects, or ready for scale?",
            "abstract": "Innovations in commercial building control sequences (ASHRAE guideline 36) and fault detection and diagnostics (FDD) technology have the potential to transform existing building operational efficiency by realizing whole-building level savings on the order of 15% and higher. However, several technical and market barriers related to up-front estimation of achievable savings, verifying implementation, and ensuring persistence are slowing down their adoption as a core offering through diverse owner-funded initiatives and rate-payer funded programs. This paper describes a suite of solutions to overcome these barriers to program delivery of advanced HVAC controls integrated with FDD energy management and information system (EMIS) technology. First, the Guideline 36 conformance test offers an automated method to confirm correct sequence implementation by the manufacturer, before field deployment. A successful test of a controller was conducted using a new manufacturer-independent hardware-in-the-loop testbed demonstrating the viability and scalability of this approach. Second, the Guideline 36 energy savings calculator uses modeling paired with high-level user inputs to estimate buildingspecific savings potential, target cost effective implementation and minimize uncertainty. Initial results indicate the baseline control sequences significantly impact energy savings, however, as much as 50% savings may be possible for the worst-case baseline scenario. Third, a functional specification provides minimum recommended EMIS-FDD capabilities, and best practices to integrate the technology into organizational practice and ensure that advanced control sequences provide persistent savings. Introduction and Background Emerging as an industry in the 70\u2019s and growing significantly during the \u201880s and \u201890s, with the introduction of microcontroller-based technology, the building controls industry has been slow to innovate and to use standardized solutions, in the last two decades (Pritoni, 2019). While some progress has been made in adopting standard communication protocols, such as BACnet1, vendors still use proprietary solutions for the implementation of the \u201csequences of operation\u201d (SOO), that is the logic that determines the coordinated operation of all the components in a building (Pritoni et al., 2018). These sequences are necessary since HVAC systems for large buildings are typically custom-designed and each one is unique in terms of components and configuration. These SOOs are commonly written in a vendor-specific language by the control contractor when the system is installed. The control programs are coded on site without quality control procedures, commonly used in software and hardware developed for 1 BACnet is a standard communication protocol for Building Automation and Control Networks developed and maintained by ASHRAE: 3-300 \u00a92020 Summer Study on Energy Efficiency in Buildings industrialized products (e.g., a control board for a packaged HVAC unit). As a result, the energy efficiency of these SOO is highly dependent on programmer skills, programmer understanding of the building systems, proper commissioning and timely update in case of operational changes. More importantly, there seemed to be a lack of shared knowledge on how to specify \u201cbest-inclass\u201d SOOs, independently from the control language they are coded in. To respond to this last need, a research committee at ASHRAE2 recently developed Guideline 36 (G36) (ASHRAE, 2018), with the purpose of \u201cproviding uniform sequences of operation for heating, ventilating, and air-conditioning (HVAC) systems that are intended to maximize HVAC system energy efficiency and performance, provide control stability, and allow for real-time fault detection and diagnostics\u201d.3 In addition, in the last few years, several new Fault Detection and Diagnostic (FDD) software platforms have become available on the market (Granderson et al, 2018). These software products can be used to verify that the performance of \u201coptimized\u201d sequences is not deteriorating due to faults or other issues. Table 1. Gaps and solutions addressed in this paper # Gap Stakeholder Impacted Solution 1 No standard performance validation method for SOO Control Vendor, G36 committee G36 Performance Validation Method and Testbed 2 Unknown savings for a specific building configuration Building Owner/Manager, Engineering Firm G36 Energy Savings Calculator 3 Unclear requirements for FDD tools Building Owner/Manager, FDD provider FDD Specification Guide for G36 While these technologies (i.e., G36 sequences coupled with FDD) have a large potential for delivering energy savings and better indoor environmental quality in buildings, there is no clear path to make them a scalable solution in the market, yet. A CEC-funded project led by Taylor Engineering in partnership with Lawrence Berkeley National Laboratory (LBNL), TRC Energy Services and Integral Group is set to tackle some of these challenges. The project will address technological and market challenges. From a technology standpoint, the project will validate the commercial viability and scalability of Best-in-Class controls retrofits in existing commercial buildings based on G36 sequences. From a market perspective, the project will demonstrate a new delivery mechanism for control systems upgrades with close collaboration with industry, utility, and other partners to address California\u2019s need for cost-effective energy and carbon reductions. A companion paper in these proceedings (Paliaga G. et al, 2020) describes the market transformation aspect of the project. This paper explores a set of solutions developed by the research team to bridge three key gaps that hinder the rapid deployment of G36 2 ASHRAE: American Society of Heating, Refrigerating and Air-Conditioning Engineers. https://www.ashrae.org/ 3 See definition of the purpose of the standard here: https://www.ashrae.org/technical-resources/standardsand-guidelines/titles-purposes-and-scopes 3-301 \u00a92020 Summer Study on Energy Efficiency in Buildings sequences in several buildings. These gaps and solutions are listed in Table 1 and detailed in the corresponding sections of the paper. Guideline 36 Performance Validation Method and Testbed ASHRAE G36 establishes a set of standardized high-performance sequences of operation. In order to achieve the intended performance, the sequences must be translated and programmed accurately in the Building Automation System (BAS) controllers. The long-term opportunity is that standardizing these sequences will allow for a more efficient product delivery mechanism where the sequences are programmed and tested centrally by each manufacturer, and then distributed to their dealers/installers/integrators. This approach would minimize the need for each installer to re-interpret and program the sequences, reduce risk of errors, and reduce the time required for commissioning in the field. To achieve this goal, a performance validation method is needed to provide independent confirmation that each manufacturer has programmed the Guideline 36 sequences accurately. A standardized method of test would also avoid the need for manual interpretation (and the associated human variability) that is required with typical functional testing4 approaches by automating the inputs and the range of expected responses. The goal of this effort is to prototype an automated test that provides standardized, repeatable, and objective validation that programming accurately conforms with the requirements of Guideline 36. The platform needs to be generic so that it can be integrated with controllers from any manufacturer. While different approaches are possible to define the boundaries of the system to be tested, the team decided to test programming on a physical controller using software to simulate the environment rather than connections to physical devices and sensors. Control inputs (or sensor feedback, e.g. temperature and flow) would be simulated using overrides over a network connection, with the controller responses (outputs) similarly monitored in software. Use of a network connection using a standardized communication protocol, like BACnet (ASHRAE, 2019) allows for the simulation platform to be generic and compatible with controllers from various manufacturers. The software environment also provides flexibility to simulate different systems and system options relatively simply. It also provides a relatively simple means to revise and repeat testing with new updates to the guideline and associated programming. This test approach focuses on the programming only and would not address issues in real practice such as point-to-point testing, sensor calibration, or control loop tuning.5 This approach best addresses the core need for the performance validation method with a practical and reasonable level of effort to test different system options, and with flexibility to repeat testing as necessary with future update cycles to Guideline 36. The testbed and test procedure were developed to be: 1) fully automated (execution of the test does not need user intervention); 2) repeatable and consistent (every iteration of the test must return the same results); 3) vendor agnostic (compatible with controllers from all manufacturers); 4) low cost; 5) user friendly (test scripts fed into software should be understandable in plain English). 4 Test of the dynamic function and operation of equipment and systems using manual (direct observation) or monitoring methods. https://www.energy.gov/sites/prod/files/2014/07/f17/commissioning_fed_facilities.pdf 5 Additional steps in the process of commissioning a control system for a building. 3-302 \u00a92020 Summer Study on Energy Efficiency in Buildings The architecture of the testbed prototyped is illustrated in Figure 1. Yellow indicates the components that are provided by the manufacturer (i.e., controller, control program and point map); blue indicates the English version of G36 devel",
            "year": 2020,
            "citationCount": 3,
            "tldr": null,
            "score": 2
        },
        {
            "id": "b83053701705eef936ab0089c4451d85986208d6",
            "paperId": "b83053701705eef936ab0089c4451d85986208d6",
            "title": "Modeling for Sustainability (Dagstuhl Seminar 18351)",
            "abstract": "This report documents the program and the outcomes of Dagstuhl Seminar 18351 \u201cModeling for Sustainability\u201d, August 26\u201331, 2018. \n \nMany different kinds of models, from engineering models to scientific models, have to be integrated and coordinated to support sustainability systems such as smart grid or cities, i.e., dynamically adaptable resource management systems that aim to improve the technoeconomic, social, and environmental dimensions of sustainability. Scientific models help understand sustainability concerns and evaluate alternatives, while engineering models support the development of sustainability systems. As the complexity of these systems increases, many challenges are posed to the computing disciplines to make data and modelbased analysis results more accessible as well as integrate scientific and engineering models while balancing trade-offs among varied stakeholders. This seminar explored the intrinsic nature of both scientific and engineering models, the underlying differences in their respective foundations, and the challenges related to their integration, evolution, analysis, and simulation including the exploration of what-if scenarios. Sustainability systems must provide facilities for the curation and monitoring of data sets and models and enable flexible (open) data and model integration, e.g., physical laws, scientific models, regulations and preferences, possibly coming from different technological foundations, abstractions, scale, technological spaces, and world views. This also includes the continuous, automated acquisition and analysis of new data sets, as well as automated export of data sets, scenarios, and decisions. The main function is to support the generation of what-if scenarios to project the effects on the different sustainability dimensions, and support the evaluation of externalities, especially for non rapidly renewable resources. Since the predictions are necessarily probabilistic, the system must be able to assess the uncertainty inherent in all its actions and provide suitable representations of uncertainty understandable by users. In addition to generating what-if scenarios to explore alternate model instantiations, the tool should be capable of generating suggestions for how to reach user-specified goals including quantifiable impacts and driving the dynamic adaptation of sustainability systems. These powerful services must be made accessible to the population at large, regardless of their individual situation, social status, and level of education. This seminar explored how Model-Driven Engineering (MDE) will help to develop such an approach, and in particular i) how modeling frameworks would support the integration of the various heterogeneous models, including both engineering and scientific models; ii) how domain specific languages (DSLs) would (a) support the required socio-technical coordination, i.e., engage engineers, scientists, decision makers, communities, and the general public; and (b) integrate analysis/probabilistic/ user models into the control loop of smart CPS (cyber physical system). DSLs are also supposed to provide the right interface (in terms of abstractions/ constructs) to be used as tools for discovering problems and evaluating ideas. The seminar served to identify critical disciplines and stakeholders to address MDE for sustainability and the research roadmap of the MDE community with regards to the development of sustainability systems. In particular, the seminar identified and explored four key areas: 1) research challenges relevant to modeling for sustainability (M4S); 2) a multidisciplinary collection of relevant literature to provide the foundation for exploring the research challenges; 3) three case studies from different application domains that provide a vehicle for illustrating the M4S challenges and for validating relevant research techniques; and 4) the human and social aspects of M4S. The cumulative results of the work performed at the seminar and subsequent collaborations will help to establish the required foundations for integrating engineering and scientific models, and to explore the required management facilities for evaluating what-if scenarios and driving adaptive systems. In addition, we envision to produce as an outcome of the seminar a representative case study that will be used by the community to assess and validate contributions in the field of modeling for sustainability.",
            "year": 2018,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The seminar served to identify critical disciplines and stakeholders to address MDE for sustainability and the research roadmap of the MDE community with regards to the development of sustainability systems and explored four key areas."
            },
            "score": 2
        },
        {
            "id": "7240d7c68e3383d49d65a13b43a7f1af3d070923",
            "paperId": "7240d7c68e3383d49d65a13b43a7f1af3d070923",
            "title": "An introduction to the paper by Haykin , Fatemi , Setoodeh , and Xue Cognitive Control",
            "abstract": "The fields of neuroscience and psychology offer control engineers plenty of inspiration for evolving simple controllers into controllers that are more sophisticated, optimized, adapted, and intelligent. Typically, the control tradeoff exists between optimality and robustness. The goal of adaptability is also highly desirable. Adaptive controllers feature adjustable parameters and update mechanisms that respond to variations and disturbances. Neurocontrollers take a different approach. They are based on black-box modeling (and sometimes hybrid approaches). Such controllers function quite well in structured environments but not so well in dynamic, uncertain environments, leading to the necessity for human intervention. Here is where psychology comes into play. If the controller could collect the sufficient information it needs to perform tasks and organize itself, human operators would not be necessary in the control loop, and the means to do that is to build cognitive functionality into the system. Cognitive control is part of a wider framework called cognitive dynamic systems, which builds on a paradigm of cognition composed of five elements: perception\u2013action cycle, memory, attention, intelligence, and language. Any closed-loop feedback control system is based on the perception\u2013action cycle. Adaptive control systems, it can be argued, incorporate the attention and intelligence aspects of the paradigm, but not memory. The language component becomes relevant to a network of cognitive agents. It is thought that in human cognition, the brain\u2019s cortex uses a fundamental information processing algorithm distinct from the nature of sensory input, which suggests that sensory input gets coded into some standard form that feeds into the algorithm for processing. Information flow in our nervous system plays a vital role in human activity; similarly, machine performance and robustness rely on proper control over the flow of information. This paper examines efforts to direct information flow in control systems and examines the concept of information gap and the risks associated with an action or decision policy. The theory of cybernetics took a multidisciplinary approach with emphasis on the concepts of information and feedback to unify theories of communications and control. Cybernetics highlighted that machines and living organisms exhibit similar behavioral mechanisms, and important parallels exist between computation and control in machines and the nervous system. In the effort to design systems that mimic human behavior, researchers observed that intentional actions can be explained with feedback, and that feedback provides the key to manipulation of information. Information theory can provide the means for quantitatively measuring information, depending on the probabilistic structure of a communication channel. However, the chosen information measure must account for the importance of uncertainties to decision makers. All of this relies upon a better understanding of the value of information. How do we define it from within a control context? It is more useful to think of information as not a thing in itself but as an attribute of the relationship between things. In other words, information is not the data but the useful or relevant portion of the data. Relevance can only be determined within the context of perception that is directed toward the goal of decision making and control. Relevance is useful for dictating, for instance, the nature of feature extraction, dimension reduction, and learning. To quantify relevance, one must apply the concept of sufficient statistics. The sufficiency of a statistic is designated by the risk associated with a control or decision policy, e.g., loss or cost. Also, data can only be valued after nuisance factors (noise, clutter) have been canceled out. In order to represent information, one must take complexity into account: the longer it takes to define a category, the more complex or difficult it is. An alternative definition of information suggests that information consists of the invariants underlying change. One extracts invariants to determine a phenomenon of interest, despite uncertainty. The operational view of information as Cognitive control depends on an understanding of the information gapVthe distancebetween relevant information extracted from measurements and the sufficient information that is required.",
            "year": 2012,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Examination of efforts to direct information flow in control systems is examined and the concept of information gap and the risks associated with an action or decision policy are examined."
            },
            "score": 2
        },
        {
            "id": "09239dac5b1cded9414c946333eaf619dca9aaa7",
            "paperId": "09239dac5b1cded9414c946333eaf619dca9aaa7",
            "title": "ChatGPT and Other Large Language Models as Evolutionary Engines for Online Interactive Collaborative Game Design",
            "abstract": "Large language models (LLMs) have taken the scientific world by storm, changing the landscape of natural language processing and human-computer interaction. These powerful tools can answer complex questions and, surprisingly, perform challenging creative tasks (e.g., generate code and applications to solve problems, write stories, pieces of music, etc.). In this paper, we present a collaborative game design framework that combines interactive evolution and large language models to simulate the typical human design process. We use the former to exploit users' feedback for selecting the most promising ideas and large language models for a very complex creative task---the recombination and variation of ideas. In our framework, the process starts with a brief and a set of candidate designs, either generated using a language model or proposed by the users. Next, users collaborate on the design process by providing feedback to an interactive genetic algorithm that selects, recombines, and mutates the most promising designs. We evaluated our framework on three game design tasks with human designers who collaborated remotely.",
            "year": 2023,
            "citationCount": 21,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A collaborative game design framework that combines interactive evolution and large language models to simulate the typical human design process and uses users' feedback for selecting the most promising ideas and largelanguage models for a very complex creative task---the recombination and variation of ideas."
            },
            "score": 2
        },
        {
            "id": "55dd09192d0b70fa2b2ff0959f93ce9f2d48b779",
            "paperId": "55dd09192d0b70fa2b2ff0959f93ce9f2d48b779",
            "title": "Automatic Robotic Development through Collaborative Framework by Large Language Models",
            "abstract": "Despite the remarkable code generation abilities of large language models (LLMs), they still face challenges in complex task handling. Robot development, a highly intricate field, inherently demands human involvement in task allocation and collaborative teamwork[1]. To enhance robot development, we propose an innovative automated collaboration framework inspired by real-world robot developers. This framework employs multiple LLMs in distinct roles-analysts, programmers, and testers. Analysts delve deep into user requirements, enabling programmers to produce precise code, while testers fine-tune the parameters based on user feedback for practical robot application. Each LLM tackles diverse, critical tasks within the development process. Clear collaboration rules emulate real-world teamwork among LLMs. Analysts, programmers, and testers form a cohesive team overseeing strategy, code, and parameter adjustments [2]. Through this framework, we achieve complex robot development without requiring specialized knowledge, relying solely on non-experts' participation.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Through this framework, complex robot development without requiring specialized knowledge is achieved without requiring specialized knowledge, relying solely on non-experts' participation."
            },
            "score": 2
        },
        {
            "id": "58ba42ffc34dd24d6a77fe58c8973b3533c369cd",
            "paperId": "58ba42ffc34dd24d6a77fe58c8973b3533c369cd",
            "title": "Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation From Deductive, Inductive and Abductive Views",
            "abstract": "Large Language Models (LLMs) have achieved great success in various natural language tasks. It has aroused much interest in evaluating the specific reasoning capability of LLMs, such as multilingual reasoning and mathematical reasoning. However, as one of the key reasoning perspectives, logical reasoning capability has not yet been thoroughly evaluated. In this work, we aim to bridge those gaps and provide comprehensive evaluations. Firstly, to offer systematic evaluations, this paper selects fifteen typical logical reasoning datasets and organizes them into deductive, inductive, abductive and mixed-form reasoning settings. Considering the comprehensiveness of evaluations, we include three representative LLMs (i.e., text-davinci-003, ChatGPT and BARD) and evaluate them on all selected datasets under zero-shot, one-shot and three-shot settings. Secondly, different from previous evaluations relying only on simple metrics (e.g., accuracy), we propose fine-level evaluations from objective and subjective manners, covering both answers and explanations. Also, to uncover the logical flaws of LLMs, bad cases will be attributed to five error types from two dimensions Evidence Selection Process and Reasoning Process . The former one includes evidence selection error and hallucination , while the latter one includes no reasoning , mistakes of reasoning perspectives and mistakes during reasoning process . Thirdly, to avoid the influences of knowledge bias and purely focus on benchmarking the logical reasoning capability of LLMs, we propose a new dataset with neutral content. It contains 3K samples and covers deductive, inductive and abductive reasoning settings. Based on the in-depth evaluations, this paper finally concludes the ability maps of logical reasoning capability from six dimensions (i.e., correct, rigorous, self-aware, active, oriented and no hallucination). It reflects the pros and cons of LLMs and gives guiding directions for future works.",
            "year": 2023,
            "citationCount": 21,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The ability maps of logical reasoning capability from six dimensions is concluded, which reflects the pros and cons of LLMs and gives guiding directions for future works."
            },
            "score": 2
        },
        {
            "id": "074d3c9c0986a7a5f2c68dc1e200516324610406",
            "paperId": "074d3c9c0986a7a5f2c68dc1e200516324610406",
            "title": "Testudo: Collaborative Intelligence for Latency-Critical Autonomous Systems",
            "abstract": "Edge computing is to be widely adopted for autonomous systems (ASs) applications as compute-intensive processing tasks can be offloaded to compute-capable servers located at the edge of the network infrastructure. Given the critical nature of numerous AS applications, their tasks are mostly governed by strict execution deadlines to alleviate any safety concerns from delayed responses. Although wireless link uncertainty has prompted recent works to designate redundant local execution as an offloading fail-safe to ensure these deadlines are met, frequent invocation of such fail-safe mechanisms can potentially undermine the extent of performance gains from offloading. In this article, we thoroughly analyze how redundant execution overheads can influence the overall performance. Then, we present TESTUDO, a methodology to optimize the energy consumption for latency-sensitive AS applications employing collaborative edge computing. Primarily, our methodology encompasses two main stages: 1) designing processing pipelines supporting optimal offloading points and fail-safe integration using modular design techniques and 2) developing a context-aware adaptive runtime solution based on deep reinforcement learning to adapt the mode of operation according to the wireless network status. Our experiments for end-to-end control and object detection use-cases have shown that TESTUDO achieved energy gains reaching up to 31% and 13.4% (15.9% and 5.3% on average) for the former and latter, respectively, while incurring little-to-no degradation in prediction scores (< 1% change) from state-of-the-art strategies.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "TESTUDO is presented, a methodology to optimize the energy consumption for latency-sensitive AS applications employing collaborative edge computing and develops a context-aware adaptive runtime solution based on deep reinforcement learning to adapt the mode of operation according to the wireless network status."
            },
            "score": 2
        },
        {
            "id": "93168612a0ff342b77ff6e02ca69439f568faef6",
            "paperId": "93168612a0ff342b77ff6e02ca69439f568faef6",
            "title": "Caution as a Response to Scientific Uncertainty: A Groundwater Game Experiment",
            "abstract": "Understanding and managing uncertainty is critical for robust governance. In groundwater management, where collaborative, community-based governance is increasingly common, scientific uncertainty about hydrological conditions could pose challenges to effective and equitable resource management. This study bridges two literatures \u2013 collaborative governance and collective action \u2013 to examine whether scientific uncertainty about hydrologic conditions undermines the performance of groups that engage in collaborative governance of shared groundwater resources. We conducted a modified groundwater game experiment, based on Meinzen-Dick et al. (2016), where participants engage as resource users in a crop choice game over multiple rounds. But unlike the original game, where participants had full information about recharge rate, two treatments introduced scientific uncertainty in water recharge: uncertainty framed as a range of estimates about groundwater recharge, and uncertainty framed as competing hydrological models predicting different groundwater recharge rates. We also expand on the original game by exploring a wider range of outcomes that include not only sustainable resource use but also group earning and equitable distribution of earnings across players. Analyzing data from 30 group games, our findings suggest that scientific uncertainty can help safeguard shared groundwater resources by prompting users to exercise caution in the face of uncertain recharge rates. This effect was more consistent for the range of estimates treatment than for the competing hydrological models treatment. To unpack the mechanisms behind the experimental result, we also analyzed participants\u2019 communications during the game to understand the strategies that collaborative groups use to cope with uncertainty. In the presence of scientific uncertainty, collaborative processes foster cautious behavior and protect shared resources.",
            "year": 2024,
            "citationCount": 0,
            "tldr": null,
            "score": 2
        },
        {
            "id": "eab7e876257923cd84339b857902501eb81c0b91",
            "paperId": "eab7e876257923cd84339b857902501eb81c0b91",
            "title": "Innovative Personal Assistance: Speech Recognition and NLP-Driven Robot Prototype",
            "abstract": "This paper presents the development and evaluation of a personal assistant robot prototype with advanced speech recognition and natural language processing (NLP) capabilities. Powered by a Raspberry Pi microprocessor, it is the core component of the robot's hardware. It is designed to receive commands and promptly respond by performing the requested actions, utilizing integrated speech recognition and NLP technologies. The prototype aims to enhance meeting efficiency and productivity through audio-to-text conversion and high-quality image capture. Results show excellent performance, with accuracy rates of 100% in Indonesian and 99% in English. The efficient processing speed, averaging 9.07 seconds per minute in Indonesian and 15.3 seconds per minute in English, further enhances the robot's functionality. Additionally, integrating a high-resolution webcam enables high-quality image capture at 1280 x 720 pixels. Real-time integration with Google Drive ensures secure storage and seamless data management. The findings highlight the prototype's effectiveness in facilitating smooth interactions and effective communication, leveraging NLP for intelligent language understanding. Integrating NLP-based speech recognition, visual documentation, and data transfer provides a comprehensive platform for managing audio, text, and image data. The personal assistant robot prototype presented in this research represents a significant advancement in human-robot interaction, particularly in meeting and collaborative work settings. Further refinements in NLP can enhance efficiency and foster seamless human-robot interaction experiences.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The personal assistant robot prototype presented in this research represents a significant advancement in human-robot interaction, particularly in meeting and collaborative work settings, as well as a comprehensive platform for managing audio, text, and image data."
            },
            "score": 2
        },
        {
            "id": "3f8a71cf290a1c981e46991997f3351fd995585d",
            "paperId": "3f8a71cf290a1c981e46991997f3351fd995585d",
            "title": "A handheld classroom dashboard: Teachers\u2019 perspectives on the use of real-time collaborative learning analytics",
            "abstract": null,
            "year": 2019,
            "citationCount": 20,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The teachers\u2019 perspectives discussed in this paper portray the promises and challenges of introducing new technologies aimed at enhancing orchestration and awareness in a CSCL classroom."
            },
            "score": 2
        },
        {
            "id": "39b06aa2aec66468ce30fdf58c27017f1a019f18",
            "paperId": "39b06aa2aec66468ce30fdf58c27017f1a019f18",
            "title": "Using Photographs and Memory-Work to Engage Novice Teachers in Collaborative Learning About Their Influence on Learner Behaviour",
            "abstract": "This article offers an account of using photographs and memory-work as a visual participative method in research conducted by a deputy principal with novice teachers in a South African primary school. The study was prompted by observations of how novice teachers struggled to manage learner behaviour in socially just and compassionate ways. It aimed to help novice teachers express the uncertainties and challenges they encounter, and prompt candid discussions on learner behaviour. The article shows how visual participative methods can facilitate collaborative learning with novice teachers. Additionally, it illustrates how the novice teachers came to see their critical role in influencing learner behaviour and the value of positive teacher-learner relationships in supporting learner behaviour. This work will be valuable to educational researchers in diverse contexts interested in growing their participative research methods repertoire. Furthermore, it illustrates how working with photographs and memory-work can facilitate the expression of participants' viewpoints and understandings and intensify educational researchers' learning from and with others in the interests of social change.",
            "year": 2021,
            "citationCount": 1,
            "tldr": null,
            "score": 2
        },
        {
            "id": "2ee0cec01fcd2eea74b5ca6f4732fa4a33871036",
            "paperId": "2ee0cec01fcd2eea74b5ca6f4732fa4a33871036",
            "title": "Verbs in Action: Improving verb understanding in video-language models",
            "abstract": "Understanding verbs is crucial to modelling how people and objects interact with each other and the environment through space and time. Recently, state-of-the-art video-language models based on CLIP have been shown to have limited verb understanding and to rely extensively on nouns, restricting their performance in real-world video applications that require action and temporal understanding. In this work, we improve verb understanding for CLIP-based video-language models by proposing a new Verb-Focused Contrastive (VFC) framework. This consists of two main components: (1) leveraging pretrained large language models (LLMs) to create hard negatives for cross-modal contrastive learning, together with a calibration strategy to balance the occurrence of concepts in positive and negative pairs; and (2) enforcing a fine-grained, verb phrase alignment loss. Our method achieves state-of-the-art results for zero-shot performance on three downstream tasks that focus on verb understanding, including video-text matching, video question-answering and video classification; while maintaining performance on noun-focused settings. To the best of our knowledge, this is the first work which proposes a method to alleviate the verb understanding problem, and does not simply highlight it. Our code is publicly available at [16] : scenic/projects/verbs_in_action.",
            "year": 2023,
            "citationCount": 29,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work improves verb understanding for CLIP-based video-language models by proposing a new Verb-Focused Contrastive (VFC) framework, and is the first work which proposes a method to alleviate the verb understanding problem, and does not simply highlight it."
            },
            "score": 1
        },
        {
            "id": "d2dedabc2f54ec9aa5a3b39f6066054afb971855",
            "paperId": "d2dedabc2f54ec9aa5a3b39f6066054afb971855",
            "title": "Never-Ending Learning for Explainable Brain Computing.",
            "abstract": "Exploring the nature of human intelligence and behavior is a longstanding pursuit in cognitive neuroscience, driven by the accumulation of knowledge, information, and data across various studies. However, achieving a unified and transparent interpretation of findings presents formidable challenges. In response, an explainable brain computing framework is proposed that employs the never-ending learning paradigm, integrating evidence combination and fusion computing within a Knowledge-Information-Data (KID) architecture. The framework supports continuous brain cognition investigation, utilizing joint knowledge-driven forward inference and data-driven reverse inference, bolstered by the pre-trained language modeling techniques and the human-in-the-loop mechanisms. In particular, it incorporates internal evidence learning through multi-task functional neuroimaging analyses and external evidence learning via topic modeling of published neuroimaging studies, all of which involve human interactions at different stages. Based on two case studies, the intricate uncertainty surrounding brain localization in human reasoning is revealed. The present study also highlights the potential of systematization to advance explainable brain computing, offering a finer-grained understanding of brain activity patterns related to human intelligence.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An explainable brain computing framework is proposed that employs the never-ending learning paradigm, integrating evidence combination and fusion computing within a Knowledge-Information-Data (KID) architecture that supports continuous brain cognition investigation."
            },
            "score": 1
        },
        {
            "id": "24a8461cf2163ce72433782839d7da074af4b40c",
            "paperId": "24a8461cf2163ce72433782839d7da074af4b40c",
            "title": "The Anthropomorphism of Intelligence",
            "abstract": "T A D 5 : 2 E D IT O R IA L The call for papers for this issue embraced the broad nature of intelligence, its multiple frameworks and its impact on fabrication, representation, and construction. The breadth of Intelligence was confirmed by the range of papers received, several of them examining systems that deal with the intersection of humans and machines, in physical and virtual realities. Research involving conventional and machine learning shared in this issue represents the interplay of intelligence with architecture and design throughout the design process into post occupancy. The essays display a shared interest in the potential of digital operations and intelligent systems\u2014aiming at improving, understanding, and creating better environments. The enumeration of terms, such as intelligent buildings and cities, smart materials, and autonomous agents is a direct projection of human anthropomorphic tendencies into our actions and outputs. Human perception, awareness, use patterns, and their feedback loop back into the built environment, personifying and individualizing systems. This issue of INTELLIGENCE is reflective of these tendencies and this dichotomy. The work demonstrates that our tools, technologies, and, ultimately, the environment we design, become a responsive and autonomous partner in our lives. This is evident in the contribution by Jeffrey Huang, Mikhael Johanes, Frederick Chando Kim, Christina Doumpioti, and GeorgChristoph Holz, connecting machine learning creativity (generative adversarial networks\u2014 GAN) with human verbal narration utilizing natural language processing (NLP). These authors go beyond data analysis and synthesize these generative qualities into computer-based creativity. This partnership is particularly encouraging since it not only provides opportunity for cultural contextualization of GANs but also enables, perhaps, the most human characteristic\u2014creativity\u2014into a broader spectrum of the physical matter. A similar conceptual interest in humans interfacing with the built environment is present in Eugene Han\u2019s research integrating eye tracking with visual simultaneous localization and mapping (VSLAM) techniques to capture and understand an individual\u2019s gaze within spatial environments at various scales. The presented framework not only transforms the established approach from screen-base to spatial analysis but also allows for less scripted and more spontaneous explorations of environments without predefined boundaries. Designing with humans in mind, particularly those more vulnerable, is the focus of the contribution by Yomna El-Ghazouly and Ahmed El Antably. The framework proposed by the authors utilizes digital human models (DHMs) to validate and design spaces that consider individual human characteristics including disabilities. This method demonstrates the promise of moving beyond often generalized ADA requirements and designs to create spaces that fit individual situations. Material-based research by Vasiliki Fragkia, Isak Worre Foged, and Anke Pasold combines computer vision (CV), machine learning, and predictive (data-driven) fabrication of behaviorallyand geometrically-complex natural materials with graded properties. Using algae and wood case studies, the research demonstrates the feasibility of the proposed predictive information modeling (PIM) framework in addressing material awareness and uncertainty, multi-scale data integration, and cyclical fabrication workflows. An affordance-based design evaluation process is used by Fauzan Alfi Agirachman and Michihiko Shinozaki, who compare the evaluation of design studio projects through a combination of affordances and virtual reality (VR) tools to assessment through nonvirtual reality (NVR) media. The findings document the important role the selection of media plays in how affordances are perceived, suggesting strengths and weaknesses that can inform future evaluation frameworks. Research around intelligence\u2014artificial and human\u2014discussed in this INTELLIGENCE issue revolves around dynamic processes; it questions established design approaches and combines work in progress with promises of more results in the near future. Pushing the boundaries of evaluation and prediction, work addressing intelligence might be naturally prone to involving anthropomorphism in search of deep and productive relationships with our places and technology. The Anthropomorphism of Intelligence",
            "year": 2021,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Research around intelligence\u2014artificial and human\u2014discussed in this INTELLIGENCE issue revolves around dynamic processes; it questions established design approaches and combines work in progress with promises of more results in the near future."
            },
            "score": 1
        },
        {
            "id": "d1a00c6502fee36d9691b3e4b213f8a70e253232",
            "paperId": "d1a00c6502fee36d9691b3e4b213f8a70e253232",
            "title": "Models and Applications of Chaos Theory in Modern Sciences",
            "abstract": "MODELS AND APPLICATIONS OF CHAOS THEORY IN MEDICINE A Chaotic View of Behavior Change: A Quantum Leap for Health Promotion: K. Resnicow and R. Vaughan Crisis and Chaos in Behavioral Nutrition and Physical Activity: T. Baranowski Memory in Astrocytes: A Hypothesis: R.M. Caudle Nonlinear Dynamics and Chaos in a Fractional-Order HIV Model: H. Ye and Y. Ding Research on the Relation of EEG Signal Chaos Characteristics with High-Level Intelligence Activity of Human Brain: X.Y. Wang, M. Juan, G.L.Tan and L.X. Zou Nonlinear Filtering of Oscillatory Measurements in Cardiovascular Applications: R.Vepa Temporal Redistribution of Plantar Pressure Points in the Healthy and Diabetics: A Time Series Analysis of the Neuro-Capillary Chaos: D.V. Oberoi, C. Pradhan, C.J. Kumar and S.C. D'Souza MODELS AND APPLICATIONS OF CHAOS THEORY IN BIOLOGY An Astrobiological Thought on Sustainable Life: T. Naganuma Matrix Frequency Analysis of Oryza Sativa (Japonica Cultivar-Group) Complete Genomes: K. Manikandakumar, S.M. Kumaran and R. Srikumar * The Capabilities of Chaos and Complexity: D.L. Abel MODELS AND APPLICATIONS OF CHAOS THEORY IN ECOLOGY A Unified Approach of Catastrophic Events: S. Nikolopoulos, P. Kapiris, K. Karamanos and K. Eftaxias Evolutionary Geomorphology: Thresholds and Nonlinearity in Landform Response to Environmental Change: J.D. Phillips MODELS AND APPLICATIONS OF CHAOS THEORY IN ECONOMICS Physics and the Foundations of Economic Science: Comments in Memory of Ilya Prigogine: Richard H. Day Neumannian Economy in Multi-Agent Approach Investigation of Stability and Instability in Economic Growth: Katalin Martinas Theoretical Aspects of the Economic Transition: Case of Romania: Cezar Scarlat and Eugen I. Scarlat Complex Dynamics in a Nonlinear Cobweb Model For Real Estate Market: Junhai Ma and Lingling Mu Chaos Models in Economics: Sorin Vlad, Paul Pascu and Nicolae Morariu Complex Dynamics of an Adnascent-type Game Model: Baogui Xin, Junhai Ma and Qin Gao A Production-Inventory Model for Deteriorating Items with Production Disruptions: Yong He and Ju He Nonlinear Noise Estimation in International Stock Markets: Coarse-Grained Entropy Method: Yong Fang MODELS AND APPLICATIONS OF CHAOS THEORY IN ELECTRONICS Simple Chaotic Oscillator: From Mathematical Model to Practical Experiment: J. Petrzela, Z. Kolka and S. Hanus A Robust Chaos-Based True Random Number Generator Embedded in Reconfigurable Switched-Capacitor Hardware: M. Drutarovsky and P. Galajda Conservative Chaos Generators with CCII+ Based on Mathematical Model of Nonlinear Oscillator: J. Petrzela and J. Slezak MODELS AND APPLICATIONS OF CHAOS THEORY IN THE HUMAN SCIENCES Ancient and Current Chaos Theories: G. Gunduz Complex Freedom: D. Penjak An Understanding of Language Development Models-Pidginization from the Perspective of Chaos Theory: G. Zhao Chaos and Natural Language Processing: M. Crisan Discrete Phase-Locked Loop Systems and Spreadsheets: S. Abramovich, E. Kudryashova, G.A. Leonov and S. Sugden Modeling Complex Spatial Dynamics of Two-Population Interaction in Urbanization Process: Y. Chen and F. Xu Characterizing Growth and Form of Fractal Cities with Allometric Scaling Exponents: Y. Chen Estimating the Distribution of Dynamic Invariants: Illustrated with an Application to Human Photo-Plethysmographic Time Series: M. Small Asymmetry, Symmetry and Beauty: H. Sabelli, A. Lawandow and A.R. Kopra Nonlinear Dynamics in Psychology: S.J. Guastello MODELS AND APPLICATIONS OF CHAOS THEORY IN THE MECHANICAL SCIENCES Chemical Reactivity Dynamics and Quantum Chaos in Highly Excited Hydrogen Atoms in an External Field: A Quantum Potential Approach: P.K. Chattaraj and B. Maiti Regular and Chaotic Motion of a Bush-Shaft System with Tribological Processes: J. Awrejcewicz and Y. Pyryev Relaxed Plasma Equilibria and Entropy-Related Plasma Self-Organiza Principles: R.L.Dewar, M.J. Hole, M. MeGann, R. Mills and S.R. Hodson Generalized Complexity and Classical-Quantum Transition: A.M. Kowalski, A. Plastino and M. Casas A New Mechanical Model for Particle Transport by Surface Waves and Applications: M. Ragulskis, E. Sakyte, J.M. Seoane and M.A.F. Sanjuan Chaotic Behavior of the Biharmonic Dynamics System: V.S. Aslanov Modeling, Chaotic Behavior and Control of Dissipation Properties of Hysteretic Systems: J. Awrejcewicz and L. Dzyubak Nonlinear Dynamics and Chaos of Microcantilever-Based TM-AFMs with Squeeze Film Damping Effects: W.M. Zhang , G. Meng, J.B. Zhou and J.Y. Chen Mathematical Identification of Homogenisation Processes in Argon Stirred Ladle: K. Michalek and K. Gryc Analysis of a Nonlinear Aeroelastic System with Parametric Uncertainties Using Polynomial Chaos Expansion: A. Desai and S. Sarkar Chaos Synchronization Criteria and Costs of Sinusoidally Coupled Horizontal Patform Systems: J. Cai, X. Wu and S. Chen",
            "year": 2011,
            "citationCount": 21,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This book discusses models and applications of Chaos Theory in medicine, economics, and electronics, as well as investigating the links between Chaos Theory and Sustainable Life, and the Foundations of Economic Science."
            },
            "score": 1
        },
        {
            "id": "1e3138d7a7b0a20981ebce1ae0aefc275ff7358d",
            "paperId": "1e3138d7a7b0a20981ebce1ae0aefc275ff7358d",
            "title": "Fusion of Qualitative Beliefs",
            "abstract": "This chapter introduces the notion of qualitative belief assignment to model beliefs of human experts expressed in natural language (with linguistic labels). We show how qualitative beliefs can be efficiently combined using an extension of Dezert-Smarandache Theory (DSmT) of plausible and paradoxical quantitative reasoning to qualitative reasoning. We propose a new arithmetic on linguistic labels which allows a direct extension of classical or hybrid DSm fusion rules. An approximate qualitative PCR5 rule is also proposed jointly with a Qualitative Average Operator. We also show how crisp or interval mappings can be used to deal indirectly with linguistic labels. A very simple example is provided to illustrate our qualitative fusion rules. 10.1 A brief historic of previous works Since fifteen years qualitative methods for reasoning under uncertainty developed in Artificial Intelligence are attracting more and more people of Information Fusion community, specially those working in the development of modern multi-source1 systems for defense. Their aim is to propose solutions for processing and combining qualitative information to take into account efficiently information provided by human sources (or semi-intelligent expert systems) and usually expressed in natural language rather than direct quantitative information. George Polya was one of the first mathematicians to attempt a formal characterization of qualitative human reasoning in 1954 [26, 27], then followed by Lofti Zadeh\u2019s works [40][45]. The interest of qualitative reasoning methods is to help in decision-making for situations in which the precise numerical methods are not appropriate (whenever the information/input are not directly expressed in numbers). Several formalisms for qualitative reasoning have been proposed as extensions on the frames of probability, possibility and/or evidence theories [2, 5, Where both computers, sensors and human experts are involved in the loop. 269 9, 12, 37, 39, 42, 45]. The limitations of numerical techniques are discussed in [22]. We browse here few main approaches. A detailed presentation of theses techniques can be found in [24]. 270 FUSION OF QUALITATIVE BELIEFS In [34], Wellman proposes a general characterization of qualitative probability to relax precision in representation and reasoning within the probabilistic framework. His basic idea was to develop Qualitative Probabilistic Networks (QPN) based on a Qualitative Probability Language (QPL) defined by a set of numerical underlying probability distributions. The major interest of QPL is to specify the partial rankings among degrees of belief rather than assessing their magnitudes on a cardinal scale. Such method cannot be considered as truly qualitative in our opinion, since it rather belongs to the family of imprecise probability [33] and probability bounds analysis (PBA) methods [11]. Some advances have been done by Darwiche in [5] for a symbolic generalization of Probability Theory; more precisely, Darwiche proposes a support (symbolic and/or numeric) structure which contains all information able to represent and conditionalize the state of belief. Darwiche shows that Probability Theory fits within his new support structure framework as several other theories, but Demspter-Shafer Theory doesn\u2019t fit in. Based on Demspter-Shafer Theory [29] (DST), Wong and Lingras [38] propose a method for generating a (numerical) basic belief functions from preference relations between each pair of propositions be specified qualitatively. The algorithm proposed doesn\u2019t provide however a unique solution and doesn\u2019t check the consistency of qualitative preference relations. Bryson and al. [4, 16] propose a procedure called Qualitative Discriminant Procedure (QDP) that involves qualitative scoring, imprecise pairwise comparisons between pairs of propositions and an optimization algorithm to generate consistent imprecise quantitative belief function to combine. Very recently, Ben Yaglane in [1] has reformulated the problem of generation of quantitative (consistent) belief functions from qualitative preference relations as a more general optimization problem under additional non linear constraints in order to minimize different uncertainty measures (Bezdek\u2019s entropy, Dubois & Prade non-specificity, etc). In [18, 19], Parsons proposes a qualitative Dempster-Shafer Theory, called Qualitative Evidence Theory (QET), by using techniques from qualitative reasoning [2]. Parsons\u2019 idea is to use qualitative belief assignments (qba), denoted here qm(.) assumed to be only 0 or +, where + means some unknown value between 0 and 1. Parsons proposes, using operation tables, a very simple arithmetic for qualitative addition + and multiplication \u00d7 operators. The combination of two (or more) qba\u2019s then actually follows the classical conjunctive consensus operator based on his qualitative multiplication table. Because of impossibility of qualitative normalization, Parsons uses the un-normalized version of Dempster\u2019s rule by committing a qualitative mass to the empty set following the open-world approach of Smets [32]. This approach cannot deal however with truly closed-world problems because there is no issue to transfer the conflicting qualitative mass or to normalize the qualitative belief assignments in the spirit of DST. An improved version of QET has been proposed [18] for using refined linguistic quantifiers as suggested by Dubois & Prade in [10]. The fusion of refined qualitative belief masses follows the un-normalized Dempster\u2019s rule based on an underlying numerical interval arithmetic associated with linguistic quantifiers. Actually, this refined QTE fits directly within DSmT framework since it corresponds to imprecise (quantitative) DSmC fusion rule [6, 30]. From 1995, Parsons seems to have switched back to qualitative probabilistic reasoning [23] and started to develop Qualitative Probabilistic Reasoner (QPR). Recently, Parsons discussed about the flaw discovered in QPR and gave some issues with new open questions [25]. In Zadeh\u2019s paradigm of computing with words (CW) [42][45] the combination of qualitative/vague information expressed in natural language is done essentially in three steps: 1) a 10.2. QUALITATIVE OPERATORS 271 translation of qualitative information into fuzzy membership functions, 2) a fuzzy combination of fuzzy membership functions; 3) a retranslation of fuzzy (quantitative) result into natural language. All these steps cannot be uniquely accomplished since they depend on the fuzzy operators chosen. A possible issue for the third step is proposed in [39]. In this chapter, we propose a simple arithmetic of linguistic labels which allows a direct extension of classical (quantitative) combination rules proposed in the DSmT framework into their qualitative counterpart. Qualitative beliefs assignments are well adapted for manipulated information expressed in natural language and usually reported by human expert or AI-based expert systems. In other words, we propose here a new method for computing directly with words (CW) and combining directly qualitative information Computing with words, more precisely computing with linguistic labels, is usually more vague, less precise than computing with numbers, but it is expected to offer a better robustness and flexibility for combining uncertain and conflicting human reports than computing with numbers because in most of cases human experts are less efficient to provide (and to justify) precise quantitative beliefs than qualitative beliefs. Before extending the quantitative DSmT-based combination rules to their qualitative counterparts, it will be necessary to define few but new important operators on linguistic labels and what is a qualitative belief assignment. Then we will show though simple examples how the combination of qualitative beliefs can be obtained in the DSmT framework. 10.2 Qualitative Operators We propose in this section a general arithmetic for computing with words (or linguistic labels). Computing with words (CW) and qualitative information is more vague, less precise than computing with numbers, but it offers the advantage of robustness if done correctly since : \u201d It would be a great mistake to suppose that vague knowledge must be false. On the contrary, a vague belief has a much better chance of being true than a precise one, because there are more possible facts that would verify it.\u201d \u2013 Bertrand Russell [28]. So let\u2019s consider a finite frame \u0398 = {\u03b81, . . . , \u03b8n} of n (exhaustive) elements \u03b8i, i = 1, 2, . . . , n, with an associated modelM(\u0398) on \u0398 (either Shafer\u2019s modelM0(\u0398), free-DSm modelMf (\u0398), or more general any Hybrid-DSm model [30]). A modelM(\u0398) is defined by the set of integrity constraints on elements of \u0398 (if any); Shafer\u2019s model M0(\u0398) assumes all elements of \u0398 truly exclusive, while free-DSm model Mf (\u0398) assumes no exclusivity constraints between elements of the frame \u0398. Let\u2019s define a finite set of linguistic labels L\u0303 = {L1, L2, . . . , Lm} where m \u2265 2 is an integer. L\u0303 is endowed with a total order relationship \u227a, so that L1 \u227a L2 \u227a . . . \u227a Lm. To work on a close linguistic set under linguistic addition and multiplication operators, we extends L\u0303 with two extreme values L0 and Lm+1 where L0 corresponds to the minimal qualitative value and Lm+1 corresponds to the maximal qualitative value, in such a way that L0 \u227a L1 \u227a L2 \u227a . . . \u227a Lm \u227a Lm+1 where \u227a means inferior to, or less (in quality) than, or smaller (in quality) than, etc. hence a relation of order from a qualitative point of view. But if we make a correspondence between qualitative labels and quantitative values on the scale [0, 1], then Lmin = L0 would correspond 272 FUSION OF QUALITATIVE BELIEFS to the numerical value 0, while Lmax = Lm+1 would correspond to the numerical value 1, and each Li would belong to [0, 1], i. e. Lmin = L0 < L1 < L2 < . . . < Lm < Lm+1 = Lmax From now on, we work on extended ordered set L of qualitative values L = {L0, L\u0303",
            "year": 2016,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This chapter introduces the notion of qualitative belief assignment to model beliefs of human experts expressed in natural language (with linguistic labels) and shows how qualitative beliefs can be efficiently combined using an extension of Dezert-Smarandache Theory (DSmT) of plausible and paradoxical quantitative reasoning to qualitative reasoning."
            },
            "score": 1
        },
        {
            "id": "be2710ffa0798a485ef3027798e676fdab07f5fb",
            "paperId": "be2710ffa0798a485ef3027798e676fdab07f5fb",
            "title": "Intersubjectivity and Intermediality in the Work of Serra",
            "abstract": "In her article \"Intersubjectivity and Intermediality in the Work of Serra\" Roc\u00edo von Jungenfeld examines the intersubjective space in which artworks are conceived and the cross boundaries of media in order to construct a general understanding of intersubjective perception in visual and plastic arts and an understanding of the processes that determine works of art, reflective perception, and intersubjective experience. Although the argument is that perception is subjective and untransferable, (i.e., a unique personal experience) influenced by innumerable factors and bound to a specific context, there are some elements of perception which can be understood intersubjectively as they apply to human beings in general. The aim of defining these elements of perception is to examine the intermedial nature of and the intersubjective components of works of art. Richard Serra's work has been selected for the implicitness of intermedial and intersubjective perceptual processes involved in the conceptualisation and materialisation of his artistic creations. Serra's artworks are complex entities with multilayered semantics, and so are the processes and the conceptual definitions of the media used in his creations. Roc\u00edo von Jungenfeld, \"Intersubjectivity and Intermediality in the Work of Serra\" page 2 of 7 CLCWeb: Comparative Literature and Culture 13.3 (2011): <http://docs.lib.purdue.edu/clcweb/vol13/iss3/24> Thematic issue New Perspectives on Material Culture and Intermedial Practice. Ed. Steven T\u00f6t\u00f6sy de Zepetnek, Asunci\u00f3n L\u00f3pez-Varela, Haun Saussy, and Jan Mieszkowski Roc\u00edo von JUNGENFELD Intersubjectivity and Intermediality in the Work of Serra The elements of intersubjectivity in visual and plastic arts are \"artwork, artist, and perceiver,\" and these are essential for the multidirectional communication process to take place (Derrida, Positions 23). Art \"mediates a dialogue, not between human beings and nature, but among persons in society\" (Ingold 351). The said elements are integral in the work of Richard Serra, as his sculptures and films are conceived in the context of process: \"the decision to work with 'processes' ... was also behind Serra's early films Hand Catching Lead (1968), Hands Tied (1968), Hands Scraping (1968), Hands Lead Fulcrum (1968), Frame (1969)\" (Buchloh 5). Serra's list of verbs of action, \"to roll, to crease, to fold, to store, to bend, to shorten\" is sculptural, graphical and time based, all at once (Buchloh 8-9), and these procedual elements locate Serra's work in the context of intermediality. In other words, through written language (i.e., list of verbs) the process, the action in which artist, artwork, and perceiver are participating is made apparent. Rosalind Krauss identifies in Serra's work this constant process as temporal but not narrative \"since there is no terminus, no proper destination\" (16). Serra's work does not produce growth, progress, or development: \"it is a time during which the action simply acts, and acts, and acts\" (Krauss 16). In the context of intersubjectivity I agree with Krauss that there is no destination or narrative and that the artwork acts and acts as an intermedial continuum, but somehow this continuous action produces in the perceiver internal growth, a change, so that it actually achieves something, to engage and to reflect, as Jacques Derrida proposes (Edmund Husserl's 56). Art results from the necessity to communicate and to translate concepts into something graspable within a particular context. The background of the artist influences the resulting creation and so does the context for which the work is produced. Serra's early work is influenced by site specificity and by the sociocultural context for which it is conceived. In a similar way to spoken and written language, in art the medium chosen to express, explain, or transmit concepts is fundamental to understanding the idiosyncrasies of a particular creation. Walter J. Ong suggests that \"human communication, verbal and other, differs from the 'medium' model most basically in that it demands anticipated feedback in order to take place at all\" (176). In the analysis of the language of art the term \"medium\" is used as it \"suggests that communication is a pipeline transfer of units of material called 'information' from one place to another\" (Ong 176). But, are the different visual and plastic languages of art merely pipelines and channels of transmission? The language of art, like any other language, is adapted to suite new situations and contexts and is reinvented and subverted (Lyotard 17). Following Jean-Fran\u00e7ois Lyotard's notion, I postulate that artists like Serra tend to push the boundaries of media. In Serra's Hand Catching Lead and Frame, the black and white film is used in conjunction with the framing technique to experiment and explore the medium, thereby creating a different language. In other words, medium and concept need to communicate with each other, but both elements can still be distinguished and analysed separately: the medium is analysed from within in the action in the process of communication. In Hand Catching Lead the pieces fall vertically and rhythmically reflecting the filmic medium, the action is perpetual and self-referential, and the medium catches 24 frames per second in a vertical continuum. In a similar way, Frame refers to the filmic medium by framing the scene, reducing and compressing the space to a mere window opening in front of the viewer (i.e., perceiver), and converting space and time to bi-dimensional animated images: \"At that time I was also making a sculpture called Base Plate Measure in which I was using measurement, it didn't seem to be very difficult to go from one measuring device in one material to another device in film\" (Serra qtd. in Michelson 32). Thus, in Serra's films the medium is not only a container, but also a participant in the process of conceptualization and materialization. The artist borrows from one medium and incorporates it into another, whereby the medium is not only the container, but part of the message as well because the medium is used as process, as a loop of reflected action. In relation to Marshall McLuhan's notion that \"the 'content' of a medium is always another medium\" (23) David Jay Bolter and Richard Grusin suggest that McLuhan conceived the medium as a collator of other media (45). The medium is not only container or means of transmission, but also carrier Roc\u00edo von Jungenfeld, \"Intersubjectivity and Intermediality in the Work of Serra\" page 3 of 7 CLCWeb: Comparative Literature and Culture 13.3 (2011): <http://docs.lib.purdue.edu/clcweb/vol13/iss3/24> Thematic issue New Perspectives on Material Culture and Intermedial Practice. Ed. Steven T\u00f6t\u00f6sy de Zepetnek, Asunci\u00f3n L\u00f3pez-Varela, Haun Saussy, and Jan Mieszkowski of meaning, and this is the case with Serra's films. In Serra's art the media is message and wrapper and the artwork is the product that facilitates sensorial possibilities which reflect meaning. An artwork mediating between artist and perceiver also incorporates other media and thus becomes a \"remediator\" (Bolter and Grusin 55), subverting and reinventing the language of media. Amongst artists this understanding of media and creative processes is intersubjective, as artists are aware of the constraints imposed by the media. Limitations of media are part of the process of translation and of the language of art itself. In Serra's work, the medium is selected for its inherent qualities to translate and recall the subtleties of the original idea in an almost metaphysical way. With regard to sculpture Serra claims that \"if you reduce sculpture to the flat place of a photograph, you're ... denying the temporal experience of the work ... reducing the sculpture to a different scale for the purposes of consumption ... denying the real content of the work\" (Foster 159). Serra conceives sculpture as a series of processes where the presence of the individual perceiver is essential for the communication to actually take place. As said, this understanding of media is intersubjective, for choosing a concrete technique or a specific medium that communicates what is intended is almost an innate exercise of tacit knowledge. In the context of verbal and written communication Ong postulates that \"communication is intersubjective,\" \"some recipient must be present,\" and that in the process of creation the artist is confronted by \"uncertainties\" about who the perceiver will be and by this \"fictionalization of the reader\" the process becomes more complex (177). When Serra uses industrial steel plates to create his processual sculptures, he chooses the specific material because of the \"fact that the technological process is revealed depersonalizes and demythologizes the idealization of the sculptor's craft\" (Serra qtd. in Crimp 158) but also because of the textural qualities the material itself transmits, as for example impenetrability, permanence, and stoutness. By manipulating the material Serra is able to challenge these qualities, bending the strict qualitative definition of the medium and incorporating lightness, elasticity, and mutability into the artwork. Hal Foster suggests that Serra's sculptural work \"is not given beforehand but must be forever proposed, tested, reworked, and proposed again\" (176). Thus the medium opens the possibility for the message to be transmitted from one individual to another in a process of intermediation. The message is the substance that gives body to the medium, but in the case of Serra also that which reflects upon and measures the medium itself. In visual and plastic arts, just like in verbal communication, the ability of an individual to communicate with others is dependent on and informed by the understanding of the concepts embedded in symbols and in the knowledge of the elements that represent the idea. In the context of sculpture but also in that of verbal language, let us imagine an abstract conc",
            "year": 2017,
            "citationCount": 0,
            "tldr": null,
            "score": 1
        },
        {
            "id": "4aa0d945ac8072ad463f36e35987506eec71f238",
            "paperId": "4aa0d945ac8072ad463f36e35987506eec71f238",
            "title": "Manufacturing processes are generally driven by single input, single output decoupled control loops. Position or angle measurements are related to motor references by analytic relationships, mainly",
            "abstract": "Manufacturing processes are generally driven by single input, single output decoupled control loops. Position or angle measurements are related to motor references by analytic relationships, mainly PID\u00b4s. Other magnitudes can be individually used like circuit breakers or alarm thresholds: electric current, temperature, force or torque, vibration (chattering), etc. Present work deals with tightly coupled multi-sensor multi-actuator systems that will probably constitute the next generation of manufacturing processes. New systems capable to make in real time adequate decisions are needed to treat with data coming from numerous and heterogeneous sensors by using spreaded sources of human knowledge ussually expressed in different languages. Complexity and uncertainty characterise this type of industrial environments in which Perception-Action loops must be established. New formal tools such as Fuzzy Logic and Pattern Recognition techniques constitute some approaches for process diagnosis and control. Heuristic modelling and empirical learning mechanisms can greatly extend the present landscape of the manufacturing world. The increasing processing power and the decreasing price and size of recent electronic devices have spread digital automation in manufacturing processes. Machine tools have four, five or more degrees of freedom in the movement of independent mechanical axis. Position and velocity control loops have angular and linear position encoders and/or tachometers as inputs, and voltage references to motors as outputs. A manufacturing planning to originate a piece in a particular machine implies the successive activation of defined control loops or elementary modules of machine behaviour in the C.N.C. programming language. Load first tool, go to initial position, set cutting speed, describe cutting trajectory, change to a new tool, etc. would be a typical program of machine operations. Decoupled linear control loops can be efficiently implemented in real time by analytic relationships such as Proportional-Integral-Derivative algorithms driven by the measured error in position or velocity. Fast interpolation techniques, look-up tables or distributed processing architectures have expanded greatly the capabilities of former C.N.C.\u00b4s to new machines and improved processes. Simultaneously, more and more sources of relevant information have become available. Initial fuses for overcurrent in electric feeding circuits and limit switches in mobile parts, has been complemented with operator protection devices, temperature, force, torque or vibration threshold alarms, incorrect feeding detection and many other devices to increase productivity and safety in manufacturing processes.",
            "year": null,
            "citationCount": 0,
            "tldr": null,
            "score": 1
        },
        {
            "id": "5dea206e2a36e672f197252bdd27d156d058f48c",
            "paperId": "5dea206e2a36e672f197252bdd27d156d058f48c",
            "title": "FinGPT: Open-Source Financial Large Language Models",
            "abstract": "Large language models (LLMs) have shown the potential of revolutionizing natural language processing tasks in diverse domains, sparking great interest in finance. Accessing high-quality financial data is the first challenge for financial LLMs (FinLLMs). While proprietary models like BloombergGPT have taken advantage of their unique data accumulation, such privileged access calls for an open-source alternative to democratize Internet-scale financial data. In this paper, we present an open-source large language model, FinGPT, for the finance sector. Unlike proprietary models, FinGPT takes a data-centric approach, providing researchers and practitioners with accessible and transparent resources to develop their FinLLMs. We highlight the importance of an automatic data curation pipeline and the lightweight low-rank adaptation technique in building FinGPT. Furthermore, we showcase several potential applications as stepping stones for users, such as robo-advising, algorithmic trading, and low-code development. Through collaborative efforts within the open-source AI4Finance community, FinGPT aims to stimulate innovation, democratize FinLLMs, and unlock new opportunities in open finance. Two associated code repos are \\url{https://github.com/AI4Finance-Foundation/FinGPT} and \\url{https://github.com/AI4Finance-Foundation/FinNLP}",
            "year": 2023,
            "citationCount": 71,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper highlights the importance of an automatic data curation pipeline and the lightweight low-rank adaptation technique in building FinGPT, and showcases several potential applications as stepping stones for users, such as robo-advising, algorithmic trading, and low-code development."
            },
            "score": 1
        },
        {
            "id": "836b9658eb81f321de90423b6259b07a398ca79b",
            "paperId": "836b9658eb81f321de90423b6259b07a398ca79b",
            "title": "CoLLiE: Collaborative Training of Large Language Models in an Efficient Way",
            "abstract": "Large language models (LLMs) are increasingly pivotal in a wide range of natural language processing tasks. Access to pre-trained models, courtesy of the open-source community, has made it possible to adapt these models to specific applications for enhanced performance. However, the substantial resources required for training these models necessitate efficient solutions. This paper introduces CoLLiE, an efficient library that facilitates collaborative training of large language models using 3D parallelism, parameter-efficient fine-tuning (PEFT) methods, and optimizers such as Lion, Adan, Sophia, LOMO and AdaLomo. With its modular design and comprehensive functionality, CoLLiE offers a balanced blend of efficiency, ease of use, and customization. CoLLiE has proven superior training efficiency in comparison with prevalent solutions in pre-training and fine-tuning scenarios. Furthermore, we provide an empirical evaluation of the correlation between model size and GPU memory consumption under different optimization methods, as well as an analysis of the throughput. Lastly, we carry out a comprehensive comparison of various optimizers and PEFT methods within the instruction-tuning context. CoLLiE is available at https://github.com/OpenLMLab/collie.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "CoLLiE is introduced, an efficient library that facilitates collaborative training of large language models using 3D parallelism, parameter-efficient fine-tuning (PEFT) methods, and optimizers such as Lion, Adan, Sophia, LOMO and AdaLomo and offers a balanced blend of efficiency, ease of use, and customization."
            },
            "score": 1
        },
        {
            "id": "5acbf917da5be89e4eebd7e98c81d87069450a5d",
            "paperId": "5acbf917da5be89e4eebd7e98c81d87069450a5d",
            "title": "Cloud-Device Collaborative Learning for Multimodal Large Language Models",
            "abstract": "The burgeoning field of Multimodal Large Language Models (MLLMs) has exhibited remarkable performance in diverse tasks such as captioning, commonsense reasoning, and visual scene understanding. However, the deployment of these large-scale MLLMs on client devices is hindered by their extensive model parameters, leading to a notable decline in generalization capabilities when these models are compressed for device deployment. Addressing this challenge, we introduce a Cloud-Device Collaborative Continual Adaptation framework, designed to enhance the performance of compressed, device-deployed MLLMs by leveraging the robust capabilities of cloud-based, larger-scale MLLMs. Our framework is structured into three key components: a device-to-cloud uplink for efficient data transmission, cloud-based knowledge adaptation, and an optimized cloud-to-device downlink for model deployment. In the uplink phase, we employ an Uncertainty-guided Token Sampling (UTS) strategy to effectively filter out-of-distribution tokens, thereby reducing transmission costs and improving training efficiency. On the cloud side, we propose Adapter-based Knowledge Distillation (AKD) method to transfer refined knowledge from large-scale to compressed, pocket-size MLLMs. Furthermore, we propose a Dynamic Weight update Compression (DWC) strategy for the downlink, which adaptively selects and quantizes updated weight parameters, enhancing transmission efficiency and reducing the representational disparity between cloud and device models. Extensive experiments on several multimodal benchmarks demonstrate the superiority of our proposed framework over prior Knowledge Distillation and device-cloud collaboration methods. Notably, we also validate the feasibility of our approach to real-world experiments.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a Cloud-Device Collaborative Continual Adaptation framework, designed to enhance the performance of compressed, device-deployed MLLMs by leveraging the robust capabilities of cloud-based, larger-scale MLLMs."
            },
            "score": 1
        },
        {
            "id": "c4affe78f824f8dc604201ad40cd00c381297d7c",
            "paperId": "c4affe78f824f8dc604201ad40cd00c381297d7c",
            "title": "Exploring the Upper Limits of Text-Based Collaborative Filtering Using Large Language Models: Discoveries and Insights",
            "abstract": "Text-based collaborative filtering (TCF) has become the mainstream approach for text and news recommendation, utilizing text encoders, also known as language models (LMs), to represent items. However, existing TCF models primarily focus on using small or medium-sized LMs. It remains uncertain what impact replacing the item encoder with one of the largest and most powerful LMs, such as the 175-billion parameter GPT-3 model, would have on recommendation performance. Can we expect unprecedented results? To this end, we conduct an extensive series of experiments aimed at exploring the performance limits of the TCF paradigm. Specifically, we increase the size of item encoders from one hundred million to one hundred billion to reveal the scaling limits of the TCF paradigm. We then examine whether these extremely large LMs could enable a universal item representation for the recommendation task. Furthermore, we compare the performance of the TCF paradigm utilizing the most powerful LMs to the currently dominant ID embedding-based paradigm and investigate the transferability of this TCF paradigm. Finally, we compare TCF with the recently popularized prompt-based recommendation using ChatGPT. Our research findings have not only yielded positive results but also uncovered some surprising and previously unknown negative outcomes, which can inspire deeper reflection and innovative thinking regarding text-based recommender systems. Codes and datasets will be released for further research.",
            "year": 2023,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An extensive series of experiments increases the size of item encoders from one hundred million to one hundred billion to reveal the scaling limits of the TCF paradigm and examines whether these extremely large LMs could enable a universal item representation for the recommendation task."
            },
            "score": 1
        },
        {
            "id": "9d2c9a7c3ff8f760b43fdd6a7082069a467c8956",
            "paperId": "9d2c9a7c3ff8f760b43fdd6a7082069a467c8956",
            "title": "Adapting Large Language Models by Integrating Collaborative Semantics for Recommendation",
            "abstract": "Recently, large language models (LLMs) have shown great potential in recommender systems, either improving existing recommendation models or serving as the backbone. However, there exists a large semantic gap between LLMs and recommender systems, since items to be recommended are often indexed by discrete identifiers (item ID) out of the LLM's vocabulary. In essence, LLMs capture language semantics while recommender systems imply collaborative semantics, making it difficult to sufficiently leverage the model capacity of LLMs for recommendation. To address this challenge, in this paper, we propose a new LLM-based recommendation model called LC-Rec, which can better integrate language and collaborative semantics for recommender systems. Our approach can directly generate items from the entire item set for recommendation, without relying on candidate items. Specifically, we make two major contributions in our approach. For item indexing, we design a learning-based vector quantization method with uniform semantic mapping, which can assign meaningful and non-conflicting IDs (called item indices) for items. For alignment tuning, we propose a series of specially designed tuning tasks to enhance the integration of collaborative semantics in LLMs. Our fine-tuning tasks enforce LLMs to deeply integrate language and collaborative semantics (characterized by the learned item indices), so as to achieve an effective adaptation to recommender systems. Extensive experiments demonstrate the effectiveness of our method, showing that our approach can outperform a number of competitive baselines including traditional recommenders and existing LLM-based recommenders. Our code is available at https://github.com/RUCAIBox/LC-Rec/.",
            "year": 2023,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new LLM-based recommendation model called LC-Rec, which can better integrate language and collaborative semantics for recommender systems and designs a learning-based vector quantization method with uniform semantic mapping for item indexing."
            },
            "score": 1
        },
        {
            "id": "2f9f52ad0f2a4bff37291a140e64431f6b318a2b",
            "paperId": "2f9f52ad0f2a4bff37291a140e64431f6b318a2b",
            "title": "In it together! Cultivating space for intergenerational dialogue, empathy and hope in a climate of uncertainty",
            "abstract": "ABSTRACT The urgent and interlocking social, economic and ecological crises faced by societies around the world require dialogue, empathy and above all, hope that transcends social divides. At a time of uncertainty and crisis, many societies are divided, with distrust and divides exacerbated by media representations pitting different groups against one another. Acknowledging intersectional interrelationships, this collaborative paper considers one type of social distinction \u2013 generation \u2013 and focuses on how trust can be rebuilt across generations. To do this, we collate key insights from eight projects that shared space within a conference session foregrounding creative, intergenerational responses to the climate and related crises. Prompted by a set of reflective questions, presenters commented on the methodological resources that were co-developed in intergenerational research and action spaces. Most of the work outlined was carried out in the UK, situated in challenges that are at once particular to local contexts, and systematic of a wider malaise that requires intergenerational collaboration. Reflecting across the projects, we suggest fostering ongoing, empathetic dialogues across generations is key to addressing these challenges of the future, securing communities that are grounded as collaborative and culturally responsive, and resilient societies able to adapt to and mitigate the impacts of change.",
            "year": 2022,
            "citationCount": 6,
            "tldr": null,
            "score": 1
        },
        {
            "id": "9fea5b0481fa16462989bc4b23dd59e5a65e7473",
            "paperId": "9fea5b0481fa16462989bc4b23dd59e5a65e7473",
            "title": "A global outlook to the interruption of education due to COVID-19 pandemic: Navigating in a time of uncertainty and crisis",
            "abstract": "Uncertain times require prompt reflexes to survive and this study is a collaborative reflex to better understand uncertainty and navigate through it. The Coronavirus (Covid-19) pandemic hit hard and interrupted many dimensions of our lives, particularly education. As a response to interruption of education due to the Covid-19 pandemic, this study is a collaborative reaction that narrates the overall view, reflections from the K-12 and higher educational landscape, lessons learned and suggestions from a total of 31 countries across the world with a representation of 62,7% of the whole world population. In addition to the value of each case by country, the synthesis of this research suggests that the current practices can be defined as emergency remote education and this practice is different from planned practices such as distance education, online learning or other derivations. Above all, this study points out how social injustice, inequity and the digital divide have been exacerbated during the pandemic and need unique and targeted measures if they are to be addressed. While there are support communities and mechanisms, parents are overburdened between regular daily/professional duties and emerging educational roles, and all parties are experiencing trauma, psychological pressure and anxiety to various degrees, which necessitates a pedagogy of care, affection and empathy. In terms of educational processes, the interruption of education signifies the importance of openness in education and highlights issues that should be taken into consideration such as using alternative assessment and evaluation methods as well as concerns about surveillance, ethics, and data privacy resulting from nearly exclusive dependency on online solutions.",
            "year": 2020,
            "citationCount": 608,
            "tldr": null,
            "score": 1
        },
        {
            "id": "45a201c8d31c09f3686257bff16ffc713af66f82",
            "paperId": "45a201c8d31c09f3686257bff16ffc713af66f82",
            "title": "Global Volcanic Hazards and Risk: Communities coping with uncertainty and reducing their risk: the collaborative monitoring and management of volcanic activity with the vig\u00edas of Tungurahua",
            "abstract": "(2015) Communities coping with uncertainty and reducing their risk: the collaborative monitoring and management of volcanic activity with the vig\u00edas of Tungurahua. In: Long-lived episodic volcanic eruptions share the risk characteristics of other forms of extensive hazard (such as flood, drought or landslides). They also have the capacity for escalations to high intensity, high impact events. Volc\u00e1n Tungurahua in the Ecuadorian Andes has been in eruption since 1999. The management of risk in areas surrounding the volcano has been facilitated by a network of community-based monitoring volunteers that has grown to fulfil multiple risk reduction roles in collaboration with the scientists and authorities. Renewed activity from Tungurahua (1999) prompted the evacuation, via Presidential Order, of the large tourist town of Ba\u00f1os and surrounding communities. Social unrest associated with the displacement and attendant loss of livelihood culminated in a forcible civil re-occupation of the land, crossing and overrunning military checkpoints (Le Pennec et al., 2012). This re-occupation prompted a radical rethink of management strategy around the volcanic hazard, shifting emphasis from enforcement to communication (Mothes et al., 2015). This enabled the community to continue their way of life alongside the volcano when it is relatively quiet and to prepare for and rapidly mobilise themselves during acute activity. To do this, a network of volunteers, formed from people already living in the communities at risk, was created with two main goals in mind: (i) to facilitate timely evacuations as part of the Civil Defence communication network, including the management of sirens, and (ii) to communicate observations about the volcano to the scientists (Stone et al., 2014). These volunteers are collectively referred to as 'vig\u00edas' and their input provides a pragmatic solution to the need for better monitoring observations and improved early warning systems when communities are living in relative proximity to the hazard. As a part of the solution, the communities feel strong ownership and involvement with the network (Stone et al., 2014). The communication pathways, formal and informal are shown in Figure 26.1.",
            "year": 2015,
            "citationCount": 1,
            "tldr": null,
            "score": 1
        },
        {
            "id": "7159b9e9835921fcbfda7ec9a93bccdd6cfebfdf",
            "paperId": "7159b9e9835921fcbfda7ec9a93bccdd6cfebfdf",
            "title": "Introduction: Earthshots from Haiti",
            "abstract": "Abstract: This long-awaited special issue on the environment is a collaborative effort involving many people, especially Lois Wilcken and Rose Elfman. Proposed after years of prompting by our visionary elder LeGrace Benson, this volume has unfolded during a period that has seen many dramatic and complicated changes in Haiti, across the world at large, and in the individual lives that are woven through and touched by these pages. We\u2019ve collectively been unsettled by the COVID pandemic, the political violence that has marred daily life and governance in Haiti and well beyond, and the uncertainty of a world struggling to look forward while wrestling with its messy past and present. While the urgency of\u00a0day-to-day\u00a0survival constricts the breath of so many, often making it impossible to see beyond immediate circumstances, the once seemingly far-off threats from anthropogenic global warming, climate change, and environmental destruction now loom mightily.",
            "year": 2023,
            "citationCount": 0,
            "tldr": null,
            "score": 1
        },
        {
            "id": "01c115fcfe0906e42d91810ad3068945bc0c4df8",
            "paperId": "01c115fcfe0906e42d91810ad3068945bc0c4df8",
            "title": "My mind is not the universe \u2013 where is the universe?",
            "abstract": "In order to describe my findings/conclusions systematically, a new semantic system (i.e., a new language) has to be intentionally defined by the present article. Humans are limited in what they know by the technical limitation of their cortical language network. A reality is a situation model (SM). For example, the conventionally-called \u201cphysical reality\u201d around my conventionally-called \u201cphysical body\u201d is actually a \u201cgeometric\u201d SM of my brain. Our universe is an autonomous objective parallel computing automaton (aka state machine) which evolves by itself automatically/unintentionally \u2013 wave-particle duality and Heisenberg\u2019s uncertainty principle can be explained under this \u201cfirst-order\u201d SM of my brain. Each elementary particle (as a building block of our universe) is an autonomous mathematical entity itself (i.e., a thing in itself). If we are happy to accept randomness, then it is obviously possible that all other worlds in the many-worlds interpretation do not exist objectively. The conventionally-called \u201cspace\u201d does not exist objectively. \u201cTime\u201d and \u201cmatter\u201d are not physical. Consciousness is the subjective-form (aka quale) of the mathematical models (of the objective universe) which are intracorporeally/subjectively used by the control logic of a Turing machine\u2019s program objectively-fatedly. A Turing machine\u2019s consciousness or deliberate decisions/choices should not be able to actually/objectively change/control/drive the (autonomous or objectively-fated) worldline of any elementary particle within this world. Besides the Schrodinger equation (or another mathematical equation/function which is yet to be discovered) which is a valid/correct/factual causality of our universe, every other causality (of our universe) is either invalid/incorrect/counterfactual or can be proved by deductive inference based on the Schrodinger equation (or the aforementioned yet-to-be-discovered mathematical equation/function) only. Consciousness plays no causal role (\u201cepiphenomenalism\u201d), or in other words, any cognitive/behavioural activity can in principle be carried out without consciousness (\u201cconscious inessentialism\u201d). If the \u201cloop quantum gravity\u201d theory is correct, then time/space does not actually/objectively exist in the objective-evolution of the objective-reality, or in other words, we should not use the subjective/mental concept of \u201ctime\u201d, \u201cstate\u201d or \u201cspace\u201d to describe/imagine the objective-evolution of our universe.",
            "year": 2021,
            "citationCount": 0,
            "tldr": null,
            "score": 0
        },
        {
            "id": "72d6c78bab7f0392b443f2841ccc63d67b9c6c61",
            "paperId": "72d6c78bab7f0392b443f2841ccc63d67b9c6c61",
            "title": "Knowledge Graph-Driven Conversational Agents",
            "abstract": "We present an approach to developing task-oriented conversational interfaces that addresses two key challenges that designers face: i) constructing a system grammar that strikes the right balance between the expressiveness necessary to carry out the task and the ability to correctly infer parses from natural language and ii) dealing with parse ambiguities. We address the latter of these with an approach to semantic parsing where the system constructs a semantic parse progressively, throughout the course of a multi-turn conversation in which the system\u2019s prompts to the user derive from parse uncertainty. We construct the system grammar by leveraging the structured types and entities of an underlying knowledge graph (KG) complemented by a machine learning (ML) driven restructuring procedure. The rationale for inducing the grammar from KG types stems from our perspective that the aim of a conversation in our system is to identify a single instance of an a priori unknown type in the KG. ML models provide suggestions for adding additional structure to the KG (and thus a re-structuring of the grammar). We provide results of an empirical study into the value of our human-in-the-loop knowledge graph restructuring algorithm showing that meaningful structure can be recovered by applying machine learning to existing data from two case studies in the insurance domain.",
            "year": 2019,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An approach to developing task-oriented conversational interfaces that addresses two key challenges that designers face by constructing a system grammar that strikes the right balance between the expressiveness necessary to carry out the task and the ability to correctly infer parses from natural language and dealing with parse ambiguities is presented."
            },
            "score": 0
        },
        {
            "id": "638fbfeb6d8e9b9518d84bd76361e8b7768de8c8",
            "paperId": "638fbfeb6d8e9b9518d84bd76361e8b7768de8c8",
            "title": "Emergence of Composite Services in Smart Environments",
            "abstract": "In recent years, Internet of Things and Cyber-Physical Systems have became a major asset, shaping almost all aspects of human life. Connected objects host physical (e.g., pollution sensing) or software (e.g., positioning on a map, itinerary planning) services. Thanks to artificial intelligence, these basic services can be automatically composed to offer composite smart services (e.g., guidance in a city) tailored to the user in her or his current situation. However, mobility and unpredictability of user needs and service\u2019s availability make the ambient environment highly unstable. In such contexts, providing working and relevant composite services is a challenging task: it demands middleware tools working despite uncertainty while maintaining the user in the loop. Therefore, we propose an innovative approach called \u201copportunistic composition\u201d: using a bottom-up approach, composite services are built, from available basic services and human-machine interaction fragments, to be presented to the user. In such a way, services emerge from the environment. Our contribution is twofold. The first aims to develop a context-aware distributed engine able to make adequate decisions at runtime about service composition, build and activate adapted composite services and their user-interface. Our engine is an adaptive multi-agent system, where agents learn about composition from experience and user feedback. The second contribution consists of presenting on the fly an emergent service to the user and let her or him modify and/or validate it. Hence, a model of the service must be provided in a user-friendly language depending on her or his profile and skills (e.g., pedestrian, bus driver), using domain specific languages and model-driven engineering. After modification and validation, the model is transformed into both executable re-composition commands and feedback information to supply agents\u2019 learning material.",
            "year": 2018,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A context-aware distributed engine able to make adequate decisions at runtime about service composition, build and activate adapted composite services and their user-interface and an adaptive multi-agent system, where agents learn about composition from experience and user feedback."
            },
            "score": 0
        }
    ],
    "novelty": "yes"
}