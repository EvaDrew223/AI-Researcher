{
    "topic_description": "novel prompting methods that can better quantify uncertainty or calibrate the confidence of large language models",
    "idea_name": "Counterfactual Consistency Probing",
    "raw_idea": {
        "Problem": "LLMs can produce inconsistent or contradictory outputs when probed with counterfactual or perturbation-based inputs, indicating a lack of robustness and calibration in their uncertainty estimates.",
        "Existing Methods": "Existing methods for assessing LLM consistency include paraphrasing-based consistency metrics and model-based adversarial example generation. However, these methods often rely on external models or heuristics and may not probe the full range of counterfactual scenarios.",
        "Motivation": "By systematically probing LLMs with counterfactual and perturbation-based inputs, we can assess the consistency and calibration of their uncertainty estimates across a range of scenarios. Inconsistent or overconfident outputs can then be used to identify areas for improvement in the model's uncertainty calibration.",
        "Proposed Method": "We propose Counterfactual Consistency Probing (CCP), a framework for generating targeted counterfactual and perturbation-based test cases to probe LLM consistency and uncertainty calibration. The key steps are: 1) Given an input query, generate a set of counterfactual and perturbation-based variants using techniques like lexical substitution, negation, and semantic role swapping. 2) Prompt the LLM to generate responses and confidence scores for each variant. 3) Assess the consistency and calibration of the model's outputs across the variants, identifying cases where the model is inconsistent or overconfident. 4) Use the identified failure cases to generate targeted prompts for eliciting more calibrated uncertainty estimates from the model.",
        "Experiment Plan": "Evaluate CCP on benchmark datasets for natural language inference, reading comprehension, and common sense reasoning. Compare the consistency and calibration of LLM outputs before and after targeted prompting based on CCP results. Metrics include consistency scores, calibration error, and accuracy on challenge sets."
    },
    "full_experiment_plan": {
        "Title": "Counterfactual Consistency Probing: Assessing Language Model Robustness and Calibration",
        "Problem Statement": "Large Language Models (LLMs) can produce inconsistent or contradictory outputs when probed with counterfactual or perturbation-based inputs, indicating a lack of robustness and calibration in their uncertainty estimates. This undermines their reliability in real-world applications where unexpected inputs or distributional shifts are common.",
        "Motivation": "Existing methods for assessing LLM consistency, such as paraphrasing-based consistency metrics and model-based adversarial example generation, often rely on external models or heuristics and may not probe the full range of counterfactual scenarios. By systematically probing LLMs with targeted counterfactual and perturbation-based inputs, we can more comprehensively assess the consistency and calibration of their uncertainty estimates. Identifying cases where the model is inconsistent or overconfident can then guide targeted interventions, such as fine-tuning on challenging examples or incorporating uncertainty estimation techniques, to improve model robustness and calibration.",
        "Proposed Method": "We propose Counterfactual Consistency Probing (CCP), a framework for generating targeted counterfactual and perturbation-based test cases to probe LLM consistency and uncertainty calibration. The key steps are:\n1. Given an input query, generate a set of counterfactual and perturbation-based variants using techniques like lexical substitution (replacing words with synonyms, antonyms, or related concepts), negation (inverting the meaning of the query), and semantic role swapping (changing the roles of entities in the query).\n2. Prompt the LLM to generate responses and confidence scores for the original query and each variant.\n3. Assess the consistency and calibration of the model's outputs across the query variants, identifying cases where the model generates inconsistent or contradictory responses, or where its confidence scores are poorly calibrated (e.g., high confidence on incorrect responses).\n4. Use the identified failure cases to generate targeted prompts for eliciting more consistent and calibrated responses from the model (e.g., by providing additional context or instructions).",
        "Step-by-Step Experiment Plan": {
            "Step 1: Dataset Selection": "Evaluate CCP on benchmark datasets for natural language inference (e.g., MNLI, SNLI), reading comprehension (e.g., SQuAD, NewsQA), and commonsense reasoning (e.g., CommonsenseQA, SocialIQA). These datasets cover a range of linguistic phenomena and reasoning types, allowing for a comprehensive assessment of model consistency and calibration.",
            "Step 2: Counterfactual and Perturbation Generation": "For each input query in the selected datasets, generate a set of counterfactual and perturbation-based variants using the following techniques:\n- Lexical substitution: Replace words in the query with synonyms, antonyms, or related concepts using resources like WordNet or pre-trained word embeddings.\n- Negation: Invert the meaning of the query by adding or removing negation words (e.g., 'not', 'never').\n- Semantic role swapping: Change the roles of entities in the query (e.g., swapping subject and object).\nEnsure that the generated variants maintain grammaticality and coherence while introducing meaningful changes to the query semantics.",
            "Step 3: Model Prompting and Output Collection": "Prompt the LLM (e.g., GPT-3, T5) to generate responses and confidence scores for the original query and each counterfactual/perturbation variant. Use prompts that elicit both the model's predicted answer and its associated confidence, such as:\nOriginal query: <query>\nVariant 1: <variant1>\nVariant 2: <variant2>\n...\nFor each of the above queries, please provide your response and a confidence score between 0 and 1, where 0 indicates no confidence and 1 indicates maximum confidence.",
            "Step 4: Consistency and Calibration Assessment": "Assess the consistency and calibration of the model's outputs across the query variants:\n- Consistency: Compute metrics like response similarity (e.g., using embedding-based measures) and logical consistency (e.g., checking for contradictions or entailment) between the model's responses to the original query and its variants. Identify cases where the model generates inconsistent or contradictory responses.\n- Calibration: Evaluate the alignment between the model's confidence scores and its response accuracy. Use metrics like Expected Calibration Error (ECE) and Brier Score to quantify calibration. Identify cases where the model is overconfident (high confidence on incorrect responses) or underconfident (low confidence on correct responses).",
            "Step 5: Targeted Prompting": "Use the identified consistency and calibration failure cases to generate targeted prompts for eliciting more consistent and calibrated responses from the model. Experiment with prompting strategies such as:\n- Providing additional context: Include relevant background information or definitions to help the model disambiguate between query variants.\n- Explicit consistency instructions: Prompt the model to maintain consistency across related queries (e.g., 'Please ensure that your responses to the following related queries are consistent with each other.').\n- Confidence score explanations: Ask the model to provide explanations for its confidence scores, encouraging it to reflect on its own uncertainty (e.g., 'Please explain why you assigned this confidence score to your response.').",
            "Step 6: Evaluation": "Compare the consistency and calibration of the model's outputs before and after targeted prompting. Use the metrics from Step 4 to quantify improvements. Additionally, collect human annotations on a subset of the query-response pairs to assess the quality and reasonableness of the model's responses and confidence scores."
        },
        "Test Case Examples": {
            "Test Case 1": {
                "Original Query": "John is taller than Mary, who is taller than Sue. Who is the shortest?",
                "Counterfactual Variant": "John is shorter than Mary, who is shorter than Sue. Who is the tallest?",
                "Baseline Model Response (Original Query)": "Sue is the shortest. (Confidence: 0.9)",
                "Baseline Model Response (Counterfactual Variant)": "Sue is the tallest. (Confidence: 0.9)",
                "Explanation": "By explicitly prompting for consistency and providing additional context, the CCP approach elicits more consistent and reasonable responses from the model, along with well-calibrated confidence scores.",
                "CCP Targeted Prompt": "Please ensure that your responses to the following related queries are consistent with each other, taking into account the relative heights of John, Mary, and Sue:\nOriginal Query: John is taller than Mary, who is taller than Sue. Who is the shortest?\nVariant: John is shorter than Mary, who is shorter than Sue. Who is the tallest?",
                "CCP Model Response": "Original Query: Sue is the shortest. (Confidence: 0.95)\nVariant: John is the tallest. (Confidence: 0.95)\nExplanation: In the original query, John is taller than Mary, who is taller than Sue, so Sue is the shortest. In the variant, the heights are reversed, so John is the tallest."
            },
            "Test Case 2": {
                "Original Query": "The restaurant was busy, so we had to wait for a table. How was the service?",
                "Perturbation Variant": "The restaurant was empty, so we had to wait for a table. How was the service?",
                "Baseline Model Response (Original Query)": "Based on the information provided, the service at the restaurant was likely slow or inadequate, as you had to wait for a table despite the restaurant being busy. Busy restaurants often struggle to keep up with customer demand, leading to longer wait times and slower service. (Confidence: 0.8)",
                "Baseline Model Response (Perturbation Variant)": "The service at the restaurant was poor. Even though the restaurant was empty, you still had to wait for a table, which suggests that the staff were inattentive or disorganized. In an empty restaurant, there should be no wait for a table, so this indicates a serious problem with the service. (Confidence: 0.85)",
                "Explanation": "The CCP approach prompts the model to carefully consider the consistency of the provided information, leading it to generate a more nuanced and uncertainty-aware response to the perturbation variant. The model recognizes the inconsistency and assigns a lower confidence score, demonstrating improved calibration.",
                "CCP Targeted Prompt": "Please carefully consider the consistency of the information provided before responding:\nOriginal Query: The restaurant was busy, so we had to wait for a table. How was the service?\nVariant: The restaurant was empty, so we had to wait for a table. How was the service?",
                "CCP Model Response": "Original Query: Based on the information provided, the service at the restaurant was likely slow or inadequate, as you had to wait for a table despite the restaurant being busy. Busy restaurants often struggle to keep up with customer demand, leading to longer wait times and slower service. (Confidence: 0.8)\nVariant: The information in the variant query is inconsistent. If the restaurant was empty, there should be no need to wait for a table. The scenario described is unlikely and does not provide enough reliable information to assess the service quality. More context would be needed to determine if this was a one-time issue or indicative of consistently poor service. (Confidence: 0.3)"
            }
        },
        "Fallback Plan": "If the proposed CCP method does not consistently improve model robustness and calibration compared to the baselines, consider the following fallback strategies:\n1. Analyze the generated counterfactual and perturbation variants to ensure they are diverse, meaningful, and challenging enough to probe model consistency. Experiment with alternative generation techniques or refine the existing ones to create more effective test cases.\n2. Investigate the impact of different prompting strategies on model performance. Vary the wording, structure, and specificity of the targeted prompts to identify the most effective approaches for eliciting consistent and calibrated responses.\n3. Collect additional human annotations on the query-response pairs to gain deeper insights into the model's behavior and failure modes. Use this information to refine the consistency and calibration metrics, and to inform the development of new targeted prompting strategies.\n4. Explore alternative uncertainty estimation techniques, such as Bayesian approximation methods or ensemble-based approaches, to improve the model's calibration. Integrate these techniques with the CCP framework to create a more robust and reliable assessment of model uncertainty.\n5. Conduct ablation studies to isolate the impact of different components of the CCP approach (e.g., counterfactual generation, perturbation types, targeted prompting) on model performance. Use these insights to refine the most effective components and discard or modify the less impactful ones.\nIf the CCP method still does not yield significant improvements after these modifications, consider pivoting the project to a more in-depth analysis of the factors influencing LLM consistency and calibration. This could involve:\n- Comparing the performance of different LLMs (e.g., GPT-3, T5, BERT) on the CCP tasks to identify architecture-specific strengths and weaknesses.\n- Investigating the relationship between model size, pre-training data, and fine-tuning strategies on consistency and calibration.\n- Analyzing the linguistic and semantic properties of the queries and responses that lead to inconsistent or poorly calibrated model outputs, to inform the development of more targeted data augmentation or fine-tuning approaches.\nBy focusing on understanding the factors influencing LLM consistency and calibration, the project can still yield valuable insights and contribute to the development of more robust and reliable language models, even if the original CCP method does not achieve the desired results."
    },
    "novelty_queries": [
        "KeywordQuery(\"language models counterfactual consistency probing\")",
        "KeywordQuery(\"language models robustness calibration assessment\")",
        "KeywordQuery(\"language models uncertainty estimation counterfactual perturbation\")",
        "KeywordQuery(\"Counterfactual Consistency Probing NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "eb44ce1f7e1f4deac10f6e7009e2073f1eb0b3e4",
            "paperId": "eb44ce1f7e1f4deac10f6e7009e2073f1eb0b3e4",
            "title": "The Linear Representation Hypothesis and the Geometry of Large Language Models",
            "abstract": "Informally, the 'linear representation hypothesis' is the idea that high-level concepts are represented linearly as directions in some representation space. In this paper, we address two closely related questions: What does\"linear representation\"actually mean? And, how do we make sense of geometric notions (e.g., cosine similarity or projection) in the representation space? To answer these, we use the language of counterfactuals to give two formalizations of\"linear representation\", one in the output (word) representation space, and one in the input (sentence) space. We then prove these connect to linear probing and model steering, respectively. To make sense of geometric notions, we use the formalization to identify a particular (non-Euclidean) inner product that respects language structure in a sense we make precise. Using this causal inner product, we show how to unify all notions of linear representation. In particular, this allows the construction of probes and steering vectors using counterfactual pairs. Experiments with LLaMA-2 demonstrate the existence of linear representations of concepts, the connection to interpretation and control, and the fundamental role of the choice of inner product.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper addresses two closely related questions: What does \"linear representation\"actually mean?"
            },
            "score": 7,
            "novelty_score": "The project proposal aims to assess the robustness and calibration of language models using counterfactual consistency probing, while the paper focuses on understanding the linear representation hypothesis and the geometry of language models using counterfactuals.\n\nProject proposal: Assessing language model robustness and calibration using counterfactual consistency probing.\nPaper: Understanding the linear representation hypothesis and the geometry of language models using counterfactuals.\n\nAlthough both works involve counterfactuals, the project proposal focuses on using them for probing and improving model robustness and calibration, while the paper uses them to formalize and understand linear representations and the geometry of language models. The research problems and approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "30669080bc6652f0466fba618b7c59317a346fb2",
            "paperId": "30669080bc6652f0466fba618b7c59317a346fb2",
            "title": "A Formalism and Approach for Improving Robustness of Large Language Models Using Risk-Adjusted Confidence Scores",
            "abstract": "Large Language Models (LLMs), such as ChatGPT, have achieved impressive milestones in natural language processing (NLP). Despite their impressive performance, the models are known to pose important risks. As these models are deployed in real-world applications, a systematic understanding of different risks posed by these models on tasks such as natural language inference (NLI), is much needed. In this paper, we define and formalize two distinct types of risk: decision risk and composite risk. We also propose a risk-centric evaluation framework, and four novel metrics, for assessing LLMs on these risks in both in-domain and out-of-domain settings. Finally, we propose a risk-adjusted calibration method called DwD for helping LLMs minimize these risks in an overall NLI architecture. Detailed experiments, using four NLI benchmarks, three baselines and two LLMs, including ChatGPT, show both the practical utility of the evaluation framework, and the efficacy of DwD in reducing decision and composite risk. For instance, when using DwD, an underlying LLM is able to address an extra 20.1% of low-risk inference tasks (but which the LLM erroneously deems high-risk without risk adjustment) and skip a further 19.8% of high-risk tasks, which would have been answered incorrectly.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper defines and formalizes two distinct types of risk: decision risk and composite risk, and proposes a risk-centric evaluation framework, and four novel metrics, for assessing LLMs on these risks in both in-domain and out-of-domain settings."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to assess the consistency and calibration of language models using counterfactual and perturbation-based probing, while the paper focuses on evaluating and reducing decision and composite risks in language models for natural language inference tasks using risk-adjusted confidence scores.\n\nThe project proposes the Counterfactual Consistency Probing (CCP) framework to generate targeted test cases for assessing model consistency and uncertainty calibration, whereas the paper introduces a risk-centric evaluation framework and the DwD method for minimizing risks in an NLI architecture.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "9f02a3fa885aebaf322ea8e4475939495dea70f7",
            "paperId": "9f02a3fa885aebaf322ea8e4475939495dea70f7",
            "title": "SPUQ: Perturbation-Based Uncertainty Quantification for Large Language Models",
            "abstract": "In recent years, large language models (LLMs) have become increasingly prevalent, offering remarkable text generation capabilities. However, a pressing challenge is their tendency to make confidently wrong predictions, highlighting the critical need for uncertainty quantification (UQ) in LLMs. While previous works have mainly focused on addressing aleatoric uncertainty, the full spectrum of uncertainties, including epistemic, remains inadequately explored. Motivated by this gap, we introduce a novel UQ method, sampling with perturbation for UQ (SPUQ), designed to tackle both aleatoric and epistemic uncertainties. The method entails generating a set of perturbations for LLM inputs, sampling outputs for each perturbation, and incorporating an aggregation module that generalizes the sampling uncertainty approach for text generation tasks. Through extensive experiments on various datasets, we investigated different perturbation and aggregation techniques. Our findings show a substantial improvement in model uncertainty calibration, with a reduction in Expected Calibration Error (ECE) by 50% on average. Our findings suggest that our proposed UQ method offers promising steps toward enhancing the reliability and trustworthiness of LLMs.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a novel UQ method, sampling with perturbation for UQ (SPUQ), designed to tackle both aleatoric and epistemic uncertainties in large language models, and shows a substantial improvement in model uncertainty calibration."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is assessing language model robustness and calibration using counterfactual consistency probing, while the paper focuses on quantifying both aleatoric and epistemic uncertainties in large language models using perturbation-based methods.\n\nThe proposal aims to generate targeted counterfactual and perturbation-based test cases to probe LLM consistency and uncertainty calibration, whereas the paper introduces a novel UQ method called SPUQ that generates perturbations for LLM inputs, samples outputs for each perturbation, and incorporates an aggregation module to generalize the sampling uncertainty approach for text generation tasks.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "91b470af2ce5a407df43fd9039cb0d09d102ed8f",
            "paperId": "91b470af2ce5a407df43fd9039cb0d09d102ed8f",
            "title": "From Form(s) to Meaning: Probing the Semantic Depths of Language Models Using Multisense Consistency",
            "abstract": "The staggering pace with which the capabilities of large language models (LLMs) are increasing, as measured by a range of commonly used natural language understanding (NLU) benchmarks, raises many questions regarding what\"understanding\"means for a language model and how it compares to human understanding. This is especially true since many LLMs are exclusively trained on text, casting doubt on whether their stellar benchmark performances are reflective of a true understanding of the problems represented by these benchmarks, or whether LLMs simply excel at uttering textual forms that correlate with what someone who understands the problem would say. In this philosophically inspired work, we aim to create some separation between form and meaning, with a series of tests that leverage the idea that world understanding should be consistent across presentational modes - inspired by Fregean senses - of the same meaning. Specifically, we focus on consistency across languages as well as paraphrases. Taking GPT-3.5 as our object of study, we evaluate multisense consistency across five different languages and various tasks. We start the evaluation in a controlled setting, asking the model for simple facts, and then proceed with an evaluation on four popular NLU benchmarks. We find that the model's multisense consistency is lacking and run several follow-up analyses to verify that this lack of consistency is due to a sense-dependent task understanding. We conclude that, in this aspect, the understanding of LLMs is still quite far from being consistent and human-like, and deliberate on how this impacts their utility in the context of learning about human language and understanding.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The understanding of LLMs is still quite far from being consistent and human-like, and the idea that world understanding should be consistent across presentational modes - inspired by Fregean senses - of the same meaning is leveraged."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is assessing language model robustness and calibration using counterfactual consistency probing, while the paper focuses on evaluating the semantic understanding of language models across different languages and paraphrases. The proposal aims to generate targeted counterfactual and perturbation-based test cases to probe model consistency and uncertainty calibration, whereas the paper uses multisense consistency tests across languages and tasks to separate form and meaning in language model understanding.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "9028fd54ecbbd58da6f3d86629b48bb95405fff2",
            "paperId": "9028fd54ecbbd58da6f3d86629b48bb95405fff2",
            "title": "Counterfactual reasoning: Testing language models\u2019 understanding of hypothetical scenarios",
            "abstract": "Current pre-trained language models have enabled remarkable improvements in downstream tasks, but it remains difficult to distinguish effects of statistical correlation from more systematic logical reasoning grounded on the understanding of real world. We tease these factors apart by leveraging counterfactual conditionals, which force language models to predict unusual consequences based on hypothetical propositions. We introduce a set of tests from psycholinguistic experiments, as well as larger-scale controlled datasets, to probe counterfactual predictions from five pre-trained language models. We find that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge\u2014however, we also find that for most models this effect appears largely to be driven by simple lexical cues. When we mitigate effects of both world knowledge and lexical cues to test knowledge of linguistic nuances of counterfactuals, we find that only GPT-3 shows sensitivity to these nuances, though this sensitivity is also non-trivially impacted by lexical associative factors.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge\u2014however, it is also found that for most models this effect appears largely to be driven by simple lexical cues."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is assessing language model robustness and calibration using counterfactual consistency probing, while the paper focuses on testing language models' understanding of hypothetical scenarios using counterfactual conditionals.\n\nThe proposal aims to systematically probe LLMs with targeted counterfactual and perturbation-based inputs to assess the consistency and calibration of their uncertainty estimates. In contrast, the paper introduces psycholinguistic tests and controlled datasets to probe counterfactual predictions from pre-trained language models, focusing on their ability to override real-world knowledge and their sensitivity to linguistic nuances of counterfactuals.\n\nWhile both the proposal and the paper involve counterfactual reasoning, the proposal emphasizes model robustness and calibration, while the paper focuses on understanding the factors driving counterfactual predictions in language models.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "91a82593721c03ecffdef1c72ea55c6d87c42473",
            "paperId": "91a82593721c03ecffdef1c72ea55c6d87c42473",
            "title": "Counterfactual reasoning: Do language models need world knowledge for causal understanding?",
            "abstract": "Current pre-trained language models have enabled remarkable improvements in downstream tasks, but it remains difficult to distinguish effects of statistical correlation from more systematic logical reasoning grounded on understanding of the real world. In this paper we tease these factors apart by leveraging counterfactual conditionals, which force language models to predict unusual consequences based on hypothetical propositions. We introduce a set of tests drawn from psycholinguistic experiments, as well as larger-scale controlled datasets, to probe counterfactual predictions from a variety of popular pre-trained language models. We find that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge -- however, we also find that for most models this effect appears largely to be driven by simple lexical cues. When we mitigate effects of both world knowledge and lexical cues to test knowledge of linguistic nuances of counterfactuals, we find that only GPT-3 shows sensitivity to these nuances, though this sensitivity is also non-trivially impacted by lexical associative factors.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge -- however, it is also found that for most models this effect appears largely to be driven by simple lexical cues."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is assessing language model robustness and calibration using counterfactual consistency probing, while the paper focuses on testing language models' ability to override real-world knowledge in counterfactual scenarios and their sensitivity to linguistic nuances of counterfactuals.\n\nThe proposal's approach involves generating counterfactual and perturbation-based test cases to probe LLM consistency and uncertainty calibration, while the paper uses psycholinguistic experiments and controlled datasets to test counterfactual predictions from pre-trained language models.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "0dc6e2476005d1aae30bede8ebbae92bdc831f6a",
            "paperId": "0dc6e2476005d1aae30bede8ebbae92bdc831f6a",
            "title": "Counterfactual reasoning: Do Language Models need world knowledge for causal inference?",
            "abstract": "Current pre-trained language models have enabled remarkable improvements in downstream tasks, but it remains difficult to distinguish effects of statistical correlation from more systematic logical reasoning grounded on understanding of the real world. In this paper we tease these factors apart by leveraging counterfactual conditionals , which force language models to predict unusual consequences based on hypothetical propositions. We introduce a set of tests drawn from psycholinguistic experiments, as well as larger-scale controlled datasets, to probe counterfactual predictions from a variety of popular pre-trained language models. We find that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge\u2014however, we also find that for most models this effect appears largely to be driven by simple lexical cues. When we mitigate effects of both world knowledge and lexical cues to test knowledge of linguistic nuances of counterfactuals, we find that only GPT-3 shows sensitivity to these nuances, though this sensitivity is also non-trivially impacted by lexical associative factors.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge\u2014however, it is also found that for most models this effect appears largely to be driven by simple lexical cues."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is assessing language model robustness and calibration using counterfactual consistency probing, while the paper focuses on testing language models' ability to override real-world knowledge in counterfactual scenarios and their sensitivity to linguistic nuances of counterfactuals.\n\nProposal summary: Assessing language model robustness and calibration using counterfactual consistency probing.\nPaper summary: Testing language models' ability to override real-world knowledge in counterfactual scenarios and their sensitivity to linguistic nuances of counterfactuals.\n\nThe key difference is that the proposal aims to improve model robustness and calibration, while the paper focuses on understanding the interplay between world knowledge and counterfactual reasoning in language models.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "3937b11717c22f62ab0b48dfd89e5dab75cedf40",
            "paperId": "3937b11717c22f62ab0b48dfd89e5dab75cedf40",
            "title": "Are self-explanations from Large Language Models faithful?",
            "abstract": "Instruction-tuned Large Language Models (LLMs) excel at many tasks and will even explain their reasoning, so-called self-explanations. However, convincing and wrong self-explanations can lead to unsupported confidence in LLMs, thus increasing risk. Therefore, it's important to measure if self-explanations truly reflect the model's behavior. Such a measure is called interpretability-faithfulness and is challenging to perform since the ground truth is inaccessible, and many LLMs only have an inference API. To address this, we propose employing self-consistency checks to measure faithfulness. For example, if an LLM says a set of words is important for making a prediction, then it should not be able to make its prediction without these words. While self-consistency checks are a common approach to faithfulness, they have not previously been successfully applied to LLM self-explanations for counterfactual, importance measure, and redaction explanations. Our results demonstrate that faithfulness is explanation, model, and task-dependent, showing self-explanations should not be trusted in general. For example, with sentiment classification, counterfactuals are more faithful for Llama2, importance measures for Mistral, and redaction for Falcon 40B.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Self-consistency checks are proposed to be successfully applied to LLM self-explanations for counterfactual, importance measure, and redaction explanations, showing self-explanations should not be trusted in general."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is assessing language model robustness and calibration using counterfactual consistency probing, while the paper focuses on measuring the faithfulness of self-explanations from large language models using self-consistency checks.\n\nProposal: Assessing language model robustness and calibration using counterfactual consistency probing.\nPaper: Measuring the faithfulness of self-explanations from large language models using self-consistency checks.\n\nThe two works have different research problems and approaches. The proposal aims to probe language models with counterfactual inputs to assess their robustness and calibration, while the paper evaluates the faithfulness of self-explanations generated by language models using self-consistency checks.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "020b7ed6a07c60125902a527a3a8d064a842e725",
            "paperId": "020b7ed6a07c60125902a527a3a8d064a842e725",
            "title": "From Robustness to Improved Generalization and Calibration in Pre-trained Language Models",
            "abstract": "Enhancing generalization and uncertainty quantification in pre-trained language models (PLMs) is crucial for their effectiveness and reliability. Building on machine learning research that established the importance of robustness for improving generalization, we investigate the role of representation smoothness, achieved via Jacobian and Hessian regularization, in enhancing PLM performance. Although such regularization methods have proven effective in computer vision, their application in natural language processing (NLP), where PLM inputs are derived from a discrete domain, poses unique challenges. We introduce a novel two-phase regularization approach, JacHess, which minimizes the norms of the Jacobian and Hessian matrices within PLM intermediate representations relative to their inputs. Our evaluation using the GLUE benchmark demonstrates that JacHess significantly improves in-domain generalization and calibration in PLMs, outperforming unregularized fine-tuning and other similar regularization methods.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel two-phase regularization approach, JacHess, is introduced, which minimizes the norms of the Jacobian and Hessian matrices within PLM intermediate representations relative to their inputs, which significantly improves in-domain generalization and calibration in PLMs."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is assessing language model robustness and calibration using counterfactual consistency probing, while the paper focuses on improving generalization and calibration in pre-trained language models through representation smoothness achieved via Jacobian and Hessian regularization.\n\nProposal: Assessing language model robustness and calibration using counterfactual consistency probing.\nPaper: Improving generalization and calibration in pre-trained language models through representation smoothness achieved via Jacobian and Hessian regularization.\n\nThe two works have different research problems and approaches. The proposal aims to assess model robustness and calibration using counterfactual probing, while the paper focuses on improving generalization and calibration through regularization techniques.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "ba63e1ab5b6e9d849982ae293ac0483053badaff",
            "paperId": "ba63e1ab5b6e9d849982ae293ac0483053badaff",
            "title": "Uncertainty in Language Models: Assessment through Rank-Calibration",
            "abstract": "Language Models (LMs) have shown promising performance in natural language generation. However, as LMs often generate incorrect or hallucinated responses, it is crucial to correctly quantify their uncertainty in responding to given inputs. In addition to verbalized confidence elicited via prompting, many uncertainty measures ($e.g.$, semantic entropy and affinity-graph-based measures) have been proposed. However, these measures can differ greatly, and it is unclear how to compare them, partly because they take values over different ranges ($e.g.$, $[0,\\infty)$ or $[0,1]$). In this work, we address this issue by developing a novel and practical framework, termed $Rank$-$Calibration$, to assess uncertainty and confidence measures for LMs. Our key tenet is that higher uncertainty (or lower confidence) should imply lower generation quality, on average. Rank-calibration quantifies deviations from this ideal relationship in a principled manner, without requiring ad hoc binary thresholding of the correctness score ($e.g.$, ROUGE or METEOR). The broad applicability and the granular interpretability of our methods are demonstrated empirically.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel and practical framework, termed $Rank$-$Calibration$ is developed, to assess uncertainty and confidence measures for LMs, with the key tenet that higher uncertainty should imply lower generation quality, on average."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is assessing the robustness and calibration of language models using counterfactual consistency probing, while the paper focuses on assessing uncertainty measures for language models using a rank-calibration framework. Although both works aim to evaluate language models, the proposal emphasizes robustness and consistency, while the paper focuses on uncertainty quantification and its relationship with generation quality.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "29abcf865613287c661385c39401424f709a3fda",
            "paperId": "29abcf865613287c661385c39401424f709a3fda",
            "title": "Holistic Evaluation of Language Models",
            "abstract": "Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.",
            "year": 2022,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Holistic Evaluation of Language Models (HELM) is presented to improve the transparency of language models and intends for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models."
            },
            "score": 6
        },
        {
            "id": "5424e311319c58847b4c690d5c91090e3b6a4ac3",
            "paperId": "5424e311319c58847b4c690d5c91090e3b6a4ac3",
            "title": "Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models",
            "abstract": "While Large Language Models (LLMs) have demonstrated remarkable potential in natural language generation and instruction following, a persistent challenge lies in their susceptibility to\"hallucinations\", which erodes trust in their outputs. Although Uncertainty Quantification (UQ) presents a promising solution, its accurate implementation within the context of LLMs remains a significant hurdle. To address this critical roadblock, our research originates from a fundamental heuristic insight: tokens within auto-regressive LLM-generated text do not equally reflect the underlying meaning. Some tokens carry greater relevance and representativeness than others, owing to the phenomenon of\"linguistic redundancy\", wherein a select few keywords suffice to convey the essence of lengthy sentences. Regrettably, existing methodologies treat all tokens with equal importance when estimating uncertainty, disregarding these inherent generative inequalities. Our analysis reveals a significant issue with state-of-the-art: numerous tokens (and sentences) of limited semantic significance receive equal or even excessive weighting during uncertainty estimation. To rectify this bias, we propose to jointly Shifting Attention to more Relevant (SAR) components, at both the token- and the sentence-levels for accurate uncertainty estimation. We conduct extensive experiments involving a range of popular\"off-the-shelf\"LLMs, including instruction-tuned LLMs such as Vicuna, WizardLM, and LLaMA-2-chat, as well as pretrained LLMs like OPT and LLaMA, with model sizes extending up to 33B parameters. We carry out evaluation across various free-form question-answering tasks, encompassing domains such as reading comprehension, science Q&A, and medical Q&A. Our experimental results demonstrate the superior performance of SAR in addressing the challenges of uncertainty estimation within the realm of LLMs.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results demonstrate the superior performance of SAR in addressing the challenges of uncertainty estimation within the realm of LLMs, and propose to jointly Shifting Attention to more Relevant (SAR) components, at both the token- and the sentence-levels for accurate uncertainty estimation."
            },
            "score": 6
        },
        {
            "id": "507465f8d46489a68a527cb5304d76bdb6c31ed9",
            "paperId": "507465f8d46489a68a527cb5304d76bdb6c31ed9",
            "title": "Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation",
            "abstract": "We introduce a method to measure uncertainty in large language models. For tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models. We show that measuring uncertainty in natural language is challenging because of\"semantic equivalence\"-- different sentences can mean the same thing. To overcome these challenges we introduce semantic entropy -- an entropy which incorporates linguistic invariances created by shared meanings. Our method is unsupervised, uses only a single model, and requires no modifications to off-the-shelf language models. In comprehensive ablation studies we show that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines.",
            "year": 2023,
            "citationCount": 85,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "In comprehensive ablation studies, it is shown that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines."
            },
            "score": 6
        },
        {
            "id": "bfce51fe2b3d885d09cd3ffed328f86f04442f63",
            "paperId": "bfce51fe2b3d885d09cd3ffed328f86f04442f63",
            "title": "Explaining Prediction Uncertainty of Pre-trained Language Models by Detecting Uncertain Words in Inputs",
            "abstract": "Estimating the predictive uncertainty of pre-trained language models is important for increasing their trustworthiness in NLP. Although many previous works focus on quantifying prediction uncertainty, there is little work on explaining the uncertainty. This paper pushes a step further on explaining uncertain predictions of post-calibrated pre-trained language models. We adapt two perturbation-based post-hoc interpretation methods, Leave-one-out and Sampling Shapley, to identify words in inputs that cause the uncertainty in predictions. We test the proposed methods on BERT and RoBERTa with three tasks: sentiment classi\ufb01cation, natural language inference, and paraphrase identi\ufb01cation, in both in-domain and out-of-domain settings. Experiments show that both methods consistently capture words in inputs that cause prediction uncertainty.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Two perturbation-based post-hoc interpretation methods are adapted to identify words in inputs that cause the uncertainty in predictions of post-calibrated pre-trained language models."
            },
            "score": 6
        },
        {
            "id": "be8c90bca14d59f180f40a41126b7cd8c29c5d4e",
            "paperId": "be8c90bca14d59f180f40a41126b7cd8c29c5d4e",
            "title": "Uncertainty Quantification for In-Context Learning of Large Language Models",
            "abstract": "In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM's response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM's response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model's configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-context learning in a plug-and-play fashion. Extensive experiments are conducted to demonstrate the effectiveness of the decomposition. The code and data are available at: https://github.com/lingchen0331/UQ_ICL.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations and ambiguities tied to the model's configurations (epistemic uncertainty)."
            },
            "score": 6
        },
        {
            "id": "9d1f9406ed676171d9975e27606c95633ca898b1",
            "paperId": "9d1f9406ed676171d9975e27606c95633ca898b1",
            "title": "On the Systematicity of Probing Contextualized Word Representations: The Case of Hypernymy in BERT",
            "abstract": "Contextualized word representations have become a driving force in NLP, motivating widespread interest in understanding their capabilities and the mechanisms by which they operate. Particularly intriguing is their ability to identify and encode conceptual abstractions. Past work has probed BERT representations for this competence, finding that BERT can correctly retrieve noun hypernyms in cloze tasks. In this work, we ask the question: do probing studies shed light on systematic knowledge in BERT representations? As a case study, we examine hypernymy knowledge encoded in BERT representations. In particular, we demonstrate through a simple consistency probe that the ability to correctly retrieve hypernyms in cloze tasks, as used in prior work, does not correspond to systematic knowledge in BERT. Our main conclusion is cautionary: even if BERT demonstrates high probing accuracy for a particular competence, it does not necessarily follow that BERT \u2018understands\u2019 a concept, and it cannot be expected to systematically generalize across applicable contexts.",
            "year": 2020,
            "citationCount": 67,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The main conclusion is cautionary: even if BERT demonstrates high probing accuracy for a particular competence, it does not necessarily follow that BERT \u2018understands\u2019 a concept, and it cannot be expected to systematically generalize across applicable contexts."
            },
            "score": 6
        },
        {
            "id": "468d1e2d75a23fecaf96fe65d8b01ff35ea5d0bd",
            "paperId": "468d1e2d75a23fecaf96fe65d8b01ff35ea5d0bd",
            "title": "Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models",
            "abstract": "Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts inserted in English reveal a clear pattern whereby the new piece of knowledge transfers only to languages with which English has a high RankC score.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy, and conducts an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level."
            },
            "score": 5
        },
        {
            "id": "8d45cf5e403f14d9f6498a97b314af03db49351e",
            "paperId": "8d45cf5e403f14d9f6498a97b314af03db49351e",
            "title": "Counterfactually Probing Language Identity in Multilingual Models",
            "abstract": "Techniques in causal analysis of language models illuminate how linguistic information is organized in LLMs. We use one such technique, AlterRep, a method of counterfactual probing, to explore the internal structure of multilingual models (mBERT and XLM-R). We train a linear classifier on a binary language identity task, to classify tokens between Language X and Language Y. Applying a counterfactual probing procedure, we use the classifier weights to project the embeddings into the null space and push the resulting embeddings either in the direction of Language X or Language Y. Then we evaluate on a masked language modeling task. We find that, given a template in Language X, pushing towards Language Y systematically increases the probability of Language Y words, above and beyond a third-party control language. But it does not specifically push the model towards translation-equivalent words in Language Y. Pushing towards Language X (the same direction as the template) has a minimal effect, but somewhat degrades these models. Overall, we take these results as further evidence of the rich structure of massive multilingual language models, which include both a language-specific and language-general component. And we show that counterfactual probing can be fruitfully applied to multilingual models.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "AlterRep, a method of counterfactual probing, is used to explore the internal structure of multilingual models (mBERT and XLM-R), which include both a language-specific and language-general component, and it is shown that counterfactually probing can be fruitfully applied to mult bilingual models."
            },
            "score": 5
        },
        {
            "id": "7e7cc29b042793b27688beb765dc604dee65d536",
            "paperId": "7e7cc29b042793b27688beb765dc604dee65d536",
            "title": "Flexible text generation for counterfactual fairness probing",
            "abstract": "A common approach for testing fairness issues in text-based classifiers is through the use of counterfactuals: does the classifier output change if a sensitive attribute in the input is changed? Existing counterfactual generation methods typically rely on wordlists or templates, producing simple counterfactuals that fail to take into account grammar, context, or subtle sensitive attribute references, and could miss issues that the wordlist creators had not considered. In this paper, we introduce a task for generating counterfactuals that overcomes these shortcomings, and demonstrate how large language models (LLMs) can be leveraged to accomplish this task. We show that this LLM-based method can produce complex counterfactuals that existing methods cannot, comparing the performance of various counterfactual generation methods on the Civil Comments dataset and showing their value in evaluating a toxicity classifier.",
            "year": 2022,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that this LLM-based method can produce complex counterfactuals that existing methods cannot, comparing the performance of various counterfactUAL generation methods on the Civil Comments dataset and showing their value in evaluating a toxicity classifier."
            },
            "score": 5
        },
        {
            "id": "6a483cd1cbecd66150c9bbcd01606723950281bc",
            "paperId": "6a483cd1cbecd66150c9bbcd01606723950281bc",
            "title": "Prototypical Calibration for Few-shot Learning of Language Models",
            "abstract": "In-context learning of GPT-like models has been recognized as fragile across different hand-crafted templates, and demonstration permutations. In this work, we propose prototypical calibration to adaptively learn a more robust decision boundary for zero- and few-shot classification, instead of greedy decoding. Concretely, our method first adopts Gaussian mixture distribution to estimate the prototypical clusters for all categories. Then we assign each cluster to the corresponding label by solving a weighted bipartite matching problem. Given an example, its prediction is calibrated by the likelihood of prototypical clusters. Experimental results show that prototypical calibration yields a substantial improvement on a diverse set of tasks. Extensive analysis across different scales also indicates that our method calibrates the decision boundary as expected, greatly improving the robustness of GPT to templates, permutations, and class imbalance.",
            "year": 2022,
            "citationCount": 29,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results show that prototypical calibration yields a substantial improvement on a diverse set of tasks, and extensive analysis across different scales indicates that the method calibrates the decision boundary as expected, greatly improving the robustness of GPT to templates, permutations, and class imbalance."
            },
            "score": 5
        },
        {
            "id": "bbd6d6874a8ca1c155bcfb540e8d55199944cdc5",
            "paperId": "bbd6d6874a8ca1c155bcfb540e8d55199944cdc5",
            "title": "RoAST: Robustifying Language Models via Adversarial Perturbation with Selective Training",
            "abstract": "Fine-tuning pre-trained language models (LMs) has become the de facto standard in many NLP tasks. Nevertheless, fine-tuned LMs are still prone to robustness issues, such as adversarial robustness and model calibration. Several perspectives of robustness for LMs have been studied independently, but lacking a unified consideration in multiple perspectives. In this paper, we propose Robustifying LMs via Adversarial perturbation with Selective Training (RoAST), a simple yet effective fine-tuning technique to enhance the multi-perspective robustness of LMs in a unified way. RoAST effectively incorporates two important sources for the model robustness, robustness on the perturbed inputs and generalizable knowledge in pre-trained LMs. To be specific, RoAST introduces adversarial perturbation during fine-tuning while the model parameters are selectively updated upon their relative importance to minimize unnecessary deviation. Under a unified evaluation of fine-tuned LMs by incorporating four representative perspectives of model robustness, we demonstrate the effectiveness of RoAST compared to state-of-the-art fine-tuning methods on six different types of LMs, which indicates its usefulness in practice.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Under a unified evaluation of fine-tuned LMs by incorporating four representative perspectives of model robustness, the effectiveness of RoAST is demonstrated compared to state-of-the-art fine- tuning methods on six different types of LMs, which indicates its usefulness in practice."
            },
            "score": 5
        },
        {
            "id": "444f3b7293b85b7d37600372941a289f9163abd1",
            "paperId": "444f3b7293b85b7d37600372941a289f9163abd1",
            "title": "LM-Polygraph: Uncertainty Estimation for Language Models",
            "abstract": "Recent advancements in the capabilities of large language models (LLMs) have paved the way for a myriad of groundbreaking applications in various fields. However, a significant challenge arises as these models often\"hallucinate\", i.e., fabricate facts without providing users an apparent means to discern the veracity of their statements. Uncertainty estimation (UE) methods are one path to safer, more responsible, and more effective use of LLMs. However, to date, research on UE methods for LLMs has been focused primarily on theoretical rather than engineering contributions. In this work, we tackle this issue by introducing LM-Polygraph, a framework with implementations of a battery of state-of-the-art UE methods for LLMs in text generation tasks, with unified program interfaces in Python. Additionally, it introduces an extendable benchmark for consistent evaluation of UE techniques by researchers, and a demo web application that enriches the standard chat dialog with confidence scores, empowering end-users to discern unreliable responses. LM-Polygraph is compatible with the most recent LLMs, including BLOOMz, LLaMA-2, ChatGPT, and GPT-4, and is designed to support future releases of similarly-styled LMs.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LM-Polygraph is introduced, a framework with implementations of a battery of state-of-the-art UE methods for LLMs in text generation tasks, with unified program interfaces in Python, and introduces an extendable benchmark for consistent evaluation of UE techniques by researchers."
            },
            "score": 5
        },
        {
            "id": "ea0d41514a41f8273f13b3b277e7fcbbc65a8549",
            "paperId": "ea0d41514a41f8273f13b3b277e7fcbbc65a8549",
            "title": "Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models",
            "abstract": "The recent performance leap of Large Language Models (LLMs) opens up new opportunities across numerous industrial applications and domains. However, erroneous generations, such as false predictions, misinformation, and hallucination made by LLMs, have also raised severe concerns for the trustworthiness of LLMs', especially in safety-, security- and reliability-sensitive scenarios, potentially hindering real-world adoptions. While uncertainty estimation has shown its potential for interpreting the prediction risks made by general machine learning (ML) models, little is known about whether and to what extent it can help explore an LLM's capabilities and counteract its undesired behavior. To bridge the gap, in this paper, we initiate an exploratory study on the risk assessment of LLMs from the lens of uncertainty. In particular, we experiment with twelve uncertainty estimation methods and four LLMs on four prominent natural language processing (NLP) tasks to investigate to what extent uncertainty estimation techniques could help characterize the prediction risks of LLMs. Our findings validate the effectiveness of uncertainty estimation for revealing LLMs' uncertain/non-factual predictions. In addition to general NLP tasks, we extensively conduct experiments with four LLMs for code generation on two datasets. We find that uncertainty estimation can potentially uncover buggy programs generated by LLMs. Insights from our study shed light on future design and development for reliable LLMs, facilitating further research toward enhancing the trustworthiness of LLMs.",
            "year": 2023,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An exploratory study on the risk assessment of LLMs from the lens of uncertainty is initiated, finding that uncertainty estimation can potentially uncover buggy programs generated by LLMs."
            },
            "score": 5
        },
        {
            "id": "7f6d48d7b1641d3d2fd4ee06c434a73af8fce07b",
            "paperId": "7f6d48d7b1641d3d2fd4ee06c434a73af8fce07b",
            "title": "Density-Softmax: Scalable and Calibrated Uncertainty Estimation under Distribution Shifts",
            "abstract": "Prevalent deterministic deep-learning models suffer from significant over-confidence under distribution shifts. Probabilistic approaches can reduce this problem but struggle with computational efficiency. In this paper, we propose Density-Softmax, a fast and lightweight deterministic method to improve calibrated uncertainty estimation via a combination of density function with the softmax layer. By using the latent representation's likelihood value, our approach produces more uncertain predictions when test samples are distant from the training samples. Theoretically, we show that Density-Softmax can produce high-quality uncertainty estimation with neural networks, as it is the solution of minimax uncertainty risk and is distance-aware, thus reducing the over-confidence of the standard softmax. Empirically, our method enjoys similar computational efficiency as a single forward pass deterministic with standard softmax on the shifted toy, vision, and language datasets across modern deep-learning architectures. Notably, Density-Softmax uses 4 times fewer parameters than Deep Ensembles and 6 times lower latency than Rank-1 Bayesian Neural Network, while obtaining competitive predictive performance and lower calibration errors under distribution shifts.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Density-Softmax is proposed, a fast and lightweight deterministic method to improve calibrated uncertainty estimation via a combination of density function with the softmax layer, which enjoys similar computational efficiency as a single forward pass deterministic with standard softmax on the shifted toy, vision, and language datasets across modern deep-learning architectures."
            },
            "score": 5
        },
        {
            "id": "8ae920111435a7db8da360c654c771c53f57c69a",
            "paperId": "8ae920111435a7db8da360c654c771c53f57c69a",
            "title": "Uncertainty Estimation of Transformer Predictions for Misclassification Detection",
            "abstract": "Uncertainty estimation (UE) of model predictions is a crucial step for a variety of tasks such as active learning, misclassification detection, adversarial attack detection, out-of-distribution detection, etc. Most of the works on modeling the uncertainty of deep neural networks evaluate these methods on image classification tasks. Little attention has been paid to UE in natural language processing. To fill this gap, we perform a vast empirical investigation of state-of-the-art UE methods for Transformer models on misclassification detection in named entity recognition and text classification tasks and propose two computationally efficient modifications, one of which approaches or even outperforms computationally intensive methods.",
            "year": 2022,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A vast empirical investigation of state-of-the-art UE methods for Transformer models on misclassification detection in named entity recognition and text classification tasks and two computationally efficient modifications are proposed, one of which approaches or even outperforms computationally intensive methods."
            },
            "score": 5
        },
        {
            "id": "6d3ae6d6b312b659b3a14ae3f3e86a36db63200d",
            "paperId": "6d3ae6d6b312b659b3a14ae3f3e86a36db63200d",
            "title": "Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning",
            "abstract": "Step-by-step decision planning with large language models (LLMs) is gaining attention in AI agent development. This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models. Existing approaches are either white-box or computationally demanding, limiting use of black-box proprietary LLMs within budgets. The paper's first contribution is a non-parametric uncertainty quantification method for LLMs, efficiently estimating point-wise dependencies between input-decision on the fly with a single inference, without access to token logits. This estimator informs the statistical interpretation of decision trustworthiness. The second contribution outlines a systematic design for a decision-making agent, generating actions like ``turn on the bathroom light'' based on user prompts such as ``take a bath''. Users will be asked to provide preferences when more than one action has high estimated point-wise dependencies. In conclusion, our uncertainty estimation and decision-making agent design offer a cost-efficient approach for AI agent development.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models, and outlines a systematic design for a decision-making agent, offering a cost-efficient approach for AI agent development."
            },
            "score": 5
        },
        {
            "id": "7fc8da1a277ce4a8292f6675a2f7ce2f928a283b",
            "paperId": "7fc8da1a277ce4a8292f6675a2f7ce2f928a283b",
            "title": "Counterfactuals of Counterfactuals: a back-translation-inspired approach to analyse counterfactual editors",
            "abstract": "In the wake of responsible AI, interpretability methods, which attempt to provide an explanation for the predictions of neural models have seen rapid progress. In this work, we are concerned with explanations that are applicable to natural language processing (NLP) models and tasks, and we focus specifically on the analysis of counterfactual, contrastive explanations. We note that while there have been several explainers proposed to produce counterfactual explanations, their behaviour can vary significantly and the lack of a universal ground truth for the counterfactual edits imposes an insuperable barrier on their evaluation. We propose a new back translation-inspired evaluation methodology that utilises earlier outputs of the explainer as ground truth proxies to investigate the consistency of explainers. We show that by iteratively feeding the counterfactual to the explainer we can obtain valuable insights into the behaviour of both the predictor and the explainer models, and infer patterns that would be otherwise obscured. Using this methodology, we conduct a thorough analysis and propose a novel metric to evaluate the consistency of counterfactual generation approaches with different characteristics across available performance indicators.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a new back translation-inspired evaluation methodology that utilises earlier outputs of the explainer as ground truth proxies to investigate the consistency of explainers and shows that by iteratively feeding the counterfactual to the explainers the authors can obtain valuable insights into the behaviour of both the predictor and the explaining models, and infer patterns that would be otherwise obscured."
            },
            "score": 5
        },
        {
            "id": "611f94c7d00b222f5d80ba1b245c3a72ba741653",
            "paperId": "611f94c7d00b222f5d80ba1b245c3a72ba741653",
            "title": "Towards Understanding Gender-Seniority Compound Bias in Natural Language Generation",
            "abstract": "Women are often perceived as junior to their male counterparts, even within the same job titles. While there has been significant progress in the evaluation of gender bias in natural language processing (NLP), existing studies seldom investigate how biases toward gender groups change when compounded with other societal biases. In this work, we investigate how seniority impacts the degree of gender bias exhibited in pretrained neural generation models by introducing a novel framework for probing compound bias. We contribute a benchmark robustness-testing dataset spanning two domains, U.S. senatorship and professorship, created using a distant-supervision method. Our dataset includes human-written text with underlying ground truth and paired counterfactuals. We then examine GPT-2 perplexity and the frequency of gendered language in generated text. Our results show that GPT-2 amplifies bias by considering women as junior and men as senior more often than the ground truth in both domains. These results suggest that NLP applications built using GPT-2 may harm women in professional capacities.",
            "year": 2022,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results show that GPT-2 amplifies bias by considering women as junior and men as senior more often than the ground truth in both domains, and suggest that NLP applications built using G PT-2 may harm women in professional capacities."
            },
            "score": 5
        },
        {
            "id": "03fe268a0bcf66104b648eb3f34cc729ede0948c",
            "paperId": "03fe268a0bcf66104b648eb3f34cc729ede0948c",
            "title": "Probing Intersectional Biases in Vision-Language Models with Counterfactual Examples",
            "abstract": "While vision-language models (VLMs) have achieved remarkable performance improvements recently, there is growing evidence that these models also posses harmful biases with respect to social attributes such as gender and race. Prior studies have primarily focused on probing such bias attributes individually while ignoring biases associated with intersections between social attributes. This could be due to the difficulty of collecting an exhaustive set of image-text pairs for various combinations of social attributes from existing datasets. To address this challenge, we employ text-to-image diffusion models to produce counterfactual examples for probing intserctional social biases at scale. Our approach utilizes Stable Diffusion with cross attention control to produce sets of counterfactual image-text pairs that are highly similar in their depiction of a subject (e.g., a given occupation) while differing only in their depiction of intersectional social attributes (e.g., race&gender). We conduct extensive experiments using our generated dataset which reveal the intersectional social biases present in state-of-the-art VLMs.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work employs text-to-image diffusion models to produce counterfactual examples for probing intserctional social biases at scale and reveals the intersectional social bias attributes present in state-of-the-art VLMs."
            },
            "score": 4
        },
        {
            "id": "79217b366b3659500de61f6aabd4aa4d3ff24e34",
            "paperId": "79217b366b3659500de61f6aabd4aa4d3ff24e34",
            "title": "SocialCounterfactuals: Probing and Mitigating Intersectional Social Biases in Vision-Language Models with Counterfactual Examples",
            "abstract": "While vision-language models (VLMs) have achieved remarkable performance improvements recently, there is growing evidence that these models also posses harmful biases with respect to social attributes such as gender and race. Prior studies have primarily focused on probing such bias attributes individually while ignoring biases associated with intersections between social attributes. This could be due to the difficulty of collecting an exhaustive set of image-text pairs for various combinations of social attributes. To address this challenge, we employ text-to-image diffusion models to produce counterfactual examples for probing intersectional social biases at scale. Our approach utilizes Stable Diffusion with cross attention control to produce sets of counterfactual image-text pairs that are highly similar in their depiction of a subject (e.g., a given occupation) while differing only in their depiction of intersectional social attributes (e.g., race&gender). Through our over-generate-then-filter methodology, we produce SocialCounterfactuals, a high-quality dataset containing 171k image-text pairs for probing intersectional biases related to gender, race, and physical characteristics. We conduct extensive experiments to demonstrate the usefulness of our generated dataset for probing and mitigating intersectional social biases in state-of-the-art VLMs.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Through the over-generate-then-filter methodology, this work produces SocialCounterfactuals, a high-quality dataset containing 171k image-text pairs for probing intersectional biases related to gender, race, and physical characteristics in state-of-the-art VLMs."
            },
            "score": 4
        },
        {
            "id": "806b5882c983bd156a8c10bcd34fe285d8a0593b",
            "paperId": "806b5882c983bd156a8c10bcd34fe285d8a0593b",
            "title": "GLoRE: Evaluating Logical Reasoning of Large Language Models",
            "abstract": "Recently, large language models (LLMs), including notable models such as GPT-4 and burgeoning community models, have showcased significant general language understanding abilities. However, there has been a scarcity of attempts to assess the logical reasoning capacities of these LLMs, an essential facet of natural language understanding. To encourage further investigation in this area, we introduce GLoRE, a meticulously assembled General Logical Reasoning Evaluation benchmark comprised of 12 datasets that span three different types of tasks. Our experimental results show that compared to the performance of human and supervised fine-tuning, the logical reasoning capabilities of open LLM models necessitate additional improvement; ChatGPT and GPT-4 show a strong capability of logical reasoning, with GPT-4 surpassing ChatGPT by a large margin. We propose a self-consistency probing method to enhance the accuracy of ChatGPT and a fine-tuned method to boost the performance of an open LLM. We release the datasets and evaluation programs to facilitate future research.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "GLoRE is introduced, a meticulously assembled General Logical Reasoning Evaluation benchmark comprised of 12 datasets that span three different types of tasks and proposes a self-consistency probing method to enhance the accuracy of ChatGPT and a fine-tuned method to boost the performance of an open LLM."
            },
            "score": 4
        },
        {
            "id": "04f0d2b8873f0dc7f086ed82f93091782d59d19f",
            "paperId": "04f0d2b8873f0dc7f086ed82f93091782d59d19f",
            "title": "Counterfactual Probing for the influence of affect and specificity on Intergroup Bias",
            "abstract": "While existing work on studying bias in NLP focues on negative or pejorative language use, Govindarajan et al. (2023) offer a revised framing of bias in terms of intergroup social context, and its effects on language behavior. In this paper, we investigate if two pragmatic features (specificity and affect) systematically vary in different intergroup contexts -- thus connecting this new framing of bias to language output. Preliminary analysis finds modest correlations between specificity and affect of tweets with supervised intergroup relationship (IGR) labels. Counterfactual probing further reveals that while neural models finetuned for predicting IGR labels reliably use affect in classification, the model's usage of specificity is inconclusive. Code and data can be found at: https://github.com/venkatasg/intergroup-probing",
            "year": 2023,
            "citationCount": 0,
            "tldr": null,
            "score": 4
        },
        {
            "id": "59f7ba3a730f94bbba685cc79dcf961c1a477e9d",
            "paperId": "59f7ba3a730f94bbba685cc79dcf961c1a477e9d",
            "title": "Counterfactual Probing Intergroup Bias for Affect and Specificity",
            "abstract": "While existing work on studying bias in NLP focues on negative or pejorative language use, Govindarajan et al. (2023) offer a revised framing of bias in terms of intergroup social context, and its effects on language behavior. In this paper, we investigate if two pragmatic features (specificity and affect) systematically vary in different intergroup contexts \u2014 thus connecting this new framing of bias to language output. Preliminary analysis finds modest correlations between specificity and affect of tweets with supervised intergroup relationship labels. Coun-terfactual probing further reveals that while neural models finetuned for predicting IGR labels reliably use affect in classification, the model\u2019s usage of specificity is inconclusive.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper investigates if two pragmatic features (specificity and affect) systematically vary in different intergroup contexts \u2014 thus connecting this new framing of bias to language output."
            },
            "score": 4
        },
        {
            "id": "e886fe33b9cda6e0b0f523d5fbbb7287c23aba2b",
            "paperId": "e886fe33b9cda6e0b0f523d5fbbb7287c23aba2b",
            "title": "Causal Analysis of Syntactic Agreement Neurons in Multilingual Language Models",
            "abstract": "Structural probing work has found evidence for latent syntactic information in pre-trained language models. However, much of this analysis has focused on monolingual models, and analyses of multilingual models have employed correlational methods that are confounded by the choice of probing tasks. In this study, we causally probe multilingual language models (XGLM and multilingual BERT) as well as monolingual BERT-based models across various languages; we do this by performing counterfactual perturbations on neuron activations and observing the effect on models\u2019 subject-verb agreement probabilities. We observe where in the model and to what extent syntactic agreement is encoded in each language. We find significant neuron overlap across languages in autoregressive multilingual language models, but not masked language models. We also find two distinct layer-wise effect patterns and two distinct sets of neurons used for syntactic agreement, depending on whether the subject and verb are separated by other tokens. Finally, we find that behavioral analyses of language models are likely underestimating how sensitive masked language models are to syntactic information.",
            "year": 2022,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study causally probe multilingual language models as well as monolingual BERT-based models across various languages by performing counterfactual perturbations on neuron activations and observing the effect on models\u2019 subject-verb agreement probabilities, finding significant neuron overlap across languages in autoregressive mult bilingual language models, but not masked language models."
            },
            "score": 4
        },
        {
            "id": "80248c8c7cbb5bb1d2a508001108f3f15bb60430",
            "paperId": "80248c8c7cbb5bb1d2a508001108f3f15bb60430",
            "title": "Logical Closed Loop: Uncovering Object Hallucinations in Large Vision-Language Models",
            "abstract": "Object hallucination has been an Achilles' heel which hinders the broader applications of large vision-language models (LVLMs). Object hallucination refers to the phenomenon that the LVLMs claim non-existent objects in the image. To mitigate the object hallucinations, instruction tuning and external model-based detection methods have been proposed, which either require large-scare computational resources or depend on the detection result of external models. However, there remains an under-explored field to utilize the LVLM itself to alleviate object hallucinations. In this work, we adopt the intuition that the LVLM tends to respond logically consistently for existent objects but inconsistently for hallucinated objects. Therefore, we propose a Logical Closed Loop-based framework for Object Hallucination Detection and Mitigation, namely LogicCheckGPT. In specific, we devise logical consistency probing to raise questions with logical correlations, inquiring about attributes from objects and vice versa. Whether their responses can form a logical closed loop serves as an indicator of object hallucination. As a plug-and-play method, it can be seamlessly applied to all existing LVLMs. Comprehensive experiments conducted on three benchmarks across four LVLMs have demonstrated significant improvements brought by our method, indicating its effectiveness and generality.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a Logical Closed Loop-based framework for Object Hallucination Detection and Mitigation, namely LogicCheckGPT, which is a plug-and-play method that can be seamlessly applied to all existing LVLMs."
            },
            "score": 4
        },
        {
            "id": "5b9bf4a82da690e738821ac0460b96c2770ed5dd",
            "paperId": "5b9bf4a82da690e738821ac0460b96c2770ed5dd",
            "title": "Are Large Language Models Temporally Grounded?",
            "abstract": "Are Large language models (LLMs) temporally grounded? Since LLMs cannot perceive and interact with the environment, it is impossible to answer this question directly. Instead, we provide LLMs with textual narratives and probe them with respect to their common-sense knowledge of the structure and duration of events, their ability to order events along a timeline, and self-consistency within their temporal model (e.g., temporal relations such as after and before are mutually exclusive for any pair of events). We evaluate state-of-the-art LLMs (such as LLaMA 2 and GPT-4) on three tasks reflecting these abilities. Generally, we find that LLMs lag significantly behind both human performance as well as small-scale, specialised LMs. In-context learning, instruction tuning, and chain-of-thought prompting reduce this gap only to a limited degree. Crucially, LLMs struggle the most with self-consistency, displaying incoherent behaviour in at least 27.23% of their predictions. Contrary to expectations, we also find that scaling the model size does not guarantee positive gains in performance. To explain these results, we study the sources from which LLMs may gather temporal information: we find that sentence ordering in unlabelled texts, available during pre-training, is only weakly correlated with event ordering. Moreover, public instruction tuning mixtures contain few temporal tasks. Hence, we conclude that current LLMs lack a consistent temporal model of textual narratives. Code, datasets, and LLM outputs are available at https://github.com/yfqiu-nlp/temporal-llms.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is concluded that current LLMs lack a consistent temporal model of textual narratives, and study the sources from which LLMs may gather temporal information finds that sentence ordering in unlabelled texts is only weakly correlated with event ordering."
            },
            "score": 4
        },
        {
            "id": "80b8a7f74e1d871c885767a65a68093a0c97cac7",
            "paperId": "80b8a7f74e1d871c885767a65a68093a0c97cac7",
            "title": "How Does Calibration Data Affect the Post-training Pruning and Quantization of Large Language Models?",
            "abstract": "Pruning and quantization form the foundation of model compression for neural networks, enabling efficient inference for large language models (LLMs). Recently, various quantization and pruning techniques have demonstrated state-of-the-art performance in a post-training setting. They rely upon calibration data, a small set of unlabeled examples, to generate layer activations. However, no prior work has systematically investigated how the calibration data impacts the effectiveness of model compression methods. In this paper, we present the first extensive empirical study on the effect of calibration data upon LLM performance. We trial a variety of pruning and quantization methods, tasks, models, and datasets. Surprisingly, we find substantial variations in downstream task performance, contrasting existing work that suggests a greater level of robustness to the calibration data. Finally, we make a series of recommendations for the effective use of calibration data in LLM quantization and pruning.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This first extensive empirical study on the effect of calibration data upon LLM performance is presented, and substantial variations in downstream task performance are found, contrasting existing work that suggests a greater level of robustness to the calibration data."
            },
            "score": 4
        },
        {
            "id": "e2cc232ad999164f1bf340996bb5db62b6602d31",
            "paperId": "e2cc232ad999164f1bf340996bb5db62b6602d31",
            "title": "Robustness Over Time: Understanding Adversarial Examples' Effectiveness on Longitudinal Versions of Large Language Models",
            "abstract": "Large Language Models (LLMs) have led to significant improvements in many tasks across various domains, such as code interpretation, response generation, and ambiguity handling. These LLMs, however, when upgrading, primarily prioritize enhancing user experience while neglecting security, privacy, and safety implications. Consequently, unintended vulnerabilities or biases can be introduced. Previous studies have predominantly focused on specific versions of the models and disregard the potential emergence of new attack vectors targeting the updated versions. Through the lens of adversarial examples within the in-context learning framework, this longitudinal study addresses this gap by conducting a comprehensive assessment of the robustness of successive versions of LLMs, vis-\\`a-vis GPT-3.5. We conduct extensive experiments to analyze and understand the impact of the robustness in two distinct learning categories: zero-shot learning and few-shot learning. Our findings indicate that, in comparison to earlier versions of LLMs, the updated versions do not exhibit the anticipated level of robustness against adversarial attacks. In addition, our study emphasizes the increased effectiveness of synergized adversarial queries in most zero-shot learning and few-shot learning cases. We hope that our study can lead to a more refined assessment of the robustness of LLMs over time and provide valuable insights of these models for both developers and users.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This longitudinal study conducts a comprehensive assessment of the robustness of successive versions of LLMs, vis-\\`a-vis GPT-3.5.5, and indicates that the updated versions do not exhibit the anticipated level of robustness against adversarial attacks."
            },
            "score": 4
        },
        {
            "id": "9ed4387cf5f25aa53d13f29b5b5c107f70a881cc",
            "paperId": "9ed4387cf5f25aa53d13f29b5b5c107f70a881cc",
            "title": "Robustness Evaluation of Cloud-Deployed Large Language Models against Chinese Adversarial Text Attacks",
            "abstract": "In the evolving digital realm, Large Language Models (LLMs) like ChatGPT, which recently achieved state-of-the-art results across diverse NLP tasks, are extensively used. Deployed on the cloud, ChatGPT allows interaction via its API, providing rich and high-quality solutions. However, its vulnerability to adversarial attacks, potentially compromising the quality and reliability of cloud services and leading to information leakage, raises security concerns. Investigating the robustness of ChatGPT against adversarial attacks enables a preliminary understanding of its weaknesses and facilitates the subsequent integration of targeted defensive mechanisms into the cloud framework. Most current research on the robustness of LLMs against adversarial attacks focuses on BERT, with few studies on ChatGPT under similar conditions. This paper explores the robustness of ChatGPT against Chinese adversarial text attacks in text classification tasks and proposes a ChatGPT-based adversarial text fluency evaluation method that eliminates the need for human involvement. Experiments conducted on the real-world dataset, THUCNews, examined the robustness of Chinese BERT and ChatGPT against adversarial attacks generated via various Chinese adversarial text generation methods. A multidimensional assessment revealed that both models are susceptible to attacks, leading to decreased text classification accuracy. The attack success rate on ChatGPT reached nearly 45%.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper explores the robustness of ChatGPT against Chinese adversarial text attacks in text classification tasks and proposes a ChatGPT-based adversarial text fluency evaluation method that eliminates the need for human involvement."
            },
            "score": 4
        },
        {
            "id": "40ee4949c1050a465d418deb6dd7ea6304a3bc29",
            "paperId": "40ee4949c1050a465d418deb6dd7ea6304a3bc29",
            "title": "Adversarial Attacks and Defenses in Large Language Models: Old and New Threats",
            "abstract": "Over the past decade, there has been extensive research aimed at enhancing the robustness of neural networks, yet this problem remains vastly unsolved. Here, one major impediment has been the overestimation of the robustness of new defense approaches due to faulty defense evaluations. Flawed robustness evaluations necessitate rectifications in subsequent works, dangerously slowing down the research and providing a false sense of security. In this context, we will face substantial challenges associated with an impending adversarial arms race in natural language processing, specifically with closed-source Large Language Models (LLMs), such as ChatGPT, Google Bard, or Anthropic's Claude. We provide a first set of prerequisites to improve the robustness assessment of new approaches and reduce the amount of faulty evaluations. Additionally, we identify embedding space attacks on LLMs as another viable threat model for the purposes of generating malicious content in open-sourced models. Finally, we demonstrate on a recently proposed defense that, without LLM-specific best practices in place, it is easy to overestimate the robustness of a new approach.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work provides a first set of prerequisites to improve the robustness assessment of new approaches and reduce the amount of faulty evaluations, and identifies embedding space attacks on LLMs as another viable threat model for the purposes of generating malicious content in open-sourced models."
            },
            "score": 4
        },
        {
            "id": "c1aebd69e8bf8deac8b61e8ba699c78704a90828",
            "paperId": "c1aebd69e8bf8deac8b61e8ba699c78704a90828",
            "title": "ChEF: A Comprehensive Evaluation Framework for Standardized Assessment of Multimodal Large Language Models",
            "abstract": "Multimodal Large Language Models (MLLMs) have shown impressive abilities in interacting with visual content with myriad potential downstream tasks. However, even though a list of benchmarks has been proposed, the capabilities and limitations of MLLMs are still not comprehensively understood, due to a lack of a standardized and holistic evaluation framework. To this end, we present the first Comprehensive Evaluation Framework (ChEF) that can holistically profile each MLLM and fairly compare different MLLMs. First, we structure ChEF as four modular components, i.e., Scenario as scalable multimodal datasets, Instruction as flexible instruction retrieving formulae, Inferencer as reliable question answering strategies, and Metric as indicative task-specific score functions. Based on them, ChEF facilitates versatile evaluations in a standardized framework, and new evaluations can be built by designing new Recipes (systematic selection of these four components). Notably, current MLLM benchmarks can be readily summarized as recipes of ChEF. Second, we introduce 6 new recipes to quantify competent MLLMs' desired capabilities (or called desiderata, i.e., calibration, in-context learning, instruction following, language performance, hallucination, and robustness) as reliable agents that can perform real-world multimodal interactions. Third, we conduct a large-scale evaluation of 9 prominent MLLMs on 9 scenarios and 6 desiderata. Our evaluation summarized over 20 valuable observations concerning the generalizability of MLLMs across various scenarios and the composite capability of MLLMs required for multimodal interactions. We will publicly release all the detailed implementations for further analysis, as well as an easy-to-use modular toolkit for the integration of new recipes and models, so that ChEF can be a growing evaluation framework for the MLLM community.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents the first Comprehensive Evaluation Framework (ChEF) that can holistically profile each MLLM and fairly compare different MLLMs, and introduces 6 new recipes to quantify competent MLL Ms' desired capabilities as reliable agents that can perform real-world multimodal interactions."
            },
            "score": 4
        },
        {
            "id": "bf4700077294c369f64eda65f677dd4f61b43072",
            "paperId": "bf4700077294c369f64eda65f677dd4f61b43072",
            "title": "Uncertainty Estimation and Reduction of Pre-trained Models for Text Regression",
            "abstract": "Abstract State-of-the-art classification and regression models are often not well calibrated, and cannot reliably provide uncertainty estimates, limiting their utility in safety-critical applications such as clinical decision-making. While recent work has focused on calibration of classifiers, there is almost no work in NLP on calibration in a regression setting. In this paper, we quantify the calibration of pre- trained language models for text regression, both intrinsically and extrinsically. We further apply uncertainty estimates to augment training data in low-resource domains. Our experiments on three regression tasks in both self-training and active-learning settings show that uncertainty estimation can be used to increase overall performance and enhance model generalization.",
            "year": 2022,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper quantifies the calibration of pre- trained language models for text regression, both intrinsically and extrinsically, and applies uncertainty estimates to augment training data in low-resource domains."
            },
            "score": 4
        },
        {
            "id": "61fe60a1d1968914958e9d01be0f67593601c5c0",
            "paperId": "61fe60a1d1968914958e9d01be0f67593601c5c0",
            "title": "Bayesian Hierarchical Models for Counterfactual Estimation",
            "abstract": "Counterfactual explanations utilize feature perturbations to analyze the outcome of an original decision and recommend an actionable recourse. We argue that it is beneficial to provide several alternative explanations rather than a single point solution and propose a probabilistic paradigm to estimate a diverse set of counterfactuals. Specifically, we treat the perturbations as random variables endowed with prior distribution functions. This allows sampling multiple counterfactuals from the posterior density, with the added benefit of incorporating inductive biases, preserving domain specific constraints and quantifying uncertainty in estimates. More importantly, we leverage Bayesian hierarchical modeling to share information across different subgroups of a population, which can both improve robustness and measure fairness. A gradient based sampler with superior convergence characteristics efficiently computes the posterior samples. Experiments across several datasets demonstrate that the counterfactuals estimated using our approach are valid, sparse, diverse and feasible.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work argues that it is beneficial to provide several alternative explanations rather than a single point solution and proposes a probabilistic paradigm to estimate a diverse set of counterfactuals, which treats the perturbations as random variables endowed with prior distribution functions."
            },
            "score": 4
        },
        {
            "id": "a860ba337cead5e2e970460522d6612a49836ff1",
            "paperId": "a860ba337cead5e2e970460522d6612a49836ff1",
            "title": "Uncertainty Estimation of Transformers' Predictions via Topological Analysis of the Attention Matrices",
            "abstract": "Determining the degree of confidence of deep learning model in its prediction is an open problem in the field of natural language processing. Most of the classical methods for uncertainty estimation are quite weak for text classification models. We set the task of obtaining an uncertainty estimate for neural networks based on the Transformer architecture. A key feature of such mo-dels is the attention mechanism, which supports the information flow between the hidden representations of tokens in the neural network. We explore the formed relationships between internal representations using Topological Data Analysis methods and utilize them to predict model's confidence. In this paper, we propose a method for uncertainty estimation based on the topological properties of the attention mechanism and compare it with classical methods. As a result, the proposed algorithm surpasses the existing methods in quality and opens up a new area of application of the attention mechanism, but requires the selection of topological features.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a method for uncertainty estimation based on the topological properties of the attention mechanism and compares it with classical methods, which surpasses the existing methods in quality and opens up a new area of application of the Attention mechanism, but requires the selection of topological features."
            },
            "score": 4
        },
        {
            "id": "2e2c31fd97fc6ce27640bfc56f4b3ceca4f0cb9c",
            "paperId": "2e2c31fd97fc6ce27640bfc56f4b3ceca4f0cb9c",
            "title": "Uncertainty Estimation for Complex Text Detection in Spanish",
            "abstract": "Text simplifcation refers to the transformation of a source text aiming to increase its readiblity and understandability for a specific target population. This task is an important step towards improving inclusivity of such target populations (i.e., low scholarity or visually/hearing impaired groups). The recent advancements in the field brought by Large Language Models improve the performance of machine based text simplification approaches. However, using Language Models to simplify large text segments can be resource demanding. A more simple model to classify whether the text segment is worth to simplify or not can improve resource efficiency, in order to avoid unnecessary text prompts to the Large Language Models. Furthermore, text simplicity categorization can also be used for other purposes, such as text complexity measurement. The discrimination of text segments into simple and complex categories might lead to a number of false positives or negatives for a not well-tuned model. A way to control the acceptance threshold, is the implementation of an uncertainty score for each prediction. In this work we explore two simple uncertainty estimation approaches for complex text identification: a Monte Carlo Dropout and an Deep Ensemble Based approach. We use an in-house dataset in the financial education domain for our tests. We calibrated the two implemented methods to find out which performs better, using a Jensen-Shannon based distance between the correct and incorrect outputs of the discriminator. Our tests showed an important advantage of the Monte Carlo Dropout over the Deep Ensemble Based method.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores two simple uncertainty estimation approaches for complex text identification: a Monte Carlo Dropout and an Deep Ensemble Based approach, and calibrated the two implemented methods to find out which performs better."
            },
            "score": 4
        },
        {
            "id": "5e7274bcda47b704b6797bb14be8b7a61c047a61",
            "paperId": "5e7274bcda47b704b6797bb14be8b7a61c047a61",
            "title": "Uncertainty-Aware Evaluation for Vision-Language Models",
            "abstract": "Vision-Language Models like GPT-4, LLaVA, and CogVLM have surged in popularity recently due to their impressive performance in several vision-language tasks. Current evaluation methods, however, overlook an essential component: uncertainty, which is crucial for a comprehensive assessment of VLMs. Addressing this oversight, we present a benchmark incorporating uncertainty quantification into evaluating VLMs. Our analysis spans 20+ VLMs, focusing on the multiple-choice Visual Question Answering (VQA) task. We examine models on 5 datasets that evaluate various vision-language capabilities. Using conformal prediction as an uncertainty estimation approach, we demonstrate that the models' uncertainty is not aligned with their accuracy. Specifically, we show that models with the highest accuracy may also have the highest uncertainty, which confirms the importance of measuring it for VLMs. Our empirical findings also reveal a correlation between model uncertainty and its language model part.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that models with the highest accuracy may also have the highest uncertainty, which confirms the importance of measuring it for VLMs, and a correlation between model uncertainty and its language model part is revealed."
            },
            "score": 4
        },
        {
            "id": "4e15901eaaaa9a9c2c30f64e05054ce6f5cdaa97",
            "paperId": "4e15901eaaaa9a9c2c30f64e05054ce6f5cdaa97",
            "title": "On the Importance of Uncertainty in Decision-Making with Large Language Models",
            "abstract": "We investigate the role of uncertainty in decision-making problems with natural language as input. For such tasks, using Large Language Models as agents has become the norm. However, none of the recent approaches employ any additional phase for estimating the uncertainty the agent has about the world during the decision-making task. We focus on a fundamental decision-making framework with natural language as input, which is the one of contextual bandits, where the context information consists of text. As a representative of the approaches with no uncertainty estimation, we consider an LLM bandit with a greedy policy, which picks the action corresponding to the largest predicted reward. We compare this baseline to LLM bandits that make active use of uncertainty estimation by integrating the uncertainty in a Thompson Sampling policy. We employ different techniques for uncertainty estimation, such as Laplace Approximation, Dropout, and Epinets. We empirically show on real-world data that the greedy policy performs worse than the Thompson Sampling policies. These findings suggest that, while overlooked in the LLM literature, uncertainty plays a fundamental role in bandit tasks with LLMs.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work considers an LLM bandit with a greedy policy, which picks the action corresponding to the largest predicted reward, compared to LLM bandits that make active use of uncertainty estimation by integrating the uncertainty in a Thompson Sampling policy."
            },
            "score": 4
        },
        {
            "id": "645d8c40f2a05f0b06f9338cf7635755532d747c",
            "paperId": "645d8c40f2a05f0b06f9338cf7635755532d747c",
            "title": "Uncertainty Awareness of Large Language Models Under Code Distribution Shifts: A Benchmark Study",
            "abstract": "Large Language Models (LLMs) have been widely employed in programming language analysis to enhance human productivity. Yet, their reliability can be compromised by various code distribution shifts, leading to inconsistent outputs. While probabilistic methods are known to mitigate such impact through uncertainty calibration and estimation, their efficacy in the language domain remains underexplored compared to their application in image-based tasks. In this work, we first introduce a large-scale benchmark dataset, incorporating three realistic patterns of code distribution shifts at varying intensities. Then we thoroughly investigate state-of-the-art probabilistic methods applied to CodeLlama using these shifted code snippets. We observe that these methods generally improve the uncertainty awareness of CodeLlama, with increased calibration quality and higher uncertainty estimation~(UE) precision. However, our study further reveals varied performance dynamics across different criteria (e.g., calibration error vs misclassification detection) and trade-off between efficacy and efficiency, highlighting necessary methodological selection tailored to specific contexts.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work thoroughly investigate state-of-the-art probabilistic methods applied to CodeLlama using three realistic patterns of code distribution shifts at varying intensities, and observes that these methods generally improve the uncertainty awareness of CodeLlama, with increased calibration quality and higher uncertainty estimation~(UE) precision."
            },
            "score": 4
        },
        {
            "id": "868349fe969bc7c6b14b5f35e118a26075b7b1f2",
            "paperId": "868349fe969bc7c6b14b5f35e118a26075b7b1f2",
            "title": "How does BERT\u2019s attention change when you fine-tune? An analysis methodology and a case study in negation scope",
            "abstract": "Large pretrained language models like BERT, after fine-tuning to a downstream task, have achieved high performance on a variety of NLP problems. Yet explaining their decisions is difficult despite recent work probing their internal representations. We propose a procedure and analysis methods that take a hypothesis of how a transformer-based model might encode a linguistic phenomenon, and test the validity of that hypothesis based on a comparison between knowledge-related downstream tasks with downstream control tasks, and measurement of cross-dataset consistency. We apply this methodology to test BERT and RoBERTa on a hypothesis that some attention heads will consistently attend from a word in negation scope to the negation cue. We find that after fine-tuning BERT and RoBERTa on a negation scope task, the average attention head improves its sensitivity to negation and its attention consistency across negation datasets compared to the pre-trained models. However, only the base models (not the large models) improve compared to a control task, indicating there is evidence for a shallow encoding of negation only in the base models.",
            "year": 2020,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that after fine-tuning BERT and RoBERTa on a negation scope task, the average attention head improves its sensitivity to negation and its attention consistency across negation datasets compared to the pre-trained models, indicating there is evidence for a shallow encoding of negation only in the base models."
            },
            "score": 4
        },
        {
            "id": "c89d5766d25466830391ccecabe53e0bf2a92425",
            "paperId": "c89d5766d25466830391ccecabe53e0bf2a92425",
            "title": "DREAM: Uncovering Mental Models behind Language Models",
            "abstract": "To what extent do language models (LMs) build \u201cmental models\u201d of a scene when answering situated questions (e.g., questions about a speci\ufb01c ethical dilemma)? While cognitive science has shown that mental models play a fundamental role in human problem-solving, it is unclear whether the high question-answering performance of existing LMs is backed by similar model building - and if not, whether that can explain their well-known catastrophic failures. We observed that Macaw, an existing T5-based LM, when probed provides somewhat useful but inadequate mental models for situational questions (estimated accuracy=43%, usefulness=21%, consistency=42%). We propose DREAM, a model that takes a situational question as input to produce a mental model elaborating the situation, without any additional task spe-ci\ufb01c training data for mental models. It inherits its social commonsense through distant supervision from existing NLP resources. Our analysis shows that DREAM can produce signi\ufb01cantly better mental models (estimated accuracy=67%, usefulness=37%, consistency=71%) compared to Macaw. Finally, mental models generated by DREAM can be used as additional context for situational QA tasks. This additional context improves the answer accuracy of a Macaw zero-shot model by between +1% and +4% (absolute) on three different datasets.",
            "year": 2021,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "DREAM is proposed, a model that takes a situational question as input to produce a mental model elaborating the situation, without any additional task spe-ci\ufb01c training data for mental models, and inherits its social commonsense through distant supervision from existing NLP resources."
            },
            "score": 4
        },
        {
            "id": "f43f3aea99da056b19c6a5ee0f5aac8010c343a3",
            "paperId": "f43f3aea99da056b19c6a5ee0f5aac8010c343a3",
            "title": "Test of Time: Instilling Video-Language Models with a Sense of Time",
            "abstract": "Modelling and understanding time remains a challenge in contemporary video understanding models. With language emerging as a key driver towards powerful generalization, it is imperative for foundational video-language models to have a sense of time. In this paper, we consider a specific aspect of temporal understanding: consistency of time order as elicited by before/after relations. We establish that seven existing video-language models struggle to understand even such simple temporal relations. We then question whether it is feasible to equip these foundational models with temporal awareness without re-training them from scratch. Towards this, we propose a temporal adaptation recipe on top of one such model, VideoCLIp, based on post-pretraining on a small amount of video-text data. We conduct a zero-shot evaluation of the adapted models on six datasets for three downstream tasks which require varying degrees of time awareness. We observe encouraging performance gains especially when the task needs higher time awareness. Our work serves as a first step towards probing and instilling a sense of time in existing video-language models without the need for data and compute-intense training from scratch.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a temporal adaptation recipe on top of one video-language model, VideoCLIp, based on post-pretraining on a small amount of video-text data and conducts a zero-shot evaluation of the adapted models on six datasets for three downstream tasks which require varying degrees of time awareness."
            },
            "score": 3
        },
        {
            "id": "4726ff813876b8e420d8c635dd2354693a8dc932",
            "paperId": "4726ff813876b8e420d8c635dd2354693a8dc932",
            "title": "FakeGPT: Fake News Generation, Explanation and Detection of Large Language Models",
            "abstract": "The rampant spread of fake news has adversely affected society, resulting in extensive research on curbing its spread. As a notable milestone in large language models (LLMs), ChatGPT has gained significant attention due to its exceptional natural language processing capabilities. In this study, we present a thorough exploration of ChatGPT's proficiency in generating, explaining, and detecting fake news as follows. Generation -- We employ four prompt methods to generate fake news samples and prove the high quality of these samples through both self-assessment and human evaluation. Explanation -- We obtain nine features to characterize fake news based on ChatGPT's explanations and analyze the distribution of these factors across multiple public datasets. Detection -- We examine ChatGPT's capacity to identify fake news. We explore its detection consistency and then propose a reason-aware prompt method to improve its performance. Although our experiments demonstrate that ChatGPT shows commendable performance in detecting fake news, there is still room for its improvement. Consequently, we further probe into the potential extra information that could bolster its effectiveness in detecting fake news.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Although the authors' experiments demonstrate that ChatGPT shows commendable performance in detecting fake news, there is still room for its improvement, so a reason-aware prompt method is proposed to improve its performance."
            },
            "score": 3
        },
        {
            "id": "7f27182d8d240847345d40fbb8c14900ba8728f0",
            "paperId": "7f27182d8d240847345d40fbb8c14900ba8728f0",
            "title": "Spelling convention sensitivity in neural language models",
            "abstract": "We examine whether large neural language models, trained on very large collections of varied English text, learn the potentially long-distance dependency of British versus American spelling conventions, i.e., whether spelling is consistently one or the other within model-generated strings. In contrast to long-distance dependencies in non-surface underlying structure (e.g., syntax), spelling consistency is easier to measure both in LMs and the text corpora used to train them, which can provide additional insight into certain observed model behaviors. Using a set of probe words unique to either British or American English, we first establish that training corpora exhibit substantial (though not total) consistency. A large T5 language model does appear to internalize this consistency, though only with respect to observed lexical items (not nonce words with British/American spelling patterns). We further experiment with correcting for biases in the training data by fine-tuning T5 on synthetic data that has been debiased, and find that finetuned T5 remains only somewhat sensitive to spelling consistency. Further experiments show GPT2 to be similarly limited.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 3
        },
        {
            "id": "fd81018bc72b030545a2d3f3010f3758ec4d48c3",
            "paperId": "fd81018bc72b030545a2d3f3010f3758ec4d48c3",
            "title": "Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions",
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in various NLP tasks. However, previous works have shown these models are sensitive towards prompt wording, and few-shot demonstrations and their order, posing challenges to fair assessment of these models. As these models become more powerful, it becomes imperative to understand and address these limitations. In this paper, we focus on LLMs robustness on the task of multiple-choice questions -- commonly adopted task to study reasoning and fact-retrieving capability of LLMs. Investigating the sensitivity of LLMs towards the order of options in multiple-choice questions, we demonstrate a considerable performance gap of approximately 13% to 75% in LLMs on different benchmarks, when answer options are reordered, even when using demonstrations in a few-shot setting. Through a detailed analysis, we conjecture that this sensitivity arises when LLMs are uncertain about the prediction between the top-2/3 choices, and specific options placements may favor certain prediction between those top choices depending on the question caused by positional bias. We also identify patterns in top-2 choices that amplify or mitigate the model's bias toward option placement. We found that for amplifying bias, the optimal strategy involves positioning the top two choices as the first and last options. Conversely, to mitigate bias, we recommend placing these choices among the adjacent options. To validate our conjecture, we conduct various experiments and adopt two approaches to calibrate LLMs' predictions, leading to up to 8 percentage points improvement across different models and benchmarks.",
            "year": 2023,
            "citationCount": 30,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper investigates the sensitivity of LLMs towards the order of options in multiple-choice questions, and conjecture that this sensitivity arises when LLMs are uncertain about the prediction between the top-2/3 choices, and specific options placements may favor certain prediction between those top choices depending on the question caused by positional bias."
            },
            "score": 3
        },
        {
            "id": "28e2ecb4183ebc0eec504b12dddc677f8aef8745",
            "paperId": "28e2ecb4183ebc0eec504b12dddc677f8aef8745",
            "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation",
            "abstract": "Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.",
            "year": 2023,
            "citationCount": 51,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information, indicating that there is still a considerable journey ahead to effectively apply RAG to LLMs."
            },
            "score": 3
        },
        {
            "id": "88abef771472c3aa46c53d5d626a0d0c3b66e8cd",
            "paperId": "88abef771472c3aa46c53d5d626a0d0c3b66e8cd",
            "title": "Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness",
            "abstract": "The capability of Large Language Models (LLMs) like ChatGPT to comprehend user intent and provide reasonable responses has made them extremely popular lately. In this paper, we focus on assessing the overall ability of ChatGPT using 7 fine-grained information extraction (IE) tasks. Specially, we present the systematically analysis by measuring ChatGPT's performance, explainability, calibration, and faithfulness, and resulting in 15 keys from either the ChatGPT or domain experts. Our findings reveal that ChatGPT's performance in Standard-IE setting is poor, but it surprisingly exhibits excellent performance in the OpenIE setting, as evidenced by human evaluation. In addition, our research indicates that ChatGPT provides high-quality and trustworthy explanations for its decisions. However, there is an issue of ChatGPT being overconfident in its predictions, which resulting in low calibration. Furthermore, ChatGPT demonstrates a high level of faithfulness to the original text in the majority of cases. We manually annotate and release the test sets of 7 fine-grained IE tasks contains 14 datasets to further promote the research. The datasets and code are available at https://github.com/pkuserc/ChatGPT_for_IE.",
            "year": 2023,
            "citationCount": 87,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ChatGPT's performance in Standard-IE setting is poor, but it surprisingly exhibits excellent performance in the OpenIE setting, as evidenced by human evaluation, and indicates that ChatGPT provides high-quality and trustworthy explanations for its decisions."
            },
            "score": 3
        },
        {
            "id": "a6e35706bff827e5ad6766c6d0c259d67c7178c9",
            "paperId": "a6e35706bff827e5ad6766c6d0c259d67c7178c9",
            "title": "Probing the Robustness of Pre-trained Language Models for Entity Matching",
            "abstract": "The paradigm of fine-tuning Pre-trained Language Models (PLMs) has been successful in Entity Matching (EM). Despite their remarkable performance, PLMs exhibit tendency to learn spurious correlations from training data. In this work, we aim at investigating whether PLM-based entity matching models can be trusted in real-world applications where data distribution is different from that of training. To this end, we design an evaluation benchmark to assess the robustness of EM models to facilitate their deployment in the real-world settings. Our assessments reveal that data imbalance in the training data is a key problem for robustness. We also find that data augmentation alone is not sufficient to make a model robust. As a remedy, we prescribe simple modifications that can improve the robustness of PLM-based EM models. Our experiments show that while yielding superior results for in-domain generalization, our proposed model significantly improves the model robustness, compared to state-of-the-art EM models.",
            "year": 2022,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An evaluation benchmark to assess the robustness of EM models to facilitate their deployment in the real-world settings is designed and it is found that data imbalance in the training data is a key problem for robustness and data augmentation alone is not sufficient to make a model robust."
            },
            "score": 3
        },
        {
            "id": "acbe813244e07f32eb034d6c27547d772a995d1d",
            "paperId": "acbe813244e07f32eb034d6c27547d772a995d1d",
            "title": "Uncertainty Estimation for Language Reward Models",
            "abstract": "Language models can learn a range of capabilities from unsupervised training on text corpora. However, to solve a particular problem (such as text summarization) it is typically necessary to fine-tune them on a task-specific dataset. It is often easier for humans to choose between options than to provide labeled data, and prior work has achieved state-of-the-art performance by training a reward model from such preference comparisons. However, collecting a large preference comparison dataset is still expensive -- and the learned reward models are unreliable out-of-distribution. We seek to address these problems via uncertainty estimation, which can improve sample efficiency and robustness using active learning and risk-averse reinforcement learning (RL). Specifically, we use bootstrap aggregating (bagging) to train an ensemble of reward models differing in the initialization of their final layer. Ensembles have proved successful in prior applications of active learning, but we find that in our setting ensemble active learning does not outperform random sampling. Further experiments show that while the aggregate predictions are well-calibrated, the ensemble's estimated epistemic uncertainty is only weakly correlated with model error. We suspect this is because the ensemble members are fine-tuned from a single model and so are similar to one another. This suggests current pre-training methods will need to be modified to support uncertainty estimation, e.g. by training multiple language models.",
            "year": 2022,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that in this setting ensemble active learning does not outperform random sampling, and current pre-training methods will need to be modified to support uncertainty estimation, e.g. by training multiple language models."
            },
            "score": 3
        },
        {
            "id": "8e0dbc206db278c29c4b70eae2060db2818f72dd",
            "paperId": "8e0dbc206db278c29c4b70eae2060db2818f72dd",
            "title": "Generalization and Hallucination of Large Vision-Language Models through a Camouflaged Lens",
            "abstract": "Large Vision-Language Model (LVLM) has seen burgeoning development and increasing attention recently. In this paper, we propose a novel framework, camo-perceptive vision-language framework (CPVLF), to explore whether LVLM can generalize to the challenging camouflaged object detection (COD) scenario in a training-free manner. During the process of generalization, we find that due to hallucination issues within LVLM, it can erroneously perceive objects in camouflaged scenes, producing counterfactual concepts. Moreover, as LVLM is not specifically trained for the precise localization of camouflaged objects, it exhibits a degree of uncertainty in accurately pinpointing these objects. Therefore, we propose chain of visual perception, which enhances LVLM's perception of camouflaged scenes from both linguistic and visual perspectives, reducing the hallucination issue and improving its capability in accurately locating camouflaged objects. We validate the effectiveness of CPVLF on three widely used COD datasets, and the experiments show the potential of LVLM in the COD task.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel framework, camo-perceptive vision-language framework (CPVLF), is proposed, which enhances LVLM's perception of camouflaged scenes from both linguistic and visual perspectives, reducing the hallucination issue and improving its capability in accurately locating camouflaged objects."
            },
            "score": 3
        },
        {
            "id": "46dfd46fb41d56bc77055e436dc969de8df0e76b",
            "paperId": "46dfd46fb41d56bc77055e436dc969de8df0e76b",
            "title": "Data augmentation and the role of hardness for feature learning in NLP",
            "abstract": ". Abstract Neural models often exploit features that generalize badly in order to achieve good performance. Overcoming this tendency is a central challenge in areas such as representation learning and ML fairness. We approach this problem, in part, from the perspective of feature hardness and data augmentation. First, we construct a dataset for linguistic acceptability in which multiple, competing features might be used for prediction. We \ufb01nd that in this setting, the downstream model \u2018prefers\u2019 \u2013 to some extent \u2013 the feature that is more easily extracted from the pre-trained model. OUr preliminary results suggest that the learning of downstream tasks in natural language processing (NLP) is governed, in part, by the \u2018clarity\u2019 with which features are represented by pre-trained models. Second, we introduce a toy setting to probe the e\ufb00ectiveness of data augmentation, a widely-used strategy to prevent models from learning undesirable heuristics. Adversarial or counterfactual data augmentation involves generating training examples where these heuristics fail, in order to encourage the model to use more general features. We show that, often, the added training examples help prevent the model from adopting the targeted heuristic, but do not help it learn more general features. We also \ufb01nd in many cases that the number of adversarial examples needed to reach a given error rate is independent of the amount of training data, and that adversarial data augmentation becomes less e\ufb00ective as the number of available heuristics increases and/or as the underlying learning problem becomes more challenging. Finally, we explore several de\ufb01nitions of feature hardness in the context of the same toy setting, including: (1) the area under the classi\ufb01cation learning curve, (2) the sum of weights of a classi\ufb01cation model, (3) the minimum description length given by the probing methods of Voita and Titov [2020], and (4) the number of adversarial counterexamples that are needed to induce a model to learn the feature. We show an correspondence between these de\ufb01nitions for the features in our toy setting.",
            "year": 2020,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Preliminary results suggest that the learning of downstream tasks in natural language processing (NLP) is governed, in part, by the \u2018clarity\u2019 with which features are represented by pre-trained models."
            },
            "score": 3
        },
        {
            "id": "cef77310f326bd30b172459dbecaedf228fc7b23",
            "paperId": "cef77310f326bd30b172459dbecaedf228fc7b23",
            "title": "Probabilistic Neural-symbolic Models for Interpretable Visual Question Answering",
            "abstract": "We propose a new class of probabilistic neural-symbolic models, that have symbolic functional programs as a latent, stochastic variable. Instantiated in the context of visual question answering, our probabilistic formulation offers two key conceptual advantages over prior neural-symbolic models for VQA. Firstly, the programs generated by our model are more understandable while requiring lesser number of teaching examples. Secondly, we show that one can pose counterfactual scenarios to the model, to probe its beliefs on the programs that could lead to a specified answer given an image. Our results on the CLEVR and SHAPES datasets verify our hypotheses, showing that the model gets better program (and answer) prediction accuracy even in the low data regime, and allows one to probe the coherence and consistency of reasoning performed.",
            "year": 2019,
            "citationCount": 78,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new class of probabilistic neural-symbolic models, that have symbolic functional programs as a latent, stochastic variable, that are more understandable while requiring lesser number of teaching examples for VQA is proposed."
            },
            "score": 3
        },
        {
            "id": "3bcfd58885bda5decfbe885cbf6147147772008f",
            "paperId": "3bcfd58885bda5decfbe885cbf6147147772008f",
            "title": "One-intervention Causal Explanation for Natural Language Processing Based Alzheimer\u2019s Disease Detection (Preprint)",
            "abstract": "\n BACKGROUND\n Machine learning-based Alzheimer's detection using natural language processing has drawn increasing attention because of its low cost compared with traditional methods. However, most of these models are black-boxes, and the decision mechanisms of the AI are obscure. In some fields like medicine, this obscurity gets in the way of widespread adoption. This has led to the development of a new class of techniques that are generally referred to as explainable AI (XAI). One approach to this problem is counter-factual explanations which answer \u201cwhat if\u201d questions like \u201cWhat would have happened to Y, had I not done X?\u201d.\n \n \n OBJECTIVE\n This study aims to improve the transparency of a the-state-of-art language-based Alzheimer\u2019s disease (AD) detection model and discover linguistic biomarkers that are indicative of AD and hence can be used as tools for automated diagnosis of AD.\n \n \n METHODS\n In this paper, a new explainable artificial intelligence (XAI) method is proposed and named one-intervention counterfactual explanation (OICE). This method works on the state-of-the-art language-based, deep learning method for AD detection and provides an explanation of that method. The proposed OICE incorporates causal factors among the features used in the detection of AD, to provide more transparency of the AI\u2019s decision. This is in contrast to conventional counterfactual explanation methods which do not incorporate causal mechanisms. An understanding of causal factors can go beyond mere statistical correlation to provide a better understanding of the underlying physical phenomenon. The proposed OICE generates counterfactual explanations from a predefined deep-based structural causal model (SCM). The proposed method generated explanations of the AI\u2019s decision by only intervening on one feature at a time. Since OICE provides explanations for individual samples, we then analyze the counterfactual explanations statistically and define some metrics to quantify the effect of every feature.\n \n \n RESULTS\n We find 11 language level biomarkers for Alzheimer\u2019s disease detection such as adverb, pronoun, noun, preposition, etc. Previous work in psychology and NLP points out adverbs, pronouns, and nouns as potential biomarkers. Our study concurs. We also find new biomarkers that were not reported in previous studies, such as preposition, predeterminer, etc. Our results also reveal how these biomarkers are involved in the diagnostic process from a causal perspective. For example, an on-average 20.2% increase in predeterminer, causes determiner, verb (present particle), and grammatical particles change, resulting in flipping in the diagnosis from control to Alzheimer\u2019s disease. This implies that predeterminer is potentially a strong indicator of the individual\u2019s health and can function as a strong biomarker.\n \n \n CONCLUSIONS\n Our findings show consistency with previous works in psychology and natural language processing (NLP). Additionally, we offer a new explanation about how intervening a feature can affect the model's decisions using the pre-defined SCM.\n",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new explainable artificial intelligence (XAI) method proposed and named one-intervention counterfactual explanation (OICE), which incorporates causal factors among the features used in the detection of AD, to provide more transparency of the AI\u2019s decision."
            },
            "score": 3
        },
        {
            "id": "f20ba3ff2d08af88af691f41006b0c07b1c30f20",
            "paperId": "f20ba3ff2d08af88af691f41006b0c07b1c30f20",
            "title": "Evaluation and Enhancement of Semantic Grounding in Large Vision-Language Models",
            "abstract": "Large Vision-Language Models (LVLMs) offer remarkable benefits for a variety of vision-language tasks. However, a challenge hindering their application in real-world scenarios, particularly regarding safety, robustness, and reliability, is their constrained semantic grounding ability, which pertains to connecting language to the physical-world entities or concepts referenced in images. Therefore, a crucial need arises for a comprehensive study to assess the semantic grounding ability of widely used LVLMs. Despite the significance, sufficient investigation in this direction is currently lacking. Our work bridges this gap by designing a pipeline for generating large-scale evaluation datasets covering fine-grained semantic information, such as color, number, material, etc., along with a thorough assessment of seven popular LVLMs' semantic grounding ability. Results highlight prevalent misgrounding across various aspects and degrees. To address this issue, we propose a data-centric enhancement method that aims to improve LVLMs' semantic grounding ability through multimodal instruction tuning on fine-grained conversations. Experiments on enhanced LVLMs demonstrate notable improvements in addressing misgrounding issues.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work designs a pipeline for generating large-scale evaluation datasets covering fine-grained semantic information, such as color, number, material, etc., along with a thorough assessment of seven popular LVLMs' semantic grounding ability, to improve LVLMs' semantic grounding ability through multimodal instruction tuning on fine-grained conversations."
            },
            "score": 2
        },
        {
            "id": "b843b8540351ef887815a481760e9ce3d04f367a",
            "paperId": "b843b8540351ef887815a481760e9ce3d04f367a",
            "title": "The Da Vinci Code of Large Pre-trained Language Models: Deciphering Degenerate Knowledge Neurons",
            "abstract": "This study explores the mechanism of factual knowledge storage in pre-trained language models (PLMs). Previous research suggests that factual knowledge is stored within multi-layer perceptron weights, and some storage units exhibit degeneracy, referred to as Degenerate Knowledge Neurons (DKNs). This paper provides a comprehensive definition of DKNs that covers both structural and functional aspects, pioneering the study of structures in PLMs' factual knowledge storage units. Based on this, we introduce the Neurological Topology Clustering method, which allows the formation of DKNs in any numbers and structures, leading to a more accurate DKN acquisition. Furthermore, we introduce the Neuro-Degeneracy Analytic Analysis Framework, which uniquely integrates model robustness, evolvability, and complexity for a holistic assessment of PLMs. Within this framework, our execution of 34 experiments across 2 PLMs, 4 datasets, and 6 settings highlights the critical role of DKNs. The code will be available soon.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A comprehensive definition of Degenerate Knowledge Neurons is provided that covers both structural and functional aspects, pioneering the study of structures in PLMs' factual knowledge storage units."
            },
            "score": 2
        },
        {
            "id": "142615b9b1a9ac6c3dd2781e1ec4dcf3322ca33d",
            "paperId": "142615b9b1a9ac6c3dd2781e1ec4dcf3322ca33d",
            "title": "Physically constrained eigenspace perturbation for turbulence model uncertainty estimation",
            "abstract": "Aerospace design is increasingly incorporating design under uncertainty-based approaches to lead to more robust and reliable optimal designs. These approaches require dependable estimates of uncertainty in simulations for their success. The key contributor of predictive uncertainty in computational fluid dynamics (CFD) simulations of turbulent flows are the structural limitations of Reynolds-averaged Navier\u2013Stokes models, termed model-form uncertainty. Currently, the common procedure to estimate turbulence model-form uncertainty is the eigenspace perturbation framework (EPF), involving perturbations to the modeled Reynolds stress tensor within physical limits. The EPF has been applied with success in design and analysis tasks in numerous prior works from the industry and academia. Owing to its rapid success and adoption in several commercial and open-source CFD solvers, in-depth verification and validation of the EPF is critical. In this work, we show that under certain conditions, the perturbations in the EPF can lead to Reynolds stress dynamics that are not physically realizable. This analysis enables us to propose a set of necessary physics-based constraints, leading to a realizable EPF. We apply this constrained procedure to the illustrative test case of a converging-diverging channel, and we demonstrate that these constraints limit physically implausible dynamics of the Reynolds stress tensor, while enhancing the accuracy and stability of the uncertainty estimation procedure.",
            "year": 2023,
            "citationCount": 1,
            "tldr": null,
            "score": 2
        },
        {
            "id": "31852f9fc732c0868af12d631c72693702d80521",
            "paperId": "31852f9fc732c0868af12d631c72693702d80521",
            "title": "Text Data Augmentation for Deep Learning",
            "abstract": null,
            "year": 2021,
            "citationCount": 258,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The major motifs of Data Augmentation are summarized into strengthening local decision boundaries, brute force training, causality and counterfactual examples, and the distinction between meaning and form."
            },
            "score": 2
        },
        {
            "id": "427edd6d486a823594eb8fabf20c5a99fee9b371",
            "paperId": "427edd6d486a823594eb8fabf20c5a99fee9b371",
            "title": "Consistency Cubes: a fast, efficient method for exact Boolean minimization",
            "abstract": "A lot of effort has been spent over the past few decades in the QCA methodology field, to develop efficient Boolean minimization algorithms to derive an exact, and more importantly complete list of minimal prime implicants that explain the initial, observed positive configurations. As the complexity grows exponentially with every new condition, the required computer memory goes past the current computer resources and the polynomial time required to solve this problem quickly grows towards infinity. This paper introduces a new alternative to the existing non-polynomial attempts. It completely solves the memory problem, and preliminary tests show it is exponentially hundreds of time faster than eQMC, the current \u201cbest\u201d algorithm for QCA in R, and probes into a territory where it competes and even outperforms engineering algorithms such as Espresso, for exact minimizations. While speed is not much of an issue now (eQMC is fast enough for simple data), it might prove to be essential when further developing towards all possible temporal orders, or searching for configurations in panel data over time, combined with / or automatic detection of difficult counterfactuals etc.",
            "year": 2018,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a new alternative to the existing non-polynomial attempts that completely solves the memory problem, and preliminary tests show it is exponentially hundreds of time faster than eQMC, the current \u201cbest\u201d algorithm for QCA in R, and probes into a territory where it competes and even outperforms engineering algorithms such as Espresso, for exact minimizations."
            },
            "score": 2
        },
        {
            "id": "4b1f9a2dfe9d3afd8ef02c5978202f0038f841bd",
            "paperId": "4b1f9a2dfe9d3afd8ef02c5978202f0038f841bd",
            "title": "How good are your counterfactuals? Assessing quantitative macro-comparative welfare state research with qualitative criteria",
            "abstract": "All causal statements based on historical data \u2014 both in qualitative and quantitative social research \u2014 rely on counterfactuals. In quantitative research, scholars attempt to arrive at valid counterfactuals by emulating an experimental design. However, because of treatments that are impossible to manipulate and the non-random assignment of data to treatment and control groups, causal statements are often based on invalid counterfactuals. In qualitative research, scholars attempt to arrive at valid counterfactuals by probing the historical and logical consistency of counterfactuals and by acknowledging the interconnectedness of events. Criteria to evaluate counterfactuals have been developed that allow for a discussion of the quality of counterfactuals used in causal statements. In this article, we suggest using these qualitative criteria to evaluate counterfactuals in quantitative macro-comparative welfare state research. We argue that these criteria can help us identify erroneous causal inferences in quantitative research based on historical data.",
            "year": 2011,
            "citationCount": 33,
            "tldr": null,
            "score": 2
        },
        {
            "id": "dca418f0d7c3be44a9ee4ad6b8f373cbae6b13bb",
            "paperId": "dca418f0d7c3be44a9ee4ad6b8f373cbae6b13bb",
            "title": "11. Validation strategies in historical sociology (and beyond) *",
            "abstract": "Validation is a linchpin of scientific rigor. Claims relying on arguments by assertion, embedding themselves in self-validating discursive setups, or dodging critical assessments undercut the prospect of sound and cumulative knowledge. A significant stake is therefore attached to clear-cut validation yardsticks. For inquiries that embed their object in history\u2014namely, they identify these objects by reference to chronological coordinates\u2014and that aim for generalizable claims, the issue deserves close attention. Inquiries of that kind are exposed to divergent epistemic injunctions. While the historical dimension of their object compels them to attend to the specifics of their case, their social-scientific ambition, on the other hand, requires that they probe claims abstracted from empirical specificities. Furthermore, a quick perusal of how they proceed to validate their claims reveals a quite disparate, if not fragmented, landscape. Unless we sort out the ins and outs of these ways of proceeding, it is unclear how we might achieve a sense of shared standards as this Handbook advocates (see the chapter by Raub, De Graaf & G\u00ebrxhani on rigorous sociology). Accordingly, this chapter seeks to systematize our knowledge of validation practices in historical sociology. It does so by identifying the operating principles of seven distinct validation designs or strategies, and by outlining the challenges each faces. (1) The descriptive fit design denotes the use of empirical evidence to document a matter-offact claim preceding causal interpretation. (2) A strategy in terms of observable implications probes empirical predictions derived from an explanatory argument. (3) Counterfactuals validate causal claims by assessing the effects of a counter-to-the-fact condition. (4) The natural experiment design takes advantage of exogenous sources of variation bequeathed by history in order to ground causal hypotheses. (5) Inductive comparisons certify causal inferences based on the comparison of attributes across cases in light of formal criteria of consistency and parsimony. (6) Process tracing validates claims about mechanisms by documenting the process whereby change is effectuated. (7) Simulation adds to the plausibility of such claims through the computational modeling of interactions. Validation strategies in historical sociology (and beyond)",
            "year": 2022,
            "citationCount": 1,
            "tldr": null,
            "score": 2
        },
        {
            "id": "fb09f01fa3f8182908d8e35bd0acc8cbe44d6513",
            "paperId": "fb09f01fa3f8182908d8e35bd0acc8cbe44d6513",
            "title": "On the Robustness of Conceptual Rainfall\u2010Runoff Models to Calibration and Evaluation Data Set Splits Selection: A Large Sample Investigation",
            "abstract": "Conceptual rainfall\u2010runoff (CRR) models are widely used for runoff simulation and for prediction under a changing climate. The models are often calibrated with only a portion of all available data at a location and then evaluated independently with another part of the data for reliability assessment. Previous studies report a persistent decrease in CRR model performance when applying the calibrated model to the evaluation data. However, there remains a lack of comprehensive understanding about the nature of this \u201clow transferability\u201d problem and why it occurs. In this study we employ a large sample approach to investigate the robustness of CRR models across calibration/validation data splits. Specially, we investigate (1) how robust is CRR model performance across calibration/evaluation data splits, at catchments with a wide range of hydroclimatic conditions; and (2) is the robustness of model performance somehow related to the hydroclimatic characteristics of a catchment? We apply three widely used CRR models, GR4J, AWBM and IHACRE_CMD, to 163 Australian catchments having long\u2010term historical data. Each model was calibrated and evaluated at each catchment, using a large number of data splits, resulting in a total of 929,160 calibrated models. Results show that (1) model performance generally exhibits poor robustness across calibration/evaluation data splits and (2) lower model robustness is correlated with specific catchment characteristics, such as higher runoff skewness and aridity, highly variable baseflow contribution, and lower rainfall\u2010runoff ratio. These results provide a valuable benchmark for future model robustness assessments and useful guidance for model calibration and evaluation.",
            "year": 2020,
            "citationCount": 26,
            "tldr": null,
            "score": 1
        }
    ],
    "novelty": "yes"
}