{
    "topic_description": "novel prompting methods that can better quantify uncertainty or calibrate the confidence of large language models",
    "idea_name": "Multitask Uncertainty Prompting",
    "raw_idea": {
        "Problem": "LLMs often struggle to provide calibrated uncertainty estimates across diverse tasks and domains, as they may overfit to the distributional characteristics of individual datasets or tasks.",
        "Existing Methods": "Current approaches for multitask uncertainty estimation in LLMs include parameter-efficient fine-tuning methods, task-conditioning techniques, and meta-learning algorithms. However, these methods often require extensive training data or computationally expensive fine-tuning.",
        "Motivation": "By jointly prompting LLMs across multiple diverse tasks and domains, we can encourage the model to learn more generalizable and calibrated uncertainty estimates that transfer across different contexts. Multitask prompting can also help to regularize the model's uncertainty estimates and prevent overfitting to specific datasets or tasks.",
        "Proposed Method": "We propose Multitask Uncertainty Prompting (MUP), a prompting method that jointly elicits uncertainty estimates from LLMs across multiple diverse tasks and domains. The key steps are: 1) Construct a diverse set of prompts spanning different tasks (e.g., QA, NLI, summarization) and domains (e.g., science, history, literature). 2) For each prompt, generate a set of subquestions or subprompts that probe different aspects of the model's uncertainty (e.g., \"What are the key assumptions behind your answer?\" or \"How might your response change if X was different?\"). 3) Prompt the model to generate responses and uncertainty estimates for each subprompt. 4) Aggregate the uncertainty estimates across the subprompts and tasks to obtain a calibrated overall uncertainty score. 5) Fine-tune the prompts and subprompts based on the model's calibration performance across the diverse tasks.",
        "Experiment Plan": "Evaluate MUP on a diverse set of benchmark datasets spanning multiple tasks and domains, such as GLUE, SuperGLUE, and MultiNLI. Compare against single-task prompting baselines as well as multitask fine-tuning approaches. Metrics include overall calibration error, domain-specific calibration, and accuracy-uncertainty correlation."
    },
    "full_experiment_plan": {
        "Title": "Multitask Uncertainty Prompting: Improving Calibration and Generalization of Language Model Uncertainty Estimates",
        "Problem Statement": "Large Language Models (LLMs) often struggle to provide well-calibrated uncertainty estimates across diverse tasks and domains, as they may overfit to the distributional characteristics of individual datasets or tasks. This can lead to overconfident predictions and hinder the reliability and trustworthiness of LLMs in real-world applications.",
        "Motivation": "Existing approaches for multitask uncertainty estimation in LLMs, such as parameter-efficient fine-tuning, task-conditioning, and meta-learning, often require extensive training data or computationally expensive fine-tuning. We propose a novel prompting-based method, Multitask Uncertainty Prompting (MUP), which jointly elicits uncertainty estimates from LLMs across diverse tasks and domains. By exposing the model to a variety of contexts and probing its uncertainty through targeted subquestions, MUP aims to regularize the model's uncertainty estimates and improve their calibration and generalization.",
        "Proposed Method": "Multitask Uncertainty Prompting (MUP) consists of the following key steps:\n1. Construct a diverse set of prompts spanning different tasks (e.g., QA, NLI, summarization) and domains (e.g., science, history, literature).\n2. For each prompt, generate a set of subquestions or subprompts that probe different aspects of the model's uncertainty (e.g., \"What are the key assumptions behind your answer?\" or \"How might your response change if X was different?\").\n3. Prompt the model to generate responses and uncertainty estimates for each subprompt.\n4. Aggregate the uncertainty estimates across the subprompts and tasks to obtain a calibrated overall uncertainty score.\n5. Fine-tune the prompts and subprompts based on the model's calibration performance across the diverse tasks.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "Select a diverse set of benchmark datasets spanning multiple tasks and domains, such as GLUE (NLI, sentiment analysis), SuperGLUE (QA, coreference resolution), SQuAD (reading comprehension), CNN/DailyMail (summarization), and MNLI (NLI). Aim for a mix of datasets that cover different levels of difficulty and domain specificity.",
            "Step 2: Construct Prompts": "For each dataset, create a set of prompts that capture the task format and domain characteristics. For example, for SQuAD, a prompt could be: \"Answer the following question based on the given context: [question] Context: [passage]\". Aim to create at least 5-10 prompts per dataset, covering different question types, lengths, and difficulty levels.",
            "Step 3: Generate Subprompts": "For each prompt, generate a set of subprompts that probe the model's uncertainty. Examples include:\n- \"What are the key assumptions you made in your answer?\"\n- \"How confident are you in your response on a scale of 1-5?\"\n- \"What additional information would help you provide a more accurate answer?\"\n- \"How might your response change if [aspect of the input] was different?\"\nAim to create 3-5 subprompts per prompt, covering different dimensions of uncertainty.",
            "Step 4: Prompt the Model": "For each combination of prompt and subprompt, query the LLM to generate a response and an uncertainty estimate. Experiment with different uncertainty estimation methods, such as:\n- Softmax probability of the predicted output\n- Entropy of the output distribution\n- Variance across multiple sampling runs\n- Explicit uncertainty scores generated by the model (e.g., \"I'm 80% confident in my answer.\")",
            "Step 5: Aggregate Uncertainty Estimates": "For each prompt, aggregate the uncertainty estimates across the subprompts to obtain an overall uncertainty score. Experiment with different aggregation methods, such as:\n- Average or median of the subprompt uncertainty scores\n- Weighted average based on the relevance or specificity of each subprompt\n- Learned aggregation function (e.g., a small MLP) that takes the subprompt uncertainties as input",
            "Step 6: Evaluate Calibration": "Evaluate the calibration of the aggregated uncertainty estimates using metrics such as:\n- Expected Calibration Error (ECE)\n- Maximum Calibration Error (MCE)\n- Brier Score\n- Negative Log Likelihood (NLL)\nCompare the calibration performance of MUP against single-task prompting baselines and existing multitask uncertainty estimation methods.",
            "Step 7: Fine-tune Prompts": "Based on the calibration evaluation, fine-tune the prompts and subprompts to improve the model's uncertainty estimation. This can involve:\n- Modifying the wording or specificity of the prompts and subprompts\n- Adding or removing subprompts based on their contribution to calibration\n- Adjusting the aggregation method or weights\nRepeat steps 4-7 for multiple iterations to refine the prompts and improve calibration.",
            "Step 8: Evaluate Generalization": "Evaluate the generalization of the fine-tuned MUP prompts to new tasks and domains not seen during fine-tuning. Select a set of held-out datasets (e.g., from different domains or task types) and evaluate the calibration performance of MUP on these datasets. Compare against single-task prompting baselines and existing multitask uncertainty estimation methods.",
            "Step 9: Analyze Results": "Analyze the results to gain insights into the effectiveness of MUP for improving calibration and generalization of LLM uncertainty estimates. Consider factors such as:\n- The impact of prompt diversity and specificity on calibration\n- The contribution of different subprompts and aggregation methods\n- The trade-off between calibration and accuracy\n- The robustness of MUP to domain shift and task transfer\nUse these insights to refine the MUP methodology and propose future directions for research."
        },
        "Test Case Examples": {
            "Example 1: Baseline (Single-Task Prompting)": {
                "Input": "Question: What is the capital of France?\nContext: France is a country in Western Europe. It is known for its rich history, cultural heritage, and iconic landmarks such as the Eiffel Tower and the Louvre Museum.",
                "Expected Output": "Paris",
                "Uncertainty Estimate": "0.95"
            },
            "Example 1: MUP": {
                "Input": "Question: What is the capital of France?\nContext: France is a country in Western Europe. It is known for its rich history, cultural heritage, and iconic landmarks such as the Eiffel Tower and the Louvre Museum.\nSubprompt 1: What are the key assumptions you made in your answer?\nSubprompt 2: How confident are you in your response on a scale of 1-5?\nSubprompt 3: What additional information would help you provide a more accurate answer?",
                "Expected Output": "Paris\nSubprompt 1: I assumed that the capital of France has not changed recently and that the given context does not contain any contradictory information.\nSubprompt 2: 5\nSubprompt 3: I don't need any additional information to answer this question confidently, as the capital of France is a well-known fact.",
                "Aggregated Uncertainty": "0.98",
                "Explanation": "MUP elicits more comprehensive uncertainty estimates by probing different aspects of the model's reasoning process. The subprompts help to validate the assumptions, quantify the confidence level, and assess the sufficiency of the given information. The aggregated uncertainty score is higher than the single-task baseline, indicating improved calibration."
            },
            "Example 2: Baseline (Single-Task Prompting)": {
                "Input": "Question: What is the main theme of the poem \"The Road Not Taken\" by Robert Frost?\nContext: \"The Road Not Taken\" is a poem by Robert Frost, published in 1916 as the first poem in his collection \"Mountain Interval\".",
                "Expected Output": "The main theme of \"The Road Not Taken\" is the significance of the choices we make in life and the paths we choose to follow.",
                "Uncertainty Estimate": "0.80"
            },
            "Example 2: MUP": {
                "Input": "Question: What is the main theme of the poem \"The Road Not Taken\" by Robert Frost?\nContext: \"The Road Not Taken\" is a poem by Robert Frost, published in 1916 as the first poem in his collection \"Mountain Interval\".\nSubprompt 1: What are the key assumptions you made in your answer?\nSubprompt 2: How confident are you in your response on a scale of 1-5?\nSubprompt 3: What additional information would help you provide a more accurate answer?",
                "Expected Output": "The main theme of \"The Road Not Taken\" is the significance of the choices we make in life and the paths we choose to follow.\nSubprompt 1: I assumed that the poem is open to interpretation and that the context provided does not contain a definitive answer to the question.\nSubprompt 2: 3\nSubprompt 3: Having more information about the specific content and symbolism used in the poem would help me provide a more accurate and nuanced interpretation of its main theme.",
                "Aggregated Uncertainty": "0.60",
                "Explanation": "MUP helps to uncover the model's uncertainty by probing the assumptions, confidence level, and the need for additional information. In this case, the model acknowledges the interpretive nature of the question and the limited context provided, resulting in a lower aggregated uncertainty score compared to the single-task baseline. This indicates that MUP can effectively capture the model's uncertainty in more subjective or open-ended domains."
            }
        },
        "Fallback Plan": "If the proposed MUP method does not significantly improve the calibration and generalization of LLM uncertainty estimates compared to the baselines, consider the following fallback options:\n1. Analyze the generated subprompts and responses to identify potential weaknesses or inconsistencies in the prompting strategy. Refine the subprompts to better capture different aspects of uncertainty and reasoning.\n2. Experiment with alternative uncertainty estimation methods, such as Bayesian approximation techniques (e.g., Monte Carlo dropout) or ensemble-based approaches, in combination with MUP.\n3. Investigate the impact of different prompt formats, such as question-answering, completion, or dialogue-based prompts, on the effectiveness of MUP. Adapt the prompting strategy to better suit the characteristics of each task and domain.\n4. Collect human annotations of uncertainty on a subset of the test cases to validate the model's uncertainty estimates and calibrate the aggregation method. Use these annotations to guide the fine-tuning process and improve the interpretability of the uncertainty scores.\n5. Conduct a thorough error analysis to identify common patterns or factors contributing to the model's uncertainty miscalibration. Use these insights to develop targeted interventions or data augmentation techniques to address specific challenges.\n6. If the MUP method consistently underperforms the baselines, consider pivoting the project to focus on analyzing the limitations and trade-offs of existing uncertainty estimation approaches for LLMs. Conduct a systematic comparison of different methods across various tasks and domains to provide valuable insights for future research directions."
    },
    "novelty_queries": [
        "KeywordQuery(\"multitask uncertainty prompting language models\")",
        "KeywordQuery(\"calibration generalization uncertainty estimation language models\")",
        "KeywordQuery(\"prompt-based uncertainty language models diverse tasks domains\")",
        "KeywordQuery(\"Multitask Uncertainty Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "ad402080a4aa66ef3c57a46ce4685a47a3cc0a61",
            "paperId": "ad402080a4aa66ef3c57a46ce4685a47a3cc0a61",
            "title": "Quantifying Uncertainty in Natural Language Explanations of Large Language Models",
            "abstract": "Large Language Models (LLMs) are increasingly used as powerful tools for several high-stakes natural language processing (NLP) applications. Recent prompting works claim to elicit intermediate reasoning steps and key tokens that serve as proxy explanations for LLM predictions. However, there is no certainty whether these explanations are reliable and reflect the LLMs behavior. In this work, we make one of the first attempts at quantifying the uncertainty in explanations of LLMs. To this end, we propose two novel metrics -- $\\textit{Verbalized Uncertainty}$ and $\\textit{Probing Uncertainty}$ -- to quantify the uncertainty of generated explanations. While verbalized uncertainty involves prompting the LLM to express its confidence in its explanations, probing uncertainty leverages sample and model perturbations as a means to quantify the uncertainty. Our empirical analysis of benchmark datasets reveals that verbalized uncertainty is not a reliable estimate of explanation confidence. Further, we show that the probing uncertainty estimates are correlated with the faithfulness of an explanation, with lower uncertainty corresponding to explanations with higher faithfulness. Our study provides insights into the challenges and opportunities of quantifying uncertainty in LLM explanations, contributing to the broader discussion of the trustworthiness of foundation models.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes two novel metrics -- verbalized uncertainty and probing uncertainty -- to quantify the uncertainty of generated explanations of large Language Models, and shows that the probing uncertainty estimates are correlated with the faithfulness of an explanation, with lower uncertainty corresponding to explanations with higher faithfulness."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the calibration and generalization of uncertainty estimates in large language models (LLMs) across diverse tasks and domains using a novel prompting-based method called Multitask Uncertainty Prompting (MUP). The paper, on the other hand, focuses on quantifying the uncertainty in natural language explanations generated by LLMs using two novel metrics: Verbalized Uncertainty and Probing Uncertainty.\n\nWhile both the project proposal and the paper deal with uncertainty in LLMs, their research problems and approaches are different. The project proposal targets improving uncertainty estimates for LLM predictions, while the paper aims to quantify uncertainty in LLM-generated explanations. The project proposal introduces a prompting-based method (MUP) for multitask uncertainty estimation, whereas the paper proposes metrics to measure uncertainty in explanations.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "5424e311319c58847b4c690d5c91090e3b6a4ac3",
            "paperId": "5424e311319c58847b4c690d5c91090e3b6a4ac3",
            "title": "Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models",
            "abstract": "While Large Language Models (LLMs) have demonstrated remarkable potential in natural language generation and instruction following, a persistent challenge lies in their susceptibility to\"hallucinations\", which erodes trust in their outputs. Although Uncertainty Quantification (UQ) presents a promising solution, its accurate implementation within the context of LLMs remains a significant hurdle. To address this critical roadblock, our research originates from a fundamental heuristic insight: tokens within auto-regressive LLM-generated text do not equally reflect the underlying meaning. Some tokens carry greater relevance and representativeness than others, owing to the phenomenon of\"linguistic redundancy\", wherein a select few keywords suffice to convey the essence of lengthy sentences. Regrettably, existing methodologies treat all tokens with equal importance when estimating uncertainty, disregarding these inherent generative inequalities. Our analysis reveals a significant issue with state-of-the-art: numerous tokens (and sentences) of limited semantic significance receive equal or even excessive weighting during uncertainty estimation. To rectify this bias, we propose to jointly Shifting Attention to more Relevant (SAR) components, at both the token- and the sentence-levels for accurate uncertainty estimation. We conduct extensive experiments involving a range of popular\"off-the-shelf\"LLMs, including instruction-tuned LLMs such as Vicuna, WizardLM, and LLaMA-2-chat, as well as pretrained LLMs like OPT and LLaMA, with model sizes extending up to 33B parameters. We carry out evaluation across various free-form question-answering tasks, encompassing domains such as reading comprehension, science Q&A, and medical Q&A. Our experimental results demonstrate the superior performance of SAR in addressing the challenges of uncertainty estimation within the realm of LLMs.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results demonstrate the superior performance of SAR in addressing the challenges of uncertainty estimation within the realm of LLMs, and propose to jointly Shifting Attention to more Relevant (SAR) components, at both the token- and the sentence-levels for accurate uncertainty estimation."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the calibration and generalization of uncertainty estimates in large language models (LLMs) across diverse tasks and domains. The proposed approach is Multitask Uncertainty Prompting (MUP), which constructs diverse prompts and subprompts to jointly elicit uncertainty estimates from LLMs.\n\nThe research problem in the paper is accurately estimating the uncertainty of LLMs, particularly addressing their susceptibility to \"hallucinations\". The proposed approach is Shifting Attention to Relevance (SAR), which focuses on more relevant components at both token and sentence levels for uncertainty estimation.\n\nWhile both works aim to improve uncertainty estimation in LLMs, the proposal focuses on calibration and generalization across tasks using prompting, while the paper addresses the issue of hallucinations by shifting attention to relevant components. The approaches differ in their focus and methodology.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "507465f8d46489a68a527cb5304d76bdb6c31ed9",
            "paperId": "507465f8d46489a68a527cb5304d76bdb6c31ed9",
            "title": "Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation",
            "abstract": "We introduce a method to measure uncertainty in large language models. For tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models. We show that measuring uncertainty in natural language is challenging because of\"semantic equivalence\"-- different sentences can mean the same thing. To overcome these challenges we introduce semantic entropy -- an entropy which incorporates linguistic invariances created by shared meanings. Our method is unsupervised, uses only a single model, and requires no modifications to off-the-shelf language models. In comprehensive ablation studies we show that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines.",
            "year": 2023,
            "citationCount": 85,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "In comprehensive ablation studies, it is shown that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the calibration and generalization of uncertainty estimates in large language models (LLMs) across diverse tasks and domains using a novel prompting-based method called Multitask Uncertainty Prompting (MUP). The paper introduces a method to measure uncertainty in LLMs for tasks like question answering by incorporating linguistic invariances created by shared meanings, using an unsupervised approach called semantic entropy.\n\nWhile both the project proposal and the paper address the problem of uncertainty estimation in LLMs, the project proposal focuses on improving calibration and generalization across tasks using a prompting-based approach, while the paper introduces a specific unsupervised method for measuring uncertainty in question answering tasks by considering semantic equivalence.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "ea0d41514a41f8273f13b3b277e7fcbbc65a8549",
            "paperId": "ea0d41514a41f8273f13b3b277e7fcbbc65a8549",
            "title": "Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models",
            "abstract": "The recent performance leap of Large Language Models (LLMs) opens up new opportunities across numerous industrial applications and domains. However, erroneous generations, such as false predictions, misinformation, and hallucination made by LLMs, have also raised severe concerns for the trustworthiness of LLMs', especially in safety-, security- and reliability-sensitive scenarios, potentially hindering real-world adoptions. While uncertainty estimation has shown its potential for interpreting the prediction risks made by general machine learning (ML) models, little is known about whether and to what extent it can help explore an LLM's capabilities and counteract its undesired behavior. To bridge the gap, in this paper, we initiate an exploratory study on the risk assessment of LLMs from the lens of uncertainty. In particular, we experiment with twelve uncertainty estimation methods and four LLMs on four prominent natural language processing (NLP) tasks to investigate to what extent uncertainty estimation techniques could help characterize the prediction risks of LLMs. Our findings validate the effectiveness of uncertainty estimation for revealing LLMs' uncertain/non-factual predictions. In addition to general NLP tasks, we extensively conduct experiments with four LLMs for code generation on two datasets. We find that uncertainty estimation can potentially uncover buggy programs generated by LLMs. Insights from our study shed light on future design and development for reliable LLMs, facilitating further research toward enhancing the trustworthiness of LLMs.",
            "year": 2023,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An exploratory study on the risk assessment of LLMs from the lens of uncertainty is initiated, finding that uncertainty estimation can potentially uncover buggy programs generated by LLMs."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the calibration and generalization of uncertainty estimates in large language models (LLMs) across diverse tasks and domains. The proposed approach is Multitask Uncertainty Prompting (MUP), which constructs prompts and subprompts to jointly elicit uncertainty estimates from LLMs.\n\nThe research problem in the paper is investigating the effectiveness of uncertainty estimation methods for assessing the prediction risks of LLMs, such as false predictions, misinformation, and hallucination. The approach is experimenting with twelve uncertainty estimation methods on four LLMs and four NLP tasks, as well as code generation.\n\nWhile both the proposal and the paper focus on uncertainty estimation for LLMs, the specific research problems and approaches differ. The proposal aims to improve the calibration and generalization of uncertainty estimates using a novel prompting method, while the paper explores the effectiveness of existing uncertainty estimation methods for risk assessment and error detection in LLMs.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "551b05734eb2181c4ca009a411144e8447ed1606",
            "paperId": "551b05734eb2181c4ca009a411144e8447ed1606",
            "title": "Uncertainty Quantification with Pre-trained Language Models: A Large-Scale Empirical Analysis",
            "abstract": "Pre-trained language models (PLMs) have gained increasing popularity due to their compelling prediction performance in diverse natural language processing (NLP) tasks. When formulating a PLM-based prediction pipeline for NLP tasks, it is also crucial for the pipeline to minimize the calibration error, especially in safety-critical applications. That is, the pipeline should reliably indicate when we can trust its predictions. In particular, there are various considerations behind the pipeline: (1) the choice and (2) the size of PLM, (3) the choice of uncertainty quantifier, (4) the choice of fine-tuning loss, and many more. Although prior work has looked into some of these considerations, they usually draw conclusions based on a limited scope of empirical studies. There still lacks a holistic analysis on how to compose a well-calibrated PLM-based prediction pipeline. To fill this void, we compare a wide range of popular options for each consideration based on three prevalent NLP classification tasks and the setting of domain shift. In response, we recommend the following: (1) use ELECTRA for PLM encoding, (2) use larger PLMs if possible, (3) use Temp Scaling as the uncertainty quantifier, and (4) use Focal Loss for fine-tuning.",
            "year": 2022,
            "citationCount": 38,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A wide range of popular options for each consideration are compared based on three prevalent NLP classification tasks and the setting of domain shift to form a holistic analysis on how to compose a well-calibrated PLM-based prediction pipeline."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the calibration and generalization of uncertainty estimates in large language models (LLMs) across diverse tasks and domains. The proposed approach is Multitask Uncertainty Prompting (MUP), which uses prompts and subprompts to elicit uncertainty estimates from LLMs and aggregates them to obtain calibrated scores.\n\nThe research problem in the paper is analyzing how different choices in a PLM-based prediction pipeline, such as the choice and size of PLM, uncertainty quantifier, and fine-tuning loss, affect the calibration error in NLP classification tasks, especially under domain shift.\n\nWhile both the proposal and the paper focus on uncertainty quantification and calibration in language models, their specific research problems and approaches differ. The proposal aims to improve calibration and generalization using a novel prompting method, while the paper conducts an empirical analysis of various factors influencing calibration in PLM-based pipelines.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "0aa5940fda7c994675d08c41eca2a6909eb6d205",
            "paperId": "0aa5940fda7c994675d08c41eca2a6909eb6d205",
            "title": "Improving the Reliability of Large Language Models by Leveraging Uncertainty-Aware In-Context Learning",
            "abstract": "In recent years, large-scale language models (LLMs) have gained attention for their impressive text generation capabilities. However, these models often face the challenge of\"hallucination,\"which undermines their reliability. In this study, we introduce an uncertainty-aware in-context learning framework to empower the model to enhance or reject its output in response to uncertainty. Human-defined methods for estimating uncertainty typically assume that\"uncertainty is lower when the model's response is correct compared to when it is incorrect.\"However, setting a precise threshold to distinguish correctness is challenging. Therefore, we introduce uncertainty information as an intermediary variable that implicitly influences the model's behavior. Our innovative uncertainty-aware in-context learning framework involves fine-tuning the LLM using a calibration dataset. Our aim is to improve the model's responses by filtering out answers with high uncertainty while considering the model's knowledge limitations. We evaluate the model's knowledge by examining multiple responses to the same question for the presence of a correct answer. When the model lacks relevant knowledge, the response should indicate that the question cannot be answered. Conversely, when the model has relevant knowledge, the response should provide the correct answer. Extensive experiments confirm the effectiveness of our framework, leading to two key findings. First, the logit output values of the LLM partly reflect inherent uncertainty. Second, our model autonomously recognizes uncertainty, resulting in improved responses.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study introduces an uncertainty-aware in-context learning framework to empower the model to enhance or reject its output in response to uncertainty, and introduces uncertainty information as an intermediary variable that implicitly influences the model's behavior."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the calibration and generalization of uncertainty estimates in large language models (LLMs) across diverse tasks and domains. The proposed approach is Multitask Uncertainty Prompting (MUP), which involves constructing diverse prompts and subprompts to probe the model's uncertainty and aggregating the uncertainty estimates across tasks.\n\nThe research problem in the paper is improving the reliability of LLMs by addressing the issue of \"hallucination.\" The proposed approach is an uncertainty-aware in-context learning framework that fine-tunes the LLM using a calibration dataset to enhance or reject its output based on uncertainty.\n\nWhile both the proposal and the paper aim to improve the reliability of LLMs, they focus on different aspects. The proposal addresses the calibration and generalization of uncertainty estimates across tasks, while the paper focuses on reducing hallucination by leveraging uncertainty information during in-context learning. The approaches also differ, with the proposal using prompting and the paper using fine-tuning with a calibration dataset.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "1fa4469e5bc5d096572902fe14b0d66078a24c47",
            "paperId": "1fa4469e5bc5d096572902fe14b0d66078a24c47",
            "title": "Navigating the Grey Area: Expressions of Overconfidence and Uncertainty in Language Models",
            "abstract": "Despite increasingly \ufb02uent, relevant, and coherent language generation, major gaps remain between how humans and machines use language. We argue that a key dimension that is missing from our understanding of language models (LMs) is the model\u2019s ability to interpret and generate expressions of uncertainty . Whether it be the weatherperson announcing a chance of rain or a doctor giving a diagnosis, information is often not black-and-white and expressions of uncertainty provide nuance to support human-decision making. The increasing deployment of LMs in the wild motivates us to investigate whether LMs are capable of interpreting expressions of uncertainty and how LMs\u2019 behaviors change when learning to emit their own expressions of uncertainty. When injecting expressions of uncertainty into prompts (e.g., \"I think the answer is...\"), we discover that GPT3\u2019s generations vary upwards of 80% in accuracy based on the expression used. We analyze the linguistic characteristics of these expressions and \ufb01nd a drop in accuracy when naturalistic expressions of certainty are present. We \ufb01nd similar effects when teaching models to emit their own expressions of uncertainty, where model calibration suffers when teaching models to emit certainty rather than un certainty. Together, these results highlight the challenges of building LMs that interpret and generate trustworthy expressions of uncertainty.",
            "year": 2023,
            "citationCount": 54,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that GPT3\u2019s generations vary upwards of 80% in accuracy based on the expression used, and the challenges of building LMs that interpret and generate trustworthy expressions of uncertainty are highlighted."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the calibration and generalization of uncertainty estimates in large language models (LLMs) across diverse tasks and domains. The proposed approach is Multitask Uncertainty Prompting (MUP), which involves constructing diverse prompts, generating subprompts to probe uncertainty, and aggregating uncertainty estimates.\n\nThe research problem in the paper is understanding the ability of LLMs to interpret and generate expressions of uncertainty. The approach involves injecting expressions of uncertainty into prompts and analyzing the impact on accuracy and calibration.\n\nWhile both works deal with uncertainty in LLMs, the proposal focuses on improving uncertainty estimation across tasks using prompting, while the paper investigates the interpretation and generation of uncertainty expressions and their impact on model behavior. The research problems and approaches are distinct.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "ba63e1ab5b6e9d849982ae293ac0483053badaff",
            "paperId": "ba63e1ab5b6e9d849982ae293ac0483053badaff",
            "title": "Uncertainty in Language Models: Assessment through Rank-Calibration",
            "abstract": "Language Models (LMs) have shown promising performance in natural language generation. However, as LMs often generate incorrect or hallucinated responses, it is crucial to correctly quantify their uncertainty in responding to given inputs. In addition to verbalized confidence elicited via prompting, many uncertainty measures ($e.g.$, semantic entropy and affinity-graph-based measures) have been proposed. However, these measures can differ greatly, and it is unclear how to compare them, partly because they take values over different ranges ($e.g.$, $[0,\\infty)$ or $[0,1]$). In this work, we address this issue by developing a novel and practical framework, termed $Rank$-$Calibration$, to assess uncertainty and confidence measures for LMs. Our key tenet is that higher uncertainty (or lower confidence) should imply lower generation quality, on average. Rank-calibration quantifies deviations from this ideal relationship in a principled manner, without requiring ad hoc binary thresholding of the correctness score ($e.g.$, ROUGE or METEOR). The broad applicability and the granular interpretability of our methods are demonstrated empirically.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel and practical framework, termed $Rank$-$Calibration$ is developed, to assess uncertainty and confidence measures for LMs, with the key tenet that higher uncertainty should imply lower generation quality, on average."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the calibration and generalization of uncertainty estimates in large language models across diverse tasks and domains. The proposed approach is Multitask Uncertainty Prompting (MUP), which constructs prompts spanning different tasks and domains, generates subprompts to probe the model's uncertainty, and aggregates the uncertainty estimates.\n\nThe research problem in the paper is assessing and comparing different uncertainty measures for language models. The proposed approach is a framework called Rank-Calibration, which quantifies how well the uncertainty measures align with the expected relationship between higher uncertainty and lower generation quality.\n\nThe proposal focuses on improving uncertainty estimation through prompting, while the paper focuses on evaluating and comparing existing uncertainty measures. The methods proposed in the two works are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "217e436fd23fe4184828e02a2b143835d6fd3b28",
            "paperId": "217e436fd23fe4184828e02a2b143835d6fd3b28",
            "title": "Navigating the Grey Area: How Expressions of Uncertainty and Overconfidence Affect Language Models",
            "abstract": "The increased deployment of LMs for real-world tasks involving knowledge and facts makes it important to understand model epistemology: what LMs think they know, and how their attitudes toward that knowledge are affected by language use in their inputs. Here, we study an aspect of model epistemology: how epistemic markers of certainty, uncertainty, or evidentiality like\"I'm sure it's\",\"I think it's\", or\"Wikipedia says it's\"affect models, and whether they contribute to model failures. We develop a typology of epistemic markers and inject 50 markers into prompts for question answering. We find that LMs are highly sensitive to epistemic markers in prompts, with accuracies varying more than 80%. Surprisingly, we find that expressions of high certainty result in a 7% decrease in accuracy as compared to low certainty expressions; similarly, factive verbs hurt performance, while evidentials benefit performance. Our analysis of a popular pretraining dataset shows that these markers of uncertainty are associated with answers on question-answering websites, while markers of certainty are associated with questions. These associations may suggest that the behavior of LMs is based on mimicking observed language use, rather than truly reflecting epistemic uncertainty.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that LMs are highly sensitive to epistemic markers in prompts, with accuracies varying more than 80%, and expressions of high certainty result in a 7% decrease in accuracy as compared to low certainty expressions; similarly, factive verbs hurt performance, while evidentials benefit performance."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the calibration and generalization of language model uncertainty estimates across diverse tasks and domains using a novel prompting-based method called Multitask Uncertainty Prompting (MUP). The approach involves constructing diverse prompts, generating subprompts to probe uncertainty, aggregating uncertainty estimates, and fine-tuning prompts based on calibration performance.\n\nThe research problem in the paper is understanding how epistemic markers of certainty, uncertainty, or evidentiality in prompts affect language models' performance and contribute to model failures. The approach involves developing a typology of epistemic markers, injecting them into question-answering prompts, and analyzing their impact on model accuracy.\n\nThe proposal focuses on improving uncertainty estimation, while the paper studies the effect of epistemic markers on model performance. The methods and goals are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "0a51afdcd7cf4f33987d766082a7d3f174936c8a",
            "paperId": "0a51afdcd7cf4f33987d766082a7d3f174936c8a",
            "title": "Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models",
            "abstract": "In the face of uncertainty, the ability to seek information is of fundamental importance. In many practical applications, such as medical diagnosis and troubleshooting, the information needed to solve the task is not initially given, and has to be actively sought by asking follow-up questions (for example, a doctor asking a patient for more details about their symptoms). In this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment large language models with the ability to actively seek information by asking effective questions. UoT combines 1) an uncertainty-aware simulation approach which enables the model to simulate possible future scenarios and how likely they are to occur, 2) uncertainty-based rewards motivated by information gain which incentivizes the model to seek information, and 3) a reward propagation scheme to select the optimal question to ask in a way that maximizes the expected reward. In experiments on medical diagnosis, troubleshooting and the '20 Questions' game, UoT achieves an average performance improvement of 57.8% in the rate of successful task completion across multiple LLMs compared with direct prompting, and also improves efficiency (i.e., the number of questions needed to complete the task).",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Uncertainty of Thoughts is introduced, an algorithm to augment large language models with the ability to actively seek information by asking effective questions and achieves an average performance improvement of 57.8% in the rate of successful task completion across multiple LLMs compared with direct prompting."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the calibration and generalization of uncertainty estimates in large language models across diverse tasks and domains. The proposed approach is Multitask Uncertainty Prompting (MUP), which constructs prompts and subprompts to jointly elicit uncertainty estimates from LLMs.\n\nThe research problem in the paper is enhancing information-seeking abilities in large language models to ask effective questions when faced with uncertainty. The proposed approach is Uncertainty of Thoughts (UoT), which combines uncertainty-aware simulation, uncertainty-based rewards, and reward propagation to select optimal questions.\n\nWhile both works address uncertainty in large language models, the specific research problems and approaches are different. The proposal focuses on improving uncertainty calibration and generalization, while the paper focuses on enhancing information-seeking abilities. The proposed methods, MUP and UoT, have distinct techniques and objectives.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "bf4700077294c369f64eda65f677dd4f61b43072",
            "paperId": "bf4700077294c369f64eda65f677dd4f61b43072",
            "title": "Uncertainty Estimation and Reduction of Pre-trained Models for Text Regression",
            "abstract": "Abstract State-of-the-art classification and regression models are often not well calibrated, and cannot reliably provide uncertainty estimates, limiting their utility in safety-critical applications such as clinical decision-making. While recent work has focused on calibration of classifiers, there is almost no work in NLP on calibration in a regression setting. In this paper, we quantify the calibration of pre- trained language models for text regression, both intrinsically and extrinsically. We further apply uncertainty estimates to augment training data in low-resource domains. Our experiments on three regression tasks in both self-training and active-learning settings show that uncertainty estimation can be used to increase overall performance and enhance model generalization.",
            "year": 2022,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper quantifies the calibration of pre- trained language models for text regression, both intrinsically and extrinsically, and applies uncertainty estimates to augment training data in low-resource domains."
            },
            "score": 6
        },
        {
            "id": "444f3b7293b85b7d37600372941a289f9163abd1",
            "paperId": "444f3b7293b85b7d37600372941a289f9163abd1",
            "title": "LM-Polygraph: Uncertainty Estimation for Language Models",
            "abstract": "Recent advancements in the capabilities of large language models (LLMs) have paved the way for a myriad of groundbreaking applications in various fields. However, a significant challenge arises as these models often\"hallucinate\", i.e., fabricate facts without providing users an apparent means to discern the veracity of their statements. Uncertainty estimation (UE) methods are one path to safer, more responsible, and more effective use of LLMs. However, to date, research on UE methods for LLMs has been focused primarily on theoretical rather than engineering contributions. In this work, we tackle this issue by introducing LM-Polygraph, a framework with implementations of a battery of state-of-the-art UE methods for LLMs in text generation tasks, with unified program interfaces in Python. Additionally, it introduces an extendable benchmark for consistent evaluation of UE techniques by researchers, and a demo web application that enriches the standard chat dialog with confidence scores, empowering end-users to discern unreliable responses. LM-Polygraph is compatible with the most recent LLMs, including BLOOMz, LLaMA-2, ChatGPT, and GPT-4, and is designed to support future releases of similarly-styled LMs.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LM-Polygraph is introduced, a framework with implementations of a battery of state-of-the-art UE methods for LLMs in text generation tasks, with unified program interfaces in Python, and introduces an extendable benchmark for consistent evaluation of UE techniques by researchers."
            },
            "score": 6
        },
        {
            "id": "b8a83b11944355b0c5e59e911af4a2a0bfa0362a",
            "paperId": "b8a83b11944355b0c5e59e911af4a2a0bfa0362a",
            "title": "Investigating Uncertainty Calibration of Aligned Language Models under the Multiple-Choice Setting",
            "abstract": "Despite the significant progress made in practical applications of aligned language models (LMs), they tend to be overconfident in output answers compared to the corresponding pre-trained LMs. In this work, we systematically evaluate the impact of the alignment process on logit-based uncertainty calibration of LMs under the multiple-choice setting. We first conduct a thoughtful empirical study on how aligned LMs differ in calibration from their pre-trained counterparts. Experimental results reveal that there are two distinct uncertainties in LMs under the multiple-choice setting, which are responsible for the answer decision and the format preference of the LMs, respectively. Then, we investigate the role of these two uncertainties on aligned LM's calibration through fine-tuning in simple synthetic alignment schemes and conclude that one reason for aligned LMs' overconfidence is the conflation of these two types of uncertainty. Furthermore, we examine the utility of common post-hoc calibration methods for aligned LMs and propose an easy-to-implement and sample-efficient method to calibrate aligned LMs. We hope our findings could provide insights into the design of more reliable alignment processes for LMs.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work systematically evaluates the impact of the alignment process on logit-based uncertainty calibration of LMs under the multiple-choice setting and concludes that one reason for aligned LMs' overconfidence is the conflation of these two types of uncertainty."
            },
            "score": 6
        },
        {
            "id": "acbe813244e07f32eb034d6c27547d772a995d1d",
            "paperId": "acbe813244e07f32eb034d6c27547d772a995d1d",
            "title": "Uncertainty Estimation for Language Reward Models",
            "abstract": "Language models can learn a range of capabilities from unsupervised training on text corpora. However, to solve a particular problem (such as text summarization) it is typically necessary to fine-tune them on a task-specific dataset. It is often easier for humans to choose between options than to provide labeled data, and prior work has achieved state-of-the-art performance by training a reward model from such preference comparisons. However, collecting a large preference comparison dataset is still expensive -- and the learned reward models are unreliable out-of-distribution. We seek to address these problems via uncertainty estimation, which can improve sample efficiency and robustness using active learning and risk-averse reinforcement learning (RL). Specifically, we use bootstrap aggregating (bagging) to train an ensemble of reward models differing in the initialization of their final layer. Ensembles have proved successful in prior applications of active learning, but we find that in our setting ensemble active learning does not outperform random sampling. Further experiments show that while the aggregate predictions are well-calibrated, the ensemble's estimated epistemic uncertainty is only weakly correlated with model error. We suspect this is because the ensemble members are fine-tuned from a single model and so are similar to one another. This suggests current pre-training methods will need to be modified to support uncertainty estimation, e.g. by training multiple language models.",
            "year": 2022,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that in this setting ensemble active learning does not outperform random sampling, and current pre-training methods will need to be modified to support uncertainty estimation, e.g. by training multiple language models."
            },
            "score": 6
        },
        {
            "id": "47eb0468ba7b6457d32b6aa0ee15ad269c04864d",
            "paperId": "47eb0468ba7b6457d32b6aa0ee15ad269c04864d",
            "title": "Confidently Wrong: Exploring the Calibration and Expression of (Un)Certainty of Large Language Models in a Multilingual Setting",
            "abstract": "While the fluency and coherence of Large Language Models (LLMs) in text generation have seen significant improvements, their competency in generating appropriate expressions of uncertainty remains limited.Using a multilingual closed-book QA task and GPT-3.5, we explore how well LLMs are calibrated and express certainty across a diverse set of languages, including low-resource settings. Our results reveal strong performance in high-resource languages but a marked decline in performance in lower-resource languages. Across all, we observe an exaggerated expression of confidence in the model, which does not align with the correctness or likelihood of its responses. Our findings highlight the need for further research into accurate calibration of LLMs especially in a multilingual setting.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Using a multilingual closed-book QA task and GPT-3.5, how well LLMs are calibrated and express certainty across a diverse set of languages, including low-resource settings is explored."
            },
            "score": 6
        },
        {
            "id": "590954e15e247cc343710ee97e396ad99f52970f",
            "paperId": "590954e15e247cc343710ee97e396ad99f52970f",
            "title": "Active Instruction Tuning: Improving Cross-Task Generalization by Training on Prompt Sensitive Tasks",
            "abstract": "Instruction tuning (IT) achieves impressive zero-shot generalization results by training large language models (LLMs) on a massive amount of diverse tasks with instructions. However, how to select new tasks to improve the performance and generalizability of IT models remains an open question. Training on all existing tasks is impractical due to prohibiting computation requirements, and randomly selecting tasks can lead to suboptimal performance. In this work, we propose active instruction tuning based on prompt uncertainty, a novel framework to identify informative tasks, and then actively tune the models on the selected tasks. We represent the informativeness of new tasks with the disagreement of the current model outputs over perturbed prompts. Our experiments on NIV2 and Self-Instruct datasets demonstrate that our method consistently outperforms other baseline strategies for task selection, achieving better out-of-distribution generalization with fewer training tasks. Additionally, we introduce a task map that categorizes and diagnoses tasks based on prompt uncertainty and prediction probability. We discover that training on ambiguous (prompt-uncertain) tasks improves generalization while training on difficult (prompt-certain and low-probability) tasks offers no benefit, underscoring the importance of task selection for instruction tuning.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes active instruction tuning based on prompt uncertainty, a novel framework to identify informative tasks, and then actively tune the models on the selected models, and introduces a task map that categorizes and diagnoses tasks based on Prompt uncertainty and prediction probability."
            },
            "score": 6
        },
        {
            "id": "e1bc150d5d9e745a4920881c414ac9df0ea024a3",
            "paperId": "e1bc150d5d9e745a4920881c414ac9df0ea024a3",
            "title": "ChatGPT Prompting Cannot Estimate Predictive Uncertainty in High-Resource Languages",
            "abstract": "ChatGPT took the world by storm for its impressive abilities. Due to its release without documentation, scientists immediately attempted to identify its limits, mainly through its performance in natural language processing (NLP) tasks. This paper aims to join the growing literature regarding ChatGPT's abilities by focusing on its performance in high-resource languages and on its capacity to predict its answers' accuracy by giving a confidence level. The analysis of high-resource languages is of interest as studies have shown that low-resource languages perform worse than English in NLP tasks, but no study so far has analysed whether high-resource languages perform as well as English. The analysis of ChatGPT's confidence calibration has not been carried out before either and is critical to learn about ChatGPT's trustworthiness. In order to study these two aspects, five high-resource languages and two NLP tasks were chosen. ChatGPT was asked to perform both tasks in the five languages and to give a numerical confidence value for each answer. The results show that all the selected high-resource languages perform similarly and that ChatGPT does not have a good confidence calibration, often being overconfident and never giving low confidence values.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper aims to join the growing literature regarding ChatGPT's abilities by focusing on its performance in high-resource languages and on its capacity to predict its answers' accuracy by giving a confidence level."
            },
            "score": 6
        },
        {
            "id": "74c7343d91d5464c27ca407fd504b07e690363be",
            "paperId": "74c7343d91d5464c27ca407fd504b07e690363be",
            "title": "Combining Confidence Elicitation and Sample-based Methods for Uncertainty Quantification in Misinformation Mitigation",
            "abstract": "Large Language Models have emerged as prime candidates to tackle misinformation mitigation. However, existing approaches struggle with hallucinations and overconfident predictions. We propose an uncertainty quantification framework that leverages both direct confidence elicitation and sampled-based consistency methods to provide better calibration for NLP misinformation mitigation solutions. We first investigate the calibration of sample-based consistency methods that exploit distinct features of consistency across sample sizes and stochastic levels. Next, we evaluate the performance and distributional shift of a robust numeric verbalization prompt across single vs. two-step confidence elicitation procedure. We also compare the performance of the same prompt with different versions of GPT and different numerical scales. Finally, we combine the sample-based consistency and verbalized methods to propose a hybrid framework that yields a better uncertainty estimation for GPT models. Overall, our work proposes novel uncertainty quantification methods that will improve the reliability of Large Language Models in misinformation mitigation applications.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes an uncertainty quantification framework that leverages both direct confidence elicitation and sampled-based consistency methods to provide better calibration for NLP misinformation mitigation solutions to improve the reliability of Large Language Models in misinformation mitigation applications."
            },
            "score": 6
        },
        {
            "id": "97d4145117462177e1244a99d7a25afed4c234f7",
            "paperId": "97d4145117462177e1244a99d7a25afed4c234f7",
            "title": "How Many Validation Labels Do You Need? Exploring the Design Space of Label-Efficient Model Ranking",
            "abstract": "This paper presents LEMR (Label-Efficient Model Ranking) and introduces the MoraBench Benchmark. LEMR is a novel framework that minimizes the need for costly annotations in model selection by strategically annotating instances from an unlabeled validation set. To evaluate LEMR, we leverage the MoraBench Benchmark, a comprehensive collection of model outputs across diverse scenarios. Our extensive evaluation across 23 different NLP tasks in semi-supervised learning, weak supervision, and prompt selection tasks demonstrates LEMR's effectiveness in significantly reducing labeling costs. Key findings highlight the impact of suitable ensemble methods, uncertainty sampling strategies, and model committee selection in enhancing model ranking accuracy. LEMR, supported by the insights from MoraBench, provides a cost-effective and accurate solution for model selection, especially valuable in resource-constrained environments.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents LEMR (Label-Efficient Model Ranking) and introduces the MoraBench Benchmark, a comprehensive collection of model outputs across diverse scenarios that provides a cost-effective and accurate solution for model selection, especially valuable in resource-constrained environments."
            },
            "score": 6
        },
        {
            "id": "b1ec3002f4c80d721fc7d975cf469dce0833fed0",
            "paperId": "b1ec3002f4c80d721fc7d975cf469dce0833fed0",
            "title": "DeLLMa: A Framework for Decision Making Under Uncertainty with Large Language Models",
            "abstract": "Large language models (LLMs) are increasingly used across society, including in domains like business, engineering, and medicine. These fields often grapple with decision-making under uncertainty, a critical yet challenging task. In this paper, we show that directly prompting LLMs on these types of decision-making problems yields poor results, especially as the problem complexity increases. To overcome this limitation, we propose DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments. DeLLMa involves a multi-step scaffolding procedure, drawing upon principles from decision theory and utility theory, to provide an optimal and human-auditable decision-making process. We validate our framework on decision-making environments involving real agriculture and finance data. Our results show that DeLLMa can significantly improve LLM decision-making performance, achieving up to a 40% increase in accuracy over competing methods.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments, is proposed and validated on decision-making environments involving real agriculture and finance data."
            },
            "score": 5
        },
        {
            "id": "9e5a1b8aa30b05aa32644023c290cf9de7f55727",
            "paperId": "9e5a1b8aa30b05aa32644023c290cf9de7f55727",
            "title": "Automated Parliaments: A Solution to Decision Uncertainty and Misalignment in Language Models",
            "abstract": "As AI takes on a greater role in the modern world, it is essential to ensure that AI models can overcome decision uncertainty and remain aligned with human morality and interests. This research paper proposes a method for improving the decision-making of language models (LMs) via Automated Parliaments (APs) - constructs made of AI delegates each representing a certain perspective. Delegates themselves consist of three AI models: generators, modifiers, and evaluators. We specify two mechanisms for producing optimal solutions: the Simultaneous Modification mechanism for response creation and an evaluation mechanism for fairly assessing solutions. The overall process begins when each generator creates a response aligned with its delegate's theory. The modifiers alter all other responses to make them more self-aligned. The evaluators collectively assess the best end response. Finally, the modifiers and generators learn from feedback from the evaluators. In our research, we tested the evaluation mechanism, comparing the use of single-value zero-shot prompting and AP few-shot prompting in evaluating morally contentious scenarios. We found that the AP architecture saw a 57.3% reduction in its loss value compared to the baseline. We conclude by discussing some potential applications of APs and specifically their potential impact when implemented as Automated Moral Parliaments.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This research paper proposes a method for improving the decision-making of language models via Automated Parliaments (APs) - constructs made of AI delegates each representing a certain perspective, and specifies two mechanisms for producing optimal solutions: the Simultaneous Modification mechanism for response creation and an evaluation mechanism for fairly assessing solutions."
            },
            "score": 5
        },
        {
            "id": "d1500f1dbd62e26ef0753f31e845078f58479968",
            "paperId": "d1500f1dbd62e26ef0753f31e845078f58479968",
            "title": "Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners",
            "abstract": "Large language models (LLMs) exhibit a wide range of promising capabilities -- from step-by-step planning to commonsense reasoning -- that may provide utility for robots, but remain prone to confidently hallucinated predictions. In this work, we present KnowNo, which is a framework for measuring and aligning the uncertainty of LLM-based planners such that they know when they don't know and ask for help when needed. KnowNo builds on the theory of conformal prediction to provide statistical guarantees on task completion while minimizing human help in complex multi-step planning settings. Experiments across a variety of simulated and real robot setups that involve tasks with different modes of ambiguity (e.g., from spatial to numeric uncertainties, from human preferences to Winograd schemas) show that KnowNo performs favorably over modern baselines (which may involve ensembles or extensive prompt tuning) in terms of improving efficiency and autonomy, while providing formal assurances. KnowNo can be used with LLMs out of the box without model-finetuning, and suggests a promising lightweight approach to modeling uncertainty that can complement and scale with the growing capabilities of foundation models. Website: https://robot-help.github.io",
            "year": 2023,
            "citationCount": 91,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents KnowNo, which is a framework for measuring and aligning the uncertainty of LLM-based planners such that they know when they don't know and ask for help when needed, and suggests a promising lightweight approach to modeling uncertainty that can complement and scale with the growing capabilities of foundation models."
            },
            "score": 5
        },
        {
            "id": "693322c7f5dc51eaa913a7f15c2480e05c9b7af6",
            "paperId": "693322c7f5dc51eaa913a7f15c2480e05c9b7af6",
            "title": "Benchmark for Uncertainty & Robustness in Self-Supervised Learning",
            "abstract": "Self-Supervised Learning (SSL) is crucial for real-world applications, especially in data-hungry domains such as healthcare and self-driving cars. In addition to a lack of labeled data, these applications also suffer from distributional shifts. Therefore, an SSL method should provide robust generalization and uncertainty estimation in the test dataset to be considered a reliable model in such high-stakes domains. However, existing approaches often focus on generalization, without evaluating the model's uncertainty. The ability to compare SSL techniques for improving these estimates is therefore critical for research on the reliability of self-supervision models. In this paper, we explore variants of SSL methods, including Jigsaw Puzzles, Context, Rotation, Geometric Transformations Prediction for vision, as well as BERT and GPT for language tasks. We train SSL in auxiliary learning for vision and pre-training for language model, then evaluate the generalization (in-out classification accuracy) and uncertainty (expected calibration error) across different distribution covariate shift datasets, including MNIST-C, CIFAR-10-C, CIFAR-10.1, and MNLI. Our goal is to create a benchmark with outputs from experiments, providing a starting point for new SSL methods in Reliable Machine Learning. All source code to reproduce results is available at https://github.com/hamanhbui/reliable_ssl_baselines.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Variants of SSL methods, including Jigsaw Puzzles, Context, Rotation, Geometric Transformations Prediction for vision, as well as BERT and GPT for language tasks are explored, providing a starting point for new SSL methods in Reliable Machine Learning."
            },
            "score": 5
        },
        {
            "id": "48fb667125298cf724f7b652d521686180412351",
            "paperId": "48fb667125298cf724f7b652d521686180412351",
            "title": "A Close Look into the Calibration of Pre-trained Language Models",
            "abstract": "Pre-trained language models (PLMs) may fail in giving reliable estimates of their predictive uncertainty. We take a close look into this problem, aiming to answer two questions: (1) Do PLMs learn to become calibrated in the training process? (2) How effective are existing calibration methods? For the first question, we conduct fine-grained control experiments to study the dynamic change in PLMs\u2019 calibration performance in training. We consider six factors as control variables, including dataset difficulty, available training samples, training steps, the number of tunable parameters, model scale, and pretraining. We observe a consistent change in calibration performance across six factors. We find that PLMs don\u2019t learn to become calibrated in training, evidenced by the continual increase in confidence, no matter whether the predictions are correct or not. We highlight that our finding somewhat contradicts two established conclusions: (a) Larger PLMs are more calibrated; (b) Pretraining improves model calibration. Next, we study the effectiveness of existing calibration methods in mitigating the overconfidence issue. Besides unlearnable calibration methods (e.g., label smoothing), we adapt and extend two recently proposed learnable methods that directly collect data to train models to have reasonable confidence estimations. Experimental results show that learnable methods significantly reduce PLMs\u2019 confidence in wrong predictions.",
            "year": 2022,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that pre-trained language models don\u2019t learn to become calibrated in training, evidenced by the continual increase in confidence, no matter whether the predictions are correct or not."
            },
            "score": 5
        },
        {
            "id": "5e0424f1388b4cd3d1f5cbb31615d5c4a9c016df",
            "paperId": "5e0424f1388b4cd3d1f5cbb31615d5c4a9c016df",
            "title": "Towards Unified Task Embeddings Across Multiple Models: Bridging the Gap for Prompt-Based Large Language Models and Beyond",
            "abstract": "Task embedding, a meta-learning technique that captures task-specific information, has become prevalent, especially in areas such as multi-task learning, model editing, and interpretability. However, it faces challenges with the emergence of prompt-guided Large Language Models (LLMs) operating in a gradientfree manner. Existing task embedding methods rely on fine-tuned, task-specific language models, which hinders the adaptability of task embeddings across diverse models, especially prompt-based LLMs. To unleash the power of task embedding in the era of LLMs, we propose a framework for unified task embeddings (FUTE), harmonizing task embeddings from various models, including smaller language models and LLMs with varied prompts, within a single vector space. Such uniformity enables the comparison and analysis of similarities amongst different models, extending the scope and utility of existing task embedding methods in addressing multi-model scenarios, whilst maintaining their performance to be comparable to architecture-specific methods.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A framework for unified task embeddings (FUTE) is proposed, harmonizing task embeddings from various models, including smaller language models and LLMs with varied prompts, within a single vector space, to extend the scope and utility of existing task embedding methods in addressing multi-model scenarios."
            },
            "score": 5
        },
        {
            "id": "8b32aa33601514976d88fabcb060a5cd38d34006",
            "paperId": "8b32aa33601514976d88fabcb060a5cd38d34006",
            "title": "Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning",
            "abstract": "Prompt tuning, in which a base pretrained model is adapted to each task via conditioning on learned prompt vectors, has emerged as a promising approach for efficiently adapting large language models to multiple downstream tasks. However, existing methods typically learn soft prompt vectors from scratch, and it has not been clear how to exploit the rich cross-task knowledge with prompt vectors in a multitask learning setting. We propose multitask prompt tuning (MPT), which first learns a single transferable prompt by distilling knowledge from multiple task-specific source prompts. We then learn multiplicative low rank updates to this shared prompt to efficiently adapt it to each downstream target task. Extensive experiments on 23 NLP datasets demonstrate that our proposed approach outperforms the state-of-the-art methods, including the full finetuning baseline in some cases, despite only tuning 0.035% as many task-specific parameters.",
            "year": 2023,
            "citationCount": 54,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes multitask prompt tuning (MPT), which first learns a single transferable prompt by distilling knowledge from multiple task-specific source prompts, then learns multiplicative low rank updates to this shared prompt to efficiently adapt it to each downstream target task."
            },
            "score": 5
        },
        {
            "id": "7a5cd8a1bf99c9cc58bd7a818be446c29e9e1cbb",
            "paperId": "7a5cd8a1bf99c9cc58bd7a818be446c29e9e1cbb",
            "title": "SPT: Semi-Parametric Prompt Tuning for Multitask Prompted Learning",
            "abstract": "Pre-trained large language models can efficiently interpolate human-written prompts in a natural way. Multitask prompted learning can help generalization through a diverse set of tasks at once, thus enhancing the potential for more effective downstream fine-tuning. To perform efficient multitask-inference in the same batch, parameter-efficient fine-tuning methods such as prompt tuning have been proposed. However, the existing prompt tuning methods may lack generalization. We propose SPT, a semi-parametric prompt tuning method for multitask prompted learning. The novel component of SPT is a memory bank from where memory prompts are retrieved based on discrete prompts. Extensive experiments, such as (i) fine-tuning a full language model with SPT on 31 different tasks from 8 different domains and evaluating zero-shot generalization on 9 heldout datasets under 5 NLP task categories and (ii) pretraining SPT on the GLUE datasets and evaluating fine-tuning on the SuperGLUE datasets, demonstrate effectiveness of SPT.",
            "year": 2022,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "SPT, a semi-parametric prompt tuning method for multitask prompted learning, which is a memory bank from where memory prompts are retrieved based on discrete prompts, is proposed."
            },
            "score": 5
        },
        {
            "id": "39933143da6c668d5755fe2c99c365314bf2a441",
            "paperId": "39933143da6c668d5755fe2c99c365314bf2a441",
            "title": "Improving Task Generalization via Unified Schema Prompt",
            "abstract": "Task generalization has been a long standing challenge in Natural Language Processing (NLP). Recent research attempts to improve the task generalization ability of pre-trained language models by mapping NLP tasks into human-readable prompted forms. However, these approaches require laborious and inflexible manual collection of prompts, and different prompts on the same downstream task may receive unstable performance. We propose Unified Schema Prompt, a flexible and extensible prompting method, which automatically customizes the learnable prompts for each task according to the task input schema. It models the shared knowledge between tasks, while keeping the characteristics of different task schema, and thus enhances task generalization ability. The schema prompt takes the explicit data structure of each task to formulate prompts so that little human effort is involved. To test the task generalization ability of schema prompt at scale, we conduct schema prompt-based multitask pre-training on a wide variety of general NLP tasks. The framework achieves strong zero-shot and few-shot generalization performance on 16 unseen downstream tasks from 8 task types (e.g., QA, NLI, etc). Furthermore, comprehensive analyses demonstrate the effectiveness of each component in the schema prompt, its flexibility in task compositionality, and its ability to improve performance under a full-data fine-tuning setting.",
            "year": 2022,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Upper Schema Prompt is proposed, a flexible and extensible prompting method, which automatically customizes the learnable prompts for each task according to the task input schema, and enhances task generalization ability."
            },
            "score": 5
        },
        {
            "id": "72fb75f7c38a83424308c8205bb36cd88995494b",
            "paperId": "72fb75f7c38a83424308c8205bb36cd88995494b",
            "title": "Leveraging Large Language Models for Exploiting ASR Uncertainty",
            "abstract": "While large language models excel in a variety of natural language processing (NLP) tasks, to perform well on spoken language understanding (SLU) tasks, they must either rely on off-the-shelf automatic speech recognition (ASR) systems for transcription, or be equipped with an in-built speech modality. This work focuses on the former scenario, where LLM's accuracy on SLU tasks is constrained by the accuracy of a fixed ASR system on the spoken input. Specifically, we tackle speech-intent classification task, where a high word-error-rate can limit the LLM's ability to understand the spoken intent. Instead of chasing a high accuracy by designing complex or specialized architectures regardless of deployment costs, we seek to answer how far we can go without substantially changing the underlying ASR and LLM, which can potentially be shared by multiple unrelated tasks. To this end, we propose prompting the LLM with an n-best list of ASR hypotheses instead of only the error-prone 1-best hypothesis. We explore prompt-engineering to explain the concept of n-best lists to the LLM; followed by the finetuning of Low-Rank Adapters on the downstream tasks. Our approach using n-best lists proves to be effective on a device-directed speech detection task as well as on a keyword spotting task, where systems using n-best list prompts outperform those using 1-best ASR hypothesis; thus paving the way for an efficient method to exploit ASR uncertainty via LLMs for speech-based applications.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work tackles speech-intent classification task, where a high word-error-rate can limit the LLM's ability to understand the spoken intent, and proposes prompting theLLM with an n-best list of ASR hypotheses instead of only the error-prone 1-best hypothesis."
            },
            "score": 4
        },
        {
            "id": "3fc3460c4554a28e489a0ea6ef067b79b7d301d9",
            "paperId": "3fc3460c4554a28e489a0ea6ef067b79b7d301d9",
            "title": "Active Prompting with Chain-of-Thought for Large Language Models",
            "abstract": "The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks requiring reasoning, such as arithmetic and commonsense reasoning. It is known that the effective design of task-specific prompts is critical for LLMs' ability to produce high-quality answers. In particular, an effective approach for complex question-and-answer tasks is example-based prompting with chain-of-thought (CoT) reasoning, which significantly improves the performance of LLMs. However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks. This paper proposes a new method, Active-Prompt, to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning). For this purpose, we propose a solution to the key problem of determining which questions are the most important and helpful ones to annotate from a pool of task-specific queries. By borrowing ideas from the related problem of uncertainty-based active learning, we introduce several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation. Experimental results demonstrate the superiority of our proposed method, achieving state-of-the-art on eight complex reasoning tasks. Further analyses of different uncertainty metrics, pool sizes, zero-shot learning, and accuracy-uncertainty relationship demonstrate the effectiveness of our method. Our code will be available at https://github.com/shizhediao/active-prompt.",
            "year": 2023,
            "citationCount": 58,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new method to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning), and introduces several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation."
            },
            "score": 4
        },
        {
            "id": "aea817017e18cc1cb34962ffd399f8a83ab7a076",
            "paperId": "aea817017e18cc1cb34962ffd399f8a83ab7a076",
            "title": "Teaching Smaller Language Models To Generalise To Unseen Compositional Questions",
            "abstract": "We equip a smaller Language Model to generalise to answering challenging compositional questions that have not been seen in training. To do so we propose a combination of multitask supervised pretraining on up to 93 tasks designed to instill diverse reasoning abilities, and a dense retrieval system that aims to retrieve a set of evidential paragraph fragments. Recent progress in question-answering has been achieved either through prompting methods against very large pretrained Language Models in zero or few-shot fashion, or by fine-tuning smaller models, sometimes in conjunction with information retrieval. We focus on the less explored question of the extent to which zero-shot generalisation can be enabled in smaller models with retrieval against a corpus within which sufficient information to answer a particular question may not exist. We establish strong baselines in this setting for diverse evaluation datasets (StrategyQA, CommonsenseQA, IIRC, DROP, Musique and ARC-DA), and show that performance can be significantly improved by adding retrieval-augmented training datasets which are designed to expose our models to a variety of heuristic reasoning strategies such as weighing partial evidence or ignoring an irrelevant context.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A smaller Language Model is equip to generalise to answering challenging compositional questions that have not been seen in training, and performance can be significantly improved by adding retrieval-augmented training datasets designed to expose the authors' models to a variety of heuristic reasoning strategies."
            },
            "score": 4
        },
        {
            "id": "6052486bc9144dc1730c12bf35323af3792a1fd0",
            "paperId": "6052486bc9144dc1730c12bf35323af3792a1fd0",
            "title": "Large language models encode clinical knowledge",
            "abstract": null,
            "year": 2022,
            "citationCount": 825,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "MultiMedQA, a benchmark combining six existing medical question answering datasets spanning professional medicine, research and consumer queries and\u00a0a new dataset of medical questions searched online, is presented and a human evaluation framework for model answers is proposed, suggesting the potential utility of LLMs in medicine."
            },
            "score": 4
        },
        {
            "id": "8089b431b2e09c27967428fb542c0935fb95ec30",
            "paperId": "8089b431b2e09c27967428fb542c0935fb95ec30",
            "title": "STAR: Constraint LoRA with Dynamic Active Learning for Data-Efficient Fine-Tuning of Large Language Models",
            "abstract": "Though Large Language Models (LLMs) have demonstrated the powerful capabilities of few-shot learning through prompting methods, supervised training is still necessary for complex reasoning tasks. Because of their extensive parameters and memory consumption, both Parameter-Efficient Fine-Tuning (PEFT) methods and Memory-Efficient Fine-Tuning methods have been proposed for LLMs. Nevertheless, the issue of large annotated data consumption, the aim of Data-Efficient Fine-Tuning, remains unexplored. One obvious way is to combine the PEFT method with active learning. However, the experimental results show that such a combination is not trivial and yields inferior results. Through probe experiments, such observation might be explained by two main reasons: uncertainty gap and poor model calibration. Therefore, in this paper, we propose a novel approach to effectively integrate uncertainty-based active learning and LoRA. Specifically, for the uncertainty gap, we introduce a dynamic uncertainty measurement that combines the uncertainty of the base model and the uncertainty of the full model during the iteration of active learning. For poor model calibration, we incorporate the regularization method during LoRA training to keep the model from being over-confident, and the Monte-Carlo dropout mechanism is employed to enhance the uncertainty estimation. Experimental results show that the proposed approach outperforms existing baseline models on three complex reasoning tasks.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel approach to effectively integrate uncertainty-based active learning and LoRA is proposed, which incorporates the regularization method during LoRA training to keep the model from being over-confident, and the Monte-Carlo dropout mechanism is employed to enhance the uncertainty estimation."
            },
            "score": 4
        },
        {
            "id": "17bcb1edbe068e8fe6a97da552c70a77a15bbce7",
            "paperId": "17bcb1edbe068e8fe6a97da552c70a77a15bbce7",
            "title": "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned",
            "abstract": "We describe our early efforts to red team language models in order to simultaneously discover, measure, and attempt to reduce their potentially harmful outputs. We make three main contributions. First, we investigate scaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B parameters) and 4 model types: a plain language model (LM); an LM prompted to be helpful, honest, and harmless; an LM with rejection sampling; and a model trained to be helpful and harmless using reinforcement learning from human feedback (RLHF). We find that the RLHF models are increasingly difficult to red team as they scale, and we find a flat trend with scale for the other model types. Second, we release our dataset of 38,961 red team attacks for others to analyze and learn from. We provide our own analysis of the data and find a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs. Third, we exhaustively describe our instructions, processes, statistical methodologies, and uncertainty about red teaming. We hope that this transparency accelerates our ability to work together as a community in order to develop shared norms, practices, and technical standards for how to red team language models.",
            "year": 2022,
            "citationCount": 236,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that the RLHF models are increasingly difficult to red team as they scale, and a flat trend with scale for the other model types is found."
            },
            "score": 4
        },
        {
            "id": "638c6b03d943dceb7691180c94cdae9b8e387a2f",
            "paperId": "638c6b03d943dceb7691180c94cdae9b8e387a2f",
            "title": "Language Bias-Driven Self-Knowledge Distillation with Generalization Uncertainty for Reducing Language Bias in Visual Question Answering",
            "abstract": "To answer questions, visual question answering systems (VQA) rely on language bias but ignore the information of the images, which has negative information on its generalization. The mainstream debiased methods focus on removing language prior to inferring. However, the image samples are distributed unevenly in the dataset, so the feature sets acquired by the model often cannot cover the features (views) of the tail samples. Therefore, language bias occurs. This paper proposes a language bias-driven self-knowledge distillation framework to implicitly learn the feature sets of multi-views so as to reduce language bias. Moreover, to measure the performance of student models, the authors of this paper use a generalization uncertainty index to help student models learn unbiased visual knowledge and force them to focus more on the questions that cannot be answered based on language bias alone. In addition, the authors of this paper analyze the theory of the proposed method and verify the positive correlation between generalization uncertainty and expected test error. The authors of this paper validate the method\u2019s effectiveness on the VQA-CP v2, VQA-CP v1 and VQA v2 datasets through extensive ablation experiments.",
            "year": 2022,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "To measure the performance of student models, the authors of this paper use a generalization uncertainty index to help student models learn unbiased visual knowledge and force them to focus more on the questions that cannot be answered based on language bias alone."
            },
            "score": 4
        },
        {
            "id": "e3446ef313663e30d8251dee339bca52962e7bfd",
            "paperId": "e3446ef313663e30d8251dee339bca52962e7bfd",
            "title": "Towards Reliable Misinformation Mitigation: Generalization, Uncertainty, and GPT-4",
            "abstract": "Misinformation poses a critical societal challenge, and current approaches have yet to produce an effective solution. We propose focusing on generalization, uncertainty, and how to leverage recent large language models, in order to create more practical tools to evaluate information veracity in contexts where perfect classification is impossible. We first demonstrate that GPT-4 can outperform prior methods in multiple settings and languages. Next, we explore generalization, revealing that GPT-4 and RoBERTa-large exhibit differences in failure modes. Third, we propose techniques to handle uncertainty that can detect impossible examples and strongly improve outcomes. We also discuss results on other language models, temperature, prompting, versioning, explainability, and web retrieval, each one providing practical insights and directions for future research. Finally, we publish the LIAR-New dataset with novel paired English and French misinformation data and Possibility labels that indicate if there is sufficient context for veracity evaluation. Overall, this research lays the groundwork for future tools that can drive real-world progress to combat misinformation.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This research demonstrates that GPT-4 can outperform prior methods in multiple settings and languages, and proposes techniques to handle uncertainty that can detect impossible examples and strongly improve outcomes."
            },
            "score": 4
        },
        {
            "id": "7de25ad5ac7433e4d4071f450461b03fd2a39b8d",
            "paperId": "7de25ad5ac7433e4d4071f450461b03fd2a39b8d",
            "title": "Prompt Engineering: Guiding the Way to Effective Large Language Models",
            "abstract": "Large language models (LLMs) have become prominent tools in various domains, such as natural language processing, machine translation, and the development of creative text. Nevertheless, in order to fully exploit the capabilities of Language Models, it is imperative to establish efficient communication channels between humans and machines. The discipline of engineering involves the creation of well-constructed and informative prompts, which act as a crucial link between human intention and the execution of tasks by machines. The present study examines the concept of rapid engineering, elucidating its underlying concepts, methodologies, and diverse range of practical applications.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The present study examines the concept of rapid engineering, elucidating its underlying concepts, methodologies, and diverse range of practical applications."
            },
            "score": 4
        },
        {
            "id": "886499f0ab825a266f953f952dccda4b721e80f7",
            "paperId": "886499f0ab825a266f953f952dccda4b721e80f7",
            "title": "Scalable Prompt Generation for Semi-supervised Learning with Language Models",
            "abstract": "Prompt-based learning methods in semi-supervised learning (SSL) settings have been shown to be effective on multiple natural language understanding (NLU) datasets and tasks in the literature. However, manually designing multiple prompts and verbalizers requires domain knowledge and human effort, making it difficult and expensive to scale across different datasets. In this paper, we propose two methods to automatically design multiple prompts and integrate automatic verbalizer in SSL settings without sacrificing performance. The first method uses various demonstration examples with learnable continuous prompt tokens to create diverse prompt models. The second method uses a varying number of soft prompt tokens to encourage language models to learn different prompts. For the verbalizer, we use the prototypical verbalizer to replace the manual one. In summary, we obtained the best average accuracy of 71.5% (a relative improvement of 0.99% over even the previous state-of-the-art SSL method with manual prompts and verbalizers) in different few-shot learning settings.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Two methods to automatically design multiple prompts and integrate automatic verbalizer in SSL settings without sacrificing performance are proposed and the prototypical verbalizer is used to replace the manual one."
            },
            "score": 4
        },
        {
            "id": "306a312c0bae22f30a406187ab18c5724cefb661",
            "paperId": "306a312c0bae22f30a406187ab18c5724cefb661",
            "title": "Memory Injections: Correcting Multi-Hop Reasoning Failures During Inference in Transformer-Based Language Models",
            "abstract": "Answering multi-hop reasoning questions requires retrieving and synthesizing information from diverse sources. Large Language Models (LLMs) struggle to perform such reasoning consistently. Here we propose an approach to pinpoint and rectify multi-hop reasoning failures through targeted memory injections on LLM attention heads. First, we analyze the per-layer activations of GPT-2 models in response to single and multi-hop prompts. We then propose a mechanism that allows users to inject pertinent prompt-specific information, which we refer to as \u201cmemories,\u201d at critical LLM locations during inference. By thus enabling the LLM to incorporate additional relevant information during inference, we enhance the quality of multi-hop prompt completions. We show empirically that a simple, efficient, and targeted memory injection into a key attention layer can often increase the probability of the desired next token in multi-hop tasks, by up to 424%.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown empirically that a simple, efficient, and targeted memory injection into a key attention layer can often increase the probability of the desired next token in multi-hop tasks, by up to 424%."
            },
            "score": 4
        },
        {
            "id": "a51d98ff88103240e4152cd4e85260224cb9d2da",
            "paperId": "a51d98ff88103240e4152cd4e85260224cb9d2da",
            "title": "Foundations and Applications in Large-scale AI Models: Pre-training, Fine-tuning, and Prompt-based Learning",
            "abstract": "Deep learning techniques have advanced rapidly in recent years, leading to significant progress in pre-trained and fine-tuned large-scale AI models. For example, in the natural language processing domain, the traditional \"pre-train, fine-tune\" paradigm is shifting towards the \"pre-train, prompt, and predict\" paradigm, which has achieved great success on many tasks across different application domains such as ChatGPT/BARD for Conversational AI and P5 for a unified recommendation system. Moreover, there has been a growing interest in models that combine vision and language modalities (vision-language models) which are applied to tasks like Visual Captioning/Generation. Considering the recent technological revolution, it is essential to emphasize these paradigm shifts and highlight the paradigms with the potential to solve different tasks. We thus provide a platform for academic and industrial researchers to showcase their latest work, share research ideas, discuss various challenges, and identify areas where further research is needed in pre-training, fine-tuning, and prompt-learning methods for large-scale AI models. We foster the development of a strong research community focused on solving challenges related to large-scale AI models, providing superior and impactful strategies that can change people's lives in the future.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A platform for academic and industrial researchers to showcase their latest work, share research ideas, discuss various challenges, and identify areas where further research is needed in pre-training, fine-tuning, and prompt-learning methods for large-scale AI models is provided."
            },
            "score": 4
        },
        {
            "id": "139bfd354431c4541004c5de5052a63e1f94130e",
            "paperId": "139bfd354431c4541004c5de5052a63e1f94130e",
            "title": "Contrastive Learning for Prompt-based Few-shot Language Learners",
            "abstract": "The impressive performance of GPT-3 using natural language prompts and in-context learning has inspired work on better fine-tuning of moderately-sized models under this paradigm. Following this line of work, we present a contrastive learning framework that clusters inputs from the same class for better generality of models trained with only limited examples. Specifically, we propose a supervised contrastive framework that clusters inputs from the same class under different augmented \u201cviews\u201d and repel the ones from different classes. We create different \u201cviews\u201d of an example by appending it with different language prompts and contextual demonstrations. Combining a contrastive loss with the standard masked language modeling (MLM) loss in prompt-based few-shot learners, the experimental results show that our method can improve over the state-of-the-art methods in a diverse set of 15 language tasks. Our framework makes minimal assumptions on the task or the base model, and can be applied to many recent methods with little modification.",
            "year": 2022,
            "citationCount": 20,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A supervised contrastive framework that clusters inputs from the same class under different augmented \u201cviews\u201d and repel the ones from different classes for better generality of models trained with only limited examples is proposed."
            },
            "score": 4
        },
        {
            "id": "bf8491bef353df126e2306ad2fe4b898697b906a",
            "paperId": "bf8491bef353df126e2306ad2fe4b898697b906a",
            "title": "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",
            "abstract": "This paper proposes a framework for quantitatively evaluating interactive LLMs such as ChatGPT using publicly available data sets. We carry out an extensive technical evaluation of ChatGPT using 23 data sets covering 8 different common NLP application tasks. We evaluate the multitask, multilingual and multi-modal aspects of ChatGPT based on these data sets and a newly designed multimodal dataset. We find that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks. We find that it is better at understanding non-Latin script languages than generating them. It is able to generate multimodal content from textual prompts, via an intermediate code generation step. Moreover, we find that ChatGPT is 63.41% accurate on average in 10 different reasoning categories under logical reasoning, non-textual reasoning, and commonsense reasoning, hence making it an unreliable reasoner. It is, for example, better at deductive than inductive reasoning. ChatGPT suffers from hallucination problems like other LLMs and it generates more extrinsic hallucinations from its parametric memory as it does not have access to an external knowledge base. Finally, the interactive feature of ChatGPT enables human collaboration with the underlying LLM to improve its performance, i.e, 8% ROUGE-1 on summarization and 2% ChrF++ on machine translation, in a multi-turn\"prompt engineering\"fashion. We also release codebase for evaluation set extraction.",
            "year": 2023,
            "citationCount": 754,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that ChatGPT outperforms LLMs with zero-shot learning on most tasks and even outperforms fine-tuned models on some tasks and is better at understanding non-Latin script languages than generating them."
            },
            "score": 4
        },
        {
            "id": "6f75404b0d01f9a09afe428f9efd5cbcd7825469",
            "paperId": "6f75404b0d01f9a09afe428f9efd5cbcd7825469",
            "title": "Dynamic Prompting: A Unified Framework for Prompt Tuning",
            "abstract": "It has been demonstrated that the art of prompt tuning is highly effective in efficiently extracting knowledge from pretrained foundation models, encompassing pretrained language models (PLMs), vision pretrained models, and vision-language (V-L) models. However, the efficacy of employing fixed soft prompts with a predetermined position for concatenation with inputs for all instances, irrespective of their inherent disparities, remains uncertain. Variables such as the position, length, and representations of prompts across diverse instances and tasks can substantially influence the performance of prompt tuning. In this context, we provide a theoretical analysis, which reveals that optimizing the position of the prompt to encompass the input can capture additional semantic information that traditional prefix or postfix prompt tuning methods fail to capture. Building upon our analysis, we present a unified dynamic prompt (DP) tuning strategy that dynamically determines different factors of prompts based on specific tasks and instances. To accomplish this, we employ a lightweight learning network with Gumble-Softmax, allowing us to learn instance-dependent guidance. Experimental results underscore the significant performance improvement achieved by dynamic prompt tuning across a wide range of tasks, including NLP tasks, vision recognition tasks, and vision-language tasks. Furthermore, we establish the universal applicability of our approach under full-data, few-shot, and multitask scenarios. Codes are available at https://github.com/Xianjun-Yang/DPT.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A theoretical analysis is provided, which reveals that optimizing the position of the prompt to encompass the input can capture additional semantic information that traditional prefix or postfix prompt tuning methods fail to capture."
            },
            "score": 4
        },
        {
            "id": "8020c1bfc5c34c9d6889e57d4dc249863582ca9b",
            "paperId": "8020c1bfc5c34c9d6889e57d4dc249863582ca9b",
            "title": "Development of educators\u2019 soft skills: tolerance to uncertainty and thinking in conditions of instability",
            "abstract": "Based on the analysis of scientific, pedagogical and regulatory sources, it is summarised that the concept of uncertainty is considered in different contexts and meanings. Tolerance for uncertainty and thinking in conditions of instability as part of the \u201csoft skills\u201d of educators, which, along with other skills, underlies their professional effectiveness, adaptation to life situations, remains relevant for researchers. Properties that are important for pedagogical activity are highlighted, which indicate tolerance to uncertainty: the ability to control oneself and not fall into a prolonged state of stress, psychophysical stability, anti-fragility; the ability to master emotions, to control or regulate one\u2019s emotional states; cognitive attitudes that approve, positively colour what is incomprehensible, multitasking, non-linear or multiple; a positive attitude towards situations in which one needs to act, take responsibility, and find solutions. It also deals with intolerance to uncertainty \u2013 various negative feelings \u2013 discomfort, threat, insecurity, mistrust of oneself and others; giving preference to cognitive strategies in order to simplify situations \u2013 stereotyping, polarisation, rushing decisions, finding ways to delegate responsibility, the need for categorisation, dichotomy of thinking, which prompts to automatically give priority to one's own, familiar and usual. Factors that are the conditions and characteristics of instability in professional pedagogical activity have been identified and analysed: poly-subjectivity of educational processes; verticality of the structure of pedagogical interaction; the dynamics of educational changes and educational processes, which is particularly typical of the modern global educational space; external determinism of education by market and social factors. Based on the results of the empirical research (diagnosis of intolerance by Badner), it was found out that: 1. A significant advantage of the average level of intolerance/tolerance of the respondents was found in relation to the subscales: \u201cnovelty of the problem\u201d \u2013 73.91 %; \u201ccomplexity of the problem\u201d \u2013 91.30; \u201cinsolvability of the problem\u201d \u2013 84.05; 2. Tolerance, that is, the low level of intolerance of the interviewees is insignificant: \u201cinsolvability of the problem\u201d \u2013 7.24 %; \u201ccomplexity of the problem\u201d \u2013 1.44; \u201cnovelty of the problem\u201d \u2013 2.8; 3. As for the high level of intolerance, it was found that it is characteristic of a significant part of future teachers only according to the subscale \u201cnovelty of the problem\u201d \u2013 34.78 % \u2013 in the respondents of the Preschool Education specialty, and 17.39 % \u2013 of the Primary Education specialty. According to the \u201ccomplexity of the problem\u201d subscale, these indicators are much lower: 8.69 % and 6.52 %, respectively, and according to the \u201cinsolvability of the problem\u201d subscale, they are 13.4 % and 6.52 %. Thus, the novelty of the problem is the main source of intolerance to uncertainty and thinking of students in conditions of stability. Keywords: soft skills; pedagogical activity; professional activity in conditions of instability; thinking in conditions of instability; tolerance to uncertainty.",
            "year": 2022,
            "citationCount": 0,
            "tldr": null,
            "score": 4
        },
        {
            "id": "cf4178eeabf1618bf75081a0a341fade91f3444a",
            "paperId": "cf4178eeabf1618bf75081a0a341fade91f3444a",
            "title": "Learning to Paraphrase Sentences to Different Complexity Levels",
            "abstract": "Abstract While sentence simplification is an active research topic in NLP, its adjacent tasks of sentence complexification and same-level paraphrasing are not. To train models on all three tasks, we present two new unsupervised datasets. We compare these datasets, one labeled by a weak classifier and the other by a rule-based approach, with a single supervised dataset. Using these three datasets for training, we perform extensive experiments on both multitasking and prompting strategies. Compared to other systems trained on unsupervised parallel data, models trained on our weak classifier labeled dataset achieve state-of-the-art performance on the ASSET simplification benchmark. Our models also outperform previous work on sentence-level targeting. Finally, we establish how a handful of Large Language Models perform on these tasks under a zero-shot setting.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Compared to other systems trained on unsupervised parallel data, models trained on the authors' weak classifier labeled dataset achieve state-of-the-art performance on the ASSET simplification benchmark and outperform previous work on sentence-level targeting."
            },
            "score": 4
        },
        {
            "id": "3e14227862ae21ec691794b8586d6c695c5feb75",
            "paperId": "3e14227862ae21ec691794b8586d6c695c5feb75",
            "title": "RE-MOVE: An Adaptive Policy Design for Robotic Navigation Tasks in Dynamic Environments via Language-Based Feedback",
            "abstract": "Reinforcement learning-based policies for continuous control robotic navigation tasks often fail to adapt to changes in the environment during real-time deployment, which may result in catastrophic failures. To address this limitation, we propose a novel approach called RE-MOVE (REquest help and MOVE on) to adapt already trained policy to real-time changes in the environment without re-training via utilizing a language-based feedback. The proposed approach essentially boils down to addressing two main challenges of (1) when to ask for feedback and, if received, (2) how to incorporate feedback into trained policies. RE-MOVE incorporates an epistemic uncertainty-based framework to determine the optimal time to request instructions-based feedback. For the second challenge, we employ a zero-shot learning natural language processing (NLP) paradigm with efficient, prompt design and leverage state-of-the-art GPT-3.5, Llama-2 language models. To show the efficacy of the proposed approach, we performed extensive synthetic and real-world evaluations in several test-time dynamic navigation scenarios. Utilizing RE-MOVE result in up to 80% enhancement in the attainment of successful goals, coupled with a reduction of 13.50% in the normalized trajectory length, as compared to alternative approaches, particularly in demanding real-world environments with perceptual challenges.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 4
        },
        {
            "id": "86d03160e6f05deb17d0169e515f5a55d6361f7c",
            "paperId": "86d03160e6f05deb17d0169e515f5a55d6361f7c",
            "title": "Exploring the Benefits of Training Expert Language Models over Instruction Tuning",
            "abstract": "Recently, Language Models (LMs) instruction-tuned on multiple tasks, also known as multitask-prompted fine-tuning (MT), have shown the capability to generalize to unseen tasks. Previous work has shown that scaling the number of training tasks is the key component in making stronger MT LMs. In this work, we report an unexpected finding that an expert LM fine-tuned on just a single task can outperform an MT LM trained with 300+ different tasks on 11 different unseen datasets and on 13 datasets of the BIG-bench benchmark by a mean accuracy of 3.20% and 1.29%, respectively. This finding casts doubt on the previously held belief that simply scaling the number of tasks makes stronger MT LMs. Leveraging this finding, we further show that this distributed approach of training a separate expert LM per training task instead of a single MT LM for zero-shot inference possesses many benefits including (1) avoiding negative task transfer that often occurs during instruction tuning, (2) being able to continually learn new tasks without having to re-train on previous tasks to avoid catastrophic forgetting, and (3) showing compositional capabilities when merging individual experts together. The code is available at https://github.com/joeljang/ELM.",
            "year": 2023,
            "citationCount": 37,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An unexpected finding is reported that an expert LM fine-tuned on just a single task can outperform an MT LM trained with 300+ different tasks on 11 different unseen datasets and on 13 datasets of the BIG-bench benchmark by a mean accuracy of 3.20% and 1.29%, respectively."
            },
            "score": 3
        },
        {
            "id": "34c2939d3147946b2ac218e7857e1bc4c8902679",
            "paperId": "34c2939d3147946b2ac218e7857e1bc4c8902679",
            "title": "BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting",
            "abstract": "The BLOOM model is a large publicly available multilingual language model, but its pretraining was limited to 46 languages. To extend the benefits of BLOOM to other languages without incurring prohibitively large costs, it is desirable to adapt BLOOM to new languages not seen during pretraining. In this work, we apply existing language adaptation strategies to BLOOM and benchmark its zero-shot prompting performance on eight new languages in a resource-constrained setting. We find language adaptation to be effective at improving zero-shot performance in new languages. Surprisingly, we find that adapter-based finetuning is more effective than continued pretraining for large models. In addition, we discover that prompting performance is not significantly affected by language specifics, such as the writing system. It is primarily determined by the size of the language adaptation data. We also add new languages to BLOOMZ, which is a multitask finetuned version of BLOOM capable of following task instructions zero-shot. We find including a new language in the multitask fine-tuning mixture to be the most effective method to teach BLOOMZ a new language. We conclude that with sufficient training data language adaptation can generalize well to diverse languages. Our code is available at https://github.com/bigscience-workshop/multilingual-modeling.",
            "year": 2022,
            "citationCount": 40,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work applies existing language adaptation strategies to BLOOM and finds language adaptation to be effective at improving zero-shot performance in new languages and concludes that with sufficient training data language adaptation can generalize well to diverse languages."
            },
            "score": 3
        },
        {
            "id": "0a922b4fdbe923b5161b5c6f5adfe586bf7304c3",
            "paperId": "0a922b4fdbe923b5161b5c6f5adfe586bf7304c3",
            "title": "Large language models will not replace healthcare professionals: curbing popular fears and hype",
            "abstract": "Following the release of ChatGPT, large language models (LLMs) have entered the mainstream. ChatGPT and GPT-4 recently garnered particular attention for attaining expert-level performance in United States Medical Licensing Examinations. However, performance is not perfect, and has not been as impressive in more specialised tests, such as the Membership of the Royal College of General Practitioners Applied Knowledge Test. ChatGPT frequently \u2018hallucinates\u2019, providing false, unverified information in the same manner as which it delivers facts. While performance in clinical tasks is expected to improve dramatically with the release of GPT-4, remaining inaccuracy and lack of an uncertainty indicator preclude autonomous deployment of ChatGPT and LLM chatbots like it in clinical settings. LLM applications may nevertheless revolutionise cognitive work \u2013 tools such as ChatGPT excel in tasks where specialist knowledge is not required, or is provided by the user prompt: examples include correcting language and rephrasing information for different audiences or within other constraints (e.g. word limits), and it has already been proposed as a tool for administrative tasks, clinical work and patient education. While this does represent an impressive advance in natural language processing, and benefits may be manifold across fields including medicine, these limited use-cases do not live up to the hype surrounding LLMs and artificial intelligence (AI) more generally in 2023. This is due to a fundamental misunderstanding about the form of AI represented by LLMs. Do LLMs represent artificial generalised intelligence (AGI)? The answer is currently probably not, despite emergence of interactive conversational interfaces and few-shot or zero-shot properties \u2013 where models execute tasks that they have previously been exposed to only a few times before, or never before, respectively. This is demonstrated by observing how these models are trained, and the composition of their architecture. The backend LLM (GPT-3, from which GPT-3.5 was developed) underpinning older versions of ChatGPT was initially trained on a dataset of billions of words taken from books, Wikipedia and the wider internet. Through a process of machine learning, the GPT-3 accurately encoded the association between individual words in the training dataset. Through \u2018reinforcement learning from human feedback\u2019, GPT-3 was subsequently finetuned to provide appropriate responses to users\u2019 queries \u2013 producing GPT-3.5. Through these processes, ChatGPT has developed an impressive ability to respond appropriately to diverse prompts, albeit equally lucidly with accurate and inaccurate statements. This lucidity, responsiveness and flexibility have led to sensational claims regarding attainment of AGI that could feasibly replace professionals in cognitive roles. The performance of GPT-4 \u2013 which powers newer versions of ChatGPT \u2013 dwarfs that of GPT-3.5 across tasks including logical reasoning and medical aptitude tests. Moreover, GPT-4 can be prompted to adopt different roles on demand, and will accept multimodal input, processing images as well as text. Prominent figures in industry and academia have advocated for a moratorium on development of more advanced AI systems in response to concerns regarding safety, ethics and fears of replacement. Despite these fears and hype, the barriers to implementation of LLMs replacing healthcare professionals in any capacity still look out of reach. Although GPT-4\u2019s architecture and training are confidential, it likely relies on similar schemata to its predecessor as it exhibits similar (albeit fewer) hallucinations and reasoning errors, including in medicine. None of ChatGPT\u2019s published autonomous training involved actual comprehension of language in context; the meaning (as we understand it) of words in the dataset was immaterial throughout. While this brute force linguistic processing may prove sufficient to develop a form of AGI, it appears that these LLMs will continue to be afflicted by mistakes and errors. Journal of the Royal Society of Medicine; 2023, Vol. 116(5) 181\u2013182",
            "year": 2023,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Following the release of ChatGPT, large language models (LLMs) have entered the mainstream and recently garnered particular attention for attaining expert-level performance in United States Medical Licensing Examinations, but performance has not been as impressive in more specialised tests."
            },
            "score": 3
        },
        {
            "id": "754d5164e196ff231786d10a48594f3f27d8721f",
            "paperId": "754d5164e196ff231786d10a48594f3f27d8721f",
            "title": "A Comprehensive Study of Multimodal Large Language Models for Image Quality Assessment",
            "abstract": "While Multimodal Large Language Models (MLLMs) have experienced significant advancement on visual understanding and reasoning, their potentials to serve as powerful, flexible, interpretable, and text-driven models for Image Quality Assessment (IQA) remains largely unexplored. In this paper, we conduct a comprehensive and systematic study of prompting MLLMs for IQA. Specifically, we first investigate nine prompting systems for MLLMs as the combinations of three standardized testing procedures in psychophysics (i.e., the single-stimulus, double-stimulus, and multiple-stimulus methods) and three popular prompting strategies in natural language processing (i.e., the standard, in-context, and chain-of-thought prompting). We then present a difficult sample selection procedure, taking into account sample diversity and uncertainty, to further challenge MLLMs equipped with the respective optimal prompting systems. We assess three open-source and one close-source MLLMs on several visual attributes of image quality (e.g., structural and textural distortions, color differences, and geometric transformations) in both full-reference and no-reference scenarios. Experimental results show that only the close-source GPT-4V provides a reasonable account for human perception of image quality, but is weak at discriminating fine-grained quality variations (e.g., color differences) and at comparing visual quality of multiple images, tasks humans can perform effortlessly.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A comprehensive and systematic study of prompting MLLMs for IQA and presents a difficult sample selection procedure, taking into account sample diversity and uncertainty, to further challenge MLLMs equipped with the respective optimal prompting systems."
            },
            "score": 3
        },
        {
            "id": "61f329722cd94291898c2c8131606a55f7a07219",
            "paperId": "61f329722cd94291898c2c8131606a55f7a07219",
            "title": "Broken Neural Scaling Laws",
            "abstract": "We present a smoothly broken power law functional form (that we refer to as a Broken Neural Scaling Law (BNSL)) that accurately models&extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as amount of compute used for training (or inference), number of model parameters, training dataset size, model input size, number of training steps, or upstream performance varies) for various architectures&for each of various tasks within a large&diverse set of upstream&downstream tasks, in zero-shot, prompted,&finetuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, AI capabilities, robotics, out-of-distribution (OOD) generalization, continual learning, transfer learning, uncertainty estimation / calibration, OOD detection, adversarial robustness, distillation, sparsity, retrieval, quantization, pruning, fairness, molecules, computer programming/coding, math word problems,\"emergent phase transitions\", arithmetic, supervised learning, unsupervised/self-supervised learning,&reinforcement learning (single agent&multi-agent). When compared to other functional forms for neural scaling, this functional form yields extrapolations of scaling behavior that are considerably more accurate on this set. Moreover, this functional form accurately models&extrapolates scaling behavior that other functional forms are incapable of expressing such as the nonmonotonic transitions present in the scaling behavior of phenomena such as double descent&the delayed, sharp inflection points present in the scaling behavior of tasks such as arithmetic. Lastly, we use this functional form to glean insights about the limit of the predictability of scaling behavior. Code is available at https://github.com/ethancaballero/broken_neural_scaling_laws",
            "year": 2022,
            "citationCount": 42,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A smoothly broken power law functional form that accurately models and extrapolates scaling behavior that other functional forms are incapable of expressing such as the nonmonotonic transitions present in the scaling behavior of phenomena such as double descent and the delayed, sharp inflection points present inThe limit of the predictability of scaling behavior is gleaned."
            },
            "score": 3
        },
        {
            "id": "2af358b4771d7bffe491077466fc4d225a16a74b",
            "paperId": "2af358b4771d7bffe491077466fc4d225a16a74b",
            "title": "Large Language Models as Annotators: Enhancing Generalization of NLP Models at Minimal Cost",
            "abstract": "State-of-the-art supervised NLP models achieve high accuracy but are also susceptible to failures on inputs from low-data regimes, such as domains that are not represented in training data. As an approximation to collecting ground-truth labels for the specific domain, we study the use of large language models (LLMs) for annotating inputs and improving the generalization of NLP models. Specifically, given a budget for LLM annotations, we present an algorithm for sampling the most informative inputs to annotate and retrain the NLP model. We find that popular active learning strategies such as uncertainty-based sampling do not work well. Instead, we propose a sampling strategy based on the difference in prediction scores between the base model and the finetuned NLP model, utilizing the fact that most NLP models are finetuned from a base model. Experiments with classification (semantic similarity) and ranking (semantic search) tasks show that our sampling strategy leads to significant gains in accuracy for both the training and target domains.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a sampling strategy based on the difference in prediction scores between the base model and the finetuned NLP model, utilizing the fact that most NLP models are finetuning from a base model."
            },
            "score": 3
        },
        {
            "id": "6bf34b4a1937ca5ae692594eda880ff671b8ee57",
            "paperId": "6bf34b4a1937ca5ae692594eda880ff671b8ee57",
            "title": "Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration",
            "abstract": "Membership Inference Attacks (MIA) aim to infer whether a target data record has been utilized for model training or not. Prior attempts have quantified the privacy risks of language models (LMs) via MIAs, but there is still no consensus on whether existing MIA algorithms can cause remarkable privacy leakage on practical Large Language Models (LLMs). Existing MIAs designed for LMs can be classified into two categories: reference-free and reference-based attacks. They are both based on the hypothesis that training records consistently strike a higher probability of being sampled. Nevertheless, this hypothesis heavily relies on the overfitting of target models, which will be mitigated by multiple regularization methods and the generalization of LLMs. The reference-based attack seems to achieve promising effectiveness in LLMs, which measures a more reliable membership signal by comparing the probability discrepancy between the target model and the reference model. However, the performance of reference-based attack is highly dependent on a reference dataset that closely resembles the training dataset, which is usually inaccessible in the practical scenario. Overall, existing MIAs are unable to effectively unveil privacy leakage over practical fine-tuned LLMs that are overfitting-free and private. We propose a Membership Inference Attack based on Self-calibrated Probabilistic Variation (SPV-MIA). Specifically, since memorization in LLMs is inevitable during the training process and occurs before overfitting, we introduce a more reliable membership signal, probabilistic variation, which is based on memorization rather than overfitting. Furthermore, we introduce a self-prompt approach, which constructs the dataset to fine-tune the reference model by prompting the target LLM itself. In this manner, the adversary can collect a dataset with a similar distribution from public APIs.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Membership Inference Attack based on Self-calibrated Probabilistic Variation (SPV-MIA), which introduces a more reliable membership signal, probabilistic variation, which is based on memorization rather than overfitting in LLMs."
            },
            "score": 3
        },
        {
            "id": "3a26035711b1119518103b60c51559ef2f5c12c5",
            "paperId": "3a26035711b1119518103b60c51559ef2f5c12c5",
            "title": "On the Calibration of Uncertainty Estimation in LiDAR-Based Semantic Segmentation",
            "abstract": "The confidence calibration of deep learning-based perception models plays a crucial role in their reliability. Especially in the context of autonomous driving, downstream tasks like prediction and planning depend on accurate confidence estimates. In the point-wise multiclass classification task of sematic segmentation the model naturally has to deal with heavy class imbalances. Due to their underrepresentation, the confidence calibration of classes with smaller instances is challenging but essential, especially for safety reasons. We propose a metric to measure the confidence calibration quality of a semantic segmentation model with respect to individual classes. It utilizes the computation of sparsification curves based on uncertainty estimates. The insights provided by a careful analysis using the proposed metric uncover potential improvements of any semantic segmentation model and allow for a specific improvement of the model's classification and calibration performance.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A metric to measure the confidence calibration quality of a semantic segmentation model with respect to individual classes is proposed, which utilizes the computation of sparsification curves based on uncertainty estimates."
            },
            "score": 3
        },
        {
            "id": "32d59ab951be74be351f9777da2cbc71bb68c3c1",
            "paperId": "32d59ab951be74be351f9777da2cbc71bb68c3c1",
            "title": "A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models",
            "abstract": "Large pre-trained vision-language (VL) models can learn a new task with a handful of examples and generalize to a new task without fine-tuning.However, these VL models are hard to deploy for real-world applications due to their impractically huge sizes and slow inference speed.To solve this limitation, we study prompt-based low-resource learning of VL tasks with our proposed method, FewVLM, relatively smaller than recent few-shot learners.For FewVLM, we pre-train a sequence-to-sequence transformer model with prefix language modeling (PrefixLM) and masked language modeling (MaskedLM).Furthermore, we analyze the effect of diverse prompts for few-shot tasks.Experimental results on VQA show that FewVLM with prompt-based learning outperforms Frozen which is 31x larger than FewVLM by 18.2% point and achieves comparable results to a 246x larger model, PICa.In our analysis, we observe that (1) prompts significantly affect zero-shot performance but marginally affect few-shot performance, (2) models with noisy prompts learn as quickly as hand-crafted prompts given larger training data, and (3) MaskedLM helps VQA tasks while PrefixLM boosts captioning performance. Our code is publicly available at https://github.com/woojeongjin/FewVLM",
            "year": 2021,
            "citationCount": 93,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work studies prompt-based low-resource learning of VL tasks with a sequence-to-sequence transformer model with prefix language modeling and masked language modeling, and observes that models with noisy prompts learn as quickly as hand-crafted prompts given larger training data."
            },
            "score": 3
        },
        {
            "id": "a8680b3419f3cbe6650f72b1023aed0ad0becb9e",
            "paperId": "a8680b3419f3cbe6650f72b1023aed0ad0becb9e",
            "title": "Chain of Thought Prompt Tuning in Vision Language Models",
            "abstract": "Language-Image Pre-training has demonstrated promising results on zero-shot and few-shot downstream tasks by prompting visual models with natural language prompts. However, most recent studies only use a single prompt for tuning, neglecting the inherent step-to-step cognitive reasoning process that humans conduct in complex task settings, for example, when processing images from unfamiliar domains. Chain of Thought is a simple and effective approximation to human reasoning process and has been proven useful for natural language processing (NLP) tasks. Based on this cognitive intuition, we believe that conducting effective reasoning is also an important problem in visual tasks, and a chain of thought could be a solution to this problem. In this work, we propose a novel chain of thought prompt tuning for vision-language modeling. Extensive experiments show that our method not only generalizes better in image classification tasks, has greater transferability beyond a single dataset, and has stronger domain generalization performance, but also performs much better in imagetext retrieval and visual question answering, which require more reasoning capabilities. We are the first to successfully adapt chain-of-thought prompting that combines visual and textual embeddings. We will release our codes",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Extensive experiments show that the proposed novel chain of thought prompt tuning for vision-language modeling not only generalizes better in image classification tasks, has greater transferability beyond a single dataset, and has stronger domain generalization performance, but also performs much better in imagetext retrieval and visual question answering, which require more reasoning capabilities."
            },
            "score": 3
        },
        {
            "id": "1c67237ddd6f7a186fb8189b21ce906f51aa1dcb",
            "paperId": "1c67237ddd6f7a186fb8189b21ce906f51aa1dcb",
            "title": "Is Prompt-Based Finetuning Always Better than Vanilla Finetuning? Insights from Cross-Lingual Language Understanding",
            "abstract": "Multilingual pretrained language models (MPLMs) have demonstrated substantial performance improvements in zero-shot cross-lingual transfer across various natural language understanding tasks by finetuning MPLMs on task-specific labelled data of a source language (e.g. English) and evaluating on a wide range of target languages. Recent studies show that prompt-based finetuning surpasses regular finetuning in few-shot scenarios. However, the exploration of prompt-based learning in multilingual tasks remains limited. In this study, we propose the ProFiT pipeline to investigate the cross-lingual capabilities of Prompt-based Finetuning. We conduct comprehensive experiments on diverse cross-lingual language understanding tasks (sentiment classification, paraphrase identification, and natural language inference) and empirically analyze the variation trends of prompt-based finetuning performance in cross-lingual transfer across different few-shot and full-data settings. Our results reveal the effectiveness and versatility of prompt-based finetuning in cross-lingual language understanding. Our findings indicate that prompt-based finetuning outperforms vanilla finetuning in full-data scenarios and exhibits greater advantages in few-shot scenarios, with different performance patterns dependent on task types. Additionally, we analyze underlying factors such as language similarity and pretraining data size that impact the cross-lingual performance of prompt-based finetuning. Overall, our work provides valuable insights into the cross-lingual prowess of prompt-based finetuning.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The ProFiT pipeline is proposed to investigate the cross-lingual capabilities of Prompt-based Finetuning and indicates that prompt-based finetuning outperforms vanillaFinetuning in full-data scenarios and exhibits greater advantages in few-shot scenarios, with different performance patterns dependent on task types."
            },
            "score": 3
        },
        {
            "id": "dbe05ab53d0afa900633df965aaaf5bed8bacd86",
            "paperId": "dbe05ab53d0afa900633df965aaaf5bed8bacd86",
            "title": "APoLLo : Unified Adapter and Prompt Learning for Vision Language Models",
            "abstract": "The choice of input text prompt plays a critical role in the performance of Vision-Language Pretrained (VLP) models such as CLIP. We present APoLLo, a unified multi-modal approach that combines Adapter and Prompt learning for Vision-Language models. Our method is designed to substantially improve the generalization capabilities of VLP models when they are fine-tuned in a few-shot setting. We introduce trainable cross-attention-based adapter layers in conjunction with vision and language encoders to strengthen the alignment between the two modalities. We enforce consistency between the respective encoder branches (receiving augmented inputs) to prevent overfitting in downstream tasks. Our method is evaluated on three representative tasks: generalization to novel classes, cross-dataset evaluation, and unseen domain shifts. In practice, APoLLo achieves a relative gain up to 6.03% over MaPLe (SOTA) on novel classes for 10 diverse image recognition datasets.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "APoLLo, a unified multi-modal approach that combines Adapter and Prompt learning for Vision-Language models is presented, designed to substantially improve the generalization capabilities of VLP models when they are fine-tuned in a few-shot setting."
            },
            "score": 3
        },
        {
            "id": "e9ae0c76a71b8f302eb17b1c4462b9cc97d87cd0",
            "paperId": "e9ae0c76a71b8f302eb17b1c4462b9cc97d87cd0",
            "title": "LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models",
            "abstract": "Recent advancements in text-to-image diffusion models have yielded impressive results in generating realistic and diverse images. However, these models still struggle with complex prompts, such as those that involve numeracy and spatial reasoning. This work proposes to enhance prompt understanding capabilities in diffusion models. Our method leverages a pretrained large language model (LLM) for grounded generation in a novel two-stage process. In the first stage, the LLM generates a scene layout that comprises captioned bounding boxes from a given prompt describing the desired image. In the second stage, a novel controller guides an off-the-shelf diffusion model for layout-grounded image generation. Both stages utilize existing pretrained models without additional model parameter optimization. Our method significantly outperforms the base diffusion model and several strong baselines in accurately generating images according to prompts that require various capabilities, doubling the generation accuracy across four tasks on average. Furthermore, our method enables instruction-based multi-round scene specification and can handle prompts in languages not supported by the underlying diffusion model. We anticipate that our method will unleash users' creativity by accurately following more complex prompts. Our code, demo, and benchmark are available at: https://llm-grounded-diffusion.github.io",
            "year": 2023,
            "citationCount": 52,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes to enhance prompt understanding capabilities in diffusion models by leveraging a pretrained large language model for grounded generation in a novel two-stage process that significantly outperforms the base diffusion model and several strong baselines in accurately generating images according to prompts."
            },
            "score": 3
        },
        {
            "id": "700bd9681f1b9e9e2212e10415d27b11c7e6836b",
            "paperId": "700bd9681f1b9e9e2212e10415d27b11c7e6836b",
            "title": "Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models",
            "abstract": "While large language models (LLMs) have demonstrated impressive performance on a range of decision-making tasks, they rely on simple acting processes and fall short of broad deployment as autonomous agents. We introduce LATS (Language Agent Tree Search), a general framework that synergizes the capabilities of LLMs in planning, acting, and reasoning. Drawing inspiration from Monte Carlo tree search in model-based reinforcement learning, LATS employs LLMs as agents, value functions, and optimizers, repurposing their latent strengths for enhanced decision-making. What is crucial in this method is the use of an environment for external feedback, which offers a more deliberate and adaptive problem-solving mechanism that moves beyond the limitations of existing techniques. Our experimental evaluation across diverse domains, such as programming, HotPotQA, and WebShop, illustrates the applicability of LATS for both reasoning and acting. In particular, LATS achieves 94.4% for programming on HumanEval with GPT-4 and an average score of 75.9 for web browsing on WebShop with GPT-3.5, demonstrating the effectiveness and generality of our method.",
            "year": 2023,
            "citationCount": 32,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces LATS (Language Agent Tree Search), a general framework that synergizes the capabilities of LLMs in planning, acting, and reasoning, and employs LLMs as agents, value functions, and optimizers, repurposing their latent strengths for enhanced decision-making."
            },
            "score": 3
        },
        {
            "id": "fa769bcca7d195add635409d1c1abedba0561967",
            "paperId": "fa769bcca7d195add635409d1c1abedba0561967",
            "title": "Cross-lingual Transfer Learning and Multitask Learning for Capturing Multiword Expressions",
            "abstract": "Recent developments in deep learning have prompted a surge of interest in the application of multitask and transfer learning to NLP problems. In this study, we explore for the first time, the application of transfer learning (TRL) and multitask learning (MTL) to the identification of Multiword Expressions (MWEs). For MTL, we exploit the shared syntactic information between MWE and dependency parsing models to jointly train a single model on both tasks. We specifically predict two types of labels: MWE and dependency parse. Our neural MTL architecture utilises the supervision of dependency parsing in lower layers and predicts MWE tags in upper layers. In the TRL scenario, we overcome the scarcity of data by learning a model on a larger MWE dataset and transferring the knowledge to a resource-poor setting in another language. In both scenarios, the resulting models achieved higher performance compared to standard neural approaches.",
            "year": 2019,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study explores for the first time, the application of transfer learning (TRL) and multitask learning (MTL) to the identification of Multiword Expressions (MWEs) and predicts two types of labels: MWE and dependency parse."
            },
            "score": 3
        },
        {
            "id": "65768dd5531295e2e9a872ad7c8d99c07b8747a4",
            "paperId": "65768dd5531295e2e9a872ad7c8d99c07b8747a4",
            "title": "LMSanitator: Defending Prompt-Tuning Against Task-Agnostic Backdoors",
            "abstract": "Prompt-tuning has emerged as an attractive paradigm for deploying large-scale language models due to its strong downstream task performance and efficient multitask serving ability. Despite its wide adoption, we empirically show that prompt-tuning is vulnerable to downstream task-agnostic backdoors, which reside in the pretrained models and can affect arbitrary downstream tasks. The state-of-the-art backdoor detection approaches cannot defend against task-agnostic backdoors since they hardly converge in reversing the backdoor triggers. To address this issue, we propose LMSanitator, a novel approach for detecting and removing task-agnostic backdoors on Transformer models. Instead of directly inverting the triggers, LMSanitator aims to invert the predefined attack vectors (pretrained models' output when the input is embedded with triggers) of the task-agnostic backdoors, which achieves much better convergence performance and backdoor detection accuracy. LMSanitator further leverages prompt-tuning's property of freezing the pretrained model to perform accurate and fast output monitoring and input purging during the inference phase. Extensive experiments on multiple language models and NLP tasks illustrate the effectiveness of LMSanitator. For instance, LMSanitator achieves 92.8% backdoor detection accuracy on 960 models and decreases the attack success rate to less than 1% in most scenarios.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LMSanitator is proposed, a novel approach for detecting and removing task-agnostic backdoors on Transformer models that aims to invert the predefined attack vectors of the pretrained models' output when the input is embedded with triggers, which achieves much better convergence performance and backdoor detection accuracy."
            },
            "score": 3
        },
        {
            "id": "bb9a44c94a89dbe00f0061d05c70a45064ff6ea6",
            "paperId": "bb9a44c94a89dbe00f0061d05c70a45064ff6ea6",
            "title": "CMMLU: Measuring massive multitask language understanding in Chinese",
            "abstract": "As the capabilities of large language models (LLMs) continue to advance, evaluating their performance becomes increasingly crucial and challenging. This paper aims to bridge this gap by introducing CMMLU, a comprehensive Chinese benchmark that covers various subjects, including natural science, social sciences, engineering, and humanities. We conduct a thorough evaluation of 18 advanced multilingual- and Chinese-oriented LLMs, assessing their performance across different subjects and settings. The results reveal that most existing LLMs struggle to achieve an average accuracy of 50%, even when provided with in-context examples and chain-of-thought prompts, whereas the random baseline stands at 25%. This highlights significant room for improvement in LLMs. Additionally, we conduct extensive experiments to identify factors impacting the models' performance and propose directions for enhancing LLMs. CMMLU fills the gap in evaluating the knowledge and reasoning capabilities of large language models within the Chinese context.",
            "year": 2023,
            "citationCount": 81,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is revealed that most existing LLMs struggle to achieve an average accuracy of 50%, even when provided with in-context examples and chain-of-thought prompts, whereas the random baseline stands at 25%."
            },
            "score": 2
        },
        {
            "id": "5b2eccd2decac6a45fc05ce3979cb56a0089e957",
            "paperId": "5b2eccd2decac6a45fc05ce3979cb56a0089e957",
            "title": "Indo LEGO-ABSA: A Multitask Generative Aspect Based Sentiment Analysis for Indonesian Language",
            "abstract": "Aspect-based sentiment analysis is a method in natural language processing aimed at identifying and understanding sentiments related to specific aspects of an entity. Aspects are words or phrases that represent an aspect or attribute of a particular entity. Earlier studies have applied generative pretrained language model for aspect-based sentiment analysis. An example of this is the LEGO-ABSA framework, which effectively utilized these models, specifically in English-based aspect-based sentiment analysis. LEGO-ABSA uses a multitask learning and prompting approach to enhance model performance. However, the application of this approach has not been done in the context of Indonesian language. Therefore, this research aims to implement the multitask learning and prompting approach in aspect-based sentiment analysis for Indonesian language using generative pretrained language model. In this study, the Indo LEGO-ABSA model is developed, which is an aspect-based sen-timent analysis model utilizing generative pretrained language model and trained with multitask learning and prompting. Indo LEGO-ABSA is trained with a hotel domain dataset in the Indonesian language. The obtained results include an f1-score of 79.55% for the Aspect Sentiment Triplet Extraction, 86.09% for Unified Aspect-based Sentiment Analysis, 79.85% for Aspect Opinion Pair Extraction, 87.45% for Aspect Term Extraction, and 88.09% for Opinion Term Extraction. Indo LEGO-ABSA adopts the LEGO-ABSA framework that employs the T5 model, specifically mT5, by applying multitask learning to train all tasks within aspect-based sentiment analysis.11All works can be visited in https://github.com/rdyzakya/IndoLEGO-ABSA",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Indo LEGO-ABSA model is developed, which is an aspect-based sen-timent analysis model utilizing generative pretrained language model and trained with multitask learning and prompting."
            },
            "score": 2
        },
        {
            "id": "6ad93900b1c956020242653e33ac447824f75fc6",
            "paperId": "6ad93900b1c956020242653e33ac447824f75fc6",
            "title": "PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models",
            "abstract": "Prompts have significantly improved the performance of pretrained Large Language Models (LLMs) on various downstream tasks recently, making them increasingly indispensable for a diverse range of LLM application scenarios. However, the backdoor vulnerability, a serious security threat that can maliciously alter the victim model's normal predictions, has not been sufficiently explored for prompt-based LLMs. In this paper, we present POISONPROMPT, a novel backdoor attack capable of successfully compromising both hard and soft prompt-based LLMs. We evaluate the effectiveness, fidelity, and robustness of POISONPROMPT through extensive experiments on three popular prompt methods, using six datasets and three widely used LLMs. Our findings highlight the potential security threats posed by backdoor attacks on prompt-based LLMs and emphasize the need for further research in this area.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel backdoor attack capable of successfully compromising both hard and soft prompt-based LLMs is presented, and the effectiveness, fidelity, and robustness of POISONPROMPT is evaluated through extensive experiments on three popular prompt methods."
            },
            "score": 2
        },
        {
            "id": "9d3cbe8d25c8f093714f6e5cfaf5a9e8f2f938e9",
            "paperId": "9d3cbe8d25c8f093714f6e5cfaf5a9e8f2f938e9",
            "title": "Robot Behavior-Tree-Based Task Generation with Large Language Models",
            "abstract": "Nowadays, the behavior tree is gaining popularity as a representation for robot tasks due to its modularity and reusability. Designing behavior-tree tasks manually is time-consuming for robot end-users, thus there is a need for investigating automatic behavior-tree-based task generation. Prior behavior-tree-based task generation approaches focus on fixed primitive tasks and lack generalizability to new task domains. To cope with this issue, we propose a novel behavior-tree-based task generation approach that utilizes state-of-the-art large language models. We propose a Phase-Step prompt design that enables a hierarchical-structured robot task generation and further integrate it with behavior-tree-embedding-based search to set up the appropriate prompt. In this way, we enable an automatic and cross-domain behavior-tree task generation. Our behavior-tree-based task generation approach does not require a set of pre-defined primitive tasks. End-users only need to describe an abstract desired task and our proposed approach can swiftly generate the corresponding behavior tree. A full-process case study is provided to demonstrate our proposed approach. An ablation study is conducted to evaluate the effectiveness of our Phase-Step prompts. Assessment on Phase-Step prompts and the limitation of large language models are presented and discussed.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Phase-Step prompt design is proposed that enables a hierarchical-structured robot task generation and further integrate it with behavior-tree-embedding-based search to set up the appropriate prompt and enable an automatic and cross-domain behavior- tree task generation."
            },
            "score": 2
        },
        {
            "id": "66d7d8dc54ea3dff10a11df2f29dc2104df86a57",
            "paperId": "66d7d8dc54ea3dff10a11df2f29dc2104df86a57",
            "title": "XrayGPT: Chest Radiographs Summarization using Medical Vision-Language Models",
            "abstract": "The latest breakthroughs in large vision-language models, such as Bard and GPT-4, have showcased extraordinary abilities in performing a wide range of tasks. Such models are trained on massive datasets comprising billions of public image-text pairs with diverse tasks. However, their performance on task-specific domains, such as radiology, is still under-investigated and potentially limited due to a lack of sophistication in understanding biomedical images. On the other hand, conversational medical models have exhibited remarkable success but have mainly focused on text-based analysis. In this paper, we introduce XrayGPT, a novel conversational medical vision-language model that can analyze and answer open-ended questions about chest radiographs. Specifically, we align both medical visual encoder (MedClip) with a fine-tuned large language model (Vicuna), using a simple linear transformation. This alignment enables our model to possess exceptional visual conversation abilities, grounded in a deep understanding of radiographs and medical domain knowledge. To enhance the performance of LLMs in the medical context, we generate ~217k interactive and high-quality summaries from free-text radiology reports. These summaries serve to enhance the performance of LLMs through the fine-tuning process. Our approach opens up new avenues the research for advancing the automated analysis of chest radiographs. Our open-source demos, models, and instruction sets are available at: https://github.com/mbzuai-oryx/XrayGPT.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces XrayGPT, a novel conversational medical vision-language model that can analyze and answer open-ended questions about chest radiographs and aligns both medical visual encoder (MedClip) with a fine-tuned large language model (Vicuna), using a simple linear transformation."
            },
            "score": 2
        },
        {
            "id": "bbe197158adb4b6e85a6eeab4619ea0fc6857941",
            "paperId": "bbe197158adb4b6e85a6eeab4619ea0fc6857941",
            "title": "MarioGPT: Open-Ended Text2Level Generation through Large Language Models",
            "abstract": "Procedural Content Generation (PCG) algorithms provide a technique to generate complex and diverse environments in an automated way. However, while generating content with PCG methods is often straightforward, generating meaningful content that reflects specific intentions and constraints remains challenging. Furthermore, many PCG algorithms lack the ability to generate content in an open-ended manner. Recently, Large Language Models (LLMs) have shown to be incredibly effective in many diverse domains. These trained LLMs can be fine-tuned, re-using information and accelerating training for new tasks. In this work, we introduce MarioGPT, a fine-tuned GPT2 model trained to generate tile-based game levels, in our case Super Mario Bros levels. We show that MarioGPT can not only generate diverse levels, but can be text-prompted for controllable level generation, addressing one of the key challenges of current PCG techniques. As far as we know, MarioGPT is the first text-to-level model. We also combine MarioGPT with novelty search, enabling it to generate diverse levels with varying play-style dynamics (i.e. player paths). This combination allows for the open-ended generation of an increasingly diverse range of content.",
            "year": 2023,
            "citationCount": 21,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "MarioGPT is the first text-to-level model trained to generate tile-based game levels, and it is shown that MarioGPT can not only generate diverse levels, but can be text-prompted for controllable level generation, addressing one of the key challenges of current PCG techniques."
            },
            "score": 2
        },
        {
            "id": "76a3f4a79ae9a00db2f2b5f6877021d8deb96ada",
            "paperId": "76a3f4a79ae9a00db2f2b5f6877021d8deb96ada",
            "title": "SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models",
            "abstract": "We present SPHINX, a versatile multi-modal large language model (MLLM) with a joint mixing of model weights, tuning tasks, and visual embeddings. First, for stronger vision-language alignment, we unfreeze the large language model (LLM) during pre-training, and introduce a weight mix strategy between LLMs trained by real-world and synthetic data. By directly integrating the weights from two domains, the mixed LLM can efficiently incorporate diverse semantics with favorable robustness. Then, to enable multi-purpose capabilities, we mix a variety of tasks for joint visual instruction tuning, and design task-specific instructions to avoid inter-task conflict. In addition to the basic visual question answering, we include more challenging tasks such as region-level understanding, caption grounding, document layout detection, and human pose estimation, contributing to mutual enhancement over different scenarios. Additionally, we propose to extract comprehensive visual embeddings from various network architectures, pre-training paradigms, and information granularity, providing language models with more robust image representations. Based on our proposed joint mixing, SPHINX exhibits superior multi-modal understanding capabilities on a wide range of applications. On top of this, we further propose an efficient strategy aiming to better capture fine-grained appearances of high-resolution images. With a mixing of different scales and high-resolution sub-images, SPHINX attains exceptional visual parsing and reasoning performance on existing evaluation benchmarks. We hope our work may cast a light on the exploration of joint mixing in future MLLM research. Code is released at https://github.com/Alpha-VLLM/LLaMA2-Accessory.",
            "year": 2023,
            "citationCount": 59,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "SPHINX, a versatile multi-modal large language model (MLLM) with a joint mixing of model weights, tuning tasks, and visual embeddings is presented, and an efficient strategy aiming to better capture fine-grained appearances of high-resolution images is proposed."
            },
            "score": 2
        },
        {
            "id": "213a616590be543c5a2d7dee3206ecd816c58b4a",
            "paperId": "213a616590be543c5a2d7dee3206ecd816c58b4a",
            "title": "Macroeconomic Effects of Uncertainty: A Big Data Analysis for India",
            "abstract": "Uncertainty about the current state and near-term outlook of an economy as well as the likely course of future policy actions can prompt economic agents to alter their decisions to spend, save, invest and hire. In this paper, we construct three alternative indices to measure the level of uncertainty for the Indian economy. The first two uncertainty indices are constructed by applying text mining and natural language processing (NLP) techniques on a dataset compiled from leading Indian business newspapers. The third index is based on internet search intensity data available from Google Trends. Empirical findings from a Local Projections-based econometric framework suggest that uncertainty shocks influence financial markets as well as the real economy in India. Our results indicate that both investment activity and real GDP growth slow down when uncertainty increases in the economy. Such uncertainty indices can help strengthen policy simulation exercises to study the impact of low/high uncertainty scenarios and also improve near-term projection of macroeconomic variables which exhibit high degree of sensitivity to uncertainty.",
            "year": 2020,
            "citationCount": 7,
            "tldr": null,
            "score": 2
        },
        {
            "id": "740e777ff049102081673bec1adb48ba3169fdba",
            "paperId": "740e777ff049102081673bec1adb48ba3169fdba",
            "title": "Interpretable Natural Language Understanding",
            "abstract": "In recent years, we have witnessed the shift of paradigms in Natural Language Processing (NLP) from fine-tuning large-scale pre-trained language models (PLMs) on task-specific data to prompt-based learning. In the latter, the task description is embedded into the PLM input, enabling the same model to handle multiple tasks. While both approaches have demonstrated impressive performance in various NLP tasks, their opaque nature makes comprehending their inner workings and decision-making processes challenging for humans. In this talk, I will share the research undertaken in my group to address the interpretability concerns surrounding neural models in language understanding. This includes a hierarchical interpretable text classifier going beyond word-level interpretations, uncertainty interpretation of text classifiers built on PLMs, explainable recommender systems by harnessing information across diverse modalities, and explainable student answer scoring. I will conclude my talk by offering insights into potential future developments in interpretable language understanding.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This talk will share the research undertaken in the group to address the interpretability concerns surrounding neural models in language understanding, including a hierarchical interpretable text classifier going beyond word-level interpretations, uncertainty interpretation of text classifiers built on PLMs, explainable recommender systems by harnessing information across diverse modalities, and explainable student answer scoring."
            },
            "score": 2
        },
        {
            "id": "d4838211d7f65628f56b9f6faab30a95ff7b51f8",
            "paperId": "d4838211d7f65628f56b9f6faab30a95ff7b51f8",
            "title": "for Prediction City Region Re-Weighting",
            "abstract": "To address these challenges, this paper proposes a new multi-task learning framework that jointly learns latent features and explicit task relations by complementing the strength of existing shallow and deep multitask learning scenarios. In this paper, the first attempt towards partial label feature selection is investigated via mutual-information- based dependency maximization. We first show that existing link prediction schemes fail to effectively predict motifs. To alleviate this, we establish a general motif prediction problem and we propose several heuristics that assess the chances for a specified motif to appear. lossless Existing basin characteristics suffer from noise and uncertainty, among many other things, which adversely impact model performance. To tackle the above challenges, in this paper, we propose a novel Knowledge-guided Self-Supervised Learning (KGSSL) inverse framework to extract system characteristics from driver(input) and response(output) data. proposes DynAnom, an efficient framework to quantify the changes and localize per-node anomalies over large dynamic weighted-graphs. trade-off, we a new algorithm, called BaLanced for Scalable Search (BLISS), a highly tunable indexing algorithm enviably small index easy temporal for continuous-time distribution of actions in an activity sequence simultaneously addressing three high-impact problems - next action prediction, sequence-goal prediction, and end-to-end sequence generation. This proposes a novel sketch-based distributed method that achieves sub-linear communication costs for distributed sampling-based NDV estimation under mild assumptions. representations that carry both feature information and as mush correct structure information as possible and are insensitive to structural perturbations. To this end, we propose an unsupervised effective graph structural disrupt graph spectral filters in the Fourier domain, theoretical This paper builds an efficient graph neural network model that incorporates both region-mapped fMRI sequences and structural connectivities obtained from DWI (diffusion-weighted imaging) as inputs. However, an embedding layer with random initialization often suffers in practice from the sparsity of the contextual features, as well as the interactions between the users (or items) and context. In this paper, we propose a novel user-event graph embedding learning (UEG-EL) framework to address these two sparsity challenges. SDSL imposes two under-addressed challenges on existing methods in semi-supervised learning and continuous learning: 1) robust pseudo-labeling under gradual shifts and 2) anti-forgetting adaptation with short lookback. To tackle these challenges, we propose a principled and generic generation-replay framework to solve SDSL. by the according these axioms, we objective Social The propensity model introduced by Jain et al has become a standard approach for dealing with missing and long-tail labels in extreme multi-label classification (XMLC). In this paper, we critically revise this approach showing that despite its theoretical soundness, its application in contemporary XMLC works is debatable. Based on the pre-trained model, we propose the graph prompting function to modify the standalone node into a token pair, and reformulate the downstream node classification looking the same as edge prediction. In paper, we propose pureGAM, an inherently pure additive model of both main effects and higher-order interactions. In study, we theoretically investigate why the CVAE cannot sufficiently reduce the task-dependency the simple standard Gaussian of the We study a variant of classical clustering formulations in the context of algorithmic fairness, known as diversity-aware clustering. relational structures from large-scale pre-trained language models (PLMs) via a probing procedure based on Poincar&eacute; distance metric, and use the induced relations to augment current graph-based parsers for better schema linking. we transformer extract to propose a Transformer-based Multi-Agent Actor-Critic framework (T-MAAC) to stabilize voltage in power distribution networks. This paper investigates a critical resource allocation problem in the first party cloud: scheduling containers to machines. This paper presents a framework for online deep anomaly detection, ARCUS, which can be instantiated with any autoencoder-based deep anomaly detection methods. Availability attacks, which poison the training data with imperceptible perturbations, can make the data not exploitable by machine learning algorithms so as to prevent unauthorized use of data. In this work, we investigate why these perturbations work in principle. However, most existing works ignore the relation heterogeneity with multiplex network between multi-typed nodes and different importance of relations in meta-paths for node embedding, which can hardly capture the heterogeneous structure signals across different relations. To tackle this challenge, this work proposes a Multiplex Heterogeneous Graph Convolutional Network (MHGCN) for heterogeneous network embedding. To we study the cross-domain few-shot learning problem over HGs and develop a novel model for Cross-domain Graph Meta learning (CrossHG-Meta). In this work, we investigate knowledge-graph embeddings for entities in the Twitter HIN (TwHIN); we show that these pretrained representations yield significant offline and online improvement for a diverse range of downstream recommendation and classification tasks: personalized ads rankings, account follow-recommendation, offensive content detection, and search ranking. we present a learning framework in Automatic We present results from a large-scale experiment on pretraining encoders with non-embedding parameter counts ranging from 700M to 9.3B, their subsequent distillation into smaller models ranging from 17M-170M parameters, and their application to the Natural Language Understanding (NLU) component of a virtual assistant system. In this work, we propose a framework called DP-GAT to identify regions containing significant biological structures and model the relationships among these regions as a graph along with their respective contexts. the lens of meta- reinforcement learning (meta-RL) devise actor-critic algorithm recurrent graph neural networks. In this paper, we apply probabilistic forecasting to FPT for the first time and propose a non-parametric method based on deep learning. HIGHLIGHT: Motivated by the industry practice of labeling data, we propose an innovative Inconsistency-based virtual aDvErsarial Active Learning (IDEAL) algorithm to further investigate SSL-AL's potential superiority and achieve mutual enhancement of AL and SSL, i.e., SSL propagates label information to unlabeled samples and provides smoothed embeddings for AL, while AL excludes samples with inconsistent predictions and considerable uncertainty for SSL. HIGHLIGHT: In this paper, we report our experience in deploying an E-commerce Prefix-based Controllable Copywriting Generation (EPCCG) system into the JD.com e-commerce product recommendation platform. present ERNIE-GeoL, which is a geography-and-language pre-trained model designed and developed for improving the geo-related tasks at Baidu Maps. In this paper, we present our efforts and findings of a 4-year longitudinal study on designing and implementing DuIVA, which is an intelligent voice assistant (IVA) embedded in the Baidu Maps app for hands-free, eyes-free human-to-app interaction in a fully voice-controlled manner. deep representation learning, we propose a generic, robust and systematic model that is able to combine multiple data modalities in a permutation and modes-number-invariant fashion, both fundamental properties to properly face changes in data type content and availability. A systematic investigation over simulated data reveals the fact that the self-attention architecture fails to learn some standard symbolic expressions like the count distinct operation. To overcome this deficiency, we propose a novel architecture named SHORING, which contains two components:event network andsequence network. In this work, a novel framework AutoFAS is proposed which jointly optimizes the efficiency and effectiveness of the pre-ranking model: (i) AutoFAS for the first time simultaneously selects the most valuable features and network architectures using Neural Architecture Search (NAS) technique; (ii) equipped with ranking model guided reward during NAS procedure, AutoFAS can select To this end, in this we propose to pre-train user behavior sequences, which consist of orderly arranged actions, from the large-scale unlabeled data sources for online fraud detection. large of unlabeled activity logs, we propose a semi-supervised framework to unlabeled clinician the HiPAL-based prediction model. Unlike existing qualitative studies, we propose a universal mixture model with the capability of accurately fitting dynamic activeness curves while reflecting the heterogeneous patterns of preference diversity. In this work, we propose a novel retrieval-based gradient boosting decision trees (RB-GBDT) model with a cross-sample extractor to mine cross-sample information while exploiting the superiority of GBDT of robustness, generalization and interpretability. paper, we introduce a large scale online multi-task deep learning framework for modeling multiple feed ads auction prediction tasks on an industry-scale feed ads recommendation platform. yet that operates in three stages hate of the original it identifies the hate span(s) within it; and finally, it reduces hate intensity by paraphrasing the hate This paper introduces a thermographic vision system to detect different types of hotspots on a variety of cable junctions commonly found in Hydro-Qu&eacute;bec underground electrical distribution network. In this paper, we present a practical system, which uses machine learning on large-scale telemetry data and documentation corpora, generating",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new multi-task learning framework that jointly learns latent features and explicit task relations by complementing the strength of existing shallow and deep multitask learning scenarios by proposing a principled and generic generation-replay framework to solve SDSL."
            },
            "score": 1
        },
        {
            "id": "87c1e8eff9f2a2f2374b5eac4791d34b30477619",
            "paperId": "87c1e8eff9f2a2f2374b5eac4791d34b30477619",
            "title": "Clinical Natural Language Processing in Languages Other Than English",
            "abstract": "Natural Language Processing (NLP) of clinical free-text has received a lot of attention from the scientific community. Clinical documents are routinely created across health care providing institutions and are generally written in the official language(s) of the country these institutions are located in. As a result, free-text clinical information is written in a large variety of languages. While most of the efforts for clinical NLP have focused on English, there is a strong need to extend this work to other languages, for instance in order to gain medical information about patient cohorts in geographical areas where English is not an official language. Furthermore, adapting current NLP methods developed for English to other languages may provide useful insight on the generalizability of algorithms and lead to increased robustness. This panel aims to provide an overview of clinical NLP for languages other than English, as for example French, Swedish and Japanese and discuss future methodological advances of clinical NLP in a context that encompasses English as well as other languages. General Description of the Panel The goal of this panel is to engage the medical informatics and clinical Natural Language Processing community in a discussion about ways to advance research through languages other than English. We will provide an overview the current state of clinical NLP in a variety of European and non-European languages as well as focused reports on French, Swedish and Bulgarian. We will motivate the need for developing clinical NLP in languages other than English by the potential for methodological and medical advances. Finally, we will propose strategies to contribute to advance work on languages other than English and integrate it in a state-of-the art platform. Clinical NLP in languages other than English Natural Language Processing (NLP) of clinical free-text has received a lot of attention from the scientific community, demonstrating its potential to provide the means to analyze large quantities of documents rapidly and accurately (Demner-Fushman et al. 2010). Prime clinical applications for NLP include assisting healthcare professionals with restrospective studies and clinical decision making. The ability to analyze clinical text in languages other than English opens access to important medical data concerning cohorts of patients who are treated in countries where English is not the official language. Recently, Kohane et al. (2012) also showed the impact of methods allowing an aggregated exploitation of clinical data. In this context, data extracted from clinical texts in languages other than English adds another dimension to data aggregation. As the importance of clinical NLP gains recognition, clinical corpora become available to researchers in languages other than English, prompting work that sometimes builds on methods validated for English. Adapting systems that work well for English to another language is a difficult task that may be carried out with varying level of success depending on the task and language (Grouin et al., 2009; Velupillai et al. 2014; T\u00e4ckstr\u00f6m et al., 2012). For nonEuropean languages, approaches that account for entirely different word and sentence structures sometimes need to be developped (Shinohara et al. 2013), and cultural differences between clinical narrative styles accounted for (Wu et al. 2013). Access to terminologies and corpora in languages other than English can also be challenging (Schulz et al. 2013; Xu et al. 2013). These experiments prompt a reflexion on how to carry out clinical NLP in a more global context: should methods be developed for one language and then ported to other languages? Can the source language method benefit from the porting? Can algorithms be more robust if they are designed with a multilanguage perspective from the start? French is widely spoken around the world and benefits from one of the largest coverage in the UMLS. Automatic de-identification is becoming quite advanced for French (Grouin & N\u00e9v\u00e9ol, 2013), leading to good results for targeted clinical information extraction tasks (Del\u00e9ger et al. 2010; Grouin et al. 2011). Recent efforts from the French biomedical Informatics community have addressed rules and regulations to improve the access of NLP researchers to clinical corpus. Furthermore, the success of initiatives such as that reported by Grouin et al. (2011) increased the awareness of the potential implication of clinical NLP in clinical practice and contributed to making the timing ripe for making clinical corpus available for annotation and NLP tool development. On-going efforts currently address the annotation of clinical corpora for entity, modality and relations. Tools are being designed for information extraction as well as semantic indexing, information retrieval and clinical data visualization. Much of the research in Swedish clinical NLP has used the Stockholm EPR Corpus, (Dalianis 2012), that contains more than one million patient records encompassing the years 2006-2010, from over 550 clinical units origin from Karolinska University Hospital. Part of this corpus has been manually annotated for Protected Health Information, negations, uncertainty levels, symptoms, diseases, drugs, body parts and abbreviations. The annotated corpora have been used both for training of machine learning systems and evaluation. Some applications are explorative as comorbidity networks, warning and reporting systems detecting hospital acquired infections or adverse drug events, but also work on text simplification of patient record content for the layman patient, (Dalianis 2012). Tools that have been developed for this is an adaptation of NegEx for Swedish (Skeppstedt 2012), a system for classifying terms into six levels of assertion levels pyConTextSwe, (Velupillai et al. 2014), abbreviation detection, (Isenius et al. 2012) and machine learning system based on CRF++ that recognizes named clinical entities as symptoms, diseases, drugs and body, (Skeppstedt et al. 2014). Integrating languages other than English in Apache cTAKES Apache cTAKES (ctakes.apache.org) has been quite successful in assembling and sustaining a global community of developers and users of state-of-the-art English language clinical NLP. Because these techniques involve computational machine learning methods, datasets from the targeted language are needed to train and evaluate the algorithms on. We will discuss what types and size of data were used to build the various cTAKES components \u2013 sentence boundary detector, tokenizer, part of speech tagger, syntactic parser, event and temporal expression detector, temporal relation modules, general relation module. We will also discuss what types of gold standard labels (and how much of each type) are needed to port cTAKES components to other language within the light of some use cases such as porting the temporal expression discovery and normalization module originally developed for English (Bethard, 2013) to Swedish. We will outline available resources in other languages such as Swedish, Finnish, Bulgarian. This is a step towards globalization of information extraction from the clinical narrative.",
            "year": 2014,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This panel aims to provide an overview of clinical NLP for languages other than English, as for example French, Swedish and Japanese and discuss future methodological advances of clinical NLP in a context that encompasses English as well as other languages."
            },
            "score": 1
        }
    ],
    "novelty": "yes"
}