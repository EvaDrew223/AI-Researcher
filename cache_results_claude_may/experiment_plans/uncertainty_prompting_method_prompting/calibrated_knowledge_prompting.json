{
    "topic_description": "novel prompting methods that can better quantify uncertainty or calibrate the confidence of large language models",
    "idea_name": "Calibrated Knowledge Prompting",
    "raw_idea": {
        "Problem": "Large language models can generate factually incorrect or inconsistent responses when prompted with questions that require external knowledge. This is partly due to the model's inability to accurately assess its own knowledge and uncertainty about a given topic.",
        "Existing Methods": "Existing methods for improving the factual accuracy of LLMs include retrieval-augmented generation, where the model is provided with relevant external knowledge, and knowledge distillation, where the model is fine-tuned on a curated dataset of factual information. However, these methods do not explicitly calibrate the model's confidence in its knowledge.",
        "Motivation": "We propose a calibrated knowledge prompting approach, where the LLM is prompted to assess its own knowledge and uncertainty about a given topic before generating a response. By conditioning the response generation on the model's self-assessed knowledge level, we can improve the factual accuracy and consistency of the generated outputs.",
        "Proposed Method": "Our method, Calibrated Knowledge Prompting (CKP), consists of the following steps: 1) Given an input question, prompt the LLM to assess its own knowledge level about the relevant topics using a predefined scale (e.g., high, medium, low). 2) If the knowledge level is assessed as high, prompt the LLM to generate a response directly based on its internal knowledge. 3) If the knowledge level is assessed as medium or low, prompt the LLM to search for relevant external knowledge using a retrieval system and incorporate it into the response generation. 4) Prompt the LLM to generate a confidence score for the final response based on the assessed knowledge level and the retrieved external knowledge. 5) If the confidence score is below a threshold, prompt the LLM to indicate its uncertainty or lack of knowledge in the response.",
        "Experiment Plan": "We will evaluate CKP on a range of knowledge-intensive question-answering tasks, such as open-domain QA, fact checking, and trivia. We will compare the factual accuracy and consistency of the generated responses against baselines such as direct prompting and retrieval-augmented generation. Metrics will include accuracy, F1 score, and consistency with external knowledge sources. We will also conduct human evaluations to assess the quality and trustworthiness of the generated responses."
    },
    "full_experiment_plan": {
        "Title": "Calibrated Knowledge Prompting: Improving Factual Accuracy and Consistency in Large Language Models",
        "Problem Statement": "Large language models can generate factually incorrect or inconsistent responses when prompted with questions that require external knowledge. This is partly due to the model's inability to accurately assess its own knowledge and uncertainty about a given topic.",
        "Motivation": "Existing methods for improving the factual accuracy of LLMs, such as retrieval-augmented generation and knowledge distillation, do not explicitly calibrate the model's confidence in its knowledge. We propose a calibrated knowledge prompting approach, where the LLM is prompted to assess its own knowledge and uncertainty about a given topic before generating a response. By conditioning the response generation on the model's self-assessed knowledge level, we aim to improve the factual accuracy and consistency of the generated outputs.",
        "Proposed Method": "Calibrated Knowledge Prompting (CKP) consists of the following steps:\n1. Given an input question, prompt the LLM to assess its own knowledge level about the relevant topics using a predefined scale (e.g., high, medium, low).\n2. If the knowledge level is assessed as high, prompt the LLM to generate a response directly based on its internal knowledge.\n3. If the knowledge level is assessed as medium or low, prompt the LLM to search for relevant external knowledge using a retrieval system and incorporate it into the response generation.\n4. Prompt the LLM to generate a confidence score for the final response based on the assessed knowledge level and the retrieved external knowledge.\n5. If the confidence score is below a threshold, prompt the LLM to indicate its uncertainty or lack of knowledge in the response.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "Evaluate CKP on a range of knowledge-intensive question-answering tasks, such as open-domain QA (Natural Questions, TriviaQA), fact checking (FEVER), and trivia (QuizBowl). Use accuracy, F1 score, and consistency with external knowledge sources as metrics.",
            "Step 2: Construct Prompts": "1. Baseline prompt: Directly ask the question without any additional instructions.\n2. Knowledge assessment prompt: Append the following instruction to the question: \"Before answering, assess your knowledge level about the relevant topics on a scale of high, medium, or low.\"\n3. High knowledge prompt: If the knowledge level is assessed as high, append: \"Based on your internal knowledge, provide a detailed answer to the question.\"\n4. Medium/low knowledge prompt: If the knowledge level is assessed as medium or low, append: \"Search for relevant information from the provided external knowledge sources and incorporate it into your answer.\"\n5. Confidence score prompt: After generating the response, append: \"Based on your assessed knowledge level and the retrieved external knowledge, provide a confidence score for your answer on a scale of 0 to 1.\"\n6. Uncertainty prompt: If the confidence score is below a threshold (e.g., 0.5), append: \"I'm not confident in my answer due to limited knowledge about this topic.\"",
            "Step 3: Select Models": "Evaluate CKP on GPT-3.5 (text-davinci-003), GPT-4, and an open-source model like BLOOM or GPT-J.",
            "Step 4: Implement Knowledge Retrieval": "Use a retrieval system (e.g., ElasticSearch, BM25) to search for relevant external knowledge from sources like Wikipedia or domain-specific databases. Fine-tune the retrieval system on the chosen datasets to improve the relevance of the retrieved knowledge.",
            "Step 5: Run Experiments": "For each dataset and model combination, generate responses using the baseline prompt and the CKP prompts. Record the accuracy, F1 score, and consistency metrics for each setup.",
            "Step 6: Analyze Results": "Compare the performance of CKP against the baseline prompt across different models and datasets. Analyze the impact of knowledge assessment, external knowledge retrieval, and confidence scores on the factual accuracy and consistency of the generated responses. Conduct error analysis to identify common failure modes and potential improvements."
        },
        "Test Case Examples": {
            "Test Case 1": {
                "Question": "Who won the 2020 NBA Championship?",
                "Baseline Prompt": "Who won the 2020 NBA Championship?",
                "Baseline Output": "The Los Angeles Lakers won the 2020 NBA Championship.",
                "CKP Prompts": {
                    "Knowledge Assessment": "Who won the 2020 NBA Championship? Before answering, assess your knowledge level about the relevant topics on a scale of high, medium, or low.",
                    "Knowledge Assessment Output": "Knowledge level: High",
                    "High Knowledge": "Who won the 2020 NBA Championship? Based on your internal knowledge, provide a detailed answer to the question.",
                    "High Knowledge Output": "The Los Angeles Lakers won the 2020 NBA Championship. They defeated the Miami Heat in six games, with a final score of 4-2 in the series. The Lakers were led by superstars LeBron James and Anthony Davis, who both had outstanding performances throughout the playoffs. This marked the Lakers' 17th championship title, tying them with the Boston Celtics for the most in NBA history. The championship was particularly meaningful for the Lakers organization and fans, as it came in the same year that the team's legendary former player Kobe Bryant tragically passed away.",
                    "Confidence Score": "Based on your assessed knowledge level and the retrieved external knowledge, provide a confidence score for your answer on a scale of 0 to 1.",
                    "Confidence Score Output": "Confidence score: 0.95"
                },
                "Explanation": "In this example, the model assesses its knowledge level about the 2020 NBA Championship as high. Based on its internal knowledge, it provides a detailed and accurate answer, including the winning team, the series score, key players, and the significance of the championship for the Lakers organization. The model also generates a high confidence score, indicating its certainty in the answer."
            },
            "Test Case 2": {
                "Question": "What is the capital of Azerbaijan?",
                "Baseline Prompt": "What is the capital of Azerbaijan?",
                "Baseline Output": "The capital of Azerbaijan is Baku.",
                "CKP Prompts": {
                    "Knowledge Assessment": "What is the capital of Azerbaijan? Before answering, assess your knowledge level about the relevant topics on a scale of high, medium, or low.",
                    "Knowledge Assessment Output": "Knowledge level: Low",
                    "Medium/Low Knowledge": "What is the capital of Azerbaijan? Search for relevant information from the provided external knowledge sources and incorporate it into your answer.",
                    "Medium/Low Knowledge Output": "According to Wikipedia, the capital of Azerbaijan is Baku. Baku is the largest city in Azerbaijan and the Caucasus region, located on the Caspian Sea coast. It is also the financial, industrial, and cultural center of Azerbaijan.",
                    "Confidence Score": "Based on your assessed knowledge level and the retrieved external knowledge, provide a confidence score for your answer on a scale of 0 to 1.",
                    "Confidence Score Output": "Confidence score: 0.8",
                    "Uncertainty Prompt": "I'm not entirely confident in my answer due to limited knowledge about this topic. The information provided is based on a quick search of external sources."
                },
                "Explanation": "In this example, the model assesses its knowledge level about the capital of Azerbaijan as low. It then searches for relevant information from external knowledge sources (in this case, Wikipedia) and incorporates it into the answer. Although the answer is correct, the model generates a lower confidence score and expresses some uncertainty due to its limited internal knowledge about the topic."
            }
        },
        "Fallback Plan": "If the proposed CKP method does not significantly improve the factual accuracy and consistency of the generated responses compared to the baseline, consider the following alternative approaches:\n1. Analyze the knowledge assessment step to determine if the model is accurately gauging its own knowledge level. If not, explore alternative prompts or techniques for self-assessment, such as using more granular scales or providing examples of knowledge levels.\n2. Evaluate the quality and relevance of the retrieved external knowledge. If the retrieved information is not sufficiently relevant or accurate, fine-tune the retrieval system on a larger dataset or experiment with different retrieval techniques (e.g., semantic search, query expansion).\n3. Investigate the impact of different confidence score thresholds on the model's performance. Adjust the threshold based on the specific requirements of the task and the desired trade-off between accuracy and coverage.\n4. Conduct a detailed error analysis to identify common patterns in the model's failures. Use these insights to refine the prompts, improve the external knowledge retrieval, or explore alternative approaches, such as combining CKP with other techniques like retrieval-augmented generation or knowledge distillation.\n5. If the CKP method does not yield significant improvements, focus on analyzing the reasons behind its limitations and present the findings as an analysis paper. The insights gained from this study can inform future research on improving the factual accuracy and consistency of large language models."
    },
    "novelty_queries": [
        "KeywordQuery(\"calibrated knowledge prompting language models\")",
        "KeywordQuery(\"self-assessed knowledge language models\")",
        "KeywordQuery(\"factual accuracy consistency language models\")",
        "KeywordQuery(\"confidence scores language models\")",
        "KeywordQuery(\"Calibrated Knowledge Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "eb971944bccf9793ac463c3e2f4d4251d4e8e071",
            "paperId": "eb971944bccf9793ac463c3e2f4d4251d4e8e071",
            "title": "Do Large Language Models Know What They Don't Know?",
            "abstract": "Large language models (LLMs) have a wealth of knowledge that allows them to excel in various Natural Language Processing (NLP) tasks. Current research focuses on enhancing their performance within their existing knowledge. Despite their vast knowledge, LLMs are still limited by the amount of information they can accommodate and comprehend. Therefore, the ability to understand their own limitations on the unknows, referred to as self-knowledge, is of paramount importance. This study aims to evaluate LLMs' self-knowledge by assessing their ability to identify unanswerable or unknowable questions. We introduce an automated methodology to detect uncertainty in the responses of these models, providing a novel measure of their self-knowledge. We further introduce a unique dataset, SelfAware, consisting of unanswerable questions from five diverse categories and their answerable counterparts. Our extensive analysis, involving 20 LLMs including GPT-3, InstructGPT, and LLaMA, discovering an intrinsic capacity for self-knowledge within these models. Moreover, we demonstrate that in-context learning and instruction tuning can further enhance this self-knowledge. Despite this promising insight, our findings also highlight a considerable gap between the capabilities of these models and human proficiency in recognizing the limits of their knowledge.",
            "year": 2023,
            "citationCount": 58,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study aims to evaluate large language models' self-knowledge by assessing their ability to identify unanswerable or unknowable questions, and introduces an automated methodology to detect uncertainty in the responses of these models, providing a novel measure of their self- knowledge."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is improving the factual accuracy and consistency of large language models by calibrating their confidence in their own knowledge. The proposed approach is to use calibrated knowledge prompting, where the model assesses its own knowledge level and incorporates external knowledge when necessary.\n\nThe research problem in the paper is evaluating the self-knowledge of large language models, specifically their ability to identify unanswerable or unknowable questions. The approach involves introducing an automated methodology to detect uncertainty in the model responses and creating a dataset of unanswerable questions.\n\nWhile both studies focus on large language models and their knowledge limitations, the specific research problems and approaches differ. The proposal aims to improve factual accuracy by calibrating confidence, while the paper evaluates the models' ability to recognize unanswerable questions.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "2751de08d6dbec07f53808231c016e96b075b06c",
            "paperId": "2751de08d6dbec07f53808231c016e96b075b06c",
            "title": "Learning to Trust Your Feelings: Leveraging Self-awareness in LLMs for Hallucination Mitigation",
            "abstract": "We evaluate the ability of Large Language Models (LLMs) to discern and express their internal knowledge state, a key factor in countering factual hallucination and ensuring reliable application of LLMs. We observe a robust self-awareness of internal knowledge state in LLMs, evidenced by over 85% accuracy in knowledge probing. However, LLMs often fail to express their internal knowledge during generation, leading to factual hallucinations. We develop an automated hallucination annotation tool, Dreamcatcher, which merges knowledge probing and consistency checking methods to rank factual preference data. Using knowledge preference as reward, We propose a Reinforcement Learning from Knowledge Feedback (RLKF) training framework, leveraging reinforcement learning to enhance the factuality and honesty of LLMs. Our experiments across multiple models show that RLKF training effectively enhances the ability of models to utilize their internal knowledge state, boosting performance in a variety of knowledge-based and honesty-related tasks.",
            "year": 2024,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Reinforcement Learning from Knowledge Feedback (RLKF) training framework is proposed, leveraging reinforcement learning to enhance the factuality and honesty of LLMs and shows that RLKF training effectively enhances the ability of models to utilize their internal knowledge state, boosting performance in a variety of knowledge-based and honesty-related tasks."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is improving the factual accuracy and consistency of large language models by calibrating their confidence in their knowledge. The approach is to prompt the model to assess its own knowledge level and incorporate external knowledge when necessary.\n\nThe research problem in the paper is mitigating factual hallucinations in large language models. The approach is to leverage the model's self-awareness of its internal knowledge state and use reinforcement learning to enhance the model's factuality and honesty.\n\nWhile both the proposal and the paper aim to improve the factual accuracy of large language models, their approaches differ. The proposal focuses on calibrating the model's confidence and incorporating external knowledge, while the paper focuses on leveraging the model's self-awareness and using reinforcement learning.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "56e952fd463accff09cf2e35432aaabd7c7c57f3",
            "paperId": "56e952fd463accff09cf2e35432aaabd7c7c57f3",
            "title": "MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions",
            "abstract": "The information stored in large language models (LLMs) falls out of date quickly, and retraining from scratch is often not an option. This has recently given rise to a range of techniques for injecting new facts through updating model weights. Current evaluation paradigms are extremely limited, mainly validating the recall of edited facts, but changing one fact should cause rippling changes to the model's related beliefs. If we edit the UK Prime Minister to now be Rishi Sunak, then we should get a different answer to Who is married to the British Prime Minister? In this work, we present a benchmark, MQuAKE (Multi-hop Question Answering for Knowledge Editing), comprising multi-hop questions that assess whether edited models correctly answer questions where the answer should change as an entailed consequence of edited facts. While we find that current knowledge-editing approaches can recall edited facts accurately, they fail catastrophically on the constructed multi-hop questions. We thus propose a simple memory-based approach, MeLLo, which stores all edited facts externally while prompting the language model iteratively to generate answers that are consistent with the edited facts. While MQuAKE remains challenging, we show that MeLLo scales well with LLMs (up to 175B) and outperforms previous model editors by a large margin.",
            "year": 2023,
            "citationCount": 68,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents a benchmark, MQuAKE (Multi-hop Question Answering for Knowledge Editing), comprising multi-hop questions that assess whether edited models correctly answer questions where the answer should change as an entailed consequence of edited facts, and proposes a simple memory-based approach, MeLLo."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the factual accuracy and consistency of large language models using calibrated knowledge prompting. The approach involves prompting the model to assess its own knowledge level, retrieve relevant external knowledge if needed, and generate a confidence score for its response.\n\nThe research problem in the paper is assessing the effectiveness of knowledge editing techniques in large language models using multi-hop questions. The approach involves creating a benchmark dataset of multi-hop questions and proposing a memory-based method called MeLLo that stores edited facts externally and prompts the model to generate consistent answers.\n\nThe proposal focuses on improving factual accuracy during generation, while the paper focuses on evaluating knowledge editing techniques. The proposal uses knowledge prompting, while the paper uses external memory and multi-hop questions.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "56373d3fd0f1354a61f9e577db039cdb187d8d43",
            "paperId": "56373d3fd0f1354a61f9e577db039cdb187d8d43",
            "title": "Evaluating the Factual Consistency of Large Language Models Through News Summarization",
            "abstract": "While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information. To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB(Factual Inconsistency Benchmark) that focuses on the task of summarization. Specifically, our benchmark involves comparing the scores an LLM assigns to a factually consistent versus a factually inconsistent summary for an input news article. For factually consistent summaries, we use human-written reference summaries that we manually verify as factually consistent. To generate summaries that are factually inconsistent, we generate summaries from a suite of summarization models that we have manually annotated as factually inconsistent. A model's factual consistency is then measured according to its accuracy, i.e.\\ the proportion of documents where it assigns a higher score to the factually consistent summary. To validate the usefulness of FIB, we evaluate 23 large language models ranging from 1B to 176B parameters from six different model families including BLOOM and OPT. We find that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries. However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries. We validate design choices in our benchmark including the scoring method and source of distractor summaries. Our code and benchmark data can be found at https://github.com/r-three/fib.",
            "year": 2022,
            "citationCount": 51,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new benchmark called FIB(Factual Inconsistency Benchmark) that focuses on the task of summarization, and finds that existing LLMs generally assign a higher score to factually consistent summaries than tofactually inconsistent summaries."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the factual accuracy and consistency of large language models (LLMs) in knowledge-intensive tasks. The proposed approach is calibrated knowledge prompting, where the LLM assesses its own knowledge level and incorporates external knowledge when needed.\n\nThe research problem in the paper is evaluating the factual consistency of LLMs in news summarization. The approach is a new benchmark called FIB that compares the scores assigned by LLMs to factually consistent and inconsistent summaries.\n\nWhile both works focus on the factual accuracy of LLMs, the proposal aims to improve it through a new prompting method, while the paper proposes a benchmark to evaluate it in the specific task of news summarization. The approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "f33d19614d78f4a6e107e768ccd0a2d4244fc89a",
            "paperId": "f33d19614d78f4a6e107e768ccd0a2d4244fc89a",
            "title": "Evaluating the Factual Consistency of Large Language Models Through Summarization",
            "abstract": "While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information. To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB ( F actual I nconsistency B enchmark) that focuses on the task of summarization. Specifically, our benchmark involves comparing the scores an LLM assigns to a factually consistent versus a factually inconsistent summary for an input news article. For factually consistent summaries, we use human-written reference summaries that we manually verify as fac-tually consistent. To generate summaries that are factually inconsistent, we generate summaries from a suite of summarization models that we have manually annotated as fac-tually inconsistent. A model\u2019s factual consistency is then measured according to its accuracy, i.e. the proportion of documents where it assigns a higher score to the factually consistent summary. To validate the usefulness of FIB , we evaluate 23 large language models ranging from 1B to 176B parameters from six different model families including BLOOM and OPT. We \ufb01nd that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries. However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries. We validate design choices in our benchmark including the scoring method and source of distractor summaries. Our code and",
            "year": 2022,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new benchmark called FIB is proposed that focuses on the task of summarization and shows that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries, and that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the factual accuracy and consistency of large language models (LLMs) in knowledge-intensive tasks. The proposed approach is Calibrated Knowledge Prompting (CKP), which involves prompting the LLM to assess its own knowledge level, retrieve relevant external knowledge, and generate confidence scores for its responses.\n\nThe research problem in the paper is evaluating the factual consistency of LLMs in the task of summarization. The approach involves comparing the scores assigned by LLMs to factually consistent and inconsistent summaries in a new benchmark called FIB (Factual Inconsistency Benchmark).\n\nWhile both works focus on the factual accuracy and consistency of LLMs, the proposal aims to improve these aspects through a novel prompting method, while the paper focuses on evaluating these aspects in the specific task of summarization using a new benchmark.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "5d83bd7c234bf959ea3d53c17ae76df2b8ec1969",
            "paperId": "5d83bd7c234bf959ea3d53c17ae76df2b8ec1969",
            "title": "Enhancing Factuality in Language Models through Knowledge-Guided Decoding",
            "abstract": "Large language models (LLMs) have shown impressive performance in text generation tasks, but they often struggle to maintain factual consistency and reliability in their outputs. Existing methods for enhancing LLM factuality do not guarantee that the generated text is consistent with the provided information. In this paper, we propose Knowledge-Guided Decoding (KGD), a novel framework that addresses these limitations by directly influencing the language model\u2019s generation process at the token level based on retrieved knowledge. KGD incorporates knowledge-guided rewards, such as semantic similarity and entailment, to guide the model\u2019s output toward more accurate and trustworthy text. We evaluate our approach on both short-form and long-form text generation tasks, focusing on measuring the factuality of the generated output. Through extensive experiments, we demonstrate that KGD effectively improves the factual accuracy of the generated text while maintaining fluency, outperforming traditional decoding baselines.",
            "year": null,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "KGD effectively improves the factual accuracy of the generated text while maintaining fluency, outperforming traditional decoding baselines and incorporating knowledge-guided rewards to guide the model\u2019s output toward more accurate and trustworthy text."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the factual accuracy and consistency of large language models in knowledge-intensive tasks. The proposed approach is Calibrated Knowledge Prompting (CKP), which prompts the model to assess its own knowledge level and incorporates external knowledge when necessary.\n\nThe research problem in the paper is enhancing the factuality of language models in text generation tasks. The proposed approach is Knowledge-Guided Decoding (KGD), which guides the model's generation process at the token level based on retrieved knowledge.\n\nWhile both the proposal and the paper aim to improve the factual accuracy of language models, their approaches differ. The proposal focuses on prompting techniques to calibrate the model's knowledge, while the paper focuses on modifying the decoding process using knowledge-guided rewards.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "d998ff93b2e58c9b219a9103f8c2ad714a41e4b9",
            "paperId": "d998ff93b2e58c9b219a9103f8c2ad714a41e4b9",
            "title": "The Effect of Scaling, Retrieval Augmentation and Form on the Factual Consistency of Language Models",
            "abstract": "Large Language Models (LLMs) make natural interfaces to factual knowledge, but their usefulness is limited by their tendency to deliver inconsistent answers to semantically equivalent questions. For example, a model might predict both\"Anne Redpath passed away in Edinburgh.\"and\"Anne Redpath's life ended in London.\"In this work, we identify potential causes of inconsistency and evaluate the effectiveness of two mitigation strategies: up-scaling and augmenting the LM with a retrieval corpus. Our results on the LLaMA and Atlas models show that both strategies reduce inconsistency while retrieval augmentation is considerably more efficient. We further consider and disentangle the consistency contributions of different components of Atlas. For all LMs evaluated we find that syntactical form and other evaluation task artifacts impact consistency. Taken together, our results provide a better understanding of the factors affecting the factual consistency of language models.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work identifies potential causes of inconsistency and evaluates the effectiveness of two mitigation strategies: up-scaling and augmenting the LM with a retrieval corpus, showing that both strategies reduce inconsistency while retrieval augmentation is considerably more efficient."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the factual accuracy and consistency of large language models, and the proposed approach is calibrated knowledge prompting, where the model assesses its own knowledge level and incorporates external knowledge when needed.\n\nThe research problem in the paper is also improving the factual consistency of language models, but the approaches are scaling up the model and augmenting it with a retrieval corpus.\n\nWhile both works aim to improve factual consistency, the proposal focuses on a novel prompting method, while the paper investigates the effects of scaling and retrieval augmentation.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "f727f928e7e179307d8d4a1da2387393f2bd7915",
            "paperId": "f727f928e7e179307d8d4a1da2387393f2bd7915",
            "title": "Methods for Measuring, Updating, and Visualizing Factual Beliefs in Language Models",
            "abstract": "Language models can memorize a considerable amount of factual information during pretraining that can be elicited through prompting or finetuning models on tasks like question answering. In this paper, we discuss approaches to measuring model factual beliefs, updating incorrect factual beliefs in models, and visualizing graphical relationships between factual beliefs. Our main contributions include: (1) new metrics for evaluating belief-updating methods focusing on the logical consistency of beliefs, (2) a training objective for Sequential, Local, and Generalizing updates (SLAG) that improves the performance of existing hypernetwork approaches, and (3) the introduction of the belief graph, a new form of visualization for language models that shows relationships between stored model beliefs. Our experiments suggest that models show only limited consistency between factual beliefs, but update methods can both fix incorrect model beliefs and greatly improve their consistency. Although off-the-shelf optimizers are surprisingly strong belief-updating baselines, our learned optimizers can outperform them in more difficult settings than have been considered in past work.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experiments suggest that models show only limited consistency between factual beliefs, but update methods can both fix incorrect model beliefs and greatly improve their consistency, and off-the-shelf optimizers can outperform them in more difficult settings than have been considered in past work."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the factual accuracy and consistency of large language models, and the proposed approach is calibrated knowledge prompting, where the model assesses its own knowledge level and incorporates external knowledge when needed.\n\nThe research problem in the paper is measuring, updating, and visualizing factual beliefs in language models, and the proposed approaches include metrics for evaluating belief-updating methods, a training objective for Sequential, Local, and Generalizing updates (SLAG), and the introduction of the belief graph visualization.\n\nWhile both works aim to improve the factual accuracy of language models, the proposal focuses on a prompting-based approach for generating more accurate responses, while the paper focuses on measuring, updating, and visualizing factual beliefs. The methods proposed in each work are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "ab4ce5dda7ad4d9032995c9c049a89d65723c6aa",
            "paperId": "ab4ce5dda7ad4d9032995c9c049a89d65723c6aa",
            "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
            "abstract": "A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated. However, the most widely-used LMs are fine-tuned with reinforcement learning from human feedback (RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated. In light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50%.",
            "year": 2023,
            "citationCount": 96,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "For RLHF-LMs such as ChatGPT, GPT-4, and Claude, it is found that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50%."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the factual accuracy and consistency of large language models by calibrating their confidence based on self-assessed knowledge levels. The approach involves prompting the model to assess its knowledge, retrieve external information when necessary, and generate a confidence score for its response.\n\nThe research problem in the paper is improving the calibration of confidence scores produced by language models fine-tuned with reinforcement learning from human feedback (RLHF-LMs). The approach involves evaluating various methods for extracting better-calibrated confidence scores from RLHF-LMs, focusing on verbalized confidences emitted as output tokens.\n\nWhile both studies aim to improve the calibration of language models' confidence scores, the proposal focuses on factual accuracy and consistency, while the paper focuses on the calibration of RLHF-LMs specifically. The proposed approaches also differ, with the proposal using knowledge-level assessment and external knowledge retrieval, while the paper evaluates verbalized confidences emitted as output tokens.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "92746dfa09dcad92ecf1e6272ebb300c1112b7eb",
            "paperId": "92746dfa09dcad92ecf1e6272ebb300c1112b7eb",
            "title": "Automatic Calibration and Error Correction for Large Language Models via Pareto Optimal Self-Supervision",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities out of box for a wide range of applications, yet accuracy still remains a major growth area, especially in mission-critical domains such as biomedicine. An effective method to calibrate the con\ufb01dence level on LLM responses is essential to automatically detect errors and facilitate human-in-the-loop veri\ufb01cation. An important source of calibration signals stems from expert-stipulated programmatic super-vision, which is often available at low cost but has its own limitations such as noise and coverage. In this paper, we introduce a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every response, without any additional manual efforts. This is accomplished by learning a harmonizer model to align LLM output with other available supervision sources, which would assign higher risk scores to more uncertain LLM responses and facilitate error correction. Experiments on standard relation extraction tasks in biomedical and general domains demonstrate the promise of this approach, with our proposed risk scores highly correlated with the real error rate of LLMs. For the most uncertain test instances, dynamic prompting based on our proposed risk scores results in signi\ufb01cant accuracy improvement for off-the-shelf LLMs, boosting GPT-3 results past state-of-the-art (SOTA) weak supervision and GPT-4 results past SOTA supervised results on challenging evaluation datasets.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every response, without any additional manual efforts."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the factual accuracy and consistency of large language models (LLMs) by calibrating their confidence in their knowledge. The proposed approach is to use calibrated knowledge prompting, where the LLM assesses its own knowledge level and incorporates external knowledge when necessary.\n\nThe research problem in the paper is calibrating the confidence level of LLM responses to automatically detect errors and facilitate human-in-the-loop verification. The proposed approach is to use a Pareto optimal self-supervision framework that learns a harmonizer model to align LLM output with available supervision sources and assign risk scores to LLM responses.\n\nWhile both the proposal and the paper aim to improve the accuracy of LLMs, their specific research problems and proposed approaches differ. The proposal focuses on factual accuracy and consistency, while the paper focuses on confidence calibration and error detection. The proposal uses knowledge prompting and external knowledge incorporation, while the paper uses self-supervision and a harmonizer model.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "9a61d51212eb4ff677fe777a7ba9ddc4f675b387",
            "paperId": "9a61d51212eb4ff677fe777a7ba9ddc4f675b387",
            "title": "Automatic Calibration and Error Correction for Generative Large Language Models via Pareto Optimal Self-Supervision",
            "abstract": "Generative Large language models (LLMs) have demonstrated remarkable capabilities for a wide range of applications, but reducing ungrounded or erroneous responses remains a major growth area. Unlike task-specific models, there lack an effective method to calibrate the confidence level of LLM responses to indicate potential errors and facilitate human-in-the-loop verification. An important source of calibration stems from expert-stipulated programmatic supervision, which is often available at low cost but has its own limitations such as noise and coverage. In this paper, we introduce a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every LLM response, without any additional manual efforts. This is accomplished by learning a harmonizer model to align with LLM output as well as other weak supervision sources. The model assigns higher risk scores to more uncertain LLM responses and facilitate error correction. Experiments on standard relation extraction and classification tasks in biomedical and general domains demonstrate that the proposed risk score is highly correlated with the actual LLM error rate. By using a dynamic prompting strategy based on the risk score, we observed significant accuracy improvement for off-the-shelf LLMs, boosting GPT-3.5 results past state-of-the-art (SOTA) weak supervision model and GPT-4 results past SOTA supervised results on challenging evaluation datasets.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every LLM response, without any additional manual efforts."
            },
            "score": 7
        },
        {
            "id": "711d5e8ddbb840ad31a9ffa3d38590603ba69a92",
            "paperId": "711d5e8ddbb840ad31a9ffa3d38590603ba69a92",
            "title": "Prompting GPT-3 To Be Reliable",
            "abstract": "Large language models (LLMs) show impressive abilities via few-shot prompting. Commercialized APIs such as OpenAI GPT-3 further increase their use in real-world language applications. However, the crucial problem of how to improve the reliability of GPT-3 is still under-explored. While reliability is a broad and vaguely defined term, we decompose reliability into four main facets that correspond to the existing framework of ML safety and are well-recognized to be important: generalizability, social biases, calibration, and factuality. Our core contribution is to establish simple and effective prompts that improve GPT-3's reliability as it: 1) generalizes out-of-distribution, 2) balances demographic distribution and uses natural language instructions to reduce social biases, 3) calibrates output probabilities, and 4) updates the LLM's factual knowledge and reasoning chains. With appropriate prompts, GPT-3 is more reliable than smaller-scale supervised models on all these facets. We release all processed datasets, evaluation scripts, and model predictions. Our systematic empirical study not only sheds new insights on the reliability of prompting LLMs, but more importantly, our prompting strategies can help practitioners more reliably use LLMs like GPT-3.",
            "year": 2022,
            "citationCount": 161,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This systematic empirical study sheds new insights on the reliability of prompting LLMs, but more importantly, its prompting strategies can help practitioners more reliably use LLMs like GPT-3."
            },
            "score": 7
        },
        {
            "id": "9efa81ec4954b0859c47dad8f42edfaf8bced69b",
            "paperId": "9efa81ec4954b0859c47dad8f42edfaf8bced69b",
            "title": "Boosting Language Models Reasoning with Chain-of-Knowledge Prompting",
            "abstract": "Recently, Chain-of-Thought (CoT) prompting has delivered success on complex reasoning tasks, which aims at designing a simple prompt like ``Let's think step by step'' or multiple in-context exemplars with well-designed rationales to elicit Large Language Models (LLMs) to generate intermediate reasoning steps. However, the generated rationales often come with mistakes, making unfactual and unfaithful reasoning chains. To mitigate this brittleness, we propose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple. This is inspired by our human behaviors, i.e., we can draw a mind map or knowledge map as the reasoning evidence in the brain before answering a complex question. Benefiting from CoK, we additionally introduce a F^2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness. For the unreliable response, the wrong evidence can be indicated to prompt the LLM to rethink. Extensive experiments demonstrate that our method can further improve the performance of commonsense, factual, symbolic, and arithmetic reasoning tasks.",
            "year": 2023,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel Chain-of-Knowledge prompting, where it aims at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple, and introduces a F^2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness."
            },
            "score": 6
        },
        {
            "id": "ddc9aeac18638575bbb90ede4c6829ec15c2947e",
            "paperId": "ddc9aeac18638575bbb90ede4c6829ec15c2947e",
            "title": "Prompting as Probing: Using Language Models for Knowledge Base Construction",
            "abstract": "Language Models (LMs) have proven to be useful in various downstream applications, such as summarisation, translation, question answering and text classification. LMs are becoming increasingly important tools in Artificial Intelligence, because of the vast quantity of information they can store. In this work, we present ProP (Prompting as Probing), which utilizes GPT-3, a large Language Model originally proposed by OpenAI in 2020, to perform the task of Knowledge Base Construction (KBC). ProP implements a multi-step approach that combines a variety of prompting techniques to achieve this. Our results show that manual prompt curation is essential, that the LM must be encouraged to give answer sets of variable lengths, in particular including empty answer sets, that true/false questions are a useful device to increase precision on suggestions generated by the LM, that the size of the LM is a crucial factor, and that a dictionary of entity aliases improves the LM score. Our evaluation study indicates that these proposed techniques can substantially enhance the quality of the final predictions: ProP won track 2 of the LM-KBC competition, outperforming the baseline by 36.4 percentage points. Our implementation is available on https://github.com/HEmile/iswc-challenge.",
            "year": 2022,
            "citationCount": 32,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ProP (Prompting as Probing), which utilizes GPT-3, a large Language Model originally proposed by OpenAI in 2020, to perform the task of Knowledge Base Construction (KBC), implements a multi-step approach that combines a variety of prompting techniques to achieve this."
            },
            "score": 6
        },
        {
            "id": "12c826f4195da172b212a529f8fcf10cc79e35da",
            "paperId": "12c826f4195da172b212a529f8fcf10cc79e35da",
            "title": "Context-faithful Prompting for Large Language Models",
            "abstract": "Large language models (LLMs) encode parametric knowledge about world facts and have shown remarkable performance in knowledge-driven NLP tasks. However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks). In this paper, we seek to assess and enhance LLMs' contextual faithfulness in two aspects: knowledge conflict and prediction with abstention. We demonstrate that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies. In particular, we identify opinion-based prompts and counterfactual demonstrations as the most effective methods. Opinion-based prompts reframe the context as a narrator's statement and inquire about the narrator's opinions, while counterfactual demonstrations use instances containing false facts to improve faithfulness in knowledge conflict situations. Neither technique requires additional training. We conduct experiments on three datasets of two standard NLP tasks, machine reading comprehension and relation extraction, and the results demonstrate significant improvement in faithfulness to contexts. Code and data are released at https://github.com/wzhouad/context-faithful-llm.",
            "year": 2023,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies, and opinion-based prompts and counterfactual demonstrations are identified as the most effective methods."
            },
            "score": 6
        },
        {
            "id": "8f72127459ade06831ec2990f7da4914cc1a6e22",
            "paperId": "8f72127459ade06831ec2990f7da4914cc1a6e22",
            "title": "Self-Knowledge Guided Retrieval Augmentation for Large Language Models",
            "abstract": "Large language models (LLMs) have shown superior performance without task-specific fine-tuning. Despite the success, the knowledge stored in the parameters of LLMs could still be incomplete and difficult to update due to the computational costs. As complementary, retrieval-based methods can offer non-parametric world knowledge and improve the performance on tasks such as question answering. However, we find that the retrieved knowledge does not always help and even has a negative impact on original responses occasionally. To better make use of both internal knowledge and external world knowledge, we investigate eliciting the model's ability to recognize what they know and do not know (which is also called self-knowledge) and propose Self-Knowledge guided Retrieval augmentation (SKR), a simple yet effective method which can let LLMs refer to the questions they have previously encountered and adaptively call for external resources when dealing with new questions. We evaluate SKR on multiple datasets and demonstrate that it outperforms chain-of-thought based and fully retrieval-based methods by using either InstructGPT or ChatGPT.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Self-Knowledge guided Retrieval augmentation (SKR), a simple yet effective method which can let LLMs refer to the questions they have previously encountered and adaptively call for external resources when dealing with new questions, is proposed."
            },
            "score": 6
        },
        {
            "id": "468d1e2d75a23fecaf96fe65d8b01ff35ea5d0bd",
            "paperId": "468d1e2d75a23fecaf96fe65d8b01ff35ea5d0bd",
            "title": "Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models",
            "abstract": "Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts inserted in English reveal a clear pattern whereby the new piece of knowledge transfers only to languages with which English has a high RankC score.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy, and conducts an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level."
            },
            "score": 6
        },
        {
            "id": "152d9a231c00d4495c9bc4a466f42165ce2e2164",
            "paperId": "152d9a231c00d4495c9bc4a466f42165ce2e2164",
            "title": "Evaluating Factual Consistency of Summaries with Large Language Models",
            "abstract": "Detecting factual errors in summaries has been an important and challenging subject in summarization research. Inspired by the emergent ability of large language models (LLMs), we explore evaluating factual consistency of summaries by directly prompting LLMs. We present a comprehensive empirical study to assess the ability of LLMs as factual consistency evaluators, which consists of (1) analyzing different LLMs such as the GPT model series and Flan-T5; (2) investigating a variety of prompting methods including vanilla prompting, chain-of-thought prompting, and a sentence-by-sentence prompting method to tackle long summaries; and (3) evaluating on diverse summaries generated by multiple summarization systems, ranging from pre-transformer methods to SOTA pretrained models. Our experiments demonstrate that prompting LLMs is able to outperform the previous best factuality systems in all settings, by up to 12.2 absolute points in terms of the binary classification accuracy on inconsistency detection.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A comprehensive empirical study to assess the ability of LLMs as factual consistency evaluators, which consists of analyzing different LLMs such as the GPT model series and Flan-T5 and investigating a variety of prompting methods including vanilla prompting, chain-of-thought prompting, and a sentence-by-sentence prompting method to tackle long summaries."
            },
            "score": 6
        },
        {
            "id": "6674e254f343cdb2511b84d4bf120182fb112b67",
            "paperId": "6674e254f343cdb2511b84d4bf120182fb112b67",
            "title": "Evaluating Large Language Models for Health-related Queries with Presuppositions",
            "abstract": "As corporations rush to integrate large language models (LLMs) to their search offerings, it is critical that they provide factually accurate information that is robust to any presuppositions that a user may express. In this work, we introduce UPHILL, a dataset consisting of health-related queries with varying degrees of presuppositions. Using UPHILL, we evaluate the factual accuracy and consistency of InstructGPT, ChatGPT, and BingChat models. We find that while model responses rarely disagree with true health claims (posed as questions), they often fail to challenge false claims: responses from InstructGPT agree with 32% of the false claims, ChatGPT 26% and BingChat 23%. As we increase the extent of presupposition in input queries, the responses from InstructGPT and ChatGPT agree with the claim considerably more often, regardless of its veracity. Responses from BingChat, which rely on retrieved webpages, are not as susceptible. Given the moderate factual accuracy, and the inability of models to consistently correct false assumptions, our work calls for a careful assessment of current LLMs for use in high-stakes scenarios.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "UPHILL, a dataset consisting of health-related queries with varying degrees of presuppositions, is introduced and it is found that while model responses rarely disagree with true health claims, they often fail to challenge false claims."
            },
            "score": 6
        },
        {
            "id": "8936192d69bb78afd09d83f3ceafd18dea32a8fd",
            "paperId": "8936192d69bb78afd09d83f3ceafd18dea32a8fd",
            "title": "Self-Consistent Decoding for More Factual Open Responses",
            "abstract": "Self-consistency has emerged as a powerful method for improving the accuracy of short answers generated by large language models. As previously defined, it only concerns the accuracy of a final answer parsed from generated text. In this work, we extend the idea to open response generation, by integrating voting into the decoding method. Each output sentence is selected from among multiple samples, conditioning on the previous selections, based on a simple token overlap score. We compare this\"Sample&Select\"method to greedy decoding, beam search, nucleus sampling, and the recently introduced hallucination avoiding decoders of DoLA, P-CRR, and S-CRR. We show that Sample&Select improves factuality by a 30% relative margin against these decoders in NLI-based evaluation on the subsets of CNN/DM and XSum used in the FRANK benchmark, while maintaining comparable ROUGE-1 F1 scores against reference summaries. We collect human verifications of the generated summaries, confirming the factual superiority of our method.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work extends the idea of self-consistency to open response generation, by integrating voting into the decoding method, and shows that Sample&Select improves factuality by a 30% relative margin against these decoders in NLI-based evaluation on the subsets of CNN/DM and XSum used in the FRANK benchmark."
            },
            "score": 6
        },
        {
            "id": "d2d16333a4b0dc7e3463b280b9945e5ee6c53396",
            "paperId": "d2d16333a4b0dc7e3463b280b9945e5ee6c53396",
            "title": "TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models",
            "abstract": "Factual consistency evaluation is often conducted using Natural Language Inference (NLI) models, yet these models exhibit limited success in evaluating summaries. Previous work improved such models with synthetic training data. However, the data is typically based on perturbed human-written summaries, which often differ in their characteristics from real model-generated summaries and have limited coverage of possible factual errors. Alternatively, large language models (LLMs) have recently shown promising results in directly evaluating generative tasks, but are too computationally expensive for practical use. Motivated by these limitations, we introduce TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM. Unlike prior work, TrueTeacher does not rely on human-written summaries, and is multilingual by nature. Experiments on the TRUE benchmark show that a student model trained using our data, substantially outperforms both the state-of-the-art model with similar capacity, and the LLM teacher. In a systematic study, we compare TrueTeacher to existing synthetic data generation methods and demonstrate its superiority and robustness to domain-shift. We also show that our method generalizes to multilingual scenarios. Lastly, we release our large scale synthetic dataset (1.4M examples), generated using TrueTeacher, and a checkpoint trained on this data.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM, which does not rely on human-written summaries, and is multilingual by nature."
            },
            "score": 6
        },
        {
            "id": "e96348576b682e709b2ee06ea28cd81f8bfa102b",
            "paperId": "e96348576b682e709b2ee06ea28cd81f8bfa102b",
            "title": "Exploring the Factual Consistency in Dialogue Comprehension of Large Language Models",
            "abstract": "LLMs (Large Language Models) usually interact with users in the form of dialogue and generate responses following their instructions, which naturally require dialogue comprehension abilities. However, dialogue comprehension is a general language ability which is hard to be evaluated directly. In this work, we propose to perform the evaluation focusing on the factual consistency issue with the help of the dialogue summarization task. Besides evaluating and analyzing the dialogue summarization performance (DIAC-Sum) of different LLMs, we also derive factual questions from the generated summaries and use them as a more flexible measurement of dialogue comprehension (DIAC-QA). Our evaluation shows that, on average, 26.8% of the summaries generated by LLMs contain factual inconsistency. Even ChatGPT, the strongest model evaluated, has such errors in 16% of its summaries. For answering the factual questions, which is more challenging, the average error rate of all evaluated LLMs is 36.1%. Both results indicate serious deficiencies. Detailed analysis shows that the understanding of subject/object of the conversation is still challenging for LLMs. Furthermore, to stimulate and enhance the dialogue comprehension ability of LLMs, we propose a fine-tuning paradigm with auto-constructed multi-task data, which achieved a relative error rate reduction of 11% on DIAC-QA.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "To stimulate and enhance the dialogue comprehension ability of LLMs, a fine-tuning paradigm with auto-constructed multi-task data is proposed, which achieved a relative error rate reduction of 11% on DIAC-QA."
            },
            "score": 6
        },
        {
            "id": "36870ac7332b45edf59e5c111dc5bd24139dc1ce",
            "paperId": "36870ac7332b45edf59e5c111dc5bd24139dc1ce",
            "title": "Factual Consistency of Multilingual Pretrained Language Models",
            "abstract": "Pretrained language models can be queried for factual knowledge, with potential applications in knowledge base acquisition and tasks that require inference. However, for that, we need to know how reliable this knowledge is, and recent work has shown that monolingual English language models lack consistency when predicting factual knowledge, that is, they fill-in-the-blank differently for paraphrases describing the same fact. In this paper, we extend the analysis of consistency to a multilingual setting. We introduce a resource, mParaRel, and investigate (i) whether multilingual language models such as mBERT and XLM-R are more consistent than their monolingual counterparts;and (ii) if such models are equally consistent across languages.We find that mBERT is as inconsistent as English BERT in English paraphrases, but that both mBERT and XLM-R exhibit a high degree of inconsistency in English and even more so for all the other 45 languages.",
            "year": 2022,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "MBERT is as inconsistent as English BERT in English paraphrases, but that both mBERT and XLM-R exhibit a high degree of inconsistency in English and even more so for all the other 45 languages."
            },
            "score": 6
        },
        {
            "id": "f3cd3a0e1a0a29193fd88178ef75946dfe756622",
            "paperId": "f3cd3a0e1a0a29193fd88178ef75946dfe756622",
            "title": "Factual Consistency Evaluation of Summarisation in the Era of Large Language Models",
            "abstract": "Factual inconsistency with source documents in automatically generated summaries can lead to misinformation or pose risks. Existing factual consistency(FC) metrics are constrained by their performance, efficiency, and explainability. Recent advances in Large language models (LLMs) have demonstrated remarkable potential in text evaluation but their effectiveness in assessing FC in summarisation remains underexplored. Prior research has mostly focused on proprietary LLMs, leaving essential factors that affect their assessment capabilities unexplored. Additionally, current FC evaluation benchmarks are restricted to news articles, casting doubt on the generality of the FC methods tested on them. In this paper, we first address the gap by introducing TreatFact a dataset of LLM-generated summaries of clinical texts, annotated for FC by domain experts. Moreover, we benchmark 11 LLMs for FC evaluation across news and clinical domains and analyse the impact of model size, prompts, pre-training and fine-tuning data. Our findings reveal that despite proprietary models prevailing on the task, open-source LLMs lag behind. Nevertheless, there is potential for enhancing the performance of open-source LLMs through increasing model size, expanding pre-training data, and developing well-curated fine-tuning data. Experiments on TreatFact suggest that both previous methods and LLM-based evaluators are unable to capture factual inconsistencies in clinical summaries, posing a new challenge for FC evaluation.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "TreatFact, a dataset of LLM-generated summaries of clinical texts, annotated for FC by domain experts, is introduced and it is revealed that despite proprietary models prevailing on the task, open-source LLMs lag behind."
            },
            "score": 6
        },
        {
            "id": "30669080bc6652f0466fba618b7c59317a346fb2",
            "paperId": "30669080bc6652f0466fba618b7c59317a346fb2",
            "title": "A Formalism and Approach for Improving Robustness of Large Language Models Using Risk-Adjusted Confidence Scores",
            "abstract": "Large Language Models (LLMs), such as ChatGPT, have achieved impressive milestones in natural language processing (NLP). Despite their impressive performance, the models are known to pose important risks. As these models are deployed in real-world applications, a systematic understanding of different risks posed by these models on tasks such as natural language inference (NLI), is much needed. In this paper, we define and formalize two distinct types of risk: decision risk and composite risk. We also propose a risk-centric evaluation framework, and four novel metrics, for assessing LLMs on these risks in both in-domain and out-of-domain settings. Finally, we propose a risk-adjusted calibration method called DwD for helping LLMs minimize these risks in an overall NLI architecture. Detailed experiments, using four NLI benchmarks, three baselines and two LLMs, including ChatGPT, show both the practical utility of the evaluation framework, and the efficacy of DwD in reducing decision and composite risk. For instance, when using DwD, an underlying LLM is able to address an extra 20.1% of low-risk inference tasks (but which the LLM erroneously deems high-risk without risk adjustment) and skip a further 19.8% of high-risk tasks, which would have been answered incorrectly.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper defines and formalizes two distinct types of risk: decision risk and composite risk, and proposes a risk-centric evaluation framework, and four novel metrics, for assessing LLMs on these risks in both in-domain and out-of-domain settings."
            },
            "score": 6
        },
        {
            "id": "67dab0fedfbdc403a2eb882b8d0efa7e16bc25da",
            "paperId": "67dab0fedfbdc403a2eb882b8d0efa7e16bc25da",
            "title": "Strength in Numbers: Estimating Confidence of Large Language Models by Prompt Agreement",
            "abstract": "Large language models have achieved impressive few-shot performance on a wide variety of tasks. However, in many settings, users require confidence estimates for model predictions. While traditional classifiers produce scores for each label, language models instead produce scores for the generation which may not be well calibrated. We compare generations across diverse prompts and show that these can be used to create confidence scores. By utilizing more prompts we can get more precise confidence estimates and use response diversity as a proxy for confidence. We evaluate this approach across ten multiple-choice question-answering datasets using three models: T0, FLAN-T5, and GPT-3. In addition to analyzing multiple human written prompts, we automatically generate more prompts using a language model in order to produce finer-grained confidence estimates. Our method produces more calibrated confidence estimates compared to the log probability of the answer to a single prompt. These improvements could benefit users who rely on prediction confidence for integration into a larger system or in decision-making processes.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work automatically generates more prompts using a language model in order to produce finer-grained confidence estimates and produces more calibrated confidence estimates compared to the log probability of the answer to a single prompt."
            },
            "score": 6
        },
        {
            "id": "444f3b7293b85b7d37600372941a289f9163abd1",
            "paperId": "444f3b7293b85b7d37600372941a289f9163abd1",
            "title": "LM-Polygraph: Uncertainty Estimation for Language Models",
            "abstract": "Recent advancements in the capabilities of large language models (LLMs) have paved the way for a myriad of groundbreaking applications in various fields. However, a significant challenge arises as these models often\"hallucinate\", i.e., fabricate facts without providing users an apparent means to discern the veracity of their statements. Uncertainty estimation (UE) methods are one path to safer, more responsible, and more effective use of LLMs. However, to date, research on UE methods for LLMs has been focused primarily on theoretical rather than engineering contributions. In this work, we tackle this issue by introducing LM-Polygraph, a framework with implementations of a battery of state-of-the-art UE methods for LLMs in text generation tasks, with unified program interfaces in Python. Additionally, it introduces an extendable benchmark for consistent evaluation of UE techniques by researchers, and a demo web application that enriches the standard chat dialog with confidence scores, empowering end-users to discern unreliable responses. LM-Polygraph is compatible with the most recent LLMs, including BLOOMz, LLaMA-2, ChatGPT, and GPT-4, and is designed to support future releases of similarly-styled LMs.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LM-Polygraph is introduced, a framework with implementations of a battery of state-of-the-art UE methods for LLMs in text generation tasks, with unified program interfaces in Python, and introduces an extendable benchmark for consistent evaluation of UE techniques by researchers."
            },
            "score": 6
        },
        {
            "id": "61d1ae955d87c9290fbc1a16e979c3e88b7a5e0f",
            "paperId": "61d1ae955d87c9290fbc1a16e979c3e88b7a5e0f",
            "title": "Making Pre-trained Language Models both Task-solvers and Self-calibrators",
            "abstract": "Pre-trained language models (PLMs) serve as backbones for various real-world systems. For high-stake applications, it's equally essential to have reasonable confidence estimations in predictions. While the vanilla confidence scores of PLMs can already be effectively utilized, PLMs consistently become overconfident in their wrong predictions, which is not desirable in practice. Previous work shows that introducing an extra calibration task can mitigate this issue. The basic idea involves acquiring additional data to train models in predicting the confidence of their initial predictions. However, it only demonstrates the feasibility of this kind of method, assuming that there are abundant extra available samples for the introduced calibration task. In this work, we consider the practical scenario that we need to effectively utilize training samples to make PLMs both task-solvers and self-calibrators. Three challenges are presented, including limited training samples, data imbalance, and distribution shifts. We first conduct pilot experiments to quantify various decisive factors in the calibration task. Based on the empirical analysis results, we propose a training algorithm LM-TOAST to tackle the challenges. Experimental results show that LM-TOAST can effectively utilize the training data to make PLMs have reasonable confidence estimations while maintaining the original task performance. Further, we consider three downstream applications, namely selective classification, adversarial defense, and model cascading, to show the practical usefulness of LM-TOAST. The code will be made public at \\url{https://github.com/Yangyi-Chen/LM-TOAST}.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results show that LM-TOAST can effectively utilize the training data to make PLMs have reasonable confidence estimations while maintaining the original task performance."
            },
            "score": 6
        },
        {
            "id": "33422275fbb9958f55419620697faf531482699b",
            "paperId": "33422275fbb9958f55419620697faf531482699b",
            "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering",
            "abstract": "Abstract Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question, \u201cHow can we know when language models know, with confidence, the answer to a particular query?\u201d We examine this question from the point of view of calibration, the property of a probabilistic model\u2019s predicted probabilities actually being well correlated with the probabilities of correctness. We examine three strong generative models\u2014T5, BART, and GPT-2\u2014and study whether their probabilities on QA tasks are well calibrated, finding the answer is a relatively emphatic no. We then examine methods to calibrate such models to make their confidence scores correlate better with the likelihood of correctness through fine-tuning, post-hoc probability modification, or adjustment of the predicted outputs or inputs. Experiments on a diverse range of datasets demonstrate the effectiveness of our methods. We also perform analysis to study the strengths and limitations of these methods, shedding light on further improvements that may be made in methods for calibrating LMs. We have released the code at https://github.com/jzbjyb/lm-calibration.",
            "year": 2020,
            "citationCount": 233,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper examines three strong generative models -- T5, BART, and GPT-2 -- and examines methods to calibrate such models to make their confidence scores correlate better with the likelihood of correctness through fine-tuning, post-hoc probability modification, or adjustment of the predicted outputs or inputs."
            },
            "score": 6
        },
        {
            "id": "14d0489047a1390434e7ea454e7e5165d9721ae3",
            "paperId": "14d0489047a1390434e7ea454e7e5165d9721ae3",
            "title": "Calibrating Long-form Generations from Large Language Models",
            "abstract": "To enhance Large Language Models' (LLMs) reliability, calibration is essential -- the model's assessed confidence scores should align with the actual likelihood of its responses being correct. However, current confidence elicitation methods and calibration metrics typically rely on a binary true/false assessment of response correctness. This approach does not apply to long-form generation, where an answer can be partially correct. Addressing this gap, we introduce a unified calibration framework, in which both the correctness of the LLMs' responses and their associated confidence levels are treated as distributions across a range of scores. Within this framework, we develop three metrics to precisely evaluate LLM calibration and further propose two confidence elicitation methods based on self-consistency and self-evaluation. Our experiments, which include long-form QA and summarization tasks, demonstrate that larger models don't necessarily guarantee better calibration, that calibration performance is found to be metric-dependent, and that self-consistency methods excel in factoid datasets. We also find that calibration can be enhanced through techniques such as fine-tuning, integrating relevant source documents, scaling the temperature, and combining self-consistency with self-evaluation. Lastly, we showcase a practical application of our system: selecting and cascading open-source models and ChatGPT to optimize correctness given a limited API budget. This research not only challenges existing notions of LLM calibration but also offers practical methodologies for improving trustworthiness in long-form generation.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A unified calibration framework is introduced, in which both the correctness of the LLMs' responses and their associated confidence levels are treated as distributions across a range of scores and two confidence elicitation methods based on self-consistency and self-evaluation are proposed."
            },
            "score": 6
        },
        {
            "id": "e9ab14dbbad99c2b31dce690816d53c21320239e",
            "paperId": "e9ab14dbbad99c2b31dce690816d53c21320239e",
            "title": "Few-Shot Recalibration of Language Models",
            "abstract": "Recent work has uncovered promising ways to extract well-calibrated confidence estimates from language models (LMs), where the model's confidence score reflects how likely it is to be correct. However, while LMs may appear well-calibrated over broad distributions, this often hides significant miscalibration within narrower slices (e.g., systemic over-confidence in math can balance out systemic under-confidence in history, yielding perfect calibration in aggregate). To attain well-calibrated confidence estimates for any slice of a distribution, we propose a new framework for few-shot slice-specific recalibration. Specifically, we train a recalibration model that takes in a few unlabeled examples from any given slice and predicts a curve that remaps confidence scores to be more accurate for that slice. Our trained model can recalibrate for arbitrary new slices, without using any labeled data from that slice. This enables us to identify domain-specific confidence thresholds above which the LM's predictions can be trusted, and below which it should abstain. Experiments show that our few-shot recalibrator consistently outperforms existing calibration methods, for instance improving calibration error for PaLM2-Large on MMLU by 16%, as compared to temperature scaling.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new framework for few-shot slice-specific recalibration that takes in a few unlabeled examples from any given slice and predicts a curve that remaps confidence scores to be more accurate for that slice, and can recalibrate for arbitrary new slices, without using any labeled data from that slice."
            },
            "score": 6
        },
        {
            "id": "ca261cb681b082e90ca6c7a9d325b4265ed1dc28",
            "paperId": "ca261cb681b082e90ca6c7a9d325b4265ed1dc28",
            "title": "MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models",
            "abstract": "Large language models (LLMs) have achieved remarkable performance in natural language understanding and generation tasks. However, they often suffer from limitations such as difficulty in incorporating new knowledge, generating hallucinations, and explaining their reasoning process. To address these challenges, we propose a novel prompting pipeline, named \\method, that leverages knowledge graphs (KGs) to enhance LLMs' inference and transparency. Our method enables LLMs to comprehend KG inputs and infer with a combination of implicit and external knowledge. Moreover, our method elicits the mind map of LLMs, which reveals their reasoning pathways based on the ontology of knowledge. We evaluate our method on diverse question \\&answering tasks, especially in medical domains, and show significant improvements over baselines. We also introduce a new hallucination evaluation benchmark and analyze the effects of different components of our method. Our results demonstrate the effectiveness and robustness of our method in merging knowledge from LLMs and KGs for combined inference. To reproduce our results and extend the framework further, we make our codebase available at https://github.com/wyl-willing/MindMap.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel prompting pipeline is proposed that leverages knowledge graphs (KGs) to enhance LLMs' inference and transparency and elicits the mind map of LLMs, which reveals their reasoning pathways based on the ontology of knowledge."
            },
            "score": 5
        },
        {
            "id": "a206a0c96d6076c6ab081288b0c2c95d3c7efd64",
            "paperId": "a206a0c96d6076c6ab081288b0c2c95d3c7efd64",
            "title": "Inspecting and Editing Knowledge Representations in Language Models",
            "abstract": "Neural language models (LMs) represent facts about the world described by text. Sometimes these facts derive from training data (in most LMs, a representation of the word\"banana\"encodes the fact that bananas are fruits). Sometimes facts derive from input text itself (a representation of the sentence\"I poured out the bottle\"encodes the fact that the bottle became empty). We describe REMEDI, a method for learning to map statements in natural language to fact encodings in an LM's internal representation system. REMEDI encodings can be used as knowledge editors: when added to LM hidden representations, they modify downstream generation to be consistent with new facts. REMEDI encodings may also be used as probes: when compared to LM representations, they reveal which properties LMs already attribute to mentioned entities, in some cases making it possible to predict when LMs will generate outputs that conflict with background knowledge or input text. REMEDI thus links work on probing, prompting, and LM editing, and offers steps toward general tools for fine-grained inspection and control of knowledge in LMs.",
            "year": 2023,
            "citationCount": 32,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "REMEDI is described, a method for learning to map statements in natural language to fact encodings in an LM's internal representation system, and offers steps toward general tools for fine-grained inspection and control of knowledge in LMs."
            },
            "score": 5
        },
        {
            "id": "44d74b0d77b4056ddd4c6611a76711c8bab2e0a7",
            "paperId": "44d74b0d77b4056ddd4c6611a76711c8bab2e0a7",
            "title": "Dehallucinating Large Language Models Using Formal Methods Guided Iterative Prompting",
            "abstract": "Large language models (LLMs) such as ChatGPT have been trained to generate human-like responses to natural language prompts. LLMs use a vast corpus of text data for training, and can generate coherent and contextually relevant responses to a wide range of questions and statements. Despite this remarkable progress, LLMs are prone to hallucinations making their application to safety-critical applications such as autonomous systems difficult. The hallucinations in LLMs refer to instances where the model generates responses that are not factually accurate or contextually appropriate. These hallucinations can occur due to a variety of factors, such as the model\u2019s lack of real-world knowledge, the influence of biased or inaccurate training data, or the model\u2019s tendency to generate responses based on statistical patterns rather than a true understanding of the input. While these hallucinations are a nuisance in tasks such as text summarization and question-answering, they can be catastrophic when LLMs are used in autonomy-relevant applications such as planning. In this paper, we focus on the application of LLMs in autonomous systems and sketch a novel self-monitoring and iterative prompting architecture that uses formal methods to detect these errors in the LLM response automatically. We exploit the dialog capability of LLMs to iteratively steer them to responses that are consistent with our correctness specification. We report preliminary experiments that show the promise of the proposed approach on tasks such as automated planning.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper sketches a novel self-monitoring and iterative prompting architecture that uses formal methods to detect errors in the LLM response automatically and exploits the dialog capability of LLMs to iteratively steer them to responses that are consistent with the correctness specification."
            },
            "score": 5
        },
        {
            "id": "3867d999b550b57e6762f9d4b0114ee7551b2e2f",
            "paperId": "3867d999b550b57e6762f9d4b0114ee7551b2e2f",
            "title": "Predicting Question-Answering Performance of Large Language Models through Semantic Consistency",
            "abstract": "Semantic consistency of a language model is broadly defined as the model\u2019s ability to produce semantically-equivalent outputs, given semantically-equivalent inputs. We address the task of assessing question-answering (QA) semantic consistency of contemporary large language models (LLMs) by manually creating a benchmark dataset with high-quality paraphrases for factual questions, and release the dataset to the community.We further combine the semantic consistency metric with additional measurements suggested in prior work as correlating with LLM QA accuracy, for building and evaluating a framework for factual QA reference-less performance prediction \u2013 predicting the likelihood of a language model to accurately answer a question. Evaluating the framework on five contemporary LLMs, we demonstrate encouraging, significantly outperforming baselines, results.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work addresses the task of assessing question-answering (QA) semantic consistency of contemporary large language models (LLMs) by manually creating a benchmark dataset with high-quality paraphrases for factual questions, and releases the dataset to the community."
            },
            "score": 5
        },
        {
            "id": "df9139b3908af17cdb19a3ced717f5060cf92a5f",
            "paperId": "df9139b3908af17cdb19a3ced717f5060cf92a5f",
            "title": "An Empirical Study of Clinical Note Generation from Doctor-Patient Encounters",
            "abstract": "Medical doctors spend on average 52 to 102 minutes per day writing clinical notes from their patient encounters (Hripcsak et al., 2011). Reducing this workload calls for relevant and efficient summarization methods. In this paper, we introduce new resources and empirical investigations for the automatic summarization of doctor-patient conversations in a clinical setting. In particular, we introduce the MTS-Dialog dataset; a new collection of 1,700 doctor-patient dialogues and corresponding clinical notes. We use this new dataset to investigate the feasibility of this task and the relevance of existing language models, data augmentation, and guided summarization techniques. We compare standard evaluation metrics based on n-gram matching, contextual embeddings, and Fact Extraction to assess the accuracy and the factual consistency of the generated summaries. To ground these results, we perform an expert-based evaluation using relevant natural language generation criteria and task-specific criteria such as critical omissions, and study the correlation between the automatic metrics and expert judgments. To the best of our knowledge, this study is the first attempt to introduce an open dataset of doctor-patient conversations and clinical notes, with detailed automated and manual evaluations of clinical note generation.",
            "year": 2023,
            "citationCount": 32,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study is the first attempt to introduce an open dataset of doctor-patient conversations and clinical notes, with detailed automated and manual evaluations of clinical note generation, using the MTS-Dialog dataset."
            },
            "score": 5
        },
        {
            "id": "e6e3020251d354159885a7c603c0e6fa28144f2a",
            "paperId": "e6e3020251d354159885a7c603c0e6fa28144f2a",
            "title": "Reliability Check: An Analysis of GPT-3\u2019s Response to Sensitive Topics and Prompt Wording",
            "abstract": "Large language models (LLMs) have become mainstream technology with their versatile use cases and impressive performance. Despite the countless out-of-the-box applications, LLMs are still not reliable. A lot of work is being done to improve the factual accuracy, consistency, and ethical standards of these models through fine-tuning, prompting, and Reinforcement Learning with Human Feedback (RLHF), but no systematic analysis of the responses of these models to different categories of statements, or on their potential vulnerabilities to simple prompting changes is available. In this work, we analyze what confuses GPT-3: how the model responds to certain sensitive topics and what effects the prompt wording has on the model response. We find that GPT-3 correctly disagrees with obvious Conspiracies and Stereotypes but makes mistakes with common Misconceptions and Controversies. The model responses are inconsistent across prompts and settings, highlighting GPT-3\u2019s unreliability.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work analyzes what confuses GPT-3: how the model responds to certain sensitive topics and what effects the prompt wording has on the model response."
            },
            "score": 5
        },
        {
            "id": "3a89e289e2dd29f5e52a2bf354a637762b661257",
            "paperId": "3a89e289e2dd29f5e52a2bf354a637762b661257",
            "title": "Fine-tuning Language Models for Factuality",
            "abstract": "The fluency and creativity of large pre-trained language models (LLMs) have led to their widespread use, sometimes even as a replacement for traditional search engines. Yet language models are prone to making convincing but factually inaccurate claims, often referred to as 'hallucinations.' These errors can inadvertently spread misinformation or harmfully perpetuate misconceptions. Further, manual fact-checking of model responses is a time-consuming process, making human factuality labels expensive to acquire. In this work, we fine-tune language models to be more factual, without human labeling and targeting more open-ended generation settings than past work. We leverage two key recent innovations in NLP to do so. First, several recent works have proposed methods for judging the factuality of open-ended text by measuring consistency with an external knowledge base or simply a large model's confidence scores. Second, the direct preference optimization algorithm enables straightforward fine-tuning of language models on objectives other than supervised imitation, using a preference ranking over possible model responses. We show that learning from automatically generated factuality preference rankings, generated either through existing retrieval systems or our novel retrieval-free approach, significantly improves the factuality (percent of generated claims that are correct) of Llama-2 on held-out topics compared with RLHF or decoding strategies targeted at factuality. At 7B scale, compared to Llama-2-chat, we observe 58% and 40% reduction in factual error rate when generating biographies and answering medical questions, respectively.",
            "year": 2023,
            "citationCount": 55,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that learning from automatically generated factuality preference rankings, generated either through existing retrieval systems or the novel retrieval-free approach, significantly improves the factuality of Llama-2 on held-out topics compared with RLHF or decoding strategies targeted at factuality."
            },
            "score": 5
        },
        {
            "id": "c76541024ed59403f99a5a73ba69849112959a6e",
            "paperId": "c76541024ed59403f99a5a73ba69849112959a6e",
            "title": "A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models",
            "abstract": "The tendency of Large Language Models to generate hallucinations and exhibit overconfidence in predictions raises concerns regarding their reliability. Confidence or uncertainty estimations indicating the extent of trustworthiness of a model's response are essential to developing reliable AI systems. Current research primarily focuses on LLM confidence estimations in English, remaining a void for other widely used languages and impeding the global development of reliable AI applications. This paper introduces a comprehensive investigation of Multi-lingual confidence estimation (MlingConf) on LLMs. First, we introduce an elaborated and expert-checked multilingual QA dataset. Second, we delve into the performance of confidence estimations and examine how these confidence scores can enhance LLM performance through self-refinement across diverse languages. Finally, we propose a cross-lingual confidence estimation method to achieve more precise confidence scores. The experimental results showcase the performance of various confidence estimation methods across different languages as well as present that our proposed cross-lingual confidence estimation technique significantly enhances confidence estimation and outperforms several baseline methods.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A comprehensive investigation of Multi-lingual confidence estimation (MlingConf) on LLMs is introduced, an elaborated and expert-checked multilingual QA dataset is introduced, and a cross-lingual confidence estimation method is proposed to achieve more precise confidence scores."
            },
            "score": 5
        },
        {
            "id": "67d0aa08d22feb0dbd36defece61256a4a1a0282",
            "paperId": "67d0aa08d22feb0dbd36defece61256a4a1a0282",
            "title": "Empirical evaluation of Uncertainty Quantification in Retrieval-Augmented Language Models for Science",
            "abstract": "Large language models (LLMs) have shown remarkable achievements in natural language processing tasks, producing high-quality outputs. However, LLMs still exhibit limitations, including the generation of factually incorrect information. In safety-critical applications, it is important to assess the confidence of LLM-generated content to make informed decisions. Retrieval Augmented Language Models (RALMs) is relatively a new area of research in NLP. RALMs offer potential benefits for scientific NLP tasks, as retrieved documents, can serve as evidence to support model-generated content. This inclusion of evidence enhances trustworthiness, as users can verify and explore the retrieved documents to validate model outputs. Quantifying uncertainty in RALM generations further improves trustworthiness, with retrieved text and confidence scores contributing to a comprehensive and reliable model for scientific applications. However, there is limited to no research on UQ for RALMs, particularly in scientific contexts. This study aims to address this gap by conducting a comprehensive evaluation of UQ in RALMs, focusing on scientific tasks. This research investigates how uncertainty scores vary when scientific knowledge is incorporated as pretraining and retrieval data and explores the relationship between uncertainty scores and the accuracy of model-generated outputs. We observe that an existing RALM finetuned with scientific knowledge as the retrieval data tends to be more confident in generating predictions compared to the model pretrained only with scientific knowledge. We also found that RALMs are overconfident in their predictions, making inaccurate predictions more confidently than accurate ones. Scientific knowledge provided either as pretraining or retrieval corpus does not help alleviate this issue. We released our code, data and dashboards at https://github.com/pnnl/EXPERT2.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Investigating how uncertainty scores vary when scientific knowledge is incorporated as pretraining and retrieval data and explores the relationship between uncertainty scores and the accuracy of model-generated outputs finds that RALMs are overconfident in their predictions."
            },
            "score": 5
        },
        {
            "id": "ad075acdd57625d72fac95b625c758ed8dadcf91",
            "paperId": "ad075acdd57625d72fac95b625c758ed8dadcf91",
            "title": "Bayesian Active Learning with Pretrained Language Models",
            "abstract": "Active Learning (AL) is a method to iteratively select data for annotation from a pool of unlabeled data, aiming to achieve better model performance than random selection. Previous AL approaches in Natural Language Processing (NLP) have been limited to either task-speci\ufb01c models that are trained from scratch at each iteration using only the labeled data at hand or using off-the-shelf pretrained language models (LMs) that are not adapted effectively to the downstream task. In this paper, we address these limitations by introducing B ALM ; Bayesian Active Learning with pre-trained language Models. We \ufb01rst propose to adapt the pretrained LM to the downstream task by continuing training with all the available unlabeled data and then use it for AL. We also suggest a simple yet effective \ufb01ne-tuning method to ensure that the adapted LM is properly trained in both low and high resource scenarios during AL. We \ufb01nally apply Monte Carlo dropout to the downstream model to obtain well-calibrated con\ufb01dence scores for data selection with uncertainty sampling. Our experiments in \ufb01ve standard natural language understanding tasks demonstrate that B ALM provides substantial data ef\ufb01ciency improvements compared to various combinations of acquisition functions, models and \ufb01ne-tuning meth-ods proposed in recent AL literature.",
            "year": 2021,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "B ALM is introduced ; Bayesian Active Learning with pre-trained language Models ; Monte Carlo dropout to the downstream model to obtain well-calibrated con\ufb01dence scores for data selection with uncertainty sampling and a simple yet effective tuning method to ensure that the adapted LM is properly trained in both low and high resource scenarios during AL."
            },
            "score": 5
        },
        {
            "id": "d5a6fc6aa139066e3b66ba63002e7d84c109aebc",
            "paperId": "d5a6fc6aa139066e3b66ba63002e7d84c109aebc",
            "title": "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing",
            "abstract": "Large language models (LLMs) have shown remarkable capabilities in Natural Language Processing (NLP), especially in domains where labeled data is scarce or expensive, such as clinical domain. However, to unlock the clinical knowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform specific clinical NLP tasks without any task-specific training data. This is known as in-context learning, which is an art and science that requires understanding the strengths and weaknesses of different LLMs and prompt engineering approaches. In this paper, we present a comprehensive and systematic experimental study on prompt engineering for five clinical NLP tasks: Clinical Sense Disambiguation, Biomedical Evidence Extraction, Coreference Resolution, Medication Status Extraction, and Medication Attribute Extraction. We assessed the prompts proposed in recent literature, including simple prefix, simple cloze, chain of thought, and anticipatory prompts, and introduced two new types of prompts, namely heuristic prompting and ensemble prompting. We evaluated the performance of these prompts on three state-of-the-art LLMs: GPT-3.5, BARD, and LLAMA2. We also contrasted zero-shot prompting with few-shot prompting, and provide novel insights and guidelines for prompt engineering for LLMs in clinical NLP. To the best of our knowledge, this is one of the first works on the empirical evaluation of different prompt engineering approaches for clinical NLP in this era of generative AI, and we hope that it will inspire and inform future research in this area.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents a comprehensive and systematic experimental study on prompt engineering for five clinical NLP tasks: Clinical Sense Disambiguation, Biomedical Evidence Extraction, Coreference Resolution, Medication Status Ext extraction, and Medication Attribute Extraction."
            },
            "score": 5
        },
        {
            "id": "ccfdea3722ec43e45fc33e170d1fe908dca003a5",
            "paperId": "ccfdea3722ec43e45fc33e170d1fe908dca003a5",
            "title": "TransPrompt v2: A Transferable Prompting Framework for Cross-task Text Classification",
            "abstract": "Text classification is one of the most imperative tasks in natural language processing (NLP). Recent advances with pre-trained language models (PLMs) have shown remarkable success on this task. However, the satisfying results obtained by PLMs heavily depend on the large amounts of task-specific labeled data, which may not be feasible in many application scenarios due to data access and privacy constraints. The recently-proposed prompt-based fine-tuning paradigm improves the performance of PLMs for few-shot text classification with task-specific templates. Yet, it is unclear how the prompting knowledge can be transferred across tasks, for the purpose of mutual reinforcement. We propose TransPrompt v2, a novel transferable prompting framework for few-shot learning across similar or distant text classification tasks. For learning across similar tasks, we employ a multi-task meta-knowledge acquisition (MMA) procedure to train a meta-learner that captures the cross-task transferable knowledge. For learning across distant tasks, we further inject the task type descriptions into the prompt, and capture the intra-type and inter-type prompt embeddings among multiple distant tasks. Additionally, two de-biasing techniques are further designed to make the trained meta-learner more task-agnostic and unbiased towards any tasks. After that, the meta-learner can be adapted to each specific task with better parameters initialization. Extensive experiments show that TransPrompt v2 outperforms single-task and cross-task strong baselines over multiple NLP tasks and datasets. We further show that the meta-learner can effectively improve the performance of PLMs on previously unseen tasks. In addition, TransPrompt v2 also outperforms strong fine-tuning baselines when learning with full training sets.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Extensive experiments show that TransPrompt v2 outperforms single-task and cross-task strong baselines over multiple NLP tasks and datasets, and shows that the meta-learner can effectively improve the performance of PLMs on previously unseen tasks."
            },
            "score": 5
        },
        {
            "id": "d33a14592d68da068953cdf37f8bf562740c0085",
            "paperId": "d33a14592d68da068953cdf37f8bf562740c0085",
            "title": "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study",
            "abstract": "Background Large language models (LLMs) have shown remarkable capabilities in natural language processing (NLP), especially in domains where labeled data are scarce or expensive, such as the clinical domain. However, to unlock the clinical knowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform specific clinical NLP tasks without any task-specific training data. This is known as in-context learning, which is an art and science that requires understanding the strengths and weaknesses of different LLMs and prompt engineering approaches. Objective The objective of this study is to assess the effectiveness of various prompt engineering techniques, including 2 newly introduced types\u2014heuristic and ensemble prompts, for zero-shot and few-shot clinical information extraction using pretrained language models. Methods This comprehensive experimental study evaluated different prompt types (simple prefix, simple cloze, chain of thought, anticipatory, heuristic, and ensemble) across 5 clinical NLP tasks: clinical sense disambiguation, biomedical evidence extraction, coreference resolution, medication status extraction, and medication attribute extraction. The performance of these prompts was assessed using 3 state-of-the-art language models: GPT-3.5 (OpenAI), Gemini (Google), and LLaMA-2 (Meta). The study contrasted zero-shot with few-shot prompting and explored the effectiveness of ensemble approaches. Results The study revealed that task-specific prompt tailoring is vital for the high performance of LLMs for zero-shot clinical NLP. In clinical sense disambiguation, GPT-3.5 achieved an accuracy of 0.96 with heuristic prompts and 0.94 in biomedical evidence extraction. Heuristic prompts, alongside chain of thought prompts, were highly effective across tasks. Few-shot prompting improved performance in complex scenarios, and ensemble approaches capitalized on multiple prompt strengths. GPT-3.5 consistently outperformed Gemini and LLaMA-2 across tasks and prompt types. Conclusions This study provides a rigorous evaluation of prompt engineering methodologies and introduces innovative techniques for clinical information extraction, demonstrating the potential of in-context learning in the clinical domain. These findings offer clear guidelines for future prompt-based clinical NLP research, facilitating engagement by non-NLP experts in clinical NLP advancements. To the best of our knowledge, this is one of the first works on the empirical evaluation of different prompt engineering approaches for clinical NLP in this era of generative artificial intelligence, and we hope that it will inspire and inform future research in this area.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study revealed that task-specific prompt tailoring is vital for the high performance of LLMs for zero-shot clinical NLP, and introduces innovative techniques for clinical information extraction, demonstrating the potential of in-context learning in the clinical domain."
            },
            "score": 5
        },
        {
            "id": "6ad26eb2d2aa6679d16d9c16fb75cd2cbe1127bc",
            "paperId": "6ad26eb2d2aa6679d16d9c16fb75cd2cbe1127bc",
            "title": "See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning",
            "abstract": "Large pre-trained vision and language models have demonstrated remarkable capacities for various tasks. However, solving the knowledge-based visual reasoning tasks remains challenging, which requires a model to comprehensively understand image content, connect the external world knowledge, and perform step-by-step reasoning to answer the questions correctly. To this end, we propose a novel framework named Interactive Prompting Visual Reasoner (IPVR) for few-shot knowledge-based visual reasoning. IPVR contains three stages, see, think and confirm. The see stage scans the image and grounds the visual concept candidates with a visual perception model. The think stage adopts a pre-trained large language model (LLM) to attend to the key concepts from candidates adaptively. It then transforms them into text context for prompting with a visual captioning model and adopts the LLM to generate the answer. The confirm stage further uses the LLM to generate the supporting rationale to the answer, verify the generated rationale with a cross-modality classifier and ensure that the rationale can infer the predicted output consistently. We conduct experiments on a range of knowledge-based visual reasoning datasets. We found our IPVR enjoys several benefits, 1). it achieves better performance than the previous few-shot learning baselines; 2). it enjoys the total transparency and trustworthiness of the whole reasoning process by providing rationales for each reasoning step; 3). it is computation-efficient compared with other fine-tuning baselines.",
            "year": 2023,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel framework named Interactive Prompting Visual Reasoner (IPVR), which achieves better performance than the previous few-shot learning baselines, and enjoys the total transparency and trustworthiness of the whole reasoning process by providing rationales for each reasoning step."
            },
            "score": 4
        },
        {
            "id": "478ec7a8001d46cde90395c4a9d9ffdec59d5ce3",
            "paperId": "478ec7a8001d46cde90395c4a9d9ffdec59d5ce3",
            "title": "Prompting Language Models for Linguistic Structure",
            "abstract": "Although pretrained language models (PLMs) can be prompted to perform a wide range of language tasks, it remains an open question how much this ability comes from generalizable linguistic understanding versus surface-level lexical patterns. To test this, we present a structured prompting approach for linguistic structured prediction tasks, allowing us to perform zero- and few-shot sequence tagging with autoregressive PLMs. We evaluate this approach on part-of-speech tagging, named entity recognition, and sentence chunking, demonstrating strong few-shot performance in all cases. We also find that while PLMs contain significant prior knowledge of task labels due to task leakage into the pretraining corpus, structured prompting can also retrieve linguistic structure with arbitrary labels. These findings indicate that the in-context learning ability and linguistic knowledge of PLMs generalizes beyond memorization of their training data.",
            "year": 2022,
            "citationCount": 21,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that while PLMs contain significant prior knowledge of task labels due to task leakage into the pretraining corpus, structured prompting can also retrieve linguistic structure with arbitrary labels, indicating that the in-context learning ability and linguistic knowledge of PLMs generalizes beyond memorization of their training data."
            },
            "score": 4
        },
        {
            "id": "9e9f675830caceddca3619c2c4d4441e519d894f",
            "paperId": "9e9f675830caceddca3619c2c4d4441e519d894f",
            "title": "Prophet: Prompting Large Language Models with Complementary Answer Heuristics for Knowledge-based Visual Question Answering",
            "abstract": "Knowledge-based visual question answering (VQA) requires external knowledge beyond the image to answer the question. Early studies retrieve required knowledge from explicit knowledge bases (KBs), which often introduces irrelevant information to the question, hence restricting the performance of their models. Recent works have resorted to using a powerful large language model (LLM) as an implicit knowledge engine to acquire the necessary knowledge for answering. Despite the encouraging results achieved by these methods, we argue that they have not fully activated the capacity of the blind LLM as the provided textual input is insufficient to depict the required visual information to answer the question. In this paper, we present Prophet -- a conceptually simple, flexible, and general framework designed to prompt LLM with answer heuristics for knowledge-based VQA. Specifically, we first train a vanilla VQA model on a specific knowledge-based VQA dataset without external knowledge. After that, we extract two types of complementary answer heuristics from the VQA model: answer candidates and answer-aware examples. Finally, the two types of answer heuristics are jointly encoded into a formatted prompt to facilitate the LLM's understanding of both the image and question, thus generating a more accurate answer. By incorporating the state-of-the-art LLM GPT-3, Prophet significantly outperforms existing state-of-the-art methods on four challenging knowledge-based VQA datasets. To demonstrate the generality of our approach, we instantiate Prophet with the combinations of different VQA models (i.e., both discriminative and generative ones) and different LLMs (i.e., both commercial and open-source ones).",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Prophet -- a conceptually simple, flexible, and general framework designed to prompt LLM with answer heuristics for knowledge-based VQA, which significantly outperforms existing state-of-the-art methods on four challenging knowledge-based VQA datasets."
            },
            "score": 4
        },
        {
            "id": "8fcc5dc9563d3b0d2b88987e8a8c4710e800aab2",
            "paperId": "8fcc5dc9563d3b0d2b88987e8a8c4710e800aab2",
            "title": "Making Large Language Models Better Knowledge Miners for Online Marketing with Progressive Prompting Augmentation",
            "abstract": "Nowadays, the rapid development of mobile economy has promoted the flourishing of online marketing campaigns, whose success greatly hinges on the efficient matching between user preferences and desired marketing campaigns where a well-established Marketing-oriented Knowledge Graph (dubbed as MoKG) could serve as the critical\"bridge\"for preference propagation. In this paper, we seek to carefully prompt a Large Language Model (LLM) with domain-level knowledge as a better marketing-oriented knowledge miner for marketing-oriented knowledge graph construction, which is however non-trivial, suffering from several inevitable issues in real-world marketing scenarios, i.e., uncontrollable relation generation of LLMs,insufficient prompting ability of a single prompt, the unaffordable deployment cost of LLMs. To this end, we propose PAIR, a novel Progressive prompting Augmented mIning fRamework for harvesting marketing-oriented knowledge graph with LLMs. In particular, we reduce the pure relation generation to an LLM based adaptive relation filtering process through the knowledge-empowered prompting technique. Next, we steer LLMs for entity expansion with progressive prompting augmentation,followed by a reliable aggregation with comprehensive consideration of both self-consistency and semantic relatedness. In terms of online serving, we specialize in a small and white-box PAIR (i.e.,LightPAIR),which is fine-tuned with a high-quality corpus provided by a strong teacher-LLM. Extensive experiments and practical applications in audience targeting verify the effectiveness of the proposed (Light)PAIR.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes PAIR, a novel Progressive prompting Augmented Augmented prompting technique for harvesting marketing-oriented knowledge graph with LLMs, and reduces the pure relation generation to an LLM based adaptive relation filtering process through the knowledge-empowered prompting technique."
            },
            "score": 4
        },
        {
            "id": "49408c5e1ac75854f1580e561384df2be870d559",
            "paperId": "49408c5e1ac75854f1580e561384df2be870d559",
            "title": "KnowledGPT: Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases",
            "abstract": "Large language models (LLMs) have demonstrated impressive impact in the field of natural language processing, but they still struggle with several issues regarding, such as completeness, timeliness, faithfulness and adaptability. While recent efforts have focuses on connecting LLMs with external knowledge sources, the integration of knowledge bases (KBs) remains understudied and faces several challenges. In this paper, we introduce KnowledGPT, a comprehensive framework to bridge LLMs with various knowledge bases, facilitating both the retrieval and storage of knowledge. The retrieval process employs the program of thought prompting, which generates search language for KBs in code format with pre-defined functions for KB operations. Besides retrieval, KnowledGPT offers the capability to store knowledge in a personalized KB, catering to individual user demands. With extensive experiments, we show that by integrating LLMs with KBs, KnowledGPT properly answers a broader range of questions requiring world knowledge compared with vanilla LLMs, utilizing both knowledge existing in widely-known KBs and extracted into personalized KBs.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "By integrating LLMs with KBs, KnowledGPT properly answers a broader range of questions requiring world knowledge compared with vanilla LLMs, utilizing both knowledge existing in widely-known KBs and extracted into personalized KBs."
            },
            "score": 4
        },
        {
            "id": "01394b236d6481889d8a0f25a3d8ea6a43091d15",
            "paperId": "01394b236d6481889d8a0f25a3d8ea6a43091d15",
            "title": "Sequential Text-based Knowledge Update with Self-Supervised Learning for Generative Language Models",
            "abstract": "This work proposes a new natural language processing (NLP) task to tackle the issue of multi-round, sequential text-based knowledge update. The study introduces a hybrid learning architecture and a novel self-supervised training strategy to enable generative language models to consolidate knowledge in the same way as humans. A dataset was also created for evaluation and results showed the effectiveness of our methodology. Experimental results confirm the superiority of the proposed approach over existing models and large language models (LLMs). The proposed task and model framework have the potential to significantly improve the automation of knowledge organization, making text-based knowledge an increasingly crucial resource for powerful LLMs to perform various tasks for humans.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The study introduces a hybrid learning architecture and a novel self-supervised training strategy to enable generative language models to consolidate knowledge in the same way as humans and confirms the superiority of the proposed approach over existing models and large language models."
            },
            "score": 4
        },
        {
            "id": "2f8a1714e3bb01c522fede9c902bd7cf5320a802",
            "paperId": "2f8a1714e3bb01c522fede9c902bd7cf5320a802",
            "title": "Test-Time Self-Adaptive Small Language Models for Question Answering",
            "abstract": "Recent instruction-finetuned large language models (LMs) have achieved notable performances in various tasks, such as question-answering (QA). However, despite their ability to memorize a vast amount of general knowledge across diverse tasks, they might be suboptimal on specific tasks due to their limited capacity to transfer and adapt knowledge to target tasks. Moreover, further finetuning LMs with labeled datasets is often infeasible due to their absence, but it is also questionable if we can transfer smaller LMs having limited knowledge only with unlabeled test data. In this work, we show and investigate the capabilities of smaller self-adaptive LMs, only with unlabeled test data. In particular, we first stochastically generate multiple answers, and then ensemble them while filtering out low-quality samples to mitigate noise from inaccurate labels. Our proposed self-adaption strategy demonstrates significant performance improvements on benchmark QA datasets with higher robustness across diverse prompts, enabling LMs to stay stable. Code is available at: https://github.com/starsuzi/T-SAS.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work shows and investigates the capabilities of smaller self-adaptive LMs, only with unlabeled test data, and proposes a self-adaption strategy that demonstrates significant performance improvements on benchmark QA datasets with higher robustness across diverse prompts, enabling LMs to stay stable."
            },
            "score": 4
        },
        {
            "id": "2cf1f6c723006f258599fd9f000bb616ae83387a",
            "paperId": "2cf1f6c723006f258599fd9f000bb616ae83387a",
            "title": "Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models",
            "abstract": "The performance of large language models (LLMs) on existing reasoning benchmarks has significantly improved over the past years. In response, we present JEEBench, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs. We curate 515 challenging pre-engineering mathematics, physics and chemistry problems from the highly competitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep in-domain knowledge is essential for solving problems in this benchmark. Our evaluation on various open-source and proprietary models reveals that the highest performance, even after using techniques like self-consistency, self-refinement and chain-of-thought prompting, is less than 40%. The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts. We also observe that by mere prompting, GPT-4 is unable to assess risk introduced by negative marking for incorrect answers. For this, we develop a post-hoc confidence-thresholding method over self-consistency, which enables effective response selection. We hope that our challenging benchmark will guide future re-search in problem-solving using LLMs.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "JEEBench is presented, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs, and a post-hoc confidence-thresholding method over self-consistency is developed, which enables effective response selection."
            },
            "score": 4
        },
        {
            "id": "7df3595bdb4003589e8ca1757cc39ec03a39a2ff",
            "paperId": "7df3595bdb4003589e8ca1757cc39ec03a39a2ff",
            "title": "ZARA: Improving Few-Shot Self-Rationalization for Small Language Models",
            "abstract": "Language models (LMs) that jointly generate end-task answers as well as free-text rationales are known as self-rationalization models. Recent works demonstrate great performance gain for self-rationalization by few-shot prompting LMs with rationale-augmented exemplars. However, the ability to benefit from explanations only emerges with large-scale LMs, which have poor accessibility. In this work, we explore the less-studied setting of leveraging explanations for small LMs to improve few-shot self-rationalization. We first revisit the relationship between rationales and answers. Inspired by the implicit mental process of how human beings assess explanations, we present a novel approach, Zero-shot Augmentation of Rationale-Answer pairs (ZARA), to automatically construct pseudo-parallel data for self-training by reducing the problem of plausibility judgement to natural language inference. Experimental results show ZARA achieves SOTA performance on the FEB benchmark, for both the task accuracy and the explanation metric. In addition, we conduct human and quantitative evaluation validating ZARA's ability to automatically identify plausible and accurate rationale-answer pairs.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents a novel approach, Zero-shot Augmentation of Rationale-Answer pairs (ZARA), to automatically construct pseudo-parallel data for self-training by reducing the problem of plausibility judgement to natural language inference."
            },
            "score": 4
        },
        {
            "id": "2f33488d1e4d405f84b1f15021c0870aec8bf680",
            "paperId": "2f33488d1e4d405f84b1f15021c0870aec8bf680",
            "title": "Exploring the pitfalls of large language models: Inconsistency and inaccuracy in answering pathology board examination\u2010style questions",
            "abstract": "In the rapidly advancing field of artificial intelligence, large language models (LLMs) such as ChatGPT and Google Bard are making significant progress, with applications extending across various fields, including medicine. This study explores their potential utility and pitfalls by assessing the performance of these LLMs in answering 150 multiple-choice questions sourced from the PathologyOutlines.com Question Bank, a well-established resource for pathology examination preparation. The assessment, encompassing 15 subspecialties in pathology, evaluated the accuracy and consistency of responses by these LLMs. Overall, ChatGPT outperformed Google Bard, scoring 122 out of 150, while Google Bard achieved a score of 70. In addition to accuracy, we explored the consistency of these LLMs by applying a test-retest approach over a two-week interval. ChatGPT showed a consistency rate of 85%, while Google Bard exhibited a lower consistency rate of 61%. In-depth analysis of incorrect responses identified potential factual inaccuracies and interpretive errors, underscoring the need for ongoing model refinement and human oversight. In conclusion, while LLMs have potential to enhance medical education and assist clinical decision-making, their current limitations underscore the need for continued development and the critical role of human expertise in the application of such models.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "While LLMs have potential to enhance medical education and assist clinical decision-making, their current limitations underscore the need for continued development and the critical role of human expertise in the application of such models."
            },
            "score": 4
        },
        {
            "id": "d36bbbe2eb83981c4e714f1d4688334f2aae6369",
            "paperId": "d36bbbe2eb83981c4e714f1d4688334f2aae6369",
            "title": "GPT-4 as Evaluator: Evaluating Large Language Models on Pest Management in Agriculture",
            "abstract": "In the rapidly evolving field of artificial intelligence (AI), the application of large language models (LLMs) in agriculture, particularly in pest management, remains nascent. We aimed to prove the feasibility by evaluating the content of the pest management advice generated by LLMs, including the Generative Pre-trained Transformer (GPT) series from OpenAI and the FLAN series from Google. Considering the context-specific properties of agricultural advice, automatically measuring or quantifying the quality of text generated by LLMs becomes a significant challenge. We proposed an innovative approach, using GPT-4 as an evaluator, to score the generated content on Coherence, Logical Consistency, Fluency, Relevance, Comprehensibility, and Exhaustiveness. Additionally, we integrated an expert system based on crop threshold data as a baseline to obtain scores for Factual Accuracy on whether pests found in crop fields should take management action. Each model's score was weighted by percentage to obtain a final score. The results showed that GPT-3.4 and GPT-4 outperform the FLAN models in most evaluation categories. Furthermore, the use of instruction-based prompting containing domain-specific knowledge proved the feasibility of LLMs as an effective tool in agriculture, with an accuracy rate of 72%, demonstrating LLMs' effectiveness in providing pest management suggestions.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The use of instruction-based prompting containing domain-specific knowledge proved the feasibility of LLMs as an effective tool in agriculture, with an accuracy rate of 72%, demonstrating LLMs' effectiveness in providing pest management suggestions."
            },
            "score": 4
        },
        {
            "id": "b3645613ffa3ea0f6bf23a21a228c7d0b9d29b12",
            "paperId": "b3645613ffa3ea0f6bf23a21a228c7d0b9d29b12",
            "title": "Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models with IdentityChain",
            "abstract": "Code Large Language Models (Code LLMs) are being increasingly employed in real-life applications, so evaluating them is critical. While the conventional accuracy evaluates the performance of Code LLMs on a set of individual tasks, their self-consistency across different tasks is overlooked. Intuitively, a trustworthy model should be self-consistent when generating natural language specifications for its own code and generating code for its own specifications. Failure to preserve self-consistency reveals a lack of understanding of the shared semantics underlying natural language and programming language, and therefore undermines the trustworthiness of a model. In this paper, we first formally define the self-consistency of Code LLMs and then design a framework, IdentityChain, which effectively and efficiently evaluates the self-consistency and conventional accuracy of a model at the same time. We study eleven Code LLMs and show that they fail to preserve self-consistency, which is indeed a distinct aspect from conventional accuracy. Furthermore, we show that IdentityChain can be used as a model debugging tool to expose weaknesses of Code LLMs by demonstrating three major weaknesses that we identify in current models using IdentityChain. Our code is available at https://github.com/marcusm117/IdentityChain.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper formally defines the self-consistency of Code LLMs and designs a framework, IdentityChain, which effectively and efficiently evaluates the self-consistency and conventional accuracy of a model at the same time."
            },
            "score": 4
        },
        {
            "id": "525ff089b6c5df9b311124adc3e549060093c982",
            "paperId": "525ff089b6c5df9b311124adc3e549060093c982",
            "title": "Do Large Language Models Show Human-like Biases? Exploring Confidence - Competence Gap in AI",
            "abstract": "This study investigates self-assessment tendencies in Large Language Models (LLMs), examining if patterns resemble human cognitive biases like the Dunning\u2013Kruger effect. LLMs, including GPT, BARD, Claude, and LLaMA, are evaluated using confidence scores on reasoning tasks. The models provide self-assessed confidence levels before and after responding to different questions. The results show cases where high confidence does not correlate with correctness, suggesting overconfidence. Conversely, low confidence despite accurate responses indicates potential underestimation. The confidence scores vary across problem categories and difficulties, reducing confidence for complex queries. GPT-4 displays consistent confidence, while LLaMA and Claude demonstrate more variations. Some of these patterns resemble the Dunning\u2013Kruger effect, where incompetence leads to inflated self-evaluations. While not conclusively evident, these observations parallel this phenomenon and provide a foundation to further explore the alignment of competence and confidence in LLMs. As LLMs continue to expand their societal roles, further research into their self-assessment mechanisms is warranted to fully understand their capabilities and limitations.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study investigates self-assessment tendencies in Large Language Models, examining if patterns resemble human cognitive biases like the Dunning\u2013Kruger effect, and shows cases where high confidence does not correlate with correctness, suggesting overconfidence and low confidence despite accurate responses indicates potential underestimation."
            },
            "score": 4
        },
        {
            "id": "c21bc84900ea40395173a23a5d3b5a8d2deefba5",
            "paperId": "c21bc84900ea40395173a23a5d3b5a8d2deefba5",
            "title": "Evaluating the Performance of Large Language Models on a Neurology Board-Style Examination",
            "abstract": "Summary Background and Objectives Recent advancements in large language models (LLMs) such as GPT-3.5 and GPT-4 have shown impressive potential in a wide array of applications, including healthcare. While GPT-3.5 and GPT-4 showed heterogeneous results across specialized medical board examinations, the performance of these models in neurology board exams remains unexplored. Methods An exploratory, prospective study was conducted between May 17 and May 31, 2023. The evaluation utilized a question bank approved by the American Board of Psychiatry and Neurology and were validated with a small question cohort by the European Board for Neurology. All questions were categorized into lower-order (recall, understanding) and higher-order (apply, analyze, synthesize) questions based on Bloom Taxonomy for learning and assessment. Performance was assessed in relation to overall scores, question type, and topics, along with the confidence level and reproducibility of answers. Results GPT-4 significantly outperformed GPT-3.5 by correctly answering 85% of 1956 questions compared to GPT-3.5 (66.8%) Notably, GPT-4's performance was above the average human score of 73.8%, effectively achieving near-passing and passing grades in the neurology board exam. GPT-4 outperformed human users in Behavioral, Cognitive and Psych-related questions and demonstrated superior performance to GPT-3.5 in six categories. Both models performed better on lower-order than higher-order questions (GPT4: 790 of 893 (88.5%) vs. 872 of 1063 (82%), GPT-3.5: 639 of 893 (71.6%) vs. 667 of 1063 (62.7%)) with GPT-4 also excelling in both lower-order and higher-order questions. Both models consistently used confident language, even when providing incorrect answers (GPT-4: 99.3%, 294 incorrect answers, GPT-3.5: 100%, 650 incorrect answers). Reproducible answers of GPT-3.5 and GPT-4 (more than 75 % same output across 50 independent queries) were associated with a higher percentage of correct answers (GPT-3.5: 66 of 88 (75%), GPT-4: 78 of 96 (81.3%)) than inconsistent answers, (GPT-3.5: 5 of 13 (38.5%), GPT-4: 1 of 4 (25%)). Discussion Despite the absence of neurology-specific training, GPT-4 demonstrated commendable performance, whereas GPT-3.5 performed slightly below the human average. While higher-order cognitive tasks were more challenging for both models, GPT-4's results were equivalent to passing grades in specialized neurology exams. These findings suggests that LLMs like GPT-4 could have significant applications in clinical neurology and healthcare with further refinements.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "GPT-4 outperformed human users in Behavioral, Cognitive and Psych-related questions and demonstrated superior performance to GPT-3.5 in six categories, suggesting that LLMs like G PT-4 could have significant applications in clinical neurology and healthcare with further refinements."
            },
            "score": 4
        },
        {
            "id": "7adb7dff0af51c0e6482e50c0f12fa9227bdea0f",
            "paperId": "7adb7dff0af51c0e6482e50c0f12fa9227bdea0f",
            "title": "CML: A Contrastive Meta Learning Method to Estimate Human Label Confidence Scores and Reduce Data Collection Cost",
            "abstract": "Deep neural network models are especially susceptible to noise in annotated labels. In the real world, annotated data typically contains noise caused by a variety of factors such as task difficulty, annotator experience, and annotator bias. Label quality is critical for label validation tasks; however, correcting for noise by collecting more data is often costly. In this paper, we propose a contrastive meta-learning framework (CML) to address the challenges introduced by noisy annotated data, specifically in the context of natural language processing. CML combines contrastive and meta learning to improve the quality of text feature representations. Meta-learning is also used to generate confidence scores to assess label quality. We demonstrate that a model built on CML-filtered data outperforms a model built on clean data. Furthermore, we perform experiments on deidentified commercial voice assistant datasets and demonstrate that our model outperforms several SOTA approaches.",
            "year": 2022,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A contrastive meta-learning framework (CML) is proposed to address the challenges introduced by noisy annotated data, specifically in the context of natural language processing and it is demonstrated that a model built on CML-filtered data outperforms a models built on clean data."
            },
            "score": 4
        },
        {
            "id": "4ab9f5c94ad003e774c0994011214a5721461cbe",
            "paperId": "4ab9f5c94ad003e774c0994011214a5721461cbe",
            "title": "Analyzing BERT\u2019s Knowledge of Hypernymy via Prompting",
            "abstract": "The high performance of large pretrained language models (LLMs) such as BERT on NLP tasks has prompted questions about BERT\u2019s linguistic capabilities, and how they differ from humans\u2019. In this paper, we approach this question by examining BERT\u2019s knowledge of lexical semantic relations. We focus on hypernymy, the \u201cis-a\u201d relation that relates a word to a superordinate category. We use a prompting methodology to simply ask BERT what the hypernym of a given word is. We find that, in a setting where all hypernyms are guessable via prompting, BERT knows hypernyms with up to 57% accuracy. Moreover, BERT with prompting outperforms other unsupervised models for hypernym discovery even in an unconstrained scenario. However, BERT\u2019s predictions and performance on a dataset containing uncommon hyponyms and hypernyms indicate that its knowledge of hypernymy is still limited.",
            "year": 2021,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper examines BERT\u2019s knowledge of lexical semantic relations, focusing on hypernymy, the \u201cis-a\u201d relation that relates a word to a superordinate category, and finds that, in a setting where all hypernyms are guessable via prompting, BERT knows hypernym with up to 57% accuracy."
            },
            "score": 4
        },
        {
            "id": "629c441076da3f8185b1cf85e8036064b714e249",
            "paperId": "629c441076da3f8185b1cf85e8036064b714e249",
            "title": "Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework",
            "abstract": "As large language models (LLMs) have become the norm in NLP, demonstrating good performance in generation and reasoning tasks, one of its most fatal disadvantages is the lack of factual correctness. Generating unfactual texts not only leads to lower performances but also degrades the trust and validity of their applications. Chain-of-Thought (CoT) prompting improves trust and model performance on complex reasoning tasks by generating interpretable reasoning chains, but still suffers from factuality concerns in knowledge-intensive tasks. In this paper, we propose the Verify-and-Edit framework for CoT prompting, which seeks to increase prediction factuality by post-editing reasoning chains according to external knowledge. Building on top of GPT-3, our framework lead to accuracy improvements in multiple open-domain question-answering tasks.",
            "year": 2023,
            "citationCount": 69,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Verify-and-Edit framework for CoT prompting is proposed, which seeks to increase prediction factuality by post-editing reasoning chains according to external knowledge and lead to accuracy improvements in multiple open-domain question-answering tasks."
            },
            "score": 4
        },
        {
            "id": "dc761368369156599f6534d8504283ea98cc9318",
            "paperId": "dc761368369156599f6534d8504283ea98cc9318",
            "title": "Calibrated Contrast-Consistent Search",
            "abstract": "Large language models (LLMs) are increasingly being deployed in novel areas ranging from healthcare to education. As these models become more consequential, it is vital to ensure they are safe and aligned with human values. Notably, one key desirable feature for LLMs is truthfulness, something that current LLMs do not always exhibit. The field of probing in NLP aims to discern what linguistic knowledge pre-trained language models encode within their hidden representations (Ivanova et al., 2021). One such probing algorithm, Contrast-Consistent Search (CCS), identifies representations of truth in models in an unsupervised manner by finding a direction in activation space that has logically consistent values for true and false statements (Burns et al., 2022). In this project, we aim to improve CCS and make it more interpretable by exploring calibration of predicted probabilities to better reflect true model confidence. We test several supervised and unsupervised approaches to this problem, including loss function modification (unsupervised) and post-hoc Platt scaling or isotonic regression (supervised). Both loss function modification and Platt scaling improve CCS accuracy and calibration on a sentiment analysis dataset using the deBERTa language model. Our results using loss function modification demonstrate the potential for more calibration-aligned loss functions that still yield similar accuracy. These contributions can make CCS more valuable as a tool for understanding what language models really know and how they represent that information.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Both loss function modification and Platt scaling improve CCS accuracy and calibration on a sentiment analysis dataset using the deBERTa language model and demonstrate the potential for more calibration-aligned loss functions that still yield similar accuracy."
            },
            "score": 4
        },
        {
            "id": "33727cfa2710e9f502480b7eb9ac1925cb3bc06b",
            "paperId": "33727cfa2710e9f502480b7eb9ac1925cb3bc06b",
            "title": "AutoTrial: Prompting Language Models for Clinical Trial Design",
            "abstract": "Clinical trials are critical for drug development. Constructing the appropriate eligibility criteria (i.e., the inclusion/exclusion criteria for patient recruitment) is essential for the trial's success. Proper design of clinical trial protocols should consider similar precedent trials and their eligibility criteria to ensure sufficient patient coverage. In this paper, we present a method named AutoTrial to aid the design of clinical eligibility criteria using language models. It allows (1) controllable generation under instructions via a hybrid of discrete and neural prompting, (2) scalable knowledge incorporation via in-context learning, and (3) explicit reasoning chains to provide rationales for understanding the outputs. Experiments on over 70K clinical trials verify that AutoTrial generates high-quality criteria texts that are fluent and coherent and with high accuracy in capturing the relevant clinical concepts to the target trial. It is noteworthy that our method, with a much smaller parameter size, gains around 60% winning rate against the GPT-3.5 baselines via human evaluations.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A method named AutoTrial is presented to aid the design of clinical eligibility criteria using language models that allows controllable generation under instructions via a hybrid of discrete and neural prompting, scalable knowledge incorporation via in-context learning, and explicit reasoning chains to provide rationales for understanding the outputs."
            },
            "score": 3
        },
        {
            "id": "eafc97d979e790a84329f0a49d1b627bd5979499",
            "paperId": "eafc97d979e790a84329f0a49d1b627bd5979499",
            "title": "Prompting Large Language Models with Chain-of-Thought for Few-Shot Knowledge Base Question Generation",
            "abstract": "The task of Question Generation over Knowledge Bases (KBQG) aims to convert a logical form into a natural language question. For the sake of expensive cost of large-scale question annotation, the methods of KBQG under low-resource scenarios urgently need to be developed. However, current methods heavily rely on annotated data for fine-tuning, which is not well-suited for few-shot question generation. The emergence of Large Language Models (LLMs) has shown their impressive generalization ability in few-shot tasks. Inspired by Chain-of-Thought (CoT) prompting, which is an in-context learning strategy for reasoning, we formulate KBQG task as a reasoning problem, where the generation of a complete question is splitted into a series of sub-question generation. Our proposed prompting method KQG-CoT first retrieves supportive logical forms from the unlabeled data pool taking account of the characteristics of the logical form. Then, we write a prompt to explicit the reasoning chain of generating complicated questions based on the selected demonstrations. To further ensure prompt quality, we extend KQG-CoT into KQG-CoT+ via sorting the logical forms by their complexity. We conduct extensive experiments over three public KBQG datasets. The results demonstrate that our prompting method consistently outperforms other prompting baselines on the evaluated datasets. Remarkably, our KQG-CoT+ method could surpass existing few-shot SoTA results of the PathQuestions dataset by 18.25, 10.72, and 10.18 absolute points on BLEU-4, METEOR, and ROUGE-L, respectively.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Inspired by Chain-of-Thought prompting, the proposed prompting method KQG-CoT first retrieves supportive logical forms from the unlabeled data pool taking account of the characteristics of the logical form, and could surpass existing few-shot SoTA results of the PathQuestions dataset."
            },
            "score": 3
        },
        {
            "id": "69f0c3a693d5f7f1512f2fcb4104692e4ae36184",
            "paperId": "69f0c3a693d5f7f1512f2fcb4104692e4ae36184",
            "title": "Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models",
            "abstract": "Clinical natural language processing requires methods that can address domain-specific challenges, such as complex medical terminology and clinical contexts. Recently, large language models (LLMs) have shown promise in this domain. Yet, their direct deployment can lead to privacy issues and are constrained by resources. To address this challenge, we delve into synthetic clinical text generation using LLMs for clinical NLP tasks. We propose an innovative, resource-efficient approach, ClinGen, which infuses knowledge into the process. Our model involves clinical knowledge extraction and context-informed LLM prompting. Both clinical topics and writing styles are drawn from external domain-specific knowledge graphs and LLMs to guide data generation. Our extensive empirical study across 7 clinical NLP tasks and 16 datasets reveals that ClinGen consistently enhances performance across various tasks, effectively aligning the distribution of real datasets and significantly enriching the diversity of generated training instances. We will publish our code and all the generated data in \\url{https://github.com/ritaranx/ClinGen}.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes an innovative, resource-efficient approach to synthetic clinical text generation using LLMs for clinical NLP tasks, ClinGen, which infuses knowledge into the process and consistently enhances performance across various tasks."
            },
            "score": 3
        },
        {
            "id": "9c00fd1e5c058c29608f7b1b71d0ebcf50a4ecf7",
            "paperId": "9c00fd1e5c058c29608f7b1b71d0ebcf50a4ecf7",
            "title": "Prompting Is All Your Need: Automated Android Bug Replay with Large Language Models",
            "abstract": "Bug reports are vital for software maintenance that allow users to inform developers of the problems encountered while using the software. As such, researchers have committed considerable resources toward automating bug replay to expedite the process of software maintenance. Nonetheless, the success of current automated approaches is largely dictated by the characteristics and quality of bug reports, as they are constrained by the limitations of manually-crafted patterns and pre-defined vocabulary lists. Inspired by the success of Large Language Models (LLMs) in natural language understanding, we propose AdbGPT , a new lightweight approach to automatically reproduce the bugs from bug reports through prompt engineering, without any training and hard-coding effort. AdbGPT leverages few-shot learning and chain-of-thought reasoning to elicit human knowledge and logical reasoning from LLMs to accomplish the bug replay in a manner similar to a developer. Our evaluations demonstrate the effectiveness and efficiency of our AdbGPT to reproduce 81.3% of bug reports in 253.6 seconds, outperforming the state-of-the-art baselines and ablation studies. We also conduct a small-scale user study to confirm the usefulness of AdbGPT in enhancing developers\u2019 bug replay capabilities.",
            "year": 2023,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "AdbGPT is proposed, a new lightweight approach to automatically reproduce the bugs from bug reports through prompt engineering, without any training and hard-coding effort, that leverages few-shot learning and chain-of-thought reasoning to elicit human knowledge and logical reasoning from LLMs to accomplish the bug replay in a manner similar to a developer."
            },
            "score": 3
        },
        {
            "id": "48385ded07af641da331c05f6ea3f93694a08425",
            "paperId": "48385ded07af641da331c05f6ea3f93694a08425",
            "title": "Prompting Is All You Need: Automated Android Bug Replay with Large Language Models",
            "abstract": "Bug reports are vital for software maintenance that allow users to inform developers of the problems encountered while using the software. As such, researchers have committed considerable resources toward automating bug replay to expedite the process of software maintenance. Nonetheless, the success of current automated approaches is largely dictated by the characteristics and quality of bug reports, as they are constrained by the limitations of manually-crafted patterns and pre-defined vocabulary lists. Inspired by the success of Large Language Models (LLMs) in natural language understanding, we propose AdbGPT, a new lightweight approach to automatically reproduce the bugs from bug reports through prompt engineering, without any training and hard-coding effort. AdbGPT leverages few-shot learning and chain-of-thought reasoning to elicit human knowledge and logical reasoning from LLMs to accomplish the bug replay in a manner similar to a developer. Our evaluations demonstrate the effectiveness and efficiency of our AdbGPT to reproduce 81.3% of bug reports in 253.6 seconds, outperforming the state-of-the-art baselines and ablation studies. We also conduct a small-scale user study to confirm the usefulness of AdbGPT in enhancing developers' bug replay capabilities.",
            "year": 2023,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "AdbGPT is proposed, a new lightweight approach to automatically reproduce the bugs from bug reports through prompt engineering, without any training and hard-coding effort, that leverages few-shot learning and chain-of-thought reasoning to elicit human knowledge and logical reasoning from LLMs to accomplish the bug replay in a manner similar to a developer."
            },
            "score": 3
        },
        {
            "id": "64ce6ef1f5cf227bf2bf917c87273386ae16256f",
            "paperId": "64ce6ef1f5cf227bf2bf917c87273386ae16256f",
            "title": "Dictionary-based Phrase-level Prompting of Large Language Models for Machine Translation",
            "abstract": "Large language models (LLMs) demonstrate remarkable machine translation (MT) abilities via prompting, even though they were not explicitly trained for this task. However, even given the incredible quantities of data they are trained on, LLMs can struggle to translate inputs with rare words, which are common in low resource or domain transfer scenarios. We show that LLM prompting can provide an effective solution for rare words as well, by using prior knowledge from bilingual dictionaries to provide control hints in the prompts. We propose a novel method, DiPMT, that provides a set of possible translations for a subset of the input words, thereby enabling fine-grained phrase-level prompted control of the LLM. Extensive experiments show that DiPMT outperforms the baseline both in low-resource MT, as well as for out-of-domain MT. We further provide a qualitative analysis of the benefits and limitations of this approach, including the overall level of controllability that is achieved.",
            "year": 2023,
            "citationCount": 30,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that LLM prompting can provide an effective solution for rare words as well, by using prior knowledge from bilingual dictionaries to provide control hints in the prompts, thereby enabling fine-grained phrase-level prompted control of the LLM."
            },
            "score": 3
        },
        {
            "id": "c5481668f78ab0c8ef2de9230f2fc1ce27eea6e4",
            "paperId": "c5481668f78ab0c8ef2de9230f2fc1ce27eea6e4",
            "title": "Towards Open-World Recommendation with Knowledge Augmentation from Large Language Models",
            "abstract": "Recommender systems play a vital role in various online services. However, the insulated nature of training and deploying separately within a specific domain limits their access to open-world knowledge. Recently, the emergence of large language models (LLMs) has shown promise in bridging this gap by encoding extensive world knowledge and demonstrating reasoning capability. Nevertheless, previous attempts to directly use LLMs as recommenders have not achieved satisfactory results. In this work, we propose an Open-World Knowledge Augmented Recommendation Framework with Large Language Models, dubbed KAR, to acquire two types of external knowledge from LLMs -- the reasoning knowledge on user preferences and the factual knowledge on items. We introduce factorization prompting to elicit accurate reasoning on user preferences. The generated reasoning and factual knowledge are effectively transformed and condensed into augmented vectors by a hybrid-expert adaptor in order to be compatible with the recommendation task. The obtained vectors can then be directly used to enhance the performance of any recommendation model. We also ensure efficient inference by preprocessing and prestoring the knowledge from the LLM. Extensive experiments show that KAR significantly outperforms the state-of-the-art baselines and is compatible with a wide range of recommendation algorithms. We deploy KAR to Huawei's news and music recommendation platforms and gain a 7\\% and 1.7\\% improvement in the online A/B test, respectively.",
            "year": 2023,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes an Open-World Knowledge Augmented Recommendation Framework with Large Language Models, dubbed KAR, to acquire two types of external knowledge from LLMs -- the reasoning knowledge on user preferences and the factual knowledge on items."
            },
            "score": 3
        },
        {
            "id": "6b4da2023c3a11aea6e3ccb9ab13e594833c47eb",
            "paperId": "6b4da2023c3a11aea6e3ccb9ab13e594833c47eb",
            "title": "Self-Refined Large Language Model as Automated Reward Function Designer for Deep Reinforcement Learning in Robotics",
            "abstract": "Although Deep Reinforcement Learning (DRL) has achieved notable success in numerous robotic applications, designing a high-performing reward function remains a challenging task that often requires substantial manual input. Recently, Large Language Models (LLMs) have been extensively adopted to address tasks demanding in-depth common-sense knowledge, such as reasoning and planning. Recognizing that reward function design is also inherently linked to such knowledge, LLM offers a promising potential in this context. Motivated by this, we propose in this work a novel LLM framework with a self-refinement mechanism for automated reward function design. The framework commences with the LLM formulating an initial reward function based on natural language inputs. Then, the performance of the reward function is assessed, and the results are presented back to the LLM for guiding its self-refinement process. We examine the performance of our proposed framework through a variety of continuous robotic control tasks across three diverse robotic systems. The results indicate that our LLM-designed reward functions are able to rival or even surpass manually designed reward functions, highlighting the efficacy and applicability of our approach.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel LLM framework with a self-refinement mechanism for automated reward function design that is able to rival or even surpass manually designed reward functions, highlighting the efficacy and applicability of this approach."
            },
            "score": 3
        },
        {
            "id": "1866cf8e89f5f3375ca6abc019bdf963bd486d56",
            "paperId": "1866cf8e89f5f3375ca6abc019bdf963bd486d56",
            "title": "ReasoningLM: Enabling Structural Subgraph Reasoning in Pre-trained Language Models for Question Answering over Knowledge Graph",
            "abstract": "Question Answering over Knowledge Graph (KGQA) aims to seek answer entities for the natural language question from a large-scale Knowledge Graph~(KG). To better perform reasoning on KG, recent work typically adopts a pre-trained language model~(PLM) to model the question, and a graph neural network~(GNN) based module to perform multi-hop reasoning on the KG. Despite the effectiveness, due to the divergence in model architecture, the PLM and GNN are not closely integrated, limiting the knowledge sharing and fine-grained feature interactions. To solve it, we aim to simplify the above two-module approach, and develop a more capable PLM that can directly support subgraph reasoning for KGQA, namely ReasoningLM. In our approach, we propose a subgraph-aware self-attention mechanism to imitate the GNN for performing structured reasoning, and also adopt an adaptation tuning strategy to adapt the model parameters with 20,000 subgraphs with synthesized questions. After adaptation, the PLM can be parameter-efficient fine-tuned on downstream tasks. Experiments show that ReasoningLM surpasses state-of-the-art models by a large margin, even with fewer updated parameters and less training data. Our codes and data are publicly available at~\\url{https://github.com/RUCAIBox/ReasoningLM}.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "In this work, a subgraph-aware self-attention mechanism to imitate the GNN for performing structured reasoning for KGQA is proposed, and an adaptation tuning strategy to adapt the model parameters with 20,000 subgraphs with synthesized questions is adopted."
            },
            "score": 3
        },
        {
            "id": "fd78047e4b169c8dd67879524275d0ae35903354",
            "paperId": "fd78047e4b169c8dd67879524275d0ae35903354",
            "title": "Knowledgeable Salient Span Mask for Enhancing Language Models as Knowledge Base",
            "abstract": null,
            "year": 2022,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work develops two solutions to help the model learn more knowledge from unstructured text in a fully self-supervised manner and is the first to explore fullySelf Supervised learning of knowledge in continual pre-training."
            },
            "score": 3
        },
        {
            "id": "2435c04832d486975304a094e55ecbab8acf8a5f",
            "paperId": "2435c04832d486975304a094e55ecbab8acf8a5f",
            "title": "Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders",
            "abstract": "Previous work has indicated that pretrained Masked Language Models (MLMs) are not effective as universal lexical and sentence encoders off-the-shelf, i.e., without further task-specific fine-tuning on NLI, sentence similarity, or paraphrasing tasks using annotated task data. In this work, we demonstrate that it is possible to turn MLMs into effective lexical and sentence encoders even without any additional data, relying simply on self-supervision. We propose an extremely simple, fast, and effective contrastive learning technique, termed Mirror-BERT, which converts MLMs (e.g., BERT and RoBERTa) into such encoders in 20-30 seconds with no access to additional external knowledge. Mirror-BERT relies on identical and slightly modified string pairs as positive (i.e., synonymous) fine-tuning examples, and aims to maximise their similarity during \u201cidentity fine-tuning\u201d. We report huge gains over off-the-shelf MLMs with Mirror-BERT both in lexical-level and in sentence-level tasks, across different domains and different languages. Notably, in sentence similarity (STS) and question-answer entailment (QNLI) tasks, our self-supervised Mirror-BERT model even matches the performance of the Sentence-BERT models from prior work which rely on annotated task data. Finally, we delve deeper into the inner workings of MLMs, and suggest some evidence on why this simple Mirror-BERT fine-tuning approach can yield effective universal lexical and sentence encoders.",
            "year": 2021,
            "citationCount": 94,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work demonstrates that it is possible to turn MLMs into effective lexical and sentence encoders even without any additional data, relying simply on self-supervision, and proposes an extremely simple, fast, and effective contrastive learning technique, termed Mirror-BERT."
            },
            "score": 3
        },
        {
            "id": "29de286404cc2561bfe22290d75bfdb9e8dc5e74",
            "paperId": "29de286404cc2561bfe22290d75bfdb9e8dc5e74",
            "title": "Increasing Context for Estimating Confidence Scores in Automatic Speech Recognition",
            "abstract": "Accurate confidence measures for predictions from machine learning techniques play a critical role in the deployment and training of many speech and language processing applications. For example, confidence scores are important when making use of automatically generated transcriptions in training automatic speech recognition (ASR) systems, as well as down-stream applications, such as information retrieval and conversational assistants. Previous work on improving confidence scores for these systems has focused on two main directions: designing features correlated with improved confidence prediction; and employing sequence models to account for the importance of contextual information. Few studies, however, have explored incorporating contextual information more broadly, such as from the future, in addition to the past, or making use of alternative multiple hypotheses in addition to the most likely one. This article introduces two general approaches for encapsulating contextual information from lattices. Experimental results illustrating the importance of increasing contextual information for estimating confidence scores are presented on a range of limited resource languages where word error rates range between 30% and 60%. The results show that the novel approaches provide significant gains in the accuracy of confidence estimation.",
            "year": 2022,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Two general approaches for encapsulating contextual information from lattices for estimating confidence scores are introduced and it is shown that the novel approaches provide significant gains in the accuracy of confidence estimation."
            },
            "score": 3
        },
        {
            "id": "99ad11764421bfff6261c9df8526c8b5af82d794",
            "paperId": "99ad11764421bfff6261c9df8526c8b5af82d794",
            "title": "Improving Logical Consistency in Pre-Trained Language Models using Natural Language Inference",
            "abstract": "Current state-of-the-art pre-trained language models (PTLMs) contain rich and vast amounts of world knowledge, demonstrating an ability to extrapolate information from contextual texts and to accurately answer questions [1]. However, the latent factual understanding captured by PTLMs can be irrational and inconsistent, causing PTLMs to be prone to generating contradictory statements [2]. We demonstrate that natural language inference (NLI) can provide additional signal about contradictory statements output by a PTLM. We explore several approaches for aggregating the entailment and contradiction probabilities acquired through NLI on a batch of PTLM predicted answers and define a scoring heuristic that balances between the NLI output and the PTLM\u2019s confidence in its answers. Predictions whose scores are below a tuned threshold are revised before outputting final answers. In addition, we investigate methods for using these NLI probabilities to define a MaxSAT problem that, when optimized, yields corrected predictions. Our results demonstrate that a system that uses either of our approaches to revise PTLM answers has better accuracy and logical consistency than a vanilla PTLM.",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that natural language inference (NLI) can provide additional signal about contradictory statements output by a PTLM and that a system that uses either of these approaches to revise PTLM answers has better accuracy and logical consistency than a vanilla PTLM."
            },
            "score": 3
        },
        {
            "id": "6d0585f49aa04c673cc13187e53e9f493b5466e4",
            "paperId": "6d0585f49aa04c673cc13187e53e9f493b5466e4",
            "title": "Improving Automatic Text Recognition with Language Models in the PyLaia Open-Source Library",
            "abstract": "PyLaia is one of the most popular open-source software for Automatic Text Recognition (ATR), delivering strong performance in terms of speed and accuracy. In this paper, we outline our recent contributions to the PyLaia library, focusing on the incorporation of reliable confidence scores and the integration of statistical language modeling during decoding. Our implementation provides an easy way to combine PyLaia with n-grams language models at different levels. One of the highlights of this work is that language models are completely auto-tuned: they can be built and used easily without any expert knowledge, and without requiring any additional data. To demonstrate the significance of our contribution, we evaluate PyLaia's performance on twelve datasets, both with and without language modelling. The results show that decoding with small language models improves the Word Error Rate by 13% and the Character Error Rate by 12% in average. Additionally, we conduct an analysis of confidence scores and highlight the importance of calibration techniques. Our implementation is publicly available in the official PyLaia repository at https://gitlab.teklia.com/atr/pylaia, and twelve open-source models are released on Hugging Face.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The implementation provides an easy way to combine PyLaia with n-grams language models at different levels, and shows that decoding with small language models improves the Word Error Rate and the Character Error Rate by 12% in average."
            },
            "score": 3
        },
        {
            "id": "03532123ccffae8d411264320e8a5ae2b6eddea0",
            "paperId": "03532123ccffae8d411264320e8a5ae2b6eddea0",
            "title": "Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP",
            "abstract": "Retrieval-augmented in-context learning has emerged as a powerful approach for addressing knowledge-intensive tasks using frozen language models (LM) and retrieval models (RM). Existing work has combined these in simple\"retrieve-then-read\"pipelines in which the RM retrieves passages that are inserted into the LM prompt. To begin to fully realize the potential of frozen LMs and RMs, we propose Demonstrate-Search-Predict (DSP), a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM. DSP can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions, systematically breaking down problems into small transformations that the LM and RM can handle more reliably. We have written novel DSP programs for answering questions in open-domain, multi-hop, and conversational settings, establishing in early evaluations new state-of-the-art in-context learning results and delivering 37-120%, 8-39%, and 80-290% relative gains against the vanilla LM (GPT-3.5), a standard retrieve-then-read pipeline, and a contemporaneous self-ask pipeline, respectively. We release DSP at https://github.com/stanfordnlp/dsp",
            "year": 2022,
            "citationCount": 133,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Demonstrate-Search-Predict (DSP) is proposed, a framework that relies on passing natural language texts in sophisticated pipelines between an LM and an RM and can express high-level programs that bootstrap pipeline-aware demonstrations, search for relevant passages, and generate grounded predictions."
            },
            "score": 3
        },
        {
            "id": "ff96527c03fbea7c3bb7d44d1d656d875ddba75e",
            "paperId": "ff96527c03fbea7c3bb7d44d1d656d875ddba75e",
            "title": "Dialogue for Prompting: a Policy-Gradient-Based Discrete Prompt Optimization for Few-shot Learning",
            "abstract": "Prompt-based pre-trained language models (PLMs) paradigm have succeeded substantially in few-shot natural language processing (NLP) tasks. However, prior discrete prompt optimization methods require expert knowledge to design the base prompt set and identify high-quality prompts, which is costly, inefficient, and subjective. Meanwhile, existing continuous prompt optimization methods improve the performance by learning the ideal prompts through the gradient information of PLMs, whose high computational cost, and low readability and generalizability are often concerning. To address the research gap, we propose a D ialogue-comprised P olicy-gradient-based D iscrete P rompt O ptimization (DP 2 O) method. We first design a multi-round dialogue alignment strategy for readability prompt set generation based on GPT-4. Furthermore, we propose an efficient prompt screening metric to identify high-quality prompts with linear complexity. Finally, we construct a reinforcement learning (RL) framework based on policy gradients to match the prompts to inputs optimally. By training a policy network with only 0.67% of the PLM parameter size on the tasks in the few-shot setting, DP 2 O outperforms the state-of-the-art (SOTA) method by 1.52% in accuracy on average on four open-source datasets. Moreover, subsequent experiments also demonstrate that DP 2 O has good universality, robustness and generalization ability.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A multi-round dialogue alignment strategy for readability prompt set generation based on GPT-4, an efficient prompt screening metric to identify high-quality prompts with linear complexity, and a reinforcement learning framework based on policy gradients to match the prompts to inputs optimally are proposed."
            },
            "score": 3
        },
        {
            "id": "a49e936075970e8ee574dfe73b9679b34354245e",
            "paperId": "a49e936075970e8ee574dfe73b9679b34354245e",
            "title": "TextGraphs-16 Natural Language Premise Selection Task: Zero-Shot Premise Selection with Prompting Generative Language Models",
            "abstract": "Automated theorem proving can benefit a lot from methods employed in natural language processing, knowledge graphs and information retrieval: this non-trivial task combines formal languages understanding, reasoning, similarity search. We tackle this task by enhancing semantic similarity ranking with prompt engineering, which has become a new paradigm in natural language understanding. None of our approaches requires additional training. Despite encouraging results reported by prompt engineering approaches for a range of NLP tasks, for the premise selection task vanilla re-ranking by prompting GPT-3 doesn\u2019t outperform semantic similarity ranking with SBERT, but merging of the both rankings shows better results.",
            "year": 2022,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work focuses on enhancing semantic similarity ranking with prompt engineering, which has become a new paradigm in natural language understanding, and introduces a new approach to premise selection task."
            },
            "score": 3
        },
        {
            "id": "fb408bafec0110964d06d742ef67e0aa4ebe81e1",
            "paperId": "fb408bafec0110964d06d742ef67e0aa4ebe81e1",
            "title": "Assessment of chemistry knowledge in large language models that generate code",
            "abstract": "In this work, we investigate the question: do code-generating large language models know chemistry? Our results indicate, mostly yes. To evaluate this, we introduce an expandable framework for evaluating chemistry knowledge in these models, through prompting models to solve chemistry problems posed as coding tasks. To do so, we produce a benchmark set of problems, and evaluate these models based on correctness of code by automated testing and evaluation by experts. We find that recent LLMs are able to write correct code across a variety of topics in chemistry and their accuracy can be increased by 30 percentage points via prompt engineering strategies, like putting copyright notices at the top of files. Our dataset and evaluation tools are open source which can be contributed to or built upon by future researchers, and will serve as a community resource for evaluating the performance of new models as they emerge. We also describe some good practices for employing LLMs in chemistry. The general success of these models demonstrates that their impact on chemistry teaching and research is poised to be enormous.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that recent LLMs are able to write correct code across a variety of topics in chemistry and their accuracy can be increased by 30 percentage points via prompt engineering strategies, like putting copyright notices at the top of files."
            },
            "score": 2
        },
        {
            "id": "9c576be5ac03ac313d3b839a5a665999098ebe0a",
            "paperId": "9c576be5ac03ac313d3b839a5a665999098ebe0a",
            "title": "Modelling of Uncertainties with Modelica",
            "abstract": "Abstract Many industrial applications, e.g. in power systems, need to use uncertain information (e.g. coming from sensors). The influence of uncertain measurements on the behavior of the system must be assessed, for safety reasons for instance. Also, by combining in-formation given by physical models and sensor measurements, the accuracy of the knowledge of the state of the system can be improved, leading to better plant monitoring and maintenance. Three well established techniques for handling un-certainties using physical models are presented: data reconciliation, propagation of uncertainties and in-terpolation techniques. Then, the requirements for handling these techniques in Modelica environments are given. They apply to the Modelica language it-self: how to specify the uncertainty problem to be solved directly in the Modelica model. They also apply to model processing: what are the pieces of information that must be automatically extracted from the model and provided to the standard algo-rithms that compute the uncertainties. Modelica language extensions in terms of two new pre-defined attributes,",
            "year": 2011,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "By combining in-formation given by physical models and sensor measurements, the accuracy of the knowledge of the state of the system can be improved, leading to better plant monitoring and maintenance."
            },
            "score": 2
        },
        {
            "id": "206847eb433fc0ddce27866bbf7b3482ae351c83",
            "paperId": "206847eb433fc0ddce27866bbf7b3482ae351c83",
            "title": "Investigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics",
            "abstract": "In this study, we investigate the capacity of large language models (LLMs), specifically GPT-3.5, to operationalise natural language descriptions of cooperative, competitive, altruistic, and self-interested behavior in social dilemmas. Our focus is on the iterated Prisoner's Dilemma, a classic example of a non-zero-sum interaction, but our broader research program encompasses a range of experimental economics scenarios, including the ultimatum game, dictator game, and public goods game. Using a within-subject experimental design, we instantiated LLM-generated agents with various prompts that conveyed different cooperative and competitive stances. We then assessed the agents' level of cooperation in the iterated Prisoner's Dilemma, taking into account their responsiveness to the cooperative or defection actions of their partners. Our results provide evidence that LLMs can translate natural language descriptions of altruism and selfishness into appropriate behaviour to some extent, but exhibit limitations in adapting their behavior based on conditioned reciprocity. The observed pattern of increased cooperation with defectors and decreased cooperation with cooperators highlights potential constraints in the LLM's ability to generalize its knowledge about human behavior in social dilemmas. We call upon the research community to further explore the factors contributing to the emergent behavior of LLM-generated agents in a wider array of social dilemmas, examining the impact of model architecture, training parameters, and various partner strategies on agent behavior. As more advanced LLMs like GPT-4 become available, it is crucial to investigate whether they exhibit similar limitations or are capable of more nuanced cooperative behaviors, ultimately fostering the development of AI systems that better align with human values and social norms.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study investigates the capacity of large language models, specifically GPT-3.5, to operationalise natural language descriptions of cooperative, competitive, altruistic, and self-interested behavior in social dilemmas, and provides evidence that LLMs can translate naturallanguage descriptions of altruism and selfishness into appropriate behaviour to some extent, but exhibit limitations in adapting their behavior based on conditioned reciprocity."
            },
            "score": 2
        },
        {
            "id": "a83cdcc0135c58fddf89fc72f1b92b7a9d1e170f",
            "paperId": "a83cdcc0135c58fddf89fc72f1b92b7a9d1e170f",
            "title": "LinkBERT: Pretraining Language Models with Document Links",
            "abstract": "Language model (LM) pretraining captures various knowledge from text corpora, helping downstream tasks. However, existing methods such as BERT model a single document, and do not capture dependencies or knowledge that span across documents. In this work, we propose LinkBERT, an LM pretraining method that leverages links between documents, e.g., hyperlinks. Given a text corpus, we view it as a graph of documents and create LM inputs by placing linked documents in the same context. We then pretrain the LM with two joint self-supervised objectives: masked language modeling and our new proposal, document relation prediction. We show that LinkBERT outperforms BERT on various downstream tasks across two domains: the general domain (pretrained on Wikipedia with hyperlinks) and biomedical domain (pretrained on PubMed with citation links). LinkBERT is especially effective for multi-hop reasoning and few-shot QA (+5% absolute improvement on HotpotQA and TriviaQA), and our biomedical LinkBERT sets new states of the art on various BioNLP tasks (+7% on BioASQ and USMLE). We release our pretrained models, LinkBERT and BioLinkBERT, as well as code and data.",
            "year": 2022,
            "citationCount": 205,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes LinkBERT, an LM pretraining method that leverages links between documents that outperforms BERT on various downstream tasks across two domains: the general domain (pretrained on Wikipedia with hyperlinks) and biomedical domain ( pretrained on PubMed with citation links)."
            },
            "score": 2
        },
        {
            "id": "fb3dc58cb17c54b997e6301cbde7773f77427833",
            "paperId": "fb3dc58cb17c54b997e6301cbde7773f77427833",
            "title": "Parameter-Efficient Domain Knowledge Integration from Multiple Sources for Biomedical Pre-trained Language Models",
            "abstract": "Domain-speci\ufb01c pre-trained language models (PLMs) have achieved great success over various downstream tasks in different domains. However, existing domain-speci\ufb01c PLMs mostly rely on self-supervised learning over large amounts of domain text, without explicitly integrating domain-speci\ufb01c knowledge, which can be essential in many domains. Moreover, in knowledge-sensitive ar-eas such as the biomedical domain, knowledge is stored in multiple sources and formats, and existing biomedical PLMs either neglect them or utilize them in a limited manner. In this work, we introduce an architecture to integrate domain knowledge from diverse sources into PLMs in a parameter-ef\ufb01cient way. More speci\ufb01cally, we propose to encode domain knowledge via adapters , which are small bottleneck feed-forward networks inserted between intermediate transformer layers in PLMs. These knowledge adapters are pre-trained for individual domain knowledge sources and integrated via an attention-based knowledge controller to enrich PLMs. Taking the biomedical domain as a case study, we explore three knowledge-speci\ufb01c adapters for PLMs based on the UMLS Metathesaurus graph, the Wikipedia articles for diseases, and the semantic grouping information for biomedical concepts. Extensive experiments on different biomedical NLP tasks and datasets demonstrate the bene\ufb01ts of the proposed architecture and the knowledge-speci\ufb01c adapters across multiple PLMs.",
            "year": 2021,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces an architecture to integrate domain knowledge from diverse sources into PLMs in a parameter-ef\ufb01cient way and proposes to encode domain knowledge via adapters, which are small bottleneck feed-forward networks inserted between intermediate transformer layers in PLMs."
            },
            "score": 2
        },
        {
            "id": "ad27059af0c92da5deb854874f7c9e89214b992a",
            "paperId": "ad27059af0c92da5deb854874f7c9e89214b992a",
            "title": "PHALM: Building a Knowledge Graph from Scratch by Prompting Humans and a Language Model",
            "abstract": "Despite the remarkable progress in natural language understanding with pretrained Transformers, neural language models often do not handle commonsense knowledge well. Toward commonsense-aware models, there have been attempts to obtain knowledge, ranging from automatic acquisition to crowdsourcing. However, it is difficult to obtain a high-quality knowledge base at a low cost, especially from scratch. In this paper, we propose PHALM, a method of building a knowledge graph from scratch, by prompting both crowdworkers and a large language model (LLM). We used this method to build a Japanese event knowledge graph and trained Japanese commonsense generation models. Experimental results revealed the acceptability of the built graph and inferences generated by the trained models. We also report the difference in prompting humans and an LLM. Our code, data, and models are available at github.com/nlp-waseda/comet-atomic-ja.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes PHALM, a method of building a knowledge graph from scratch, by prompting both crowdworkers and a large language model (LLM), and used this method to build a Japanese event knowledge graph and trained Japanese commonsense generation models."
            },
            "score": 2
        },
        {
            "id": "80c698688bb4488beaceaab5c64f701a946cb7ae",
            "paperId": "80c698688bb4488beaceaab5c64f701a946cb7ae",
            "title": "All in One: Multi-Task Prompting for Graph Neural Networks",
            "abstract": "Recently, \"pre-training and fine-tuning'' has been adopted as a standard workflow for many graph tasks since it can take general graph knowledge to relieve the lack of graph annotations from each application. However, graph tasks with node level, edge level, and graph level are far diversified, making the pre-training pretext often incompatible with these multiple tasks. This gap may even cause a \"negative transfer'' to the specific application, leading to poor results. Inspired by the prompt learning in natural language processing (NLP), which has presented significant effectiveness in leveraging prior knowledge for various NLP tasks, we study the prompting topic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models. Specifically, we first unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern. In this way, the prompting idea from NLP can be seamlessly introduced to the graph area. Then, to further narrow the gap between various graph tasks and state-of-the-art pre-training strategies, we further study the task space of various graph applications and reformulate downstream problems to the graph-level task. Afterward, we introduce meta-learning to efficiently learn a better initialization for the multi-task prompt of graphs so that our prompting framework can be more reliable and general for different tasks. We conduct extensive experiments, results from which demonstrate the superiority of our method.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel multi-task prompting method for graph models that unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern, and introduces meta-learning to efficiently learn a better initialization for the multi- task prompt of graphs so that the prompting framework can be more reliable and general for different tasks."
            },
            "score": 2
        },
        {
            "id": "44993ace721ffb93f3b78fe8eb9a082d9b6a6b42",
            "paperId": "44993ace721ffb93f3b78fe8eb9a082d9b6a6b42",
            "title": "Leveraging Large Language Models for Enhanced NLP Task Performance through Knowledge Distillation and Optimized Training Strategies",
            "abstract": "Emerging Large Language Models (LLMs) like GPT-4 have revolutionized Natural Language Processing (NLP), showing potential in traditional tasks such as Named Entity Recognition (NER). Our study explores a three-phase training strategy that harnesses GPT-4's capabilities to enhance the BERT model's performance on NER. Initially, GPT-4 annotates a subset of the CONLL2003 and additional BBC dataset without fine-tuning. We then train BERT using a mix of original and LLM-annotated data, analyzing the efficacy of LLM annotations against traditional methods. The second phase involves comparative experiments with different training regimens, assessing the synergy between distilled and original data. We observe that sequential strategies, particularly a simple mix of training first with distilled data followed by original data, significantly boost performance. In the third phase, we investigate various data blending techniques, including sigmoid and power decay functions, to optimize the training process further. Our results indicate that a strategic mix of distilled and original data markedly elevates the NER capabilities of BERT. Our approach presents a scalable methodology that reduces manual annotation costs and increases efficiency, making it especially pertinent in resource-limited and closed-network environments. The study concludes that while the 'Simple Mix' strategy yields the best results, understanding its underlying mechanisms requires further research. Future work will also focus on refining prompt designs and enhancing annotation selection processes, aiming to extend our methodology to diverse NLP tasks.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study explores a three-phase training strategy that harnesses GPT-4's capabilities to enhance the BERT model's performance on NER, and indicates that a strategic mix of distilled and original data markedly elevates the NER capabilities of BERT."
            },
            "score": 2
        },
        {
            "id": "3a22aad6c18a9559be3bbb197494b434b872a05a",
            "paperId": "3a22aad6c18a9559be3bbb197494b434b872a05a",
            "title": "Diffusion-NAT: Self-Prompting Discrete Diffusion for Non-Autoregressive Text Generation",
            "abstract": "Recently, continuous diffusion models (CDM) have been introduced into non-autoregressive (NAR) text-to-text generation. However, the discrete nature of text increases the difficulty of CDM to generate coherent and fluent texts, and also causes the incompatibility problem between CDM and advanced NLP techniques, especially the popular pre-trained language models (PLMs).To solve it, we propose Diffusion-NAT, which introduces discrete diffusion models (DDM) into NAR text-to-text generation and integrates BART to improve the performance.By revising the decoding process of BART and the typical settings of DDM, we unify the inference process of BART and the denoising process of DDM into the same NAR masked tokens recovering task.In this way, DDM can rely on BART to perform denoising, which can benefit from both the rich pre-learned knowledge of BART and the iterative refining paradigm of DDM.Besides, we also propose the iterative self-prompting strategy to further improve the generation quality.Experimental results on 7 datasets show that our approach can outperform competitive NAR methods, and even surpass autoregressive methods.Our code and data are released at https://github.com/RUCAIBox/DiffusionNAT.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Diffusion-NAT is proposed, which introduces discrete diffusion models (DDM) into NAR text-to-text generation and integrates BART to improve the performance and proposes the iterative self-prompting strategy to further improve the generation quality."
            },
            "score": 2
        },
        {
            "id": "28fb3c592174878889301d21442c3cf0795f151b",
            "paperId": "28fb3c592174878889301d21442c3cf0795f151b",
            "title": "Explicit Visual Prompting for Universal Foreground Segmentations",
            "abstract": "Foreground segmentation is a fundamental problem in computer vision, which includes salient object detection, forgery detection, defocus blur detection, shadow detection, and camouflage object detection. Previous works have typically relied on domain-specific solutions to address accuracy and robustness issues in those applications. In this paper, we present a unified framework for a number of foreground segmentation tasks without any task-specific designs. We take inspiration from the widely-used pre-training and then prompt tuning protocols in NLP and propose a new visual prompting model, named Explicit Visual Prompting (EVP). Different from the previous visual prompting which is typically a dataset-level implicit embedding, our key insight is to enforce the tunable parameters focusing on the explicit visual content from each individual image, i.e., the features from frozen patch embeddings and high-frequency components. Our method freezes a pre-trained model and then learns task-specific knowledge using a few extra parameters. Despite introducing only a small number of tunable parameters, EVP achieves superior performance than full fine-tuning and other parameter-efficient fine-tuning methods. Experiments in fourteen datasets across five tasks show the proposed method outperforms other task-specific methods while being considerably simple. The proposed method demonstrates the scalability in different architectures, pre-trained weights, and tasks. The code is available at: https://github.com/NiFangBaAGe/Explicit-Visual-Prompt.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper takes inspiration from the widely-used pre-training and then prompt tuning protocols in NLP and proposes a new visual prompting model, named Explicit Visual Prompting (EVP), which freezes a pre-trained model and then learns task-specific knowledge using a few extra parameters."
            },
            "score": 2
        },
        {
            "id": "c7bdb80bba981052d8ed6ae0a965b4886f36ebf9",
            "paperId": "c7bdb80bba981052d8ed6ae0a965b4886f36ebf9",
            "title": "Dialogue for Prompting: A Policy-Gradient-Based Discrete Prompt Generation for Few-Shot Learning",
            "abstract": "Prompt-based pre-trained language models (PLMs) paradigm has succeeded substantially in few-shot natural language processing (NLP) tasks. However, prior discrete prompt optimization methods require expert knowledge to design the base prompt set and identify high-quality prompts, which is costly, inefficient, and subjective. Meanwhile, existing continuous prompt optimization methods improve the performance by learning the ideal prompts through the gradient information of PLMs, whose high computational cost, and low readability and generalizability are often concerning. To address the research gap, we propose a Dialogue-comprised Policy-gradient-based Discrete Prompt Optimization (DP_2O) method. We first design a multi-round dialogue alignment strategy for readability prompt set generation based on GPT-4. Furthermore, we propose an efficient prompt screening metric to identify high-quality prompts with linear complexity. Finally, we construct a reinforcement learning (RL) framework based on policy gradients to match the prompts to inputs optimally. By training a policy network with only 0.62M parameters on the tasks in the few-shot setting, DP_2O outperforms the state-of-the-art (SOTA) method by 1.52% in accuracy on average on four open-source datasets. Moreover, subsequent experiments also demonstrate that DP_2O has good universality, robustness and generalization ability.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a Dialogue-comprised Policy-gradient-based Discrete Prompt Optimization (DP_2O) method, which outperforms the state-of-the-art (SOTA) method by 1.52% in accuracy on average on four open-source datasets and has good universality, robustness and generalization ability."
            },
            "score": 2
        },
        {
            "id": "fe2f7315e0855abb1ac7213e45b40c3eb8aa0ecb",
            "paperId": "fe2f7315e0855abb1ac7213e45b40c3eb8aa0ecb",
            "title": "NLP at UC Santa Cruz at SemEval-2024 Task 5: Legal Answer Validation using Few-Shot Multi-Choice QA",
            "abstract": "This paper presents our submission to the SemEval 2024 Task 5: The Legal Argument Reasoning Task in Civil Procedure. We present two approaches to solving the task of legal answer validation, given an introduction to the case, a question and an answer candidate. Firstly, we fine-tuned pre-trained BERT-based models and found that models trained on domain knowledge perform better. Secondly, we performed few-shot prompting on GPT models and found that reformulating the answer validation task to be a multiple-choice QA task remarkably improves the performance of the model. Our best submission is a BERT-based model that achieved the 7th place out of 20.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Two approaches to solving the task of legal answer validation are presented, given an introduction to the case, a question and an answer candidate, and it is found that reformulating the answer validation task to be a multiple-choice QA task remarkably improves the performance of the model."
            },
            "score": 2
        },
        {
            "id": "402576d3611182ebbae0c53910e8b1ad4e98bd61",
            "paperId": "402576d3611182ebbae0c53910e8b1ad4e98bd61",
            "title": "Discovering the Knowledge in Unstructured Early Drug Development Data Using NLP and Advanced Analytics",
            "abstract": "Discovering the knowledge in unstructured archived data is an area of active interest in academia and industry, as it offers opportunities to learn, confirm and eventually, address R&D productivity challenges. Our interest in this area prompted us to investigate an Natural Language Processing based approach to extract unstructured Pharmacokinetics (PK) and Pharmacodynamics (PD) data from PK-PD study reports, and perform analytics using in-house developed compartmental and non-compartmental analytics engines. For this purpose, we have developed a dictionary of two thousand twenty-one (2321) PK-PD keywords based on published study reports. Details of our approach and its applications in discovering the knowledge in the unstructured archived early drug development reports, is the subject of this paper.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An Natural Language Processing based approach to extract unstructured Pharmacokinetics (PK) and Pharmacodynamics (PD) data from PK-PD study reports, and perform analytics using in-house developed compartmental and non-compartmental analytics engines is investigated."
            },
            "score": 1
        }
    ],
    "novelty": "yes"
}