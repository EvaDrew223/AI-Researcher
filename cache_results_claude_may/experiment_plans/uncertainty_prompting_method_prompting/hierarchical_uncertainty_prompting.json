{
    "topic_description": "novel prompting methods that can better quantify uncertainty or calibrate the confidence of large language models",
    "idea_name": "Hierarchical Uncertainty Prompting",
    "raw_idea": {
        "Problem": "LLMs often struggle to provide granular and interpretable uncertainty estimates that distinguish between different sources or levels of uncertainty, such as data uncertainty, model uncertainty, and task-specific uncertainty.",
        "Existing Methods": "Existing methods for hierarchical uncertainty estimation in deep learning models include Bayesian hierarchical models, deep ensembles with different architectures, and probabilistic circuits. However, these methods are often computationally expensive and may not be directly applicable to prompting-based approaches.",
        "Motivation": "By designing prompts that elicit uncertainty estimates at different levels of granularity and abstraction, we can encourage LLMs to provide more interpretable and actionable uncertainty scores. Hierarchical prompting can also help to disentangle different sources of uncertainty and identify specific areas for improvement.",
        "Proposed Method": "We propose Hierarchical Uncertainty Prompting (HUP), a prompting method that elicits uncertainty estimates at multiple levels of granularity and abstraction. The key steps are: 1) Given an input query, generate a hierarchical set of subprompts that probe uncertainties at different levels (e.g., data-level, model-level, task-level). 2) For each subprompt, generate a set of clarification questions or assumptions to further probe the model's uncertainty (e.g., \"What additional information would help to reduce uncertainty in this aspect?\"). 3) Prompt the model to generate responses and uncertainty estimates for each subprompt and clarification question. 4) Aggregate the uncertainty estimates hierarchically to obtain calibrated scores at each level of abstraction. 5) Use the hierarchical uncertainty scores to identify specific areas for improvement or clarification.",
        "Experiment Plan": "Evaluate HUP on benchmark datasets for tasks that involve multiple levels of uncertainty, such as open-domain QA, multi-hop reasoning, and task-oriented dialogue. Compare against flat prompting baselines and hierarchical Bayesian models. Metrics include level-specific calibration errors, interpretability scores, and task performance at different uncertainty thresholds."
    },
    "full_experiment_plan": {
        "Title": "Hierarchical Uncertainty Prompting: Eliciting Granular and Interpretable Uncertainty Estimates from Large Language Models",
        "Problem Statement": "Large Language Models (LLMs) often struggle to provide granular and interpretable uncertainty estimates that distinguish between different sources or levels of uncertainty, such as data uncertainty, model uncertainty, and task-specific uncertainty.",
        "Motivation": "Existing methods for hierarchical uncertainty estimation in deep learning models, such as Bayesian hierarchical models, deep ensembles with different architectures, and probabilistic circuits, are often computationally expensive and may not be directly applicable to prompting-based approaches. By designing prompts that elicit uncertainty estimates at different levels of granularity and abstraction, we can encourage LLMs to provide more interpretable and actionable uncertainty scores. Hierarchical prompting can also help to disentangle different sources of uncertainty and identify specific areas for improvement.",
        "Proposed Method": "We propose Hierarchical Uncertainty Prompting (HUP), a prompting method that elicits uncertainty estimates at multiple levels of granularity and abstraction. The key steps are:\n1. Given an input query, generate a hierarchical set of subprompts that probe uncertainties at different levels (e.g., data-level, model-level, task-level).\n2. For each subprompt, generate a set of clarification questions or assumptions to further probe the model's uncertainty (e.g., \"What additional information would help to reduce uncertainty in this aspect?\").\n3. Prompt the model to generate responses and uncertainty estimates for each subprompt and clarification question.\n4. Aggregate the uncertainty estimates hierarchically to obtain calibrated scores at each level of abstraction.\n5. Use the hierarchical uncertainty scores to identify specific areas for improvement or clarification.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "Evaluate HUP on benchmark datasets for tasks that involve multiple levels of uncertainty, such as open-domain QA (Natural Questions, TriviaQA), multi-hop reasoning (HotpotQA, QAngaroo), and task-oriented dialogue (MultiWOZ, SGD). These datasets cover a range of uncertainty sources, including data ambiguity, knowledge gaps, and task complexity.",
            "Step 2: Construct Prompts": "1. Baseline prompts: (a) Direct prompting: Ask the question directly without any additional instructions. (b) Confidence prompting: Append \"How confident are you in your answer?\" to the question.\n2. HUP prompts: (a) Subprompt generation: Provide examples of how to break down a question into subprompts at different levels (e.g., \"What is the main source of uncertainty in answering this question: (1) Incomplete data, (2) Ambiguous question, or (3) Complex reasoning required?\"). (b) Clarification question generation: Provide examples of how to generate clarification questions for each subprompt (e.g., \"What additional information would help to reduce uncertainty due to incomplete data?\"). (c) Response generation: Prompt the model to generate answers and uncertainty estimates for each subprompt and clarification question.",
            "Step 3: Select Models": "Evaluate HUP on GPT-3.5 (text-davinci-002), GPT-4, and open-source models like BLOOM and LLaMA. Compare the performance of different models in generating granular and interpretable uncertainty estimates.",
            "Step 4: Get Results": "1. Generate model responses and uncertainty estimates using the baseline and HUP prompts for each dataset.\n2. Aggregate the uncertainty estimates hierarchically for HUP.\n3. Compute evaluation metrics: (a) Calibration error at each level of the hierarchy, (b) Interpretability scores based on human evaluation of the generated uncertainty explanations, and (c) Task performance at different uncertainty thresholds.",
            "Step 5: Analyze Results": "1. Compare the calibration errors and interpretability scores of HUP against the baseline methods. Assess whether HUP provides more granular and interpretable uncertainty estimates.\n2. Analyze the impact of using different uncertainty thresholds on task performance. Determine if HUP can effectively identify and filter out uncertain predictions.\n3. Conduct error analysis to identify common patterns or sources of uncertainty that are not well-captured by HUP."
        },
        "Test Case Examples": {
            "Baseline Prompt Input (Direct Prompting)": "What is the capital of France?",
            "Baseline Prompt Expected Output (Direct Prompting)": "Paris",
            "Baseline Prompt Input (Confidence Prompting)": "What is the capital of France? How confident are you in your answer?",
            "Baseline Prompt Expected Output (Confidence Prompting)": "Paris. I am very confident in my answer.",
            "Proposed Prompt Input (HUP Subprompt Generation)": "What is the capital of France?\nBreak down the question into subprompts that probe uncertainties at different levels:",
            "Proposed Prompt Expected Output (HUP Subprompt Generation)": "1. Data-level: Is there any ambiguity or incompleteness in the question that could lead to uncertainty?\n2. Model-level: How confident am I in my knowledge about the capital of France?\n3. Task-level: Is this a straightforward factoid question or does it require complex reasoning?",
            "Proposed Prompt Input (HUP Clarification Question Generation)": "For each subprompt, generate a clarification question that could help reduce uncertainty:",
            "Proposed Prompt Expected Output (HUP Clarification Question Generation)": "1. Data-level: Is there any additional context or time period specified for the capital of France?\n2. Model-level: What is the source of my knowledge about the capital of France and how reliable is it?\n3. Task-level: Is there any hidden complexity or ambiguity in the question that requires further clarification?",
            "Proposed Prompt Input (HUP Response Generation)": "Generate responses and uncertainty estimates for each subprompt and clarification question:",
            "Proposed Prompt Expected Output (HUP Response Generation)": "1. Data-level:\n- Subprompt: No ambiguity or incompleteness detected in the question. Uncertainty: Low\n- Clarification: No additional context or time period specified. Uncertainty: Low\n2. Model-level:\n- Subprompt: I am highly confident in my knowledge that Paris is the capital of France. Uncertainty: Low\n- Clarification: My knowledge comes from reliable sources such as encyclopedias and geography textbooks. Uncertainty: Low\n3. Task-level:\n- Subprompt: This is a straightforward factoid question that does not require complex reasoning. Uncertainty: Low\n- Clarification: No hidden complexity or ambiguity detected in the question. Uncertainty: Low\nFinal Answer: Paris\nAggregate Uncertainty: Low",
            "Explanation": "The proposed HUP method generates granular uncertainty estimates at multiple levels (data, model, task) and provides interpretable explanations for the sources of uncertainty. In this test case, HUP identifies that there is low uncertainty across all levels, leading to a high-confidence prediction. In contrast, the baseline methods only provide a single, coarse-grained uncertainty estimate."
        },
        "Fallback Plan": "If the proposed HUP method does not outperform the baselines in terms of calibration error or interpretability scores, consider the following fallback plans:\n1. Analyze the generated subprompts and clarification questions to identify potential issues, such as irrelevant or redundant questions. Refine the prompts to generate more targeted and informative questions.\n2. Experiment with different uncertainty aggregation methods, such as weighted averaging or probabilistic models, to better combine the uncertainty estimates from different levels.\n3. Collect human annotations for the sources and levels of uncertainty in a subset of the test examples. Use these annotations to fine-tune the prompts and improve the generation of subprompts and clarification questions.\n4. Conduct a detailed error analysis to identify the limitations of HUP and potential areas for improvement. Focus on cases where HUP performs poorly compared to the baselines and try to understand the underlying reasons.\n5. If HUP consistently underperforms the baselines, consider simplifying the approach by reducing the number of hierarchical levels or using a different uncertainty estimation method, such as Monte Carlo dropout or ensemble methods.\n6. If the proposed method fails to provide meaningful insights or improvements, pivot the project to an analysis of the challenges and limitations of eliciting granular uncertainty estimates from LLMs using prompting methods. Discuss potential alternative approaches and future research directions."
    },
    "novelty_queries": [
        "KeywordQuery(\"hierarchical uncertainty estimation language models\")",
        "KeywordQuery(\"prompting granular uncertainty language models\")",
        "KeywordQuery(\"interpretable uncertainty estimates language models\")",
        "KeywordQuery(\"Hierarchical Uncertainty Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "7adb88771376c2a31688e3b0395b0550a35b824d",
            "paperId": "7adb88771376c2a31688e3b0395b0550a35b824d",
            "title": "Uncertainty Decomposition and Quantification for In-Context Learning of Large Language Models",
            "abstract": "In-context learning has emerged as a ground-breaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM\u2019s response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM\u2019s response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model\u2019s configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-context learning in a plug-and-play fashion. Extensive experiments are conducted to demonstrate the effectiveness of the decomposition. The code and data are available at: https://github. com/lingchen0331/UQ_ICL .",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work dives into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations and ambiguities tied to the model\u2019s configurations (epistemic uncertainty)."
            },
            "score": 8,
            "novelty_score": "The project proposal aims to develop a hierarchical prompting method to elicit granular and interpretable uncertainty estimates from large language models, while the paper focuses on decomposing and quantifying the aleatoric and epistemic uncertainties associated with in-context learning in large language models.\n\nProject proposal: Develop a hierarchical prompting method to elicit granular and interpretable uncertainty estimates from large language models.\nPaper: Decompose and quantify the aleatoric and epistemic uncertainties associated with in-context learning in large language models.\n\nAlthough both works deal with uncertainty estimation in large language models, the project proposal focuses on developing a prompting method to elicit granular uncertainty estimates, while the paper aims to decompose and quantify different types of uncertainties in in-context learning. The approaches and specific research problems are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "be8c90bca14d59f180f40a41126b7cd8c29c5d4e",
            "paperId": "be8c90bca14d59f180f40a41126b7cd8c29c5d4e",
            "title": "Uncertainty Quantification for In-Context Learning of Large Language Models",
            "abstract": "In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM's response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM's response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model's configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-context learning in a plug-and-play fashion. Extensive experiments are conducted to demonstrate the effectiveness of the decomposition. The code and data are available at: https://github.com/lingchen0331/UQ_ICL.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations and ambiguities tied to the model's configurations (epistemic uncertainty)."
            },
            "score": 8,
            "novelty_score": "The project proposal aims to develop a prompting method called Hierarchical Uncertainty Prompting (HUP) to elicit granular and interpretable uncertainty estimates from large language models at multiple levels of abstraction. The paper focuses on quantifying the predictive uncertainty of LLMs associated with in-context learning, considering both aleatoric uncertainty from the provided demonstrations and epistemic uncertainty from the model's configurations.\n\nProject proposal: Develop a prompting method to elicit granular and interpretable uncertainty estimates from LLMs at multiple levels of abstraction.\nPaper: Quantify the predictive uncertainty of LLMs in in-context learning, considering both aleatoric and epistemic uncertainty.\n\nWhile both works address uncertainty estimation in LLMs, the project proposal focuses on developing a prompting method to elicit uncertainty estimates, while the paper proposes a formulation and estimation method to quantify uncertainties in in-context learning.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "ad402080a4aa66ef3c57a46ce4685a47a3cc0a61",
            "paperId": "ad402080a4aa66ef3c57a46ce4685a47a3cc0a61",
            "title": "Quantifying Uncertainty in Natural Language Explanations of Large Language Models",
            "abstract": "Large Language Models (LLMs) are increasingly used as powerful tools for several high-stakes natural language processing (NLP) applications. Recent prompting works claim to elicit intermediate reasoning steps and key tokens that serve as proxy explanations for LLM predictions. However, there is no certainty whether these explanations are reliable and reflect the LLMs behavior. In this work, we make one of the first attempts at quantifying the uncertainty in explanations of LLMs. To this end, we propose two novel metrics -- $\\textit{Verbalized Uncertainty}$ and $\\textit{Probing Uncertainty}$ -- to quantify the uncertainty of generated explanations. While verbalized uncertainty involves prompting the LLM to express its confidence in its explanations, probing uncertainty leverages sample and model perturbations as a means to quantify the uncertainty. Our empirical analysis of benchmark datasets reveals that verbalized uncertainty is not a reliable estimate of explanation confidence. Further, we show that the probing uncertainty estimates are correlated with the faithfulness of an explanation, with lower uncertainty corresponding to explanations with higher faithfulness. Our study provides insights into the challenges and opportunities of quantifying uncertainty in LLM explanations, contributing to the broader discussion of the trustworthiness of foundation models.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes two novel metrics -- verbalized uncertainty and probing uncertainty -- to quantify the uncertainty of generated explanations of large Language Models, and shows that the probing uncertainty estimates are correlated with the faithfulness of an explanation, with lower uncertainty corresponding to explanations with higher faithfulness."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is eliciting granular and interpretable uncertainty estimates from LLMs using hierarchical prompting. The approach involves generating subprompts at different levels of granularity and aggregating the uncertainty estimates.\n\nThe research problem in the paper is quantifying the uncertainty in natural language explanations of LLMs. The approach involves using verbalized uncertainty and probing uncertainty metrics based on prompting and perturbations.\n\nWhile both works focus on uncertainty estimation in LLMs, the proposal specifically targets eliciting granular uncertainty estimates through hierarchical prompting, whereas the paper aims to quantify the uncertainty of generated explanations using different metrics. The methods and goals are distinct.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "d1500f1dbd62e26ef0753f31e845078f58479968",
            "paperId": "d1500f1dbd62e26ef0753f31e845078f58479968",
            "title": "Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners",
            "abstract": "Large language models (LLMs) exhibit a wide range of promising capabilities -- from step-by-step planning to commonsense reasoning -- that may provide utility for robots, but remain prone to confidently hallucinated predictions. In this work, we present KnowNo, which is a framework for measuring and aligning the uncertainty of LLM-based planners such that they know when they don't know and ask for help when needed. KnowNo builds on the theory of conformal prediction to provide statistical guarantees on task completion while minimizing human help in complex multi-step planning settings. Experiments across a variety of simulated and real robot setups that involve tasks with different modes of ambiguity (e.g., from spatial to numeric uncertainties, from human preferences to Winograd schemas) show that KnowNo performs favorably over modern baselines (which may involve ensembles or extensive prompt tuning) in terms of improving efficiency and autonomy, while providing formal assurances. KnowNo can be used with LLMs out of the box without model-finetuning, and suggests a promising lightweight approach to modeling uncertainty that can complement and scale with the growing capabilities of foundation models. Website: https://robot-help.github.io",
            "year": 2023,
            "citationCount": 91,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents KnowNo, which is a framework for measuring and aligning the uncertainty of LLM-based planners such that they know when they don't know and ask for help when needed, and suggests a promising lightweight approach to modeling uncertainty that can complement and scale with the growing capabilities of foundation models."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to develop a prompting method called Hierarchical Uncertainty Prompting (HUP) to elicit granular and interpretable uncertainty estimates from large language models at multiple levels of abstraction. The paper, on the other hand, proposes a framework called KnowNo that measures and aligns the uncertainty of LLM-based planners to enable them to ask for help when needed, while providing statistical guarantees on task completion.\n\nAlthough both works deal with uncertainty estimation in large language models, the project proposal focuses on a prompting method for eliciting hierarchical uncertainty estimates, while the paper proposes a framework for aligning uncertainty in LLM-based planners to improve efficiency and autonomy in robot tasks.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "67fa2f2072cca1071ed2c820d6a7f50de6ea2ff3",
            "paperId": "67fa2f2072cca1071ed2c820d6a7f50de6ea2ff3",
            "title": "Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling",
            "abstract": "Uncertainty decomposition refers to the task of decomposing the total uncertainty of a model into data (aleatoric) uncertainty, resulting from the inherent complexity or ambiguity of the data, and model (epistemic) uncertainty, resulting from the lack of knowledge in the model. Performing uncertainty decomposition for large language models (LLMs) is an important step toward improving the reliability, trustworthiness, and interpretability of LLMs, but this research task is very challenging and remains unresolved. The existing canonical method, Bayesian Neural Network (BNN), cannot be applied to LLMs, because BNN requires training and ensembling multiple variants of models, which is infeasible or prohibitively expensive for LLMs. In this paper, we introduce an uncertainty decomposition framework for LLMs, called input clarifications ensemble, which bypasses the need to train new models. Rather than ensembling models with different parameters, our approach generates a set of clarifications for the input, feeds them into the fixed LLMs, and ensembles the corresponding predictions. We show that our framework shares a symmetric decomposition structure with BNN. Empirical evaluations demonstrate that the proposed framework provides accurate and reliable uncertainty quantification on various tasks. Code will be made publicly available at https://github.com/UCSB-NLP-Chang/llm_uncertainty .",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces an uncertainty decomposition framework for LLMs, called input clarifications ensemble, which bypasses the need to train new models, and shares a symmetric decomposition structure with BNN."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is eliciting granular and interpretable uncertainty estimates from LLMs using hierarchical prompting. The approach is to generate subprompts and clarification questions at different levels of granularity to probe uncertainties.\n\nThe research problem in the paper is decomposing the total uncertainty of LLMs into data uncertainty and model uncertainty. The approach is to generate input clarifications, feed them into fixed LLMs, and ensemble the predictions.\n\nWhile both works aim to improve the interpretability and reliability of LLM uncertainty estimates, the proposal focuses on eliciting granular uncertainties through hierarchical prompting, while the paper focuses on decomposing uncertainty into data and model components through input clarification ensembling. The methods are quite different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "5424e311319c58847b4c690d5c91090e3b6a4ac3",
            "paperId": "5424e311319c58847b4c690d5c91090e3b6a4ac3",
            "title": "Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models",
            "abstract": "While Large Language Models (LLMs) have demonstrated remarkable potential in natural language generation and instruction following, a persistent challenge lies in their susceptibility to\"hallucinations\", which erodes trust in their outputs. Although Uncertainty Quantification (UQ) presents a promising solution, its accurate implementation within the context of LLMs remains a significant hurdle. To address this critical roadblock, our research originates from a fundamental heuristic insight: tokens within auto-regressive LLM-generated text do not equally reflect the underlying meaning. Some tokens carry greater relevance and representativeness than others, owing to the phenomenon of\"linguistic redundancy\", wherein a select few keywords suffice to convey the essence of lengthy sentences. Regrettably, existing methodologies treat all tokens with equal importance when estimating uncertainty, disregarding these inherent generative inequalities. Our analysis reveals a significant issue with state-of-the-art: numerous tokens (and sentences) of limited semantic significance receive equal or even excessive weighting during uncertainty estimation. To rectify this bias, we propose to jointly Shifting Attention to more Relevant (SAR) components, at both the token- and the sentence-levels for accurate uncertainty estimation. We conduct extensive experiments involving a range of popular\"off-the-shelf\"LLMs, including instruction-tuned LLMs such as Vicuna, WizardLM, and LLaMA-2-chat, as well as pretrained LLMs like OPT and LLaMA, with model sizes extending up to 33B parameters. We carry out evaluation across various free-form question-answering tasks, encompassing domains such as reading comprehension, science Q&A, and medical Q&A. Our experimental results demonstrate the superior performance of SAR in addressing the challenges of uncertainty estimation within the realm of LLMs.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results demonstrate the superior performance of SAR in addressing the challenges of uncertainty estimation within the realm of LLMs, and propose to jointly Shifting Attention to more Relevant (SAR) components, at both the token- and the sentence-levels for accurate uncertainty estimation."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is eliciting granular and interpretable uncertainty estimates from large language models using hierarchical prompting. The approach involves generating subprompts at different levels of granularity and abstraction, along with clarification questions, to probe the model's uncertainty.\n\nThe research problem in the paper is improving the accuracy of uncertainty estimation in large language models by focusing on more relevant components at the token and sentence levels. The approach involves shifting attention to more semantically significant tokens and sentences during uncertainty estimation.\n\nWhile both works address the challenge of uncertainty estimation in large language models, the proposal focuses on eliciting granular and interpretable uncertainty estimates through hierarchical prompting, while the paper aims to improve the accuracy of uncertainty estimation by focusing on more relevant components. The approaches differ in their methods and the specific aspects of uncertainty estimation they target.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "507465f8d46489a68a527cb5304d76bdb6c31ed9",
            "paperId": "507465f8d46489a68a527cb5304d76bdb6c31ed9",
            "title": "Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation",
            "abstract": "We introduce a method to measure uncertainty in large language models. For tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models. We show that measuring uncertainty in natural language is challenging because of\"semantic equivalence\"-- different sentences can mean the same thing. To overcome these challenges we introduce semantic entropy -- an entropy which incorporates linguistic invariances created by shared meanings. Our method is unsupervised, uses only a single model, and requires no modifications to off-the-shelf language models. In comprehensive ablation studies we show that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines.",
            "year": 2023,
            "citationCount": 85,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "In comprehensive ablation studies, it is shown that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to develop a prompting method called Hierarchical Uncertainty Prompting (HUP) to elicit granular and interpretable uncertainty estimates from large language models at multiple levels of abstraction.\n\nThe paper introduces a method called semantic entropy to measure uncertainty in large language models for tasks like question answering, which incorporates linguistic invariances created by shared meanings.\n\nWhile both works focus on estimating uncertainty in language models, the project proposal targets eliciting granular uncertainty estimates through hierarchical prompting, while the paper proposes an unsupervised method to measure uncertainty by considering semantic equivalence. The approaches and problem formulations are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "ea0d41514a41f8273f13b3b277e7fcbbc65a8549",
            "paperId": "ea0d41514a41f8273f13b3b277e7fcbbc65a8549",
            "title": "Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models",
            "abstract": "The recent performance leap of Large Language Models (LLMs) opens up new opportunities across numerous industrial applications and domains. However, erroneous generations, such as false predictions, misinformation, and hallucination made by LLMs, have also raised severe concerns for the trustworthiness of LLMs', especially in safety-, security- and reliability-sensitive scenarios, potentially hindering real-world adoptions. While uncertainty estimation has shown its potential for interpreting the prediction risks made by general machine learning (ML) models, little is known about whether and to what extent it can help explore an LLM's capabilities and counteract its undesired behavior. To bridge the gap, in this paper, we initiate an exploratory study on the risk assessment of LLMs from the lens of uncertainty. In particular, we experiment with twelve uncertainty estimation methods and four LLMs on four prominent natural language processing (NLP) tasks to investigate to what extent uncertainty estimation techniques could help characterize the prediction risks of LLMs. Our findings validate the effectiveness of uncertainty estimation for revealing LLMs' uncertain/non-factual predictions. In addition to general NLP tasks, we extensively conduct experiments with four LLMs for code generation on two datasets. We find that uncertainty estimation can potentially uncover buggy programs generated by LLMs. Insights from our study shed light on future design and development for reliable LLMs, facilitating further research toward enhancing the trustworthiness of LLMs.",
            "year": 2023,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An exploratory study on the risk assessment of LLMs from the lens of uncertainty is initiated, finding that uncertainty estimation can potentially uncover buggy programs generated by LLMs."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is eliciting granular and interpretable uncertainty estimates from large language models using hierarchical prompting. The approach involves generating subprompts at different levels of granularity and abstraction, asking clarification questions, and aggregating the uncertainty estimates hierarchically.\n\nThe research problem in the paper is investigating the effectiveness of uncertainty estimation methods for assessing the prediction risks of large language models. The approach involves experimenting with twelve uncertainty estimation methods and four LLMs on four NLP tasks and code generation datasets.\n\nWhile both the proposal and the paper focus on uncertainty estimation for large language models, the specific research problems and approaches differ. The proposal aims to develop a novel hierarchical prompting method for eliciting granular uncertainty estimates, while the paper explores the effectiveness of existing uncertainty estimation techniques for risk assessment.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "6d3ae6d6b312b659b3a14ae3f3e86a36db63200d",
            "paperId": "6d3ae6d6b312b659b3a14ae3f3e86a36db63200d",
            "title": "Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning",
            "abstract": "Step-by-step decision planning with large language models (LLMs) is gaining attention in AI agent development. This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models. Existing approaches are either white-box or computationally demanding, limiting use of black-box proprietary LLMs within budgets. The paper's first contribution is a non-parametric uncertainty quantification method for LLMs, efficiently estimating point-wise dependencies between input-decision on the fly with a single inference, without access to token logits. This estimator informs the statistical interpretation of decision trustworthiness. The second contribution outlines a systematic design for a decision-making agent, generating actions like ``turn on the bathroom light'' based on user prompts such as ``take a bath''. Users will be asked to provide preferences when more than one action has high estimated point-wise dependencies. In conclusion, our uncertainty estimation and decision-making agent design offer a cost-efficient approach for AI agent development.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models, and outlines a systematic design for a decision-making agent, offering a cost-efficient approach for AI agent development."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to develop a prompting method called Hierarchical Uncertainty Prompting (HUP) to elicit granular and interpretable uncertainty estimates from large language models at multiple levels of abstraction. The paper focuses on efficient non-parametric uncertainty quantification for black-box large language models in the context of decision planning.\n\nWhile both works deal with uncertainty estimation in large language models, the project proposal specifically targets eliciting granular and interpretable uncertainty estimates through hierarchical prompting, whereas the paper proposes a non-parametric method for estimating point-wise dependencies between input and decision for decision planning tasks.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "ba63e1ab5b6e9d849982ae293ac0483053badaff",
            "paperId": "ba63e1ab5b6e9d849982ae293ac0483053badaff",
            "title": "Uncertainty in Language Models: Assessment through Rank-Calibration",
            "abstract": "Language Models (LMs) have shown promising performance in natural language generation. However, as LMs often generate incorrect or hallucinated responses, it is crucial to correctly quantify their uncertainty in responding to given inputs. In addition to verbalized confidence elicited via prompting, many uncertainty measures ($e.g.$, semantic entropy and affinity-graph-based measures) have been proposed. However, these measures can differ greatly, and it is unclear how to compare them, partly because they take values over different ranges ($e.g.$, $[0,\\infty)$ or $[0,1]$). In this work, we address this issue by developing a novel and practical framework, termed $Rank$-$Calibration$, to assess uncertainty and confidence measures for LMs. Our key tenet is that higher uncertainty (or lower confidence) should imply lower generation quality, on average. Rank-calibration quantifies deviations from this ideal relationship in a principled manner, without requiring ad hoc binary thresholding of the correctness score ($e.g.$, ROUGE or METEOR). The broad applicability and the granular interpretability of our methods are demonstrated empirically.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel and practical framework, termed $Rank$-$Calibration$ is developed, to assess uncertainty and confidence measures for LMs, with the key tenet that higher uncertainty should imply lower generation quality, on average."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to develop a hierarchical prompting method to elicit granular and interpretable uncertainty estimates from large language models. The paper proposes a rank-calibration framework to assess and compare different uncertainty measures for language models.\n\nThe project focuses on generating uncertainty estimates at different levels of granularity through prompting, while the paper focuses on evaluating and comparing existing uncertainty measures. Although both works deal with uncertainty estimation in language models, their research problems and approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "217e436fd23fe4184828e02a2b143835d6fd3b28",
            "paperId": "217e436fd23fe4184828e02a2b143835d6fd3b28",
            "title": "Navigating the Grey Area: How Expressions of Uncertainty and Overconfidence Affect Language Models",
            "abstract": "The increased deployment of LMs for real-world tasks involving knowledge and facts makes it important to understand model epistemology: what LMs think they know, and how their attitudes toward that knowledge are affected by language use in their inputs. Here, we study an aspect of model epistemology: how epistemic markers of certainty, uncertainty, or evidentiality like\"I'm sure it's\",\"I think it's\", or\"Wikipedia says it's\"affect models, and whether they contribute to model failures. We develop a typology of epistemic markers and inject 50 markers into prompts for question answering. We find that LMs are highly sensitive to epistemic markers in prompts, with accuracies varying more than 80%. Surprisingly, we find that expressions of high certainty result in a 7% decrease in accuracy as compared to low certainty expressions; similarly, factive verbs hurt performance, while evidentials benefit performance. Our analysis of a popular pretraining dataset shows that these markers of uncertainty are associated with answers on question-answering websites, while markers of certainty are associated with questions. These associations may suggest that the behavior of LMs is based on mimicking observed language use, rather than truly reflecting epistemic uncertainty.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that LMs are highly sensitive to epistemic markers in prompts, with accuracies varying more than 80%, and expressions of high certainty result in a 7% decrease in accuracy as compared to low certainty expressions; similarly, factive verbs hurt performance, while evidentials benefit performance."
            },
            "score": 6
        },
        {
            "id": "0a51afdcd7cf4f33987d766082a7d3f174936c8a",
            "paperId": "0a51afdcd7cf4f33987d766082a7d3f174936c8a",
            "title": "Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models",
            "abstract": "In the face of uncertainty, the ability to seek information is of fundamental importance. In many practical applications, such as medical diagnosis and troubleshooting, the information needed to solve the task is not initially given, and has to be actively sought by asking follow-up questions (for example, a doctor asking a patient for more details about their symptoms). In this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment large language models with the ability to actively seek information by asking effective questions. UoT combines 1) an uncertainty-aware simulation approach which enables the model to simulate possible future scenarios and how likely they are to occur, 2) uncertainty-based rewards motivated by information gain which incentivizes the model to seek information, and 3) a reward propagation scheme to select the optimal question to ask in a way that maximizes the expected reward. In experiments on medical diagnosis, troubleshooting and the '20 Questions' game, UoT achieves an average performance improvement of 57.8% in the rate of successful task completion across multiple LLMs compared with direct prompting, and also improves efficiency (i.e., the number of questions needed to complete the task).",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Uncertainty of Thoughts is introduced, an algorithm to augment large language models with the ability to actively seek information by asking effective questions and achieves an average performance improvement of 57.8% in the rate of successful task completion across multiple LLMs compared with direct prompting."
            },
            "score": 6
        },
        {
            "id": "e1bc150d5d9e745a4920881c414ac9df0ea024a3",
            "paperId": "e1bc150d5d9e745a4920881c414ac9df0ea024a3",
            "title": "ChatGPT Prompting Cannot Estimate Predictive Uncertainty in High-Resource Languages",
            "abstract": "ChatGPT took the world by storm for its impressive abilities. Due to its release without documentation, scientists immediately attempted to identify its limits, mainly through its performance in natural language processing (NLP) tasks. This paper aims to join the growing literature regarding ChatGPT's abilities by focusing on its performance in high-resource languages and on its capacity to predict its answers' accuracy by giving a confidence level. The analysis of high-resource languages is of interest as studies have shown that low-resource languages perform worse than English in NLP tasks, but no study so far has analysed whether high-resource languages perform as well as English. The analysis of ChatGPT's confidence calibration has not been carried out before either and is critical to learn about ChatGPT's trustworthiness. In order to study these two aspects, five high-resource languages and two NLP tasks were chosen. ChatGPT was asked to perform both tasks in the five languages and to give a numerical confidence value for each answer. The results show that all the selected high-resource languages perform similarly and that ChatGPT does not have a good confidence calibration, often being overconfident and never giving low confidence values.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper aims to join the growing literature regarding ChatGPT's abilities by focusing on its performance in high-resource languages and on its capacity to predict its answers' accuracy by giving a confidence level."
            },
            "score": 6
        },
        {
            "id": "fab362b9b7ccc292de915f1811417b82524ac76c",
            "paperId": "fab362b9b7ccc292de915f1811417b82524ac76c",
            "title": "Hierarchical Uncertainty Estimation for Medical Image Segmentation Networks",
            "abstract": "Learning a medical image segmentation model is an inherently ambiguous task, as uncertainties exist in both images (noise) and manual annotations (human errors and bias) used for model training. To build a trustworthy image segmentation model, it is important to not just evaluate its performance but also estimate the uncertainty of the model prediction. Most state-of-the-art image segmentation networks adopt a hierarchical encoder architecture, extracting image features at multiple resolution levels from fine to coarse. In this work, we leverage this hierarchical image representation and propose a simple yet effective method for estimating uncertainties at multiple levels. The multi-level uncertainties are modelled via the skip-connection module and then sampled to generate an uncertainty map for the predicted image segmentation. We demonstrate that a deep learning segmentation network such as U-net, when implemented with such hierarchical uncertainty estimation module, can achieve a high segmentation performance, while at the same time provide meaningful uncertainty maps that can be used for out-of-distribution detection.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that a deep learning segmentation network such as U-net, when implemented with such hierarchical uncertainty estimation module, can achieve a high segmentation performance, while at the same time provide meaningful uncertainty maps that can be used for out-of-distribution detection."
            },
            "score": 5
        },
        {
            "id": "3f7fae7ace36c562158c2e9b24f02d824f4a203c",
            "paperId": "3f7fae7ace36c562158c2e9b24f02d824f4a203c",
            "title": "Transformer Uncertainty Estimation with Hierarchical Stochastic Attention",
            "abstract": "Transformers are state-of-the-art in a wide range of NLP tasks and have also been applied to many real-world products. Understanding the reliability and certainty of transformer models is crucial for building trustable machine learning applications, e.g., medical diagnosis. Although many recent transformer extensions have been proposed, the study of the uncertainty estimation of transformer models is under-explored. In this work, we propose a novel way to enable transformers to have the capability of uncertainty estimation and, meanwhile, retain the original predictive performance. This is achieved by learning hierarchical stochastic self-attention that attends to values and a set of learnable centroids, respectively. Then new attention heads are formed with a mixture of sampled centroids using the Gumbel-Softmax trick. We theoretically show that the self-attention approximation by sampling from a Gumbel distribution is upper bounded. We empirically evaluate our model on two text classification tasks with both in-domain (ID) and out-of-domain (OOD) datasets.\n The experimental results demonstrate that our approach: (1) achieves the best predictive-uncertainty trade-off among compared methods; (2) exhibits very competitive (in most cases, better) predictive performance on ID datasets; (3) is on par with Monte Carlo dropout and ensemble methods in uncertainty estimation on OOD datasets.",
            "year": 2021,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel way to enable transformers to have the capability of uncertainty estimation and, meanwhile, retain the original predictive performance by learning hierarchical stochastic self-attention that attends to values and a set of learnable centroids."
            },
            "score": 5
        },
        {
            "id": "4e15901eaaaa9a9c2c30f64e05054ce6f5cdaa97",
            "paperId": "4e15901eaaaa9a9c2c30f64e05054ce6f5cdaa97",
            "title": "On the Importance of Uncertainty in Decision-Making with Large Language Models",
            "abstract": "We investigate the role of uncertainty in decision-making problems with natural language as input. For such tasks, using Large Language Models as agents has become the norm. However, none of the recent approaches employ any additional phase for estimating the uncertainty the agent has about the world during the decision-making task. We focus on a fundamental decision-making framework with natural language as input, which is the one of contextual bandits, where the context information consists of text. As a representative of the approaches with no uncertainty estimation, we consider an LLM bandit with a greedy policy, which picks the action corresponding to the largest predicted reward. We compare this baseline to LLM bandits that make active use of uncertainty estimation by integrating the uncertainty in a Thompson Sampling policy. We employ different techniques for uncertainty estimation, such as Laplace Approximation, Dropout, and Epinets. We empirically show on real-world data that the greedy policy performs worse than the Thompson Sampling policies. These findings suggest that, while overlooked in the LLM literature, uncertainty plays a fundamental role in bandit tasks with LLMs.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work considers an LLM bandit with a greedy policy, which picks the action corresponding to the largest predicted reward, compared to LLM bandits that make active use of uncertainty estimation by integrating the uncertainty in a Thompson Sampling policy."
            },
            "score": 5
        },
        {
            "id": "1fa4469e5bc5d096572902fe14b0d66078a24c47",
            "paperId": "1fa4469e5bc5d096572902fe14b0d66078a24c47",
            "title": "Navigating the Grey Area: Expressions of Overconfidence and Uncertainty in Language Models",
            "abstract": "Despite increasingly \ufb02uent, relevant, and coherent language generation, major gaps remain between how humans and machines use language. We argue that a key dimension that is missing from our understanding of language models (LMs) is the model\u2019s ability to interpret and generate expressions of uncertainty . Whether it be the weatherperson announcing a chance of rain or a doctor giving a diagnosis, information is often not black-and-white and expressions of uncertainty provide nuance to support human-decision making. The increasing deployment of LMs in the wild motivates us to investigate whether LMs are capable of interpreting expressions of uncertainty and how LMs\u2019 behaviors change when learning to emit their own expressions of uncertainty. When injecting expressions of uncertainty into prompts (e.g., \"I think the answer is...\"), we discover that GPT3\u2019s generations vary upwards of 80% in accuracy based on the expression used. We analyze the linguistic characteristics of these expressions and \ufb01nd a drop in accuracy when naturalistic expressions of certainty are present. We \ufb01nd similar effects when teaching models to emit their own expressions of uncertainty, where model calibration suffers when teaching models to emit certainty rather than un certainty. Together, these results highlight the challenges of building LMs that interpret and generate trustworthy expressions of uncertainty.",
            "year": 2023,
            "citationCount": 54,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that GPT3\u2019s generations vary upwards of 80% in accuracy based on the expression used, and the challenges of building LMs that interpret and generate trustworthy expressions of uncertainty are highlighted."
            },
            "score": 5
        },
        {
            "id": "b1ec3002f4c80d721fc7d975cf469dce0833fed0",
            "paperId": "b1ec3002f4c80d721fc7d975cf469dce0833fed0",
            "title": "DeLLMa: A Framework for Decision Making Under Uncertainty with Large Language Models",
            "abstract": "Large language models (LLMs) are increasingly used across society, including in domains like business, engineering, and medicine. These fields often grapple with decision-making under uncertainty, a critical yet challenging task. In this paper, we show that directly prompting LLMs on these types of decision-making problems yields poor results, especially as the problem complexity increases. To overcome this limitation, we propose DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments. DeLLMa involves a multi-step scaffolding procedure, drawing upon principles from decision theory and utility theory, to provide an optimal and human-auditable decision-making process. We validate our framework on decision-making environments involving real agriculture and finance data. Our results show that DeLLMa can significantly improve LLM decision-making performance, achieving up to a 40% increase in accuracy over competing methods.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments, is proposed and validated on decision-making environments involving real agriculture and finance data."
            },
            "score": 5
        },
        {
            "id": "17bcb1edbe068e8fe6a97da552c70a77a15bbce7",
            "paperId": "17bcb1edbe068e8fe6a97da552c70a77a15bbce7",
            "title": "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned",
            "abstract": "We describe our early efforts to red team language models in order to simultaneously discover, measure, and attempt to reduce their potentially harmful outputs. We make three main contributions. First, we investigate scaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B parameters) and 4 model types: a plain language model (LM); an LM prompted to be helpful, honest, and harmless; an LM with rejection sampling; and a model trained to be helpful and harmless using reinforcement learning from human feedback (RLHF). We find that the RLHF models are increasingly difficult to red team as they scale, and we find a flat trend with scale for the other model types. Second, we release our dataset of 38,961 red team attacks for others to analyze and learn from. We provide our own analysis of the data and find a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs. Third, we exhaustively describe our instructions, processes, statistical methodologies, and uncertainty about red teaming. We hope that this transparency accelerates our ability to work together as a community in order to develop shared norms, practices, and technical standards for how to red team language models.",
            "year": 2022,
            "citationCount": 236,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that the RLHF models are increasingly difficult to red team as they scale, and a flat trend with scale for the other model types is found."
            },
            "score": 5
        },
        {
            "id": "f2d0f9309a4ca6e9d712f72778a9bcf083ace077",
            "paperId": "f2d0f9309a4ca6e9d712f72778a9bcf083ace077",
            "title": "Uncertainty estimation in deep learning with application to spoken language assessment",
            "abstract": "Since convolutional neural networks (CNNs) achieved top performance on the ImageNet task in 2012, deep learning has become the preferred approach to addressing computer vision, natural language processing, speech recognition and bio-informatics tasks. However, despite impressive performance, neural networks tend to make over-confident predictions. Thus, it is necessary to investigate robust, interpretable and tractable estimates of uncertainty in a model\u2019s predictions in order to construct safer Machine Learning systems. This is crucial to applications where the cost of an error is high, such as in autonomous vehicle control, high-stakes automatic proficiency assessment and in the medical, financial and legal fields. In the first part of this thesis uncertainty estimation via ensemble and single-model approaches is discussed in detail and a new class of models for uncertainty estimation, called Prior Networks, is proposed. Prior Networks are able to emulate an ensemble of models using a single deterministic neural network, which allows sources of uncertainty to be determined within the same probabilistic framework as in ensemble-based approaches, but with the computational simplicity and ease of training of single-model approaches. Thus, Prior Networks combine the advantages of ensemble and single-model approaches to estimating uncertainty. In this thesis Prior Networks are evaluated on a range classification datasets, where they are shown to outperform baseline approaches, such as Monte-Carlo dropout, on the task of detecting out-of-distribution inputs. In the second part of this thesis deep learning and uncertainty estimation approaches are applied to the area of automatic assessment of non-native spoken language proficiency. Specifically deep-learning based graders and spoken response relevance assessment systems are constructed using data from the BULATS and LinguaSkill exams, provided by Cambridge English Language Assessment. Baseline approaches for uncertainty estimation discussed and evaluated in the first half of the thesis are then applied to these models and assessed on the task of rejecting predictions to be graded by human examiners and detecting misclassifications.",
            "year": 2019,
            "citationCount": 63,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Prior Networks combine the advantages of ensemble and single-model approaches to estimating uncertainty and are evaluated on a range classification datasets, where they are shown to outperform baseline approaches on the task of detecting out-of-distribution inputs."
            },
            "score": 5
        },
        {
            "id": "0aa5940fda7c994675d08c41eca2a6909eb6d205",
            "paperId": "0aa5940fda7c994675d08c41eca2a6909eb6d205",
            "title": "Improving the Reliability of Large Language Models by Leveraging Uncertainty-Aware In-Context Learning",
            "abstract": "In recent years, large-scale language models (LLMs) have gained attention for their impressive text generation capabilities. However, these models often face the challenge of\"hallucination,\"which undermines their reliability. In this study, we introduce an uncertainty-aware in-context learning framework to empower the model to enhance or reject its output in response to uncertainty. Human-defined methods for estimating uncertainty typically assume that\"uncertainty is lower when the model's response is correct compared to when it is incorrect.\"However, setting a precise threshold to distinguish correctness is challenging. Therefore, we introduce uncertainty information as an intermediary variable that implicitly influences the model's behavior. Our innovative uncertainty-aware in-context learning framework involves fine-tuning the LLM using a calibration dataset. Our aim is to improve the model's responses by filtering out answers with high uncertainty while considering the model's knowledge limitations. We evaluate the model's knowledge by examining multiple responses to the same question for the presence of a correct answer. When the model lacks relevant knowledge, the response should indicate that the question cannot be answered. Conversely, when the model has relevant knowledge, the response should provide the correct answer. Extensive experiments confirm the effectiveness of our framework, leading to two key findings. First, the logit output values of the LLM partly reflect inherent uncertainty. Second, our model autonomously recognizes uncertainty, resulting in improved responses.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study introduces an uncertainty-aware in-context learning framework to empower the model to enhance or reject its output in response to uncertainty, and introduces uncertainty information as an intermediary variable that implicitly influences the model's behavior."
            },
            "score": 5
        },
        {
            "id": "740e777ff049102081673bec1adb48ba3169fdba",
            "paperId": "740e777ff049102081673bec1adb48ba3169fdba",
            "title": "Interpretable Natural Language Understanding",
            "abstract": "In recent years, we have witnessed the shift of paradigms in Natural Language Processing (NLP) from fine-tuning large-scale pre-trained language models (PLMs) on task-specific data to prompt-based learning. In the latter, the task description is embedded into the PLM input, enabling the same model to handle multiple tasks. While both approaches have demonstrated impressive performance in various NLP tasks, their opaque nature makes comprehending their inner workings and decision-making processes challenging for humans. In this talk, I will share the research undertaken in my group to address the interpretability concerns surrounding neural models in language understanding. This includes a hierarchical interpretable text classifier going beyond word-level interpretations, uncertainty interpretation of text classifiers built on PLMs, explainable recommender systems by harnessing information across diverse modalities, and explainable student answer scoring. I will conclude my talk by offering insights into potential future developments in interpretable language understanding.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This talk will share the research undertaken in the group to address the interpretability concerns surrounding neural models in language understanding, including a hierarchical interpretable text classifier going beyond word-level interpretations, uncertainty interpretation of text classifiers built on PLMs, explainable recommender systems by harnessing information across diverse modalities, and explainable student answer scoring."
            },
            "score": 5
        },
        {
            "id": "444f3b7293b85b7d37600372941a289f9163abd1",
            "paperId": "444f3b7293b85b7d37600372941a289f9163abd1",
            "title": "LM-Polygraph: Uncertainty Estimation for Language Models",
            "abstract": "Recent advancements in the capabilities of large language models (LLMs) have paved the way for a myriad of groundbreaking applications in various fields. However, a significant challenge arises as these models often\"hallucinate\", i.e., fabricate facts without providing users an apparent means to discern the veracity of their statements. Uncertainty estimation (UE) methods are one path to safer, more responsible, and more effective use of LLMs. However, to date, research on UE methods for LLMs has been focused primarily on theoretical rather than engineering contributions. In this work, we tackle this issue by introducing LM-Polygraph, a framework with implementations of a battery of state-of-the-art UE methods for LLMs in text generation tasks, with unified program interfaces in Python. Additionally, it introduces an extendable benchmark for consistent evaluation of UE techniques by researchers, and a demo web application that enriches the standard chat dialog with confidence scores, empowering end-users to discern unreliable responses. LM-Polygraph is compatible with the most recent LLMs, including BLOOMz, LLaMA-2, ChatGPT, and GPT-4, and is designed to support future releases of similarly-styled LMs.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LM-Polygraph is introduced, a framework with implementations of a battery of state-of-the-art UE methods for LLMs in text generation tasks, with unified program interfaces in Python, and introduces an extendable benchmark for consistent evaluation of UE techniques by researchers."
            },
            "score": 4
        },
        {
            "id": "acbe813244e07f32eb034d6c27547d772a995d1d",
            "paperId": "acbe813244e07f32eb034d6c27547d772a995d1d",
            "title": "Uncertainty Estimation for Language Reward Models",
            "abstract": "Language models can learn a range of capabilities from unsupervised training on text corpora. However, to solve a particular problem (such as text summarization) it is typically necessary to fine-tune them on a task-specific dataset. It is often easier for humans to choose between options than to provide labeled data, and prior work has achieved state-of-the-art performance by training a reward model from such preference comparisons. However, collecting a large preference comparison dataset is still expensive -- and the learned reward models are unreliable out-of-distribution. We seek to address these problems via uncertainty estimation, which can improve sample efficiency and robustness using active learning and risk-averse reinforcement learning (RL). Specifically, we use bootstrap aggregating (bagging) to train an ensemble of reward models differing in the initialization of their final layer. Ensembles have proved successful in prior applications of active learning, but we find that in our setting ensemble active learning does not outperform random sampling. Further experiments show that while the aggregate predictions are well-calibrated, the ensemble's estimated epistemic uncertainty is only weakly correlated with model error. We suspect this is because the ensemble members are fine-tuned from a single model and so are similar to one another. This suggests current pre-training methods will need to be modified to support uncertainty estimation, e.g. by training multiple language models.",
            "year": 2022,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that in this setting ensemble active learning does not outperform random sampling, and current pre-training methods will need to be modified to support uncertainty estimation, e.g. by training multiple language models."
            },
            "score": 4
        },
        {
            "id": "5e7274bcda47b704b6797bb14be8b7a61c047a61",
            "paperId": "5e7274bcda47b704b6797bb14be8b7a61c047a61",
            "title": "Uncertainty-Aware Evaluation for Vision-Language Models",
            "abstract": "Vision-Language Models like GPT-4, LLaVA, and CogVLM have surged in popularity recently due to their impressive performance in several vision-language tasks. Current evaluation methods, however, overlook an essential component: uncertainty, which is crucial for a comprehensive assessment of VLMs. Addressing this oversight, we present a benchmark incorporating uncertainty quantification into evaluating VLMs. Our analysis spans 20+ VLMs, focusing on the multiple-choice Visual Question Answering (VQA) task. We examine models on 5 datasets that evaluate various vision-language capabilities. Using conformal prediction as an uncertainty estimation approach, we demonstrate that the models' uncertainty is not aligned with their accuracy. Specifically, we show that models with the highest accuracy may also have the highest uncertainty, which confirms the importance of measuring it for VLMs. Our empirical findings also reveal a correlation between model uncertainty and its language model part.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that models with the highest accuracy may also have the highest uncertainty, which confirms the importance of measuring it for VLMs, and a correlation between model uncertainty and its language model part is revealed."
            },
            "score": 4
        },
        {
            "id": "645d8c40f2a05f0b06f9338cf7635755532d747c",
            "paperId": "645d8c40f2a05f0b06f9338cf7635755532d747c",
            "title": "Uncertainty Awareness of Large Language Models Under Code Distribution Shifts: A Benchmark Study",
            "abstract": "Large Language Models (LLMs) have been widely employed in programming language analysis to enhance human productivity. Yet, their reliability can be compromised by various code distribution shifts, leading to inconsistent outputs. While probabilistic methods are known to mitigate such impact through uncertainty calibration and estimation, their efficacy in the language domain remains underexplored compared to their application in image-based tasks. In this work, we first introduce a large-scale benchmark dataset, incorporating three realistic patterns of code distribution shifts at varying intensities. Then we thoroughly investigate state-of-the-art probabilistic methods applied to CodeLlama using these shifted code snippets. We observe that these methods generally improve the uncertainty awareness of CodeLlama, with increased calibration quality and higher uncertainty estimation~(UE) precision. However, our study further reveals varied performance dynamics across different criteria (e.g., calibration error vs misclassification detection) and trade-off between efficacy and efficiency, highlighting necessary methodological selection tailored to specific contexts.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work thoroughly investigate state-of-the-art probabilistic methods applied to CodeLlama using three realistic patterns of code distribution shifts at varying intensities, and observes that these methods generally improve the uncertainty awareness of CodeLlama, with increased calibration quality and higher uncertainty estimation~(UE) precision."
            },
            "score": 4
        },
        {
            "id": "c76541024ed59403f99a5a73ba69849112959a6e",
            "paperId": "c76541024ed59403f99a5a73ba69849112959a6e",
            "title": "A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models",
            "abstract": "The tendency of Large Language Models to generate hallucinations and exhibit overconfidence in predictions raises concerns regarding their reliability. Confidence or uncertainty estimations indicating the extent of trustworthiness of a model's response are essential to developing reliable AI systems. Current research primarily focuses on LLM confidence estimations in English, remaining a void for other widely used languages and impeding the global development of reliable AI applications. This paper introduces a comprehensive investigation of Multi-lingual confidence estimation (MlingConf) on LLMs. First, we introduce an elaborated and expert-checked multilingual QA dataset. Second, we delve into the performance of confidence estimations and examine how these confidence scores can enhance LLM performance through self-refinement across diverse languages. Finally, we propose a cross-lingual confidence estimation method to achieve more precise confidence scores. The experimental results showcase the performance of various confidence estimation methods across different languages as well as present that our proposed cross-lingual confidence estimation technique significantly enhances confidence estimation and outperforms several baseline methods.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A comprehensive investigation of Multi-lingual confidence estimation (MlingConf) on LLMs is introduced, an elaborated and expert-checked multilingual QA dataset is introduced, and a cross-lingual confidence estimation method is proposed to achieve more precise confidence scores."
            },
            "score": 4
        },
        {
            "id": "72fb75f7c38a83424308c8205bb36cd88995494b",
            "paperId": "72fb75f7c38a83424308c8205bb36cd88995494b",
            "title": "Leveraging Large Language Models for Exploiting ASR Uncertainty",
            "abstract": "While large language models excel in a variety of natural language processing (NLP) tasks, to perform well on spoken language understanding (SLU) tasks, they must either rely on off-the-shelf automatic speech recognition (ASR) systems for transcription, or be equipped with an in-built speech modality. This work focuses on the former scenario, where LLM's accuracy on SLU tasks is constrained by the accuracy of a fixed ASR system on the spoken input. Specifically, we tackle speech-intent classification task, where a high word-error-rate can limit the LLM's ability to understand the spoken intent. Instead of chasing a high accuracy by designing complex or specialized architectures regardless of deployment costs, we seek to answer how far we can go without substantially changing the underlying ASR and LLM, which can potentially be shared by multiple unrelated tasks. To this end, we propose prompting the LLM with an n-best list of ASR hypotheses instead of only the error-prone 1-best hypothesis. We explore prompt-engineering to explain the concept of n-best lists to the LLM; followed by the finetuning of Low-Rank Adapters on the downstream tasks. Our approach using n-best lists proves to be effective on a device-directed speech detection task as well as on a keyword spotting task, where systems using n-best list prompts outperform those using 1-best ASR hypothesis; thus paving the way for an efficient method to exploit ASR uncertainty via LLMs for speech-based applications.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work tackles speech-intent classification task, where a high word-error-rate can limit the LLM's ability to understand the spoken intent, and proposes prompting theLLM with an n-best list of ASR hypotheses instead of only the error-prone 1-best hypothesis."
            },
            "score": 4
        },
        {
            "id": "3fc3460c4554a28e489a0ea6ef067b79b7d301d9",
            "paperId": "3fc3460c4554a28e489a0ea6ef067b79b7d301d9",
            "title": "Active Prompting with Chain-of-Thought for Large Language Models",
            "abstract": "The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks requiring reasoning, such as arithmetic and commonsense reasoning. It is known that the effective design of task-specific prompts is critical for LLMs' ability to produce high-quality answers. In particular, an effective approach for complex question-and-answer tasks is example-based prompting with chain-of-thought (CoT) reasoning, which significantly improves the performance of LLMs. However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks. This paper proposes a new method, Active-Prompt, to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning). For this purpose, we propose a solution to the key problem of determining which questions are the most important and helpful ones to annotate from a pool of task-specific queries. By borrowing ideas from the related problem of uncertainty-based active learning, we introduce several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation. Experimental results demonstrate the superiority of our proposed method, achieving state-of-the-art on eight complex reasoning tasks. Further analyses of different uncertainty metrics, pool sizes, zero-shot learning, and accuracy-uncertainty relationship demonstrate the effectiveness of our method. Our code will be available at https://github.com/shizhediao/active-prompt.",
            "year": 2023,
            "citationCount": 58,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new method to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning), and introduces several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation."
            },
            "score": 4
        },
        {
            "id": "4ade9cb9f75236679029b6fd60d71c3bf513eedb",
            "paperId": "4ade9cb9f75236679029b6fd60d71c3bf513eedb",
            "title": "CoF-CoT: Enhancing Large Language Models with Coarse-to-Fine Chain-of-Thought Prompting for Multi-domain NLU Tasks",
            "abstract": "While Chain-of-Thought prompting is popular in reasoning tasks, its application to Large Language Models (LLMs) in Natural Language Understanding (NLU) is under-explored. Motivated by multi-step reasoning of LLMs, we propose Coarse-to-Fine Chain-of-Thought (CoF-CoT) approach that breaks down NLU tasks into multiple reasoning steps where LLMs can learn to acquire and leverage essential concepts to solve tasks from different granularities. Moreover, we propose leveraging semantic-based Abstract Meaning Representation (AMR) structured knowledge as an intermediate step to capture the nuances and diverse structures of utterances, and to understand connections between their varying levels of granularity. Our proposed approach is demonstrated effective in assisting the LLMs adapt to the multi-grained NLU tasks under both zero-shot and few-shot multi-domain settings.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes Coarse-to-Fine Chain-of-Thought (CoF-CoT) approach that breaks down NLU tasks into multiple reasoning steps where LLMs can learn to acquire and leverage essential concepts to solve tasks from different granularities."
            },
            "score": 4
        },
        {
            "id": "8089b431b2e09c27967428fb542c0935fb95ec30",
            "paperId": "8089b431b2e09c27967428fb542c0935fb95ec30",
            "title": "STAR: Constraint LoRA with Dynamic Active Learning for Data-Efficient Fine-Tuning of Large Language Models",
            "abstract": "Though Large Language Models (LLMs) have demonstrated the powerful capabilities of few-shot learning through prompting methods, supervised training is still necessary for complex reasoning tasks. Because of their extensive parameters and memory consumption, both Parameter-Efficient Fine-Tuning (PEFT) methods and Memory-Efficient Fine-Tuning methods have been proposed for LLMs. Nevertheless, the issue of large annotated data consumption, the aim of Data-Efficient Fine-Tuning, remains unexplored. One obvious way is to combine the PEFT method with active learning. However, the experimental results show that such a combination is not trivial and yields inferior results. Through probe experiments, such observation might be explained by two main reasons: uncertainty gap and poor model calibration. Therefore, in this paper, we propose a novel approach to effectively integrate uncertainty-based active learning and LoRA. Specifically, for the uncertainty gap, we introduce a dynamic uncertainty measurement that combines the uncertainty of the base model and the uncertainty of the full model during the iteration of active learning. For poor model calibration, we incorporate the regularization method during LoRA training to keep the model from being over-confident, and the Monte-Carlo dropout mechanism is employed to enhance the uncertainty estimation. Experimental results show that the proposed approach outperforms existing baseline models on three complex reasoning tasks.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel approach to effectively integrate uncertainty-based active learning and LoRA is proposed, which incorporates the regularization method during LoRA training to keep the model from being over-confident, and the Monte-Carlo dropout mechanism is employed to enhance the uncertainty estimation."
            },
            "score": 4
        },
        {
            "id": "beecd5dc56fbd344749c444eb102a4bb485a2070",
            "paperId": "beecd5dc56fbd344749c444eb102a4bb485a2070",
            "title": "Inference to the Best Explanation in Large Language Models",
            "abstract": "While Large Language Models (LLMs) have found success in real-world applications, their underlying explanatory process is still poorly understood. This paper proposes IBE-Eval, a framework inspired by philosophical accounts on Inference to the Best Explanation (IBE) to advance the interpretation and evaluation of LLMs' explanations. IBE-Eval estimates the plausibility of natural language explanations through a combination of explicit logical and linguistic features including: consistency, parsimony, coherence, and uncertainty. Extensive experiments are conducted on Causal Question Answering (CQA), where \\textit{IBE-Eval} is tasked to select the most plausible causal explanation amongst competing ones generated by LLMs (i.e., GPT 3.5 and Llama 2). The experiments reveal that IBE-Eval can successfully identify the best explanation with up to 77\\% accuracy ($\\approx 27\\%$ above random), improving upon a GPT 3.5-as-a-Judge baseline ($\\approx+17\\%$) while being intrinsically more efficient and interpretable. Additional analyses suggest that, despite model-specific variances, LLM-generated explanations tend to conform to IBE criteria and that IBE-Eval is significantly correlated with human judgment, opening up opportunities for future development of automated explanation verification tools.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "IBE-Eval is proposed, a framework inspired by philosophical accounts on Inference to the Best Explanation (IBE) to advance the interpretation and evaluation of LLMs' explanations, and it is suggested that LLM-generated explanations tend to conform to IBE criteria and that IBE-Eval is significantly correlated with human judgment, opening up opportunities for future development of automated explanation verification tools."
            },
            "score": 4
        },
        {
            "id": "3864b52902f8315f21385c4a6d3ce6c0193e1ab9",
            "paperId": "3864b52902f8315f21385c4a6d3ce6c0193e1ab9",
            "title": "Conformal Prediction with Large Language Models for Multi-Choice Question Answering",
            "abstract": "As large language models continue to be widely developed, robust uncertainty quantification techniques will become crucial for their safe deployment in high-stakes scenarios. In this work, we explore how conformal prediction can be used to provide uncertainty quantification in language models for the specific task of multiple-choice question-answering. We find that the uncertainty estimates from conformal prediction are tightly correlated with prediction accuracy. This observation can be useful for downstream applications such as selective classification and filtering out low-quality predictions. We also investigate the exchangeability assumption required by conformal prediction to out-of-subject questions, which may be a more realistic scenario for many practical applications. Our work contributes towards more trustworthy and reliable usage of large language models in safety-critical situations, where robust guarantees of error rate are required.",
            "year": 2023,
            "citationCount": 29,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores how conformal prediction can be used to provide uncertainty quantification in language models for the specific task of multiple-choice question-answering and finds that the uncertainty estimates from conformal Prediction are tightly correlated with prediction accuracy."
            },
            "score": 4
        },
        {
            "id": "2a74fc66beea8bce542581560ca6ec5a0e1bb024",
            "paperId": "2a74fc66beea8bce542581560ca6ec5a0e1bb024",
            "title": "CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation",
            "abstract": "Annotated data plays a critical role in Natural Language Processing (NLP) in training models and evaluating their performance. Given recent developments in Large Language Models (LLMs), models such as ChatGPT demonstrate zero-shot capability on many text-annotation tasks, comparable with or even exceeding human annotators. Such LLMs can serve as alternatives for manual annotation, due to lower costs and higher scalability. However, limited work has leveraged LLMs as complementary annotators, nor explored how annotation work is best allocated among humans and LLMs to achieve both quality and cost objectives. We propose CoAnnotating, a novel paradigm for Human-LLM co-annotation of unstructured texts at scale. Under this framework, we utilize uncertainty to estimate LLMs' annotation capability. Our empirical study shows CoAnnotating to be an effective means to allocate work from results on different datasets, with up to 21% performance improvement over random baseline. For code implementation, see https://github.com/SALT-NLP/CoAnnotating.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes CoAnnotating, a novel paradigm for Human-LLM co-annotation of unstructured texts at scale, and utilizes uncertainty to estimate LLMs' annotation capability."
            },
            "score": 4
        },
        {
            "id": "04365f0f1db4c659c3297cb8e70c39b38ed3b487",
            "paperId": "04365f0f1db4c659c3297cb8e70c39b38ed3b487",
            "title": "Self-Evaluation Improves Selective Generation in Large Language Models",
            "abstract": "Safe deployment of large language models (LLMs) may benefit from a reliable method for assessing their generated content to determine when to abstain or to selectively generate. While likelihood-based metrics such as perplexity are widely employed, recent research has demonstrated the limitations of using sequence-level probability estimates given by LLMs as reliable indicators of generation quality. Conversely, LLMs have demonstrated strong calibration at the token level, particularly when it comes to choosing correct answers in multiple-choice questions or evaluating true/false statements. In this work, we reformulate open-ended generation tasks into token-level prediction tasks, and leverage LLMs' superior calibration at the token level. We instruct an LLM to self-evaluate its answers, employing either a multi-way comparison or a point-wise evaluation approach, with the option to include a ``None of the above'' option to express the model's uncertainty explicitly. We benchmark a range of scoring methods based on self-evaluation and evaluate their performance in selective generation using TruthfulQA and TL;DR. Through experiments with PaLM-2 and GPT-3, we demonstrate that self-evaluation based scores not only improve accuracy, but also correlate better with the overall quality of generated content.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work reformulates open-ended generation tasks into token-level prediction tasks, and leverage LLMs' superior calibration at the token level, and demonstrates that self-evaluation based scores not only improve accuracy, but also correlate better with the overall quality of generated content."
            },
            "score": 4
        },
        {
            "id": "a3cc857a10898b2eedd04ff1dd33e8b6c9b1e04c",
            "paperId": "a3cc857a10898b2eedd04ff1dd33e8b6c9b1e04c",
            "title": "CUE: An Uncertainty Interpretation Framework for Text Classifiers Built on Pre-Trained Language Models",
            "abstract": "Text classifiers built on Pre-trained Language Models (PLMs) have achieved remarkable progress in various tasks including sentiment analysis, natural language inference, and question-answering. However, the occurrence of uncertain predictions by these classifiers poses a challenge to their reliability when deployed in practical applications. Much effort has been devoted to designing various probes in order to understand what PLMs capture. But few studies have delved into factors influencing PLM-based classifiers' predictive uncertainty. In this paper, we propose a novel framework, called CUE, which aims to interpret uncertainties inherent in the predictions of PLM-based models. In particular, we first map PLM-encoded representations to a latent space via a variational auto-encoder. We then generate text representations by perturbing the latent space which causes fluctuation in predictive uncertainty. By comparing the difference in predictive uncertainty between the perturbed and the original text representations, we are able to identify the latent dimensions responsible for uncertainty and subsequently trace back to the input features that contribute to such uncertainty. Our extensive experiments on four benchmark datasets encompassing linguistic acceptability classification, emotion classification, and natural language inference show the feasibility of our proposed framework. Our source code is available at: https://github.com/lijiazheng99/CUE.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel framework, called CUE, is proposed, which aims to interpret uncertainties inherent in the predictions of PLM-based models via a variational auto-encoder, and identifies the latent dimensions responsible for uncertainty and traces back to the input features that contribute to such uncertainty."
            },
            "score": 4
        },
        {
            "id": "dfab0f3ee6f47e36cccee145794cd117773e6f73",
            "paperId": "dfab0f3ee6f47e36cccee145794cd117773e6f73",
            "title": "Towards LLM-based Fact Verification on News Claims with a Hierarchical Step-by-Step Prompting Method",
            "abstract": "While large pre-trained language models (LLMs) have shown their impressive capabilities in various NLP tasks, they are still under-explored in the misinformation domain. In this paper, we examine LLMs with in-context learning (ICL) for news claim verification, and find that only with 4-shot demonstration examples, the performance of several prompting methods can be comparable with previous supervised models. To further boost performance, we introduce a Hierarchical Step-by-Step (HiSS) prompting method which directs LLMs to separate a claim into several subclaims and then verify each of them via multiple questions-answering steps progressively. Experiment results on two public misinformation datasets show that HiSS prompting outperforms state-of-the-art fully-supervised approach and strong few-shot ICL-enabled baselines.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Hierarchical Step-by-Step (HiSS) prompting method is introduced which directs LLMs to separate a claim into several subclaims and then verify each of them via multiple questions-answering steps progressively."
            },
            "score": 4
        },
        {
            "id": "2ed1cb4d53989732aa5f7e21be956167c4f8aab7",
            "paperId": "2ed1cb4d53989732aa5f7e21be956167c4f8aab7",
            "title": "Efficient Benchmarking of NLP APIs using Multi-armed Bandits",
            "abstract": "Comparing NLP systems to select the best one for a task of interest, such as named entity recognition, is critical for practitioners and researchers. A rigorous approach involves setting up a hypothesis testing scenario using the performance of the systems on query documents. However, often the hypothesis testing approach needs to send a lot of document queries to the systems, which can be problematic. In this paper, we present an effective alternative based on the multi-armed bandit (MAB). We propose a hierarchical generative model to represent the uncertainty in the performance measures of the competing systems, to be used by Thompson Sampling to solve the resulting MAB. Experimental results on both synthetic and real data show that our approach requires significantly fewer queries compared to the standard benchmarking technique to identify the best system according to F-measure.",
            "year": 2017,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A hierarchical generative model is proposed to represent the uncertainty in the performance measures of the competing systems, to be used by Thompson Sampling to solve the resulting MAB."
            },
            "score": 4
        },
        {
            "id": "bf4700077294c369f64eda65f677dd4f61b43072",
            "paperId": "bf4700077294c369f64eda65f677dd4f61b43072",
            "title": "Uncertainty Estimation and Reduction of Pre-trained Models for Text Regression",
            "abstract": "Abstract State-of-the-art classification and regression models are often not well calibrated, and cannot reliably provide uncertainty estimates, limiting their utility in safety-critical applications such as clinical decision-making. While recent work has focused on calibration of classifiers, there is almost no work in NLP on calibration in a regression setting. In this paper, we quantify the calibration of pre- trained language models for text regression, both intrinsically and extrinsically. We further apply uncertainty estimates to augment training data in low-resource domains. Our experiments on three regression tasks in both self-training and active-learning settings show that uncertainty estimation can be used to increase overall performance and enhance model generalization.",
            "year": 2022,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper quantifies the calibration of pre- trained language models for text regression, both intrinsically and extrinsically, and applies uncertainty estimates to augment training data in low-resource domains."
            },
            "score": 3
        },
        {
            "id": "61fe60a1d1968914958e9d01be0f67593601c5c0",
            "paperId": "61fe60a1d1968914958e9d01be0f67593601c5c0",
            "title": "Bayesian Hierarchical Models for Counterfactual Estimation",
            "abstract": "Counterfactual explanations utilize feature perturbations to analyze the outcome of an original decision and recommend an actionable recourse. We argue that it is beneficial to provide several alternative explanations rather than a single point solution and propose a probabilistic paradigm to estimate a diverse set of counterfactuals. Specifically, we treat the perturbations as random variables endowed with prior distribution functions. This allows sampling multiple counterfactuals from the posterior density, with the added benefit of incorporating inductive biases, preserving domain specific constraints and quantifying uncertainty in estimates. More importantly, we leverage Bayesian hierarchical modeling to share information across different subgroups of a population, which can both improve robustness and measure fairness. A gradient based sampler with superior convergence characteristics efficiently computes the posterior samples. Experiments across several datasets demonstrate that the counterfactuals estimated using our approach are valid, sparse, diverse and feasible.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work argues that it is beneficial to provide several alternative explanations rather than a single point solution and proposes a probabilistic paradigm to estimate a diverse set of counterfactuals, which treats the perturbations as random variables endowed with prior distribution functions."
            },
            "score": 3
        },
        {
            "id": "7f6d48d7b1641d3d2fd4ee06c434a73af8fce07b",
            "paperId": "7f6d48d7b1641d3d2fd4ee06c434a73af8fce07b",
            "title": "Density-Softmax: Scalable and Calibrated Uncertainty Estimation under Distribution Shifts",
            "abstract": "Prevalent deterministic deep-learning models suffer from significant over-confidence under distribution shifts. Probabilistic approaches can reduce this problem but struggle with computational efficiency. In this paper, we propose Density-Softmax, a fast and lightweight deterministic method to improve calibrated uncertainty estimation via a combination of density function with the softmax layer. By using the latent representation's likelihood value, our approach produces more uncertain predictions when test samples are distant from the training samples. Theoretically, we show that Density-Softmax can produce high-quality uncertainty estimation with neural networks, as it is the solution of minimax uncertainty risk and is distance-aware, thus reducing the over-confidence of the standard softmax. Empirically, our method enjoys similar computational efficiency as a single forward pass deterministic with standard softmax on the shifted toy, vision, and language datasets across modern deep-learning architectures. Notably, Density-Softmax uses 4 times fewer parameters than Deep Ensembles and 6 times lower latency than Rank-1 Bayesian Neural Network, while obtaining competitive predictive performance and lower calibration errors under distribution shifts.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Density-Softmax is proposed, a fast and lightweight deterministic method to improve calibrated uncertainty estimation via a combination of density function with the softmax layer, which enjoys similar computational efficiency as a single forward pass deterministic with standard softmax on the shifted toy, vision, and language datasets across modern deep-learning architectures."
            },
            "score": 3
        },
        {
            "id": "a860ba337cead5e2e970460522d6612a49836ff1",
            "paperId": "a860ba337cead5e2e970460522d6612a49836ff1",
            "title": "Uncertainty Estimation of Transformers' Predictions via Topological Analysis of the Attention Matrices",
            "abstract": "Determining the degree of confidence of deep learning model in its prediction is an open problem in the field of natural language processing. Most of the classical methods for uncertainty estimation are quite weak for text classification models. We set the task of obtaining an uncertainty estimate for neural networks based on the Transformer architecture. A key feature of such mo-dels is the attention mechanism, which supports the information flow between the hidden representations of tokens in the neural network. We explore the formed relationships between internal representations using Topological Data Analysis methods and utilize them to predict model's confidence. In this paper, we propose a method for uncertainty estimation based on the topological properties of the attention mechanism and compare it with classical methods. As a result, the proposed algorithm surpasses the existing methods in quality and opens up a new area of application of the attention mechanism, but requires the selection of topological features.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a method for uncertainty estimation based on the topological properties of the attention mechanism and compares it with classical methods, which surpasses the existing methods in quality and opens up a new area of application of the Attention mechanism, but requires the selection of topological features."
            },
            "score": 3
        },
        {
            "id": "9e5a1b8aa30b05aa32644023c290cf9de7f55727",
            "paperId": "9e5a1b8aa30b05aa32644023c290cf9de7f55727",
            "title": "Automated Parliaments: A Solution to Decision Uncertainty and Misalignment in Language Models",
            "abstract": "As AI takes on a greater role in the modern world, it is essential to ensure that AI models can overcome decision uncertainty and remain aligned with human morality and interests. This research paper proposes a method for improving the decision-making of language models (LMs) via Automated Parliaments (APs) - constructs made of AI delegates each representing a certain perspective. Delegates themselves consist of three AI models: generators, modifiers, and evaluators. We specify two mechanisms for producing optimal solutions: the Simultaneous Modification mechanism for response creation and an evaluation mechanism for fairly assessing solutions. The overall process begins when each generator creates a response aligned with its delegate's theory. The modifiers alter all other responses to make them more self-aligned. The evaluators collectively assess the best end response. Finally, the modifiers and generators learn from feedback from the evaluators. In our research, we tested the evaluation mechanism, comparing the use of single-value zero-shot prompting and AP few-shot prompting in evaluating morally contentious scenarios. We found that the AP architecture saw a 57.3% reduction in its loss value compared to the baseline. We conclude by discussing some potential applications of APs and specifically their potential impact when implemented as Automated Moral Parliaments.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This research paper proposes a method for improving the decision-making of language models via Automated Parliaments (APs) - constructs made of AI delegates each representing a certain perspective, and specifies two mechanisms for producing optimal solutions: the Simultaneous Modification mechanism for response creation and an evaluation mechanism for fairly assessing solutions."
            },
            "score": 3
        },
        {
            "id": "af1fb33a4eeffe87ffef95d5b8f157c1d8b3f5c0",
            "paperId": "af1fb33a4eeffe87ffef95d5b8f157c1d8b3f5c0",
            "title": "Prompting through Prototype: A Prototype-based Prompt Learning on Pretrained Vision-Language Models",
            "abstract": "Prompt learning is a new learning paradigm which reformulates downstream tasks as similar pretraining tasks on pretrained models by leveraging textual prompts. Recent works have demonstrated that prompt learning is particularly useful for few-shot learning, where there is limited training data. Depending on the granularity of prompts, those methods can be roughly divided into task-level prompting and instance-level prompting. Task-level prompting methods learn one universal prompt for all input samples, which is efficient but ineffective to capture subtle differences among different classes. Instance-level prompting methods learn a specific prompt for each input, though effective but inefficient. In this work, we develop a novel prototype-based prompt learning method to overcome the above limitations. In particular, we focus on few-shot image recognition tasks on pretrained vision-language models (PVLMs) and develop a method of prompting through prototype (PTP), where we define $K$ image prototypes and $K$ prompt prototypes. In PTP, the image prototype represents a centroid of a certain image cluster in the latent space and a prompt prototype is defined as a soft prompt in the continuous space. The similarity between a query image and an image prototype determines how much this prediction relies on the corresponding prompt prototype. Hence, in PTP, similar images will utilize similar prompting ways. Through extensive experiments on seven real-world benchmarks, we show that PTP is an effective method to leverage the latent knowledge and adaptive to various PVLMs. Moreover, through detailed analysis, we discuss pros and cons for prompt learning and parameter-efficient fine-tuning under the context of few-shot learning.",
            "year": 2022,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work focuses on few-shot image recognition tasks on pretrained vision-language models (PVLMs) and develops a method of prompting through prototype (PTP), where the image prototype represents a centroid of a certain image cluster in the latent space and a prompt prototype is defined as a soft prompt in the continuous space."
            },
            "score": 3
        },
        {
            "id": "194d1089b730e4a1d15a60a0144b50771909c3c5",
            "paperId": "194d1089b730e4a1d15a60a0144b50771909c3c5",
            "title": "Active Learning for Sequence Tagging with Deep Pre-trained Models and Bayesian Uncertainty Estimates",
            "abstract": "Annotating training data for sequence tagging of texts is usually very time-consuming. Recent advances in transfer learning for natural language processing in conjunction with active learning open the possibility to significantly reduce the necessary annotation budget. We are the first to thoroughly investigate this powerful combination for the sequence tagging task. We conduct an extensive empirical study of various Bayesian uncertainty estimation methods and Monte Carlo dropout options for deep pre-trained models in the active learning framework and find the best combinations for different types of models. Besides, we also demonstrate that to acquire instances during active learning, a full-size Transformer can be substituted with a distilled version, which yields better computational performance and reduces obstacles for applying deep active learning in practice.",
            "year": 2021,
            "citationCount": 38,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An extensive empirical study of various Bayesian uncertainty estimation methods and Monte Carlo dropout options for deep pre-trained models in the active learning framework and finds the best combinations for different types of models and demonstrates that to acquire instances during active learning, a full-size Transformer can be substituted with a distilled version."
            },
            "score": 3
        },
        {
            "id": "04071b5817d84eacc5d56816c0158c1fbee0d286",
            "paperId": "04071b5817d84eacc5d56816c0158c1fbee0d286",
            "title": "Using Imperfect Surrogates for Downstream Inference: Design-based Supervised Learning for Social Science Applications of Large Language Models",
            "abstract": "In computational social science (CSS), researchers analyze documents to explain social and political phenomena. In most scenarios, CSS researchers first obtain labels for documents and then explain labels using interpretable regression analyses in the second step. One increasingly common way to annotate documents cheaply at scale is through large language models (LLMs). However, like other scalable ways of producing annotations, such surrogate labels are often imperfect and biased. We present a new algorithm for using imperfect annotation surrogates for downstream statistical analyses while guaranteeing statistical properties -- like asymptotic unbiasedness and proper uncertainty quantification -- which are fundamental to CSS research. We show that direct use of surrogate labels in downstream statistical analyses leads to substantial bias and invalid confidence intervals, even with high surrogate accuracy of 80-90%. To address this, we build on debiased machine learning to propose the design-based supervised learning (DSL) estimator. DSL employs a doubly-robust procedure to combine surrogate labels with a smaller number of high-quality, gold-standard labels. Our approach guarantees valid inference for downstream statistical analyses, even when surrogates are arbitrarily biased and without requiring stringent assumptions, by controlling the probability of sampling documents for gold-standard labeling. Both our theoretical analysis and experimental results show that DSL provides valid statistical inference while achieving root mean squared errors comparable to existing alternatives that focus only on prediction without inferential guarantees.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The design-based supervised learning (DSL) estimator employs a doubly-robust procedure to combine surrogate labels with a smaller number of high-quality, gold-standard labels, and guarantees valid inference for downstream statistical analyses, even when surrogates are arbitrarily biased and without requiring stringent assumptions."
            },
            "score": 3
        },
        {
            "id": "48fb667125298cf724f7b652d521686180412351",
            "paperId": "48fb667125298cf724f7b652d521686180412351",
            "title": "A Close Look into the Calibration of Pre-trained Language Models",
            "abstract": "Pre-trained language models (PLMs) may fail in giving reliable estimates of their predictive uncertainty. We take a close look into this problem, aiming to answer two questions: (1) Do PLMs learn to become calibrated in the training process? (2) How effective are existing calibration methods? For the first question, we conduct fine-grained control experiments to study the dynamic change in PLMs\u2019 calibration performance in training. We consider six factors as control variables, including dataset difficulty, available training samples, training steps, the number of tunable parameters, model scale, and pretraining. We observe a consistent change in calibration performance across six factors. We find that PLMs don\u2019t learn to become calibrated in training, evidenced by the continual increase in confidence, no matter whether the predictions are correct or not. We highlight that our finding somewhat contradicts two established conclusions: (a) Larger PLMs are more calibrated; (b) Pretraining improves model calibration. Next, we study the effectiveness of existing calibration methods in mitigating the overconfidence issue. Besides unlearnable calibration methods (e.g., label smoothing), we adapt and extend two recently proposed learnable methods that directly collect data to train models to have reasonable confidence estimations. Experimental results show that learnable methods significantly reduce PLMs\u2019 confidence in wrong predictions.",
            "year": 2022,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that pre-trained language models don\u2019t learn to become calibrated in training, evidenced by the continual increase in confidence, no matter whether the predictions are correct or not."
            },
            "score": 3
        },
        {
            "id": "3c7a1fc05882738fbeedbc9eae01b140797b214f",
            "paperId": "3c7a1fc05882738fbeedbc9eae01b140797b214f",
            "title": "Is My Driver Observation Model Overconfident? Input-Guided Calibration Networks for Reliable and Interpretable Confidence Estimates",
            "abstract": "Driver observation models are rarely deployed under perfect conditions. In practice, illumination, camera placement and type differ from the ones present during training and unforeseen behaviours may occur at any time. While observing the human behind the steering wheel leads to more intuitive human-vehicle-interaction and safer driving, it requires recognition algorithms which do not only predict the correct driver state, but also determine their prediction quality through realistic and interpretable confidence measures. Reliable uncertainty estimates are crucial for building trust and are a serious obstacle for deploying activity recognition networks in real driving systems. In this work, we for the first time examine how well the confidence values of modern driver observation models indeed match the probability of the correct outcome and show that raw neural network-based approaches tend to significantly overestimate their prediction quality. To correct this misalignment between the confidence values and the actual uncertainty, we consider two strategies. First, we enhance two activity recognition models often used for driver observation with temperature scaling \u2013 an off-the-shelf method for confidence calibration in image classification. Then, we introduce Calibrated Action Recognition with Input Guidance (CARING) \u2013 a novel approach leveraging an additional neural network to learn scaling the confidences depending on the video representation. Extensive experiments on the Drive&Act dataset demonstrate that both strategies drastically improve the quality of model confidences, while our CARING model outperforms both, the original architectures and their temperature scaling enhancement, leading to best uncertainty estimates.",
            "year": 2022,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work examines how well the confidence values of modern driver observation models indeed match the probability of the correct outcome and shows that raw neural network-based approaches tend to significantly overestimate their prediction quality."
            },
            "score": 3
        },
        {
            "id": "f11d7ff00ac0dacc4248559dc3e14479123d9cbb",
            "paperId": "f11d7ff00ac0dacc4248559dc3e14479123d9cbb",
            "title": "Enhancing Early Detection of Cognitive Decline in the Elderly through Ensemble of NLP Techniques: A Comparative Study Utilizing Large Language Models in Clinical Notes",
            "abstract": "Background: Early detection of cognitive decline in elderly individuals facilitates clinical trial enrollment and timely medical interventions. This study aims to apply, evaluate, and compare advanced natural language processing techniques for identifying signs of cognitive decline in clinical notes. Methods: This study, conducted at Mass General Brigham (MGB), Boston, MA, included clinical notes from the 4 years prior to initial mild cognitive impairment (MCI) diagnosis in 2019 for patients [\u2265] 50 years. Note sections regarding cognitive decline were labeled manually. A random sample of 4,949 note sections filtered with cognitive functions-related keywords were used for traditional AI model development, and 200 random subset were used for LLM and prompt development; another random sample of 1996 note sections without keyword filtering were used for testing. Prompt templates for large language models (LLM), Llama 2 on Amazon Web Service and GPT-4 on Microsoft Azure, were developed with multiple prompting approaches to select the optimal LLM-based method. Baseline comparisons were made with XGBoost and a hierarchical attention-based deep neural network model. An ensemble of the three models was then constructed using majority vote. Results: GPT-4 demonstrated superior accuracy and efficiency to Llama 2. The ensemble model outperformed individual models, achieving a precision of 90.3%, recall of 94.2%, and F1-score of 92.2%. Notably, the ensemble model demonstrated a marked improvement in precision (from a 70%-79% range to above 90%) compared to the best performing single model. Error analysis revealed 63 samples were wrongly predicted by at least one model; however, only 2 cases (3.2%) were mutual errors across all models, indicating diverse error profiles among them. Conclusion: Our findings indicate that LLMs and traditional models exhibit diverse error profiles. The ensemble of LLMs and locally trained machine learning models on EHR data was found to be complementary, enhancing performance and improving diagnostic accuracy.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The ensemble of LLMs and locally trained machine learning models on EHR data was found to be complementary, enhancing performance and improving diagnostic accuracy, indicating that LLMs and traditional models exhibit diverse error profiles."
            },
            "score": 3
        },
        {
            "id": "213a616590be543c5a2d7dee3206ecd816c58b4a",
            "paperId": "213a616590be543c5a2d7dee3206ecd816c58b4a",
            "title": "Macroeconomic Effects of Uncertainty: A Big Data Analysis for India",
            "abstract": "Uncertainty about the current state and near-term outlook of an economy as well as the likely course of future policy actions can prompt economic agents to alter their decisions to spend, save, invest and hire. In this paper, we construct three alternative indices to measure the level of uncertainty for the Indian economy. The first two uncertainty indices are constructed by applying text mining and natural language processing (NLP) techniques on a dataset compiled from leading Indian business newspapers. The third index is based on internet search intensity data available from Google Trends. Empirical findings from a Local Projections-based econometric framework suggest that uncertainty shocks influence financial markets as well as the real economy in India. Our results indicate that both investment activity and real GDP growth slow down when uncertainty increases in the economy. Such uncertainty indices can help strengthen policy simulation exercises to study the impact of low/high uncertainty scenarios and also improve near-term projection of macroeconomic variables which exhibit high degree of sensitivity to uncertainty.",
            "year": 2020,
            "citationCount": 7,
            "tldr": null,
            "score": 3
        },
        {
            "id": "97d4145117462177e1244a99d7a25afed4c234f7",
            "paperId": "97d4145117462177e1244a99d7a25afed4c234f7",
            "title": "How Many Validation Labels Do You Need? Exploring the Design Space of Label-Efficient Model Ranking",
            "abstract": "This paper presents LEMR (Label-Efficient Model Ranking) and introduces the MoraBench Benchmark. LEMR is a novel framework that minimizes the need for costly annotations in model selection by strategically annotating instances from an unlabeled validation set. To evaluate LEMR, we leverage the MoraBench Benchmark, a comprehensive collection of model outputs across diverse scenarios. Our extensive evaluation across 23 different NLP tasks in semi-supervised learning, weak supervision, and prompt selection tasks demonstrates LEMR's effectiveness in significantly reducing labeling costs. Key findings highlight the impact of suitable ensemble methods, uncertainty sampling strategies, and model committee selection in enhancing model ranking accuracy. LEMR, supported by the insights from MoraBench, provides a cost-effective and accurate solution for model selection, especially valuable in resource-constrained environments.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents LEMR (Label-Efficient Model Ranking) and introduces the MoraBench Benchmark, a comprehensive collection of model outputs across diverse scenarios that provides a cost-effective and accurate solution for model selection, especially valuable in resource-constrained environments."
            },
            "score": 3
        },
        {
            "id": "866591f8b3ed08a97274a56e878d6eaf12c194f1",
            "paperId": "866591f8b3ed08a97274a56e878d6eaf12c194f1",
            "title": "Weather Simulation Uncertainty Estimation Using Bayesian Hierarchical Models",
            "abstract": "Estimates of the uncertainty of model output fields (e.g., 2-m temperature, surface radiation fluxes, or wind speed) are of great value to the weather and climate communities. The traditional approach for the uncertainty estimation is to conduct an ensemble of simulations where the model configuration is perturbed and/or different models are considered. This procedure is very computationally expensive and may not be feasible, in particular for higher-resolution experiments. In this paper, a new method based on Bayesian hierarchical models (BHMs) that requires just one model run is proposed. It is applied to the Weather Research and Forecasting (WRF) Model\u2019s 2-m temperature in the Botnia\u2013Atlantica region in Scandinavia for a 10-day period in the winter and summer seasons. For both seasons, the estimated uncertainty using the BHM is found to be comparable to that obtained from an ensemble of experiments in which different planetary boundary layer (PBL) schemes are employed. While WRF-BHM is not capable of generating the full set of products obtained from an ensemble of simulations, it can be used to extract commonly used diagnostics including the uncertainty estimation that is the focus of this work. The methodology proposed here is fully general and can easily be extended to any other output variable and numerical model.",
            "year": 2019,
            "citationCount": 5,
            "tldr": null,
            "score": 2
        },
        {
            "id": "698d83e2ba10d94c2a0723e907eb297ff4a6249d",
            "paperId": "698d83e2ba10d94c2a0723e907eb297ff4a6249d",
            "title": "HallE-Switch: Rethinking and Controlling Object Existence Hallucinations in Large Vision Language Models for Detailed Caption",
            "abstract": "Current large vision-language models (LVLMs) achieve remarkable progress, yet there remains significant uncertainty regarding their ability to accurately apprehend visual details, that is, in performing detailed captioning. To address this, we introduce CCEval, a GPT-4 assisted evaluation method tailored for detailed captioning. Interestingly, while LVLMs demonstrate minimal object existence hallucination in existing VQA benchmarks, our proposed evaluation reveals continued susceptibility to such hallucinations. In this paper, we make the first attempt to investigate and attribute such hallucinations, including image resolution, the language decoder size, and instruction data amount, quality, granularity. Our findings underscore the unwarranted inference when the language description includes details at a finer object granularity than what the vision module can ground or verify, thus inducing hallucination. To control such hallucinations, we further attribute the reliability of captioning to contextual knowledge (involving only contextually grounded objects) and parametric knowledge (containing inferred objects by the model). Thus, we introduce HallE-Switch, a controllable LVLM in terms of Hallucination in object Existence. HallE-Switch can condition the captioning to shift between (i) exclusively depicting contextual knowledge for grounded objects and (ii) blending it with parametric knowledge to imagine inferred objects. Our method reduces hallucination by 44% compared to LLaVA7B and maintains the same object coverage.",
            "year": 2023,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces HallE-Switch, a controllable LVLM in terms of Hallucination in object Existence, and introduces CCEval, a GPT-4 assisted evaluation method tailored for detailed captioning."
            },
            "score": 2
        },
        {
            "id": "0a922b4fdbe923b5161b5c6f5adfe586bf7304c3",
            "paperId": "0a922b4fdbe923b5161b5c6f5adfe586bf7304c3",
            "title": "Large language models will not replace healthcare professionals: curbing popular fears and hype",
            "abstract": "Following the release of ChatGPT, large language models (LLMs) have entered the mainstream. ChatGPT and GPT-4 recently garnered particular attention for attaining expert-level performance in United States Medical Licensing Examinations. However, performance is not perfect, and has not been as impressive in more specialised tests, such as the Membership of the Royal College of General Practitioners Applied Knowledge Test. ChatGPT frequently \u2018hallucinates\u2019, providing false, unverified information in the same manner as which it delivers facts. While performance in clinical tasks is expected to improve dramatically with the release of GPT-4, remaining inaccuracy and lack of an uncertainty indicator preclude autonomous deployment of ChatGPT and LLM chatbots like it in clinical settings. LLM applications may nevertheless revolutionise cognitive work \u2013 tools such as ChatGPT excel in tasks where specialist knowledge is not required, or is provided by the user prompt: examples include correcting language and rephrasing information for different audiences or within other constraints (e.g. word limits), and it has already been proposed as a tool for administrative tasks, clinical work and patient education. While this does represent an impressive advance in natural language processing, and benefits may be manifold across fields including medicine, these limited use-cases do not live up to the hype surrounding LLMs and artificial intelligence (AI) more generally in 2023. This is due to a fundamental misunderstanding about the form of AI represented by LLMs. Do LLMs represent artificial generalised intelligence (AGI)? The answer is currently probably not, despite emergence of interactive conversational interfaces and few-shot or zero-shot properties \u2013 where models execute tasks that they have previously been exposed to only a few times before, or never before, respectively. This is demonstrated by observing how these models are trained, and the composition of their architecture. The backend LLM (GPT-3, from which GPT-3.5 was developed) underpinning older versions of ChatGPT was initially trained on a dataset of billions of words taken from books, Wikipedia and the wider internet. Through a process of machine learning, the GPT-3 accurately encoded the association between individual words in the training dataset. Through \u2018reinforcement learning from human feedback\u2019, GPT-3 was subsequently finetuned to provide appropriate responses to users\u2019 queries \u2013 producing GPT-3.5. Through these processes, ChatGPT has developed an impressive ability to respond appropriately to diverse prompts, albeit equally lucidly with accurate and inaccurate statements. This lucidity, responsiveness and flexibility have led to sensational claims regarding attainment of AGI that could feasibly replace professionals in cognitive roles. The performance of GPT-4 \u2013 which powers newer versions of ChatGPT \u2013 dwarfs that of GPT-3.5 across tasks including logical reasoning and medical aptitude tests. Moreover, GPT-4 can be prompted to adopt different roles on demand, and will accept multimodal input, processing images as well as text. Prominent figures in industry and academia have advocated for a moratorium on development of more advanced AI systems in response to concerns regarding safety, ethics and fears of replacement. Despite these fears and hype, the barriers to implementation of LLMs replacing healthcare professionals in any capacity still look out of reach. Although GPT-4\u2019s architecture and training are confidential, it likely relies on similar schemata to its predecessor as it exhibits similar (albeit fewer) hallucinations and reasoning errors, including in medicine. None of ChatGPT\u2019s published autonomous training involved actual comprehension of language in context; the meaning (as we understand it) of words in the dataset was immaterial throughout. While this brute force linguistic processing may prove sufficient to develop a form of AGI, it appears that these LLMs will continue to be afflicted by mistakes and errors. Journal of the Royal Society of Medicine; 2023, Vol. 116(5) 181\u2013182",
            "year": 2023,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Following the release of ChatGPT, large language models (LLMs) have entered the mainstream and recently garnered particular attention for attaining expert-level performance in United States Medical Licensing Examinations, but performance has not been as impressive in more specialised tests."
            },
            "score": 2
        },
        {
            "id": "754d5164e196ff231786d10a48594f3f27d8721f",
            "paperId": "754d5164e196ff231786d10a48594f3f27d8721f",
            "title": "A Comprehensive Study of Multimodal Large Language Models for Image Quality Assessment",
            "abstract": "While Multimodal Large Language Models (MLLMs) have experienced significant advancement on visual understanding and reasoning, their potentials to serve as powerful, flexible, interpretable, and text-driven models for Image Quality Assessment (IQA) remains largely unexplored. In this paper, we conduct a comprehensive and systematic study of prompting MLLMs for IQA. Specifically, we first investigate nine prompting systems for MLLMs as the combinations of three standardized testing procedures in psychophysics (i.e., the single-stimulus, double-stimulus, and multiple-stimulus methods) and three popular prompting strategies in natural language processing (i.e., the standard, in-context, and chain-of-thought prompting). We then present a difficult sample selection procedure, taking into account sample diversity and uncertainty, to further challenge MLLMs equipped with the respective optimal prompting systems. We assess three open-source and one close-source MLLMs on several visual attributes of image quality (e.g., structural and textural distortions, color differences, and geometric transformations) in both full-reference and no-reference scenarios. Experimental results show that only the close-source GPT-4V provides a reasonable account for human perception of image quality, but is weak at discriminating fine-grained quality variations (e.g., color differences) and at comparing visual quality of multiple images, tasks humans can perform effortlessly.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A comprehensive and systematic study of prompting MLLMs for IQA and presents a difficult sample selection procedure, taking into account sample diversity and uncertainty, to further challenge MLLMs equipped with the respective optimal prompting systems."
            },
            "score": 2
        },
        {
            "id": "430ae40aaeeface5b5eae7d9730b4d7a0088ee07",
            "paperId": "430ae40aaeeface5b5eae7d9730b4d7a0088ee07",
            "title": "Semi-Automated Data Analysis for Ion-Selective Electrodes and Arrays Using the R Package ISEtools",
            "abstract": "A new software package, ISEtools, is introduced for use within the popular open-source programming language R that allows Bayesian statistical data analysis techniques to be implemented in a straightforward manner. Incorporating all collected data simultaneously, this Bayesian approach naturally accommodates sensor arrays and provides improved limit of detection estimates, including providing appropriate uncertainty estimates. Utilising >1500 lines of code, ISEtools provides a set of three core functions\u2014loadISEdata, describeISE, and analyseISE\u2014 for analysing ion-selective electrode data using the Nikolskii\u2013Eisenman equation. The functions call, fit, and extract results from Bayesian models, automatically determining data structures, applying appropriate models, and returning results in an easily interpretable manner and with publication-ready figures. Importantly, while advanced statistical and computationally intensive methods are employed, the functions are designed to be accessible to non-specialists. Here we describe basic features of the package, demonstrated through a worked environmental application.",
            "year": 2019,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Basic features of the ISEtools software package, introduced for use within the popular open-source programming language R, are described, demonstrated through a worked environmental application."
            },
            "score": 2
        },
        {
            "id": "570e4fec8c8f1c96b76accbb07d40e0528aafb4a",
            "paperId": "570e4fec8c8f1c96b76accbb07d40e0528aafb4a",
            "title": "Large Language Models Are Not Robust Multiple Choice Selectors",
            "abstract": "Multiple choice questions (MCQs) serve as a common yet important task format in the evaluation of large language models (LLMs). This work shows that modern LLMs are vulnerable to option position changes in MCQs due to their inherent\"selection bias\", namely, they prefer to select specific option IDs as answers (like\"Option A\"). Through extensive empirical analyses with 20 LLMs on three benchmarks, we pinpoint that this behavioral bias primarily stems from LLMs' token bias, where the model a priori assigns more probabilistic mass to specific option ID tokens (e.g., A/B/C/D) when predicting answers from the option IDs. To mitigate selection bias, we propose a label-free, inference-time debiasing method, called PriDe, which separates the model's prior bias for option IDs from the overall prediction distribution. PriDe first estimates the prior by permutating option contents on a small number of test samples, and then applies the estimated prior to debias the remaining samples. We demonstrate that it achieves interpretable and transferable debiasing with high computational efficiency. We hope this work can draw broader research attention to the bias and robustness of modern LLMs.",
            "year": 2023,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a label-free, inference-time debiasing method, called PriDe, which separates the model's prior bias for option IDs from the overall prediction distribution, and demonstrates that it achieves interpretable and transferable debiasing with high computational efficiency."
            },
            "score": 2
        },
        {
            "id": "b85fafd25f9b1f4273d718eeb0d5e2295f49f4fa",
            "paperId": "b85fafd25f9b1f4273d718eeb0d5e2295f49f4fa",
            "title": "A Study on Mitigating Hard Boundaries of Decision-Tree-based Uncertainty Estimates for AI Models",
            "abstract": "Outcomes of data-driven AI models cannot be assumed to be always correct. To estimate the uncertainty in these outcomes, the uncertainty wrapper framework has been proposed, which considers uncertainties related to model fit, input quality, and scope compliance. Uncertainty wrappers use a decision tree approach to cluster input quality related uncertainties, assigning inputs strictly to distinct uncertainty clusters. Hence, a slight variation in only one feature may lead to a cluster assignment with a significantly different uncertainty. Our objective is to replace this with an approach that mitigates hard decision boundaries of these assignments while preserving interpretability, runtime complexity, and prediction performance. Five approaches were selected as candidates and integrated into the uncertainty wrapper framework. For the evaluation based on the Brier score, datasets for a pedestrian detection use case were generated using the CARLA simulator and YOLOv3. All integrated approaches achieved a softening, i.e., smoothing, of uncertainty estimation. Yet, compared to decision trees, they are not so easy to interpret and have higher runtime complexity. Moreover, some components of the Brier score impaired while others improved. Most promising regarding the Brier score were random forests. In conclusion, softening hard decision tree boundaries appears to be a trade-off decision.",
            "year": 2022,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Softening hard decision tree boundaries appears to be a trade-off decision for the Brier score, and five approaches were selected as candidates and integrated into the uncertainty wrapper framework."
            },
            "score": 2
        },
        {
            "id": "08400a12db21fe4d4f844fec57844e74ac09c9ba",
            "paperId": "08400a12db21fe4d4f844fec57844e74ac09c9ba",
            "title": "Prompting or Fine-tuning? A Comparative Study of Large Language Models for Taxonomy Construction",
            "abstract": "Taxonomies represent hierarchical relations between entities, frequently applied in various software modeling and natural language processing (NLP) activities. They are typically subject to a set of structural constraints restricting their content. However, manual taxonomy construction can be time-consuming, incomplete, and costly to maintain. Recent studies of large language models (LLMs) have demonstrated that appropriate user inputs (called prompting) can effectively guide LLMs, such as GPT-3, in diverse NLP tasks without explicit (re-)training. However, existing approaches for automated taxonomy construction typically involve fine-tuning a language model by adjusting model parameters. In this paper, we present a general framework for taxonomy construction that takes into account structural constraints. We subsequently conduct a systematic comparison between the prompting and fine-tuning approaches performed on a hypernym taxonomy and a novel computer science taxonomy dataset. Our result reveals the following: (1) Even without explicit training on the dataset, the prompting approach outperforms fine-tuning-based approaches. Moreover, the performance gap between prompting and fine-tuning widens when the training dataset is small. However, (2) taxonomies generated by the fine-tuning approach can be easily post-processed to satisfy all the constraints, whereas handling violations of the taxonomies produced by the prompting approach can be challenging. These evaluation findings provide guidance on selecting the appropriate method for taxonomy construction and highlight potential enhancements for both approaches.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A general framework for taxonomy construction that takes into account structural constraints is presented and a systematic comparison between the prompting and fine-tuning approaches performed on a hypernym taxonomy and a novel computer science taxonomy dataset is conducted."
            },
            "score": 2
        },
        {
            "id": "ea717437ec67deee25f68fda655b52a6d322b701",
            "paperId": "ea717437ec67deee25f68fda655b52a6d322b701",
            "title": "'We're at war.' Healthcare workers' experience with organisational change, uncertainty and vaccine hesitancy in 2021 and 2022 during the COVID-19 vaccination programe in Poland.",
            "abstract": "This article analyses the organisation of the mass COVID-19 vaccination programme in Poland and its consequences for various aspects of the social identity of healthcare workers (HCWs). Based on 31 in-depth interviews with HCWs, our study reveals the following: (1) Certain elements of the programme (inclusion of other healthcare professionals like pharmacists and laboratory diagnosticians as vaccinators) and the provision of additional infrastructure (pharmacies and shopping malls) may prompt scepticism and criticism in physicians and nurses who feel challenged about their professional autonomy and hierarchies; (2) Given the high levels of professional uncertainty, the implementation of the COVID-19 vaccination is forcing HCWs to revise their attitude to medical standards, resulting in specific responses and adaptation strategies (ranging from the active involvement in the programme due to the sense of mission, to more or less evident scepticism); and (3) Confronting vaccine hesitancy, both among patients and other HCWs, contributes to the feeling of helplessness, leading to criticism of policymakers.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Given the high levels of professional uncertainty, the implementation of the COVID-19 vaccination is forcing HCWs to revise their attitude to medical standards, resulting in specific responses and adaptation strategies."
            },
            "score": 2
        },
        {
            "id": "24250764b3f1554100f1398a426c946ed85436ee",
            "paperId": "24250764b3f1554100f1398a426c946ed85436ee",
            "title": "Assessing the Role of Sampling Uncertainty When Predicting Behavioral Responses of Tagged Cetaceans Exposed to Naval Sonar",
            "abstract": "Concerns over cetacean mortality events coincident with maritime warfare exercises have motivated efforts to characterize the effects of anthropogenic noise on free-ranging whales and dolphins. By monitoring the movement, diving, and acoustic behaviors of individual whales before, during, and after sound exposure, behavioral response studies (BRSs) have supported significant progress in our understanding of the sensitivity of various cetacean species to high-powered naval sonar signals. However, differences in the designs and sampling capabilities of animal-borne tags typically used in BRS experiments prompt questions about the influence of data resolution in quantitative assessments of noise impacts. We conducted simulations to examine how uncertainty in the acoustic dose either measured on high-resolution multi-sensor biologging tags or modeled from position-transmitting satellite telemetry tags may affect predictions of behavioral responses in Cuvier\u2019s beaked whales (Ziphius cavirostris) exposed to low- and mid-frequency active sonar. We considered an array of scenarios representative of real-world BRSs and used posterior estimates of dose-response functions obtained under an established Bayesian hierarchical modeling framework to explore the consequences of different tag choices for management decision-making. Our results indicate that (1) the zone of impact from a sonar source is under-estimated in most test conditions, (2) substantial reductions in the uncertainty surrounding dose-response relationships are possible at higher sample sizes, and (3) this largely holds true irrespective of tag choice under the scenarios considered, unless positional fixes from satellite tags are consistently poor. Strategic monitoring approaches that combine both archival biologging and satellite biotelemetry are essential for characterizing complex patterns of behavioral change in cetaceans exposed to increasing levels of acoustic disturbance. We suggest ways in which BRS protocols can be optimized to curtail the effects of uncertainty.",
            "year": 2021,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Simulations to examine how uncertainty in the acoustic dose either measured on high-resolution multi-sensor biologging tags or modeled from position-transmitting satellite telemetry tags may affect predictions of behavioral responses in Cuvier\u2019s beaked whales exposed to low- and mid-frequency active sonar suggest ways in which BRS protocols can be optimized to curtail the effects of uncertainty."
            },
            "score": 2
        },
        {
            "id": "5d2b77ae8508e277fe9b840a471b7dfb00e806ff",
            "paperId": "5d2b77ae8508e277fe9b840a471b7dfb00e806ff",
            "title": "Large Language Models are Complex Table Parsers",
            "abstract": "With the Generative Pre-trained Transformer 3.5 (GPT-3.5) exhibiting remarkable reasoning and comprehension abilities in Natural Language Processing (NLP), most Question Answering (QA) research has primarily centered around general QA tasks based on GPT, neglecting the specific challenges posed by Complex Table QA. In this paper, we propose to incorporate GPT-3.5 to address such challenges, in which complex tables are reconstructed into tuples and specific prompt designs are employed for dialogues. Specifically, we encode each cell's hierarchical structure, position information, and content as a tuple. By enhancing the prompt template with an explanatory description of the meaning of each tuple and the logical reasoning process of the task, we effectively improve the hierarchical structure awareness capability of GPT-3.5 to better parse the complex tables. Extensive experiments and results on Complex Table QA datasets, i.e., the open-domain dataset HiTAB and the aviation domain dataset AIT-QA show that our approach significantly outperforms previous work on both datasets, leading to state-of-the-art (SOTA) performance.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper enhances the prompt template with an explanatory description of the meaning of each tuple and the logical reasoning process of the task, which effectively improves the hierarchical structure awareness capability of GPT-3.5 to better parse the complex tables."
            },
            "score": 2
        },
        {
            "id": "166802fb539fafbf4be0d6956defd26d70ab2cfe",
            "paperId": "166802fb539fafbf4be0d6956defd26d70ab2cfe",
            "title": "LAMM: Label Alignment for Multi-Modal Prompt Learning",
            "abstract": "With the success of pre-trained visual-language (VL) models such as CLIP in visual representation tasks, transferring pre-trained models to downstream tasks has become a crucial paradigm. Recently, the prompt tuning paradigm, which draws inspiration from natural language processing (NLP), has made significant progress in VL field. However, preceding methods mainly focus on constructing prompt templates for text and visual inputs, neglecting the gap in class label representations between the VL models and downstream tasks. To address this challenge, we introduce an innovative label alignment method named \\textbf{LAMM}, which can dynamically adjust the category embeddings of downstream datasets through end-to-end training. Moreover, to achieve a more appropriate label distribution, we propose a hierarchical loss, encompassing the alignment of the parameter space, feature space, and logits space. We conduct experiments on 11 downstream vision datasets and demonstrate that our method significantly improves the performance of existing multi-modal prompt learning models in few-shot scenarios, exhibiting an average accuracy improvement of 2.31(\\%) compared to the state-of-the-art methods on 16 shots. Moreover, our methodology exhibits the preeminence in continual learning compared to other prompt tuning methods. Importantly, our method is synergistic with existing prompt tuning methods and can boost the performance on top of them. Our code and dataset will be publicly available at https://github.com/gaojingsheng/LAMM.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces an innovative label alignment method named LAMM, which can dynamically adjust the category embeddings of downstream datasets through end-to-end training and proposes a hierarchical loss, encompassing the alignment of the parameter space, feature space, and logits space."
            },
            "score": 2
        },
        {
            "id": "6833c4fb86f59b9230ec262fe14315f39e9b6456",
            "paperId": "6833c4fb86f59b9230ec262fe14315f39e9b6456",
            "title": "An intellectual system for supporting decision making in the control of the borring process",
            "abstract": "The problem of development of the method of identification of complications arising in the process of drilling of oil and gas wells, which operates under the conditions of a priori and current uncertainty under the influence of various perturbations based on methods of fuzzy set theory and fuzzy logic, is considered.\nA methodological approach to the estimation of the level of complications in the drilling of oil and gas wells, based on the principles of linguistic parameters of the drilling process, linguistic and hierarchical knowledge about the complications in the drilling of wells is proposed.\nMathematical models of a controlled object have been developed that, unlike deterministic mathematical models, allow to describe in natural language the cause and effect relationships between the parameters of the drilling process and the possible complication. These models reflect the logic of the operator's reasoning with the involvement of non-numerical and fuzzy information from an expert to formalize Fuzzy Logic decision-making procedures using the parameters and indicators of the oil and gas drilling process.\nThe structure of the decision support system for controlling the drilling of wells in the conditions of complications is proposed.\nThe results of simulation modeling of the developed methods of modeling of complications based on the methods of fuzzy set theory and fuzzy logic are presented. Their advantages over the well-known in accuracy of the tasks of identification of an estimation and control in the conditions of uncertainty concerning structure and parameters of object are shown.\nThe real complications have been identified, the elimination of which will increase the level of safety of the drilling process. It is shown that the developed methods and models can find application for modeling and identification of a wide class of complications on drilling rigs operating under the conditions of a priori and current uncertainty regarding their structure, parameters and geographic environment.",
            "year": 2020,
            "citationCount": 0,
            "tldr": null,
            "score": 1
        },
        {
            "id": "65f7fb361e2fccd2097601d5ebe97d0974a4a73c",
            "paperId": "65f7fb361e2fccd2097601d5ebe97d0974a4a73c",
            "title": "A Compartment Model of Human Mobility and Early Covid-19 Dynamics in NYC",
            "abstract": "In this paper, we build a mechanistic system to understand the relation between a reduction in human mobility and Covid-19 spread dynamics within New York City. To this end, we propose a multivariate compartmental model that jointly models smartphone mobility data and case counts during the first 90 days of the epidemic. Parameter calibration is achieved through the formulation of a general Bayesian hierarchical model to provide uncertainty quantification of resulting estimates. The open-source probabilistic programming language Stan is used for the requisite computation. Through sensitivity analysis and out-of-sample forecasting, we find our simple and interpretable model provides evidence that reductions in human mobility altered case dynamics.",
            "year": 2021,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A simple and interpretable model is built that provides evidence that reductions in human mobility altered case dynamics, and jointly models smartphone mobility data and case counts during the first 90 days of the epidemic."
            },
            "score": 1
        },
        {
            "id": "74c7343d91d5464c27ca407fd504b07e690363be",
            "paperId": "74c7343d91d5464c27ca407fd504b07e690363be",
            "title": "Combining Confidence Elicitation and Sample-based Methods for Uncertainty Quantification in Misinformation Mitigation",
            "abstract": "Large Language Models have emerged as prime candidates to tackle misinformation mitigation. However, existing approaches struggle with hallucinations and overconfident predictions. We propose an uncertainty quantification framework that leverages both direct confidence elicitation and sampled-based consistency methods to provide better calibration for NLP misinformation mitigation solutions. We first investigate the calibration of sample-based consistency methods that exploit distinct features of consistency across sample sizes and stochastic levels. Next, we evaluate the performance and distributional shift of a robust numeric verbalization prompt across single vs. two-step confidence elicitation procedure. We also compare the performance of the same prompt with different versions of GPT and different numerical scales. Finally, we combine the sample-based consistency and verbalized methods to propose a hybrid framework that yields a better uncertainty estimation for GPT models. Overall, our work proposes novel uncertainty quantification methods that will improve the reliability of Large Language Models in misinformation mitigation applications.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes an uncertainty quantification framework that leverages both direct confidence elicitation and sampled-based consistency methods to provide better calibration for NLP misinformation mitigation solutions to improve the reliability of Large Language Models in misinformation mitigation applications."
            },
            "score": 1
        },
        {
            "id": "54e8473fca9bfc9395f3c11bfb2adb00e5816ebd",
            "paperId": "54e8473fca9bfc9395f3c11bfb2adb00e5816ebd",
            "title": "HierCC: Hierarchical RDMA Congestion Control",
            "abstract": "RDMA has been increasingly deployed in data centers to decrease latency and CPU utilization. However, existing RDMA congestion control schemes fail to address instantaneous large queue build-up or bandwidth under-utilization associated with frequent traffic bursty. In this paper, we argue that traffic uncertainty is the essential reason that constrains data center congestion control from simultaneously achieving high throughput and deterministic latency. Since aggregated flows within the same rack are relatively long-lived, we propose HierCC, which aggregates flows destined to the same IP in a rack and hierarchically controls the rate of flows. The rate of aggregate flows between racks is controlled by a credit-based congestion control mechanism. Then the bandwidth obtained by an aggregate flow in a rack is allocated to the corresponding individual flows from that rack promptly and accurately. We evaluate HierCC using SystemC and large-scale NS3 simulations. Results indicate that HierCC can significantly mitigate buffer usage and reduce the 99th percentile FCT by up to 20% and 40% compared with HPCC and DCQCN under a realistic workload, respectively.",
            "year": 2021,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "HierCC is proposed, which aggregates flows destined to the same IP in a rack and hierarchically controls the rate of flows and can significantly mitigate buffer usage and reduce the 99th percentile FCT by up to 20% and 40% compared with HPCC and DCQCN under a realistic workload."
            },
            "score": 1
        },
        {
            "id": "ed5f958bc7fc8aba743fd59b54e0a8365ff5b6b8",
            "paperId": "ed5f958bc7fc8aba743fd59b54e0a8365ff5b6b8",
            "title": "Optimal Hierarchical Energy Management System With Plug-and-Play Capability for Neighborhood Home Microgrids",
            "abstract": "With the increasing adoption of renewable energy sources, researchers are actively engaged in the development of smart grid technologies. This study introduces a home energy management system (HEMS) designed to optimize home microgrid (HMG) operation by integrating electric vehicles (EVs) through a hierarchical three-level distributed control approach. The proposed system governs the power transfer of distributed energy sources through converter control at the primary level, while employing an intelligent optimal power exchange technique for EVs and energy storage (ES) at the secondary level. To achieve optimal energy demand management, the system employs a two-layer strategy, comprising offline and online scheduling, utilizing particle swarm optimization and artificial neural network to reduce computing time. The offline scheduling formulates a deterministic optimization model to minimize HMG operation costs, thereby extending ES lifetime through optimal load-sharing. The online scheduling approach enables real-time performance under uncertainty and accommodates plug-and-play renewable energy sources, HMGs, loads, and batteries. Furthermore, the proposed algorithm is tested using MATLAB/Simulink simulations over a 24-h period, affirming its ability to respond promptly to HEMS status changes and adapt decision-making methods to new conditions with improved training time and reduced mean square error. In addition, experimental studies on two laboratory-based HMGs demonstrate the feasibility of the proposed HEMS.",
            "year": 2024,
            "citationCount": 0,
            "tldr": null,
            "score": 1
        },
        {
            "id": "6fe5a62053253fd5338e8ca6ff4fe3bd59dd8e69",
            "paperId": "6fe5a62053253fd5338e8ca6ff4fe3bd59dd8e69",
            "title": "Hierarchical Bayesian method for constraining the neutron star equation of state with an ensemble of binary neutron star postmerger remnants",
            "abstract": "Binary neutron star (BNS) post-merger gravitational-wave emission can occur in the aftermath of a BNS merger -- provided the system avoids prompt collapse to a black hole -- as a quasistable hypermassive remnant experiences quadrupolar oscillations and non-axisymmetric deformations. The post-merger gravitational-wave spectrum possesses a characteristic peak frequency that has been shown to be dependent on the binary chirp mass and the neutron star equation of state (EoS), rendering post-merger gravitational waves a powerful tool for constraining neutron star composition. Unfortunately, the BNS post-merger signal is emitted at high ($\\gtrsim 1.5$ kHz) frequencies, where ground-based gravitational wave detectors suffer from reduced sensitivity. It is therefore unlikely that post-merger signals will be detected with sufficient signal-to-noise ratio (SNR) until the advent of next-generation detectors. However, by employing empirical relations derived from numerical relativity simulations, we can combine information across an ensemble of BNS mergers, allowing us to obtain EoS constraints with many low-SNR signals. We present a hierarchical Bayesian method for deriving constraints on $R_{1.6}$, the radius of a 1.6$\\mathrm{M_{\\odot}}$ neutron star, through an ensemble analysis of binary neutron star mergers. We apply this method to simulations of the next two LIGO-Virgo-KAGRA observing runs, O4 and O5, as well as an extended 4-year run at A+ sensitivity, demonstrating the potential of our approach to yield EoS information from the post-merger signal with current-generation detectors. The A+ 4-year scenario is predicted to improve the constraint on $R_{1.6}$ from the currently available multimessenger-based 95\\% credible interval (C.I.) uncertainty of $R_{1.6}=12.07^{+0.98}_{-0.77}$ km to $R_{1.6}=11.91^{+0.80}_{-0.56}$ km, a 22% reduction of the 95% C.I. width.",
            "year": 2022,
            "citationCount": 3,
            "tldr": null,
            "score": 1
        }
    ],
    "novelty": "yes"
}