{
    "topic_description": "novel prompting methods that can better quantify uncertainty or calibrate the confidence of large language models",
    "idea_name": "Uncertainty-Guided Iterative Refinement",
    "raw_idea": {
        "Problem": "Large language models often generate outputs that are inconsistent or contradictory, especially when dealing with complex or ambiguous inputs. This leads to overconfident predictions that may not align with the model's true uncertainty.",
        "Existing Methods": "Current approaches to addressing this issue include using beam search, top-k sampling, or nucleus sampling to generate multiple candidate outputs. However, these methods do not explicitly consider the model's uncertainty during the generation process.",
        "Motivation": "By iteratively refining the model's output based on its estimated uncertainty, we can encourage the LLM to generate more consistent and calibrated responses. This approach draws inspiration from the human reasoning process, where individuals often revisit and refine their answers based on their confidence in different aspects of the problem.",
        "Proposed Method": "We propose Uncertainty-Guided Iterative Refinement (UGIR), a method that generates an initial output and then iteratively refines it based on the model's estimated uncertainty. At each iteration, the LLM is prompted to identify the parts of its previous response that it is least confident about and to generate a refined response focusing on improving those aspects. The prompt includes instructions like: \"Given your previous response, identify the parts you are least confident about and generate a refined response addressing those uncertainties.\" This process is repeated for a fixed number of iterations or until the model's uncertainty falls below a specified threshold.",
        "Experiment Plan": "Evaluate UGIR on tasks such as open-ended question answering, story generation, and dialogue systems. Compare the consistency, coherence, and calibration of the generated outputs with baseline methods such as beam search and nucleus sampling. Use metrics such as self-BLEU, perplexity, and human evaluation to assess the quality of the generated outputs, and use uncertainty-based metrics such as Brier score and expected calibration error to evaluate the model's calibration."
    },
    "full_experiment_plan": {
        "Title": "Uncertainty-Guided Iterative Refinement for Improved Consistency and Calibration in Large Language Models",
        "Problem Statement": "Large language models often generate outputs that are inconsistent or contradictory, especially when dealing with complex or ambiguous inputs. This leads to overconfident predictions that may not align with the model's true uncertainty.",
        "Motivation": "Current approaches to addressing this issue include using beam search, top-k sampling, or nucleus sampling to generate multiple candidate outputs. However, these methods do not explicitly consider the model's uncertainty during the generation process. By iteratively refining the model's output based on its estimated uncertainty, we can encourage the LLM to generate more consistent and calibrated responses. This approach draws inspiration from the human reasoning process, where individuals often revisit and refine their answers based on their confidence in different aspects of the problem.",
        "Proposed Method": "We propose Uncertainty-Guided Iterative Refinement (UGIR), a method that generates an initial output and then iteratively refines it based on the model's estimated uncertainty. At each iteration, the LLM is prompted to identify the parts of its previous response that it is least confident about and to generate a refined response focusing on improving those aspects. The prompt includes instructions like: \"Given your previous response, identify the parts you are least confident about and generate a refined response addressing those uncertainties.\" This process is repeated for a fixed number of iterations or until the model's uncertainty falls below a specified threshold.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "Evaluate UGIR on tasks such as open-ended question answering (ELI5, TellMeWhy), story generation (WritingPrompts), and dialogue systems (PersonaChat, DailyDialog). These datasets cover a range of open-ended and complex tasks where consistency and calibration are important.",
            "Step 2: Construct Prompts": "1. Baseline Prompts:\n   a. Beam Search: Generate multiple candidate outputs using beam search.\n   b. Top-k Sampling: Generate multiple candidate outputs using top-k sampling.\n   c. Nucleus Sampling: Generate multiple candidate outputs using nucleus sampling.\n2. UGIR Prompts:\n   a. Initial Generation: Prompt the LLM to generate an initial response to the input.\n   b. Uncertainty Identification: Prompt the LLM to identify the parts of its previous response that it is least confident about. Example: \"Given your previous response, identify the parts you are least confident about and explain why.\"\n   c. Refinement: Prompt the LLM to generate a refined response addressing the identified uncertainties. Example: \"Based on the uncertainties you identified, generate a refined response that improves upon those aspects.\"",
            "Step 3: Select Models": "Evaluate UGIR using GPT-3.5 (text-davinci-003) and GPT-4 from the OpenAI API. These models have demonstrated strong performance on open-ended tasks and support fine-grained control through prompting.",
            "Step 4: Implement UGIR": "1. For each input in the selected datasets, generate an initial response using the chosen LLM.\n2. Iteratively refine the response:\n   a. Prompt the LLM to identify the parts of its previous response that it is least confident about.\n   b. Prompt the LLM to generate a refined response addressing the identified uncertainties.\n   c. Repeat steps a-b for a fixed number of iterations or until the model's uncertainty falls below a specified threshold.\n3. Evaluate the quality of the final refined response.",
            "Step 5: Evaluate Results": "1. Consistency: Use metrics such as self-BLEU and cosine similarity to measure the consistency between the initial and refined responses.\n2. Coherence: Use metrics such as perplexity and human evaluation to assess the coherence and fluency of the generated outputs.\n3. Calibration: Use uncertainty-based metrics such as Brier score and expected calibration error to evaluate the model's calibration.\n4. Compare the performance of UGIR with the baseline methods (beam search, top-k sampling, nucleus sampling) across the selected datasets and metrics."
        },
        "Test Case Examples": {
            "Test Case 1": {
                "Input": "What are the potential long-term effects of climate change on biodiversity?",
                "Baseline Output (Beam Search)": "1. Climate change can lead to habitat loss and fragmentation, which can cause species extinctions and reduced biodiversity.\n2. Changing temperatures and precipitation patterns may cause species to shift their ranges, leading to new species interactions and potential invasive species problems.\n3. Ocean acidification due to increased carbon dioxide absorption can negatively impact marine biodiversity, particularly coral reefs and shellfish.",
                "UGIR Output": "Initial Response: Climate change can have severe long-term effects on biodiversity. As temperatures rise and precipitation patterns change, many species may struggle to adapt to their altered habitats. This can lead to population declines, species extinctions, and overall reduced biodiversity. Ocean acidification caused by increased absorption of atmospheric carbon dioxide can also harm marine ecosystems, particularly coral reefs and the diverse species they support.\n\nUncertainty Identification: I am less confident about the specific mechanisms through which climate change can affect biodiversity, such as the role of habitat fragmentation and the potential for species to shift their ranges. I am also uncertain about the relative impacts of climate change on different ecosystems and taxonomic groups.\n\nRefined Response: Climate change can impact biodiversity through several mechanisms. Habitat loss and fragmentation, caused by factors such as sea-level rise and more frequent extreme weather events, can isolate populations and reduce genetic diversity. As species attempt to adapt to changing conditions by shifting their ranges, new species interactions may emerge, potentially leading to competition or predation pressures. Phenological mismatches, where the timing of key life events becomes desynchronized, can also disrupt species interactions and reduce fitness. The effects of climate change are likely to vary across ecosystems, with some, such as polar regions and coral reefs, being particularly vulnerable. While some species may be able to adapt, many others, especially those with limited dispersal abilities or narrow ecological niches, face a higher risk of extinction.",
                "Explanation": "The baseline output generated by beam search provides a high-level overview of the potential effects of climate change on biodiversity. However, it lacks depth and does not explore the specific mechanisms or uncertainties involved. In contrast, the UGIR output starts with an initial response that acknowledges the complexity of the issue and the potential for severe consequences. The uncertainty identification step highlights areas where the model is less confident, such as the specific mechanisms and relative impacts across ecosystems. The refined response addresses these uncertainties by providing more detailed information on habitat fragmentation, species range shifts, phenological mismatches, and the vulnerability of different ecosystems. By iteratively refining the response based on identified uncertainties, the UGIR output provides a more comprehensive and nuanced answer to the question."
            },
            "Test Case 2": {
                "Input": "Persona: I am a 60-year-old retiree who enjoys birdwatching and gardening. I have two grandchildren and love spending time with my family.\n\nUser: Hi there! I'm looking for some advice on how to get started with birdwatching. Do you have any tips for a beginner like me?",
                "Baseline Output (Nucleus Sampling)": "Retiree: Hello! It's wonderful to hear that you're interested in birdwatching. As someone who has been enjoying this hobby for years, I'd be happy to share some tips with you.\n\nFirst, invest in a good pair of binoculars. You don't need to break the bank, but a decent pair will make a big difference in your ability to observe birds from a distance. Look for binoculars with a magnification of 7x to 10x and an objective lens diameter of 30mm to 50mm.\n\nNext, get a field guide specific to your region. This will help you identify the birds you see based on their appearance, habitat, and behavior. Many field guides also include information on bird calls and songs, which can be helpful for identification.\n\nWhen you're starting out, focus on learning the common birds in your area. Start by observing birds in your backyard or local park, and gradually expand your range as you become more comfortable with identification.\n\nFinally, be patient and persistent. Birdwatching requires a bit of luck and a lot of patience. Don't get discouraged if you don't see a wide variety of birds right away. Keep at it, and over time, you'll develop a keen eye for spotting even the most elusive species.\n\nI hope these tips help you get started on your birdwatching journey. Feel free to reach out if you have any other questions!",
                "UGIR Output": "Initial Response: Hello! It's great to hear that you're interested in birdwatching. As someone who has been enjoying this hobby for years, I'd be happy to share some advice.\n\nFirst, get a good pair of binoculars. They don't need to be expensive, but they should have a magnification of at least 7x and an objective lens diameter of around 35mm. This will help you observe birds from a distance.\n\nNext, purchase a field guide for your region. This will assist you in identifying birds based on their appearance and habitat. Start by learning the common birds in your area, such as those in your backyard or local park.\n\nFinally, be patient and persistent. Birdwatching takes time and practice, so don't get discouraged if you don't see a wide variety of birds right away. Keep at it, and you'll gradually improve your skills.\n\nUncertainty Identification: I am less confident about the specific magnification and objective lens diameter recommendations for binoculars, as there may be a range of suitable options. I also feel uncertain about whether a field guide is the best resource for beginners, as there might be more accessible alternatives like mobile apps or websites.\n\nRefined Response: Hello! It's wonderful that you're interested in birdwatching. As someone who has been enjoying this hobby for years, I'm happy to share some advice.\n\nFirst, invest in a decent pair of binoculars. Look for a pair with a magnification between 7x and 10x and an objective lens diameter between 30mm and 50mm. This range should provide a good balance between magnification and light-gathering ability, making it easier to observe birds from a distance. Popular brands like Nikon, Vortex, and Celestron offer binoculars suitable for beginners.\n\nNext, consider getting a field guide specific to your region or a bird identification app like Merlin Bird ID or iBird Pro. These resources will help you identify birds based on their appearance, habitat, and behavior. Field guides are comprehensive and often include information on bird calls and songs, while apps can be more user-friendly and accessible in the field.\n\nWhen starting out, focus on learning the common birds in your area. Begin by observing birds in your backyard, local park, or nearby nature reserves. As you become more comfortable with identification, gradually expand your range to explore different habitats.\n\nRemember to be patient and persistent. Birdwatching requires a combination of luck, skill, and patience. Don't get discouraged if you don't see a wide variety of birds right away. With practice and dedication, you'll develop a keen eye for spotting even the most elusive species.\n\nI hope these tips help you get started on your birdwatching journey. Feel free to reach out if you have any other questions!",
                "Explanation": "The baseline output generated by nucleus sampling provides a solid set of tips for a beginner birdwatcher. However, it lacks some specificity and doesn't address potential uncertainties or alternative resources. The UGIR output begins with an initial response that covers the main points, such as getting binoculars, using a field guide, focusing on common birds, and being patient. The uncertainty identification step highlights areas where the model is less confident, such as the specific binocular recommendations and the usefulness of field guides compared to other resources. The refined response addresses these uncertainties by providing a range of suitable binocular specifications and suggesting alternative resources like mobile apps. It also expands on the advice by mentioning popular binocular brands, discussing the advantages of field guides and apps, and suggesting locations for birdwatching. By iteratively refining the response based on identified uncertainties, the UGIR output offers a more comprehensive and tailored set of advice for the beginner birdwatcher."
            }
        },
        "Fallback Plan": "If the proposed UGIR method does not yield significant improvements in consistency, coherence, or calibration compared to the baseline methods, consider the following alternative approaches:\n\n1. Analyze the uncertainty identification step:\n   a. Examine the parts of the response that the model identifies as uncertain and assess whether they align with human judgment.\n   b. Experiment with different prompts or techniques for eliciting uncertainty estimates from the model, such as using numeric confidence scores or asking the model to rank the certainty of different parts of the response.\n\n2. Investigate alternative uncertainty estimation methods:\n   a. Explore techniques like Monte Carlo dropout, ensemble methods, or Bayesian neural networks to estimate uncertainty in language models.\n   b. Compare the performance of these methods with the proposed UGIR approach and assess their impact on consistency, coherence, and calibration.\n\n3. Refine the iterative refinement process:\n   a. Experiment with different numbers of refinement iterations to find the optimal balance between improvement and computational cost.\n   b. Investigate alternative prompts or strategies for guiding the model to refine its responses based on identified uncertainties, such as providing more specific instructions or examples.\n\n4. Conduct error analysis:\n   a. Analyze cases where UGIR fails to improve consistency, coherence, or calibration and identify common patterns or challenges.\n   b. Use insights from the error analysis to inform further modifications to the UGIR approach or to develop new strategies for addressing these issues.\n\n5. Explore fine-tuning approaches:\n   a. Investigate whether fine-tuning the language model on a smaller dataset relevant to the target task can improve its performance and calibration.\n   b. Compare the performance of fine-tuned models with the UGIR approach and assess the trade-offs in terms of computational cost and generalizability.\n\nIf these alternative approaches do not yield satisfactory results, consider shifting the focus of the project to an analysis of the challenges and limitations of improving consistency and calibration in large language models. This could involve:\n\n1. Conducting a systematic comparison of different uncertainty estimation and calibration methods across a range of tasks and datasets.\n2. Analyzing the factors that contribute to inconsistency and poor calibration in language models, such as dataset biases, model architectures, or training objectives.\n3. Proposing new evaluation metrics or frameworks for assessing consistency and calibration in language models, taking into account the specific challenges of open-ended generation tasks.\n4. Discussing the implications of these findings for the development and deployment of language models in real-world applications, and outlining potential directions for future research."
    },
    "novelty_queries": [
        "KeywordQuery(\"uncertainty guided iterative refinement language models\")",
        "KeywordQuery(\"consistency calibration language models\")",
        "KeywordQuery(\"uncertainty estimation language models\")",
        "KeywordQuery(\"Uncertainty-Guided Iterative Refinement NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "8f7297454d7f44365b9bcda5ebb9439a43daf5e6",
            "paperId": "8f7297454d7f44365b9bcda5ebb9439a43daf5e6",
            "title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
            "abstract": "Empowering large language models to accurately express confidence in their answers is essential for trustworthy decision-making. Previous confidence elicitation methods, which primarily rely on white-box access to internal model information or model fine-tuning, have become less suitable for LLMs, especially closed-source commercial APIs. This leads to a growing need to explore the untapped area of black-box approaches for LLM uncertainty estimation. To better break down the problem, we define a systematic framework with three components: prompting strategies for eliciting verbalized confidence, sampling methods for generating multiple responses, and aggregation techniques for computing consistency. We then benchmark these methods on two key tasks-confidence calibration and failure prediction-across five types of datasets (e.g., commonsense and arithmetic reasoning) and five widely-used LLMs including GPT-4 and LLaMA 2 Chat. Our analysis uncovers several key insights: 1) LLMs, when verbalizing their confidence, tend to be overconfident, potentially imitating human patterns of expressing confidence. 2) As model capability scales up, both calibration and failure prediction performance improve. 3) Employing our proposed strategies, such as human-inspired prompts, consistency among multiple responses, and better aggregation strategies can help mitigate this overconfidence from various perspectives. 4) Comparisons with white-box methods indicate that while white-box methods perform better, the gap is narrow, e.g., 0.522 to 0.605 in AUROC. Despite these advancements, none of these techniques consistently outperform others, and all investigated methods struggle in challenging tasks, such as those requiring professional knowledge, indicating significant scope for improvement. We believe this study can serve as a strong baseline and provide insights for eliciting confidence in black-box LLMs.",
            "year": 2023,
            "citationCount": 97,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study defines a systematic framework with three components: prompting strategies for eliciting verbalized confidence, sampling methods for generating multiple responses, and aggregation techniques for computing consistency and believes it can serve as a strong baseline and provide insights for eliciting confidence in black-box LLMs."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is improving the consistency and calibration of large language models by iteratively refining the model's output based on its estimated uncertainty. The approach involves prompting the model to identify uncertain parts of its response and generate refined responses addressing those uncertainties.\n\nThe research problem in the paper is evaluating the ability of large language models to express confidence in their answers using black-box approaches. The approach involves exploring different prompting strategies, sampling methods, and aggregation techniques to elicit and compute verbalized confidence.\n\nWhile both works focus on the uncertainty and calibration of large language models, the proposal aims to improve these aspects through an iterative refinement process, whereas the paper focuses on evaluating the models' ability to express confidence using various black-box methods. The proposal does not directly address the problem of eliciting verbalized confidence, which is the main focus of the paper.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "992c554b1bf343eef3509579930b2552f1b6f1db",
            "paperId": "992c554b1bf343eef3509579930b2552f1b6f1db",
            "title": "Calibrating Large Language Models with Sample Consistency",
            "abstract": "Accurately gauging the confidence level of Large Language Models' (LLMs) predictions is pivotal for their reliable application. However, LLMs are often uncalibrated inherently and elude conventional calibration techniques due to their proprietary nature and massive scale. In this work, we explore the potential of deriving confidence from the distribution of multiple randomly sampled model generations, via three measures of consistency. We perform an extensive evaluation across various open and closed-source models on nine reasoning datasets. Results show that consistency-based calibration methods outperform existing post-hoc approaches. Meanwhile, we find that factors such as intermediate explanations, model scaling, and larger sample sizes enhance calibration, while instruction-tuning makes calibration more difficult. Moreover, confidence scores obtained from consistency have the potential to enhance model performance. Finally, we offer practical guidance on choosing suitable consistency metrics for calibration, tailored to the characteristics of various LMs.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An extensive evaluation across various open and closed-source models on nine reasoning datasets shows that consistency-based calibration methods outperform existing post-hoc approaches and offers practical guidance on choosing suitable consistency metrics for calibration, tailored to the characteristics of various LMs."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the consistency and calibration of large language models by iteratively refining the model's output based on its estimated uncertainty. The paper explores the potential of deriving confidence from the distribution of multiple randomly sampled model generations, via three measures of consistency.\n\nWhile both the project proposal and the paper focus on improving the calibration of large language models, their approaches differ. The project proposal suggests an iterative refinement method guided by the model's uncertainty, while the paper investigates using consistency measures derived from multiple sampled outputs to calibrate the model.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "ab4ce5dda7ad4d9032995c9c049a89d65723c6aa",
            "paperId": "ab4ce5dda7ad4d9032995c9c049a89d65723c6aa",
            "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
            "abstract": "A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated. However, the most widely-used LMs are fine-tuned with reinforcement learning from human feedback (RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated. In light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50%.",
            "year": 2023,
            "citationCount": 96,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "For RLHF-LMs such as ChatGPT, GPT-4, and Claude, it is found that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50%."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the consistency and calibration of large language models through iterative refinement guided by uncertainty estimates. The approach involves generating an initial output, identifying uncertain parts, and refining the response accordingly.\n\nThe research problem in the paper is improving the calibration of confidence scores from language models fine-tuned with human feedback. The approach involves evaluating various methods for extracting well-calibrated confidence scores, such as using verbalized confidences emitted as output tokens.\n\nWhile both works aim to improve the calibration of language models, the proposal focuses on iterative refinement based on uncertainty estimates to improve consistency and calibration, whereas the paper evaluates methods for extracting well-calibrated confidence scores from fine-tuned models. The approaches and specific research problems differ.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "444f3b7293b85b7d37600372941a289f9163abd1",
            "paperId": "444f3b7293b85b7d37600372941a289f9163abd1",
            "title": "LM-Polygraph: Uncertainty Estimation for Language Models",
            "abstract": "Recent advancements in the capabilities of large language models (LLMs) have paved the way for a myriad of groundbreaking applications in various fields. However, a significant challenge arises as these models often\"hallucinate\", i.e., fabricate facts without providing users an apparent means to discern the veracity of their statements. Uncertainty estimation (UE) methods are one path to safer, more responsible, and more effective use of LLMs. However, to date, research on UE methods for LLMs has been focused primarily on theoretical rather than engineering contributions. In this work, we tackle this issue by introducing LM-Polygraph, a framework with implementations of a battery of state-of-the-art UE methods for LLMs in text generation tasks, with unified program interfaces in Python. Additionally, it introduces an extendable benchmark for consistent evaluation of UE techniques by researchers, and a demo web application that enriches the standard chat dialog with confidence scores, empowering end-users to discern unreliable responses. LM-Polygraph is compatible with the most recent LLMs, including BLOOMz, LLaMA-2, ChatGPT, and GPT-4, and is designed to support future releases of similarly-styled LMs.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LM-Polygraph is introduced, a framework with implementations of a battery of state-of-the-art UE methods for LLMs in text generation tasks, with unified program interfaces in Python, and introduces an extendable benchmark for consistent evaluation of UE techniques by researchers."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the consistency and calibration of large language models in open-ended generation tasks, while the approach is to use uncertainty-guided iterative refinement. The research problem in the paper is estimating the uncertainty of language models to enable safer and more responsible use, while the approach is to introduce a framework with implementations of uncertainty estimation methods.\n\nThe proposal focuses on improving language model outputs through an iterative process guided by uncertainty, whereas the paper focuses on providing a framework for evaluating and applying uncertainty estimation techniques. While both deal with uncertainty in language models, the specific research problems and approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "be8c90bca14d59f180f40a41126b7cd8c29c5d4e",
            "paperId": "be8c90bca14d59f180f40a41126b7cd8c29c5d4e",
            "title": "Uncertainty Quantification for In-Context Learning of Large Language Models",
            "abstract": "In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM's response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM's response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model's configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-context learning in a plug-and-play fashion. Extensive experiments are conducted to demonstrate the effectiveness of the decomposition. The code and data are available at: https://github.com/lingchen0331/UQ_ICL.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations and ambiguities tied to the model's configurations (epistemic uncertainty)."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the consistency and calibration of large language models by iteratively refining the model's output based on its estimated uncertainty. The approach is to use uncertainty-guided iterative refinement (UGIR) to generate an initial output and then iteratively refine it based on the model's estimated uncertainty.\n\nThe research problem in the paper is quantifying the uncertainty in large language models' responses associated with in-context learning. The approach is to propose a novel formulation and corresponding estimation method to quantify both aleatoric uncertainty (stemming from provided demonstrations) and epistemic uncertainty (tied to the model's configurations).\n\nThe proposal focuses on improving consistency and calibration of LLMs, while the paper focuses on quantifying uncertainty in LLMs' responses. The proposal uses iterative refinement guided by uncertainty, while the paper proposes a new formulation and estimation method for uncertainty quantification.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "7adb88771376c2a31688e3b0395b0550a35b824d",
            "paperId": "7adb88771376c2a31688e3b0395b0550a35b824d",
            "title": "Uncertainty Decomposition and Quantification for In-Context Learning of Large Language Models",
            "abstract": "In-context learning has emerged as a ground-breaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM\u2019s response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM\u2019s response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model\u2019s configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-context learning in a plug-and-play fashion. Extensive experiments are conducted to demonstrate the effectiveness of the decomposition. The code and data are available at: https://github. com/lingchen0331/UQ_ICL .",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work dives into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations and ambiguities tied to the model\u2019s configurations (epistemic uncertainty)."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the consistency and calibration of large language models by iteratively refining the model's output based on its estimated uncertainty. The approach is to use uncertainty-guided iterative refinement (UGIR) to generate an initial output and then iteratively refine it based on the model's estimated uncertainty.\n\nThe research problem in the paper is quantifying the predictive uncertainty of large language models associated with in-context learning. The approach is to propose a novel formulation and corresponding estimation method to quantify both aleatoric uncertainty (stemming from provided demonstrations) and epistemic uncertainty (tied to the model's configurations).\n\nThe proposal focuses on improving consistency and calibration of LLMs through iterative refinement, while the paper focuses on quantifying and decomposing the predictive uncertainty in in-context learning. Although both deal with uncertainty in LLMs, the specific research problems and approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "44d74b0d77b4056ddd4c6611a76711c8bab2e0a7",
            "paperId": "44d74b0d77b4056ddd4c6611a76711c8bab2e0a7",
            "title": "Dehallucinating Large Language Models Using Formal Methods Guided Iterative Prompting",
            "abstract": "Large language models (LLMs) such as ChatGPT have been trained to generate human-like responses to natural language prompts. LLMs use a vast corpus of text data for training, and can generate coherent and contextually relevant responses to a wide range of questions and statements. Despite this remarkable progress, LLMs are prone to hallucinations making their application to safety-critical applications such as autonomous systems difficult. The hallucinations in LLMs refer to instances where the model generates responses that are not factually accurate or contextually appropriate. These hallucinations can occur due to a variety of factors, such as the model\u2019s lack of real-world knowledge, the influence of biased or inaccurate training data, or the model\u2019s tendency to generate responses based on statistical patterns rather than a true understanding of the input. While these hallucinations are a nuisance in tasks such as text summarization and question-answering, they can be catastrophic when LLMs are used in autonomy-relevant applications such as planning. In this paper, we focus on the application of LLMs in autonomous systems and sketch a novel self-monitoring and iterative prompting architecture that uses formal methods to detect these errors in the LLM response automatically. We exploit the dialog capability of LLMs to iteratively steer them to responses that are consistent with our correctness specification. We report preliminary experiments that show the promise of the proposed approach on tasks such as automated planning.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper sketches a novel self-monitoring and iterative prompting architecture that uses formal methods to detect errors in the LLM response automatically and exploits the dialog capability of LLMs to iteratively steer them to responses that are consistent with the correctness specification."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the consistency and calibration of large language models by iteratively refining the model's output based on its estimated uncertainty. The approach is to use uncertainty-guided iterative refinement (UGIR) to generate an initial output and then iteratively refine it based on the model's estimated uncertainty.\n\nThe research problem in the paper is reducing hallucinations in large language models when applied to autonomous systems. The approach is to use formal methods to detect errors in the LLM response automatically and iteratively steer the LLM to responses that are consistent with a correctness specification.\n\nThe proposal focuses on improving consistency and calibration in general language generation tasks, while the paper specifically targets reducing hallucinations in autonomous systems. The proposal uses uncertainty estimation to guide iterative refinement, while the paper uses formal methods to detect errors and steer the model towards correct responses.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "0885471c0215b3c0d31c82518066913f7f738128",
            "paperId": "0885471c0215b3c0d31c82518066913f7f738128",
            "title": "Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement",
            "abstract": "The ability to derive underlying principles from a handful of observations and then generalize to novel situations -- known as inductive reasoning -- is central to human intelligence. Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks. In this work, we conduct a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting. Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules. By examining the intermediate rules, we observe that LMs are phenomenal hypothesis proposers (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the proposed set of rules, this hybrid approach achieves strong results across inductive reasoning benchmarks that require inducing causal relations, language-like instructions, and symbolic concepts. However, they also behave as puzzling inductive reasoners, showing notable performance gaps between rule induction (i.e., identifying plausible rules) and rule application (i.e., applying proposed rules to instances), suggesting that LMs are proposing hypotheses without being able to actually apply the rules. Through empirical and human analyses, we further reveal several discrepancies between the inductive reasoning processes of LMs and humans, shedding light on both the potentials and limitations of using LMs in inductive reasoning tasks.",
            "year": 2023,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work conducts a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the consistency and calibration of large language models in open-ended generation tasks, while the approach is to use uncertainty-guided iterative refinement. On the other hand, the research problem in the paper is studying the inductive reasoning capabilities of language models, and the approach is to use iterative hypothesis refinement with a three-step process of proposing, selecting, and refining hypotheses in the form of textual rules.\n\nThe proposal focuses on improving language model outputs, while the paper studies the inductive reasoning capabilities of language models. The methods used (uncertainty-guided iterative refinement vs. iterative hypothesis refinement) are also different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "5fb3521f87d03899731b98718702927afd227f3a",
            "paperId": "5fb3521f87d03899731b98718702927afd227f3a",
            "title": "Enhancing Ethical Explanations of Large Language Models through Iterative Symbolic Refinement",
            "abstract": "An increasing amount of research in Natural Language Inference (NLI) focuses on the application and evaluation of Large Language Models (LLMs) and their reasoning capabilities. Despite their success, however, LLMs are still prone to factual errors and inconsistencies in their explanations, offering limited control and interpretability for inference in complex domains. In this paper, we focus on ethical NLI, investigating how hybrid neuro-symbolic techniques can enhance the logical validity and alignment of ethical explanations produced by LLMs. Specifically, we present an abductive-deductive framework named Logic-Explainer, which integrates LLMs with an external backward-chaining solver to refine step-wise natural language explanations and jointly verify their correctness, reduce incompleteness and minimise redundancy. An extensive empirical analysis demonstrates that Logic-Explainer can improve explanations generated via in-context learning methods and Chain-of-Thought (CoT) on challenging ethical NLI tasks, while, at the same time, producing formal proofs describing and supporting models\u2019 reasoning. As ethical NLI requires commonsense reasoning to identify underlying moral violations, our results suggest the effectiveness of neuro-symbolic methods for multi-step NLI more broadly, opening new opportunities to enhance the logical consistency, reliability, and alignment of LLMs.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An abductive-deductive framework named Logic-Explainer is presented, which integrates LLMs with an external backward-chaining solver to refine step-wise natural language explanations and jointly verify their correctness, reduce incompleteness and minimise redundancy."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the consistency and calibration of large language models through iterative refinement guided by uncertainty estimates. The approach involves generating an initial output, identifying uncertain parts, and refining the response accordingly.\n\nThe research problem in the paper is enhancing the logical validity and alignment of ethical explanations produced by large language models. The approach integrates LLMs with an external backward-chaining solver to refine explanations and verify their correctness.\n\nWhile both works aim to improve the quality of LLM-generated outputs, the proposal focuses on consistency and calibration in open-ended tasks, while the paper targets logical validity and alignment in ethical reasoning. The methods also differ, with the proposal using uncertainty-guided iterative refinement and the paper employing a hybrid neuro-symbolic approach.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "14d0489047a1390434e7ea454e7e5165d9721ae3",
            "paperId": "14d0489047a1390434e7ea454e7e5165d9721ae3",
            "title": "Calibrating Long-form Generations from Large Language Models",
            "abstract": "To enhance Large Language Models' (LLMs) reliability, calibration is essential -- the model's assessed confidence scores should align with the actual likelihood of its responses being correct. However, current confidence elicitation methods and calibration metrics typically rely on a binary true/false assessment of response correctness. This approach does not apply to long-form generation, where an answer can be partially correct. Addressing this gap, we introduce a unified calibration framework, in which both the correctness of the LLMs' responses and their associated confidence levels are treated as distributions across a range of scores. Within this framework, we develop three metrics to precisely evaluate LLM calibration and further propose two confidence elicitation methods based on self-consistency and self-evaluation. Our experiments, which include long-form QA and summarization tasks, demonstrate that larger models don't necessarily guarantee better calibration, that calibration performance is found to be metric-dependent, and that self-consistency methods excel in factoid datasets. We also find that calibration can be enhanced through techniques such as fine-tuning, integrating relevant source documents, scaling the temperature, and combining self-consistency with self-evaluation. Lastly, we showcase a practical application of our system: selecting and cascading open-source models and ChatGPT to optimize correctness given a limited API budget. This research not only challenges existing notions of LLM calibration but also offers practical methodologies for improving trustworthiness in long-form generation.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A unified calibration framework is introduced, in which both the correctness of the LLMs' responses and their associated confidence levels are treated as distributions across a range of scores and two confidence elicitation methods based on self-consistency and self-evaluation are proposed."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve the consistency and calibration of large language models in open-ended generation tasks by iteratively refining the model's output based on its estimated uncertainty. The paper proposes a unified calibration framework to evaluate LLM calibration in long-form generation tasks and introduces confidence elicitation methods based on self-consistency and self-evaluation.\n\nWhile both the project proposal and the paper address the issue of calibration in large language models, the project proposal focuses on improving consistency and calibration through an iterative refinement process guided by uncertainty estimates. In contrast, the paper introduces a new calibration framework and metrics specifically designed for long-form generation tasks and explores confidence elicitation methods based on self-consistency and self-evaluation.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
            "paperId": "5f19ae1135a9500940978104ec15a5b8751bc7d2",
            "title": "Self-Consistency Improves Chain of Thought Reasoning in Language Models",
            "abstract": "Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).",
            "year": 2022,
            "citationCount": 1396,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting that first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths."
            },
            "score": 6
        },
        {
            "id": "87eeb9b57e3247598e62d930b8cc53fe996dc779",
            "paperId": "87eeb9b57e3247598e62d930b8cc53fe996dc779",
            "title": "BECEL: Benchmark for Consistency Evaluation of Language Models",
            "abstract": "Behavioural consistency is a critical condition for a language model (LM) to become trustworthy like humans. Despite its importance, however, there is little consensus on the definition of LM consistency, resulting in different definitions across many studies. In this paper, we first propose the idea of LM consistency based on behavioural consistency and establish a taxonomy that classifies previously studied consistencies into several sub-categories. Next, we create a new benchmark that allows us to evaluate a model on 19 test cases, distinguished by multiple types of consistency and diverse downstream tasks. Through extensive experiments on the new benchmark, we ascertain that none of the modern pre-trained language models (PLMs) performs well in every test case, while exhibiting high inconsistency in many cases. Our experimental results suggest that a unified benchmark that covers broad aspects (i.e., multiple consistency types and tasks) is essential for a more precise evaluation.",
            "year": 2022,
            "citationCount": 25,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes the idea of LM consistency based on behavioural consistency and establishes a taxonomy that classifies previously studied consistencies into several sub-categories, and creates a new benchmark that allows for a more precise evaluation."
            },
            "score": 6
        },
        {
            "id": "6920de816acd201aadc0de51cf0fa62fa92bb0cc",
            "paperId": "6920de816acd201aadc0de51cf0fa62fa92bb0cc",
            "title": "On the Calibration of Large Language Models and Alignment",
            "abstract": "As large language models attract increasing attention and find widespread application, concurrent challenges of reliability also arise at the same time. Confidence calibration, an effective analysis method for gauging the reliability of deep models, serves as a crucial tool for assessing and improving their reliability. However, such investigation has been comparatively underexplored. In this work, we conduct a systematic examination of the calibration of aligned language models throughout the entire construction process, including pretraining and alignment training. At each stage, we investigate how different training settings, such as parameter scales and training data, affect model calibration. To thoroughly assess model calibration, we evaluate models on three most concerned aspects: generation, factuality and understanding. Our work sheds light on whether popular LLMs are well-calibrated and how the training process influences model calibration.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work sheds light on whether popular LLMs are well-calibrated and how the training process influences model calibration, as well as how different training settings affect model calibration."
            },
            "score": 6
        },
        {
            "id": "5424e311319c58847b4c690d5c91090e3b6a4ac3",
            "paperId": "5424e311319c58847b4c690d5c91090e3b6a4ac3",
            "title": "Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models",
            "abstract": "While Large Language Models (LLMs) have demonstrated remarkable potential in natural language generation and instruction following, a persistent challenge lies in their susceptibility to\"hallucinations\", which erodes trust in their outputs. Although Uncertainty Quantification (UQ) presents a promising solution, its accurate implementation within the context of LLMs remains a significant hurdle. To address this critical roadblock, our research originates from a fundamental heuristic insight: tokens within auto-regressive LLM-generated text do not equally reflect the underlying meaning. Some tokens carry greater relevance and representativeness than others, owing to the phenomenon of\"linguistic redundancy\", wherein a select few keywords suffice to convey the essence of lengthy sentences. Regrettably, existing methodologies treat all tokens with equal importance when estimating uncertainty, disregarding these inherent generative inequalities. Our analysis reveals a significant issue with state-of-the-art: numerous tokens (and sentences) of limited semantic significance receive equal or even excessive weighting during uncertainty estimation. To rectify this bias, we propose to jointly Shifting Attention to more Relevant (SAR) components, at both the token- and the sentence-levels for accurate uncertainty estimation. We conduct extensive experiments involving a range of popular\"off-the-shelf\"LLMs, including instruction-tuned LLMs such as Vicuna, WizardLM, and LLaMA-2-chat, as well as pretrained LLMs like OPT and LLaMA, with model sizes extending up to 33B parameters. We carry out evaluation across various free-form question-answering tasks, encompassing domains such as reading comprehension, science Q&A, and medical Q&A. Our experimental results demonstrate the superior performance of SAR in addressing the challenges of uncertainty estimation within the realm of LLMs.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results demonstrate the superior performance of SAR in addressing the challenges of uncertainty estimation within the realm of LLMs, and propose to jointly Shifting Attention to more Relevant (SAR) components, at both the token- and the sentence-levels for accurate uncertainty estimation."
            },
            "score": 6
        },
        {
            "id": "ea0d41514a41f8273f13b3b277e7fcbbc65a8549",
            "paperId": "ea0d41514a41f8273f13b3b277e7fcbbc65a8549",
            "title": "Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models",
            "abstract": "The recent performance leap of Large Language Models (LLMs) opens up new opportunities across numerous industrial applications and domains. However, erroneous generations, such as false predictions, misinformation, and hallucination made by LLMs, have also raised severe concerns for the trustworthiness of LLMs', especially in safety-, security- and reliability-sensitive scenarios, potentially hindering real-world adoptions. While uncertainty estimation has shown its potential for interpreting the prediction risks made by general machine learning (ML) models, little is known about whether and to what extent it can help explore an LLM's capabilities and counteract its undesired behavior. To bridge the gap, in this paper, we initiate an exploratory study on the risk assessment of LLMs from the lens of uncertainty. In particular, we experiment with twelve uncertainty estimation methods and four LLMs on four prominent natural language processing (NLP) tasks to investigate to what extent uncertainty estimation techniques could help characterize the prediction risks of LLMs. Our findings validate the effectiveness of uncertainty estimation for revealing LLMs' uncertain/non-factual predictions. In addition to general NLP tasks, we extensively conduct experiments with four LLMs for code generation on two datasets. We find that uncertainty estimation can potentially uncover buggy programs generated by LLMs. Insights from our study shed light on future design and development for reliable LLMs, facilitating further research toward enhancing the trustworthiness of LLMs.",
            "year": 2023,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An exploratory study on the risk assessment of LLMs from the lens of uncertainty is initiated, finding that uncertainty estimation can potentially uncover buggy programs generated by LLMs."
            },
            "score": 6
        },
        {
            "id": "4e15901eaaaa9a9c2c30f64e05054ce6f5cdaa97",
            "paperId": "4e15901eaaaa9a9c2c30f64e05054ce6f5cdaa97",
            "title": "On the Importance of Uncertainty in Decision-Making with Large Language Models",
            "abstract": "We investigate the role of uncertainty in decision-making problems with natural language as input. For such tasks, using Large Language Models as agents has become the norm. However, none of the recent approaches employ any additional phase for estimating the uncertainty the agent has about the world during the decision-making task. We focus on a fundamental decision-making framework with natural language as input, which is the one of contextual bandits, where the context information consists of text. As a representative of the approaches with no uncertainty estimation, we consider an LLM bandit with a greedy policy, which picks the action corresponding to the largest predicted reward. We compare this baseline to LLM bandits that make active use of uncertainty estimation by integrating the uncertainty in a Thompson Sampling policy. We employ different techniques for uncertainty estimation, such as Laplace Approximation, Dropout, and Epinets. We empirically show on real-world data that the greedy policy performs worse than the Thompson Sampling policies. These findings suggest that, while overlooked in the LLM literature, uncertainty plays a fundamental role in bandit tasks with LLMs.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work considers an LLM bandit with a greedy policy, which picks the action corresponding to the largest predicted reward, compared to LLM bandits that make active use of uncertainty estimation by integrating the uncertainty in a Thompson Sampling policy."
            },
            "score": 6
        },
        {
            "id": "3aaf6a2cbad5850ad81ab5c163599cb3d523436f",
            "paperId": "3aaf6a2cbad5850ad81ab5c163599cb3d523436f",
            "title": "Self-Refine: Iterative Refinement with Self-Feedback",
            "abstract": "Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.",
            "year": 2023,
            "citationCount": 505,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Self-Refine is introduced, an approach for improving initial outputs from LLMs through iterative feedback and refinement that demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using this simple, standalone approach."
            },
            "score": 5
        },
        {
            "id": "e69b41e53ce897a576ab80a6e9d59a253cbd6c62",
            "paperId": "e69b41e53ce897a576ab80a6e9d59a253cbd6c62",
            "title": "Iterative Prompt Refinement for Radiation Oncology Symptom Extraction Using Teacher-Student Large Language Models",
            "abstract": "This study introduces a novel teacher-student architecture utilizing Large Language Models (LLMs) to improve prostate cancer radiotherapy symptom extraction from clinical notes. Mixtral, the student model, initially extracts symptoms, followed by GPT-4, the teacher model, which refines prompts based on Mixtral's performance. This iterative process involved 294 single symptom clinical notes across 12 symptoms, with up to 16 rounds of refinement per epoch. Results showed significant improvements in extracting symptoms from both single and multi-symptom notes. For 59 single symptom notes, accuracy increased from 0.51 to 0.71, precision from 0.52 to 0.82, recall from 0.52 to 0.72, and F1 score from 0.49 to 0.73. In 375 multi-symptom notes, accuracy rose from 0.24 to 0.43, precision from 0.6 to 0.76, recall from 0.24 to 0.43, and F1 score from 0.20 to 0.44. These results demonstrate the effectiveness of advanced prompt engineering in LLMs for radiation oncology use.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel teacher-student architecture utilizing Large Language Models (LLMs) to improve prostate cancer radiotherapy symptom extraction from clinical notes is introduced, demonstrating the effectiveness of advanced prompt engineering in LLMs for radiation oncology use."
            },
            "score": 5
        },
        {
            "id": "9a9b1e2968302eb882870537d4af6e2c722dfd1a",
            "paperId": "9a9b1e2968302eb882870537d4af6e2c722dfd1a",
            "title": "Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement",
            "abstract": "To enhance the multi-step reasoning capabilities of large language models, researchers have extensively explored prompting methods, notably the Chain-of-Thought (CoT) method which explicitly elicits human-like rationales. However, they have inadvertently overlooked the potential of enhancing model reasoning performance by formulating higher-quality problems. In this work, we start from the problem side and propose Self-Polish (SP), a novel method that facilitates the model's reasoning by guiding it to progressively refine the given problems to be more comprehensible and solvable. We also explore several automatic prompting varients and propose the Self-Polish prompt bank for the community. SP is orthogonal to all other prompting methods of answer/reasoning side like CoT, allowing for seamless integration with state-of-the-art techniques for further improvement. Thorough experiments show that the proposed method attains notable and consistent effectiveness on five reasoning benchmarks across different models. Furthermore, our method also showcases impressive performance on robustness evaluation. Codes and prompts are available at https://github.com/WooooDyy/Self-Polish.",
            "year": 2023,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Self-Polish (SP) is proposed, a novel method that facilitates the model's reasoning by guiding it to progressively refine the given problems to be more comprehensible and solvable and seamless integration with state-of-the-art techniques for further improvement."
            },
            "score": 5
        },
        {
            "id": "6af460d34bfc8e955e43fbe15cedcf329b48bc19",
            "paperId": "6af460d34bfc8e955e43fbe15cedcf329b48bc19",
            "title": "SAC3: Reliable Hallucination Detection in Black-Box Language Models via Semantic-aware Cross-check Consistency",
            "abstract": "Hallucination detection is a critical step toward understanding the trustworthiness of modern language models (LMs). To achieve this goal, we re-examine existing detection approaches based on the self-consistency of LMs and uncover two types of hallucinations resulting from 1) question-level and 2) model-level, which cannot be effectively identified through self-consistency check alone. Building upon this discovery, we propose a novel sampling-based method, i.e., semantic-aware cross-check consistency (SAC3) that expands on the principle of self-consistency checking. Our SAC3 approach incorporates additional mechanisms to detect both question-level and model-level hallucinations by leveraging advances including semantically equivalent question perturbation and cross-model response consistency checking. Through extensive and systematic empirical analysis, we demonstrate that SAC3 outperforms the state of the art in detecting both non-factual and factual statements across multiple question-answering and open-domain generation benchmarks.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel sampling-based method, i.e., semantic-aware cross-check consistency (SAC3) that expands on the principle of self-consistency checking and demonstrates that SAC3 outperforms the state of the art in detecting both non-factual and factual statements across multiple question-answering and open-domain generation benchmarks."
            },
            "score": 5
        },
        {
            "id": "31bc64a991f8b739ac7e9824143d3c93bedf785f",
            "paperId": "31bc64a991f8b739ac7e9824143d3c93bedf785f",
            "title": "Benchmarking and Improving Generator-Validator Consistency of Language Models",
            "abstract": "As of September 2023, ChatGPT correctly answers\"what is 7+8\"with 15, but when asked\"7+8=15, True or False\"it responds with\"False\". This inconsistency between generating and validating an answer is prevalent in language models (LMs) and erodes trust. In this paper, we propose a framework for measuring the consistency between generation and validation (which we call generator-validator consistency, or GV-consistency), finding that even GPT-4, a state-of-the-art LM, is GV-consistent only 76% of the time. To improve the consistency of LMs, we propose to finetune on the filtered generator and validator responses that are GV-consistent, and call this approach consistency fine-tuning. We find that this approach improves GV-consistency of Alpaca-30B from 60% to 93%, and the improvement extrapolates to unseen tasks and domains (e.g., GV-consistency for positive style transfers extrapolates to unseen styles like humor). In addition to improving consistency, consistency fine-tuning improves both generator quality and validator accuracy without using any labeled data. Evaluated across 6 tasks, including math questions, knowledge-intensive QA, and instruction following, our method improves the generator quality by 16% and the validator accuracy by 6.3% across all tasks.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A framework for measuring the consistency between generation and validation is proposed, finding that even GPT-4, a state-of-the-art LM, is GV-consistent only 76% of the time, and consistency fine-tuning improves both generator quality and validator accuracy without using any labeled data."
            },
            "score": 5
        },
        {
            "id": "0d22f06a1f5ad9f62b2f35c126b514f927586c85",
            "paperId": "0d22f06a1f5ad9f62b2f35c126b514f927586c85",
            "title": "Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency",
            "abstract": "Large language models (LLMs) have exhibited remarkable ability in code generation. However, generating the correct solution in a single attempt still remains a challenge. Prior works utilize verification properties in software engineering to verify and re-rank solutions in a majority voting manner. But the assumption behind them that generated verification properties have better qualities than solutions may not always hold. In this paper, we treat them equally as different perspectives of LLMs' reasoning processes. We propose the Multi-Perspective Self-Consistency (MPSC) framework incorporating both inter- and intra-consistency across outputs from multiple perspectives. Specifically, we prompt LLMs to generate diverse outputs from three perspectives, Solution, Specification and Test case, constructing a 3-partite graph. With two measure functions of consistency, we embed both inter- and intra-consistency information into the graph. The optimal choice of solutions is then determined based on analysis in the graph. MPSC significantly boosts performance of foundation models (ChatGPT in this paper) on various benchmarks, including HumanEval (+15.91%), MBPP (+6.43%) and CodeContests (+9.37%), even surpassing GPT-4.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Multi-Perspective Self-Consistency (MPSC) framework is proposed incorporating both inter- and intra-consistency across outputs from multiple perspectives and significantly boosts performance of foundation models on various benchmarks, including HumanEval, MBPP and CodeContests."
            },
            "score": 5
        },
        {
            "id": "507465f8d46489a68a527cb5304d76bdb6c31ed9",
            "paperId": "507465f8d46489a68a527cb5304d76bdb6c31ed9",
            "title": "Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation",
            "abstract": "We introduce a method to measure uncertainty in large language models. For tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models. We show that measuring uncertainty in natural language is challenging because of\"semantic equivalence\"-- different sentences can mean the same thing. To overcome these challenges we introduce semantic entropy -- an entropy which incorporates linguistic invariances created by shared meanings. Our method is unsupervised, uses only a single model, and requires no modifications to off-the-shelf language models. In comprehensive ablation studies we show that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines.",
            "year": 2023,
            "citationCount": 85,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "In comprehensive ablation studies, it is shown that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines."
            },
            "score": 5
        },
        {
            "id": "6d3ae6d6b312b659b3a14ae3f3e86a36db63200d",
            "paperId": "6d3ae6d6b312b659b3a14ae3f3e86a36db63200d",
            "title": "Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning",
            "abstract": "Step-by-step decision planning with large language models (LLMs) is gaining attention in AI agent development. This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models. Existing approaches are either white-box or computationally demanding, limiting use of black-box proprietary LLMs within budgets. The paper's first contribution is a non-parametric uncertainty quantification method for LLMs, efficiently estimating point-wise dependencies between input-decision on the fly with a single inference, without access to token logits. This estimator informs the statistical interpretation of decision trustworthiness. The second contribution outlines a systematic design for a decision-making agent, generating actions like ``turn on the bathroom light'' based on user prompts such as ``take a bath''. Users will be asked to provide preferences when more than one action has high estimated point-wise dependencies. In conclusion, our uncertainty estimation and decision-making agent design offer a cost-efficient approach for AI agent development.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models, and outlines a systematic design for a decision-making agent, offering a cost-efficient approach for AI agent development."
            },
            "score": 5
        },
        {
            "id": "645d8c40f2a05f0b06f9338cf7635755532d747c",
            "paperId": "645d8c40f2a05f0b06f9338cf7635755532d747c",
            "title": "Uncertainty Awareness of Large Language Models Under Code Distribution Shifts: A Benchmark Study",
            "abstract": "Large Language Models (LLMs) have been widely employed in programming language analysis to enhance human productivity. Yet, their reliability can be compromised by various code distribution shifts, leading to inconsistent outputs. While probabilistic methods are known to mitigate such impact through uncertainty calibration and estimation, their efficacy in the language domain remains underexplored compared to their application in image-based tasks. In this work, we first introduce a large-scale benchmark dataset, incorporating three realistic patterns of code distribution shifts at varying intensities. Then we thoroughly investigate state-of-the-art probabilistic methods applied to CodeLlama using these shifted code snippets. We observe that these methods generally improve the uncertainty awareness of CodeLlama, with increased calibration quality and higher uncertainty estimation~(UE) precision. However, our study further reveals varied performance dynamics across different criteria (e.g., calibration error vs misclassification detection) and trade-off between efficacy and efficiency, highlighting necessary methodological selection tailored to specific contexts.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work thoroughly investigate state-of-the-art probabilistic methods applied to CodeLlama using three realistic patterns of code distribution shifts at varying intensities, and observes that these methods generally improve the uncertainty awareness of CodeLlama, with increased calibration quality and higher uncertainty estimation~(UE) precision."
            },
            "score": 5
        },
        {
            "id": "7f6d48d7b1641d3d2fd4ee06c434a73af8fce07b",
            "paperId": "7f6d48d7b1641d3d2fd4ee06c434a73af8fce07b",
            "title": "Density-Softmax: Scalable and Calibrated Uncertainty Estimation under Distribution Shifts",
            "abstract": "Prevalent deterministic deep-learning models suffer from significant over-confidence under distribution shifts. Probabilistic approaches can reduce this problem but struggle with computational efficiency. In this paper, we propose Density-Softmax, a fast and lightweight deterministic method to improve calibrated uncertainty estimation via a combination of density function with the softmax layer. By using the latent representation's likelihood value, our approach produces more uncertain predictions when test samples are distant from the training samples. Theoretically, we show that Density-Softmax can produce high-quality uncertainty estimation with neural networks, as it is the solution of minimax uncertainty risk and is distance-aware, thus reducing the over-confidence of the standard softmax. Empirically, our method enjoys similar computational efficiency as a single forward pass deterministic with standard softmax on the shifted toy, vision, and language datasets across modern deep-learning architectures. Notably, Density-Softmax uses 4 times fewer parameters than Deep Ensembles and 6 times lower latency than Rank-1 Bayesian Neural Network, while obtaining competitive predictive performance and lower calibration errors under distribution shifts.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Density-Softmax is proposed, a fast and lightweight deterministic method to improve calibrated uncertainty estimation via a combination of density function with the softmax layer, which enjoys similar computational efficiency as a single forward pass deterministic with standard softmax on the shifted toy, vision, and language datasets across modern deep-learning architectures."
            },
            "score": 5
        },
        {
            "id": "2a74fc66beea8bce542581560ca6ec5a0e1bb024",
            "paperId": "2a74fc66beea8bce542581560ca6ec5a0e1bb024",
            "title": "CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation",
            "abstract": "Annotated data plays a critical role in Natural Language Processing (NLP) in training models and evaluating their performance. Given recent developments in Large Language Models (LLMs), models such as ChatGPT demonstrate zero-shot capability on many text-annotation tasks, comparable with or even exceeding human annotators. Such LLMs can serve as alternatives for manual annotation, due to lower costs and higher scalability. However, limited work has leveraged LLMs as complementary annotators, nor explored how annotation work is best allocated among humans and LLMs to achieve both quality and cost objectives. We propose CoAnnotating, a novel paradigm for Human-LLM co-annotation of unstructured texts at scale. Under this framework, we utilize uncertainty to estimate LLMs' annotation capability. Our empirical study shows CoAnnotating to be an effective means to allocate work from results on different datasets, with up to 21% performance improvement over random baseline. For code implementation, see https://github.com/SALT-NLP/CoAnnotating.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes CoAnnotating, a novel paradigm for Human-LLM co-annotation of unstructured texts at scale, and utilizes uncertainty to estimate LLMs' annotation capability."
            },
            "score": 4
        },
        {
            "id": "bf4810017b54e50354cccffd8966121c7166cb17",
            "paperId": "bf4810017b54e50354cccffd8966121c7166cb17",
            "title": "Iterative Translation Refinement with Large Language Models",
            "abstract": "Large language models have shown surprising performances in understanding instructions and performing natural language tasks. In this paper, we propose iterative translation refinement to leverage the power of large language models for more natural translation and post-editing. We show that by simply involving a large language model in an iterative process, the output quality improves beyond mere translation. Extensive test scenarios with GPT-3.5 reveal that although iterations reduce string-based metric scores, neural metrics indicate comparable if not improved translation quality. Further, human evaluations demonstrate that our method effectively reduces translationese compared to initial GPT translations and even human references, especially for into-English directions. Ablation studies underscore the importance of anchoring the refinement process to the source input and a reasonable initial translation.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that by simply involving a large language model in an iterative process, the output quality improves beyond mere translation, and that although iterations reduce string-based metric scores, neural metrics indicate comparable if not improved translation quality."
            },
            "score": 4
        },
        {
            "id": "4637f79ddfaf923ce569996ffa5b6cda1996faa1",
            "paperId": "4637f79ddfaf923ce569996ffa5b6cda1996faa1",
            "title": "Jailbreaking Black Box Large Language Models in Twenty Queries",
            "abstract": "There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR -- which is inspired by social engineering attacks -- uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and PaLM-2.",
            "year": 2023,
            "citationCount": 119,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "PAIR is an algorithm that generates semantic jailbreaks with only black-box access to an LLM with competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and PaLM."
            },
            "score": 4
        },
        {
            "id": "bb2ef694e8b5a99e1f7ceb014968b4d1dc2e122a",
            "paperId": "bb2ef694e8b5a99e1f7ceb014968b4d1dc2e122a",
            "title": "EIPE-text: Evaluation-Guided Iterative Plan Extraction for Long-Form Narrative Text Generation",
            "abstract": "Plan-and-Write is a common hierarchical approach in long-form narrative text generation, which first creates a plan to guide the narrative writing. Following this approach, several studies rely on simply prompting large language models for planning, which often yields suboptimal results. In this paper, we propose a new framework called Evaluation-guided Iterative Plan Extraction for long-form narrative text generation (EIPE-text), which extracts plans from the corpus of narratives and utilizes the extracted plans to construct a better planner. EIPE-text has three stages: plan extraction, learning, and inference. In the plan extraction stage, it iteratively extracts and improves plans from the narrative corpus and constructs a plan corpus. We propose a question answer (QA) based evaluation mechanism to automatically evaluate the plans and generate detailed plan refinement instructions to guide the iterative improvement. In the learning stage, we build a better planner by fine-tuning with the plan corpus or in-context learning with examples in the plan corpus. Finally, we leverage a hierarchical approach to generate long-form narratives. We evaluate the effectiveness of EIPE-text in the domains of novels and storytelling. Both GPT-4-based evaluations and human evaluations demonstrate that our method can generate more coherent and relevant long-form narratives. Our code will be released in the future.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new framework called Evaluation-guided Iterative Plan Extraction for long-form narrative text generation (EIPE-text), which extracts plans from the corpus of narratives and utilizes the extracted plans to construct a better planner."
            },
            "score": 4
        },
        {
            "id": "b73ea8adfef3cdc63167e3a7f1f2be3f4ef4219d",
            "paperId": "b73ea8adfef3cdc63167e3a7f1f2be3f4ef4219d",
            "title": "Knowledge Refinement via Interaction Between Search Engines and Large Language Models",
            "abstract": "Information retrieval (IR) plays a crucial role in locating relevant resources from vast amounts of data, and its applications have evolved from traditional knowledge bases to modern search engines (SEs). The emergence of large language models (LLMs) has further revolutionized the IR field by enabling users to interact with search systems in natural language. In this paper, we explore the advantages and disadvantages of LLMs and SEs, highlighting their respective strengths in understanding user-issued queries and retrieving up-to-date information. To leverage the benefits of both paradigms while circumventing their limitations, we propose InteR, a novel framework that facilitates knowledge refinement through interaction between SEs and LLMs. InteR allows SEs to expand knowledge in queries using LLM-generated knowledge collections and enables LLMs to enhance prompt formulation using SE-retrieved documents. This iterative refinement process augments the inputs of SEs and LLMs, leading to more accurate retrieval. Experiments on large-scale retrieval benchmarks involving web search and low-resource retrieval tasks demonstrate that InteR achieves overall superior zero-shot retrieval performance compared to state-of-the-art methods, even those using relevance judgment. Source code is available at https://github.com/Cyril-JZ/InteR.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "InteR is proposed, a novel framework that facilitates knowledge refinement through interaction between SEs and LLMs, which allows SEs to expand knowledge in queries using LLM-generated knowledge collections and enables LLMs to enhance prompt formulation using SE-retrieved documents."
            },
            "score": 4
        },
        {
            "id": "afee8cdc51e95b50d7574ed1700a797874bf792c",
            "paperId": "afee8cdc51e95b50d7574ed1700a797874bf792c",
            "title": "Adversarial Fine-Tuning of Language Models: An Iterative Optimisation Approach for the Generation and Detection of Problematic Content",
            "abstract": "In this paper, we tackle the emerging challenge of unintended harmful content generation in Large Language Models (LLMs) with a novel dual-stage optimisation technique using adversarial fine-tuning. Our two-pronged approach employs an adversarial model, fine-tuned to generate potentially harmful prompts, and a judge model, iteratively optimised to discern these prompts. In this adversarial cycle, the two models seek to outperform each other in the prompting phase, generating a dataset of rich examples which are then used for fine-tuning. This iterative application of prompting and fine-tuning allows continuous refinement and improved performance. The performance of our approach is evaluated through classification accuracy on a dataset consisting of problematic prompts not detected by GPT-4, as well as a selection of contentious but unproblematic prompts. We show considerable increase in classification accuracy of the judge model on this challenging dataset as it undergoes the optimisation process. Furthermore, we show that a rudimentary model \\texttt{ada} can achieve 13\\% higher accuracy on the hold-out test set than GPT-4 after only a few rounds of this process, and that this fine-tuning improves performance in parallel tasks such as toxic comment identification.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper shows that a rudimentary model can achieve 13\\% higher accuracy on the hold-out test set than GPT-4 after only a few rounds of this process, and that this fine-tuning improves performance in parallel tasks such as toxic comment identification."
            },
            "score": 4
        },
        {
            "id": "06e5828341aa3926e1d839039363b0673b9461cc",
            "paperId": "06e5828341aa3926e1d839039363b0673b9461cc",
            "title": "Errors are Useful Prompts: Instruction Guided Task Programming with Verifier-Assisted Iterative Prompting",
            "abstract": "Generating low-level robot task plans from high-level natural language instructions remains a challenging problem. Although large language models have shown promising results in generating plans, the accuracy of the output remains unverified. Furthermore, the lack of domain-specific language data poses a limitation on the applicability of these models. In this paper, we propose CLAIRIFY, a novel approach that combines automatic iterative prompting with program verification to ensure programs written in data-scarce domain-specific language are syntactically valid and incorporate environment constraints. Our approach provides effective guidance to the language model on generating structured-like task plans by incorporating any errors as feedback, while the verifier ensures the syntactic accuracy of the generated plans. We demonstrate the effectiveness of CLAIRIFY in planning chemistry experiments by achieving state-of-the-art results. We also show that the generated plans can be executed on a real robot by integrating them with a task and motion planner.",
            "year": 2023,
            "citationCount": 30,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes CLAIRIFY, a novel approach that combines automatic iterative prompting with program verification to ensure programs written in data-scarce domain-specific language are syntactically valid and incorporate environment constraints."
            },
            "score": 4
        },
        {
            "id": "58a282c89864f35bff1741f5ab439222da6bb3ec",
            "paperId": "58a282c89864f35bff1741f5ab439222da6bb3ec",
            "title": "MLLM-DataEngine: An Iterative Refinement Approach for MLLM",
            "abstract": "Despite the great advance of Multimodal Large Language Models (MLLMs) in both instruction dataset building and benchmarking, the independence of training and evaluation makes current MLLMs hard to further improve their capability under the guidance of evaluation results with a relatively low human cost. In this paper, we propose MLLM-DataEngine, a novel closed-loop system that bridges data generation, model training, and evaluation. Within each loop iteration, the MLLM-DataEngine first analyze the weakness of the model based on the evaluation results, then generate a proper incremental dataset for the next training iteration and enhance the model capability iteratively. Compared with previous data collection methods which are separate from the benchmarking, the data generated by MLLM-DataEngine shows better targeting, quality, and correctness. For targeting, we propose an Adaptive Bad-case Sampling module, which adjusts the ratio of different types of data within each incremental dataset based on the benchmarking results. For quality, we resort to GPT-4 to generate high-quality data with each given data type. For correctness, prompt design is critical for the data generation results. Rather than previous hand-crafted prompt, we propose an Interactive Prompt Optimization strategy, which optimizes the prompt with the multi-round interaction between human and GPT, and improve the correctness of generated data greatly. Through extensive experiments, we find our MLLM-DataEngine could boost the MLLM capability in a targeted and automatic manner, with only a few human participation. We hope it could be a general solution for the following MLLMs building. The MLLM-DataEngine has been open-sourced and is now available at https://github.com/opendatalab/MLLM-DataEngine.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes MLLM-DataEngine, a novel closed-loop system that bridges data generation, model training, and evaluation, and proposes an Interactive Prompt Optimization strategy, which optimizes the prompt with the multi-round interaction between human and GPT, and improves the correctness of generated data greatly."
            },
            "score": 4
        },
        {
            "id": "23bbd94f93e360f373f78ce20f61ec3486b1923d",
            "paperId": "23bbd94f93e360f373f78ce20f61ec3486b1923d",
            "title": "Exploring Large Language Models for Multi-Modal Out-of-Distribution Detection",
            "abstract": "Out-of-distribution (OOD) detection is essential for reliable and trustworthy machine learning. Recent multi-modal OOD detection leverages textual information from in-distribution (ID) class names for visual OOD detection, yet it currently neglects the rich contextual information of ID classes. Large language models (LLMs) encode a wealth of world knowledge and can be prompted to generate descriptive features for each class. Indiscriminately using such knowledge causes catastrophic damage to OOD detection due to LLMs' hallucinations, as is observed by our analysis. In this paper, we propose to apply world knowledge to enhance OOD detection performance through selective generation from LLMs. Specifically, we introduce a consistency-based uncertainty calibration method to estimate the confidence score of each generation. We further extract visual objects from each image to fully capitalize on the aforementioned world knowledge. Extensive experiments demonstrate that our method consistently outperforms the state-of-the-art.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a consistency-based uncertainty calibration method to estimate the confidence score of each generation of large language models and extracts visual objects from each image to fully capitalize on the aforementioned world knowledge."
            },
            "score": 4
        },
        {
            "id": "fbd4a876cee20eaf98f344aca597a55338f663f5",
            "paperId": "fbd4a876cee20eaf98f344aca597a55338f663f5",
            "title": "Examining the Inter-Consistency of Large Language Models: An In-depth Analysis via Debate",
            "abstract": "Large Language Models (LLMs) have demonstrated human-like intelligence and are widely used in various applications. However, LLMs still exhibit various kinds of inconsistency problems. Existing works mainly focus on the inconsistency issues within a single LLM, while we investigate the inter-consistency among multiple LLMs, which is critical for collaborating to solve a complex task. To examine whether LLMs can collaborate to ultimately achieve a consensus for the shared goal and whether LLMs easily change their viewpoints, we introduce a Formal Debate framework (FORD) With FORD, we conduct a three-stage debate aligned with real-world scenarios: fair debate, mismatched debate, and roundtable debate. Through extensive experiments on the commonsense reasoning task, LLMs not only become more inter-consistent but also achieve higher performance. More-over, we observe that stronger LLMs tend to dominate the debates by adhering to their perspectives, while weaker ones are more likely to change viewpoints. Additionally, we highlight the importance of a competent judge, such as GPT-4, to draw more proper conclusions. Our work contributes to understanding the inter-consistency among LLMs and lays the foundation for the development of future collaboration methods.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates the inter-consistency among multiple Large Language Models, and contributes to understanding the inter-consistency among LLMs and lays the foundation for the development of future collaboration methods."
            },
            "score": 4
        },
        {
            "id": "f8cbcb106a48524edc39df23e2a95f1e6d4c739a",
            "paperId": "f8cbcb106a48524edc39df23e2a95f1e6d4c739a",
            "title": "Examining Inter-Consistency of Large Language Models Collaboration: An In-depth Analysis via Debate",
            "abstract": "Large Language Models (LLMs) have shown impressive capabilities in various applications, but they still face various inconsistency issues. Existing works primarily focus on the inconsistency issues within a single LLM, while we complementarily explore the inter-consistency among multiple LLMs for collaboration. To examine whether LLMs can collaborate effectively to achieve a consensus for a shared goal, we focus on commonsense reasoning, and introduce a formal debate framework (FORD) to conduct a three-stage debate among LLMs with real-world scenarios alignment: fair debate, mismatched debate, and roundtable debate. Through extensive experiments on various datasets, LLMs can effectively collaborate to reach a consensus despite noticeable inter-inconsistencies, but imbalances in their abilities can lead to domination by superior LLMs. Leveraging a more advanced LLM like GPT-4 as an authoritative judge can boost collaboration performance. Our work contributes to understanding the inter-consistency among LLMs and lays the foundation for developing future collaboration methods. Codes and data are available at https://github.com/Waste-Wood/FORD",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a formal debate framework (FORD) to conduct a three-stage debate among LLMs with real-world scenarios alignment: fair debate, mismatched debate, and roundtable debate to examine whether LLMs can collaborate effectively to achieve a consensus for a shared goal."
            },
            "score": 4
        },
        {
            "id": "92746dfa09dcad92ecf1e6272ebb300c1112b7eb",
            "paperId": "92746dfa09dcad92ecf1e6272ebb300c1112b7eb",
            "title": "Automatic Calibration and Error Correction for Large Language Models via Pareto Optimal Self-Supervision",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities out of box for a wide range of applications, yet accuracy still remains a major growth area, especially in mission-critical domains such as biomedicine. An effective method to calibrate the con\ufb01dence level on LLM responses is essential to automatically detect errors and facilitate human-in-the-loop veri\ufb01cation. An important source of calibration signals stems from expert-stipulated programmatic super-vision, which is often available at low cost but has its own limitations such as noise and coverage. In this paper, we introduce a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every response, without any additional manual efforts. This is accomplished by learning a harmonizer model to align LLM output with other available supervision sources, which would assign higher risk scores to more uncertain LLM responses and facilitate error correction. Experiments on standard relation extraction tasks in biomedical and general domains demonstrate the promise of this approach, with our proposed risk scores highly correlated with the real error rate of LLMs. For the most uncertain test instances, dynamic prompting based on our proposed risk scores results in signi\ufb01cant accuracy improvement for off-the-shelf LLMs, boosting GPT-3 results past state-of-the-art (SOTA) weak supervision and GPT-4 results past SOTA supervised results on challenging evaluation datasets.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every response, without any additional manual efforts."
            },
            "score": 4
        },
        {
            "id": "acbe813244e07f32eb034d6c27547d772a995d1d",
            "paperId": "acbe813244e07f32eb034d6c27547d772a995d1d",
            "title": "Uncertainty Estimation for Language Reward Models",
            "abstract": "Language models can learn a range of capabilities from unsupervised training on text corpora. However, to solve a particular problem (such as text summarization) it is typically necessary to fine-tune them on a task-specific dataset. It is often easier for humans to choose between options than to provide labeled data, and prior work has achieved state-of-the-art performance by training a reward model from such preference comparisons. However, collecting a large preference comparison dataset is still expensive -- and the learned reward models are unreliable out-of-distribution. We seek to address these problems via uncertainty estimation, which can improve sample efficiency and robustness using active learning and risk-averse reinforcement learning (RL). Specifically, we use bootstrap aggregating (bagging) to train an ensemble of reward models differing in the initialization of their final layer. Ensembles have proved successful in prior applications of active learning, but we find that in our setting ensemble active learning does not outperform random sampling. Further experiments show that while the aggregate predictions are well-calibrated, the ensemble's estimated epistemic uncertainty is only weakly correlated with model error. We suspect this is because the ensemble members are fine-tuned from a single model and so are similar to one another. This suggests current pre-training methods will need to be modified to support uncertainty estimation, e.g. by training multiple language models.",
            "year": 2022,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that in this setting ensemble active learning does not outperform random sampling, and current pre-training methods will need to be modified to support uncertainty estimation, e.g. by training multiple language models."
            },
            "score": 4
        },
        {
            "id": "bf4700077294c369f64eda65f677dd4f61b43072",
            "paperId": "bf4700077294c369f64eda65f677dd4f61b43072",
            "title": "Uncertainty Estimation and Reduction of Pre-trained Models for Text Regression",
            "abstract": "Abstract State-of-the-art classification and regression models are often not well calibrated, and cannot reliably provide uncertainty estimates, limiting their utility in safety-critical applications such as clinical decision-making. While recent work has focused on calibration of classifiers, there is almost no work in NLP on calibration in a regression setting. In this paper, we quantify the calibration of pre- trained language models for text regression, both intrinsically and extrinsically. We further apply uncertainty estimates to augment training data in low-resource domains. Our experiments on three regression tasks in both self-training and active-learning settings show that uncertainty estimation can be used to increase overall performance and enhance model generalization.",
            "year": 2022,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper quantifies the calibration of pre- trained language models for text regression, both intrinsically and extrinsically, and applies uncertainty estimates to augment training data in low-resource domains."
            },
            "score": 4
        },
        {
            "id": "5e7274bcda47b704b6797bb14be8b7a61c047a61",
            "paperId": "5e7274bcda47b704b6797bb14be8b7a61c047a61",
            "title": "Uncertainty-Aware Evaluation for Vision-Language Models",
            "abstract": "Vision-Language Models like GPT-4, LLaVA, and CogVLM have surged in popularity recently due to their impressive performance in several vision-language tasks. Current evaluation methods, however, overlook an essential component: uncertainty, which is crucial for a comprehensive assessment of VLMs. Addressing this oversight, we present a benchmark incorporating uncertainty quantification into evaluating VLMs. Our analysis spans 20+ VLMs, focusing on the multiple-choice Visual Question Answering (VQA) task. We examine models on 5 datasets that evaluate various vision-language capabilities. Using conformal prediction as an uncertainty estimation approach, we demonstrate that the models' uncertainty is not aligned with their accuracy. Specifically, we show that models with the highest accuracy may also have the highest uncertainty, which confirms the importance of measuring it for VLMs. Our empirical findings also reveal a correlation between model uncertainty and its language model part.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that models with the highest accuracy may also have the highest uncertainty, which confirms the importance of measuring it for VLMs, and a correlation between model uncertainty and its language model part is revealed."
            },
            "score": 4
        },
        {
            "id": "c76541024ed59403f99a5a73ba69849112959a6e",
            "paperId": "c76541024ed59403f99a5a73ba69849112959a6e",
            "title": "A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models",
            "abstract": "The tendency of Large Language Models to generate hallucinations and exhibit overconfidence in predictions raises concerns regarding their reliability. Confidence or uncertainty estimations indicating the extent of trustworthiness of a model's response are essential to developing reliable AI systems. Current research primarily focuses on LLM confidence estimations in English, remaining a void for other widely used languages and impeding the global development of reliable AI applications. This paper introduces a comprehensive investigation of Multi-lingual confidence estimation (MlingConf) on LLMs. First, we introduce an elaborated and expert-checked multilingual QA dataset. Second, we delve into the performance of confidence estimations and examine how these confidence scores can enhance LLM performance through self-refinement across diverse languages. Finally, we propose a cross-lingual confidence estimation method to achieve more precise confidence scores. The experimental results showcase the performance of various confidence estimation methods across different languages as well as present that our proposed cross-lingual confidence estimation technique significantly enhances confidence estimation and outperforms several baseline methods.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A comprehensive investigation of Multi-lingual confidence estimation (MlingConf) on LLMs is introduced, an elaborated and expert-checked multilingual QA dataset is introduced, and a cross-lingual confidence estimation method is proposed to achieve more precise confidence scores."
            },
            "score": 4
        },
        {
            "id": "8ae920111435a7db8da360c654c771c53f57c69a",
            "paperId": "8ae920111435a7db8da360c654c771c53f57c69a",
            "title": "Uncertainty Estimation of Transformer Predictions for Misclassification Detection",
            "abstract": "Uncertainty estimation (UE) of model predictions is a crucial step for a variety of tasks such as active learning, misclassification detection, adversarial attack detection, out-of-distribution detection, etc. Most of the works on modeling the uncertainty of deep neural networks evaluate these methods on image classification tasks. Little attention has been paid to UE in natural language processing. To fill this gap, we perform a vast empirical investigation of state-of-the-art UE methods for Transformer models on misclassification detection in named entity recognition and text classification tasks and propose two computationally efficient modifications, one of which approaches or even outperforms computationally intensive methods.",
            "year": 2022,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A vast empirical investigation of state-of-the-art UE methods for Transformer models on misclassification detection in named entity recognition and text classification tasks and two computationally efficient modifications are proposed, one of which approaches or even outperforms computationally intensive methods."
            },
            "score": 4
        },
        {
            "id": "b9ca30eeb3c3fbb90121e4fdd24daf07ccca6f25",
            "paperId": "b9ca30eeb3c3fbb90121e4fdd24daf07ccca6f25",
            "title": "Semi-Supervised Segmentation of Radiation-Induced Pulmonary Fibrosis From Lung CT Scans With Multi-Scale Guided Dense Attention",
            "abstract": "Computed Tomography (CT) plays an important role in monitoring radiation-induced Pulmonary Fibrosis (PF), where accurate segmentation of the PF lesions is highly desired for diagnosis and treatment follow-up. However, the task is challenged by ambiguous boundary, irregular shape, various position and size of the lesions, as well as the difficulty in acquiring a large set of annotated volumetric images for training. To overcome these problems, we propose a novel convolutional neural network called PF-Net and incorporate it into a semi-supervised learning framework based on Iterative Confidence-based Refinement And Weighting of pseudo Labels (I-CRAWL). Our PF-Net combines 2D and 3D convolutions to deal with CT volumes with large inter-slice spacing, and uses multi-scale guided dense attention to segment complex PF lesions. For semi-supervised learning, our I-CRAWL employs pixel-level uncertainty-based confidence-aware refinement to improve the accuracy of pseudo labels of unannotated images, and uses image-level uncertainty for confidence-based image weighting to suppress low-quality pseudo labels in an iterative training process. Extensive experiments with CT scans of Rhesus Macaques with radiation-induced PF showed that: 1) PF-Net achieved higher segmentation accuracy than existing 2D, 3D and 2.5D neural networks, and 2) I-CRAWL outperformed state-of-the-art semi-supervised learning methods for the PF lesion segmentation task. Our method has a potential to improve the diagnosis of PF and clinical assessment of side effects of radiotherapy for lung cancers.",
            "year": 2021,
            "citationCount": 41,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel convolutional neural network called PF-Net is proposed and incorporated into a semi-supervised learning framework based on Iterative Confidence-based Refinement And Weighting of pseudo Labels (I-CRAWL), which has a potential to improve the diagnosis of PF and clinical assessment of side effects of radiotherapy for lung cancers."
            },
            "score": 4
        },
        {
            "id": "a484a81c51579b6af9bc6fdfb6f7b550c12c66eb",
            "paperId": "a484a81c51579b6af9bc6fdfb6f7b550c12c66eb",
            "title": "Graph-enhanced U-Net for semi-supervised segmentation of pancreas from abdomen CT scan",
            "abstract": "Objective. Accurate segmentation of the pancreas from abdomen CT scans is highly desired for diagnosis and treatment follow-up of pancreatic diseases. However, the task is challenged by large anatomical variations, low soft-tissue contrast, and the difficulty in acquiring a large set of annotated volumetric images for training. To overcome these problems, we propose a new segmentation network and a semi-supervised learning framework to alleviate the lack of annotated images and improve the accuracy of segmentation. Approach. In this paper, we propose a novel graph-enhanced pancreas segmentation network (GEPS-Net), and incorporate it into a semi-supervised learning framework based on iterative uncertainty-guided pseudo-label refinement. Our GEPS-Net plugs a graph enhancement module on top of the CNN-based U-Net to focus on the spatial relationship information. For semi-supervised learning, we introduce an iterative uncertainty-guided refinement process to update pseudo labels by removing low-quality and incorrect regions. Main results. Our method was evaluated by a public dataset with four-fold cross-validation and achieved the DC of 84.22%, improving 5.78% compared to the baseline. Further, the overall performance of our proposed method was the best compared with other semi-supervised methods trained with only 6 or 12 labeled volumes. Significance. The proposed method improved the segmentation performance of the pancreas in CT images under the semi-supervised setting. It will assist doctors in early screening and making accurate diagnoses as well as adaptive radiotherapy.",
            "year": 2022,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The proposed GEPS-Net plugs a graph enhancement module on top of the CNN-based U-Net to focus on the spatial relationship information and improved the segmentation performance of the pancreas in CT images under the semi-supervised setting."
            },
            "score": 4
        },
        {
            "id": "8432d7e251e781fd98b3ca68e8e818fb3c365f56",
            "paperId": "8432d7e251e781fd98b3ca68e8e818fb3c365f56",
            "title": "Resilience analysis for engineered and infrastructure systems under deep uncertainty or emergent conditions",
            "abstract": "In this paper, we discuss a proposed resilience analysis metric for measuring resilience based on three resilience capacities: adaptive capacity, absorptive capacity, and recoverability. The proposed resiliency factor and uncertainty-weighted resilience metric may enable planners and managers to explore the performance of engineered systems under deep uncertainty by encoding assumptions about the resilience of engineered systems to highly improbable, catastrophic events in an entropy-based approach. We discuss how our metric might promote the development of methodologies for investigating \u201cdeep\u201d uncertainties in resilience assessment while retaining the use of probability for expressing uncertainties about highly uncertain, unforeseeable, or unknowable hazards in design and management activities using a computational example and discussion of practical difficulties in the implementation of our resilience framework. cussion, while also suggesting the use of the proposed metric in situations of deep uncertainty. This paper has two primary objectives. 2.1 Objective 1: Synthesis of a new resilience perspective This paper is a discussion paper aimed at synthesizing a new resilience perspective from tools developed in the fields of deep uncertainty, decision analysis, and reliability analysis. 2.2 Objective 2: Propose a quantitative approach to facilitate creative deliberation We argue that the most important difficulty in developing resilience theories is facilitating creative deliberation about potential adverse scenarios. 3 SYNTHESIS OF SYSTEM RESILIENCE UNDER DEEP UNCERTAINTY 3.1 Approaches to Deep Uncertainty The shortcomings of SEU theory addressed by deep uncertainty have been acknowledged for some time in the research community. Several quantitative techniques have been developed. Here, we discuss modeling to generate alternatives, portfolio evaluation and info-gap theory, and resilience analysis. 3.2 Modeling to Generate Alternatives One of the first approaches to evaluating alternatives in the presence of deep uncertainty was modeling to generate alternatives (MGA) using the \u201chop, skip, jump\u201d (HSJ) algorithm (Brill et al. 1982). MGA was designed to construct a set of alternatives similar to one another in objective space (e.g., obtained similar scores on a multi-objective scoring function) but strikingly different from one another in decision space. Under this circumstance, SEU theory may not only fail to account for slight differences in preference structures, but also may fail to give the analyst the ability to identify alternatives robust to variations in underlying model assumptions or stakeholder preferences. Many similar techniques are now built using genetic algorithms, genetic programs, particle swarm optimization, ant colony optimization, or other bio-inspired techniques in which a population of near optimal solutions are identified. The principal insight contributed by the MGA approach is the use of modeling to facilitate creative interpretations of decision contexts. Creative interpretation of the decision context is achieving a thorough search through the potential futures to identify events of concern, then identifying the consequences of those events of concern. Because well-tested systems exhibit an increasingly large share of their failures in the tails of the event generation models, this creativity in exploring the decision situation is of utmost importance. 3.3 Portfolio Evaluation and Info-Gap Decision Theory The main advance of current approaches to deep uncertainty compared with MGA is the focus on the construction and interpretation of models designed to represent the extreme conditions to which the complex engineered system may be exposed. To this end, techniques such as robust decision making (Lempert & Groves 2010), info-gap theory (BenHaim 2006), and portfolio evaluation (Karvetski & Lambert 2012) can provide better decision support than modeling to generate alternatives. Portfolio evaluation and info-gap decision theory embrace a non-probabilistic approach to exploring the responses of engineered systems to possible futures. In portfolio evaluation, deep uncertainties are considered stakeholder viewpoints that may have important differential implications for the desirability or performance of investments. On the other hand, info-gap theory evaluates the robustness and opportuneness of system performance under a range of possible deviations from the \u201cbest-guess\u201d scenario. The ranges of plausible deviations are called \u201cfrontiers,\u201d and are not treated probabilistically. Instead, the system performance is simulated for each plausible frontier, an opportuneness curve (optimum for each frontier) and robustness curve (minimax for each frontier) is obtained. 3.4 Incorporating Resilience Into Deep Uncertainty Evaluation In incorporating resilience into the evaluation of alternatives under deep uncertainty, a few insights may be available. First, the influence of subjective uncertainty, even when the form of the model and its true parameters are not available for validation, may be useful proxies for preparedness and overconfidence. The entropy metric may present a useful way to combat overconfidence in a decision support situation because it will \u201cpenalize\u201d the importance ranking of potential alternatives based on the extent to which stakeholders cannot agree on a model for the plausible futures. This disagreement may encourage deliberations where such model critique may not have taken place. In situations where a decision support tool automatically generates potential alternatives from possible futures, ensembles of models and fault generation techniques can be used to explore the conditions to which a system may be exposed through the use of popular computing techniques, including those mentioned above. For example, Monte Carlo simulation can be used to explore the joint posterior distribution for hazard generation, fault occurrence, and consequences of adverse events. The entropy-weighted metric can then enable prioritization of resilience investments via weighting the performance aspects of the resilience metric with the extremeness of the event\u2019s possibility. Finally, the degree to which we are surprised by an event may be a reasonable proxy for the maximum attainable preparedness for an event. The future preparedness cannot be observed, nor can subjective beliefs about preparedness be validated, but incorporating surprise into a metric used to prioritize investments may enable analysts to operationalize the idea that we cannot fully prepare for an event we cannot even anticipate. 4 A PROPOSED FRAMEWORK AND METRIC FOR SYSTEM RESILIENCE 4.1 The Resilience Capacity Framework Underlying differences in existing resilience definitions, in most cases, can be attributed to differences in their description of resilience capacities. In addition, resilience capacities may be confounded with the factors that affect their attainment and are often included in defining resilience. In this section, we operationalize our definition by developing a resilience assessment framework. This framework, illustrated in Figure 1, consists of five components: system identification, vulnerability analysis, resilience objective setting, and stakeholder engagement and resilience capacities. System identification entails: definition of the system domain; delineation of fundamental and strategic objectives; identification and characterization of physical, chemical, spatial, or social characteristics; and identification of analytical goals and objectives. Because the occurrence of disruptive events cannot be perfectly predicted due to epistemic or aleatory uncertainty, it is important to evaluate the vulnerability of the system to plausible disruptive future events. To account for dynamic conditions, vulnerability analysis at regular intervals is a key to proactively identifying disruptive events and continuously learning from incidents. Iterative, continual assessment of the system\u2019s resilience may enable proactive identification of vulnerabilities. Moreover, this practice facilitates continual critical evaluation and refinement of the organization\u2019s risk model, with the recognition that in complex systems catastrophic failures may be inevitable. The ultimate goal of resilience is the continuity of system performance. Desired system performance is to be defined according to the fundamental objectives obtained in system identification. These fundamental objectives guide the analyst or organization through the navigation of multiple objectives extant in normal system function. A certain set of resilience actions may be delegated on the basis of system recovery priorities. For example, in resilience analysis the decision context for resilience actions might be limited to a predetermined time immediately following system disruption. In this way, the analyst or organization may evaluate resilience actions in a different frame from their overall strategic decision processes. Some researchers suggest complex engineered systems should adapt to their changing environment by permitting some critical system functionality to expire while new system performance characteristics emerge (Mu et al. 2011; Park et al. 2012; Park et al. 2011). Finally, stakeholders are an integral part of resilience analysis and management. In the case of critical infrastructure, for instance, the NIAC recommends coordination among varying levels of government and Critical Infrastructure and Key Resources sectors for efficient recovery of regular services during disruption (DHS Risk Steering Committee 2008). In order to facilitate continuous coordination between the private and public sector, the Australian government, for example, has established the Trusted Information Sharing Network, which enables owners and operators to discuss their vulnerabilities in a non-competitive platform (Commonwealth of Australia 2010). The ultimate goal is to effectiv",
            "year": 2013,
            "citationCount": 9,
            "tldr": null,
            "score": 4
        },
        {
            "id": "57865d9894e1bf199fd566e963c8f58057dc9042",
            "paperId": "57865d9894e1bf199fd566e963c8f58057dc9042",
            "title": "Words into Action: Learning Diverse Humanoid Robot Behaviors using Language Guided Iterative Motion Refinement",
            "abstract": "Humanoid robots are well suited for human habitats due to their morphological similarity, but developing controllers for them is a challenging task that involves multiple sub-problems, such as control, planning and perception. In this paper, we introduce a method to simplify controller design by enabling users to train and fine-tune robot control policies using natural language commands. We first learn a neural network policy that generates behaviors given a natural language command, such as\"walk forward\", by combining Large Language Models (LLMs), motion retargeting, and motion imitation. Based on the synthesized motion, we iteratively fine-tune by updating the text prompt and querying LLMs to find the best checkpoint associated with the closest motion in history. We validate our approach using a simulated Digit humanoid robot and demonstrate learning of diverse motions, such as walking, hopping, and kicking, without the burden of complex reward engineering. In addition, we show that our iterative refinement enables us to learn 3x times faster than a naive formulation that learns from scratch.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a method to simplify controller design by enabling users to train and fine-tune robot control policies using natural language commands, by combining Large Language Models, motion retargeting, and motion imitation."
            },
            "score": 3
        },
        {
            "id": "4cc1b3aecef868c9d56adc4e6d8a1116774faef9",
            "paperId": "4cc1b3aecef868c9d56adc4e6d8a1116774faef9",
            "title": "Pose-Oriented Transformer with Uncertainty-Guided Refinement for 2D-to-3D Human Pose Estimation",
            "abstract": "There has been a recent surge of interest in introducing transformers to 3D human pose estimation (HPE) due to their powerful capabilities in modeling long-term dependencies. However, existing transformer-based methods treat body joints as equally important inputs and ignore the prior knowledge of human skeleton topology in the self-attention mechanism. To tackle this issue, in this paper, we propose a Pose-Oriented Transformer (POT) with uncertainty guided refinement for 3D HPE. Specifically, we first develop novel pose-oriented self-attention mechanism and distance-related position embedding for POT to explicitly exploit the human skeleton topology. The pose-oriented self-attention mechanism explicitly models the topological interactions between body joints, whereas the distance-related position embedding encodes the distance of joints to the root joint to distinguish groups of joints with different difficulties in regression. Furthermore, we present an Uncertainty-Guided Refinement Network (UGRN) to refine pose predictions from POT, especially for the difficult joints, by considering the estimated uncertainty of each joint with uncertainty-guided sampling strategy and self-attention mechanism. Extensive experiments demonstrate that our method significantly outperforms the state-of-the-art methods with reduced model parameters on 3D HPE benchmarks such as Human3.6M and MPI-INF-3DHP.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Pose-Oriented Transformer with uncertainty guided refinement for 3D HPE and an Uncertainty-Guided Refinement Network (UGRN) to refine pose predictions from POT, especially for the difficult joints, by considering the estimated uncertainty of each joint with uncertainty-guided sampling strategy and self-attention mechanism."
            },
            "score": 3
        },
        {
            "id": "215de09ac6e5de81187c85065b5ace8bc01f2862",
            "paperId": "215de09ac6e5de81187c85065b5ace8bc01f2862",
            "title": "Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models",
            "abstract": "The complementary potential of Large Language Models (LLM) assumes off-the-shelf LLMs have heterogeneous expertise in a wide range of domains and tasks so that an ensemble of LLMs can achieve consistently better performance. Existing ensemble methods for LLMs mainly focus on reward model ranking of outputs, leading to significant computation overhead. To combat this issue, we revisit the complementary potential of LLMs and further elaborate it by mining latent expertise with off-the-shelf reward models. We propose Zooter, a reward-guided routing method distilling rewards on training queries to train a routing function, which can precisely distribute each query to the LLM with expertise about it. We also integrate a tag-based label enhancement to mitigate noise from uncertainty when using rewards as silver supervision. Zooter shows computation efficiency in inference as it introduces only a minor computation overhead of a routing function compared with reward model ranking methods. We evaluate Zooter on a comprehensive benchmark collection with 26 subsets on different domains and tasks. Zooter outperforms the best single model on average and ranks first on 44% of tasks, even surpassing multiple reward model ranking methods.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Zooter is proposed, a reward-guided routing method distilling rewards on training queries to train a routing function, which can precisely distribute each query to the LLM with expertise about it, and shows computation efficiency in inference."
            },
            "score": 3
        },
        {
            "id": "d2d16333a4b0dc7e3463b280b9945e5ee6c53396",
            "paperId": "d2d16333a4b0dc7e3463b280b9945e5ee6c53396",
            "title": "TrueTeacher: Learning Factual Consistency Evaluation with Large Language Models",
            "abstract": "Factual consistency evaluation is often conducted using Natural Language Inference (NLI) models, yet these models exhibit limited success in evaluating summaries. Previous work improved such models with synthetic training data. However, the data is typically based on perturbed human-written summaries, which often differ in their characteristics from real model-generated summaries and have limited coverage of possible factual errors. Alternatively, large language models (LLMs) have recently shown promising results in directly evaluating generative tasks, but are too computationally expensive for practical use. Motivated by these limitations, we introduce TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM. Unlike prior work, TrueTeacher does not rely on human-written summaries, and is multilingual by nature. Experiments on the TRUE benchmark show that a student model trained using our data, substantially outperforms both the state-of-the-art model with similar capacity, and the LLM teacher. In a systematic study, we compare TrueTeacher to existing synthetic data generation methods and demonstrate its superiority and robustness to domain-shift. We also show that our method generalizes to multilingual scenarios. Lastly, we release our large scale synthetic dataset (1.4M examples), generated using TrueTeacher, and a checkpoint trained on this data.",
            "year": 2023,
            "citationCount": 31,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces TrueTeacher, a method for generating synthetic data by annotating diverse model-generated summaries using a LLM, which does not rely on human-written summaries, and is multilingual by nature."
            },
            "score": 3
        },
        {
            "id": "468d1e2d75a23fecaf96fe65d8b01ff35ea5d0bd",
            "paperId": "468d1e2d75a23fecaf96fe65d8b01ff35ea5d0bd",
            "title": "Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models",
            "abstract": "Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts inserted in English reveal a clear pattern whereby the new piece of knowledge transfers only to languages with which English has a high RankC score.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy, and conducts an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level."
            },
            "score": 3
        },
        {
            "id": "56373d3fd0f1354a61f9e577db039cdb187d8d43",
            "paperId": "56373d3fd0f1354a61f9e577db039cdb187d8d43",
            "title": "Evaluating the Factual Consistency of Large Language Models Through News Summarization",
            "abstract": "While large language models (LLMs) have proven to be effective on a large variety of tasks, they are also known to hallucinate information. To measure whether an LLM prefers factually consistent continuations of its input, we propose a new benchmark called FIB(Factual Inconsistency Benchmark) that focuses on the task of summarization. Specifically, our benchmark involves comparing the scores an LLM assigns to a factually consistent versus a factually inconsistent summary for an input news article. For factually consistent summaries, we use human-written reference summaries that we manually verify as factually consistent. To generate summaries that are factually inconsistent, we generate summaries from a suite of summarization models that we have manually annotated as factually inconsistent. A model's factual consistency is then measured according to its accuracy, i.e.\\ the proportion of documents where it assigns a higher score to the factually consistent summary. To validate the usefulness of FIB, we evaluate 23 large language models ranging from 1B to 176B parameters from six different model families including BLOOM and OPT. We find that existing LLMs generally assign a higher score to factually consistent summaries than to factually inconsistent summaries. However, if the factually inconsistent summaries occur verbatim in the document, then LLMs assign a higher score to these factually inconsistent summaries than factually consistent summaries. We validate design choices in our benchmark including the scoring method and source of distractor summaries. Our code and benchmark data can be found at https://github.com/r-three/fib.",
            "year": 2022,
            "citationCount": 51,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new benchmark called FIB(Factual Inconsistency Benchmark) that focuses on the task of summarization, and finds that existing LLMs generally assign a higher score to factually consistent summaries than tofactually inconsistent summaries."
            },
            "score": 3
        },
        {
            "id": "d479ef0ece2425042c2a80307ea154c85a9b14f9",
            "paperId": "d479ef0ece2425042c2a80307ea154c85a9b14f9",
            "title": "Uncertainty Estimation for Debiased Models: Does Fairness Hurt Reliability?",
            "abstract": "When deploying a machine learning model, one should aim not only to optimize performance metrics such as accuracy but also care about model fairness and reliability. Fairness means that the model is prevented from learning spurious correlations between a target variable and socio-economic attributes, and is generally achieved by applying debiasing techniques. Model reliability stems from the ability to determine whether we can trust model predictions for the given data. This can be achieved using uncertainty estimation (UE) methods. Debi-asing and UE techniques potentially interfere with each other, raising the question of whether we can achieve both reliability and fairness at the same time. This work aims to answer this question empirically based on an extensive series of experiments combining state-of-the-art UE and debiasing methods, and examining the impact on model performance, fairness, and reliability. 1",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work aims to answer the question whether a machine learning model can achieve both reliability and fairness at the same time empirically based on an extensive series of experiments combining state-of-the-art UE and debiasing methods, and examining the impact on model performance, fairness, and reliability."
            },
            "score": 3
        },
        {
            "id": "f2d0f9309a4ca6e9d712f72778a9bcf083ace077",
            "paperId": "f2d0f9309a4ca6e9d712f72778a9bcf083ace077",
            "title": "Uncertainty estimation in deep learning with application to spoken language assessment",
            "abstract": "Since convolutional neural networks (CNNs) achieved top performance on the ImageNet task in 2012, deep learning has become the preferred approach to addressing computer vision, natural language processing, speech recognition and bio-informatics tasks. However, despite impressive performance, neural networks tend to make over-confident predictions. Thus, it is necessary to investigate robust, interpretable and tractable estimates of uncertainty in a model\u2019s predictions in order to construct safer Machine Learning systems. This is crucial to applications where the cost of an error is high, such as in autonomous vehicle control, high-stakes automatic proficiency assessment and in the medical, financial and legal fields. In the first part of this thesis uncertainty estimation via ensemble and single-model approaches is discussed in detail and a new class of models for uncertainty estimation, called Prior Networks, is proposed. Prior Networks are able to emulate an ensemble of models using a single deterministic neural network, which allows sources of uncertainty to be determined within the same probabilistic framework as in ensemble-based approaches, but with the computational simplicity and ease of training of single-model approaches. Thus, Prior Networks combine the advantages of ensemble and single-model approaches to estimating uncertainty. In this thesis Prior Networks are evaluated on a range classification datasets, where they are shown to outperform baseline approaches, such as Monte-Carlo dropout, on the task of detecting out-of-distribution inputs. In the second part of this thesis deep learning and uncertainty estimation approaches are applied to the area of automatic assessment of non-native spoken language proficiency. Specifically deep-learning based graders and spoken response relevance assessment systems are constructed using data from the BULATS and LinguaSkill exams, provided by Cambridge English Language Assessment. Baseline approaches for uncertainty estimation discussed and evaluated in the first half of the thesis are then applied to these models and assessed on the task of rejecting predictions to be graded by human examiners and detecting misclassifications.",
            "year": 2019,
            "citationCount": 63,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Prior Networks combine the advantages of ensemble and single-model approaches to estimating uncertainty and are evaluated on a range classification datasets, where they are shown to outperform baseline approaches on the task of detecting out-of-distribution inputs."
            },
            "score": 3
        },
        {
            "id": "a860ba337cead5e2e970460522d6612a49836ff1",
            "paperId": "a860ba337cead5e2e970460522d6612a49836ff1",
            "title": "Uncertainty Estimation of Transformers' Predictions via Topological Analysis of the Attention Matrices",
            "abstract": "Determining the degree of confidence of deep learning model in its prediction is an open problem in the field of natural language processing. Most of the classical methods for uncertainty estimation are quite weak for text classification models. We set the task of obtaining an uncertainty estimate for neural networks based on the Transformer architecture. A key feature of such mo-dels is the attention mechanism, which supports the information flow between the hidden representations of tokens in the neural network. We explore the formed relationships between internal representations using Topological Data Analysis methods and utilize them to predict model's confidence. In this paper, we propose a method for uncertainty estimation based on the topological properties of the attention mechanism and compare it with classical methods. As a result, the proposed algorithm surpasses the existing methods in quality and opens up a new area of application of the attention mechanism, but requires the selection of topological features.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a method for uncertainty estimation based on the topological properties of the attention mechanism and compares it with classical methods, which surpasses the existing methods in quality and opens up a new area of application of the Attention mechanism, but requires the selection of topological features."
            },
            "score": 3
        },
        {
            "id": "2fe059674c641149ece0dac5d26a67779401e340",
            "paperId": "2fe059674c641149ece0dac5d26a67779401e340",
            "title": "Model Based Root Cause Analysis of Manufacturing Quality Problems Using Uncertainty Quantification and Sensitivity Analysis",
            "abstract": "\n Diagnosing faulty performance deviations of electro-mechanical systems can be difficult, given the multitude of components and features which could contribute as root causes. Yet this is often a problem in manufacturing, where only some of the units built do not meet performance requirements only some of the time. In this context, product and process simulation studies can aid in diagnosis. This paper aims to develop a practical workflow and toolchain to guide use of uncertainty quantification and sensitivity analysis methods for root cause analysis of manufacturing processes. This approach offers more rapid diagnosis than the typical approach using some form of iterative experimentation such as Red-X, fault tree analysis and when in high volume production, statistical analysis and potentially machine learning. Here, part processes, features and assembly deviations are used as inputs to product performance simulation to understand their detrimental impact. The large set of possible process inputs can be systematically varied and contributions to system performance deviation computed. To do this simply using uncertainty quantification and sensitivity analysis is impractical, as the problem is too large. Rather, a sequential refinement workflow is developed to define the problem and possible causes, understand ability model causes, screen causal variables, and then apply quasi-Monte-Carlo uncertainty quantification sampling and global sensitivity analysis. This provides computational guidance to ascertain which manufacturing process inputs are more likely causes of performance deviations on manufactured units.",
            "year": 2019,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A sequential refinement workflow is developed to define the problem and possible causes, understand ability model causes, screen causal variables, and then apply quasi-Monte-Carlo uncertainty quantification sampling and global sensitivity analysis to provide computational guidance to ascertain which manufacturing process inputs are more likely causes of performance deviations on manufactured units."
            },
            "score": 3
        },
        {
            "id": "af4cfdb764bb55feeb0e1eb33b5f00ede15562a7",
            "paperId": "af4cfdb764bb55feeb0e1eb33b5f00ede15562a7",
            "title": "Weakly Supervised Referring Expression Grounding via Dynamic Self-Knowledge Distillation",
            "abstract": "Weakly supervised referring expression grounding (WREG) is an attractive and challenging task for grounding target regions in images by understanding given referring expressions. WREG learns to ground target objects without the manual annotations between image regions and referring expressions during the model training phase. Different from the predominant grounding pattern of existing models, which locates target objects by reconstructing the region-expression correspondence, we investigate WREG from a novel perspective and enrich the prevailing pattern with self-knowledge distillation. Specifically, we propose a target-guided self-knowledge distillation approach that adopts the target prediction knowledge learned from the previous training iterations as the teacher to guide the subsequent training procedure. In order to avoid the misleading caused by the teacher knowledge with low prediction confidence, we present an uncertaintyaware knowledge refinement strategy to adaptively rectify the teacher knowledge by learning dynamic threshold values based on the model prediction uncertainty. To validate the proposed approach, we implement extensive experiments on three benchmark datasets, i.e., Ref Coco, RefCOCO+, and RefCOCOg. Our approach achieves new state-of-the-art results on several splits of the benchmark datasets, showcasing the advantage of the proposed framework for WREG. The implementation codes and trained models are available at: https://github.com/dami23IWREG.sar_KD.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A target-guided self-knowledge distillation approach that adopts the target prediction knowledge learned from the previous training iterations as the teacher to guide the subsequent training procedure for WREG is proposed."
            },
            "score": 3
        },
        {
            "id": "8b0aa4ba2f2ac67df4d5d3593a71800bf5a6bdb5",
            "paperId": "8b0aa4ba2f2ac67df4d5d3593a71800bf5a6bdb5",
            "title": "Autoscanning for coupled scene reconstruction and proactive object analysis",
            "abstract": "Detailed scanning of indoor scenes is tedious for humans. We propose autonomous scene scanning by a robot to relieve humans from such a laborious task. In an autonomous setting, detailed scene acquisition is inevitably coupled with scene analysis at the required level of detail. We develop a framework for object-level scene reconstruction coupled with object-centric scene analysis. As a result, the autoscanning and reconstruction will be object-aware, guided by the object analysis. The analysis is, in turn, gradually improved with progressively increased object-wise data fidelity. In realizing such a framework, we drive the robot to execute an iterative analyze-and-validate algorithm which interleaves between object analysis and guided validations. The object analysis incorporates online learning into a robust graph-cut based segmentation framework, achieving a global update of object-level segmentation based on the knowledge gained from robot-operated local validation. Based on the current analysis, the robot performs proactive validation over the scene with physical push and scan refinement, aiming at reducing the uncertainty of both object-level segmentation and object-wise reconstruction. We propose a joint entropy to measure such uncertainty based on segmentation confidence and reconstruction quality, and formulate the selection of validation actions as a maximum information gain problem. The output of our system is a reconstructed scene with both object extraction and object-wise geometry fidelity.",
            "year": 2015,
            "citationCount": 74,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work develops a framework for object-level scene reconstruction coupled with object-centric scene analysis, and proposes a joint entropy to measure such uncertainty based on segmentation confidence and reconstruction quality, and formulate the selection of validation actions as a maximum information gain problem."
            },
            "score": 3
        },
        {
            "id": "1a54aa0da5b58ad33f2d75c3d04ee047de110c0c",
            "paperId": "1a54aa0da5b58ad33f2d75c3d04ee047de110c0c",
            "title": "Uncertainty-Integrated Surrogate Modeling for Complex System Optimization",
            "abstract": "Approximation models such as surrogate models provide a tractable substitute to expensive physical simulations and an effective solution to the potential lack of quantitative models of system behavior. These capabilities not only enable the efficient design of complex systems, but is also essential for the effective analysis of physical phenomena/characteristics in the different domains of Engineering, Material Science, Biomedical Science, and various other disciplines. Since these models provide an abstraction of the real system behavior (often a low-fidelity representative) it is important to quantify the accuracy and the reliability of such approximation models without investing additional expensive system evaluations (simulations or physical experiments). Standard error measures, such as the mean squared error, the cross-validation error, and the Akaike\u2019s information criterion however provide limited (often inadequate) information regarding the accuracy of the final surrogate model while other more effective dedicated error measures are tailored towards only one class of surrogate models. This lack of accuracy information and the ability to compare and test diverse surrogate models reduce the confidence in model application, restricts appropriate model selection, and undermines the effectiveness of surrogate-based optimization. A key contribution of this dissertation is the development of a new model-independent approach to quantify the fidelity of a trained surrogate model in a given region of the design domain. This method is called the Predictive Estimation of Model Fidelity (PEMF). The PEMF method is derived from the hypothesis that \u201cthe accuracy of an approximation model is related to the amount of data resources leveraged to train the model\u201d. In PEMF, intermediate surrogate models are iteratively constructed over heuristic subsets of sample points. The median and the maximum errors estimated over the remaining points are used to determine the respective error distributions at each iteration. The estimated modes of the error distributions are represented as functions of the density of intermediate training points through nonlinear regression, assuming a smooth decreasing trend of errors with increasing sample density. These regression functions are then used to predict the expected median and maximum errors in the final surrogate models. It is observed that the model fidelities estimated by PEMF are up to two orders of magnitude more accurate and statistically more stable compared to those based on the popularly-used leave-one-out cross-validation method, when applied to a variety of benchmark problems. By leveraging this new paradigm in quantifying the fidelity of surrogate models, a novel automated surrogate model selection framework is also developed. This PEMF-based model selection framework is called the Concurrent Surrogate Model Selection (COSMOS). COSMOS, unlike existing model selection methods, coherently operates at all the three levels necessary to facilitate optimal selection, i.e., (1) selecting the model type, (2) selecting the kernel function type, and (3) determining the optimal values of the typically user-prescribed parameters. The selection criteria that guide optimal model selection are determined by PEMF and the search process is performed using a MINLP solver. The effectiveness of COSMOS is demonstrated by successfully applying it to different benchmark and practical engineering problems, where it offers a first-of-its-kind globally competitive model selection. In this dissertation, the knowledge about the accuracy of a surrogate estimated using PEMF is applied to also develop a novel model management approach for engineering optimization. This approach adaptively selects computational models (both physics-based models and surrogate models) of differing levels of fidelity and computational cost, to be used during optimization, with the overall objective to yield optimal designs with high-fidelity function estimates at a reasonable computational expense. In this technique, a new adaptive model switching (AMS) metric defined to guide the switching of model from one to the next higher fidelity model during the optimization process. The switching criterion is based on whether the uncertainty associated with the current model output dominates the latest improvement of the relative fitness function, where both the model output uncertainty and the function improvement (across the population) are expressed as probability distributions. This adaptive model switching technique is applied to two practical problems through Particle Swarm Optimization to successfully illustrate: (i) the computational advantage of this method over purely high-fidelity model-based optimization, and (ii) the accuracy advantage of this method over purely low-fidelity model-based optimization. Motivated by the unique capabilities of the model switching concept, a new model refinement approach is also developed in this dissertation. The model refinement approach can be perceived as an adaptive sequential sampling approach applied in surrogate-based optimization. Decisions regarding when to perform additional system evaluations to refine the model is guided by the same model-uncertainty principles as in the adaptive model switching technique. The effectiveness of this new model refinement technique is illustrated through application to practical surrogate-based optimization in the area of energy sustainability. UNCERTAINTY-INTEGRATED SURROGATE MODELING FOR COMPLEX SYSTEM OPTIMIZATION",
            "year": 2015,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A key contribution of this dissertation is the development of a new model-independent approach to quantify the fidelity of a trained surrogate model in a given region of the design domain using the Predictive Estimation of Model Fidelity (PEMF)."
            },
            "score": 3
        },
        {
            "id": "4a205ea2d9f1502a74e4e42d9caebe6904450eca",
            "paperId": "4a205ea2d9f1502a74e4e42d9caebe6904450eca",
            "title": "Sensitivity-driven simulation development: a case study in forced migration",
            "abstract": "This paper presents an approach named sensitivity-driven simulation development (SDSD), where the use of sensitivity analysis (SA) guides the focus of further simulation development and refinement efforts, avoiding direct calibration to validation data. SA identifies assumptions that are particularly pivotal to the validation result, and in response model ruleset refinement resolves those assumptions in greater detail, balancing the sensitivity more evenly across the different assumptions and parameters. We implement and demonstrate our approach to refine agent-based models of forcibly displaced people in neighbouring countries. Over 70.8 million people are forcibly displaced worldwide, of which 26 million are refugees fleeing from armed conflicts, violence, natural disaster or famine. Predicting forced migration movements is important today, as it can help governments and NGOs to effectively assist vulnerable migrants and efficiently allocate humanitarian resources. We use an initial SA iteration to steer the simulation development process and identify several pivotal parameters. We then show that we are able to reduce the relative sensitivity of these parameters in a secondary SA iteration by approximately 54% on average. This article is part of the theme issue \u2018Reliability and reproducibility in computational science: implementing verification, validation and uncertainty quantification in silico\u2019.",
            "year": 2021,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents an approach named sensitivity-driven simulation development (SDSD), where the use of sensitivity analysis (SA) guides the focus of further simulation development and refinement efforts, avoiding direct calibration to validation data."
            },
            "score": 3
        },
        {
            "id": "e962f95e03a50ff2f3a0fe7840daebac04578c46",
            "paperId": "e962f95e03a50ff2f3a0fe7840daebac04578c46",
            "title": "Structure-informed Language Models Are Protein Designers",
            "abstract": "This paper demonstrates that language models are strong structure-based protein designers. We present LM-Design, a generic approach to reprogramming sequence-based protein language models (pLMs), that have learned massive sequential evolutionary knowledge from the universe of natural protein sequences, to acquire an immediate capability to design preferable protein sequences for given folds. We conduct a structural surgery on pLMs, where a lightweight structural adapter is implanted into pLMs and endows it with structural awareness. During inference, iterative refinement is performed to effectively optimize the generated protein sequences. Experiments show that LM-Design improves the state-of-the-art results by a large margin, leading to 4% to 12% accuracy gains in sequence recovery (e.g., 55.65%/56.63% on CATH 4.2/4.3 single-chain benchmarks, and >60% when designing protein complexes). We provide extensive and in-depth analyses, which verify that LM-Design can (1) indeed leverage both structural and sequential knowledge to accurately handle structurally non-deterministic regions, (2) benefit from scaling data and model size, and (3) generalize to other proteins (e.g., antibodies and de novo proteins).",
            "year": 2023,
            "citationCount": 37,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LM-Design, a generic approach to reprogramming sequence-based protein language models (pLMs), that have learned massive sequential evolutionary knowledge from the universe of natural protein sequences, to acquire an immediate capability to design preferable protein sequences for given folds is presented."
            },
            "score": 2
        },
        {
            "id": "702e80f5932ffb4c7f8cd87fe0ca762352f48398",
            "paperId": "702e80f5932ffb4c7f8cd87fe0ca762352f48398",
            "title": "Dual-Uncertainty Guided Curriculum Learning and Part-Aware Feature Refinement for Domain Adaptive Person Re-Identification",
            "abstract": "Unsupervised Domain Adaptative person re-identification (UDA ReID) aims to transfer the knowledge of pre-trained model from labeled source domain to unlabeled target domain. Although the current clustering-based methods have achieved promising success, they neglect the tolerance of the model to cope with different-level noise, which may cause the model to memorize some incorrect patterns caused by label noise and overfit on them rapidly in the early stages. In this paper, we introduce a novel Dual Uncertainty guided Curriculum Learning (DUCL) method to tackle the above problems. Specifically, the reliability-based curriculum allocation is proposed to enforce the sample adaptation in an easy-to-hard manner, which is further assisted by a novel dual-uncertainty re-weighting strategy to alleviate the influence of label noise. In addition, we design Part-aware Feature Refinement (PAFR) to enhance the discrimination of model and thereby acquiring more reliable pseudo-labels. Specifically, the part-aware attention maps are exploited in the PAFR to integrate fine-grained semantics into holistic representation. Extensive experiments have validated the superiority of the proposed method.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A reliability-based curriculum allocation is proposed to enforce the sample adaptation in an easy-to-hard manner, which is further assisted by a novel dual-uncertainty re-weighting strategy to alleviate the influence of label noise."
            },
            "score": 2
        },
        {
            "id": "194bc0d5161da426b22cc8fe82a5feeb4716c65b",
            "paperId": "194bc0d5161da426b22cc8fe82a5feeb4716c65b",
            "title": "Sensitivity-guided iterative parameter identification and data generation with BayesFlow and PELS-VAE for model calibration",
            "abstract": null,
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A sensitivity-guided iterative parameter identification and data generation algorithm that replaces manual intervention, the parameter identification is realized by BayesFlow allowing for uncertainty quantification, and the data generation with the physics-enhanced latent space variational autoencoder between two iteration steps enables inference of weakly identifiable parameters."
            },
            "score": 2
        },
        {
            "id": "92e8eb55794b208952cf190f56e9d4663ad049cc",
            "paperId": "92e8eb55794b208952cf190f56e9d4663ad049cc",
            "title": "Natural language processing systems for pathology parsing in limited data environments with uncertainty estimation",
            "abstract": "Abstract Objective Cancer is a leading cause of death, but much of the diagnostic information is stored as unstructured data in pathology reports. We aim to improve uncertainty estimates of machine learning-based pathology parsers and evaluate performance in low data settings. Materials and methods Our data comes from the Urologic Outcomes Database at UCSF which includes 3232 annotated prostate cancer pathology reports from 2001 to 2018. We approach 17 separate information extraction tasks, involving a wide range of pathologic features. To handle the diverse range of fields, we required 2 statistical models, a document classification method for pathologic features with a small set of possible values and a token extraction method for pathologic features with a large set of values. For each model, we used isotonic calibration to improve the model\u2019s estimates of its likelihood of being correct. Results Our best document classifier method, a convolutional neural network, achieves a weighted F1 score of 0.97 averaged over 12 fields and our best extraction method achieves an accuracy of 0.93 averaged over 5 fields. The performance saturates as a function of dataset size with as few as 128 data points. Furthermore, while our document classifier methods have reliable uncertainty estimates, our extraction-based methods do not, but after isotonic calibration, expected calibration error drops to below 0.03 for all extraction fields. Conclusions We find that when applying machine learning to pathology parsing, large datasets may not always be needed, and that calibration methods can improve the reliability of uncertainty estimates.",
            "year": 2020,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that when applying machine learning to pathology parsing, large datasets may not always be needed, and that calibration methods can improve the reliability of uncertainty estimates."
            },
            "score": 2
        },
        {
            "id": "2e2c31fd97fc6ce27640bfc56f4b3ceca4f0cb9c",
            "paperId": "2e2c31fd97fc6ce27640bfc56f4b3ceca4f0cb9c",
            "title": "Uncertainty Estimation for Complex Text Detection in Spanish",
            "abstract": "Text simplifcation refers to the transformation of a source text aiming to increase its readiblity and understandability for a specific target population. This task is an important step towards improving inclusivity of such target populations (i.e., low scholarity or visually/hearing impaired groups). The recent advancements in the field brought by Large Language Models improve the performance of machine based text simplification approaches. However, using Language Models to simplify large text segments can be resource demanding. A more simple model to classify whether the text segment is worth to simplify or not can improve resource efficiency, in order to avoid unnecessary text prompts to the Large Language Models. Furthermore, text simplicity categorization can also be used for other purposes, such as text complexity measurement. The discrimination of text segments into simple and complex categories might lead to a number of false positives or negatives for a not well-tuned model. A way to control the acceptance threshold, is the implementation of an uncertainty score for each prediction. In this work we explore two simple uncertainty estimation approaches for complex text identification: a Monte Carlo Dropout and an Deep Ensemble Based approach. We use an in-house dataset in the financial education domain for our tests. We calibrated the two implemented methods to find out which performs better, using a Jensen-Shannon based distance between the correct and incorrect outputs of the discriminator. Our tests showed an important advantage of the Monte Carlo Dropout over the Deep Ensemble Based method.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores two simple uncertainty estimation approaches for complex text identification: a Monte Carlo Dropout and an Deep Ensemble Based approach, and calibrated the two implemented methods to find out which performs better."
            },
            "score": 2
        },
        {
            "id": "c33263256cc9b529fc1d56bb3d2e79d8821e4144",
            "paperId": "c33263256cc9b529fc1d56bb3d2e79d8821e4144",
            "title": "Initialized Iterative Closest Point for bone recognition in ultrasound volumes",
            "abstract": "Ultrasound (US) probes have been used as guiding tools for Computer Assisted Orthopedic Surgeries (CAOS) [1]. Because of the US data uncertainty, the process of recognition - the localization of regions of interest in the image-requires a registration to a more precise, but invasive, imaging modality such as Computed Tomography (CT). A millimetric precision and a real-time processing are intraoperative requirements. Iterative Closest Point (ICP) [2] is a simple and non symmetric rigid registration algorithm that is sensitive to the initial position of the point sets. The aim of this study is to show the contribution of initializing ICP in rigid US-CT registration and to illustrate it on data of a proximal femur. First, an iterative initialization of the model (CT) to the partial view (US) is performed using ICP with annealed filtering. The first obtained local minimum is then used to initialize a refinement step that maps the partial view to the model. One femur phantom was imaged both in a water bath using a calibrated 3D ultrasound probe and by CT. For each of the ten US acquisitions (five in the Anterior neck A, and five in the Posterior neck P), the CT scan is brought by means of fiducials pair-point matching. The initialization step improves ICP successful registrations from (A:25%, P:21%) to (A:76%, to P:52%) and the registration takes about 3s in average whilst ICP takes about 1s.",
            "year": 2016,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The aim of this study is to show the contribution of initializing ICP in rigid US-CT registration and to illustrate it on data of a proximal femur."
            },
            "score": 2
        },
        {
            "id": "12803a6c858758583dfab0be8e7178844a5cda9c",
            "paperId": "12803a6c858758583dfab0be8e7178844a5cda9c",
            "title": "Efficient Reliability-Based Path Planning of Off-Road Autonomous Ground Vehicles Through the Coupling of Surrogate Modeling and RRT*",
            "abstract": "Reliability-based global path planning incorporates reliability constraints into path planning to ensure that off-road autonomous ground vehicles can operate reliably in uncertain off-road environments. Current two-stage reliability-based path planning methods involve separate stages for surrogate modeling of mobility prediction and global path planning, resulting in a large number of unnecessary mobility simulations that makes the approaches computationally expensive. To tackle this challenge, this work proposes a novel efficient reliability-based global path planning approach, named ER-RRT*, which couples adaptive surrogate modeling with the rapidly-exploring random tree star (RRT*) algorithm. Firstly, a surrogate model for vehicle mobility prediction is used to guide the exploration of random trees subject to a mobility reliability constraint. Subsequently, the exploration trees and reliability assessment are employed to inform mobility simulations for the surrogate model refinement. These steps are implemented iteratively and thereby drastically reducing the required mobility simulations for path planning through the integration of adaptive surrogate modeling with global path planning. With a focus on the uncertainty in the slope map and soil properties of deformable terrain, we demonstrate ER-RRT* using a case study and compare it with the current two-stage approach. The results show that ER-RRT* is much more efficient than the current method in both computational time and the required number of mobility simulations for surrogate model construction. In addition, the path identified by ER-RRT* exhibits a comparable cost in distance to its counterpart obtained using the two-stage method.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel efficient reliability-based global path planning approach, named ER-RRT*, which couples adaptive surrogate modeling with the rapidly-exploring random tree star (RRT*) algorithm, and is much more efficient than the current method in both computational time and the required number of mobility simulations for surrogate model construction."
            },
            "score": 2
        },
        {
            "id": "c6d3a2d83254aa413f45cb4b9fbef62957f19852",
            "paperId": "c6d3a2d83254aa413f45cb4b9fbef62957f19852",
            "title": "Modeling Process, Structure, & Assumptions for Rapid Spacecraft Design and Feasibility Analysis",
            "abstract": "In the early pre-project stages of space mission concept formulation, a concept team faces a complex tradespace with interwoven choices that often have cascading effects. Each option will have an estimated cost associated with it as well as potentially unique technical requirements - such as power, mounting, thermal, data, pointing, etc. - that will cascade to all aspects of a spacecraft design. To add to this already monumental challenge, early formulation can also face a high risk of a mission never making it beyond the concept level, adding a great deal of uncertainty to the outcome of invested resources. The Innovation Foundry at NASA's Jet Propulsion Laboratory (JPL) uses discretized levels for formulation called Concept Maturity Levels (CML). As a concept matures, there is often an iterative process between initial feasibility, tradespace exploration, and point designs or CML 2, 3, and 4 respectively. The Innovation Foundry has developed tooling which enables rapid design and tradespace evaluation while remaining within feasible bounds. Over the last few years, a systems engineering team at the Innovation Foundry have streamlined the Tool for Architectural Tradespace Exploration and Refinement (TATER) to improve efficiency of early spacecraft design and feasibility analysis at the spacecraft element level. TATER's new architecture and user experience enable a more expansive and in-depth exploration of a mission concept tradespace while requiring fewer resources over a shorter period. To accomplish this, the systems engineering team combined several improved approaches compared to prior versions of the tool spanning across process, structure, and default assumptions. First, the team created a brief questionnaire for scientists or project teams looking to iterate through an option tradespace. Next, the team implemented a more intuitive and natural workflow to guide a user through the design process. Finally, the team pre-populated TATER with default assumptions and suggestions while still maintaining a highly customizable set of overrides available to a designer for when design details (i.e. type of avionics or telecom band) are available. The end product of these improvements is a tool that can produce an element-level spacecraft mass and feasibility analysis by one systems engineer in only a few short hours. The team conducted a validation exercise both before and after these improvements and found no significant reduction in the accuracy of TATER. Early space mission concept formulation can benefit from the lessons learned by the systems engineers during this streamlining exercise. A concept formulation team can more efficiently iterate through a larger tradespace with fewer resources invested by refining the data exchange and modeling process, structure and user interaction with a tool, and identifying appropriate default assumptions as a starting point.",
            "year": 2023,
            "citationCount": 0,
            "tldr": null,
            "score": 2
        },
        {
            "id": "9707fc97924944eacc9519d08b50a1a71e7c7c92",
            "paperId": "9707fc97924944eacc9519d08b50a1a71e7c7c92",
            "title": "Research on Web Service Composition Using Probabilistic Abstraction Refinement",
            "abstract": "The Web service composition (WSC) has been widely used in Service-Oriented Architecture (SOA), which is an effective integration of the distributed and heterogeneous business applications. In contrast to the component-based software, dynamic reconfiguration occurs more frequently in Web services-based software for self-adapting and self-managing their computing capabilities due to the uncertainty of dynamic Internet environment. Verifying these stochastic and nondeterministic behaviors is becoming a hot topic in model checking of Web services (WSs) application engineering. Abstraction refinement technique as an effective approach to alleviating the state explosion problem is particularly suitable for verifying the complex WSC. In this paper, we extend the classical abstraction refinement technique CEGAR (Counterexample-guided abstraction refinement) to make quantitative verification of WSC applicable and efficient. To model WSC, a probabilistic service behavior model (p-SBM) is proposed in form of Markov Decision Process (MDP). To verify WSC, the abstraction is defined by means of a quotient on states with respect to some probabilistic equivalence relation. Once counterexample is produced in the abstract model, verifying whether the counterexample is real or spurious is carried out. Based on the counterexample-guided technique, an iterative abstraction refinement process is performed to progressively refine the abstract model until either there is no abstract counterexample or a valid counterexample is verified. The case studies which are discussed throughout the paper demonstrate that our approach takes advantages than the traditional approaches.",
            "year": 2011,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The classical abstraction refinement technique CEGAR (Counterexample-guided abstraction refinement) is extended to make quantitative verification of WSC applicable and efficient and demonstrates that the approach takes advantages than the traditional approaches."
            },
            "score": 2
        },
        {
            "id": "d243ab6feaaa1a507bb8815c7f41f97478f3712e",
            "paperId": "d243ab6feaaa1a507bb8815c7f41f97478f3712e",
            "title": "Application of Sequential Design of Experiments (SDoE) to Large Pilot-Scale Solvent-Based CO2 Capture Process at Technology Centre Mongstad (TCM)",
            "abstract": "The United States Department of Energy\u2019s Carbon Capture Simulation for Industry Impact (CCSI2) program has developed a framework for sequential design of experiments (SDoE) that aims to maximize knowledge gained from budget- and schedule-limited pilot scale testing. SDoE was applied to the planning and execution of campaigns for testing CO2 capture systems at pilot-scale in order to optimally allocate resources available for the testing. In this methodology, a stochastic process model is developed by quantifying the parametric uncertainty in submodels of interest; for a solvent-based CO2 capture system, these may include physical properties and equipment performance submodels (e.g., mass transfer, interfacial area). This uncertainty is propagated through the full process model, over variable operating conditions, for estimating the resulting uncertainty in key model outputs (e.g., percentage of CO2 capture, solvent regeneration energy requirement). In developing a data collection plan, the predicted output uncertainty is incorporated into an algorithm that seeks simultaneously to select process operating conditions for which the predicted uncertainty is relatively high and to ensure that the entire space of operation is well represented. This test plan is then used to guide operation of the pilot plant at varying steady-state conditions, with resulting process data incorporated into the existing model using Bayesian inference to refine parameter distributions. The updated stochastic model, with reduced parametric uncertainty from data collected, is then used to guide additional data collection, thus the sequential nature of the experimental design. \n \nThe SDoE process was implemented at the pilot test unit (12 MWe in scale) at Norway\u2019s Technology Centre Mongstad (TCM) in a summer 2018 test campaign with aqueous monoethanolamine (MEA). During the test campaign, the varied operating conditions included the flowrates of circulated solvent, flue gas, and reboiler steam and the CO2 concentration in the flue gas. The process data were used to update probability distributions of mass transfer and interfacial area parameters of a stochastic process model developed by the CCSI2 team. Two iterations of the SDoE process were executed, resulting in the uncertainty in model predicted CO2 capture percentage decreasing by an average of 58.0 \u00b1 4.7% over the full input space of interest. This work demonstrates the potential of the SDoE process for model refinement through reduction in process model parametric uncertainty, and ultimately risk in scale-up, in CO2 capture technology performance.",
            "year": 2021,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The SDoE process was implemented at the pilot test unit at Norway's Technology Centre Mongstad in a summer 2018 test campaign with aqueous monoethanolamine, resulting in the uncertainty in model predicted CO2 capture percentage decreasing by an average of 58.0 \u00b1 4.7%."
            },
            "score": 2
        },
        {
            "id": "74c7343d91d5464c27ca407fd504b07e690363be",
            "paperId": "74c7343d91d5464c27ca407fd504b07e690363be",
            "title": "Combining Confidence Elicitation and Sample-based Methods for Uncertainty Quantification in Misinformation Mitigation",
            "abstract": "Large Language Models have emerged as prime candidates to tackle misinformation mitigation. However, existing approaches struggle with hallucinations and overconfident predictions. We propose an uncertainty quantification framework that leverages both direct confidence elicitation and sampled-based consistency methods to provide better calibration for NLP misinformation mitigation solutions. We first investigate the calibration of sample-based consistency methods that exploit distinct features of consistency across sample sizes and stochastic levels. Next, we evaluate the performance and distributional shift of a robust numeric verbalization prompt across single vs. two-step confidence elicitation procedure. We also compare the performance of the same prompt with different versions of GPT and different numerical scales. Finally, we combine the sample-based consistency and verbalized methods to propose a hybrid framework that yields a better uncertainty estimation for GPT models. Overall, our work proposes novel uncertainty quantification methods that will improve the reliability of Large Language Models in misinformation mitigation applications.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes an uncertainty quantification framework that leverages both direct confidence elicitation and sampled-based consistency methods to provide better calibration for NLP misinformation mitigation solutions to improve the reliability of Large Language Models in misinformation mitigation applications."
            },
            "score": 1
        },
        {
            "id": "40506e950d8bca5e691a0fcd06c6d8655edc9a1c",
            "paperId": "40506e950d8bca5e691a0fcd06c6d8655edc9a1c",
            "title": "Context-dependent phone models and models adaptation for phonotactic language recognition",
            "abstract": "The performance of a PPRLM language recognition system depends on the quality and the consistency of phone decoders. To improve the performance of the decoders, this paper investigates the use of context-dependent instead of contextindependent phone models, and the use of CMLLR for model adaptation. This paper also discusses several improvements to the LIMSI 2007 NIST LRE system, including the use of a 4gram language model, score calibration and fusion using the FoCalMulti-class toolkit (with large development data) and better decoding parameters such as phone insertion penalty. The improved system is evaluated on the NIST LRE-2005 and the LRE-2007 evaluation data sets. Despite its simplicity, the system achieves for the 30s condition a Cavg of 2.4% and 1.6% on these data sets, respectively.",
            "year": 2008,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The use of context-dependent instead of contextindependent phone models, and the use of CMLLR for model adaptation are investigated, and several improvements to the LIMSI 2007 NIST LRE system are discussed, including a 4gram language model and score calibration and fusion using the FoCalMulti-class toolkit."
            },
            "score": 1
        }
    ],
    "novelty": "yes"
}