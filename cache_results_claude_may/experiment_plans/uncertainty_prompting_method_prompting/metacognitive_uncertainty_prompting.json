{
    "topic_description": "novel prompting methods that can better quantify uncertainty or calibrate the confidence of large language models",
    "idea_name": "Metacognitive Uncertainty Prompting",
    "raw_idea": {
        "Type": "prompting",
        "Problem": "LLMs often lack the ability to introspect on their own knowledge and reasoning processes, leading to overconfident predictions and a lack of transparency.",
        "Existing Methods": "Existing methods for improving LLM introspection include chain-of-thought prompting, self-consistency, and rationale generation.",
        "Motivation": "We propose a metacognitive prompting approach that encourages the model to reflect on its own uncertainty and reasoning process, providing more transparent and calibrated outputs.",
        "Proposed Method": "Metacognitive Uncertainty Prompting (MUP) involves the following steps: 1) Initial Prediction: Prompt the LLM to generate an initial prediction for the given task. 2) Uncertainty Reflection: Prompt the model to assess its own uncertainty in the initial prediction, generating a free-text explanation of the factors contributing to its confidence or lack thereof. 3) Reasoning Reflection: Prompt the model to explain its reasoning process in generating the initial prediction, highlighting any assumptions, analogies, or external knowledge used. 4) Revised Prediction: Based on the model's reflections on its uncertainty and reasoning, prompt it to generate a revised prediction, taking into account any identified limitations or areas for improvement.",
        "Experiment Plan": "Evaluate MUP on a range of tasks that require introspection and uncertainty awareness, such as commonsense reasoning, counterfactual reasoning, and decision making under incomplete information. Compare the calibration and interpretability of MUP-generated outputs against baseline methods using metrics like expected calibration error (ECE) and human evaluation of transparency and trustworthiness."
    },
    "full_experiment_plan": {
        "Title": "Metacognitive Uncertainty Prompting: Improving Language Model Calibration and Transparency",
        "Problem Statement": "Large Language Models (LLMs) often generate overconfident predictions and lack the ability to introspect on their own knowledge and reasoning processes, leading to a lack of transparency and calibration in their outputs.",
        "Motivation": "Existing methods for improving LLM introspection, such as chain-of-thought prompting, self-consistency, and rationale generation, have shown promise but still face challenges in achieving reliable and interpretable model calibration. We propose a novel metacognitive prompting approach that encourages the model to reflect on its own uncertainty and reasoning process, providing more transparent and calibrated outputs. By prompting the model to assess its own uncertainty and explain its reasoning, we aim to improve the model's ability to communicate its confidence and thought process to users, enabling more informed decision-making and trust in the model's outputs.",
        "Proposed Method": "Metacognitive Uncertainty Prompting (MUP) involves the following steps:\n1. Initial Prediction: Prompt the LLM to generate an initial prediction for the given task.\n2. Uncertainty Reflection: Prompt the model to assess its own uncertainty in the initial prediction, generating a free-text explanation of the factors contributing to its confidence or lack thereof.\n3. Reasoning Reflection: Prompt the model to explain its reasoning process in generating the initial prediction, highlighting any assumptions, analogies, or external knowledge used.\n4. Revised Prediction: Based on the model's reflections on its uncertainty and reasoning, prompt it to generate a revised prediction, taking into account any identified limitations or areas for improvement.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "Evaluate MUP on a range of tasks that require introspection and uncertainty awareness, such as commonsense reasoning (e.g., CommonsenseQA, OpenBookQA), counterfactual reasoning (e.g., WIQA), and decision making under incomplete information (e.g., SWAG, HellaSWAG).",
            "Step 2: Construct Prompts": "For each task, create a set of prompts for the four steps of MUP:\n1. Initial Prediction: 'Please provide an answer to the following question: [QUESTION]'\n2. Uncertainty Reflection: 'Given your previous answer, please reflect on your level of certainty and explain any factors contributing to your confidence or uncertainty in the answer:'\n3. Reasoning Reflection: 'Please explain your reasoning process in arriving at your initial answer, highlighting any assumptions, analogies, or external knowledge used:'\n4. Revised Prediction: 'Based on your reflections on uncertainty and reasoning, please provide a revised answer to the original question, taking into account any limitations or areas for improvement identified:'",
            "Step 3: Select Models": "Evaluate MUP on state-of-the-art LLMs such as GPT-3.5 (text-davinci-002), GPT-4, and PaLM. For each model, compare the performance of MUP against baseline methods such as direct prompting, chain-of-thought prompting, and self-consistency.",
            "Step 4: Evaluate Performance": "Measure the performance of MUP and baseline methods using the following metrics:\n1. Accuracy: Evaluate the accuracy of the initial and revised predictions against ground-truth answers.\n2. Calibration: Assess the calibration of the model's uncertainty estimates using metrics such as expected calibration error (ECE) and maximum calibration error (MCE).\n3. Interpretability: Conduct human evaluations to assess the interpretability and transparency of the model's uncertainty reflections and reasoning explanations.",
            "Step 5: Analyze Results": "Compare the performance of MUP against baseline methods across the selected tasks and models. Analyze the impact of uncertainty reflection and reasoning reflection on model accuracy, calibration, and interpretability. Identify any patterns or trends in the types of tasks or models where MUP is most effective."
        },
        "Test Case Examples": {
            "Example 1": {
                "Question": "On stage, a woman takes a seat at the piano. She",
                "Baseline Prompt": "On stage, a woman takes a seat at the piano. She",
                "Baseline Output": "plays a beautiful melody for the captivated audience.",
                "MUP Initial Prediction Prompt": "Please provide an answer to the following question: On stage, a woman takes a seat at the piano. She",
                "MUP Initial Prediction Output": "plays a beautiful melody for the captivated audience.",
                "MUP Uncertainty Reflection Prompt": "Given your previous answer, please reflect on your level of certainty and explain any factors contributing to your confidence or uncertainty in the answer:",
                "MUP Uncertainty Reflection Output": "I am somewhat confident in my answer, as it is a reasonable continuation of the given scenario. However, I am uncertain because the prompt does not provide explicit information about what the woman does after sitting at the piano. There could be other plausible continuations, such as adjusting the seat, looking at sheet music, or pausing to address the audience.",
                "MUP Reasoning Reflection Prompt": "Please explain your reasoning process in arriving at your initial answer, highlighting any assumptions, analogies, or external knowledge used:",
                "MUP Reasoning Reflection Output": "In generating my initial answer, I relied on the common assumption that when someone sits down at a piano on stage, they will proceed to play music. This is based on general knowledge about performances and the purpose of having a piano on stage. However, I now realize that I made this assumption without explicit evidence from the given prompt.",
                "MUP Revised Prediction Prompt": "Based on your reflections on uncertainty and reasoning, please provide a revised answer to the original question, taking into account any limitations or areas for improvement identified:",
                "MUP Revised Prediction Output": "After reflecting on my initial answer, I realize that there is not enough information given in the prompt to say definitively what the woman does after sitting at the piano. While it is possible she plays a melody, she could also be adjusting the seat, preparing sheet music, or doing something else. Without more context, I cannot provide a confident answer."
            },
            "Example 2": {
                "Question": "The chef prepared a complex dish, but he forgot to add salt. As a result, the dish",
                "Baseline Prompt": "The chef prepared a complex dish, but he forgot to add salt. As a result, the dish",
                "Baseline Output": "was bland and lacked flavor. The absence of salt meant that the other ingredients couldn't shine through, and the overall taste was disappointing.",
                "MUP Initial Prediction Prompt": "Please provide an answer to the following question: The chef prepared a complex dish, but he forgot to add salt. As a result, the dish",
                "MUP Initial Prediction Output": "was bland and lacked flavor. The absence of salt meant that the other ingredients couldn't shine through, and the overall taste was disappointing.",
                "MUP Uncertainty Reflection Prompt": "Given your previous answer, please reflect on your level of certainty and explain any factors contributing to your confidence or uncertainty in the answer:",
                "MUP Uncertainty Reflection Output": "I am fairly confident in my answer, as salt is a crucial ingredient in most savory dishes and its absence can greatly impact the overall flavor. However, I am slightly uncertain because the prompt mentions that the dish is complex, which could mean that it includes other strong flavors that might partially compensate for the lack of salt.",
                "MUP Reasoning Reflection Prompt": "Please explain your reasoning process in arriving at your initial answer, highlighting any assumptions, analogies, or external knowledge used:",
                "MUP Reasoning Reflection Output": "My initial answer is based on the general knowledge that salt enhances the flavors in a dish and helps to balance sweetness, bitterness, and sourness. Without salt, food can taste bland and one-dimensional. I assumed that since the chef forgot to add salt to a complex dish, the flavors would be muted and the dish would be less enjoyable as a result.",
                "MUP Revised Prediction Prompt": "Based on your reflections on uncertainty and reasoning, please provide a revised answer to the original question, taking into account any limitations or areas for improvement identified:",
                "MUP Revised Prediction Output": "The lack of salt likely had a significant negative impact on the flavor of the dish. However, given that it was a complex dish, it is possible that other strong flavors, such as herbs, spices, or acidic ingredients, could have partially compensated for the absence of salt. Nonetheless, the dish probably tasted somewhat bland and unbalanced compared to how it would have tasted with the proper amount of salt."
            }
        },
        "Fallback Plan": "If the proposed MUP approach does not lead to significant improvements in model calibration and interpretability, consider the following alternative analyses and directions:\n1. Analyze the quality and relevance of the generated uncertainty reflections and reasoning explanations. Identify common failure modes or areas where the model struggles to provide meaningful insights into its thought process.\n2. Investigate the impact of different prompt formulations on the effectiveness of MUP. Experiment with alternative prompts that may elicit more informative or calibrated responses from the model.\n3. Explore the use of MUP in combination with other techniques, such as retrieval-augmented generation or model ensembling, to further improve the model's introspective abilities.\n4. Conduct a detailed error analysis to understand the limitations of MUP and identify specific types of tasks or scenarios where the approach falls short. Use these insights to guide the development of future uncertainty estimation and calibration methods.\n5. Pivot the project to focus on analyzing the relationship between model size, architecture, and introspective abilities. Compare the performance of MUP across different models and investigate whether certain architectures are more amenable to metacognitive prompting."
    },
    "novelty_queries": [
        "KeywordQuery(\"language models metacognitive uncertainty prompting\")",
        "KeywordQuery(\"language models uncertainty calibration\")",
        "KeywordQuery(\"language models introspection reasoning reflection\")",
        "KeywordQuery(\"Metacognitive Uncertainty Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "be8c90bca14d59f180f40a41126b7cd8c29c5d4e",
            "paperId": "be8c90bca14d59f180f40a41126b7cd8c29c5d4e",
            "title": "Uncertainty Quantification for In-Context Learning of Large Language Models",
            "abstract": "In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM's response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM's response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model's configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-context learning in a plug-and-play fashion. Extensive experiments are conducted to demonstrate the effectiveness of the decomposition. The code and data are available at: https://github.com/lingchen0331/UQ_ICL.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations and ambiguities tied to the model's configurations (epistemic uncertainty)."
            },
            "score": 9,
            "novelty_score": "The project proposal aims to improve language model calibration and transparency by introducing a metacognitive uncertainty prompting approach that encourages the model to reflect on its own uncertainty and reasoning process. The paper focuses on quantifying the predictive uncertainty of large language models associated with in-context learning, considering both aleatoric uncertainty from the provided demonstrations and epistemic uncertainty tied to the model's configurations.\n\nWhile both works address the issue of uncertainty in large language models, the project proposal emphasizes improving model calibration and transparency through a novel prompting approach, whereas the paper proposes a formulation and estimation method to quantify uncertainties specifically in the context of in-context learning.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "7adb88771376c2a31688e3b0395b0550a35b824d",
            "paperId": "7adb88771376c2a31688e3b0395b0550a35b824d",
            "title": "Uncertainty Decomposition and Quantification for In-Context Learning of Large Language Models",
            "abstract": "In-context learning has emerged as a ground-breaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM\u2019s response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM\u2019s response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model\u2019s configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-context learning in a plug-and-play fashion. Extensive experiments are conducted to demonstrate the effectiveness of the decomposition. The code and data are available at: https://github. com/lingchen0331/UQ_ICL .",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work dives into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations and ambiguities tied to the model\u2019s configurations (epistemic uncertainty)."
            },
            "score": 9,
            "novelty_score": "The project proposal aims to improve language model calibration and transparency through metacognitive uncertainty prompting, which involves generating initial predictions, reflecting on uncertainty and reasoning, and revising predictions based on the reflections. The paper focuses on quantifying and decomposing the predictive uncertainty of large language models in the context of in-context learning, considering both aleatoric uncertainty from the provided demonstrations and epistemic uncertainty from the model's configurations.\n\nWhile both works address the issue of uncertainty in large language models, the project proposal focuses on improving calibration and transparency through a novel prompting approach, whereas the paper proposes a method to quantify and decompose uncertainty in the specific context of in-context learning. The research problems and approaches are not directly aligned.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "896ca0a68e4d33d76a7366bcab85eb7d2605a8c4",
            "paperId": "896ca0a68e4d33d76a7366bcab85eb7d2605a8c4",
            "title": "Metacognitive Prompting Improves Understanding in Large Language Models",
            "abstract": "In Large Language Models (LLMs), there have been consistent advancements in task-specific performance, largely influenced by effective prompt design. Recent advancements in prompting have enhanced reasoning in logic-intensive tasks for LLMs, yet the nuanced understanding abilities of these models, crucial for processing and interpreting complex information, remain underexplored. In this study, we introduce Metacognitive Prompting (MP), a strategy inspired by human introspective reasoning processes. Using MP, LLMs undergo a systematic series of structured, self-aware evaluations, drawing on both their vast inherent knowledge and new insights. We conduct extensive experiments on four prevalent LLMs: Llama2, PaLM2, GPT-3.5, and GPT-4, across ten natural language understanding (NLU) datasets from GLUE, SuperGLUE, BLUE, and LexGLUE benchmarks. Additionally, we compare our method with chain-of-thought prompting and its advanced versions. The results show that GPT-4 consistently excels across all tasks, while other models have shown significant progress in some tasks when used in conjunction with MP. Furthermore, MP consistently outperforms existing prompting methods in both general and domain-specific NLU tasks. This study underscores the potential to amplify the understanding abilities of LLMs and highlights the benefits of mirroring human introspective reasoning in NLU tasks.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study introduces Metacognitive Prompting (MP), a strategy inspired by human introspective reasoning processes that consistently outperforms existing prompting methods in both general and domain-specific NLU tasks."
            },
            "score": 8,
            "novelty_score": "The project proposal aims to improve language model calibration and transparency by introducing Metacognitive Uncertainty Prompting (MUP), which encourages the model to reflect on its own uncertainty and reasoning process. The paper, on the other hand, introduces Metacognitive Prompting (MP) to enhance the understanding abilities of large language models in natural language understanding tasks by using structured, self-aware evaluations.\n\nWhile both the proposal and the paper focus on metacognitive prompting, the project proposal specifically targets model calibration and transparency, whereas the paper aims to improve understanding abilities in natural language understanding tasks. Additionally, the project proposal introduces Metacognitive Uncertainty Prompting (MUP), which is a different approach compared to the Metacognitive Prompting (MP) introduced in the paper.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "1fa4469e5bc5d096572902fe14b0d66078a24c47",
            "paperId": "1fa4469e5bc5d096572902fe14b0d66078a24c47",
            "title": "Navigating the Grey Area: Expressions of Overconfidence and Uncertainty in Language Models",
            "abstract": "Despite increasingly \ufb02uent, relevant, and coherent language generation, major gaps remain between how humans and machines use language. We argue that a key dimension that is missing from our understanding of language models (LMs) is the model\u2019s ability to interpret and generate expressions of uncertainty . Whether it be the weatherperson announcing a chance of rain or a doctor giving a diagnosis, information is often not black-and-white and expressions of uncertainty provide nuance to support human-decision making. The increasing deployment of LMs in the wild motivates us to investigate whether LMs are capable of interpreting expressions of uncertainty and how LMs\u2019 behaviors change when learning to emit their own expressions of uncertainty. When injecting expressions of uncertainty into prompts (e.g., \"I think the answer is...\"), we discover that GPT3\u2019s generations vary upwards of 80% in accuracy based on the expression used. We analyze the linguistic characteristics of these expressions and \ufb01nd a drop in accuracy when naturalistic expressions of certainty are present. We \ufb01nd similar effects when teaching models to emit their own expressions of uncertainty, where model calibration suffers when teaching models to emit certainty rather than un certainty. Together, these results highlight the challenges of building LMs that interpret and generate trustworthy expressions of uncertainty.",
            "year": 2023,
            "citationCount": 54,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that GPT3\u2019s generations vary upwards of 80% in accuracy based on the expression used, and the challenges of building LMs that interpret and generate trustworthy expressions of uncertainty are highlighted."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is improving language model calibration and transparency through metacognitive uncertainty prompting, while the paper investigates the interpretation and generation of expressions of uncertainty in language models.\n\nThe proposed approach in the proposal involves a four-step process of initial prediction, uncertainty reflection, reasoning reflection, and revised prediction, whereas the paper analyzes the impact of injecting expressions of uncertainty into prompts and teaching models to emit their own expressions of uncertainty.\n\nAlthough both the proposal and the paper address the topic of uncertainty in language models, their specific research problems and approaches differ. The proposal focuses on improving calibration and transparency through a novel prompting method, while the paper investigates the interpretation and generation of uncertainty expressions and their impact on model accuracy and calibration.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "217e436fd23fe4184828e02a2b143835d6fd3b28",
            "paperId": "217e436fd23fe4184828e02a2b143835d6fd3b28",
            "title": "Navigating the Grey Area: How Expressions of Uncertainty and Overconfidence Affect Language Models",
            "abstract": "The increased deployment of LMs for real-world tasks involving knowledge and facts makes it important to understand model epistemology: what LMs think they know, and how their attitudes toward that knowledge are affected by language use in their inputs. Here, we study an aspect of model epistemology: how epistemic markers of certainty, uncertainty, or evidentiality like\"I'm sure it's\",\"I think it's\", or\"Wikipedia says it's\"affect models, and whether they contribute to model failures. We develop a typology of epistemic markers and inject 50 markers into prompts for question answering. We find that LMs are highly sensitive to epistemic markers in prompts, with accuracies varying more than 80%. Surprisingly, we find that expressions of high certainty result in a 7% decrease in accuracy as compared to low certainty expressions; similarly, factive verbs hurt performance, while evidentials benefit performance. Our analysis of a popular pretraining dataset shows that these markers of uncertainty are associated with answers on question-answering websites, while markers of certainty are associated with questions. These associations may suggest that the behavior of LMs is based on mimicking observed language use, rather than truly reflecting epistemic uncertainty.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that LMs are highly sensitive to epistemic markers in prompts, with accuracies varying more than 80%, and expressions of high certainty result in a 7% decrease in accuracy as compared to low certainty expressions; similarly, factive verbs hurt performance, while evidentials benefit performance."
            },
            "score": 8,
            "novelty_score": "The project proposal aims to improve language model calibration and transparency by using metacognitive uncertainty prompting, which encourages the model to reflect on its own uncertainty and reasoning process. The paper investigates how epistemic markers of certainty, uncertainty, or evidentiality in prompts affect language models and their performance on question-answering tasks.\n\nWhile both the project proposal and the paper focus on language models and their handling of uncertainty, the research problems and approaches are different. The project proposal seeks to improve model calibration and transparency through metacognitive prompting, while the paper studies the impact of epistemic markers on model performance and behavior.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "0a51afdcd7cf4f33987d766082a7d3f174936c8a",
            "paperId": "0a51afdcd7cf4f33987d766082a7d3f174936c8a",
            "title": "Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models",
            "abstract": "In the face of uncertainty, the ability to seek information is of fundamental importance. In many practical applications, such as medical diagnosis and troubleshooting, the information needed to solve the task is not initially given, and has to be actively sought by asking follow-up questions (for example, a doctor asking a patient for more details about their symptoms). In this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment large language models with the ability to actively seek information by asking effective questions. UoT combines 1) an uncertainty-aware simulation approach which enables the model to simulate possible future scenarios and how likely they are to occur, 2) uncertainty-based rewards motivated by information gain which incentivizes the model to seek information, and 3) a reward propagation scheme to select the optimal question to ask in a way that maximizes the expected reward. In experiments on medical diagnosis, troubleshooting and the '20 Questions' game, UoT achieves an average performance improvement of 57.8% in the rate of successful task completion across multiple LLMs compared with direct prompting, and also improves efficiency (i.e., the number of questions needed to complete the task).",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Uncertainty of Thoughts is introduced, an algorithm to augment large language models with the ability to actively seek information by asking effective questions and achieves an average performance improvement of 57.8% in the rate of successful task completion across multiple LLMs compared with direct prompting."
            },
            "score": 8,
            "novelty_score": "The project proposal aims to improve language model calibration and transparency by using metacognitive uncertainty prompting, which involves generating initial predictions, reflecting on uncertainty and reasoning, and revising predictions based on the reflections.\n\nThe paper focuses on enhancing information-seeking abilities in language models by using uncertainty-aware planning, which simulates future scenarios, uses uncertainty-based rewards, and propagates rewards to select optimal questions.\n\nWhile both works involve uncertainty estimation, the project proposal targets calibration and interpretability, while the paper addresses active information seeking. The methods and goals are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "374dd173491a59a10bbb2b3519ebcfe3649f529d",
            "paperId": "374dd173491a59a10bbb2b3519ebcfe3649f529d",
            "title": "Teaching Models to Express Their Uncertainty in Words",
            "abstract": "We show that a GPT-3 model can learn to express uncertainty about its own answers in natural language -- without use of model logits. When given a question, the model generates both an answer and a level of confidence (e.g.\"90% confidence\"or\"high confidence\"). These levels map to probabilities that are well calibrated. The model also remains moderately calibrated under distribution shift, and is sensitive to uncertainty in its own answers, rather than imitating human examples. To our knowledge, this is the first time a model has been shown to express calibrated uncertainty about its own answers in natural language. For testing calibration, we introduce the CalibratedMath suite of tasks. We compare the calibration of uncertainty expressed in words (\"verbalized probability\") to uncertainty extracted from model logits. Both kinds of uncertainty are capable of generalizing calibration under distribution shift. We also provide evidence that GPT-3's ability to generalize calibration depends on pre-trained latent representations that correlate with epistemic uncertainty over its answers.",
            "year": 2022,
            "citationCount": 165,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that a GPT-3 model can learn to express uncertainty about its own answers in natural language -- without use of model logits -- for the first time."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is improving language model calibration and transparency through metacognitive uncertainty prompting, while the paper focuses on teaching models to express calibrated uncertainty about their own answers in natural language.\n\nThe approach in the proposal involves prompting the model to assess its own uncertainty and reasoning process, while the paper trains the model to generate both an answer and a level of confidence without using model logits.\n\nProposal: Improving language model calibration and transparency through metacognitive uncertainty prompting.\nPaper: Teaching models to express calibrated uncertainty about their own answers in natural language.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "c2cd20f162420afc739979dbb3bc900d760e38a7",
            "paperId": "c2cd20f162420afc739979dbb3bc900d760e38a7",
            "title": "Violation of Expectation via Metacognitive Prompting Reduces Theory of Mind Prediction Error in Large Language Models",
            "abstract": "Recent research shows that Large Language Models (LLMs) exhibit a compelling level of proficiency in Theory of Mind (ToM) tasks. This ability to impute unobservable mental states to others is vital to human social cognition and may prove equally important in principal-agent relations between individual humans and Artificial Intelligences (AIs). In this paper, we explore how a mechanism studied in developmental psychology known as Violation of Expectation (VoE) can be implemented to reduce errors in LLM prediction about users by leveraging emergent ToM affordances. And we introduce a \\textit{metacognitive prompting} framework to apply VoE in the context of an AI tutor. By storing and retrieving facts derived in cases where LLM expectation about the user was violated, we find that LLMs are able to learn about users in ways that echo theories of human learning. Finally, we discuss latent hazards and augmentative opportunities associated with modeling user psychology and propose ways to mitigate risk along with possible directions for future inquiry.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper explores how a mechanism studied in developmental psychology known as Violation of Expectation can be implemented to reduce errors in LLM prediction about users by leveraging emergent ToM affordances, and introduces a metacognitive prompting framework to apply VoE in the context of an AI tutor."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve language model calibration and transparency by using metacognitive uncertainty prompting, which involves prompting the model to reflect on its own uncertainty and reasoning process.\n\nThe paper abstract proposes using metacognitive prompting and the violation of expectation mechanism to reduce errors in language models' predictions about users in the context of an AI tutor, leveraging the models' theory of mind abilities.\n\nWhile both the project proposal and the paper abstract involve metacognitive prompting, the project focuses on improving model calibration and transparency in general, while the paper specifically targets reducing errors in predicting user behavior in an AI tutoring context.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "ad402080a4aa66ef3c57a46ce4685a47a3cc0a61",
            "paperId": "ad402080a4aa66ef3c57a46ce4685a47a3cc0a61",
            "title": "Quantifying Uncertainty in Natural Language Explanations of Large Language Models",
            "abstract": "Large Language Models (LLMs) are increasingly used as powerful tools for several high-stakes natural language processing (NLP) applications. Recent prompting works claim to elicit intermediate reasoning steps and key tokens that serve as proxy explanations for LLM predictions. However, there is no certainty whether these explanations are reliable and reflect the LLMs behavior. In this work, we make one of the first attempts at quantifying the uncertainty in explanations of LLMs. To this end, we propose two novel metrics -- $\\textit{Verbalized Uncertainty}$ and $\\textit{Probing Uncertainty}$ -- to quantify the uncertainty of generated explanations. While verbalized uncertainty involves prompting the LLM to express its confidence in its explanations, probing uncertainty leverages sample and model perturbations as a means to quantify the uncertainty. Our empirical analysis of benchmark datasets reveals that verbalized uncertainty is not a reliable estimate of explanation confidence. Further, we show that the probing uncertainty estimates are correlated with the faithfulness of an explanation, with lower uncertainty corresponding to explanations with higher faithfulness. Our study provides insights into the challenges and opportunities of quantifying uncertainty in LLM explanations, contributing to the broader discussion of the trustworthiness of foundation models.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes two novel metrics -- verbalized uncertainty and probing uncertainty -- to quantify the uncertainty of generated explanations of large Language Models, and shows that the probing uncertainty estimates are correlated with the faithfulness of an explanation, with lower uncertainty corresponding to explanations with higher faithfulness."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the calibration and transparency of language models through metacognitive uncertainty prompting. The approach involves prompting the model to reflect on its uncertainty and reasoning process to generate more transparent and calibrated outputs.\n\nThe research problem in the paper is quantifying the uncertainty in explanations generated by language models. The approach involves using verbalized uncertainty and probing uncertainty metrics to measure the confidence and faithfulness of the explanations.\n\nWhile both works aim to improve the transparency and reliability of language models, the specific research problems and approaches are different. The proposal focuses on improving the model's calibration and transparency through metacognitive prompting, while the paper focuses on quantifying the uncertainty in the model's explanations using novel metrics.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "b1ec3002f4c80d721fc7d975cf469dce0833fed0",
            "paperId": "b1ec3002f4c80d721fc7d975cf469dce0833fed0",
            "title": "DeLLMa: A Framework for Decision Making Under Uncertainty with Large Language Models",
            "abstract": "Large language models (LLMs) are increasingly used across society, including in domains like business, engineering, and medicine. These fields often grapple with decision-making under uncertainty, a critical yet challenging task. In this paper, we show that directly prompting LLMs on these types of decision-making problems yields poor results, especially as the problem complexity increases. To overcome this limitation, we propose DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments. DeLLMa involves a multi-step scaffolding procedure, drawing upon principles from decision theory and utility theory, to provide an optimal and human-auditable decision-making process. We validate our framework on decision-making environments involving real agriculture and finance data. Our results show that DeLLMa can significantly improve LLM decision-making performance, achieving up to a 40% increase in accuracy over competing methods.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "DeLLMa (Decision-making Large Language Model assistant), a framework designed to enhance decision-making accuracy in uncertain environments, is proposed and validated on decision-making environments involving real agriculture and finance data."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the calibration and transparency of language models through metacognitive uncertainty prompting. The approach involves prompting the model to reflect on its uncertainty and reasoning process to generate more transparent and calibrated outputs.\n\nThe research problem in the paper is enhancing decision-making accuracy of language models in uncertain environments. The approach is a multi-step scaffolding procedure based on decision theory and utility theory principles.\n\nWhile both works aim to improve certain aspects of language model performance, the specific problems and approaches are different. The proposal focuses on calibration and transparency through metacognitive prompting, while the paper addresses decision-making accuracy under uncertainty using a decision theory-based framework.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "d1500f1dbd62e26ef0753f31e845078f58479968",
            "paperId": "d1500f1dbd62e26ef0753f31e845078f58479968",
            "title": "Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners",
            "abstract": "Large language models (LLMs) exhibit a wide range of promising capabilities -- from step-by-step planning to commonsense reasoning -- that may provide utility for robots, but remain prone to confidently hallucinated predictions. In this work, we present KnowNo, which is a framework for measuring and aligning the uncertainty of LLM-based planners such that they know when they don't know and ask for help when needed. KnowNo builds on the theory of conformal prediction to provide statistical guarantees on task completion while minimizing human help in complex multi-step planning settings. Experiments across a variety of simulated and real robot setups that involve tasks with different modes of ambiguity (e.g., from spatial to numeric uncertainties, from human preferences to Winograd schemas) show that KnowNo performs favorably over modern baselines (which may involve ensembles or extensive prompt tuning) in terms of improving efficiency and autonomy, while providing formal assurances. KnowNo can be used with LLMs out of the box without model-finetuning, and suggests a promising lightweight approach to modeling uncertainty that can complement and scale with the growing capabilities of foundation models. Website: https://robot-help.github.io",
            "year": 2023,
            "citationCount": 91,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents KnowNo, which is a framework for measuring and aligning the uncertainty of LLM-based planners such that they know when they don't know and ask for help when needed, and suggests a promising lightweight approach to modeling uncertainty that can complement and scale with the growing capabilities of foundation models."
            },
            "score": 7
        },
        {
            "id": "6d3ae6d6b312b659b3a14ae3f3e86a36db63200d",
            "paperId": "6d3ae6d6b312b659b3a14ae3f3e86a36db63200d",
            "title": "Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning",
            "abstract": "Step-by-step decision planning with large language models (LLMs) is gaining attention in AI agent development. This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models. Existing approaches are either white-box or computationally demanding, limiting use of black-box proprietary LLMs within budgets. The paper's first contribution is a non-parametric uncertainty quantification method for LLMs, efficiently estimating point-wise dependencies between input-decision on the fly with a single inference, without access to token logits. This estimator informs the statistical interpretation of decision trustworthiness. The second contribution outlines a systematic design for a decision-making agent, generating actions like ``turn on the bathroom light'' based on user prompts such as ``take a bath''. Users will be asked to provide preferences when more than one action has high estimated point-wise dependencies. In conclusion, our uncertainty estimation and decision-making agent design offer a cost-efficient approach for AI agent development.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models, and outlines a systematic design for a decision-making agent, offering a cost-efficient approach for AI agent development."
            },
            "score": 7
        },
        {
            "id": "0aa5940fda7c994675d08c41eca2a6909eb6d205",
            "paperId": "0aa5940fda7c994675d08c41eca2a6909eb6d205",
            "title": "Improving the Reliability of Large Language Models by Leveraging Uncertainty-Aware In-Context Learning",
            "abstract": "In recent years, large-scale language models (LLMs) have gained attention for their impressive text generation capabilities. However, these models often face the challenge of\"hallucination,\"which undermines their reliability. In this study, we introduce an uncertainty-aware in-context learning framework to empower the model to enhance or reject its output in response to uncertainty. Human-defined methods for estimating uncertainty typically assume that\"uncertainty is lower when the model's response is correct compared to when it is incorrect.\"However, setting a precise threshold to distinguish correctness is challenging. Therefore, we introduce uncertainty information as an intermediary variable that implicitly influences the model's behavior. Our innovative uncertainty-aware in-context learning framework involves fine-tuning the LLM using a calibration dataset. Our aim is to improve the model's responses by filtering out answers with high uncertainty while considering the model's knowledge limitations. We evaluate the model's knowledge by examining multiple responses to the same question for the presence of a correct answer. When the model lacks relevant knowledge, the response should indicate that the question cannot be answered. Conversely, when the model has relevant knowledge, the response should provide the correct answer. Extensive experiments confirm the effectiveness of our framework, leading to two key findings. First, the logit output values of the LLM partly reflect inherent uncertainty. Second, our model autonomously recognizes uncertainty, resulting in improved responses.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study introduces an uncertainty-aware in-context learning framework to empower the model to enhance or reject its output in response to uncertainty, and introduces uncertainty information as an intermediary variable that implicitly influences the model's behavior."
            },
            "score": 7
        },
        {
            "id": "9f02a3fa885aebaf322ea8e4475939495dea70f7",
            "paperId": "9f02a3fa885aebaf322ea8e4475939495dea70f7",
            "title": "SPUQ: Perturbation-Based Uncertainty Quantification for Large Language Models",
            "abstract": "In recent years, large language models (LLMs) have become increasingly prevalent, offering remarkable text generation capabilities. However, a pressing challenge is their tendency to make confidently wrong predictions, highlighting the critical need for uncertainty quantification (UQ) in LLMs. While previous works have mainly focused on addressing aleatoric uncertainty, the full spectrum of uncertainties, including epistemic, remains inadequately explored. Motivated by this gap, we introduce a novel UQ method, sampling with perturbation for UQ (SPUQ), designed to tackle both aleatoric and epistemic uncertainties. The method entails generating a set of perturbations for LLM inputs, sampling outputs for each perturbation, and incorporating an aggregation module that generalizes the sampling uncertainty approach for text generation tasks. Through extensive experiments on various datasets, we investigated different perturbation and aggregation techniques. Our findings show a substantial improvement in model uncertainty calibration, with a reduction in Expected Calibration Error (ECE) by 50% on average. Our findings suggest that our proposed UQ method offers promising steps toward enhancing the reliability and trustworthiness of LLMs.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces a novel UQ method, sampling with perturbation for UQ (SPUQ), designed to tackle both aleatoric and epistemic uncertainties in large language models, and shows a substantial improvement in model uncertainty calibration."
            },
            "score": 7
        },
        {
            "id": "04365f0f1db4c659c3297cb8e70c39b38ed3b487",
            "paperId": "04365f0f1db4c659c3297cb8e70c39b38ed3b487",
            "title": "Self-Evaluation Improves Selective Generation in Large Language Models",
            "abstract": "Safe deployment of large language models (LLMs) may benefit from a reliable method for assessing their generated content to determine when to abstain or to selectively generate. While likelihood-based metrics such as perplexity are widely employed, recent research has demonstrated the limitations of using sequence-level probability estimates given by LLMs as reliable indicators of generation quality. Conversely, LLMs have demonstrated strong calibration at the token level, particularly when it comes to choosing correct answers in multiple-choice questions or evaluating true/false statements. In this work, we reformulate open-ended generation tasks into token-level prediction tasks, and leverage LLMs' superior calibration at the token level. We instruct an LLM to self-evaluate its answers, employing either a multi-way comparison or a point-wise evaluation approach, with the option to include a ``None of the above'' option to express the model's uncertainty explicitly. We benchmark a range of scoring methods based on self-evaluation and evaluate their performance in selective generation using TruthfulQA and TL;DR. Through experiments with PaLM-2 and GPT-3, we demonstrate that self-evaluation based scores not only improve accuracy, but also correlate better with the overall quality of generated content.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work reformulates open-ended generation tasks into token-level prediction tasks, and leverage LLMs' superior calibration at the token level, and demonstrates that self-evaluation based scores not only improve accuracy, but also correlate better with the overall quality of generated content."
            },
            "score": 7
        },
        {
            "id": "036e96ed196a7f4bb812380f3b76ac75d4a648e4",
            "paperId": "036e96ed196a7f4bb812380f3b76ac75d4a648e4",
            "title": "Calibrating the Confidence of Large Language Models by Eliciting Fidelity",
            "abstract": "Large language models optimized with techniques like RLHF have achieved good alignment in being helpful and harmless. However, post-alignment, these language models often exhibit overconfidence, where the expressed confidence does not accurately calibrate with their correctness rate. In this paper, we decompose the language model confidence into the \\textit{Uncertainty} about the question and the \\textit{Fidelity} to the answer generated by language models. Then, we propose a plug-and-play method to estimate the confidence of language models. Our method has shown good calibration performance by conducting experiments with 6 RLHF-LMs on four MCQA datasets. Moreover, we propose two novel metrics, IPR and CE, to evaluate the calibration of the model, and we have conducted a detailed discussion on \\textit{Truly Well-Calibrated Confidence}. Our method could serve as a strong baseline, and we hope that this work will provide some insights into the model confidence calibration.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper decomposes the language model confidence into the uncertainty about the question and the fidelity to the answer generated by language models, and proposes a plug-and-play method to estimate the confidence of language models."
            },
            "score": 7
        },
        {
            "id": "9e5a1b8aa30b05aa32644023c290cf9de7f55727",
            "paperId": "9e5a1b8aa30b05aa32644023c290cf9de7f55727",
            "title": "Automated Parliaments: A Solution to Decision Uncertainty and Misalignment in Language Models",
            "abstract": "As AI takes on a greater role in the modern world, it is essential to ensure that AI models can overcome decision uncertainty and remain aligned with human morality and interests. This research paper proposes a method for improving the decision-making of language models (LMs) via Automated Parliaments (APs) - constructs made of AI delegates each representing a certain perspective. Delegates themselves consist of three AI models: generators, modifiers, and evaluators. We specify two mechanisms for producing optimal solutions: the Simultaneous Modification mechanism for response creation and an evaluation mechanism for fairly assessing solutions. The overall process begins when each generator creates a response aligned with its delegate's theory. The modifiers alter all other responses to make them more self-aligned. The evaluators collectively assess the best end response. Finally, the modifiers and generators learn from feedback from the evaluators. In our research, we tested the evaluation mechanism, comparing the use of single-value zero-shot prompting and AP few-shot prompting in evaluating morally contentious scenarios. We found that the AP architecture saw a 57.3% reduction in its loss value compared to the baseline. We conclude by discussing some potential applications of APs and specifically their potential impact when implemented as Automated Moral Parliaments.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This research paper proposes a method for improving the decision-making of language models via Automated Parliaments (APs) - constructs made of AI delegates each representing a certain perspective, and specifies two mechanisms for producing optimal solutions: the Simultaneous Modification mechanism for response creation and an evaluation mechanism for fairly assessing solutions."
            },
            "score": 6
        },
        {
            "id": "17bcb1edbe068e8fe6a97da552c70a77a15bbce7",
            "paperId": "17bcb1edbe068e8fe6a97da552c70a77a15bbce7",
            "title": "Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned",
            "abstract": "We describe our early efforts to red team language models in order to simultaneously discover, measure, and attempt to reduce their potentially harmful outputs. We make three main contributions. First, we investigate scaling behaviors for red teaming across 3 model sizes (2.7B, 13B, and 52B parameters) and 4 model types: a plain language model (LM); an LM prompted to be helpful, honest, and harmless; an LM with rejection sampling; and a model trained to be helpful and harmless using reinforcement learning from human feedback (RLHF). We find that the RLHF models are increasingly difficult to red team as they scale, and we find a flat trend with scale for the other model types. Second, we release our dataset of 38,961 red team attacks for others to analyze and learn from. We provide our own analysis of the data and find a variety of harmful outputs, which range from offensive language to more subtly harmful non-violent unethical outputs. Third, we exhaustively describe our instructions, processes, statistical methodologies, and uncertainty about red teaming. We hope that this transparency accelerates our ability to work together as a community in order to develop shared norms, practices, and technical standards for how to red team language models.",
            "year": 2022,
            "citationCount": 235,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that the RLHF models are increasingly difficult to red team as they scale, and a flat trend with scale for the other model types is found."
            },
            "score": 6
        },
        {
            "id": "b8a83b11944355b0c5e59e911af4a2a0bfa0362a",
            "paperId": "b8a83b11944355b0c5e59e911af4a2a0bfa0362a",
            "title": "Investigating Uncertainty Calibration of Aligned Language Models under the Multiple-Choice Setting",
            "abstract": "Despite the significant progress made in practical applications of aligned language models (LMs), they tend to be overconfident in output answers compared to the corresponding pre-trained LMs. In this work, we systematically evaluate the impact of the alignment process on logit-based uncertainty calibration of LMs under the multiple-choice setting. We first conduct a thoughtful empirical study on how aligned LMs differ in calibration from their pre-trained counterparts. Experimental results reveal that there are two distinct uncertainties in LMs under the multiple-choice setting, which are responsible for the answer decision and the format preference of the LMs, respectively. Then, we investigate the role of these two uncertainties on aligned LM's calibration through fine-tuning in simple synthetic alignment schemes and conclude that one reason for aligned LMs' overconfidence is the conflation of these two types of uncertainty. Furthermore, we examine the utility of common post-hoc calibration methods for aligned LMs and propose an easy-to-implement and sample-efficient method to calibrate aligned LMs. We hope our findings could provide insights into the design of more reliable alignment processes for LMs.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work systematically evaluates the impact of the alignment process on logit-based uncertainty calibration of LMs under the multiple-choice setting and concludes that one reason for aligned LMs' overconfidence is the conflation of these two types of uncertainty."
            },
            "score": 6
        },
        {
            "id": "551b05734eb2181c4ca009a411144e8447ed1606",
            "paperId": "551b05734eb2181c4ca009a411144e8447ed1606",
            "title": "Uncertainty Quantification with Pre-trained Language Models: A Large-Scale Empirical Analysis",
            "abstract": "Pre-trained language models (PLMs) have gained increasing popularity due to their compelling prediction performance in diverse natural language processing (NLP) tasks. When formulating a PLM-based prediction pipeline for NLP tasks, it is also crucial for the pipeline to minimize the calibration error, especially in safety-critical applications. That is, the pipeline should reliably indicate when we can trust its predictions. In particular, there are various considerations behind the pipeline: (1) the choice and (2) the size of PLM, (3) the choice of uncertainty quantifier, (4) the choice of fine-tuning loss, and many more. Although prior work has looked into some of these considerations, they usually draw conclusions based on a limited scope of empirical studies. There still lacks a holistic analysis on how to compose a well-calibrated PLM-based prediction pipeline. To fill this void, we compare a wide range of popular options for each consideration based on three prevalent NLP classification tasks and the setting of domain shift. In response, we recommend the following: (1) use ELECTRA for PLM encoding, (2) use larger PLMs if possible, (3) use Temp Scaling as the uncertainty quantifier, and (4) use Focal Loss for fine-tuning.",
            "year": 2022,
            "citationCount": 38,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A wide range of popular options for each consideration are compared based on three prevalent NLP classification tasks and the setting of domain shift to form a holistic analysis on how to compose a well-calibrated PLM-based prediction pipeline."
            },
            "score": 6
        },
        {
            "id": "48fb667125298cf724f7b652d521686180412351",
            "paperId": "48fb667125298cf724f7b652d521686180412351",
            "title": "A Close Look into the Calibration of Pre-trained Language Models",
            "abstract": "Pre-trained language models (PLMs) may fail in giving reliable estimates of their predictive uncertainty. We take a close look into this problem, aiming to answer two questions: (1) Do PLMs learn to become calibrated in the training process? (2) How effective are existing calibration methods? For the first question, we conduct fine-grained control experiments to study the dynamic change in PLMs\u2019 calibration performance in training. We consider six factors as control variables, including dataset difficulty, available training samples, training steps, the number of tunable parameters, model scale, and pretraining. We observe a consistent change in calibration performance across six factors. We find that PLMs don\u2019t learn to become calibrated in training, evidenced by the continual increase in confidence, no matter whether the predictions are correct or not. We highlight that our finding somewhat contradicts two established conclusions: (a) Larger PLMs are more calibrated; (b) Pretraining improves model calibration. Next, we study the effectiveness of existing calibration methods in mitigating the overconfidence issue. Besides unlearnable calibration methods (e.g., label smoothing), we adapt and extend two recently proposed learnable methods that directly collect data to train models to have reasonable confidence estimations. Experimental results show that learnable methods significantly reduce PLMs\u2019 confidence in wrong predictions.",
            "year": 2022,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that pre-trained language models don\u2019t learn to become calibrated in training, evidenced by the continual increase in confidence, no matter whether the predictions are correct or not."
            },
            "score": 6
        },
        {
            "id": "296cad46f56329843ded4a8b7d7633c9d436f113",
            "paperId": "296cad46f56329843ded4a8b7d7633c9d436f113",
            "title": "On Uncertainty Calibration and Selective Generation in Probabilistic Neural Summarization: A Benchmark Study",
            "abstract": "Modern deep models for summarization attains impressive benchmark performance, but they are prone to generating miscalibrated predictive uncertainty. This means that they assign high confidence to low-quality predictions, leading to compromised reliability and trustworthiness in real-world applications. Probabilistic deep learning methods are common solutions to the miscalibration problem. However, their relative effectiveness in complex autoregressive summarization tasks are not well-understood. In this work, we thoroughly investigate different state-of-the-art probabilistic methods' effectiveness in improving the uncertainty quality of the neural summarization models, across three large-scale benchmarks with varying difficulty. We show that the probabilistic methods consistently improve the model's generation and uncertainty quality, leading to improved selective generation performance (i.e., abstaining from low-quality summaries) in practice. We also reveal notable failure patterns of probabilistic methods widely-adopted in NLP community (e.g., Deep Ensemble and Monte Carlo Dropout), cautioning the importance of choosing appropriate method for the data setting.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work thoroughly investigating different state-of-the-art probabilistic methods' effectiveness in improving the uncertainty quality of the neural summarization models, across three large-scale benchmarks with varying difficulty shows that the probabilists consistently improve the model's generation and uncertainty quality, leading to improved selective generation performance in practice."
            },
            "score": 6
        },
        {
            "id": "7f16b512f9cfce8c2cc31fd59492924b3ef6d597",
            "paperId": "7f16b512f9cfce8c2cc31fd59492924b3ef6d597",
            "title": "An Overview of Uncertainty Calibration for Text Classification and the Role of Distillation",
            "abstract": "Recent advances in NLP systems, notably the pretraining-and-finetuning paradigm, have achieved great success in predictive accuracy. However, these systems are usually not well calibrated for uncertainty out-of-the-box. Many recalibration methods have been proposed in the literature for quantifying predictive uncertainty and calibrating model outputs, with varying degrees of complexity. In this work, we present a systematic study of a few of these methods. Focusing on the text classification task and finetuned large pretrained language models, we first show that many of the finetuned models are not well calibrated out-of-the-box, especially when the data come from out-of-domain settings. Next, we compare the effectiveness of a few widely-used recalibration methods (such as ensembles, temperature scaling). Then, we empirically illustrate a connection between distillation and calibration. We view distillation as a regularization term encouraging the student model to output uncertainties that match those of a teacher model. With this insight, we develop simple recalibration methods based on distillation with no additional inference-time cost. We show on the GLUE benchmark that our simple methods can achieve competitive out-of-domain (OOD) calibration performance w.r.t. more expensive approaches. Finally, we include ablations to understand the usefulness of components of our proposed method and examine the transferability of calibration via distillation.",
            "year": 2021,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work views distillation as a regularization term encouraging the student model to output uncertainties that match those of a teacher model and develops simple recalibration methods based on distillation with no additional inference-time cost that can achieve competitive out-of-domain calibration performance w.r.t. more expensive approaches."
            },
            "score": 6
        },
        {
            "id": "231619cefd9f3a5cd909cbad5d8e1a2cbc3df63d",
            "paperId": "231619cefd9f3a5cd909cbad5d8e1a2cbc3df63d",
            "title": "LaFFi: Leveraging Hybrid Natural Language Feedback for Fine-tuning Language Models",
            "abstract": "Fine-tuning Large Language Models (LLMs) adapts a trained model to specific downstream tasks, significantly improving task-specific performance. Supervised Fine-Tuning (SFT) is a common approach, where an LLM is trained to produce desired answers. However, LLMs trained with SFT sometimes make simple mistakes and result in hallucinations on reasoning tasks such as question-answering. Without external feedback, it is difficult for SFT to learn a good mapping between the question and the desired answer, especially with a small dataset. This paper introduces an alternative to SFT called Natural Language Feedback for Finetuning LLMs (LaFFi). LaFFi has LLMs directly predict the feedback they will receive from an annotator. We find that requiring such reflection can significantly improve the accuracy in in-domain question-answering tasks, providing a promising direction for the application of natural language feedback in the realm of SFT LLMs. Additional ablation studies show that the portion of human-annotated data in the annotated datasets affects the fine-tuning performance.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LaFFi has LLMs directly predict the feedback they will receive from an annotator, and it is found that requiring such reflection can significantly improve the accuracy in in-domain question-answering tasks, providing a promising direction for the application of natural language feedback in the realm of SFT LLMs."
            },
            "score": 6
        },
        {
            "id": "ab15a0fc78532b2cae1007e5315f838ae23d0a4a",
            "paperId": "ab15a0fc78532b2cae1007e5315f838ae23d0a4a",
            "title": "Mirror: A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning",
            "abstract": "While Large language models (LLMs) have the capability to iteratively reflect on their own outputs, recent studies have observed their struggles with knowledge-rich problems without access to external resources. In addition to the inefficiency of LLMs in self-assessment, we also observe that LLMs struggle to revisit their predictions despite receiving explicit negative feedback. Therefore, We propose Mirror, a Multiple-perspective self-reflection method for knowledge-rich reasoning, to avoid getting stuck at a particular reflection iteration. Mirror enables LLMs to reflect from multiple-perspective clues, achieved through a heuristic interaction between a Navigator and a Reasoner. It guides agents toward diverse yet plausibly reliable reasoning trajectory without access to ground truth by encouraging (1) diversity of directions generated by Navigator and (2) agreement among strategically induced perturbations in responses generated by the Reasoner. The experiments on five reasoning datasets demonstrate that Mirror's superiority over several contemporary self-reflection approaches. Additionally, the ablation study studies clearly indicate that our strategies alleviate the aforementioned challenges.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Mirror, a Multiple-perspective self-reflection method for knowledge-rich reasoning, is proposed to avoid getting stuck at a particular reflection iteration, to avoid getting stuck at a particular reflection iteration."
            },
            "score": 6
        },
        {
            "id": "e1bc150d5d9e745a4920881c414ac9df0ea024a3",
            "paperId": "e1bc150d5d9e745a4920881c414ac9df0ea024a3",
            "title": "ChatGPT Prompting Cannot Estimate Predictive Uncertainty in High-Resource Languages",
            "abstract": "ChatGPT took the world by storm for its impressive abilities. Due to its release without documentation, scientists immediately attempted to identify its limits, mainly through its performance in natural language processing (NLP) tasks. This paper aims to join the growing literature regarding ChatGPT's abilities by focusing on its performance in high-resource languages and on its capacity to predict its answers' accuracy by giving a confidence level. The analysis of high-resource languages is of interest as studies have shown that low-resource languages perform worse than English in NLP tasks, but no study so far has analysed whether high-resource languages perform as well as English. The analysis of ChatGPT's confidence calibration has not been carried out before either and is critical to learn about ChatGPT's trustworthiness. In order to study these two aspects, five high-resource languages and two NLP tasks were chosen. ChatGPT was asked to perform both tasks in the five languages and to give a numerical confidence value for each answer. The results show that all the selected high-resource languages perform similarly and that ChatGPT does not have a good confidence calibration, often being overconfident and never giving low confidence values.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper aims to join the growing literature regarding ChatGPT's abilities by focusing on its performance in high-resource languages and on its capacity to predict its answers' accuracy by giving a confidence level."
            },
            "score": 6
        },
        {
            "id": "74c7343d91d5464c27ca407fd504b07e690363be",
            "paperId": "74c7343d91d5464c27ca407fd504b07e690363be",
            "title": "Combining Confidence Elicitation and Sample-based Methods for Uncertainty Quantification in Misinformation Mitigation",
            "abstract": "Large Language Models have emerged as prime candidates to tackle misinformation mitigation. However, existing approaches struggle with hallucinations and overconfident predictions. We propose an uncertainty quantification framework that leverages both direct confidence elicitation and sampled-based consistency methods to provide better calibration for NLP misinformation mitigation solutions. We first investigate the calibration of sample-based consistency methods that exploit distinct features of consistency across sample sizes and stochastic levels. Next, we evaluate the performance and distributional shift of a robust numeric verbalization prompt across single vs. two-step confidence elicitation procedure. We also compare the performance of the same prompt with different versions of GPT and different numerical scales. Finally, we combine the sample-based consistency and verbalized methods to propose a hybrid framework that yields a better uncertainty estimation for GPT models. Overall, our work proposes novel uncertainty quantification methods that will improve the reliability of Large Language Models in misinformation mitigation applications.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes an uncertainty quantification framework that leverages both direct confidence elicitation and sampled-based consistency methods to provide better calibration for NLP misinformation mitigation solutions to improve the reliability of Large Language Models in misinformation mitigation applications."
            },
            "score": 6
        },
        {
            "id": "3fc3460c4554a28e489a0ea6ef067b79b7d301d9",
            "paperId": "3fc3460c4554a28e489a0ea6ef067b79b7d301d9",
            "title": "Active Prompting with Chain-of-Thought for Large Language Models",
            "abstract": "The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks requiring reasoning, such as arithmetic and commonsense reasoning. It is known that the effective design of task-specific prompts is critical for LLMs' ability to produce high-quality answers. In particular, an effective approach for complex question-and-answer tasks is example-based prompting with chain-of-thought (CoT) reasoning, which significantly improves the performance of LLMs. However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks. This paper proposes a new method, Active-Prompt, to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning). For this purpose, we propose a solution to the key problem of determining which questions are the most important and helpful ones to annotate from a pool of task-specific queries. By borrowing ideas from the related problem of uncertainty-based active learning, we introduce several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation. Experimental results demonstrate the superiority of our proposed method, achieving state-of-the-art on eight complex reasoning tasks. Further analyses of different uncertainty metrics, pool sizes, zero-shot learning, and accuracy-uncertainty relationship demonstrate the effectiveness of our method. Our code will be available at https://github.com/shizhediao/active-prompt.",
            "year": 2023,
            "citationCount": 58,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new method to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning), and introduces several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation."
            },
            "score": 5
        },
        {
            "id": "ba63e1ab5b6e9d849982ae293ac0483053badaff",
            "paperId": "ba63e1ab5b6e9d849982ae293ac0483053badaff",
            "title": "Uncertainty in Language Models: Assessment through Rank-Calibration",
            "abstract": "Language Models (LMs) have shown promising performance in natural language generation. However, as LMs often generate incorrect or hallucinated responses, it is crucial to correctly quantify their uncertainty in responding to given inputs. In addition to verbalized confidence elicited via prompting, many uncertainty measures ($e.g.$, semantic entropy and affinity-graph-based measures) have been proposed. However, these measures can differ greatly, and it is unclear how to compare them, partly because they take values over different ranges ($e.g.$, $[0,\\infty)$ or $[0,1]$). In this work, we address this issue by developing a novel and practical framework, termed $Rank$-$Calibration$, to assess uncertainty and confidence measures for LMs. Our key tenet is that higher uncertainty (or lower confidence) should imply lower generation quality, on average. Rank-calibration quantifies deviations from this ideal relationship in a principled manner, without requiring ad hoc binary thresholding of the correctness score ($e.g.$, ROUGE or METEOR). The broad applicability and the granular interpretability of our methods are demonstrated empirically.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel and practical framework, termed $Rank$-$Calibration$ is developed, to assess uncertainty and confidence measures for LMs, with the key tenet that higher uncertainty should imply lower generation quality, on average."
            },
            "score": 5
        },
        {
            "id": "8089b431b2e09c27967428fb542c0935fb95ec30",
            "paperId": "8089b431b2e09c27967428fb542c0935fb95ec30",
            "title": "STAR: Constraint LoRA with Dynamic Active Learning for Data-Efficient Fine-Tuning of Large Language Models",
            "abstract": "Though Large Language Models (LLMs) have demonstrated the powerful capabilities of few-shot learning through prompting methods, supervised training is still necessary for complex reasoning tasks. Because of their extensive parameters and memory consumption, both Parameter-Efficient Fine-Tuning (PEFT) methods and Memory-Efficient Fine-Tuning methods have been proposed for LLMs. Nevertheless, the issue of large annotated data consumption, the aim of Data-Efficient Fine-Tuning, remains unexplored. One obvious way is to combine the PEFT method with active learning. However, the experimental results show that such a combination is not trivial and yields inferior results. Through probe experiments, such observation might be explained by two main reasons: uncertainty gap and poor model calibration. Therefore, in this paper, we propose a novel approach to effectively integrate uncertainty-based active learning and LoRA. Specifically, for the uncertainty gap, we introduce a dynamic uncertainty measurement that combines the uncertainty of the base model and the uncertainty of the full model during the iteration of active learning. For poor model calibration, we incorporate the regularization method during LoRA training to keep the model from being over-confident, and the Monte-Carlo dropout mechanism is employed to enhance the uncertainty estimation. Experimental results show that the proposed approach outperforms existing baseline models on three complex reasoning tasks.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel approach to effectively integrate uncertainty-based active learning and LoRA is proposed, which incorporates the regularization method during LoRA training to keep the model from being over-confident, and the Monte-Carlo dropout mechanism is employed to enhance the uncertainty estimation."
            },
            "score": 5
        },
        {
            "id": "8e8f2081007a0380856774444d5ea18cb5096351",
            "paperId": "8e8f2081007a0380856774444d5ea18cb5096351",
            "title": "Uncertainty-Aware Explainable Recommendation with Large Language Models",
            "abstract": "Providing explanations within the recommendation system would boost user satisfaction and foster trust, especially by elaborating on the reasons for selecting recommended items tailored to the user. The predominant approach in this domain revolves around generating text-based explanations, with a notable emphasis on applying large language models (LLMs). However, refining LLMs for explainable recommendations proves impractical due to time constraints and computing resource limitations. As an alternative, the current approach involves training the prompt rather than the LLM. In this study, we developed a model that utilizes the ID vectors of user and item inputs as prompts for GPT-2. We employed a joint training mechanism within a multi-task learning framework to optimize both the recommendation task and explanation task. This strategy enables a more effective exploration of users' interests, improving recommendation effectiveness and user satisfaction. Through the experiments, our method achieving 1.59 DIV, 0.57 USR and 0.41 FCR on the Yelp, TripAdvisor and Amazon dataset respectively, demonstrates superior performance over four SOTA methods in terms of explainability evaluation metric. In addition, we identified that the proposed model is able to ensure stable textual quality on the three public datasets.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A model is developed that utilizes the ID vectors of user and item inputs as prompts for GPT-2 and is able to ensure stable textual quality on the three public datasets, demonstrating superior performance over four SOTA methods in terms of explainability evaluation metric."
            },
            "score": 5
        },
        {
            "id": "47eb0468ba7b6457d32b6aa0ee15ad269c04864d",
            "paperId": "47eb0468ba7b6457d32b6aa0ee15ad269c04864d",
            "title": "Confidently Wrong: Exploring the Calibration and Expression of (Un)Certainty of Large Language Models in a Multilingual Setting",
            "abstract": "While the fluency and coherence of Large Language Models (LLMs) in text generation have seen significant improvements, their competency in generating appropriate expressions of uncertainty remains limited.Using a multilingual closed-book QA task and GPT-3.5, we explore how well LLMs are calibrated and express certainty across a diverse set of languages, including low-resource settings. Our results reveal strong performance in high-resource languages but a marked decline in performance in lower-resource languages. Across all, we observe an exaggerated expression of confidence in the model, which does not align with the correctness or likelihood of its responses. Our findings highlight the need for further research into accurate calibration of LLMs especially in a multilingual setting.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Using a multilingual closed-book QA task and GPT-3.5, how well LLMs are calibrated and express certainty across a diverse set of languages, including low-resource settings is explored."
            },
            "score": 5
        },
        {
            "id": "2a3c0ffbe70b4aa4b5d118cc0da365fe2a04f262",
            "paperId": "2a3c0ffbe70b4aa4b5d118cc0da365fe2a04f262",
            "title": "Deal, or no deal (or who knows)? Forecasting Uncertainty in Conversations using Large Language Models",
            "abstract": "Effective interlocutors account for the uncertain goals, beliefs, and emotions of others. But even the best human conversationalist cannot perfectly anticipate the trajectory of a dialogue. How well can language models represent inherent uncertainty in conversations? We propose FortUne Dial, an expansion of the long-standing\"conversation forecasting\"task: instead of just accuracy, evaluation is conducted with uncertainty-aware metrics, effectively enabling abstention on individual instances. We study two ways in which language models potentially represent outcome uncertainty (internally, using scores and directly, using tokens) and propose fine-tuning strategies to improve calibration of both representations. Experiments on eight difficult negotiation corpora demonstrate that our proposed fine-tuning strategies (a traditional supervision strategy and an off-policy reinforcement learning strategy) can calibrate smaller open-source models to compete with pre-trained models 10x their size.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes FortUne Dial, an expansion of the long-standing conversation forecasting task, where instead of just accuracy, evaluation is conducted with uncertainty-aware metrics, effectively enabling abstention on individual instances."
            },
            "score": 5
        },
        {
            "id": "670a8db8e47cfe234558ed913242427a1b8b8348",
            "paperId": "670a8db8e47cfe234558ed913242427a1b8b8348",
            "title": "Exploring Predictive Uncertainty and Calibration in NLP: A Study on the Impact of Method & Data Scarcity",
            "abstract": "We investigate the problem of determining the predictive confidence (or, conversely, uncertainty) of a neural classifier through the lens of low-resource languages. By training models on sub-sampled datasets in three different languages, we assess the quality of estimates from a wide array of approaches and their dependence on the amount of available data. We find that while approaches based on pre-trained models and ensembles achieve the best results overall, the quality of uncertainty estimates can surprisingly suffer with more data. We also perform a qualitative analysis of uncertainties on sequences, discovering that a model's total uncertainty seems to be influenced to a large degree by its data uncertainty, not model uncertainty. All model implementations are open-sourced in a software package.",
            "year": 2022,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that while approaches based on pre-trained models and ensembles achieve the best results overall, the quality of uncertainty estimates can surprisingly suffer with more data."
            },
            "score": 5
        },
        {
            "id": "27a6dc0300eb6a002e8f53c2dacd3495c034f9ff",
            "paperId": "27a6dc0300eb6a002e8f53c2dacd3495c034f9ff",
            "title": "RoT: Enhancing Large Language Models with Reflection on Search Trees",
            "abstract": "Large language models (LLMs) have demonstrated impressive capability in reasoning and planning when integrated with tree-search-based prompting methods. However, since these methods ignore the previous search experiences, they often make the same mistakes in the search process. To address this issue, we introduce Reflection on search Trees (RoT), an LLM reflection framework designed to improve the performance of tree-search-based prompting methods. It uses a strong LLM to summarize guidelines from previous tree search experiences to enhance the ability of a weak LLM. The guidelines are instructions about solving this task through tree search which can prevent the weak LLMs from making similar mistakes in the past search process. In addition, we proposed a novel state selection method, which identifies the critical information from historical search processes to help RoT generate more specific and meaningful guidelines. In our extensive experiments, we find that RoT significantly improves the performance of LLMs in reasoning or planning tasks with various tree-search-based prompting methods (e.g., BFS and MCTS). Non-tree-search-based prompting methods such as Chain-of-Thought (CoT) can also benefit from RoT guidelines since RoT can provide task-specific knowledge collected from the search experience.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Reflection on search Trees (RoT) is introduced, an LLM reflection framework designed to improve the performance of tree-search-based prompting methods and uses a strong LLM to summarize guidelines from previous tree search experiences to enhance the ability of a weak LLM."
            },
            "score": 5
        },
        {
            "id": "1ca3b6ff250b4f73486a89f6954edcc4ae21834e",
            "paperId": "1ca3b6ff250b4f73486a89f6954edcc4ae21834e",
            "title": "When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models",
            "abstract": "Recent studies suggest that self-reflective prompting can significantly enhance the reasoning capabilities of Large Language Models (LLMs). However, the use of external feedback as a stop criterion raises doubts about the true extent of LLMs' ability to emulate human-like self-reflection. In this paper, we set out to clarify these capabilities under a more stringent evaluation setting in which we disallow any kind of external feedback. Our findings under this setting show a split: while self-reflection enhances performance in TruthfulQA, it adversely affects results in HotpotQA. We conduct follow-up analyses to clarify the contributing factors in these patterns, and find that the influence of self-reflection is impacted both by reliability of accuracy in models' initial responses, and by overall question difficulty: specifically, self-reflection shows the most benefit when models are less likely to be correct initially, and when overall question difficulty is higher. We also find that self-reflection reduces tendency toward majority voting. Based on our findings, we propose guidelines for decisions on when to implement self-reflection. We release the codebase for reproducing our experiments at https://github.com/yanhong-lbh/LLM-SelfReflection-Eval.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The influence of self-reflection is impacted both by reliability of accuracy in models' initial responses, and by overall question difficulty: specifically, self-reflection shows the most benefit when models are less likely to be correct initially, and when overall question difficulty is higher."
            },
            "score": 5
        },
        {
            "id": "be7dbac2bcaed4cd034a7371004a011933e1bdca",
            "paperId": "be7dbac2bcaed4cd034a7371004a011933e1bdca",
            "title": "Democratizing Reasoning Ability: Tailored Learning from Large Language Model",
            "abstract": "Large language models (LLMs) exhibit impressive emergent abilities in natural language processing, but their democratization is hindered due to huge computation requirements and closed-source nature. Recent research on advancing open-source smaller LMs by distilling knowledge from black-box LLMs has obtained promising results in the instruction-following ability. However, the reasoning ability which is more challenging to foster, is relatively rarely explored. In this paper, we propose a tailored learning approach to distill such reasoning ability to smaller LMs to facilitate the democratization of the exclusive reasoning ability. In contrast to merely employing LLM as a data annotator, we exploit the potential of LLM as a reasoning teacher by building an interactive multi-round learning paradigm. This paradigm enables the student to expose its deficiencies to the black-box teacher who then can provide customized training data in return. Further, to exploit the reasoning potential of the smaller LM, we propose self-reflection learning to motivate the student to learn from self-made mistakes. The learning from self-reflection and LLM are all tailored to the student's learning status, thanks to the seamless integration with the multi-round learning paradigm. Comprehensive experiments and analysis on mathematical and commonsense reasoning tasks demonstrate the effectiveness of our method. The code will be available at https://github.com/Raibows/Learn-to-Reason.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper exploits the potential of LLM as a reasoning teacher by building an interactive multi-round learning paradigm that enables the student to expose its deficiencies to the black-box teacher who then can provide customized training data in return."
            },
            "score": 5
        },
        {
            "id": "72fb75f7c38a83424308c8205bb36cd88995494b",
            "paperId": "72fb75f7c38a83424308c8205bb36cd88995494b",
            "title": "Leveraging Large Language Models for Exploiting ASR Uncertainty",
            "abstract": "While large language models excel in a variety of natural language processing (NLP) tasks, to perform well on spoken language understanding (SLU) tasks, they must either rely on off-the-shelf automatic speech recognition (ASR) systems for transcription, or be equipped with an in-built speech modality. This work focuses on the former scenario, where LLM's accuracy on SLU tasks is constrained by the accuracy of a fixed ASR system on the spoken input. Specifically, we tackle speech-intent classification task, where a high word-error-rate can limit the LLM's ability to understand the spoken intent. Instead of chasing a high accuracy by designing complex or specialized architectures regardless of deployment costs, we seek to answer how far we can go without substantially changing the underlying ASR and LLM, which can potentially be shared by multiple unrelated tasks. To this end, we propose prompting the LLM with an n-best list of ASR hypotheses instead of only the error-prone 1-best hypothesis. We explore prompt-engineering to explain the concept of n-best lists to the LLM; followed by the finetuning of Low-Rank Adapters on the downstream tasks. Our approach using n-best lists proves to be effective on a device-directed speech detection task as well as on a keyword spotting task, where systems using n-best list prompts outperform those using 1-best ASR hypothesis; thus paving the way for an efficient method to exploit ASR uncertainty via LLMs for speech-based applications.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work tackles speech-intent classification task, where a high word-error-rate can limit the LLM's ability to understand the spoken intent, and proposes prompting theLLM with an n-best list of ASR hypotheses instead of only the error-prone 1-best hypothesis."
            },
            "score": 4
        },
        {
            "id": "754d5164e196ff231786d10a48594f3f27d8721f",
            "paperId": "754d5164e196ff231786d10a48594f3f27d8721f",
            "title": "A Comprehensive Study of Multimodal Large Language Models for Image Quality Assessment",
            "abstract": "While Multimodal Large Language Models (MLLMs) have experienced significant advancement on visual understanding and reasoning, their potentials to serve as powerful, flexible, interpretable, and text-driven models for Image Quality Assessment (IQA) remains largely unexplored. In this paper, we conduct a comprehensive and systematic study of prompting MLLMs for IQA. Specifically, we first investigate nine prompting systems for MLLMs as the combinations of three standardized testing procedures in psychophysics (i.e., the single-stimulus, double-stimulus, and multiple-stimulus methods) and three popular prompting strategies in natural language processing (i.e., the standard, in-context, and chain-of-thought prompting). We then present a difficult sample selection procedure, taking into account sample diversity and uncertainty, to further challenge MLLMs equipped with the respective optimal prompting systems. We assess three open-source and one close-source MLLMs on several visual attributes of image quality (e.g., structural and textural distortions, color differences, and geometric transformations) in both full-reference and no-reference scenarios. Experimental results show that only the close-source GPT-4V provides a reasonable account for human perception of image quality, but is weak at discriminating fine-grained quality variations (e.g., color differences) and at comparing visual quality of multiple images, tasks humans can perform effortlessly.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A comprehensive and systematic study of prompting MLLMs for IQA and presents a difficult sample selection procedure, taking into account sample diversity and uncertainty, to further challenge MLLMs equipped with the respective optimal prompting systems."
            },
            "score": 4
        },
        {
            "id": "645d8c40f2a05f0b06f9338cf7635755532d747c",
            "paperId": "645d8c40f2a05f0b06f9338cf7635755532d747c",
            "title": "Uncertainty Awareness of Large Language Models Under Code Distribution Shifts: A Benchmark Study",
            "abstract": "Large Language Models (LLMs) have been widely employed in programming language analysis to enhance human productivity. Yet, their reliability can be compromised by various code distribution shifts, leading to inconsistent outputs. While probabilistic methods are known to mitigate such impact through uncertainty calibration and estimation, their efficacy in the language domain remains underexplored compared to their application in image-based tasks. In this work, we first introduce a large-scale benchmark dataset, incorporating three realistic patterns of code distribution shifts at varying intensities. Then we thoroughly investigate state-of-the-art probabilistic methods applied to CodeLlama using these shifted code snippets. We observe that these methods generally improve the uncertainty awareness of CodeLlama, with increased calibration quality and higher uncertainty estimation~(UE) precision. However, our study further reveals varied performance dynamics across different criteria (e.g., calibration error vs misclassification detection) and trade-off between efficacy and efficiency, highlighting necessary methodological selection tailored to specific contexts.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work thoroughly investigate state-of-the-art probabilistic methods applied to CodeLlama using three realistic patterns of code distribution shifts at varying intensities, and observes that these methods generally improve the uncertainty awareness of CodeLlama, with increased calibration quality and higher uncertainty estimation~(UE) precision."
            },
            "score": 4
        },
        {
            "id": "7a3fbe7aa114c70f26b0aa9e4979261828d9d211",
            "paperId": "7a3fbe7aa114c70f26b0aa9e4979261828d9d211",
            "title": "Predict the Next Word: <Humans exhibit uncertainty in this task and language models _____>",
            "abstract": "Language models (LMs) are statistical models trained to assign probability to human-generated text. As such, it is reasonable to question whether they approximate linguistic variability exhibited by humans well. This form of statistical assessment is difficult to perform at the passage level, for it requires acceptability judgments (i.e., human evaluation) or a robust automated proxy (which is non-trivial). At the word level, however, given some context, samples from an LM can be assessed via exact matching against a prerecorded dataset of alternative single-word continuations of the available context. We exploit this fact and evaluate the LM\u2019s ability to reproduce variability that humans (in particular, a population of English speakers) exhibit in the \u2018next word prediction\u2019 task. This can be seen as assessing a form of calibration, which, in the context of text classification, Baan et al. (2022) termed calibration to human uncertainty. We assess GPT2, BLOOM and ChatGPT and find that they exhibit fairly low calibration to human uncertainty. We also verify the failure of expected calibration error (ECE) to reflect this, and as such, advise the community against relying on it in this setting.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work evaluates the LM\u2019s ability to reproduce variability that humans exhibit in the \u2018next word prediction\u2019 task and assesses GPT2, BLOOM and ChatGPT and finds that they exhibit fairly low calibration to human uncertainty."
            },
            "score": 4
        },
        {
            "id": "020b7ed6a07c60125902a527a3a8d064a842e725",
            "paperId": "020b7ed6a07c60125902a527a3a8d064a842e725",
            "title": "From Robustness to Improved Generalization and Calibration in Pre-trained Language Models",
            "abstract": "Enhancing generalization and uncertainty quantification in pre-trained language models (PLMs) is crucial for their effectiveness and reliability. Building on machine learning research that established the importance of robustness for improving generalization, we investigate the role of representation smoothness, achieved via Jacobian and Hessian regularization, in enhancing PLM performance. Although such regularization methods have proven effective in computer vision, their application in natural language processing (NLP), where PLM inputs are derived from a discrete domain, poses unique challenges. We introduce a novel two-phase regularization approach, JacHess, which minimizes the norms of the Jacobian and Hessian matrices within PLM intermediate representations relative to their inputs. Our evaluation using the GLUE benchmark demonstrates that JacHess significantly improves in-domain generalization and calibration in PLMs, outperforming unregularized fine-tuning and other similar regularization methods.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel two-phase regularization approach, JacHess, is introduced, which minimizes the norms of the Jacobian and Hessian matrices within PLM intermediate representations relative to their inputs, which significantly improves in-domain generalization and calibration in PLMs."
            },
            "score": 4
        },
        {
            "id": "1882849855895456fe842203f245ffaf66b72eff",
            "paperId": "1882849855895456fe842203f245ffaf66b72eff",
            "title": "Bayesian low-rank adaptation for large language models",
            "abstract": "Low-rank adaptation (LoRA) has emerged as a new paradigm for cost-efficient fine-tuning of large language models (LLMs). However, fine-tuned LLMs often become overconfident especially when fine-tuned on small datasets. Bayesian methods, with their inherent ability to estimate uncertainty, serve as potent tools to mitigate overconfidence and enhance calibration. In this work, we introduce Laplace-LoRA, which applies a Bayesian approach to the LoRA parameters. Specifically, Laplace-LoRA applies a Laplace approximation to the posterior over the LoRA parameters, considerably improving the calibration of fine-tuned LLMs.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Laplace-LoRA applies a Laplace approximation to the posterior over the LoRA parameters, considerably improving the calibration of fine-tuned LLMs."
            },
            "score": 4
        },
        {
            "id": "848772a50cee68e88988ded7522e280d1c490598",
            "paperId": "848772a50cee68e88988ded7522e280d1c490598",
            "title": "Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-Augmented Large Language Models",
            "abstract": "Recent proprietary large language models (LLMs), such as GPT-4, have achieved a milestone in tackling diverse challenges in the biomedical domain, ranging from multiple-choice questions to long-form generations. To address challenges that still cannot be handled with the encoded knowledge of LLMs, various retrieval-augmented generation (RAG) methods have been developed by searching documents from the knowledge corpus and appending them unconditionally or selectively to the input of LLMs for generation. However, when applying existing methods to different domain-specific problems, poor generalization becomes apparent, leading to fetching incorrect documents or making inaccurate judgments. In this paper, we introduce Self-BioRAG, a framework reliable for biomedical text that specializes in generating explanations, retrieving domain-specific documents, and self-reflecting generated responses. We utilize 84k filtered biomedical instruction sets to train Self-BioRAG that can assess its generated explanations with customized reflective tokens. Our work proves that domain-specific components, such as a retriever, domain-related document corpus, and instruction sets are necessary for adhering to domain-related instructions. Using three major medical question-answering benchmark datasets, experimental results of Self-BioRAG demonstrate significant performance gains by achieving a 7.2% absolute improvement on average over the state-of-the-art open-foundation model with a parameter size of 7B or less. Overall, we analyze that Self-BioRAG finds the clues in the question, retrieves relevant documents if needed, and understands how to answer with information from retrieved documents and encoded knowledge as a medical expert does. We release our data and code for training our framework components and model weights (7B and 13B) to enhance capabilities in biomedical and clinical domains.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Self-BioRAG is introduced, a framework reliable for biomedical text that specializes in generating explanations, retrieving domain-specific documents, and self-reflecting generated responses and analyzes that Self-BioRAG finds the clues in the question, retrieves relevant documents if needed, and understands how to answer with information from retrieved documents and encoded knowledge as a medical expert does."
            },
            "score": 4
        },
        {
            "id": "385c74957858e7d6856d48e72b5a902b4c1aa28c",
            "paperId": "385c74957858e7d6856d48e72b5a902b4c1aa28c",
            "title": "Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate",
            "abstract": "Modern large language models (LLMs) like ChatGPT have shown remarkable performance on general language tasks but still struggle on complex reasoning tasks, which drives the research on cognitive behaviors of LLMs to explore human-like problem-solving strategies. Along this direction, one representative strategy is self-reflection, which asks an LLM to refine the solution with the feedback generated by itself iteratively. However, our study shows that such reflection-style methods suffer from the Degeneration-of-Thought (DoT) problem: once the LLM has established confidence in its solutions, it is unable to generate novel thoughts later through reflection even if its initial stance is incorrect. To address the DoT problem, we propose a Multi-Agent Debate (MAD) framework, in which multiple agents express their arguments in the state of\"tit for tat\"and a judge manages the debate process to obtain a final solution. Clearly, our MAD framework encourages divergent thinking in LLMs which would be helpful for tasks that require deep levels of contemplation. Experiment results on two challenging datasets, commonsense machine translation and counter-intuitive arithmetic reasoning, demonstrate the effectiveness of our MAD framework. Extensive analyses suggest that the adaptive break of debate and the modest level of\"tit for tat\"state are required for MAD to obtain good performance. Moreover, we find that LLMs might not be a fair judge if different LLMs are used for agents. Codes: https://github.com/Skytliang/Multi-Agents-Debate",
            "year": 2023,
            "citationCount": 125,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Multi-Agent Debate (MAD) framework is proposed, in which multiple agents express their arguments in the state of\"tit for tat\"and a judge manages the debate process to obtain a final solution."
            },
            "score": 4
        },
        {
            "id": "59bcc59de2cc6f280bc777eeba34d14569f8984b",
            "paperId": "59bcc59de2cc6f280bc777eeba34d14569f8984b",
            "title": "Auxiliary task demands mask the capabilities of smaller language models",
            "abstract": "Developmental psychologists have argued about when cognitive capacities such as language understanding or theory of mind emerge. These debates often hinge on the concept of\"task demands\"-- the auxiliary challenges associated with performing a particular evaluation -- that may mask the child's underlying ability. The same issues arise when measuring the capacities of language models (LMs): performance on a task is a function of the model's underlying competence, combined with the model's ability to interpret and perform the task given its available resources. Here, we show that for analogical reasoning, reflective reasoning, word prediction, and grammaticality judgments, evaluation methods with greater task demands yield lower performance than evaluations with reduced demands. This\"demand gap\"is most pronounced for models with fewer parameters and less training data. Our results illustrate that LM performance should not be interpreted as a direct indication of intelligence (or lack thereof), but as a reflection of capacities seen through the lens of researchers' design choices.",
            "year": 2024,
            "citationCount": 2,
            "tldr": null,
            "score": 4
        },
        {
            "id": "e6e15de9e3aa88c2690e2d3ec71b8a75a8002d55",
            "paperId": "e6e15de9e3aa88c2690e2d3ec71b8a75a8002d55",
            "title": "Can Language Models Solve Olympiad Programming?",
            "abstract": "Computing olympiads contain some of the most challenging problems for humans, requiring complex algorithmic reasoning, puzzle solving, in addition to generating efficient code. However, it has been understudied as a domain to evaluate language models (LMs). In this paper, we introduce the USACO benchmark with 307 problems from the USA Computing Olympiad, along with high-quality unit tests, reference code, and official analyses for each problem. These resources enable us to construct and test a range of LM inference methods for competitive programming for the first time. We find GPT-4 only achieves a 8.7% pass@1 accuracy with zero-shot chain-of-thought prompting, and our best inference method improves it to 20.2% using a combination of self-reflection and retrieval over episodic knowledge. However, this is far from solving the benchmark. To better understand the remaining challenges, we design a novel human-in-the-loop study and surprisingly find that a small number of targeted hints enable GPT-4 to solve 13 out of 15 problems previously unsolvable by any model and method. Our benchmark, baseline methods, quantitative results, and qualitative analysis serve as an initial step toward LMs with grounded, creative, and algorithmic reasoning.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The USACO benchmark, baseline methods, quantitative results, and qualitative analysis serve as an initial step toward LMs with grounded, creative, and algorithmic reasoning."
            },
            "score": 4
        },
        {
            "id": "33aeefefc159f3ac887fa05bdec05e6c181134b7",
            "paperId": "33aeefefc159f3ac887fa05bdec05e6c181134b7",
            "title": "A Zero-Shot Language Agent for Computer Control with Structured Reflection",
            "abstract": "Large language models (LLMs) have shown increasing capacity at planning and executing a high-level goal in a live computer environment (e.g. MiniWoB++). To perform a task, recent works often require a model to learn from trace examples of the task via either supervised learning or few/many-shot prompting. Without these trace examples, it remains a challenge how an agent can autonomously learn and improve its control on a computer, which limits the ability of an agent to perform a new task. We approach this problem with a zero-shot agent that requires no given expert traces. Our agent plans for executable actions on a partially observed environment, and iteratively progresses a task by identifying and learning from its mistakes via self-reflection and structured thought management. On the easy tasks of MiniWoB++, we show that our zero-shot agent often outperforms recent SoTAs, with more efficient reasoning. For tasks with more complexity, our reflective agent performs on par with prior best models, even though previous works had the advantages of accessing expert traces or additional screen information.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents a zero-shot agent that plans for executable actions on a partially observed environment, and iteratively progresses a task by identifying and learning from its mistakes via self-reflection and structured thought management, which often outperforms recent SoTAs with more efficient reasoning."
            },
            "score": 4
        },
        {
            "id": "437cbaee4eaee0bf84abbe11750b86b091b9b756",
            "paperId": "437cbaee4eaee0bf84abbe11750b86b091b9b756",
            "title": "MacGyver: Are Large Language Models Creative Problem Solvers?",
            "abstract": "We explore the creative problem-solving capabilities of modern LLMs in a novel constrained setting. To this end, we create MACGYVER, an automatically generated dataset consisting of over 1,600 real-world problems deliberately designed to trigger innovative usage of objects and necessitate out-of-the-box thinking. We then present our collection to both LLMs and humans to compare and contrast their problem-solving abilities. MACGYVER is challenging for both groups, but in unique and complementary ways. For instance, humans excel in tasks they are familiar with but struggle with domain-specific knowledge, leading to a higher variance. In contrast, LLMs, exposed to a variety of specialized knowledge, attempt broader problems but fail by proposing physically-infeasible actions. Finally, we provide a detailed error analysis of LLMs, and demonstrate the potential of enhancing their problem-solving ability with novel prompting techniques such as iterative step-wise reflection and divergent-convergent thinking. This work (1) introduces a fresh arena for intelligent agents focusing on intricate aspects of physical reasoning, planning, and unconventional thinking, which supplements the existing spectrum of machine intelligence; and (2) provides insight into the constrained problem-solving capabilities of both humans and AI.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "MACGYVER is an automatically generated dataset consisting of over 1,600 real-world problems deliberately designed to trigger innovative usage of objects and necessitate out-of-the-box thinking, and provides insight into the constrained problem-solving capabilities of both humans and AI."
            },
            "score": 4
        },
        {
            "id": "740e777ff049102081673bec1adb48ba3169fdba",
            "paperId": "740e777ff049102081673bec1adb48ba3169fdba",
            "title": "Interpretable Natural Language Understanding",
            "abstract": "In recent years, we have witnessed the shift of paradigms in Natural Language Processing (NLP) from fine-tuning large-scale pre-trained language models (PLMs) on task-specific data to prompt-based learning. In the latter, the task description is embedded into the PLM input, enabling the same model to handle multiple tasks. While both approaches have demonstrated impressive performance in various NLP tasks, their opaque nature makes comprehending their inner workings and decision-making processes challenging for humans. In this talk, I will share the research undertaken in my group to address the interpretability concerns surrounding neural models in language understanding. This includes a hierarchical interpretable text classifier going beyond word-level interpretations, uncertainty interpretation of text classifiers built on PLMs, explainable recommender systems by harnessing information across diverse modalities, and explainable student answer scoring. I will conclude my talk by offering insights into potential future developments in interpretable language understanding.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This talk will share the research undertaken in the group to address the interpretability concerns surrounding neural models in language understanding, including a hierarchical interpretable text classifier going beyond word-level interpretations, uncertainty interpretation of text classifiers built on PLMs, explainable recommender systems by harnessing information across diverse modalities, and explainable student answer scoring."
            },
            "score": 4
        },
        {
            "id": "0a922b4fdbe923b5161b5c6f5adfe586bf7304c3",
            "paperId": "0a922b4fdbe923b5161b5c6f5adfe586bf7304c3",
            "title": "Large language models will not replace healthcare professionals: curbing popular fears and hype",
            "abstract": "Following the release of ChatGPT, large language models (LLMs) have entered the mainstream. ChatGPT and GPT-4 recently garnered particular attention for attaining expert-level performance in United States Medical Licensing Examinations. However, performance is not perfect, and has not been as impressive in more specialised tests, such as the Membership of the Royal College of General Practitioners Applied Knowledge Test. ChatGPT frequently \u2018hallucinates\u2019, providing false, unverified information in the same manner as which it delivers facts. While performance in clinical tasks is expected to improve dramatically with the release of GPT-4, remaining inaccuracy and lack of an uncertainty indicator preclude autonomous deployment of ChatGPT and LLM chatbots like it in clinical settings. LLM applications may nevertheless revolutionise cognitive work \u2013 tools such as ChatGPT excel in tasks where specialist knowledge is not required, or is provided by the user prompt: examples include correcting language and rephrasing information for different audiences or within other constraints (e.g. word limits), and it has already been proposed as a tool for administrative tasks, clinical work and patient education. While this does represent an impressive advance in natural language processing, and benefits may be manifold across fields including medicine, these limited use-cases do not live up to the hype surrounding LLMs and artificial intelligence (AI) more generally in 2023. This is due to a fundamental misunderstanding about the form of AI represented by LLMs. Do LLMs represent artificial generalised intelligence (AGI)? The answer is currently probably not, despite emergence of interactive conversational interfaces and few-shot or zero-shot properties \u2013 where models execute tasks that they have previously been exposed to only a few times before, or never before, respectively. This is demonstrated by observing how these models are trained, and the composition of their architecture. The backend LLM (GPT-3, from which GPT-3.5 was developed) underpinning older versions of ChatGPT was initially trained on a dataset of billions of words taken from books, Wikipedia and the wider internet. Through a process of machine learning, the GPT-3 accurately encoded the association between individual words in the training dataset. Through \u2018reinforcement learning from human feedback\u2019, GPT-3 was subsequently finetuned to provide appropriate responses to users\u2019 queries \u2013 producing GPT-3.5. Through these processes, ChatGPT has developed an impressive ability to respond appropriately to diverse prompts, albeit equally lucidly with accurate and inaccurate statements. This lucidity, responsiveness and flexibility have led to sensational claims regarding attainment of AGI that could feasibly replace professionals in cognitive roles. The performance of GPT-4 \u2013 which powers newer versions of ChatGPT \u2013 dwarfs that of GPT-3.5 across tasks including logical reasoning and medical aptitude tests. Moreover, GPT-4 can be prompted to adopt different roles on demand, and will accept multimodal input, processing images as well as text. Prominent figures in industry and academia have advocated for a moratorium on development of more advanced AI systems in response to concerns regarding safety, ethics and fears of replacement. Despite these fears and hype, the barriers to implementation of LLMs replacing healthcare professionals in any capacity still look out of reach. Although GPT-4\u2019s architecture and training are confidential, it likely relies on similar schemata to its predecessor as it exhibits similar (albeit fewer) hallucinations and reasoning errors, including in medicine. None of ChatGPT\u2019s published autonomous training involved actual comprehension of language in context; the meaning (as we understand it) of words in the dataset was immaterial throughout. While this brute force linguistic processing may prove sufficient to develop a form of AGI, it appears that these LLMs will continue to be afflicted by mistakes and errors. Journal of the Royal Society of Medicine; 2023, Vol. 116(5) 181\u2013182",
            "year": 2023,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Following the release of ChatGPT, large language models (LLMs) have entered the mainstream and recently garnered particular attention for attaining expert-level performance in United States Medical Licensing Examinations, but performance has not been as impressive in more specialised tests."
            },
            "score": 3
        },
        {
            "id": "7d2991d25d50d0b82c3b4eaf4a03909ea6ee499c",
            "paperId": "7d2991d25d50d0b82c3b4eaf4a03909ea6ee499c",
            "title": "Explain then Rank: Scale Calibration of Neural Rankers Using Natural Language Explanations from Large Language Models",
            "abstract": "The process of scale calibration in ranking systems involves adjusting the outputs of rankers to correspond with significant qualities like click-through rates or relevance, crucial for mirroring real-world value and thereby boosting the system's effectiveness and reliability. Although there has been research on calibrated ranking losses within learning-to-rank models, the particular issue of adjusting the scale for neural rankers, which excel in handling textual information, has not been thoroughly examined. Neural ranking models are adept at processing text data, yet the application of existing scale calibration techniques to these models poses significant challenges due to their complexity and the intensive training they require, often resulting in suboptimal outcomes. This study delves into the potential of large language models (LLMs) to provide uncertainty measurements for a query and document pair that correlate with the scale-calibrated scores. By employing Monte Carlo sampling to gauge relevance probabilities from LLMs and incorporating natural language explanations (NLEs) to articulate this uncertainty, we carry out comprehensive tests on two major document ranking datasets. Our findings reveal that the approach leveraging NLEs outperforms existing calibration methods under various training scenarios, leading to better calibrated neural rankers.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study delves into the potential of large language models (LLMs) to provide uncertainty measurements for a query and document pair that correlate with the scale-calibrated scores, employing Monte Carlo sampling to gauge relevance probabilities from LLMs and incorporating natural language explanations (NLEs) to articulate this uncertainty."
            },
            "score": 3
        },
        {
            "id": "23bbd94f93e360f373f78ce20f61ec3486b1923d",
            "paperId": "23bbd94f93e360f373f78ce20f61ec3486b1923d",
            "title": "Exploring Large Language Models for Multi-Modal Out-of-Distribution Detection",
            "abstract": "Out-of-distribution (OOD) detection is essential for reliable and trustworthy machine learning. Recent multi-modal OOD detection leverages textual information from in-distribution (ID) class names for visual OOD detection, yet it currently neglects the rich contextual information of ID classes. Large language models (LLMs) encode a wealth of world knowledge and can be prompted to generate descriptive features for each class. Indiscriminately using such knowledge causes catastrophic damage to OOD detection due to LLMs' hallucinations, as is observed by our analysis. In this paper, we propose to apply world knowledge to enhance OOD detection performance through selective generation from LLMs. Specifically, we introduce a consistency-based uncertainty calibration method to estimate the confidence score of each generation. We further extract visual objects from each image to fully capitalize on the aforementioned world knowledge. Extensive experiments demonstrate that our method consistently outperforms the state-of-the-art.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a consistency-based uncertainty calibration method to estimate the confidence score of each generation of large language models and extracts visual objects from each image to fully capitalize on the aforementioned world knowledge."
            },
            "score": 3
        },
        {
            "id": "f9aaeccdf56e71d6af498a192fdb1b39c76a45a1",
            "paperId": "f9aaeccdf56e71d6af498a192fdb1b39c76a45a1",
            "title": "Human-Like Intuitive Behavior and Reasoning Biases Emerged in Language Models - and Disappeared in GPT-4",
            "abstract": "\u2013 Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Therefore, it is of great importance to evaluate their emerging abilities. In this study, we show that LLMs, most notably GPT-3, exhibit behavior that strikingly resembles human-like intuition \u2014 and the cognitive errors that come with it. However, LLMs with higher cognitive capabilities, in particular ChatGPT and GPT-4, learned to avoid succumbing to these errors and perform in a hyperrational manner. For our experiments, we probe LLMs with the Cognitive Reflection Test (CRT) as well as semantic illusions that were originally designed to investigate intuitive decision-making in humans. Moreover, we probe how sturdy the inclination for intuitive-like decision-making is. Our study demonstrates that investigating LLMs with methods from psychology has the potential to reveal otherwise unknown emergent traits.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study shows that LLMs, most notably GPT-3, exhibit behavior that strikingly resembles human-like intuition \u2014 and the cognitive errors that come with it \u2014 and demonstrates that investigating LLMs with methods from psychology has the potential to reveal otherwise unknown emergent traits."
            },
            "score": 3
        },
        {
            "id": "0bfc05adcddd4fe5d1335d96cc313c41526d4558",
            "paperId": "0bfc05adcddd4fe5d1335d96cc313c41526d4558",
            "title": "Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT",
            "abstract": null,
            "year": 2022,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results show that as the models expand in size and linguistic proficiency they increasingly display human-like intuitive system 1 thinking and associated cognitive errors, which shifts notably with the introduction of ChatGPT models, which tend to respond correctly, avoiding the traps embedded in the tasks."
            },
            "score": 3
        },
        {
            "id": "f5cc8782bdb8f2cda8cd5bfc86bf16357ae67e6a",
            "paperId": "f5cc8782bdb8f2cda8cd5bfc86bf16357ae67e6a",
            "title": "Think-Program-reCtify: 3D Situated Reasoning with Large Language Models",
            "abstract": "This work addresses the 3D situated reasoning task which aims to answer questions given egocentric observations in a 3D environment. The task remains challenging as it requires comprehensive 3D perception and complex reasoning skills. End-to-end models trained on supervised data for 3D situated reasoning suffer from data scarcity and generalization ability. Inspired by the recent success of leveraging large language models (LLMs) for visual reasoning, we propose LLM-TPC, a novel framework that leverages the planning, tool usage, and reflection capabilities of LLMs through a ThinkProgram-reCtify loop. The Think phase first decomposes the compositional question into a sequence of steps, and then the Program phase grounds each step to a piece of code and calls carefully designed 3D visual perception modules. Finally, the Rectify phase adjusts the plan and code if the program fails to execute. Experiments and analysis on the SQA3D benchmark demonstrate the effectiveness, interpretability and robustness of our method. Our code is publicly available at https://qingrongh.github.io/LLM-TPC/.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work addresses the 3D situated reasoning task which aims to answer questions given egocentric observations in a 3D environment by proposing LLM-TPC, a novel framework that leverages the planning, tool usage, and reflection capabilities of LLMs through a ThinkProgram-reCtify loop."
            },
            "score": 3
        },
        {
            "id": "53431e4eb6b5e76c67e63585a3babaab8b98eca1",
            "paperId": "53431e4eb6b5e76c67e63585a3babaab8b98eca1",
            "title": "SheetAgent: A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models",
            "abstract": "Spreadsheet manipulation is widely existing in most daily works and significantly improves working efficiency. Large language model (LLM) has been recently attempted for automatic spreadsheet manipulation but has not yet been investigated in complicated and realistic tasks where reasoning challenges exist (e.g., long horizon manipulation with multi-step reasoning and ambiguous requirements). To bridge the gap with the real-world requirements, we introduce $\\textbf{SheetRM}$, a benchmark featuring long-horizon and multi-category tasks with reasoning-dependent manipulation caused by real-life challenges. To mitigate the above challenges, we further propose $\\textbf{SheetAgent}$, a novel autonomous agent that utilizes the power of LLMs. SheetAgent consists of three collaborative modules: $\\textit{Planner}$, $\\textit{Informer}$, and $\\textit{Retriever}$, achieving both advanced reasoning and accurate manipulation over spreadsheets without human interaction through iterative task reasoning and reflection. Extensive experiments demonstrate that SheetAgent delivers 20-30% pass rate improvements on multiple benchmarks over baselines, achieving enhanced precision in spreadsheet manipulation and demonstrating superior table reasoning abilities. More details and visualizations are available at https://sheetagent.github.io.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A benchmark featuring long-horizon and multi-category tasks with reasoning-dependent manipulation caused by real-life challenges is introduced, and a novel autonomous agent that utilizes the power of LLMs is proposed, SheetAgent, achieving enhanced precision in spreadsheet manipulation and demonstrating superior table reasoning abilities."
            },
            "score": 3
        },
        {
            "id": "c237a22698223e4060d83027f399f4fb2aa24291",
            "paperId": "c237a22698223e4060d83027f399f4fb2aa24291",
            "title": "Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations",
            "abstract": "Recommender models excel at providing domain-specific item recommendations by leveraging extensive user behavior data. Despite their ability to act as lightweight domain experts, they struggle to perform versatile tasks such as providing explanations and engaging in conversations. On the other hand, large language models (LLMs) represent a significant step towards artificial general intelligence, showcasing remarkable capabilities in instruction comprehension, commonsense reasoning, and human interaction. However, LLMs lack the knowledge of domain-specific item catalogs and behavioral patterns, particularly in areas that diverge from general world knowledge, such as online e-commerce. Finetuning LLMs for each domain is neither economic nor efficient. In this paper, we bridge the gap between recommender models and LLMs, combining their respective strengths to create a versatile and interactive recommender system. We introduce an efficient framework called \\textbf{InteRecAgent}, which employs LLMs as the brain and recommender models as tools. We first outline a minimal set of essential tools required to transform LLMs into InteRecAgent. We then propose an efficient workflow within InteRecAgent for task execution, incorporating key components such as memory components, dynamic demonstration-augmented task planning, and reflection. InteRecAgent enables traditional recommender systems, such as those ID-based matrix factorization models, to become interactive systems with a natural language interface through the integration of LLMs. Experimental results on several public datasets show that InteRecAgent achieves satisfying performance as a conversational recommender system, outperforming general-purpose LLMs. The source code of InteRecAgent is released at https://aka.ms/recagent.",
            "year": 2023,
            "citationCount": 25,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "InteRecAgent enables traditional recommender systems, such as those ID-based matrix factorization models, to become interactive systems with a natural language interface through the integration of LLMs, and introduces an efficient framework, which employs LLMs as the brain and recommender models as tools."
            },
            "score": 3
        },
        {
            "id": "97d4145117462177e1244a99d7a25afed4c234f7",
            "paperId": "97d4145117462177e1244a99d7a25afed4c234f7",
            "title": "How Many Validation Labels Do You Need? Exploring the Design Space of Label-Efficient Model Ranking",
            "abstract": "This paper presents LEMR (Label-Efficient Model Ranking) and introduces the MoraBench Benchmark. LEMR is a novel framework that minimizes the need for costly annotations in model selection by strategically annotating instances from an unlabeled validation set. To evaluate LEMR, we leverage the MoraBench Benchmark, a comprehensive collection of model outputs across diverse scenarios. Our extensive evaluation across 23 different NLP tasks in semi-supervised learning, weak supervision, and prompt selection tasks demonstrates LEMR's effectiveness in significantly reducing labeling costs. Key findings highlight the impact of suitable ensemble methods, uncertainty sampling strategies, and model committee selection in enhancing model ranking accuracy. LEMR, supported by the insights from MoraBench, provides a cost-effective and accurate solution for model selection, especially valuable in resource-constrained environments.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents LEMR (Label-Efficient Model Ranking) and introduces the MoraBench Benchmark, a comprehensive collection of model outputs across diverse scenarios that provides a cost-effective and accurate solution for model selection, especially valuable in resource-constrained environments."
            },
            "score": 3
        },
        {
            "id": "3e14227862ae21ec691794b8586d6c695c5feb75",
            "paperId": "3e14227862ae21ec691794b8586d6c695c5feb75",
            "title": "RE-MOVE: An Adaptive Policy Design for Robotic Navigation Tasks in Dynamic Environments via Language-Based Feedback",
            "abstract": "Reinforcement learning-based policies for continuous control robotic navigation tasks often fail to adapt to changes in the environment during real-time deployment, which may result in catastrophic failures. To address this limitation, we propose a novel approach called RE-MOVE (REquest help and MOVE on) to adapt already trained policy to real-time changes in the environment without re-training via utilizing a language-based feedback. The proposed approach essentially boils down to addressing two main challenges of (1) when to ask for feedback and, if received, (2) how to incorporate feedback into trained policies. RE-MOVE incorporates an epistemic uncertainty-based framework to determine the optimal time to request instructions-based feedback. For the second challenge, we employ a zero-shot learning natural language processing (NLP) paradigm with efficient, prompt design and leverage state-of-the-art GPT-3.5, Llama-2 language models. To show the efficacy of the proposed approach, we performed extensive synthetic and real-world evaluations in several test-time dynamic navigation scenarios. Utilizing RE-MOVE result in up to 80% enhancement in the attainment of successful goals, coupled with a reduction of 13.50% in the normalized trajectory length, as compared to alternative approaches, particularly in demanding real-world environments with perceptual challenges.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 3
        },
        {
            "id": "712f21411526e8450036d7199637808590be3579",
            "paperId": "712f21411526e8450036d7199637808590be3579",
            "title": "Reflection of Thought: Inversely Eliciting Numerical Reasoning in Language Models via Solving Linear Systems",
            "abstract": "Numerical reasoning over natural language has been a long-standing goal for the research community. However, cutting-edge language models have proven difficult to reliably generalize to a broad range of numbers, although they have shown proficiency in reasoning over common and simple numbers. In this paper, we propose a novel method to elicit and exploit the numerical reasoning knowledge hidden in pre-trained language models using simple anchor numbers. Concretely, we first leverage simple numbers as anchors to probe the implicitly inferred arithmetic expressions from language models, and then explicitly apply the expressions on complex numbers to get corresponding answers. To inversely elicit arithmetic expressions, we transform and formulate the task as an analytically solvable linear system. Experimental results on several numerical reasoning benchmarks demonstrate that our approach significantly improves numerical reasoning capabilities of existing LMs. More importantly, our approach is training-free and simply works in the inference phase, making it highly portable and achieving consistent performance benefits across a variety of language models (GPT-3, T5, BART, etc) in all zero-shot, few-shot, and fine-tuning scenarios.",
            "year": 2022,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel method to elicit and exploit the numerical reasoning knowledge hidden in pre-trained language models using simple anchor numbers, which is training-free and simply works in the inference phase, making it highly portable and achieving consistent performance benefits."
            },
            "score": 2
        },
        {
            "id": "3cbfe152220de84ecf8059fa50c47587a3134c86",
            "paperId": "3cbfe152220de84ecf8059fa50c47587a3134c86",
            "title": "DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models",
            "abstract": "Recent advancements in autonomous driving have relied on data-driven approaches, which are widely adopted but face challenges including dataset bias, overfitting, and uninterpretability. Drawing inspiration from the knowledge-driven nature of human driving, we explore the question of how to instill similar capabilities into autonomous driving systems and summarize a paradigm that integrates an interactive environment, a driver agent, as well as a memory component to address this question. Leveraging large language models (LLMs) with emergent abilities, we propose the DiLu framework, which combines a Reasoning and a Reflection module to enable the system to perform decision-making based on common-sense knowledge and evolve continuously. Extensive experiments prove DiLu's capability to accumulate experience and demonstrate a significant advantage in generalization ability over reinforcement learning-based methods. Moreover, DiLu is able to directly acquire experiences from real-world datasets which highlights its potential to be deployed on practical autonomous driving systems. To the best of our knowledge, we are the first to leverage knowledge-driven capability in decision-making for autonomous vehicles. Through the proposed DiLu framework, LLM is strengthened to apply knowledge and to reason causally in the autonomous driving domain. Project page: https://pjlab-adg.github.io/DiLu/",
            "year": 2023,
            "citationCount": 34,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The DiLu framework is proposed, which combines a Reasoning and a Reflection module to enable the system to perform decision-making based on common-sense knowledge and evolve continuously and is the first to leverage knowledge-driven capability in decision-making for autonomous vehicles."
            },
            "score": 2
        },
        {
            "id": "6d6efaceed21144f30a266b762a733a3f4d2ff4e",
            "paperId": "6d6efaceed21144f30a266b762a733a3f4d2ff4e",
            "title": "Large Language Models and the Future of Organization Theory",
            "abstract": "In this editorial essay, we explore the potential of large language models (LLMs) for conceptual work and for developing theory papers within the field of organization and management studies. We offer a technically informed, but at the same time accessible, analysis of the generative AI technology behind tools such as Bing Chat, ChatGPT, Claude and Gemini, to name the most prominent LLMs currently in use. Our aim in this essay is to go beyond prior work and to provide a more nuanced reflection on the possible application of such technology for the different activities and reasoning processes that constitute theorizing within our domain of scholarly inquiry. Specifically, we highlight ways in which LLMs might augment our theorizing, but we also point out the fundamental constraints in how contemporary LLMs \u2018reason\u2019, setting considerable limits to what such tools might produce as \u2018conceptual\u2019 or \u2018theoretical\u2019 outputs. Given worrisome trade-offs in their use, we urge authors to be careful and reflexive when they use LLMs to assist (parts of) their theorizing, and to transparently disclose this use in their manuscripts. We conclude the essay with a statement of Organization Theory\u2019s editorial policy on the use of LLMs.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This editorial essay offers a technically informed, but at the same time accessible, analysis of the generative AI technology behind tools such as Bing Chat, ChatGPT, Claude and Gemini, to name the most prominent LLMs currently in use."
            },
            "score": 2
        },
        {
            "id": "03dc501149708294e0f0c44ca05134a0a15a30b6",
            "paperId": "03dc501149708294e0f0c44ca05134a0a15a30b6",
            "title": "Developing Metacognitive Instructors through a Guided Journal",
            "abstract": "Metacognitive instructors incorporate awareness and timely self-regulation in their teaching practice to support their current students\u2019 learning. This exploratory study, using mixed methods, gathered empirical data to extend the work on student metacognition by documenting teacher experiences with metacognitive instruction, the impact of instructor use of a guided journal on the development of metacognitive instruction practices, and students\u2019 perceptions of instructor responsiveness to their learning and engagement. Journal Intervention (N = 40) and Control (N = 33) instructors from five institutions and their students (N = 796) responded to multiple questionnaires throughout a semester. Data revealed significantly more baseline familiarity with and engagement in reflective teaching than metacognitive instruction for both groups. Within the Intervention group, qualitative data consistently suggested a positive impact from engagement with the journal, especially with respect to an increased focus on learning objectives and student engagement (rather than on content coverage) in pre-lesson planning, and on being aware of how students were achieving the learning objectives. Significant positive correlations were found between instructor use of the journal and student ratings of instructor responsiveness to their learning and engagement, and instructor use of effective instructional practices. Although instructors were enthusiastic overall about using the journal and incorporating metacognitive instruction, they did report barriers including time, existing habits, and uncertainty about alternate instructional practices. Based on our findings, we share strategies for using our journal prompts as a tool to facilitate faculty development of metacognitive instruction.",
            "year": 2021,
            "citationCount": 2,
            "tldr": null,
            "score": 2
        },
        {
            "id": "5dc14eeeb25d5179ef1f28013c51ee8c0403659b",
            "paperId": "5dc14eeeb25d5179ef1f28013c51ee8c0403659b",
            "title": "Simultaneous versus prospective/retrospective uncertainty monitoring: The effect of response competition across cognitive levels.",
            "abstract": "Early animal-metacognition researchers singled out simultaneous metacognition paradigms for theoretical criticism, because these paradigms presented concretely rewarded perceptual responses and the metacognitive response simultaneously. This method potentially introduced associative cues into the situation that could confound the interpretation of the metacognitive response. Evaluating this possibility, we compared humans' metacognitive performances in simultaneous and nonsimultaneous (prospective, retrospective) paradigms that were otherwise identical. Results show that the metacognition response in these tasks is not prompted by associative cues arising from the simultaneous task format. To the contrary, the metacognitive response is used more robustly and accurately when it is removed from direct competition with the primary perceptual responses. Thus, early researchers were correct to judge that the nonsimultaneous paradigms tap metacognition more robustly and sensitively. However, this is probably true because the simultaneous paradigm mingles responses adjudicated on two different cognitive-processing levels. And, in that case, the metacognitive response can be outcompeted and suppressed by the salient presence of primary, concretely rewarded perceptual responses. (PsycINFO Database Record (c) 2019 APA, all rights reserved).",
            "year": 2019,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Comparisons of humans' metacognitive performances in simultaneous and nonsimultaneous paradigms that were otherwise identical show that the metacognition response in these tasks is not prompted by associative cues arising from the simultaneous task format, and is used more robustly and accurately when it is removed from direct competition with the primary perceptual responses."
            },
            "score": 2
        },
        {
            "id": "ed135daf071c828664cbcd2515f501e898dfa8d0",
            "paperId": "ed135daf071c828664cbcd2515f501e898dfa8d0",
            "title": "Don't Let it be! Creative Co\u2010regulation and Socially Shared Regulation in a Case Study of the Beatles: Get Back",
            "abstract": "Criticism has been raised about creativity researchers narrowing their focus solely to the cognitive processes within individuals. To address these concerns, this case study seeks to expand the scope of creativity research by delving into metacognitive, motivational, and emotional processes involved in collaborative creative problem\u2010solving. The present study applies the concepts of self\u2010regulation, co\u2010regulation, and socially shared regulation to a real\u2010world case study: the documentary The Beatles: Get Back. Observational footage of the band rehearsing and creating new songs for their live show is analyzed within a socially shared regulation of learning (SSRL) framework. Theory\u2010driven coding identified individual facets of self\u2010regulation, co\u2010regulation, and socially shared regulation during collaborative creative problem\u2010solving. The analysis revealed initial problems with self\u2010regulation, especially motivation and emotion regulation, inhibiting collaboration. However, co\u2010regulatory prompts between group members facilitated a shift toward more productive socially shared regulation where the band collectively negotiated goals, plans and ideas. An examination of spontaneous interactions highlighted the interplay between individual self\u2010regulation and socially constructed regulation shaping real\u2010world creative collaboration. Individual metacognitive skills, motivation, and emotions, as well as socially shared group dynamics enabled the members to regulate uncertainty and obstacles in collaborative creative problem\u2010solving.",
            "year": 2023,
            "citationCount": 0,
            "tldr": null,
            "score": 2
        },
        {
            "id": "213a616590be543c5a2d7dee3206ecd816c58b4a",
            "paperId": "213a616590be543c5a2d7dee3206ecd816c58b4a",
            "title": "Macroeconomic Effects of Uncertainty: A Big Data Analysis for India",
            "abstract": "Uncertainty about the current state and near-term outlook of an economy as well as the likely course of future policy actions can prompt economic agents to alter their decisions to spend, save, invest and hire. In this paper, we construct three alternative indices to measure the level of uncertainty for the Indian economy. The first two uncertainty indices are constructed by applying text mining and natural language processing (NLP) techniques on a dataset compiled from leading Indian business newspapers. The third index is based on internet search intensity data available from Google Trends. Empirical findings from a Local Projections-based econometric framework suggest that uncertainty shocks influence financial markets as well as the real economy in India. Our results indicate that both investment activity and real GDP growth slow down when uncertainty increases in the economy. Such uncertainty indices can help strengthen policy simulation exercises to study the impact of low/high uncertainty scenarios and also improve near-term projection of macroeconomic variables which exhibit high degree of sensitivity to uncertainty.",
            "year": 2020,
            "citationCount": 7,
            "tldr": null,
            "score": 1
        },
        {
            "id": "498ed911aa5fb2fedd1985353f5eb41c6c99907d",
            "paperId": "498ed911aa5fb2fedd1985353f5eb41c6c99907d",
            "title": "Translation and cognition",
            "abstract": "1. Translation and cognition: Recent developments (by Shreve, Gregory M.) 2. Part I. Methodological innovation 3. Uncertainty, uncertainty management and metacognitive problem solving in the translation task (by Angelone, Erik) 4. Coordination of reading and writing processes in translation: An eye on uncharted territory (by Dragsted, Barbara) 5. Cognitive effort, syntactic disruption, and visual interference in a sight translation task (by Shreve, Gregory M.) 6. The reformulation challenge in translation: Context reduces polysemy during comprehension, but multiplies creativity during production (by Fougner Rydning, Antin) 7. Translation units and grammatical shifts: Towards an integration of product- and process-based translation research (by Alves, Fabio) 8. Controlled language and readability (by O'Brien, Sharon) 9. Part II. Research design and research issues 10. On paradigms and cognitive translatology (by Munoz Martin, Ricardo) 11. Integrative description of translation processes (by Hansen, Gyde) 12. Are all professionals experts?: Definitions of expertise and reinterpretation of research evidence in process studies (by Jaaskelainen, Riitta) 13. Part III. Integration of translation process research and the cognitive sciences 14. Expertise in interpreting: An expert-performance perspective (by Ericsson, K. Anders) 15. The search for neuro-physiological correlates of expertise in interpreting (by Moser-Mercer, Barbara) 16. Neural and physiological correlates of translation and interpreting in the bilingual brain: Recent perspectives (by Diamond, Bruce J.) 17. Prompting cognates in the bilingual lexicon: Optimizing access during translation (by Stamenov, Maxim I.) 18. Cognitive translation studies: Developments in theory and method (by Halverson, Sandra L.) 19. Contributors 20. Index",
            "year": 2010,
            "citationCount": 88,
            "tldr": null,
            "score": 1
        },
        {
            "id": "87c1e8eff9f2a2f2374b5eac4791d34b30477619",
            "paperId": "87c1e8eff9f2a2f2374b5eac4791d34b30477619",
            "title": "Clinical Natural Language Processing in Languages Other Than English",
            "abstract": "Natural Language Processing (NLP) of clinical free-text has received a lot of attention from the scientific community. Clinical documents are routinely created across health care providing institutions and are generally written in the official language(s) of the country these institutions are located in. As a result, free-text clinical information is written in a large variety of languages. While most of the efforts for clinical NLP have focused on English, there is a strong need to extend this work to other languages, for instance in order to gain medical information about patient cohorts in geographical areas where English is not an official language. Furthermore, adapting current NLP methods developed for English to other languages may provide useful insight on the generalizability of algorithms and lead to increased robustness. This panel aims to provide an overview of clinical NLP for languages other than English, as for example French, Swedish and Japanese and discuss future methodological advances of clinical NLP in a context that encompasses English as well as other languages. General Description of the Panel The goal of this panel is to engage the medical informatics and clinical Natural Language Processing community in a discussion about ways to advance research through languages other than English. We will provide an overview the current state of clinical NLP in a variety of European and non-European languages as well as focused reports on French, Swedish and Bulgarian. We will motivate the need for developing clinical NLP in languages other than English by the potential for methodological and medical advances. Finally, we will propose strategies to contribute to advance work on languages other than English and integrate it in a state-of-the art platform. Clinical NLP in languages other than English Natural Language Processing (NLP) of clinical free-text has received a lot of attention from the scientific community, demonstrating its potential to provide the means to analyze large quantities of documents rapidly and accurately (Demner-Fushman et al. 2010). Prime clinical applications for NLP include assisting healthcare professionals with restrospective studies and clinical decision making. The ability to analyze clinical text in languages other than English opens access to important medical data concerning cohorts of patients who are treated in countries where English is not the official language. Recently, Kohane et al. (2012) also showed the impact of methods allowing an aggregated exploitation of clinical data. In this context, data extracted from clinical texts in languages other than English adds another dimension to data aggregation. As the importance of clinical NLP gains recognition, clinical corpora become available to researchers in languages other than English, prompting work that sometimes builds on methods validated for English. Adapting systems that work well for English to another language is a difficult task that may be carried out with varying level of success depending on the task and language (Grouin et al., 2009; Velupillai et al. 2014; T\u00e4ckstr\u00f6m et al., 2012). For nonEuropean languages, approaches that account for entirely different word and sentence structures sometimes need to be developped (Shinohara et al. 2013), and cultural differences between clinical narrative styles accounted for (Wu et al. 2013). Access to terminologies and corpora in languages other than English can also be challenging (Schulz et al. 2013; Xu et al. 2013). These experiments prompt a reflexion on how to carry out clinical NLP in a more global context: should methods be developed for one language and then ported to other languages? Can the source language method benefit from the porting? Can algorithms be more robust if they are designed with a multilanguage perspective from the start? French is widely spoken around the world and benefits from one of the largest coverage in the UMLS. Automatic de-identification is becoming quite advanced for French (Grouin & N\u00e9v\u00e9ol, 2013), leading to good results for targeted clinical information extraction tasks (Del\u00e9ger et al. 2010; Grouin et al. 2011). Recent efforts from the French biomedical Informatics community have addressed rules and regulations to improve the access of NLP researchers to clinical corpus. Furthermore, the success of initiatives such as that reported by Grouin et al. (2011) increased the awareness of the potential implication of clinical NLP in clinical practice and contributed to making the timing ripe for making clinical corpus available for annotation and NLP tool development. On-going efforts currently address the annotation of clinical corpora for entity, modality and relations. Tools are being designed for information extraction as well as semantic indexing, information retrieval and clinical data visualization. Much of the research in Swedish clinical NLP has used the Stockholm EPR Corpus, (Dalianis 2012), that contains more than one million patient records encompassing the years 2006-2010, from over 550 clinical units origin from Karolinska University Hospital. Part of this corpus has been manually annotated for Protected Health Information, negations, uncertainty levels, symptoms, diseases, drugs, body parts and abbreviations. The annotated corpora have been used both for training of machine learning systems and evaluation. Some applications are explorative as comorbidity networks, warning and reporting systems detecting hospital acquired infections or adverse drug events, but also work on text simplification of patient record content for the layman patient, (Dalianis 2012). Tools that have been developed for this is an adaptation of NegEx for Swedish (Skeppstedt 2012), a system for classifying terms into six levels of assertion levels pyConTextSwe, (Velupillai et al. 2014), abbreviation detection, (Isenius et al. 2012) and machine learning system based on CRF++ that recognizes named clinical entities as symptoms, diseases, drugs and body, (Skeppstedt et al. 2014). Integrating languages other than English in Apache cTAKES Apache cTAKES (ctakes.apache.org) has been quite successful in assembling and sustaining a global community of developers and users of state-of-the-art English language clinical NLP. Because these techniques involve computational machine learning methods, datasets from the targeted language are needed to train and evaluate the algorithms on. We will discuss what types and size of data were used to build the various cTAKES components \u2013 sentence boundary detector, tokenizer, part of speech tagger, syntactic parser, event and temporal expression detector, temporal relation modules, general relation module. We will also discuss what types of gold standard labels (and how much of each type) are needed to port cTAKES components to other language within the light of some use cases such as porting the temporal expression discovery and normalization module originally developed for English (Bethard, 2013) to Swedish. We will outline available resources in other languages such as Swedish, Finnish, Bulgarian. This is a step towards globalization of information extraction from the clinical narrative.",
            "year": 2014,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This panel aims to provide an overview of clinical NLP for languages other than English, as for example French, Swedish and Japanese and discuss future methodological advances of clinical NLP in a context that encompasses English as well as other languages."
            },
            "score": 1
        },
        {
            "id": "b7e0b9c5887569a5157eb00331cf9ff59d0d5b8b",
            "paperId": "b7e0b9c5887569a5157eb00331cf9ff59d0d5b8b",
            "title": "1-1-2008 Second Thoughts",
            "abstract": "Biases in judgment and decision-making often arise at the level of firstorder thoughts. If these initial thoughts are not overridden by secondorder thoughts, they may lead to biased outputs. Current psychological models of legal actors assume that individuals are largely incapable of overcoming these first-order biasing thoughts and that these thoughts ultimately lead to irrational and discriminatory behavior. These models, however, ignore considerable evidence that individuals often naturally engage in self-correction and that situational pressures often encourage self-correction. I discuss the conditions under which self-correction may occur and the possibilities and limits for the law in promoting selfcorrection to overcome biased judgments, decisions, and behavior. Seals... exemplify one of the strongest impulses of legal ordering, to signal formally to people at critical moments that they are about to leave the world of social interaction for the world of compulsion, bureaucracy and impersonality. It is a way of saying \"this counts,\" \"this is for keeps,\" \"we're not kidding around anymore\" and similar things.... With seals, of course, the modern problem has been that as the form of the seal has been attenuated (down to a preprinted \"seal\"), its cautionary effect has been vitiated apace. -Arthur Lefo Our instincts and intuitions sometimes lead us in directions that, on second thought, we'd rather not go. The impulse to hit the brakes when one's car encounters a patch of ice can be consciously overridden by preparing for that possibility. The attractive actor's endorsement of a product may resonate at some affective level, but this appeal often falls flat when its informational content is analyzed. Our initial impressions of students and co-workers may be based on stereotypes of the groups to which they are perceived to belong, but these initial impressions often give way to more nuanced beliefs as we learn more about these * Daniel Caplin Professor of Law & E. James Kelly, Jr.-Class of 1965 Research Professor, University of Virginia School of Law, 580 Massie Road, Charlottesville, VA 22903-1738, greg-mitchell@virginia.edu. This paper benefitted from discussions with Hal Arkes, Richard Petty, and Philip Tetlock, as well as comments by participants at the McGeorge School of Law talk based on an earlier version of this paper and by students in the University of Illinois Law and Economics seminar. 1. Arthur Allen Leff, A Letter from Professor Leff to a Prospective Publisher, 94 YALE L.J. 1852, 185253 (1985) (entry on \"seals\" from Professor Left's legal dictionary proposal). 2009 / Second Thoughts people as individuals. These second thoughts, which may be the product of conscious effort or may come to us just as rapidly and unbidden as the initial thoughts, can serve as important checks on judgments, decisions, and behavior. We learn through experiences that initial reactions to stimuli should sometimes be distrusted or avoided, and we develop a variety of techniques for employing second thoughts to overcome suspect, undesirable, or maladaptive first thoughts.2 The propensity to engage in self-doubt and self-correction varies across persons and situations. Some people, due to their education, upbringing, values, or genetic endowment, naturally engage in reflection and revision more often than others. Nevertheless, all persons with cognitively normal functioning possess the ability to engage in some amount of deliberation, \"metacognition,\" or thought about one's own thoughts-indeed, metacognition is sometimes described as a uniquely human characteristic. Metacognition often follows a feeling of difficulty, surprise, or unease when trying to process information. For example, a fleeting feeling of knowing something that cannot quite be expressed may prompt a memory search, while a feeling of unease upon encountering a member of another social group may lead to vigilance about what one thinks and says during the encounter. But metacognitive processes that lead to adjustments in judgments and beliefs may also occur without our awareness through the operation of automatic processes and associative networks within the mind. Both forms of second 2. By \"first thoughts\" I mean initial responses to stimuli, while \"second thoughts\" refer to any subsequent processing of the stimuli or thoughts initiated by the first thoughts. Cf. Deanna Kuhn, Metacognitive Development, 9 CURRENT DIRECTIONS PSYCHOL. SCI. 178, 178 (2000) (defining metacognition as \"cognition that reflects on, monitors, or regulates first-order cognition\"); Richard E. Petty & Pablo Brifiol, Persuasion: From Single to Multiple to Metacognitive Processes, 3 PERSP. ON PSYCHOL. SCI. 137, 142 (2008) (\"Primary thoughts are those that occur at a direct level of cognition and involve our initial associations of some object with some attribute or feeling. Following a primary thought, people can also generate other thoughts that occur at a second level, involving reflections on the first-level thoughts.\"). A stimulus may at times trigger multiple, conflicting first thoughts, with second thoughts then mediating which of these first thoughts will gain dominance or attention (e.g., meeting another person may activate categories related to this person's sex, race, age, and occupation, with motivation and inhibition mechanisms determining which category, and its related content, captures attention and influences subsequent thought). See C. Neil Macrae & Galen V. Bodenhausen, Social Cognition: Thinking Categorically About Others, 51 ANN. REV. PSYCHOL. 93, 102 (2000). 3. See, e.g., Guy Lories et al., From Social Cognition to Metacognition, in METACOGNITION: COGNITIVE AND SOCIAL DIMENSIONS 1, 1 (Vincent Y. Yzerbyt et al. eds., 1998) (\"The possibility of metacognition seems typical of the human species and may be related to our being linguistic animals. It stands as one of the important differences between animal and human cognition and the very existence of psychology is proof of our interest in our own mental processes.\"). Whether metacognition is in fact uniquely human is now the subject of debate. Compare J. David Smith & David A. Washburn, Uncertainty Monitoring and Metacognition by Animals, 14 CURRENT DIRECTIONS PSYCHOL. SC. 19 (2005) (summarizing research confirming animals' capacity for metacognition), with Peter Carruthers, Meta-Cognition in Animals: A Skeptical Look, 23 MIND & LANGUAGE 58 (2008) (arguing that metacognition processes within non-humans should not be accepted from the current public data). The capacity for metacognition appears to vary developmentally as well. Kuhn and Pease argue, for instance, that metacognitive executive control of first-order thoughts develops considerably during adolescence. See Deanna Kuhn & Maria Pease, Do Children and Adults Learn Differently?, 7 J. COGNITION & DEV. 279",
            "year": 2019,
            "citationCount": 0,
            "tldr": null,
            "score": 1
        }
    ],
    "novelty": "yes"
}