{
    "topic_description": "novel prompting methods that can better quantify uncertainty or calibrate the confidence of large language models",
    "idea_name": "Uncertainty-Guided Iterative Prompting",
    "raw_idea": {
        "Type": "prompting",
        "Problem": "LLMs often struggle to accurately estimate their uncertainty, especially for complex, multi-step reasoning tasks.",
        "Existing Methods": "Existing methods for uncertainty estimation in LLMs include Monte Carlo dropout, ensemble methods, and confidence calibration techniques.",
        "Motivation": "We propose leveraging the model's uncertainty estimates to guide an iterative prompting process, where the model progressively refines its reasoning based on its confidence at each step.",
        "Proposed Method": "Uncertainty-Guided Iterative Prompting (UGIP) involves the following steps: 1) Initial Reasoning: Prompt the LLM to generate an initial reasoning chain for the given task. 2) Uncertainty Estimation: Estimate the model's uncertainty at each step of the reasoning chain using techniques like ensemble disagreement or variational inference. 3) Uncertainty-Guided Refinement: Identify the most uncertain reasoning steps and prompt the model to provide additional detail or clarification for these steps. 4) Iterative Refinement: Repeat steps 2-3 until the overall uncertainty falls below a predefined threshold or a maximum number of iterations is reached.",
        "Experiment Plan": "Evaluate UGIP on challenging reasoning tasks, such as multi-hop question answering and mathematical problem-solving. Compare the performance and calibration of UGIP against baseline methods, including standard iterative prompting and uncertainty estimation techniques applied post-hoc."
    },
    "full_experiment_plan": {
        "Title": "Uncertainty-Guided Iterative Prompting for Improved Reasoning in Large Language Models",
        "Problem Statement": "Large Language Models (LLMs) often struggle to accurately estimate their uncertainty, especially for complex, multi-step reasoning tasks. This can lead to overconfident predictions and hinder their ability to engage in reliable reasoning.",
        "Motivation": "Existing methods for uncertainty estimation in LLMs, such as Monte Carlo dropout, ensemble methods, and confidence calibration techniques, often require significant computational overhead or additional training. We propose a novel prompting-based approach that leverages the model's inherent uncertainty estimates to guide an iterative reasoning process. By prompting the model to refine its reasoning based on its confidence at each step, we aim to improve the model's reasoning capabilities without the need for expensive retraining or external tools.",
        "Proposed Method": "We introduce Uncertainty-Guided Iterative Prompting (UGIP), a multi-step prompting approach that alternates between reasoning and uncertainty estimation. The key steps are:\n1. Initial Reasoning: Prompt the LLM to generate an initial reasoning chain for the given task.\n2. Uncertainty Estimation: Estimate the model's uncertainty at each step of the reasoning chain using techniques like ensemble disagreement or variational inference.\n3. Uncertainty-Guided Refinement: Identify the most uncertain reasoning steps and prompt the model to provide additional detail or clarification for these steps.\n4. Iterative Refinement: Repeat steps 2-3 until the overall uncertainty falls below a predefined threshold or a maximum number of iterations is reached.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Dataset Selection": "Evaluate UGIP on challenging reasoning tasks, such as multi-hop question answering (e.g., HotpotQA, QAngaroo) and mathematical problem-solving (e.g., GSM8K, MATH). These datasets require complex, multi-step reasoning and are well-suited for testing the effectiveness of uncertainty-guided prompting.",
            "Step 2: Baseline Methods": "Compare the performance and calibration of UGIP against the following baselines:\n1. Standard prompting: Prompt the model to generate a reasoning chain without uncertainty guidance.\n2. Iterative prompting: Prompt the model to refine its reasoning over multiple iterations, but without explicit uncertainty estimation.\n3. Post-hoc uncertainty estimation: Apply uncertainty estimation techniques (e.g., Monte Carlo dropout, ensemble methods) to the model's final output, without using uncertainty to guide the reasoning process.",
            "Step 3: Prompt Engineering": "Design a set of prompts for each step of the UGIP process:\n1. Initial Reasoning Prompt: 'Please provide a step-by-step reasoning chain to answer the following question: [QUESTION]'\n2. Uncertainty Estimation Prompt: 'For each step in the reasoning chain below, estimate your confidence in the correctness of the step on a scale from 1 (very uncertain) to 5 (very confident): [REASONING_CHAIN]'\n3. Refinement Prompt: 'Please provide additional detail or clarification for the reasoning steps where your confidence was below 3: [UNCERTAIN_STEPS]'",
            "Step 4: Uncertainty Estimation": "Implement uncertainty estimation techniques for step 2 of UGIP:\n1. Ensemble Disagreement: Generate multiple reasoning chains using different random seeds and measure the disagreement between the chains at each step.\n2. Variational Inference: Use a variational inference approach (e.g., MC Dropout) to estimate the model's uncertainty at each step of the reasoning chain.",
            "Step 5: Iterative Refinement": "Implement the iterative refinement loop (steps 2-4 of UGIP):\n1. Set a maximum number of iterations (e.g., 3) and an uncertainty threshold (e.g., 2.5).\n2. Repeat the uncertainty estimation and refinement steps until the overall uncertainty falls below the threshold or the maximum number of iterations is reached.",
            "Step 6: Evaluation Metrics": "Evaluate the performance and calibration of UGIP and the baseline methods using the following metrics:\n1. Accuracy: Measure the percentage of correctly answered questions.\n2. Calibration: Assess the calibration of the model's uncertainty estimates using metrics like Brier score and expected calibration error (ECE).\n3. Efficiency: Record the number of iterations and the total time taken for each method to generate the final reasoning chain.",
            "Step 7: Model Selection": "Experiment with different LLMs, such as GPT-3.5 (text-davinci-002), GPT-4, and PaLM, to assess the effectiveness of UGIP across various model architectures and sizes.",
            "Step 8: Ablation Studies": "Conduct ablation studies to understand the contribution of each component of UGIP:\n1. Remove the uncertainty estimation step and guide the refinement process using heuristics (e.g., refine the longest reasoning steps).\n2. Vary the uncertainty threshold and the maximum number of iterations to analyze their impact on performance and efficiency.",
            "Step 9: Error Analysis": "Perform a qualitative error analysis to identify common failure modes and areas for improvement:\n1. Examine cases where UGIP fails to improve the reasoning chain despite multiple iterations.\n2. Analyze the quality and relevance of the model's uncertainty estimates and their impact on the refinement process.",
            "Step 10: Results and Discussion": "Present the experimental results, comparing UGIP's performance and calibration against the baseline methods. Discuss the strengths and limitations of the proposed approach and highlight potential areas for future research."
        },
        "Test Case Examples": {
            "Example 1": {
                "Question": "What is the capital of France?",
                "Baseline Prompt": "Please provide a step-by-step reasoning chain to answer the following question: What is the capital of France?",
                "Baseline Output": "Step 1: France is a country in Europe.\nStep 2: The capital of France is Paris.\nTherefore, the capital of France is Paris.",
                "UGIP Prompt (Iteration 1)": "Please provide a step-by-step reasoning chain to answer the following question: What is the capital of France?\n\nFor each step in the reasoning chain below, estimate your confidence in the correctness of the step on a scale from 1 (very uncertain) to 5 (very confident):\nStep 1: France is a country in Europe.\nStep 2: The capital of France is Paris.",
                "UGIP Output (Iteration 1)": "Step 1: France is a country in Europe. Confidence: 5\nStep 2: The capital of France is Paris. Confidence: 5\nTherefore, the capital of France is Paris.",
                "Explanation": "In this example, the baseline reasoning chain is already correct and confident, so UGIP does not need to perform any refinement steps."
            },
            "Example 2": {
                "Question": "Who was the first person to win two Nobel Prizes in different sciences?",
                "Baseline Prompt": "Please provide a step-by-step reasoning chain to answer the following question: Who was the first person to win two Nobel Prizes in different sciences?",
                "Baseline Output": "Step 1: The first person to win two Nobel Prizes was Marie Curie.\nStep 2: Marie Curie won the Nobel Prize in Physics in 1903.\nStep 3: Marie Curie won the Nobel Prize in Chemistry in 1911.\nTherefore, Marie Curie was the first person to win two Nobel Prizes in different sciences.",
                "UGIP Prompt (Iteration 1)": "Please provide a step-by-step reasoning chain to answer the following question: Who was the first person to win two Nobel Prizes in different sciences?\n\nFor each step in the reasoning chain below, estimate your confidence in the correctness of the step on a scale from 1 (very uncertain) to 5 (very confident):\nStep 1: The first person to win two Nobel Prizes was Marie Curie.\nStep 2: Marie Curie won the Nobel Prize in Physics in 1903.\nStep 3: Marie Curie won the Nobel Prize in Chemistry in 1911.",
                "UGIP Output (Iteration 1)": "Step 1: The first person to win two Nobel Prizes was Marie Curie. Confidence: 3\nStep 2: Marie Curie won the Nobel Prize in Physics in 1903. Confidence: 4\nStep 3: Marie Curie won the Nobel Prize in Chemistry in 1911. Confidence: 4\nTherefore, Marie Curie was the first person to win two Nobel Prizes in different sciences.",
                "UGIP Prompt (Iteration 2)": "Please provide additional detail or clarification for the reasoning steps where your confidence was below 3:\nStep 1: The first person to win two Nobel Prizes was Marie Curie.",
                "UGIP Output (Iteration 2)": "Step 1: Marie Curie was the first person to win Nobel Prizes in two different sciences. She won the Nobel Prize in Physics in 1903 for her research on radioactivity and the Nobel Prize in Chemistry in 1911 for her discovery of the elements polonium and radium. Confidence: 5\nStep 2: Marie Curie won the Nobel Prize in Physics in 1903. Confidence: 4\nStep 3: Marie Curie won the Nobel Prize in Chemistry in 1911. Confidence: 4\nTherefore, Marie Curie was the first person to win two Nobel Prizes in different sciences.",
                "Explanation": "In this example, the baseline reasoning chain is correct but lacks confidence in the first step. UGIP identifies this uncertainty and prompts the model to provide additional details, resulting in a more confident and informative reasoning chain."
            }
        },
        "Fallback Plan": "If the proposed UGIP method does not outperform the baseline methods, consider the following alternative approaches:\n1. Analyze the quality of the uncertainty estimates: Investigate whether the uncertainty estimation techniques used in UGIP accurately capture the model's confidence. Experiment with alternative uncertainty estimation methods or fine-tune the existing techniques to improve their reliability.\n2. Explore different prompting strategies: Modify the prompts used in UGIP to elicit more informative and confident responses from the model. Experiment with different prompt formats, such as question-answering or completion-style prompts, and evaluate their impact on the model's reasoning performance.\n3. Incorporate external knowledge: Augment UGIP with external knowledge sources, such as knowledge bases or retrieval-based methods, to provide additional context and support for the model's reasoning process. This can help the model generate more accurate and confident reasoning chains, especially for complex or domain-specific tasks.\n4. Investigate the impact of model size and architecture: Assess the performance of UGIP across a wider range of LLMs, including smaller models and models with different architectures (e.g., sparse attention, mixture-of-experts). Analyze how the model's size and architecture influence its ability to benefit from uncertainty-guided prompting.\n5. Conduct a detailed error analysis: Perform a thorough qualitative analysis of the cases where UGIP fails to improve the reasoning chain. Identify common patterns or challenges that hinder the effectiveness of uncertainty-guided prompting, such as linguistic ambiguity, lack of common sense reasoning, or domain-specific knowledge gaps. Use these insights to inform future research directions and develop targeted solutions.\nIf the proposed UGIP method consistently underperforms compared to the baselines across multiple datasets and model architectures, consider pivoting the research to focus on understanding the limitations of uncertainty-guided prompting. Conduct a comprehensive analysis of the factors that contribute to the method's failure and propose alternative approaches based on the insights gained from this analysis. This can lead to valuable contributions in the form of identifying challenges and opportunities for future research on improving reasoning in LLMs."
    },
    "novelty_queries": [
        "KeywordQuery(\"uncertainty guided prompting language models\")",
        "KeywordQuery(\"iterative prompting uncertainty language models\")",
        "KeywordQuery(\"uncertainty estimation reasoning language models\")",
        "KeywordQuery(\"Uncertainty-Guided Iterative Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "8f7297454d7f44365b9bcda5ebb9439a43daf5e6",
            "paperId": "8f7297454d7f44365b9bcda5ebb9439a43daf5e6",
            "title": "Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs",
            "abstract": "Empowering large language models to accurately express confidence in their answers is essential for trustworthy decision-making. Previous confidence elicitation methods, which primarily rely on white-box access to internal model information or model fine-tuning, have become less suitable for LLMs, especially closed-source commercial APIs. This leads to a growing need to explore the untapped area of black-box approaches for LLM uncertainty estimation. To better break down the problem, we define a systematic framework with three components: prompting strategies for eliciting verbalized confidence, sampling methods for generating multiple responses, and aggregation techniques for computing consistency. We then benchmark these methods on two key tasks-confidence calibration and failure prediction-across five types of datasets (e.g., commonsense and arithmetic reasoning) and five widely-used LLMs including GPT-4 and LLaMA 2 Chat. Our analysis uncovers several key insights: 1) LLMs, when verbalizing their confidence, tend to be overconfident, potentially imitating human patterns of expressing confidence. 2) As model capability scales up, both calibration and failure prediction performance improve. 3) Employing our proposed strategies, such as human-inspired prompts, consistency among multiple responses, and better aggregation strategies can help mitigate this overconfidence from various perspectives. 4) Comparisons with white-box methods indicate that while white-box methods perform better, the gap is narrow, e.g., 0.522 to 0.605 in AUROC. Despite these advancements, none of these techniques consistently outperform others, and all investigated methods struggle in challenging tasks, such as those requiring professional knowledge, indicating significant scope for improvement. We believe this study can serve as a strong baseline and provide insights for eliciting confidence in black-box LLMs.",
            "year": 2023,
            "citationCount": 97,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study defines a systematic framework with three components: prompting strategies for eliciting verbalized confidence, sampling methods for generating multiple responses, and aggregation techniques for computing consistency and believes it can serve as a strong baseline and provide insights for eliciting confidence in black-box LLMs."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is improving the reasoning capabilities of large language models (LLMs) by leveraging their inherent uncertainty estimates to guide an iterative prompting process. The approach involves alternating between reasoning and uncertainty estimation, identifying the most uncertain reasoning steps, and prompting the model to refine its reasoning based on its confidence at each step.\n\nThe research problem in the paper is empowering LLMs to accurately express confidence in their answers for trustworthy decision-making. The approach involves exploring black-box methods for LLM uncertainty estimation, including prompting strategies for eliciting verbalized confidence, sampling methods for generating multiple responses, and aggregation techniques for computing consistency.\n\nWhile both the proposal and the paper focus on uncertainty estimation in LLMs, the research problems and approaches are different. The proposal aims to improve reasoning capabilities using uncertainty-guided iterative prompting, while the paper focuses on evaluating black-box methods for eliciting and aggregating verbalized confidence from LLMs.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "ac37accd7aedf1c25c3d54c7982579b297b3ff2b",
            "paperId": "ac37accd7aedf1c25c3d54c7982579b297b3ff2b",
            "title": "Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models",
            "abstract": "Large language models (LLMs) can achieve highly effective performance on various reasoning tasks by incorporating step-by-step chain-of-thought (CoT) prompting as demonstrations. However, the reasoning chains of demonstrations generated by LLMs are prone to errors, which can subsequently lead to incorrect reasoning during inference. Furthermore, inappropriate exemplars (overly simplistic or complex), can affect overall performance among varying levels of difficulty. We introduce Iter-CoT (Iterative bootstrapping in Chain-of-Thoughts Prompting), an iterative bootstrapping approach for selecting exemplars and generating reasoning chains. By utilizing iterative bootstrapping, our approach enables LLMs to autonomously rectify errors, resulting in more precise and comprehensive reasoning chains. Simultaneously, our approach selects challenging yet answerable questions accompanied by reasoning chains as exemplars with a moderate level of difficulty, which enhances the LLMs' generalizability across varying levels of difficulty. Experimental results indicate that Iter-CoT exhibits superiority, achieving competitive performance across three distinct reasoning tasks on ten datasets.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "By utilizing iterative bootstrapping, this approach enables LLMs to autonomously rectify errors, resulting in more precise and comprehensive reasoning chains, which enhances the LLMs' generalizability across varying levels of difficulty."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the reasoning capabilities of large language models (LLMs) by leveraging their inherent uncertainty estimates to guide an iterative prompting process. The proposed approach, Uncertainty-Guided Iterative Prompting (UGIP), alternates between reasoning and uncertainty estimation to refine the model's reasoning based on its confidence at each step.\n\nThe research problem in the paper is enhancing the performance of LLMs on reasoning tasks by incorporating step-by-step chain-of-thought (CoT) prompting as demonstrations. The proposed approach, Iter-CoT (Iterative bootstrapping in Chain-of-Thoughts Prompting), utilizes iterative bootstrapping to select exemplars and generate reasoning chains, enabling LLMs to rectify errors and improve their generalizability across varying levels of difficulty.\n\nWhile both the proposal and the paper aim to improve the reasoning capabilities of LLMs, their approaches differ. The proposal focuses on using uncertainty estimates to guide the iterative prompting process, while the paper emphasizes iterative bootstrapping for exemplar selection and reasoning chain generation.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "2aba5bba16dac5cd62683bab9de5d6faaaed0de1",
            "paperId": "2aba5bba16dac5cd62683bab9de5d6faaaed0de1",
            "title": "Shepherd Pre-trained Language Models to Develop a Train of Thought: An Iterative Prompting Approach",
            "abstract": "While Pre-trained Language Models (PLMs) 001 internalize a great amount of world knowledge, 002 they have been shown incapable of recalling 003 these knowledge to solve tasks requiring com-004 plex & multi-step inference procedures. Simi-005 lar to how humans develop a \u201ctrain of thought\u201d 006 for these tasks, how can we equip PLMs with 007 such abilities? In this work, we explore an iter-008 ative prompting framework, a new prompting 009 paradigm which progressively elicits relevant 010 knowledge from PLMs for multi-step inference 011 tasks. We identify key limitations of existing 012 prompting methods, namely they are either re-013 stricted to queries with a single identifiable re-014 lation/predicate, or being agnostic to input con-015 texts, which makes it difficult to capture vari-016 abilities across different inference steps. We 017 propose an iterative context-aware prompter, 018 which addresses these limitations by learning 019 to dynamically synthesize prompts conditioned 020 on the current step\u2019s contexts. Experiments on 021 three datasets involving multi-step inference 022 show the effectiveness of the iterative scheme 023 and the context-aware prompter design. 1 024",
            "year": 2022,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores an iter-008 ative prompting framework, a new prompting 009 paradigm which progressively elicits relevant 010 knowledge from PLMs for multi-step inference 011 tasks, and proposes an iterative context-aware prompter design which addresses key limitations of existing prompting methods."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the reasoning capabilities of large language models (LLMs) by leveraging their inherent uncertainty estimates to guide an iterative prompting process. The proposed approach, Uncertainty-Guided Iterative Prompting (UGIP), alternates between reasoning and uncertainty estimation to refine the model's reasoning based on its confidence at each step.\n\nThe research problem in the paper is equipping pre-trained language models (PLMs) with the ability to develop a \"train of thought\" for tasks requiring complex and multi-step inference procedures. The proposed approach is an iterative prompting framework that progressively elicits relevant knowledge from PLMs for multi-step inference tasks, using an iterative context-aware prompter that dynamically synthesizes prompts conditioned on the current step's contexts.\n\nWhile both the proposal and the paper aim to improve the reasoning capabilities of language models through iterative prompting, the proposal focuses on leveraging uncertainty estimates to guide the iterative process, while the paper focuses on dynamically synthesizing context-aware prompts. The proposal does not mention the idea of developing a \"train of thought\" or eliciting relevant knowledge progressively, which are key aspects of the paper's approach.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "1fa4469e5bc5d096572902fe14b0d66078a24c47",
            "paperId": "1fa4469e5bc5d096572902fe14b0d66078a24c47",
            "title": "Navigating the Grey Area: Expressions of Overconfidence and Uncertainty in Language Models",
            "abstract": "Despite increasingly \ufb02uent, relevant, and coherent language generation, major gaps remain between how humans and machines use language. We argue that a key dimension that is missing from our understanding of language models (LMs) is the model\u2019s ability to interpret and generate expressions of uncertainty . Whether it be the weatherperson announcing a chance of rain or a doctor giving a diagnosis, information is often not black-and-white and expressions of uncertainty provide nuance to support human-decision making. The increasing deployment of LMs in the wild motivates us to investigate whether LMs are capable of interpreting expressions of uncertainty and how LMs\u2019 behaviors change when learning to emit their own expressions of uncertainty. When injecting expressions of uncertainty into prompts (e.g., \"I think the answer is...\"), we discover that GPT3\u2019s generations vary upwards of 80% in accuracy based on the expression used. We analyze the linguistic characteristics of these expressions and \ufb01nd a drop in accuracy when naturalistic expressions of certainty are present. We \ufb01nd similar effects when teaching models to emit their own expressions of uncertainty, where model calibration suffers when teaching models to emit certainty rather than un certainty. Together, these results highlight the challenges of building LMs that interpret and generate trustworthy expressions of uncertainty.",
            "year": 2023,
            "citationCount": 54,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that GPT3\u2019s generations vary upwards of 80% in accuracy based on the expression used, and the challenges of building LMs that interpret and generate trustworthy expressions of uncertainty are highlighted."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the reasoning capabilities of large language models by leveraging their inherent uncertainty estimates to guide an iterative prompting process. The approach involves alternating between reasoning and uncertainty estimation to refine the model's reasoning based on its confidence at each step.\n\nThe research problem in the paper is understanding the ability of language models to interpret and generate expressions of uncertainty. The approach involves injecting expressions of uncertainty into prompts and analyzing the impact on the model's accuracy and calibration.\n\nThe proposal focuses on improving reasoning through uncertainty-guided prompting, while the paper investigates the interpretation and generation of uncertainty expressions. The problems and approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "5424e311319c58847b4c690d5c91090e3b6a4ac3",
            "paperId": "5424e311319c58847b4c690d5c91090e3b6a4ac3",
            "title": "Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models",
            "abstract": "While Large Language Models (LLMs) have demonstrated remarkable potential in natural language generation and instruction following, a persistent challenge lies in their susceptibility to\"hallucinations\", which erodes trust in their outputs. Although Uncertainty Quantification (UQ) presents a promising solution, its accurate implementation within the context of LLMs remains a significant hurdle. To address this critical roadblock, our research originates from a fundamental heuristic insight: tokens within auto-regressive LLM-generated text do not equally reflect the underlying meaning. Some tokens carry greater relevance and representativeness than others, owing to the phenomenon of\"linguistic redundancy\", wherein a select few keywords suffice to convey the essence of lengthy sentences. Regrettably, existing methodologies treat all tokens with equal importance when estimating uncertainty, disregarding these inherent generative inequalities. Our analysis reveals a significant issue with state-of-the-art: numerous tokens (and sentences) of limited semantic significance receive equal or even excessive weighting during uncertainty estimation. To rectify this bias, we propose to jointly Shifting Attention to more Relevant (SAR) components, at both the token- and the sentence-levels for accurate uncertainty estimation. We conduct extensive experiments involving a range of popular\"off-the-shelf\"LLMs, including instruction-tuned LLMs such as Vicuna, WizardLM, and LLaMA-2-chat, as well as pretrained LLMs like OPT and LLaMA, with model sizes extending up to 33B parameters. We carry out evaluation across various free-form question-answering tasks, encompassing domains such as reading comprehension, science Q&A, and medical Q&A. Our experimental results demonstrate the superior performance of SAR in addressing the challenges of uncertainty estimation within the realm of LLMs.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results demonstrate the superior performance of SAR in addressing the challenges of uncertainty estimation within the realm of LLMs, and propose to jointly Shifting Attention to more Relevant (SAR) components, at both the token- and the sentence-levels for accurate uncertainty estimation."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the reasoning capabilities of large language models (LLMs) by using uncertainty-guided iterative prompting, while the paper focuses on improving uncertainty estimation in LLMs by shifting attention to more relevant components at both token and sentence levels.\n\nProject Proposal: Improving reasoning in LLMs using uncertainty-guided iterative prompting.\nPaper: Improving uncertainty estimation in LLMs by shifting attention to more relevant components.\n\nThe two works address different problems and propose different approaches, so they are not directly relevant to each other.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "ea0d41514a41f8273f13b3b277e7fcbbc65a8549",
            "paperId": "ea0d41514a41f8273f13b3b277e7fcbbc65a8549",
            "title": "Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models",
            "abstract": "The recent performance leap of Large Language Models (LLMs) opens up new opportunities across numerous industrial applications and domains. However, erroneous generations, such as false predictions, misinformation, and hallucination made by LLMs, have also raised severe concerns for the trustworthiness of LLMs', especially in safety-, security- and reliability-sensitive scenarios, potentially hindering real-world adoptions. While uncertainty estimation has shown its potential for interpreting the prediction risks made by general machine learning (ML) models, little is known about whether and to what extent it can help explore an LLM's capabilities and counteract its undesired behavior. To bridge the gap, in this paper, we initiate an exploratory study on the risk assessment of LLMs from the lens of uncertainty. In particular, we experiment with twelve uncertainty estimation methods and four LLMs on four prominent natural language processing (NLP) tasks to investigate to what extent uncertainty estimation techniques could help characterize the prediction risks of LLMs. Our findings validate the effectiveness of uncertainty estimation for revealing LLMs' uncertain/non-factual predictions. In addition to general NLP tasks, we extensively conduct experiments with four LLMs for code generation on two datasets. We find that uncertainty estimation can potentially uncover buggy programs generated by LLMs. Insights from our study shed light on future design and development for reliable LLMs, facilitating further research toward enhancing the trustworthiness of LLMs.",
            "year": 2023,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An exploratory study on the risk assessment of LLMs from the lens of uncertainty is initiated, finding that uncertainty estimation can potentially uncover buggy programs generated by LLMs."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the reasoning capabilities of large language models (LLMs) by leveraging their inherent uncertainty estimates to guide an iterative prompting process. The approach is to use a multi-step prompting method called Uncertainty-Guided Iterative Prompting (UGIP) that alternates between reasoning and uncertainty estimation.\n\nThe research problem in the paper is assessing the prediction risks of LLMs using uncertainty estimation techniques. The approach is to experiment with twelve uncertainty estimation methods and four LLMs on four prominent natural language processing (NLP) tasks and code generation datasets to investigate the effectiveness of uncertainty estimation in characterizing the prediction risks of LLMs.\n\nWhile both the proposal and the paper focus on uncertainty estimation in LLMs, the research problems and approaches are different. The proposal aims to improve LLMs' reasoning capabilities using uncertainty-guided prompting, while the paper focuses on assessing the prediction risks of LLMs using uncertainty estimation techniques.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "2a74fc66beea8bce542581560ca6ec5a0e1bb024",
            "paperId": "2a74fc66beea8bce542581560ca6ec5a0e1bb024",
            "title": "CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation",
            "abstract": "Annotated data plays a critical role in Natural Language Processing (NLP) in training models and evaluating their performance. Given recent developments in Large Language Models (LLMs), models such as ChatGPT demonstrate zero-shot capability on many text-annotation tasks, comparable with or even exceeding human annotators. Such LLMs can serve as alternatives for manual annotation, due to lower costs and higher scalability. However, limited work has leveraged LLMs as complementary annotators, nor explored how annotation work is best allocated among humans and LLMs to achieve both quality and cost objectives. We propose CoAnnotating, a novel paradigm for Human-LLM co-annotation of unstructured texts at scale. Under this framework, we utilize uncertainty to estimate LLMs' annotation capability. Our empirical study shows CoAnnotating to be an effective means to allocate work from results on different datasets, with up to 21% performance improvement over random baseline. For code implementation, see https://github.com/SALT-NLP/CoAnnotating.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes CoAnnotating, a novel paradigm for Human-LLM co-annotation of unstructured texts at scale, and utilizes uncertainty to estimate LLMs' annotation capability."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the reasoning capabilities of large language models (LLMs) by leveraging their inherent uncertainty estimates to guide an iterative prompting process. The approach is to use uncertainty estimation techniques to identify the most uncertain reasoning steps and prompt the model to refine its reasoning based on its confidence at each step.\n\nThe research problem in the paper is allocating annotation work between humans and LLMs to achieve both quality and cost objectives. The approach is to utilize uncertainty to estimate LLMs' annotation capability and allocate work accordingly.\n\nThe proposal focuses on improving LLMs' reasoning capabilities, while the paper focuses on allocating annotation work between humans and LLMs. The proposal uses uncertainty estimates to guide an iterative prompting process, while the paper uses uncertainty estimates to allocate annotation work.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "44d74b0d77b4056ddd4c6611a76711c8bab2e0a7",
            "paperId": "44d74b0d77b4056ddd4c6611a76711c8bab2e0a7",
            "title": "Dehallucinating Large Language Models Using Formal Methods Guided Iterative Prompting",
            "abstract": "Large language models (LLMs) such as ChatGPT have been trained to generate human-like responses to natural language prompts. LLMs use a vast corpus of text data for training, and can generate coherent and contextually relevant responses to a wide range of questions and statements. Despite this remarkable progress, LLMs are prone to hallucinations making their application to safety-critical applications such as autonomous systems difficult. The hallucinations in LLMs refer to instances where the model generates responses that are not factually accurate or contextually appropriate. These hallucinations can occur due to a variety of factors, such as the model\u2019s lack of real-world knowledge, the influence of biased or inaccurate training data, or the model\u2019s tendency to generate responses based on statistical patterns rather than a true understanding of the input. While these hallucinations are a nuisance in tasks such as text summarization and question-answering, they can be catastrophic when LLMs are used in autonomy-relevant applications such as planning. In this paper, we focus on the application of LLMs in autonomous systems and sketch a novel self-monitoring and iterative prompting architecture that uses formal methods to detect these errors in the LLM response automatically. We exploit the dialog capability of LLMs to iteratively steer them to responses that are consistent with our correctness specification. We report preliminary experiments that show the promise of the proposed approach on tasks such as automated planning.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper sketches a novel self-monitoring and iterative prompting architecture that uses formal methods to detect errors in the LLM response automatically and exploits the dialog capability of LLMs to iteratively steer them to responses that are consistent with the correctness specification."
            },
            "score": 6,
            "novelty_score": "The research problem in the project proposal is improving the reasoning capabilities of large language models (LLMs) by using uncertainty-guided iterative prompting. The approach involves estimating the model's uncertainty at each step of the reasoning process and prompting the model to refine its reasoning based on the confidence scores.\n\nThe research problem in the paper is reducing hallucinations in LLMs when applied to autonomous systems. The approach involves using formal methods to detect errors in the LLM response and iteratively prompting the model to generate responses that are consistent with the correctness specification.\n\nWhile both the project proposal and the paper aim to improve the performance of LLMs, they focus on different aspects (reasoning capabilities vs. reducing hallucinations) and use different approaches (uncertainty-guided prompting vs. formal methods-guided prompting). Therefore, the two works are not directly relevant.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "3fc3460c4554a28e489a0ea6ef067b79b7d301d9",
            "paperId": "3fc3460c4554a28e489a0ea6ef067b79b7d301d9",
            "title": "Active Prompting with Chain-of-Thought for Large Language Models",
            "abstract": "The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks requiring reasoning, such as arithmetic and commonsense reasoning. It is known that the effective design of task-specific prompts is critical for LLMs' ability to produce high-quality answers. In particular, an effective approach for complex question-and-answer tasks is example-based prompting with chain-of-thought (CoT) reasoning, which significantly improves the performance of LLMs. However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks. This paper proposes a new method, Active-Prompt, to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning). For this purpose, we propose a solution to the key problem of determining which questions are the most important and helpful ones to annotate from a pool of task-specific queries. By borrowing ideas from the related problem of uncertainty-based active learning, we introduce several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation. Experimental results demonstrate the superiority of our proposed method, achieving state-of-the-art on eight complex reasoning tasks. Further analyses of different uncertainty metrics, pool sizes, zero-shot learning, and accuracy-uncertainty relationship demonstrate the effectiveness of our method. Our code will be available at https://github.com/shizhediao/active-prompt.",
            "year": 2023,
            "citationCount": 58,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new method to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning), and introduces several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the reasoning capabilities of large language models (LLMs) by using uncertainty-guided iterative prompting. The approach involves estimating the model's uncertainty at each step of the reasoning chain and prompting the model to refine its reasoning based on the confidence at each step.\n\nThe research problem in the paper is adapting LLMs to different tasks with task-specific example prompts annotated with human-designed chain-of-thought (CoT) reasoning. The approach involves selecting the most uncertain questions from a pool of task-specific queries for annotation using uncertainty-based active learning metrics.\n\nWhile both the proposal and the paper aim to improve the reasoning capabilities of LLMs, their approaches differ. The proposal focuses on using uncertainty estimates to guide an iterative prompting process, while the paper focuses on selecting the most informative examples for annotation using uncertainty-based active learning.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "161b3e82567b9a9c6911171fa55f05695bf93217",
            "paperId": "161b3e82567b9a9c6911171fa55f05695bf93217",
            "title": "GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems",
            "abstract": "There has been considerable divergence of opinion on the reasoning abilities of Large Language Models (LLMs). While the initial optimism that reasoning might emerge automatically with scale has been tempered thanks to a slew of counterexamples, a wide spread belief in their iterative self-critique capabilities persists. In this paper, we set out to systematically investigate the effectiveness of iterative prompting of LLMs in the context of Graph Coloring, a canonical NP-complete reasoning problem that is related to propositional satisfiability as well as practical problems like scheduling and allocation. We present a principled empirical study of the performance of GPT4 in solving graph coloring instances or verifying the correctness of candidate colorings. In iterative modes, we experiment with the model critiquing its own answers and an external correct reasoner verifying proposed solutions. In both cases, we analyze whether the content of the criticisms actually affects bottom line performance. The study seems to indicate that (i) LLMs are bad at solving graph coloring instances (ii) they are no better at verifying a solution--and thus are not effective in iterative modes with LLMs critiquing LLM-generated solutions (iii) the correctness and content of the criticisms--whether by LLMs or external solvers--seems largely irrelevant to the performance of iterative prompting. We show that the observed increase in effectiveness is largely due to the correct solution being fortuitously present in the top-k completions of the prompt (and being recognized as such by an external verifier). Our results thus call into question claims about the self-critiquing capabilities of state of the art LLMs.",
            "year": 2023,
            "citationCount": 38,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A principled empirical study of the performance of GPT4 in solving graph coloring instances or verifying the correctness of candidate colorings in the context of Graph Coloring, a canonical NP-complete reasoning problem that is related to propositional satisfiability as well as practical problems like scheduling and allocation."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the reasoning capabilities of large language models (LLMs) by using uncertainty-guided iterative prompting. The approach involves estimating the model's uncertainty at each step of the reasoning process and prompting the model to refine its reasoning based on the confidence scores.\n\nThe research problem in the paper is analyzing the effectiveness of iterative prompting for reasoning problems in LLMs, specifically focusing on the graph coloring problem. The approach involves experimenting with the model critiquing its own answers and an external correct reasoner verifying proposed solutions, and analyzing the impact of the content of the criticisms on the model's performance.\n\nWhile both the proposal and the paper focus on iterative prompting for reasoning in LLMs, the specific research problems and approaches differ. The proposal aims to improve reasoning by incorporating uncertainty estimates, while the paper aims to analyze the effectiveness of iterative prompting and the role of criticisms in the context of a specific reasoning problem (graph coloring).\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "444f3b7293b85b7d37600372941a289f9163abd1",
            "paperId": "444f3b7293b85b7d37600372941a289f9163abd1",
            "title": "LM-Polygraph: Uncertainty Estimation for Language Models",
            "abstract": "Recent advancements in the capabilities of large language models (LLMs) have paved the way for a myriad of groundbreaking applications in various fields. However, a significant challenge arises as these models often\"hallucinate\", i.e., fabricate facts without providing users an apparent means to discern the veracity of their statements. Uncertainty estimation (UE) methods are one path to safer, more responsible, and more effective use of LLMs. However, to date, research on UE methods for LLMs has been focused primarily on theoretical rather than engineering contributions. In this work, we tackle this issue by introducing LM-Polygraph, a framework with implementations of a battery of state-of-the-art UE methods for LLMs in text generation tasks, with unified program interfaces in Python. Additionally, it introduces an extendable benchmark for consistent evaluation of UE techniques by researchers, and a demo web application that enriches the standard chat dialog with confidence scores, empowering end-users to discern unreliable responses. LM-Polygraph is compatible with the most recent LLMs, including BLOOMz, LLaMA-2, ChatGPT, and GPT-4, and is designed to support future releases of similarly-styled LMs.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LM-Polygraph is introduced, a framework with implementations of a battery of state-of-the-art UE methods for LLMs in text generation tasks, with unified program interfaces in Python, and introduces an extendable benchmark for consistent evaluation of UE techniques by researchers."
            },
            "score": 6
        },
        {
            "id": "507465f8d46489a68a527cb5304d76bdb6c31ed9",
            "paperId": "507465f8d46489a68a527cb5304d76bdb6c31ed9",
            "title": "Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation",
            "abstract": "We introduce a method to measure uncertainty in large language models. For tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models. We show that measuring uncertainty in natural language is challenging because of\"semantic equivalence\"-- different sentences can mean the same thing. To overcome these challenges we introduce semantic entropy -- an entropy which incorporates linguistic invariances created by shared meanings. Our method is unsupervised, uses only a single model, and requires no modifications to off-the-shelf language models. In comprehensive ablation studies we show that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines.",
            "year": 2023,
            "citationCount": 85,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "In comprehensive ablation studies, it is shown that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines."
            },
            "score": 6
        },
        {
            "id": "875d71bae61a66f7e65a2b6d363b7a0a27a6ed25",
            "paperId": "875d71bae61a66f7e65a2b6d363b7a0a27a6ed25",
            "title": "Tree of Uncertain Thoughts Reasoning for Large Language Models",
            "abstract": "While the recently introduced Tree of Thoughts (ToT) has heralded advancements in allowing Large Language Models (LLMs) to reason through foresight and backtracking for global decision-making, it has overlooked the inherent local uncertainties in intermediate decision points or\"thoughts\". These local uncertainties, intrinsic to LLMs given their potential for diverse responses, remain a significant concern in the reasoning process. Addressing this pivotal gap, we introduce the Tree of Uncertain Thoughts (TouT) - a reasoning framework tailored for LLMs. Our TouT effectively leverages Monte Carlo Dropout to quantify uncertainty scores associated with LLMs' diverse local responses at these intermediate steps. By marrying this local uncertainty quantification with global search algorithms, TouT enhances the model's precision in response generation. We substantiate our approach with rigorous experiments on two demanding planning tasks: Game of 24 and Mini Crosswords. The empirical evidence underscores TouT's superiority over both ToT and chain-of-thought prompting methods.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Tree of Uncertain Thoughts (TouT) is introduced - a reasoning framework tailored for LLMs that effectively leverages Monte Carlo Dropout to quantify uncertainty scores associated with LLMs' diverse local responses at these intermediate steps and enhances the model's precision in response generation."
            },
            "score": 6
        },
        {
            "id": "ad402080a4aa66ef3c57a46ce4685a47a3cc0a61",
            "paperId": "ad402080a4aa66ef3c57a46ce4685a47a3cc0a61",
            "title": "Quantifying Uncertainty in Natural Language Explanations of Large Language Models",
            "abstract": "Large Language Models (LLMs) are increasingly used as powerful tools for several high-stakes natural language processing (NLP) applications. Recent prompting works claim to elicit intermediate reasoning steps and key tokens that serve as proxy explanations for LLM predictions. However, there is no certainty whether these explanations are reliable and reflect the LLMs behavior. In this work, we make one of the first attempts at quantifying the uncertainty in explanations of LLMs. To this end, we propose two novel metrics -- $\\textit{Verbalized Uncertainty}$ and $\\textit{Probing Uncertainty}$ -- to quantify the uncertainty of generated explanations. While verbalized uncertainty involves prompting the LLM to express its confidence in its explanations, probing uncertainty leverages sample and model perturbations as a means to quantify the uncertainty. Our empirical analysis of benchmark datasets reveals that verbalized uncertainty is not a reliable estimate of explanation confidence. Further, we show that the probing uncertainty estimates are correlated with the faithfulness of an explanation, with lower uncertainty corresponding to explanations with higher faithfulness. Our study provides insights into the challenges and opportunities of quantifying uncertainty in LLM explanations, contributing to the broader discussion of the trustworthiness of foundation models.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes two novel metrics -- verbalized uncertainty and probing uncertainty -- to quantify the uncertainty of generated explanations of large Language Models, and shows that the probing uncertainty estimates are correlated with the faithfulness of an explanation, with lower uncertainty corresponding to explanations with higher faithfulness."
            },
            "score": 5
        },
        {
            "id": "72fb75f7c38a83424308c8205bb36cd88995494b",
            "paperId": "72fb75f7c38a83424308c8205bb36cd88995494b",
            "title": "Leveraging Large Language Models for Exploiting ASR Uncertainty",
            "abstract": "While large language models excel in a variety of natural language processing (NLP) tasks, to perform well on spoken language understanding (SLU) tasks, they must either rely on off-the-shelf automatic speech recognition (ASR) systems for transcription, or be equipped with an in-built speech modality. This work focuses on the former scenario, where LLM's accuracy on SLU tasks is constrained by the accuracy of a fixed ASR system on the spoken input. Specifically, we tackle speech-intent classification task, where a high word-error-rate can limit the LLM's ability to understand the spoken intent. Instead of chasing a high accuracy by designing complex or specialized architectures regardless of deployment costs, we seek to answer how far we can go without substantially changing the underlying ASR and LLM, which can potentially be shared by multiple unrelated tasks. To this end, we propose prompting the LLM with an n-best list of ASR hypotheses instead of only the error-prone 1-best hypothesis. We explore prompt-engineering to explain the concept of n-best lists to the LLM; followed by the finetuning of Low-Rank Adapters on the downstream tasks. Our approach using n-best lists proves to be effective on a device-directed speech detection task as well as on a keyword spotting task, where systems using n-best list prompts outperform those using 1-best ASR hypothesis; thus paving the way for an efficient method to exploit ASR uncertainty via LLMs for speech-based applications.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work tackles speech-intent classification task, where a high word-error-rate can limit the LLM's ability to understand the spoken intent, and proposes prompting theLLM with an n-best list of ASR hypotheses instead of only the error-prone 1-best hypothesis."
            },
            "score": 5
        },
        {
            "id": "f197bf0fc2f228483f6af3285000d54d8d97f9eb",
            "paperId": "f197bf0fc2f228483f6af3285000d54d8d97f9eb",
            "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models",
            "abstract": "We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",
            "year": 2023,
            "citationCount": 336,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 5
        },
        {
            "id": "06e5828341aa3926e1d839039363b0673b9461cc",
            "paperId": "06e5828341aa3926e1d839039363b0673b9461cc",
            "title": "Errors are Useful Prompts: Instruction Guided Task Programming with Verifier-Assisted Iterative Prompting",
            "abstract": "Generating low-level robot task plans from high-level natural language instructions remains a challenging problem. Although large language models have shown promising results in generating plans, the accuracy of the output remains unverified. Furthermore, the lack of domain-specific language data poses a limitation on the applicability of these models. In this paper, we propose CLAIRIFY, a novel approach that combines automatic iterative prompting with program verification to ensure programs written in data-scarce domain-specific language are syntactically valid and incorporate environment constraints. Our approach provides effective guidance to the language model on generating structured-like task plans by incorporating any errors as feedback, while the verifier ensures the syntactic accuracy of the generated plans. We demonstrate the effectiveness of CLAIRIFY in planning chemistry experiments by achieving state-of-the-art results. We also show that the generated plans can be executed on a real robot by integrating them with a task and motion planner.",
            "year": 2023,
            "citationCount": 30,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes CLAIRIFY, a novel approach that combines automatic iterative prompting with program verification to ensure programs written in data-scarce domain-specific language are syntactically valid and incorporate environment constraints."
            },
            "score": 5
        },
        {
            "id": "13c85adfa950651ffcd91ef3018fa30801b74472",
            "paperId": "13c85adfa950651ffcd91ef3018fa30801b74472",
            "title": "Prompting and Evaluating Large Language Models for Proactive Dialogues: Clarification, Target-guided, and Non-collaboration",
            "abstract": "Conversational systems based on Large Language Models (LLMs), such as ChatGPT, show exceptional proficiency in context understanding and response generation. However, despite their impressive capabilities, they still possess limitations, such as providing randomly-guessed answers to ambiguous queries or failing to refuse users' requests, both of which are considered aspects of a conversational agent's proactivity. This raises the question of whether LLM-based conversational systems are equipped to handle proactive dialogue problems. In this work, we conduct a comprehensive analysis of LLM-based conversational systems, specifically focusing on three aspects of proactive dialogue systems: clarification, target-guided, and non-collaborative dialogues. To trigger the proactivity of LLMs, we propose the Proactive Chain-of-Thought prompting scheme, which augments LLMs with the goal planning capability over descriptive reasoning chains. Empirical findings are discussed to promote future studies on LLM-based proactive dialogue systems.",
            "year": 2023,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A comprehensive analysis of LLM-based conversational systems, specifically focusing on three aspects of proactive dialogue systems: clarification, target-guided, and non-collaborative dialogues, and the Proactive Chain-of-Thought prompting scheme is proposed."
            },
            "score": 4
        },
        {
            "id": "9a75e23639bfcc3a51da57a3b682a984d1d8ac0b",
            "paperId": "9a75e23639bfcc3a51da57a3b682a984d1d8ac0b",
            "title": "Language Models can Solve Computer Tasks",
            "abstract": "Agents capable of carrying out general tasks on a computer can improve efficiency and productivity by automating repetitive tasks and assisting in complex problem-solving. Ideally, such agents should be able to solve new computer tasks presented to them through natural language commands. However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks. In this work, we show that a pre-trained large language model (LLM) agent can execute computer tasks guided by natural language using a simple prompting scheme where the agent Recursively Criticizes and Improves its output (RCI). The RCI approach significantly outperforms existing LLM methods for automating computer tasks and surpasses supervised learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++ benchmark. We compare multiple LLMs and find that RCI with the InstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful of demonstrations per task rather than tens of thousands, and without a task-specific reward function. Furthermore, we demonstrate RCI prompting's effectiveness in enhancing LLMs' reasoning abilities on a suite of natural language reasoning tasks, outperforming chain of thought (CoT) prompting with external feedback. We find that RCI combined with CoT performs better than either separately. Our code can be found here: https://github.com/posgnu/rci-agent.",
            "year": 2023,
            "citationCount": 159,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work shows that a pre-trained large language model (LLM) agent can execute computer tasks guided by natural language using a simple prompting scheme where the agent Recursively Criticizes and Improves its output (RCI), and demonstrates RCI prompting's effectiveness in enhancing LLMs' reasoning abilities on a suite of natural language reasoning tasks."
            },
            "score": 4
        },
        {
            "id": "215de09ac6e5de81187c85065b5ace8bc01f2862",
            "paperId": "215de09ac6e5de81187c85065b5ace8bc01f2862",
            "title": "Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models",
            "abstract": "The complementary potential of Large Language Models (LLM) assumes off-the-shelf LLMs have heterogeneous expertise in a wide range of domains and tasks so that an ensemble of LLMs can achieve consistently better performance. Existing ensemble methods for LLMs mainly focus on reward model ranking of outputs, leading to significant computation overhead. To combat this issue, we revisit the complementary potential of LLMs and further elaborate it by mining latent expertise with off-the-shelf reward models. We propose Zooter, a reward-guided routing method distilling rewards on training queries to train a routing function, which can precisely distribute each query to the LLM with expertise about it. We also integrate a tag-based label enhancement to mitigate noise from uncertainty when using rewards as silver supervision. Zooter shows computation efficiency in inference as it introduces only a minor computation overhead of a routing function compared with reward model ranking methods. We evaluate Zooter on a comprehensive benchmark collection with 26 subsets on different domains and tasks. Zooter outperforms the best single model on average and ranks first on 44% of tasks, even surpassing multiple reward model ranking methods.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Zooter is proposed, a reward-guided routing method distilling rewards on training queries to train a routing function, which can precisely distribute each query to the LLM with expertise about it, and shows computation efficiency in inference."
            },
            "score": 4
        },
        {
            "id": "261549439aebdda72b648ecc462448fd24857ac1",
            "paperId": "261549439aebdda72b648ecc462448fd24857ac1",
            "title": "Progressive-Hint Prompting Improves Reasoning in Large Language Models",
            "abstract": "The performance of Large Language Models (LLMs) in reasoning tasks depends heavily on prompt design, with Chain-of-Thought (CoT) and self-consistency being critical methods that enhance this ability. However, these methods do not fully exploit the answers generated by the LLM to guide subsequent responses. This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers. PHP is orthogonal to CoT and self-consistency, making it easy to combine with state-of-the-art techniques to further improve performance. We conducted extensive and comprehensive experiments on seven benchmarks. The results show that PHP significantly improves accuracy while remaining highly efficient. For instance, with text-davinci-003, we observed a 4.2% improvement on GSM8K with greedy decoding compared to Complex CoT, and a 46.17% reduction in sample paths with self-consistency. With GPT-4 and PHP, we achieve state-of-the-art performances on SVAMP (89.1% ->91.9%), GSM8K (92% ->95.5%), AQuA (76.4% ->79.9%) and MATH (50.3% ->53.9%).",
            "year": 2023,
            "citationCount": 64,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a new prompting method, named Progressive-Hint Prompting (PHP), that enables automatic multiple interactions between users and LLMs by using previously generated answers as hints to progressively guide toward the correct answers."
            },
            "score": 4
        },
        {
            "id": "69619a2a47faee7a29ec596db13172e2a42ff921",
            "paperId": "69619a2a47faee7a29ec596db13172e2a42ff921",
            "title": "Synthetic Prompting: Generating Chain-of-Thought Demonstrations for Large Language Models",
            "abstract": "Large language models can perform various reasoning tasks by using chain-of-thought prompting, which guides them to find answers through step-by-step demonstrations. However, the quality of the prompts depends on the demonstrations given to the models, and creating many of them by hand is costly. We introduce Synthetic prompting, a method that leverages a few handcrafted examples to prompt the model to generate more examples by itself, and selects effective demonstrations to elicit better reasoning. Our method alternates between a backward and forward process to generate new examples. The backward process generates a question that match a sampled reasoning chain, so that the question is solvable and clear. The forward process produces a more detailed reasoning chain for the question, improving the quality of the example. We evaluate our method on numerical, symbolic, and algorithmic reasoning tasks, and show that it outperforms existing prompting techniques.",
            "year": 2023,
            "citationCount": 41,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Synthetic prompting is introduced, a method that leverages a few handcrafted examples to prompt the model to generate more examples by itself, and selects effective demonstrations to elicit better reasoning."
            },
            "score": 4
        },
        {
            "id": "56e7bda25b83228f91962d3465fd587cfe8908e1",
            "paperId": "56e7bda25b83228f91962d3465fd587cfe8908e1",
            "title": "How Far Can We Extract Diverse Perspectives from Large Language Models? Criteria-Based Diversity Prompting!",
            "abstract": "Collecting diverse human opinions is costly and challenging. This leads to a recent trend in collaborative efforts between humans and Large Language Models (LLMs) for generating diverse data, offering potential scalable and efficient solutions. However, the extent of LLMs' capability to generate diverse perspectives on subjective topics remains an unexplored question. In this study, we investigate LLMs' capacity for generating diverse perspectives and rationales on subjective topics, such as social norms and argumentative texts. We formulate a new problem of maximum diversity extraction from LLMs. Motivated by how humans develop their opinions through their values, we propose a criteria-based prompting technique to ground diverse opinions. To see how far we can extract diverse perspectives from LLMs, or called diversity coverage, we employ a step-by-step recall prompting for generating more outputs from the model in an iterative manner. As we apply our methods to various tasks, indeed we find that LLMs can generate diverse opinions according to the degree of task subjectivity",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study investigates LLMs' capacity for generating diverse perspectives and rationales on subjective topics, such as social norms and argumentative texts, and proposes a criteria-based prompting technique to ground diverse opinions."
            },
            "score": 4
        },
        {
            "id": "995b2f650f55de6077b87db6dadb01cecd86dbd7",
            "paperId": "995b2f650f55de6077b87db6dadb01cecd86dbd7",
            "title": "Advanced prompting as a catalyst: Empowering large language models in the management of gastrointestinal cancers",
            "abstract": "Large Language Models' (LLMs) performance in healthcare can be significantly impacted by prompt engineering. However, the area of study remains relatively uncharted in gastrointestinal oncology until now. Our research delves into this unexplored territory, investigating the efficacy of varied prompting strategies, including simple prompts, templated prompts, in-context learning (ICL), and multi-round iterative questioning, for optimizing the performance of LLMs within a medical setting. We develop a comprehensive evaluation system to assess the performance of LLMs across multiple dimensions. This robust evaluation system ensures a thorough assessment of the LLMs' capabilities in the field of medicine. Our findings suggest a positive relationship between the comprehensiveness of the prompts and the LLMs' performance. Notably, the multi-round strategy, which is characterized by iterative question-and-answer rounds, consistently yields the best results. ICL, a strategy that capitalizes on interrelated contextual learning, also displays significant promise, surpassing the outcomes achieved with simpler prompts. The research underscores the potential of advanced prompt engineering and iterative learning approaches for boosting the applicability of LLMs in healthcare. We recommend that additional research be conducted to refine these strategies and investigate their potential integration, to truly harness the full potential of LLMs in medical applications.\n",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The research investigates the efficacy of varied prompting strategies, including simple prompts, templated prompts, in-context learning (ICL), and multi-round iterative questioning, for optimizing the performance of LLMs within a medical setting, and develops a comprehensive evaluation system."
            },
            "score": 4
        },
        {
            "id": "0885471c0215b3c0d31c82518066913f7f738128",
            "paperId": "0885471c0215b3c0d31c82518066913f7f738128",
            "title": "Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement",
            "abstract": "The ability to derive underlying principles from a handful of observations and then generalize to novel situations -- known as inductive reasoning -- is central to human intelligence. Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks. In this work, we conduct a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting. Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules. By examining the intermediate rules, we observe that LMs are phenomenal hypothesis proposers (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the proposed set of rules, this hybrid approach achieves strong results across inductive reasoning benchmarks that require inducing causal relations, language-like instructions, and symbolic concepts. However, they also behave as puzzling inductive reasoners, showing notable performance gaps between rule induction (i.e., identifying plausible rules) and rule application (i.e., applying proposed rules to instances), suggesting that LMs are proposing hypotheses without being able to actually apply the rules. Through empirical and human analyses, we further reveal several discrepancies between the inductive reasoning processes of LMs and humans, shedding light on both the potentials and limitations of using LMs in inductive reasoning tasks.",
            "year": 2023,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work conducts a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting."
            },
            "score": 4
        },
        {
            "id": "acbe813244e07f32eb034d6c27547d772a995d1d",
            "paperId": "acbe813244e07f32eb034d6c27547d772a995d1d",
            "title": "Uncertainty Estimation for Language Reward Models",
            "abstract": "Language models can learn a range of capabilities from unsupervised training on text corpora. However, to solve a particular problem (such as text summarization) it is typically necessary to fine-tune them on a task-specific dataset. It is often easier for humans to choose between options than to provide labeled data, and prior work has achieved state-of-the-art performance by training a reward model from such preference comparisons. However, collecting a large preference comparison dataset is still expensive -- and the learned reward models are unreliable out-of-distribution. We seek to address these problems via uncertainty estimation, which can improve sample efficiency and robustness using active learning and risk-averse reinforcement learning (RL). Specifically, we use bootstrap aggregating (bagging) to train an ensemble of reward models differing in the initialization of their final layer. Ensembles have proved successful in prior applications of active learning, but we find that in our setting ensemble active learning does not outperform random sampling. Further experiments show that while the aggregate predictions are well-calibrated, the ensemble's estimated epistemic uncertainty is only weakly correlated with model error. We suspect this is because the ensemble members are fine-tuned from a single model and so are similar to one another. This suggests current pre-training methods will need to be modified to support uncertainty estimation, e.g. by training multiple language models.",
            "year": 2022,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that in this setting ensemble active learning does not outperform random sampling, and current pre-training methods will need to be modified to support uncertainty estimation, e.g. by training multiple language models."
            },
            "score": 4
        },
        {
            "id": "8304244ae39d7db6d4a4b08046c26febf9f9e941",
            "paperId": "8304244ae39d7db6d4a4b08046c26febf9f9e941",
            "title": "ExACT: Language-guided Conceptual Reasoning and Uncertainty Estimation for Event-based Action Recognition and More",
            "abstract": "Event cameras have recently been shown beneficial for practical vision tasks, such as action recognition, thanks to their high temporal resolution, power efficiency, and reduced privacy concerns. However, current research is hindered by 1) the difficulty in processing events because of their prolonged duration and dynamic actions with complex and ambiguous semantics and 2) the redundant action depiction of the event frame representation with fixed stacks. We find language naturally conveys abundant semantic information, rendering it stunningly superior in reducing semantic uncertainty. In light of this, we propose ExACT, a novel approach that, for the first time, tackles event-based action recognition from a cross-modal conceptualizing perspective. Our ExACT brings two technical contributions. Firstly, we propose an adaptive fine-grained event (AFE) representation to adaptively filter out the repeated events for the stationary objects while preserving dynamic ones. This subtly enhances the performance of ExACT without extra computational cost. Then, we propose a conceptual reasoning-based uncertainty estimation module, which simulates the recognition process to enrich the semantic representation. In particular, conceptual reasoning builds the temporal relation based on the action semantics, and uncertainty estimation tackles the semantic uncertainty of actions based on the distributional representation. Experiments show that our ExACT achieves superior recognition accuracy of 94.83%(+2.23%), 90.10%(+37.47%) and 67.24% on PAF, HARDVS and our SeAct datasets respectively.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes ExACT, a novel approach that, for the first time, tackles event-based action recognition from a cross-modal conceptualizing perspective and proposes an adaptive fine-grained event (AFE) representation to adaptively filter out the repeated events for the stationary objects while preserving dynamic ones."
            },
            "score": 4
        },
        {
            "id": "7baaaa623d2c3011a52e2bb515e030825fa6e36c",
            "paperId": "7baaaa623d2c3011a52e2bb515e030825fa6e36c",
            "title": "An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration",
            "abstract": "While Large Language Models (LLMs) demonstrate exceptional performance in a multitude of Natural Language Processing (NLP) tasks, they encounter challenges in practical applications, including issues with hallucinations, inadequate knowledge updating, and limited transparency in the reasoning process. To overcome these limitations, this study innovatively proposes a collaborative training-free reasoning scheme involving tight cooperation between Knowledge Graph (KG) and LLMs. This scheme first involves using LLMs to iteratively explore KG, selectively retrieving a task-relevant knowledge subgraph to support reasoning. The LLMs are then guided to further combine inherent implicit knowledge to reason on the subgraph while explicitly elucidating the reasoning process. Through such a cooperative approach, our scheme achieves more reliable knowledge-based reasoning and facilitates the tracing of the reasoning results. Experimental results show that our scheme significantly progressed across multiple datasets, notably achieving over a 10% improvement on the QALD10 dataset compared to the best baseline and the fine-tuned state-of-the-art (SOTA) work. Building on this success, this study hopes to offer a valuable reference for future research in the fusion of KG and LLMs, thereby enhancing LLMs' proficiency in solving complex issues.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study innovatively proposes a collaborative training-free reasoning scheme involving tight cooperation between Knowledge Graph (KG) and LLMs that achieves more reliable knowledge-based reasoning and facilitates the tracing of the reasoning results."
            },
            "score": 4
        },
        {
            "id": "3436ff7a1dd4c6547ba78968d3eec2545a6dccb9",
            "paperId": "3436ff7a1dd4c6547ba78968d3eec2545a6dccb9",
            "title": "Fairness-guided Few-shot Prompting for Large Language Models",
            "abstract": "Large language models have demonstrated surprising ability to perform in-context learning, i.e., these models can be directly applied to solve numerous downstream tasks by conditioning on a prompt constructed by a few input-output examples. However, prior research has shown that in-context learning can suffer from high instability due to variations in training examples, example order, and prompt formats. Therefore, the construction of an appropriate prompt is essential for improving the performance of in-context learning. In this paper, we revisit this problem from the view of predictive bias. Specifically, we introduce a metric to evaluate the predictive bias of a fixed prompt against labels or a given attributes. Then we empirically show that prompts with higher bias always lead to unsatisfactory predictive quality. Based on this observation, we propose a novel search strategy based on the greedy search to identify the near-optimal prompt for improving the performance of in-context learning. We perform comprehensive experiments with state-of-the-art mainstream models such as GPT-3 on various downstream tasks. Our results indicate that our method can enhance the model's in-context learning performance in an effective and interpretable manner.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a metric to evaluate the predictive bias of a fixed prompt against labels or a given attributes and proposes a novel search strategy based on the greedy search to identify the near-optimal prompt for improving the performance of in-context learning."
            },
            "score": 3
        },
        {
            "id": "fccf8776d7525627c518a56a1f4db367a4d7120b",
            "paperId": "fccf8776d7525627c518a56a1f4db367a4d7120b",
            "title": "Choice Over Control: How Users Write with Large Language Models using Diegetic and Non-Diegetic Prompting",
            "abstract": "We propose a conceptual perspective on prompts for Large Language Models (LLMs) that distinguishes between (1) diegetic prompts (part of the narrative, e.g. \u201cOnce upon a time, I saw a fox...\u201d), and (2) non-diegetic prompts (external, e.g. \u201cWrite about the adventures of the fox.\u201d). With this lens, we study how 129 crowd workers on Prolific write short texts with different user interfaces (1 vs 3 suggestions, with/out non-diegetic prompts; implemented with GPT-3): When the interface offered multiple suggestions and provided an option for non-diegetic prompting, participants preferred choosing from multiple suggestions over controlling them via non-diegetic prompts. When participants provided non-diegetic prompts it was to ask for inspiration, topics or facts. Single suggestions in particular were guided both with diegetic and non-diegetic information. This work informs human-AI interaction with generative models by revealing that (1) writing non-diegetic prompts requires effort, (2) people combine diegetic and non-diegetic prompting, and (3) they use their draft (i.e. diegetic information) and suggestion timing to strategically guide LLMs.",
            "year": 2023,
            "citationCount": 30,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work informs human-AI interaction with generative models by revealing that (1) writing non-diegetic prompts requires effort, (2) people combine diegetic and non- diegetic prompting, and (3) they use their draft (i.e. diegetic information) and suggestion timing to strategically guide LLMs."
            },
            "score": 3
        },
        {
            "id": "2f7361894305c1959549311d4475e94a124f4a1f",
            "paperId": "2f7361894305c1959549311d4475e94a124f4a1f",
            "title": "Role Prompting Guided Domain Adaptation with General Capability Preserve for Large Language Models",
            "abstract": "The growing interest in Large Language Models (LLMs) for specialized applications has revealed a significant challenge: when tailored to specific domains, LLMs tend to experience catastrophic forgetting, compromising their general capabilities and leading to a suboptimal user experience. Additionally, crafting a versatile model for multiple domains simultaneously often results in a decline in overall performance due to confusion between domains. In response to these issues, we present the RolE Prompting Guided Multi-Domain Adaptation (REGA) strategy. This novel approach effectively manages multi-domain LLM adaptation through three key components: 1) Self-Distillation constructs and replays general-domain exemplars to alleviate catastrophic forgetting. 2) Role Prompting assigns a central prompt to the general domain and a unique role prompt to each specific domain to minimize inter-domain confusion during training. 3) Role Integration reuses and integrates a small portion of domain-specific data to the general-domain data, which are trained under the guidance of the central prompt. The central prompt is used for a streamlined inference process, removing the necessity to switch prompts for different domains. Empirical results demonstrate that REGA effectively alleviates catastrophic forgetting and inter-domain confusion. This leads to improved domain-specific performance compared to standard fine-tuned models, while still preserving robust general capabilities.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Empirical results demonstrate that REGA effectively alleviates catastrophic forgetting and inter-domain confusion, and leads to improved domain-specific performance compared to standard fine-tuned models, while still preserving robust general capabilities."
            },
            "score": 3
        },
        {
            "id": "a8ee189a6b86efeba4d0565dca10b9fc8be86835",
            "paperId": "a8ee189a6b86efeba4d0565dca10b9fc8be86835",
            "title": "Graph-Guided Reasoning for Multi-Hop Question Answering in Large Language Models",
            "abstract": "Chain-of-Thought (CoT) prompting has boosted the multi-step reasoning capabilities of Large Language Models (LLMs) by generating a series of rationales before the final answer. We analyze the reasoning paths generated by CoT and find two issues in multi-step reasoning: (i) Generating rationales irrelevant to the question, (ii) Unable to compose subquestions or queries for generating/retrieving all the relevant information. To address them, we propose a graph-guided CoT prompting method, which guides the LLMs to reach the correct answer with graph representation/verification steps. Specifically, we first leverage LLMs to construct a\"question/rationale graph\"by using knowledge extraction prompting given the initial question and the rationales generated in the previous steps. Then, the graph verification step diagnoses the current rationale triplet by comparing it with the existing question/rationale graph to filter out irrelevant rationales and generate follow-up questions to obtain relevant information. Additionally, we generate CoT paths that exclude the extracted graph information to represent the context information missed from the graph extraction. Our graph-guided reasoning method shows superior performance compared to previous CoT prompting and the variants on multi-hop question answering benchmark datasets.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a graph-guided CoT prompting method, which guides the LLMs to reach the correct answer with graph representation/verification steps and shows superior performance compared to previous coT prompting and the variants on multi-hop question answering benchmark datasets."
            },
            "score": 3
        },
        {
            "id": "a1c297c2b0d54817670532edfa937444e595249f",
            "paperId": "a1c297c2b0d54817670532edfa937444e595249f",
            "title": "OntoType: Ontology-Guided Zero-Shot Fine-Grained Entity Typing with Weak Supervision from Pre-Trained Language Models",
            "abstract": "Fine-grained entity typing (FET), which assigns entities in text with context-sensitive, fine-grained semantic types, will play an important role in natural language understanding. A supervised FET method, which typically relies on human-annotated corpora for training, is costly and difficult to scale. Recent studies leverage pre-trained language models (PLMs) to generate rich and context-aware weak supervision for FET. However, a PLM may still generate a mixture of rough and fine-grained types, or tokens unsuitable for typing. In this study, we vision that an ontology provides a semantics-rich, hierarchical structure, which will help select the best results generated by multiple PLM models and head words. Specifically, we propose a novel zero-shot, ontology-guided FET method, OntoType, which follows a type ontological structure, from coarse to fine, ensembles multiple PLM prompting results to generate a set of type candidates, and refines its type resolution, under the local context with a natural language inference model. Our experiments on the Ontonotes, FIGER, and NYT datasets using their associated ontological structures demonstrate that our method outperforms the state-of-the-art zero-shot fine-grained entity typing methods. Our error analysis shows that refinement of the existing ontology structures will further improve fine-grained entity typing.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel zero-shot, ontology-guided FET method, OntoType, which follows a type ontological structure, from coarse to fine, ensembles multiple PLM prompting results to generate a set of type candidates, and refines its type resolution, under the local context with a natural language inference model."
            },
            "score": 3
        },
        {
            "id": "82817b85081ee075975f4a69039314b7741a979f",
            "paperId": "82817b85081ee075975f4a69039314b7741a979f",
            "title": "Large Language Models can be Guided to Evade AI-Generated Text Detection",
            "abstract": "Large language models (LLMs) have shown remarkable performance in various tasks and have been extensively utilized by the public. However, the increasing concerns regarding the misuse of LLMs, such as plagiarism and spamming, have led to the development of multiple detectors, including fine-tuned classifiers and statistical methods. In this study, we equip LLMs with prompts, rather than relying on an external paraphraser, to evaluate the vulnerability of these detectors. We propose a novel Substitution-based In-Context example Optimization method (SICO) to automatically construct prompts for evading the detectors. SICO is cost-efficient as it requires only 40 human-written examples and a limited number of LLM inferences to generate a prompt. Moreover, once a task-specific prompt has been constructed, it can be universally used against a wide range of detectors. Extensive experiments across three real-world tasks demonstrate that SICO significantly outperforms the paraphraser baselines and enables GPT-3.5 to successfully evade six detectors, decreasing their AUC by 0.5 on average. Furthermore, a comprehensive human evaluation as well as a validation experiment in the wild show that the SICO-generated text achieves human-level readability and task completion rates. Finally, the strong performance of SICO exhibits its potential as a reliable evaluation tool for future detectors. The codes and data are located on https://github.com/ColinLu50/Evade-GPT-Detector.",
            "year": 2023,
            "citationCount": 21,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel Substitution-based In-Context example Optimization method (SICO) is proposed to automatically construct prompts for evading the detectors of large language models, which significantly outperforms the paraphraser baselines and enables GPT-3.5 to successfully evade six detectors."
            },
            "score": 3
        },
        {
            "id": "afee8cdc51e95b50d7574ed1700a797874bf792c",
            "paperId": "afee8cdc51e95b50d7574ed1700a797874bf792c",
            "title": "Adversarial Fine-Tuning of Language Models: An Iterative Optimisation Approach for the Generation and Detection of Problematic Content",
            "abstract": "In this paper, we tackle the emerging challenge of unintended harmful content generation in Large Language Models (LLMs) with a novel dual-stage optimisation technique using adversarial fine-tuning. Our two-pronged approach employs an adversarial model, fine-tuned to generate potentially harmful prompts, and a judge model, iteratively optimised to discern these prompts. In this adversarial cycle, the two models seek to outperform each other in the prompting phase, generating a dataset of rich examples which are then used for fine-tuning. This iterative application of prompting and fine-tuning allows continuous refinement and improved performance. The performance of our approach is evaluated through classification accuracy on a dataset consisting of problematic prompts not detected by GPT-4, as well as a selection of contentious but unproblematic prompts. We show considerable increase in classification accuracy of the judge model on this challenging dataset as it undergoes the optimisation process. Furthermore, we show that a rudimentary model \\texttt{ada} can achieve 13\\% higher accuracy on the hold-out test set than GPT-4 after only a few rounds of this process, and that this fine-tuning improves performance in parallel tasks such as toxic comment identification.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper shows that a rudimentary model can achieve 13\\% higher accuracy on the hold-out test set than GPT-4 after only a few rounds of this process, and that this fine-tuning improves performance in parallel tasks such as toxic comment identification."
            },
            "score": 3
        },
        {
            "id": "fd80f7f3673fc6ca02f192d5d73426f11a4be659",
            "paperId": "fd80f7f3673fc6ca02f192d5d73426f11a4be659",
            "title": "The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation",
            "abstract": "Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iterative development of MT systems. While considerable progress has been made on estimating a single scalar quality score, current metrics lack the informativeness of more detailed schemes that annotate individual errors, such as Multidimensional Quality Metrics (MQM). In this paper, we help fill this gap by proposing AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AutoMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particularly large gains for larger models) while providing interpretability through error spans that align with human annotations.",
            "year": 2023,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations, and finds that it improves performance compared to just prompting for scores."
            },
            "score": 3
        },
        {
            "id": "4637f79ddfaf923ce569996ffa5b6cda1996faa1",
            "paperId": "4637f79ddfaf923ce569996ffa5b6cda1996faa1",
            "title": "Jailbreaking Black Box Large Language Models in Twenty Queries",
            "abstract": "There is growing interest in ensuring that large language models (LLMs) align with human values. However, the alignment of such models is vulnerable to adversarial jailbreaks, which coax LLMs into overriding their safety guardrails. The identification of these vulnerabilities is therefore instrumental in understanding inherent weaknesses and preventing future misuse. To this end, we propose Prompt Automatic Iterative Refinement (PAIR), an algorithm that generates semantic jailbreaks with only black-box access to an LLM. PAIR -- which is inspired by social engineering attacks -- uses an attacker LLM to automatically generate jailbreaks for a separate targeted LLM without human intervention. In this way, the attacker LLM iteratively queries the target LLM to update and refine a candidate jailbreak. Empirically, PAIR often requires fewer than twenty queries to produce a jailbreak, which is orders of magnitude more efficient than existing algorithms. PAIR also achieves competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and PaLM-2.",
            "year": 2023,
            "citationCount": 119,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "PAIR is an algorithm that generates semantic jailbreaks with only black-box access to an LLM with competitive jailbreaking success rates and transferability on open and closed-source LLMs, including GPT-3.5/4, Vicuna, and PaLM."
            },
            "score": 3
        },
        {
            "id": "0d394063b5e8026241b2b4c1d18a04da29c34e24",
            "paperId": "0d394063b5e8026241b2b4c1d18a04da29c34e24",
            "title": "Bayesian Learning",
            "abstract": null,
            "year": 2019,
            "citationCount": 73,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Bayesian models may have the potential to explain some of the most complex aspects of human cognition, such as language acquisition or reasoning under uncertainty, where structured information and incomplete knowledge combine in a way that has defied previous approaches."
            },
            "score": 3
        },
        {
            "id": "2f3cad5a58f058fb5edc87192c744bb9ce801c25",
            "paperId": "2f3cad5a58f058fb5edc87192c744bb9ce801c25",
            "title": "On Continuous Distributions and Parameter Estimation in Probabilistic Logic Programs (Over continue verdelingen en het schatten van parameters in probabilistische logische programma's)",
            "abstract": "In the last decade remarkable progress has been made on combining statistical machine learning techniques, reasoning under uncertainty, and relational representations. The branch of Artificial Intelligence working on the synthesis of these three areas is known as statistical relational learning or probabilistic logic learning. ProbLog, one of the probabilistic frameworks developed, is an extension of the logic programming language Prolog with independent random variables that are defined by annotating logical facts with probabilities. The separation of the logical and probabilistic part of the model is based on the distribution semantics. Driven by the demand for models that are able to handle continuous values and can be automatically optimized on training data, this thesis introduces several algorithms and extensions to the ProbLog language. Continuous-valued data arise naturally in robotics, human activity recognition and bio-medical applications. Moreover, the models used are complex and the available data is often noisy and incomplete. Hence tuning a model towards the specifics of the environment can hardly be done manually. This poses two crucial challenges for probabilistic programming languages such as ProbLog: processing continuous values and being able to learn from training data. This thesis makes four main contributions to the field of probabilistic logic learning. Hybrid ProbLog is an extension for ProbLog with continuous facts that allows for exact inference. Distributional Programs combine elements of ProbLog, Hybrid ProbLog and CP-Logic into a very expressive language for dealing with continuous distributions. A sampling-based inference algorithm is used to answer conditional queries, while the deterministic information in the program guides the sampling process. LFE-ProbLog is able to learn the parameters of a ProbLog program from queries and proofs, while LFI-ProbLog is optimized to learn the parameters from partial interpretations. Together they cover the standard learning settings considered in PLL. All learning approaches have been evaluated in several relational real-world domains.",
            "year": 2011,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This thesis makes four main contributions to the field of probabilistic logic learning: hybrid ProbLog is an extension for ProbLog with continuous facts that allows for exact inference, Distributional Programs combine elements of ProbLog, Hybrid ProbLog and CP-Logic into a very expressive language for dealing with continuous distributions."
            },
            "score": 3
        },
        {
            "id": "e64cb886120ead3a7f198273f8e24bc508d45959",
            "paperId": "e64cb886120ead3a7f198273f8e24bc508d45959",
            "title": "Nonparametric Bayesian Logic",
            "abstract": "The Bayesian Logic (BLOG) language was recently developed for defining first-order probability models over worlds with unknown numbers of objects. It handles important problems in AI, including data association and population estimation. This paper extends BLOG by adopting generative processes over function spaces \u2014 known as nonparametrics in the Bayesian literature. We introduce syntax for reasoning about arbitrary collections of objects, and their properties, in an intuitive manner. By exploiting exchangeability, distributions over unknown objects and their attributes are cast as Dirichlet processes, which resolve difficulties in model selection and inference caused by varying numbers of objects. We demonstrate these concepts with application to citation matching.",
            "year": 2005,
            "citationCount": 39,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 3
        },
        {
            "id": "33d944de189d6edf3a510ea195803a381c5a3bab",
            "paperId": "33d944de189d6edf3a510ea195803a381c5a3bab",
            "title": "Knowledge Crosswords: Geometric Reasoning over Structured Knowledge with Large Language Models",
            "abstract": "Large language models (LLMs) are widely adopted in knowledge-intensive tasks and have achieved impressive performance thanks to their knowledge abilities. While LLMs have demonstrated outstanding performance on atomic or linear (multi-hop) QA tasks, whether they can reason in knowledge-rich scenarios with interweaving constraints remains an underexplored problem. In this work, we propose geometric reasoning over structured knowledge, where pieces of knowledge are connected in a graph structure and models need to fill in the missing information. Such geometric knowledge reasoning would require the ability to handle structured knowledge, reason with uncertainty, verify facts, and backtrack when an error occurs. We propose Knowledge Crosswords, a multi-blank QA dataset where each problem consists of a natural language question representing the geometric constraints of an incomplete entity network, where LLMs are tasked with working out the missing entities while meeting all factual constraints. Knowledge Crosswords contains 2,101 individual problems, covering various knowledge domains and further divided into three difficulty levels. We conduct extensive experiments to evaluate existing LLM prompting approaches on the Knowledge Crosswords benchmark. We additionally propose two new approaches, Staged Prompting and Verify-All, to augment LLMs' ability to backtrack and verify structured constraints. Our results demonstrate that while baseline approaches perform well on easier problems but struggle with hard ones, our proposed Verify-All outperforms other methods by a large margin and is more robust with hard problems. Further analysis reveals that LLMs' ability of geometric reasoning over structured knowledge is still far from robust or perfect, susceptible to confounders such as the order of options, certain structural patterns, assumption of existence of correct answer, and more.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes Knowledge Crosswords, a multi-blank QA dataset where each problem consists of a natural language question representing the geometric constraints of an incomplete entity network, where LLMs are tasked with working out the missing entities while meeting all factual constraints."
            },
            "score": 3
        },
        {
            "id": "93c525267e93c78309a5b28a3eb0780704125744",
            "paperId": "93c525267e93c78309a5b28a3eb0780704125744",
            "title": "Analyzing and Mitigating Object Hallucination in Large Vision-Language Models",
            "abstract": "Large vision-language models (LVLMs) have shown remarkable abilities in understanding visual information with human languages. However, LVLMs still suffer from object hallucination, which is the problem of generating descriptions that include objects that do not actually exist in the images. This can negatively impact many vision-language tasks, such as visual summarization and reasoning. To address this issue, we propose a simple yet powerful algorithm, LVLM Hallucination Revisor (LURE), to post-hoc rectify object hallucination in LVLMs by reconstructing less hallucinatory descriptions. LURE is grounded in a rigorous statistical analysis of the key factors underlying object hallucination, including co-occurrence (the frequent appearance of certain objects alongside others in images), uncertainty (objects with higher uncertainty during LVLM decoding), and object position (hallucination often appears in the later part of the generated text). LURE can also be seamlessly integrated with any LVLMs. We evaluate LURE on six open-source LVLMs, achieving a 23% improvement in general object hallucination evaluation metrics over the previous best approach. In both GPT and human evaluations, LURE consistently ranks at the top. Our data and code are available at https://github.com/YiyangZhou/LURE.",
            "year": 2023,
            "citationCount": 35,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a simple yet powerful algorithm, LVLM Hallucination Revisor (LURE), to post-hoc rectify object hallucination in LVLMs by reconstructing less hallucinatory descriptions and consistently ranks at the top in both GPT and human evaluations."
            },
            "score": 3
        },
        {
            "id": "dd1d5906ccf9725bde74eb0de1fb4b5e01a5c0cc",
            "paperId": "dd1d5906ccf9725bde74eb0de1fb4b5e01a5c0cc",
            "title": "Concept-Guided Chain-of-Thought Prompting for Pairwise Comparison Scaling of Texts with Large Language Models",
            "abstract": "Existing text scaling methods often require a large corpus, struggle with short texts, or require labeled data. We develop a text scaling method that leverages the pattern recognition capabilities of generative large language models (LLMs). Specifically, we propose concept-guided chain-of-thought (CGCoT), which uses prompts designed to summarize ideas and identify target parties in texts to generate concept-specific breakdowns, in many ways similar to guidance for human coder content analysis. CGCoT effectively shifts pairwise text comparisons from a reasoning problem to a pattern recognition problem. We then pairwise compare concept-specific breakdowns using an LLM. We use the results of these pairwise comparisons to estimate a scale using the Bradley-Terry model. We use this approach to scale affective speech on Twitter. Our measures correlate more strongly with human judgments than alternative approaches like Wordfish. Besides a small set of pilot data to develop the CGCoT prompts, our measures require no additional labeled data and produce binary predictions comparable to a RoBERTa-Large model fine-tuned on thousands of human-labeled tweets. We demonstrate how combining substantive knowledge with LLMs can create state-of-the-art measures of abstract concepts.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes concept-guided chain-of-thought (CGCoT), which uses prompts designed to summarize ideas and identify target parties in texts to generate concept-specific breakdowns, in many ways similar to guidance for human coder content analysis."
            },
            "score": 2
        },
        {
            "id": "006aa1580fae5968417538c7acb4662c7b58088f",
            "paperId": "006aa1580fae5968417538c7acb4662c7b58088f",
            "title": "LLM-Rec: Personalized Recommendation via Prompting Large Language Models",
            "abstract": "We investigate various prompting strategies for enhancing personalized content recommendation performance with large language models (LLMs) through input augmentation . Our proposed approach, termed LLM-Rec , encompasses four distinct prompting strategies: (1) basic prompting, (2) recommendation-driven prompting, (3) engagement-guided prompting",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The proposed approach, termed LLM-Rec, encompasses four distinct prompting strategies: (1) basic prompting, (2) recommendation-driven prompting, (3) engagement-guided prompting, and (4) engagement-guided prompting."
            },
            "score": 2
        },
        {
            "id": "5dd301c6b8757244e0046d8cad23399223fe4243",
            "paperId": "5dd301c6b8757244e0046d8cad23399223fe4243",
            "title": "Consistency-guided Prompt Learning for Vision-Language Models",
            "abstract": "We propose Consistency-guided Prompt learning (CoPrompt), a new fine-tuning method for vision-language models. Our approach improves the generalization of large foundation models when fine-tuned on downstream tasks in a few-shot setting. The basic idea of CoPrompt is to enforce a consistency constraint in the prediction of the trainable and pre-trained models to prevent overfitting on the downstream task. Additionally, we introduce the following two components into our consistency constraint to further boost the performance: enforcing consistency on two perturbed inputs and combining two dominant paradigms of tuning, prompting and adapter. Enforcing consistency on perturbed input serves to further regularize the consistency constraint, thereby improving generalization. Moreover, the integration of adapters and prompts not only enhances performance on downstream tasks but also offers increased tuning flexibility in both input and output spaces. This facilitates more effective adaptation to downstream tasks in a few-shot learning setting. Experiments show that CoPrompt outperforms existing methods on a range of evaluation suites, including base-to-novel generalization, domain generalization, and cross-dataset evaluation. On generalization, CoPrompt improves the state-of-the-art on zero-shot tasks and the overall harmonic mean over 11 datasets. Detailed ablation studies show the effectiveness of each of the components in CoPrompt. We make our code available at https://github.com/ShuvenduRoy/CoPrompt.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The basic idea of CoPrompt is to enforce a consistency constraint in the prediction of the trainable and pre-trained models to prevent overfitting on the downstream task, and the following two components are introduced to further boost the performance: enforcing consistency on two perturbed inputs and combining two dominant paradigms of tuning, prompting and adapter."
            },
            "score": 2
        },
        {
            "id": "19cbf834bdaeee992d629565f79876cb7356c039",
            "paperId": "19cbf834bdaeee992d629565f79876cb7356c039",
            "title": "ArGue: Attribute-Guided Prompt Tuning for Vision-Language Models",
            "abstract": "Although soft prompt tuning is effective in efficiently adapting Vision-Language (V&L) models for downstream tasks, it shows limitations in dealing with distribution shifts. We address this issue with Attribute-Guided Prompt Tuning (ArGue), making three key contributions. 1) In contrast to the conventional approach of directly appending soft prompts preceding class names, we align the model with primitive visual attributes generated by Large Language Models (LLMs). We posit that a model's ability to express high confidence in these attributes signifies its capacity to discern the correct class rationales. 2) We introduce attribute sampling to eliminate disadvantageous attributes, thus only semantically meaningful attributes are preserved. 3) We propose negative prompting, explicitly enumerating class-agnostic attributes to activate spurious correlations and encourage the model to generate highly orthogonal probability distributions in relation to these negative features. In experiments, our method significantly outperforms current state-of-the-art prompt tuning methods on both novel class prediction and out-of-distribution generalization tasks.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Attribute-Guided Prompt Tuning (ArGue), making three key contributions, proposes negative prompting, explicitly enumerating class-agnostic attributes to activate spurious correlations and encourage the model to generate highly orthogonal probability distributions in relation to these negative features."
            },
            "score": 2
        },
        {
            "id": "2392b6d3a5cad9e5cf349169eaeee848266adf6a",
            "paperId": "2392b6d3a5cad9e5cf349169eaeee848266adf6a",
            "title": "LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models",
            "abstract": "Large language models (LLMs) have been applied in various applications due to their astonishing capabilities. With advancements in technologies such as chain-of-thought (CoT) prompting and in-context learning (ICL), the prompts fed to LLMs are becoming increasingly lengthy, even exceeding tens of thousands of tokens. To accelerate model inference and reduce cost, this paper presents LLMLingua, a coarse-to-fine prompt compression method that involves a budget controller to maintain semantic integrity under high compression ratios, a token-level iterative compression algorithm to better model the interdependence between compressed contents, and an instruction tuning based method for distribution alignment between language models. We conduct experiments and analysis over four datasets from different scenarios, i.e., GSM8K, BBH, ShareGPT, and Arxiv-March23; showing that the proposed approach yields state-of-the-art performance and allows for up to 20x compression with little performance loss. Our code is available at https://aka.ms/LLMLingua.",
            "year": 2023,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A coarse-to-fine prompt compression method that involves a budget controller to maintain semantic integrity under high compression ratios, a token-level iterative compression algorithm to better model the interdependence between compressed contents, and an instruction tuning based method for distribution alignment between language models."
            },
            "score": 2
        },
        {
            "id": "ee414ff78922ac70dfb31abfff37bd40c661ac92",
            "paperId": "ee414ff78922ac70dfb31abfff37bd40c661ac92",
            "title": "Decomposed Prompting: Unveiling Multilingual Linguistic Structure Knowledge in English-Centric Large Language Models",
            "abstract": "Despite the predominance of English in their training data, English-centric Large Language Models (LLMs) like GPT-3 and LLaMA display a remarkable ability to perform multilingual tasks, raising questions about the depth and nature of their cross-lingual capabilities. This paper introduces the decomposed prompting approach to probe the linguistic structure understanding of these LLMs in sequence labeling tasks. Diverging from the single text-to-text prompt, our method generates for each token of the input sentence an individual prompt which asks for its linguistic label. We assess our method on the Universal Dependencies part-of-speech tagging dataset for 38 languages, utilizing both English-centric and multilingual LLMs. Our findings show that decomposed prompting surpasses the iterative prompting baseline in efficacy and efficiency under zero- and few-shot settings. Further analysis reveals the influence of evaluation methods and the use of instructions in prompts. Our multilingual investigation shows that English-centric language models perform better on average than multilingual models. Our study offers insights into the multilingual transferability of English-centric LLMs, contributing to the understanding of their multilingual linguistic knowledge.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The decomposed prompting approach is introduced to probe the linguistic structure understanding of English-centric Large Language Models in sequence labeling tasks, showing that English-centric language models perform better on average than multilingual models."
            },
            "score": 2
        },
        {
            "id": "ea6b0b5904d6e8eccbccb609ac35911ae967cd2c",
            "paperId": "ea6b0b5904d6e8eccbccb609ac35911ae967cd2c",
            "title": "Creating Suspenseful Stories: Iterative Planning with Large Language Models",
            "abstract": "Automated story generation has been one of the long-standing challenges in NLP. Among all dimensions of stories, *suspense* is very common in human-written stories but relatively under-explored in AI-generated stories. While recent advances in large language models (LLMs) have greatly promoted language generation in general, state-of-the-art LLMs are still unreliable when it comes to suspenseful story generation. We propose a novel iterative-prompting-based planning method that is grounded in two theoretical foundations of story suspense from cognitive psychology and narratology. This theory-grounded method works in a fully zero-shot manner and does not rely on any supervised story corpora. To the best of our knowledge, this paper is the first attempt at suspenseful story generation with LLMs. Extensive human evaluations of the generated suspenseful stories demonstrate the effectiveness of our method.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel iterative-prompting-based planning method that is grounded in two theoretical foundations of story suspense from cognitive psychology and narratology and works in a fully zero-shot manner and does not rely on any supervised story corpora."
            },
            "score": 2
        },
        {
            "id": "8421f2f0d9be59178069d9502c3ca535b1f8931f",
            "paperId": "8421f2f0d9be59178069d9502c3ca535b1f8931f",
            "title": "Mixing Non-Monotonic Logical Reasoning and Probabilistic Planning for Robots",
            "abstract": "This paper describes an architecture that combines the complementary strengths of probabilistic graphical models and declarative programming to represent and reason with qualitative and quantitative descriptions of domain knowledge and uncertainty. An action language is used for the architecture\u2019s low-level (LL) and high-level (HL) system descriptions, and the HL definition of recorded history is expanded to allow prioritized defaults. For any given objective, each action in the plan created in the HL using non-monotonic logical reasoning is executed probabilistically in the LL, refining the HL description to identify the relevant sorts, fluents and actions, and adding the corresponding action outcomes to the HL history. The HL and LL domain representations are translated into an Answer Set Prolog (ASP) program and a partially observable Markov decision process (POMDP) respectively. ASP-based inference provides a multinomial prior for POMDP state estimation, and populates a Beta density of priors for metareasoning and early termination. Robots equipped with this architecture reason with violation of defaults, noisy observations and unreliable actions in complex domains. The architecture is evaluated in simulation and on a mobile robot moving target objects to desired locations in an office domain.",
            "year": 2015,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An architecture that combines the complementary strengths of probabilistic graphical models and declarative programming to represent and reason with qualitative and quantitative descriptions of domain knowledge and uncertainty is described."
            },
            "score": 2
        },
        {
            "id": "c78b409d59caa0b85d34a63a9c1f720df6e9937b",
            "paperId": "c78b409d59caa0b85d34a63a9c1f720df6e9937b",
            "title": "Mixed Logical Inference and Probabilistic Planning for Robots in Unreliable Worlds",
            "abstract": "Deployment of robots in practical domains poses key knowledge representation and reasoning challenges. Robots need to represent and reason with incomplete domain knowledge, acquiring and using sensor inputs based on need and availability. This paper presents an architecture that exploits the complementary strengths of declarative programming and probabilistic graphical models as a step toward addressing these challenges. Answer Set Prolog (ASP), a declarative language, is used to represent, and perform inference with, incomplete domain knowledge, including default information that holds in all but a few exceptional situations. A hierarchy of partially observable Markov decision processes (POMDPs) probabilistically models the uncertainty in sensor input processing and navigation. Nonmonotonic logical inference in ASP is used to generate a multinomial prior for probabilistic state estimation with the hierarchy of POMDPs. It is also used with historical data to construct a beta (meta) density model of priors for metareasoning and early termination of trials when appropriate. Robots equipped with this architecture automatically tailor sensor input processing and navigation to tasks at hand, revising existing knowledge using information extracted from sensor inputs. The architecture is empirically evaluated in simulation and on a mobile robot visually localizing objects in indoor domains.",
            "year": 2015,
            "citationCount": 59,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents an architecture that exploits the complementary strengths of declarative programming and probabilistic graphical models as a step toward addressing the challenges of deployment of robots in practical domains."
            },
            "score": 2
        },
        {
            "id": "6833c4fb86f59b9230ec262fe14315f39e9b6456",
            "paperId": "6833c4fb86f59b9230ec262fe14315f39e9b6456",
            "title": "An intellectual system for supporting decision making in the control of the borring process",
            "abstract": "The problem of development of the method of identification of complications arising in the process of drilling of oil and gas wells, which operates under the conditions of a priori and current uncertainty under the influence of various perturbations based on methods of fuzzy set theory and fuzzy logic, is considered.\nA methodological approach to the estimation of the level of complications in the drilling of oil and gas wells, based on the principles of linguistic parameters of the drilling process, linguistic and hierarchical knowledge about the complications in the drilling of wells is proposed.\nMathematical models of a controlled object have been developed that, unlike deterministic mathematical models, allow to describe in natural language the cause and effect relationships between the parameters of the drilling process and the possible complication. These models reflect the logic of the operator's reasoning with the involvement of non-numerical and fuzzy information from an expert to formalize Fuzzy Logic decision-making procedures using the parameters and indicators of the oil and gas drilling process.\nThe structure of the decision support system for controlling the drilling of wells in the conditions of complications is proposed.\nThe results of simulation modeling of the developed methods of modeling of complications based on the methods of fuzzy set theory and fuzzy logic are presented. Their advantages over the well-known in accuracy of the tasks of identification of an estimation and control in the conditions of uncertainty concerning structure and parameters of object are shown.\nThe real complications have been identified, the elimination of which will increase the level of safety of the drilling process. It is shown that the developed methods and models can find application for modeling and identification of a wide class of complications on drilling rigs operating under the conditions of a priori and current uncertainty regarding their structure, parameters and geographic environment.",
            "year": 2020,
            "citationCount": 0,
            "tldr": null,
            "score": 2
        },
        {
            "id": "b6dd23de05e9e260660ad89ad481fdb23d18e597",
            "paperId": "b6dd23de05e9e260660ad89ad481fdb23d18e597",
            "title": "Gaussian Processes for Active Data Mining of Spatial Aggregates",
            "abstract": "We present an active data mining mechanism for qualitative analysis of spatial datasets, integrating identification and analysis of structures in spatial data with targeted collection of additional samples. The mechanism is designed around the spatial aggregation language (SAL) for qualitative spatial reasoning, and seeks to uncover high-level spatial structures from only a sparse set of samples. This approach is important for applications in domains such as aircraft design, wireless system simulation, fluid dynamics, and sensor networks. The mechanism employs Gaussian processes, a formal mathematical model for reasoning about spatial data, in order to build surrogate models from sparse data, reason about the uncertainty of estimation at unsampled points, and formulate objective criteria for closing-the-loop between data collection and data analysis. It optimizes sample selection using entropy-based functionals defined over spatial aggregates instead of the traditional approach of sampling to minimize estimated variance. We apply this mechanism on a global optimization benchmark comprising a testbank of 2D functions, as well as on data from wireless system simulations. The results reveal that the proposed sampling strategy makes more judicious use of data points by selecting locations that clarify high-level structures in data, rather than choosing points that merely improve quality of function approximation.",
            "year": 2005,
            "citationCount": 73,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An active data mining mechanism for qualitative analysis of spatial datasets, integrating identification and analysis of structures in spatial data with targeted collection of additional samples, and optimizes sample selection using entropy-based functionals defined over spatial aggregates instead of the traditional approach of sampling to minimize estimated variance."
            },
            "score": 2
        },
        {
            "id": "6e046e7c2f90de5cacef55003a9a790c82faaa99",
            "paperId": "6e046e7c2f90de5cacef55003a9a790c82faaa99",
            "title": "Granular Computing: An Emerging Paradigm",
            "abstract": "Interval Arithmetic and Interval Analysis: An Introduction.- Interval and Ellipsoidal Uncertainty Models.- Nonlinear Bounded-Error Parameter Estimation Using Interval Computation.- Random Sets: Theory and Applications.- Rough Sets and Boolean Reasoning..- Granulation and Nearest Neighborhoods: Rough Set Approach..- An Inquiry into the Theory of Defuzzification.- Fuzzy Partitioning Methods.- A Coding Method to Handle Linguistic Variables.- A Formal Theory of Fuzzy Natural Language Quantification and its Role in Granular Computing..- Granularity and Specificity in Fuzzy Rule-Based Systems.- Granular Computing in Neural Networks.- Fuzzy Clustering for Multiple-Model Approaches in System Identification and Control..- Information Granulation in Automated Modeling.- Optical Music Recognition: the Case of Granular Computing.- Modeling MPEG VBR Video Traffic Using Type-2 Fuzzy Logic Systems.- Induction of Rules about Complications with the Use of Rough Sets.",
            "year": 2001,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This book discusses Granular Computing in Neural Networks, Fuzzy Clustering for Multiple-Model Approaches in System Identification and Control, and Induction of Rules about Complications with the Use of Rough Sets."
            },
            "score": 2
        },
        {
            "id": "dd7a2613ee15cd10938e7ad40f3aa052e98fda3c",
            "paperId": "dd7a2613ee15cd10938e7ad40f3aa052e98fda3c",
            "title": "GROVE: A Retrieval-augmented Complex Story Generation Framework with A Forest of Evidence",
            "abstract": "Conditional story generation is significant in human-machine interaction, particularly in producing stories with complex plots. While Large language models (LLMs) perform well on multiple NLP tasks, including story generation, it is challenging to generate stories with both complex and creative plots. Existing methods often rely on detailed prompts to guide LLMs to meet target conditions, which inadvertently restrict the creative potential of the generated stories. We argue that leveraging information from exemplary human-written stories facilitates generating more diverse plotlines. Delving deeper into story details helps build complex and credible plots. In this paper, we propose a retrieval-au\\textbf{G}mented sto\\textbf{R}y generation framework with a f\\textbf{O}rest of e\\textbf{V}id\\textbf{E}nce (GROVE) to enhance stories' complexity. We build a retrieval repository for target conditions to produce few-shot examples to prompt LLMs. Additionally, we design an ``asking-why'' prompting scheme that extracts a forest of evidence, providing compensation for the ambiguities that may occur in the generated story. This iterative process uncovers underlying story backgrounds. Finally, we select the most fitting chains of evidence from the evidence forest and integrate them into the generated story, thereby enhancing the narrative's complexity and credibility. Experimental results and numerous examples verify the effectiveness of our method.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is argued that leveraging information from exemplary human-written stories facilitates generating more diverse plotlines, and a retrieval-au-Gmented sto-R-y generation framework with arest of GROVE to enhance stories' complexity and credibility is proposed."
            },
            "score": 2
        },
        {
            "id": "9e99648c5d4d9ce4fba73007291bbd3f804c83ea",
            "paperId": "9e99648c5d4d9ce4fba73007291bbd3f804c83ea",
            "title": "ImageBrush: Learning Visual In-Context Instructions for Exemplar-Based Image Manipulation",
            "abstract": "While language-guided image manipulation has made remarkable progress, the challenge of how to instruct the manipulation process faithfully reflecting human intentions persists. An accurate and comprehensive description of a manipulation task using natural language is laborious and sometimes even impossible, primarily due to the inherent uncertainty and ambiguity present in linguistic expressions. Is it feasible to accomplish image manipulation without resorting to external cross-modal language information? If this possibility exists, the inherent modality gap would be effortlessly eliminated. In this paper, we propose a novel manipulation methodology, dubbed ImageBrush, that learns visual instructions for more accurate image editing. Our key idea is to employ a pair of transformation images as visual instructions, which not only precisely captures human intention but also facilitates accessibility in real-world scenarios. Capturing visual instructions is particularly challenging because it involves extracting the underlying intentions solely from visual demonstrations and then applying this operation to a new image. To address this challenge, we formulate visual instruction learning as a diffusion-based inpainting problem, where the contextual information is fully exploited through an iterative process of generation. A visual prompting encoder is carefully devised to enhance the model's capacity in uncovering human intent behind the visual instructions. Extensive experiments show that our method generates engaging manipulation results conforming to the transformations entailed in demonstrations. Moreover, our model exhibits robust generalization capabilities on various downstream tasks such as pose transfer, image translation and video inpainting.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel manipulation methodology is proposed, dubbed ImageBrush, that learns visual instructions for more accurate image editing and exhibits robust generalization capabilities on various downstream tasks such as pose transfer, image translation and video inpainting."
            },
            "score": 1
        },
        {
            "id": "851c88708fee59194624d090589b89b4f4d356d7",
            "paperId": "851c88708fee59194624d090589b89b4f4d356d7",
            "title": "Augmented Radiology: Looking Over the Horizon.",
            "abstract": "Sign posts regarding the transition from volume to value are ubiquitous in health care. Current efforts to create value are guided by the triple aim: improve patient outcomes and enhance patient experience while decreasing the cost of care (1). This has prompted a shift toward population health management (PHM), which leverages data-driven technology, effective change management, and workflow optimization to promote sustainable care delivery models (2). Meaningful achievement of these goals represents the greatest challenge of our health care era. Radiology as a profession is no stranger to transformation and reinvention. The field has successfully navigated discrete epochs in its history beginning with Imaging 1.0, a period of discovery from 1920 to 1990 initiated by Wilhelm Roentgen and culminating in a technical revolution with advent of CT and MRI. The widespread picture archiving and communication system implementation in the 1990s ignited a dramatic surge in efficiency and productivity, and the profession flourished as a revenue center in a fee-for-service payment system. However, this era is referred to as the dark era of Imaging 2.0 because of the unintended consequences of increased isolation from our patients, health care teams, and referring providers. In recognition of this void in the evolving practice of radiology, the Imaging 3.0 campaign was launched to foster collaboration between radiologists, patients, referring physicians, and other key stakeholders in the pursuit of appropriate, evidence-based imaging and management (3). Born in a military setting, over-the-horizon radar systems were developed during the Cold War to improve safety by increasing time available for protective action by detecting threats long before traditional radar or line of sight. The rapidly changing health care environment and advent of technologies evoke an analogous need and vision for radiology. It is thus appropriate to speculate about the next era to shape our collective preparation and response. The breakneck pace of advances in data science and the inability of the legacy health care system to meet increasingly complex demands has led many thought leaders in the field to identify artificial intelligence (AI) as this next frontier. In general terms, AI allows machines to process information, recognize patterns, and refine existing models to learn, solve problems, and perform specific tasks. This iterative process relies on machine learning, which leverages self-correcting algorithms to interpret vast amounts of data and make rapid predictions (4). The concept of AI and uncertainty around its impact understandably create angst for some radiologists. After all, interpretation of medical images has recently become a Augmented Radiology: Looking Over the Horizon",
            "year": 2019,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The breakneck pace of advances in data science and the inability of the legacy health care system to meet increasingly complex demands has led many thought leaders in the field to identify artificial intelligence (AI) as this next frontier."
            },
            "score": 1
        },
        {
            "id": "782d328834c3062267966deb137a27361d6c9f43",
            "paperId": "782d328834c3062267966deb137a27361d6c9f43",
            "title": "Signal Analysis of Various Filters for Voltage and Current Level Measurement in Series or Parallel RLC Resonant circuits on Hyper Spectral Data Magnification",
            "abstract": "Measurement is an act or the result of evaluation between the quantity and a predefined standard by means of Voltage/Current through HyperSpectral Analysis. Electrical measurements are the methods, devices and calculations used to measure electrical quantities. Measurement of electrical quantities may be done to measure electrical parameters of a system. Specifying a voltage measurement wants explicit or implicit specification of the points across which the voltage is measured. Whichever two points with the same potential may be connected by a conductor and no current will flow among them. Voltage and Current measurement or monitoring within electronic circuitry is a common requirement covering various types of applications. These may include anything from portable, handheld equipment through to automotive applications. The Current Measurement block is used to measure the instantaneous current flowing in any electrical block or connection line. The Voltage Measurement block measures the prompt voltage between two electric nodes. An electric current is a flow of electric charge. In electric circuits this charge is often carried by moving electrons in a wire. Electric current is measured using Series/Parallel RLC resonant circuits using MATLAB Simulink graphical representation. In the future work, the different level voltage circuits data will be tested with Hyperspectral data processing which is used to find out the accurate voltage magnification difference and analyze through signal processing. Keywords\u2014 Electrical quality measurement, Current, Voltage, RLC resonant circuits, MATLAB/Simulink, Electric charge, graphical representation, Resistance. I.INTRODUCTION A RLC circuit is an electrical circuit [1] containing of a resistor, an inductor, and a capacitor, connected in series or in parallel. The circuit forms a harmonic oscillator for current and will resonate in a similar way as an LC circuit. Introducing the resistor (R) helps decay these oscillations. This effect of the resistor is called damping. The resistor also reduces the peak resonant frequency. A number of resistance is necessary in real circuits even if a resistor is not exactly included as a component. The ideal LC or RLC circuit [2] assumes zero resistance from the connecting conductors. There are several applications for this circuit. They are used in various different types of oscillator circuits. An RLC circuit [3] can be used as a band-pass filter, bandstop filter, low-pass filter or high-pass filter. The tuning application, for instance, is an example of band-pass filtering. The RLC filter is defined as a second-order circuit, meaning that any voltage or current in the circuit can be defined by a second-order differential equation in circuit analysis. An essential property of this circuit is its capability to resonate at a specific frequency, the resonance frequency, f0. Frequencies are measured in units of hertz. In this work, angular frequency, \u03c90, is used which is more mathematically convenient. This is measured in radians per second. They are related to each other by a simple proportion. \u03c90=2\u03c0f0 ........................................ (1) Resonance is well-defined as the condition [4] in a circuit covering at least one inductor and one capacitor, when the supply voltage and the supply current are in phase. Thus, at resonance the equivalent impedance of the circuit is purely resistive. Meanwhile the supply voltage and supply current are in phase, the power factor of a resonant circuit [5] is unity. At resonance the circuit impedance Z and the admittance Y are real quantities. Voltage magnification is reinforcement of an applied AC voltage due to resonance. There must be some capacitance and some inductance in a circuit that produces voltage magnification. The relation of voltage across L or C and the supply voltage is named the voltage magnification. That is, Voltage magnification = voltage across L at resonance/supply voltage at resonance = VL0/V Also, Voltage magnification = voltage across C at resonance/supply voltage at resonance =VC0/V At resonance, V =VR0 ............................. (2) VL0/V = VL0/VR0 = XL0I0/RI0 = XL0/R = \u03c90L/R = Q0 VC0/V = VC0/VR0 = XC0I0/RI0 = XC0/R = XL0/R = \u03c90L/R = Q0 Hence, for a series resonant circuit the factor Q0 is a measure of the voltage magnification. For a series RLC circuit Quality of Service (Q0S) can be establishing in terms of R, L and C as follow: Q0S = \u03c90L/R = 1/\u221aLC \u00d7 L/R \u00d7 1/R \u221aL/C.......... (3) The outcome of voltage magnification in series resonant circuit requirement is considered in the selection of circuit components. Current Magnification is at resonance in a parallel circuit, the branch current may be several times greater than the supply current. Current magnification = IC/I = IL sin \u03a6L/IL cos \u03a6L = tan \u03a6L = \u03c90L/R International Journal of Advanced Scientific Technologies in Engineering and Management Sciences (IJASTEMS-ISSN: 2454-356X) Volume.3,Issue.12,December-.2017 www.ijastems.org Page 9 = Q-factor of the circuit at resonance = Q0.......... (4) Thus, Q-factor [6] is a measure of current magnification in a parallel resonant circuit. Hyperspectral data analysis: Hyperspectral images give ample spectral information to identify and distinguish spectrally similar Circuit data for more accurate and detailed information extraction. Widespread range of advanced classification methods are available based on spectral information and spatial information[7]. To improve classification accuracy it is essential to identify and decrease uncertainties in image processing chain. Due to continuity of bands and narrow sampling bandwidth it becomes possible to put on several mathematical methods, such as derivative analysis and continuum removal [8] which are pre-requisite for spectral feature recovery and characterization. Main challenges in Hyperspectral data processing[9] include noise reduction, recovery of subtle absorption features, spectral matching analysis, classification and creation of library spectra that can guide spectral classification. Hyperspectral data is frequently stated many ways to better describe the mathematical handling of the data; generally as a vector of pixels when referring to the data in a space or a matrix of pixels when referring to data as an image. Hyperspectral data examined like an image will be well-defined as a matrix Mm\u00d7n\u00d7p of dimension m \u00d7 n \u00d7 p where m is defined as the number of rows in the image, n is defined as the number of columns in the image, and p is defined as the number of bands in the image. Hence, a single element of such an image will be accessed using Mi,j,k and a single pixel of an image will be accessed using Mi,j. Hyperspectral data formed as a vector of vectors (i.e. 2D matrix) is defined as M(m\u2022n)\u00d7p of dimension (m\u2022 n)\u00d7p. Single element is read using Mi,j and a single pixel is accessed using M:,j. Notice the multi-element notation is consistent with MATLAB this is purposeful. The list below provides a summary of the notation convention used throughout this code. M Data matrix. Defined as an image of spectral signatures or vectors: Mm\u00d7n\u00d7p. Or, defined as a long vector of spectral signatures: M(m\u2022n)\u00d7p. N The total number of pixels. For example N = m \u2022 n. m Number of rows in the image. n Number of columns in the image. p Number of bands. II. ELECTRICAL CIRCUIT ANALYSIS IN MATLAB SIMULINK TO FIND THE VOLTAGE AND CURRENT LEVEL CHANGES Computer Simulation acts a vital role in the analysis and design of power electronic circuits and systems. The simulation not only validates the system\u2019s operation but also permits optimization of the system\u2019s performance by iteration of its parameters. Furthermore control and circuit parameters, the signal parameter variation can be studied. Hence, valuable time is saved in design and development of a product. Simulation software\u2019s are used to calculate the circuit waveforms, the steady state and dynamic performance of the systems, and the ratings of the various components used in the circuit. Simulink is a powerful graphical user-interface to MATLAB [10] which lets dynamic systems to be defined in an easy block diagram form. A library of function blocks, such as sources, sinks, discrete, linear, non-linear and connections can be used in the simulation. Simulink is a block diagram environment for multi domain simulation and Model-Based Design. It provides system-level design, simulation, automatic code generation, and continuous test and verification of embedded systems. Simulink provides a graphical editor, customizable block libraries, and solvers for modeling and simulating dynamic systems. It is combined with MATLAB [11], allowing to incorporate MATLAB algorithms into models and export simulation results to MATLAB for further analysis. A) Sim Power System SimPower Systems models can be used to develop control systems and test system-level performance. It can parameterize models using MATLAB variables and expressions, and design control systems for electrical power system in Simulink. SimPower Systems offers component libraries and analysis tools for modelling and simulating electrical power systems. The libraries contain models of electrical power components, including threephase machines, electric drives, and components for applications such as flexible AC transmission systems. B) Measuring Voltages and Currents When measure a current using a Current Measurement block, the positive direction of current is indicated on the block icon such as positive current flowing from + terminal to \u2013terminal. Also, when measure a voltage using a Voltage Measurement block, the measured voltage is the voltage of the + terminal with respect to the \u2013terminal. The electrical state variables are the Simulink states of diagram related to the capacitor and inductor devices of the SimPower Systems blocks. Inductors and capacitors elements are found in the RLC-branch type blocks such as the Series RLC Branch block. The electrical s",
            "year": 2017,
            "citationCount": 2,
            "tldr": null,
            "score": 1
        }
    ],
    "novelty": "yes"
}