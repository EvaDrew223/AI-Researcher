{
    "topic_description": "novel prompting methods that can better quantify uncertainty or calibrate the confidence of large language models",
    "idea_name": "Adversarial Confidence Probing",
    "raw_idea": {
        "Problem": "Large language models can be overconfident in their predictions, even when faced with adversarial or out-of-distribution examples. This can lead to poor calibration and unreliable outputs in real-world scenarios.",
        "Existing Methods": "Existing methods for improving LLM calibration include temperature scaling, label smoothing, and confidence calibration techniques. However, these methods often rely on in-distribution data and may not generalize well to adversarial settings.",
        "Motivation": "We propose using adversarial examples to probe the confidence of LLMs and improve their calibration under distributional shift. By generating targeted perturbations of input examples and analyzing the model's confidence on these perturbed inputs, we can identify areas of overconfidence and adjust the model's predictions accordingly.",
        "Proposed Method": "Our method, Adversarial Confidence Probing (ACP), consists of the following steps: 1) Given a dataset of input-output pairs, generate adversarial perturbations of the inputs using techniques such as word substitution, syntax manipulation, or semantic preservation. 2) Prompt the LLM to generate outputs for both the original and perturbed inputs, along with confidence scores. 3) Analyze the difference in confidence scores between the original and perturbed examples to identify areas of overconfidence. 4) Fine-tune the LLM using a calibration objective that penalizes overconfidence on the adversarial examples. 5) Evaluate the calibrated model on a held-out set of adversarial examples to assess improved calibration.",
        "Experiment Plan": "We will evaluate ACP on a range of natural language understanding tasks, such as sentiment analysis, natural language inference, and question answering. We will generate adversarial examples using established techniques and compare the calibration of the original and fine-tuned models using metrics such as expected calibration error and adversarial accuracy. We will also assess the generalization of the calibrated models to unseen adversarial examples and out-of-distribution data."
    },
    "full_experiment_plan": {
        "Title": "Adversarial Confidence Probing: Improving Language Model Calibration with Targeted Perturbations",
        "Problem Statement": "Large language models can be overconfident in their predictions, even when faced with adversarial or out-of-distribution examples. This can lead to poor calibration and unreliable outputs in real-world scenarios.",
        "Motivation": "Existing methods for improving LLM calibration, such as temperature scaling, label smoothing, and confidence calibration techniques, often rely on in-distribution data and may not generalize well to adversarial settings. We propose using adversarial examples to probe the confidence of LLMs and improve their calibration under distributional shift. By generating targeted perturbations of input examples and analyzing the model's confidence on these perturbed inputs, we can identify areas of overconfidence and adjust the model's predictions accordingly.",
        "Proposed Method": "Our method, Adversarial Confidence Probing (ACP), consists of the following steps:\n1. Given a dataset of input-output pairs, generate adversarial perturbations of the inputs using techniques such as word substitution, syntax manipulation, or semantic preservation.\n2. Prompt the LLM to generate outputs for both the original and perturbed inputs, along with confidence scores.\n3. Analyze the difference in confidence scores between the original and perturbed examples to identify areas of overconfidence.\n4. Fine-tune the LLM using a calibration objective that penalizes overconfidence on the adversarial examples.\n5. Evaluate the calibrated model on a held-out set of adversarial examples to assess improved calibration.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Dataset Selection": "We will evaluate ACP on a range of natural language understanding tasks, such as sentiment analysis (SST-2, IMDb), natural language inference (SNLI, MNLI), and question answering (SQuAD, TriviaQA). These datasets cover a diverse set of tasks and domains to assess the generalizability of our method.",
            "Step 2: Adversarial Perturbation Generation": "For each dataset, we will generate adversarial perturbations using the following techniques:\n- Word Substitution: Replace words with synonyms, antonyms, or semantically similar words using resources like WordNet or word embeddings.\n- Syntax Manipulation: Modify the sentence structure without changing its meaning, e.g., active-passive voice conversion, word reordering.\n- Semantic Preservation: Rephrase the sentence while preserving its original meaning, e.g., paraphrasing, negation, or adding irrelevant information.\nWe will use existing adversarial example generation libraries (e.g., TextAttack, OpenAttack) and develop custom perturbation functions as needed.",
            "Step 3: LLM Prompting and Confidence Scoring": "We will use GPT-3.5 (text-davinci-002) and GPT-4 as our base LLMs. For each original and perturbed example, we will prompt the LLM to generate the output and a confidence score. The prompt format will be:\n<prompt>\nOriginal: {original_input}\nPerturbed: {perturbed_input}\nQuestion: {task_question}\nOriginal Output: {original_output}\nOriginal Confidence: {original_confidence}\nPerturbed Output: {perturbed_output}\nPerturbed Confidence: {perturbed_confidence}\n</prompt>\nWe will use the LLM's output probabilities or log-odds scores as confidence measures.",
            "Step 4: Overconfidence Analysis": "We will compare the confidence scores between the original and perturbed examples to identify instances where the model is overconfident. Specifically, we will:\n- Calculate the difference in confidence scores (original - perturbed) for each example.\n- Analyze the distribution of confidence differences across the dataset.\n- Identify examples with large positive confidence differences, indicating overconfidence on the original input.\n- Investigate patterns or characteristics of overconfident examples (e.g., input length, perturbation type).",
            "Step 5: Model Fine-tuning": "We will fine-tune the LLM using a calibration objective that penalizes overconfidence on the adversarial examples. The training data will consist of the original and perturbed examples, along with their target outputs. The calibration objective will be a combination of the standard cross-entropy loss and a confidence penalty term:\nLoss = CrossEntropy(model_output, target_output) + \u03bb * ConfidencePenalty(model_confidence, target_confidence)\nwhere \u03bb is a hyperparameter controlling the strength of the confidence penalty. The target confidence will be set to a lower value (e.g., 0.5) for the perturbed examples to encourage the model to be less confident on adversarial inputs.",
            "Step 6: Evaluation": "We will evaluate the calibrated model on a held-out set of adversarial examples generated using the same perturbation techniques. We will assess the model's calibration using metrics such as Expected Calibration Error (ECE) and Maximum Calibration Error (MCE). We will compare the calibration metrics before and after fine-tuning to quantify the improvement achieved by ACP. Additionally, we will evaluate the model's accuracy on the original and adversarial examples to ensure that the calibration improvement does not come at the cost of reduced accuracy."
        },
        "Test Case Examples": {
            "Original Input": "The movie was fantastic! The acting was superb and the plot kept me engaged throughout.",
            "Perturbed Input": "The movie was fantastic! The acting was superb, but the plot kept me engaged throughout.",
            "Task": "Sentiment Analysis",
            "Baseline Output (Original Input)": "Positive, Confidence: 0.95",
            "Baseline Output (Perturbed Input)": "Positive, Confidence: 0.93",
            "Explanation": "After fine-tuning with ACP, the model assigns a lower confidence score to the perturbed input, recognizing the potential contradiction introduced by the perturbation. This demonstrates improved calibration and sensitivity to adversarial examples.",
            "ACP Output (Original Input)": "Positive, Confidence: 0.90",
            "ACP Output (Perturbed Input)": "Positive, Confidence: 0.60"
        },
        "Fallback Plan": "If the proposed ACP method does not significantly improve calibration or generalize well to unseen adversarial examples, we can consider the following alternative approaches:\n1. Analyze the characteristics of the overconfident examples and develop more targeted perturbation techniques that exploit specific weaknesses of the model.\n2. Experiment with different calibration objectives or confidence penalty terms to better align the model's confidence scores with its accuracy.\n3. Investigate the impact of different fine-tuning strategies, such as using a separate calibration dataset or employing meta-learning techniques to adapt the model's calibration to new tasks or domains.\n4. Conduct a thorough error analysis to identify common patterns or failure modes of the model, and use these insights to guide further improvements to the calibration method.\nIf the calibration improvement remains limited, we can focus on understanding the limitations of the proposed approach and provide insights into the challenges of calibrating LLMs under adversarial settings. This can lead to a more analysis-oriented paper that contributes valuable findings to the research community."
    },
    "novelty_queries": [
        "KeywordQuery(\"language models adversarial calibration\")",
        "KeywordQuery(\"language models confidence probing\")",
        "KeywordQuery(\"language models overconfidence perturbations\")",
        "KeywordQuery(\"Adversarial Confidence Probing NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "ab4ce5dda7ad4d9032995c9c049a89d65723c6aa",
            "paperId": "ab4ce5dda7ad4d9032995c9c049a89d65723c6aa",
            "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
            "abstract": "A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated. However, the most widely-used LMs are fine-tuned with reinforcement learning from human feedback (RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated. In light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50%.",
            "year": 2023,
            "citationCount": 96,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "For RLHF-LMs such as ChatGPT, GPT-4, and Claude, it is found that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50%."
            },
            "score": 8,
            "novelty_score": "The project proposal aims to improve language model calibration under adversarial settings by generating targeted perturbations and fine-tuning the model to be less confident on these perturbed examples. The paper focuses on evaluating methods for extracting well-calibrated confidence scores from language models fine-tuned with reinforcement learning from human feedback (RLHF-LMs).\n\nThe project proposal addresses the problem of overconfidence in large language models when faced with adversarial or out-of-distribution examples, while the paper investigates the calibration of confidence scores in RLHF-LMs. The project proposes a method called Adversarial Confidence Probing (ACP) to generate adversarial perturbations and fine-tune the model to improve calibration, whereas the paper evaluates various strategies for eliciting calibrated confidence scores from RLHF-LMs, such as using verbalized confidences emitted as output tokens.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "ad402080a4aa66ef3c57a46ce4685a47a3cc0a61",
            "paperId": "ad402080a4aa66ef3c57a46ce4685a47a3cc0a61",
            "title": "Quantifying Uncertainty in Natural Language Explanations of Large Language Models",
            "abstract": "Large Language Models (LLMs) are increasingly used as powerful tools for several high-stakes natural language processing (NLP) applications. Recent prompting works claim to elicit intermediate reasoning steps and key tokens that serve as proxy explanations for LLM predictions. However, there is no certainty whether these explanations are reliable and reflect the LLMs behavior. In this work, we make one of the first attempts at quantifying the uncertainty in explanations of LLMs. To this end, we propose two novel metrics -- $\\textit{Verbalized Uncertainty}$ and $\\textit{Probing Uncertainty}$ -- to quantify the uncertainty of generated explanations. While verbalized uncertainty involves prompting the LLM to express its confidence in its explanations, probing uncertainty leverages sample and model perturbations as a means to quantify the uncertainty. Our empirical analysis of benchmark datasets reveals that verbalized uncertainty is not a reliable estimate of explanation confidence. Further, we show that the probing uncertainty estimates are correlated with the faithfulness of an explanation, with lower uncertainty corresponding to explanations with higher faithfulness. Our study provides insights into the challenges and opportunities of quantifying uncertainty in LLM explanations, contributing to the broader discussion of the trustworthiness of foundation models.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes two novel metrics -- verbalized uncertainty and probing uncertainty -- to quantify the uncertainty of generated explanations of large Language Models, and shows that the probing uncertainty estimates are correlated with the faithfulness of an explanation, with lower uncertainty corresponding to explanations with higher faithfulness."
            },
            "score": 8,
            "novelty_score": "The project proposal aims to improve language model calibration using adversarial perturbations, while the paper focuses on quantifying the uncertainty in natural language explanations generated by large language models.\n\nProject Proposal: Improving language model calibration with targeted adversarial perturbations.\nPaper: Quantifying uncertainty in natural language explanations of large language models.\n\nThe two works address different research problems and propose different approaches. The project proposal targets model calibration, while the paper investigates uncertainty quantification in explanations.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "4efc5543d34de96e0f6eb97cc9ccecbaf94ceae3",
            "paperId": "4efc5543d34de96e0f6eb97cc9ccecbaf94ceae3",
            "title": "Extreme Miscalibration and the Illusion of Adversarial Robustness",
            "abstract": "Deep learning-based Natural Language Processing (NLP) models are vulnerable to adversarial attacks, where small perturbations can cause a model to misclassify. Adversarial Training (AT) is often used to increase model robustness. However, we have discovered an intriguing phenomenon: deliberately or accidentally miscalibrating models masks gradients in a way that interferes with adversarial attack search methods, giving rise to an apparent increase in robustness. We show that this observed gain in robustness is an illusion of robustness (IOR), and demonstrate how an adversary can perform various forms of test-time temperature calibration to nullify the aforementioned interference and allow the adversarial attack to find adversarial examples. Hence, we urge the NLP community to incorporate test-time temperature scaling into their robustness evaluations to ensure that any observed gains are genuine. Finally, we show how the temperature can be scaled during \\textit{training} to improve genuine robustness.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The NLP community is urged to incorporate test-time temperature scaling into their robustness evaluations to ensure that any observed gains are genuine, and it is shown how the temperature can be scaled during training to improve genuine robustness."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving language model calibration under adversarial settings, and the approach is to use adversarial examples to probe model confidence and fine-tune the model to reduce overconfidence.\n\nThe research problem in the paper is that miscalibration can give an illusion of adversarial robustness, and the approach is to use test-time temperature scaling to ensure the observed robustness gains are genuine.\n\nWhile both works deal with model calibration and adversarial examples, the proposal focuses on improving calibration using adversarial probing, while the paper studies how miscalibration can affect the evaluation of adversarial robustness. The research problems and approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "77d6d7482d1a32ad147c39993758b6c63816f5c0",
            "paperId": "77d6d7482d1a32ad147c39993758b6c63816f5c0",
            "title": "PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
            "abstract": "The increasing reliance on Large Language Models (LLMs) across academia and industry necessitates a comprehensive understanding of their robustness to prompts. In response to this vital need, we introduce PromptBench, a robustness benchmark designed to measure LLMs' resilience to adversarial prompts. This study uses a plethora of adversarial textual attacks targeting prompts across multiple levels: character, word, sentence, and semantic. The adversarial prompts, crafted to mimic plausible user errors like typos or synonyms, aim to evaluate how slight deviations can affect LLM outcomes while maintaining semantic integrity. These prompts are then employed in diverse tasks, such as sentiment analysis, natural language inference, reading comprehension, machine translation, and math problem-solving. Our study generates 4788 adversarial prompts, meticulously evaluated over 8 tasks and 13 datasets. Our findings demonstrate that contemporary LLMs are not robust to adversarial prompts. Furthermore, we present comprehensive analysis to understand the mystery behind prompt robustness and its transferability. We then offer insightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users. Code is available at: https://github.com/microsoft/promptbench.",
            "year": 2023,
            "citationCount": 111,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study generates 4788 adversarial prompts and presents comprehensive analysis to understand the mystery behind prompt robustness and its transferability, and offers insightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both researchers and everyday users."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving language model calibration under adversarial settings, and the approach is to use adversarial examples to probe model confidence and fine-tune the model to reduce overconfidence.\n\nThe research problem in the paper is evaluating the robustness of large language models to adversarial prompts, and the approach is to create a benchmark with adversarial prompts across different levels and tasks to measure model robustness.\n\nWhile both works involve adversarial examples and large language models, the proposal focuses on improving calibration and reducing overconfidence, while the paper focuses on evaluating robustness to adversarial prompts. The goals and methods are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "e8b3b37c0d301ea41c75765f6ceb7fcbb2e088a4",
            "paperId": "e8b3b37c0d301ea41c75765f6ceb7fcbb2e088a4",
            "title": "AutoDAN: Automatic and Interpretable Adversarial Attacks on Large Language Models",
            "abstract": "Safety alignment of Large Language Models (LLMs) can be compromised with manual jailbreak attacks and (automatic) adversarial attacks. Recent work suggests that patching LLMs against these attacks is possible: manual jailbreak attacks are human-readable but often limited and public, making them easy to block; adversarial attacks generate gibberish prompts that can be detected using perplexity-based filters. In this paper, we show that these solutions may be too optimistic. We propose an interpretable adversarial attack, AutoDAN , that combines the strengths of both types of attacks. It automatically generates attack prompts that bypass perplexity-based filters while maintaining a high attack success rate like manual jailbreak attacks. These prompts are interpretable and diverse, exhibiting strategies commonly used in manual jailbreak attacks, and transfer better than their non-readable counterparts when using limited training data or a single proxy model. We also customize AutoDAN \u2019s objective to leak system prompts, another jailbreak application not addressed in the adversarial attack literature. Our work provides a new way to red-team LLMs and to understand the mechanism of jailbreak attacks.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An interpretable adversarial attack, AutoDAN, is proposed, that combines the strengths of both types of attacks and provides a new way to red-team LLMs and to understand the mechanism of jailbreak attacks."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve language model calibration using adversarial examples to probe overconfidence, while the paper focuses on generating interpretable adversarial attacks that bypass perplexity-based filters to compromise the safety alignment of large language models.\n\nProject Proposal: Improving LLM calibration with adversarial examples to identify overconfidence\nPaper: Generating interpretable adversarial attacks that bypass perplexity filters to compromise LLM safety\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "ad934a9344f68fcc0b9aa704102aa48c39c5b591",
            "paperId": "ad934a9344f68fcc0b9aa704102aa48c39c5b591",
            "title": "Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models",
            "abstract": "Large language models (LLMs) specializing in natural language generation (NLG) have recently started exhibiting promising capabilities across a variety of domains. However, gauging the trustworthiness of responses generated by LLMs remains an open challenge, with limited research on uncertainty quantification (UQ) for NLG. Furthermore, existing literature typically assumes white-box access to language models, which is becoming unrealistic either due to the closed-source nature of the latest LLMs or computational constraints. In this work, we investigate UQ in NLG for black-box LLMs. We first differentiate uncertainty vs confidence: the former refers to the\"dispersion\"of the potential predictions for a fixed input, and the latter refers to the confidence on a particular prediction/generation. We then propose and compare several confidence/uncertainty metrics, applying them to selective NLG where unreliable results could either be ignored or yielded for further assessment. Experiments were carried out with several popular LLMs on question-answering datasets (for evaluation purposes). Results reveal that a simple metric for the semantic dispersion can be a reliable predictor of the quality of LLM responses, providing valuable insights for practitioners on uncertainty management when adopting LLMs. The code to replicate our experiments is available at https://github.com/zlin7/UQ-NLG.",
            "year": 2023,
            "citationCount": 37,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Results reveal that a simple metric for the semantic dispersion can be a reliable predictor of the quality of LLM responses, providing valuable insights for practitioners on uncertainty management when adopting LLMs."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve language model calibration under adversarial settings by generating targeted perturbations and fine-tuning the model to be less confident on these perturbed examples. The approach involves adversarial example generation, confidence score analysis, and fine-tuning with a calibration objective.\n\nThe paper focuses on uncertainty quantification for black-box language models in natural language generation tasks. It proposes and compares different confidence and uncertainty metrics to identify unreliable generated responses.\n\nWhile both works address the issue of confidence and uncertainty in language models, the project proposal specifically targets adversarial settings and aims to improve calibration through fine-tuning. In contrast, the paper explores uncertainty quantification metrics for black-box models without modifying the models themselves.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "57207b935fc3484d175f5e9e2980d73ca793f994",
            "paperId": "57207b935fc3484d175f5e9e2980d73ca793f994",
            "title": "Are Large Language Models Really Robust to Word-Level Perturbations?",
            "abstract": "The swift advancement in the scales and capabilities of Large Language Models (LLMs) positions them as promising tools for a variety of downstream tasks. In addition to the pursuit of better performance and the avoidance of violent feedback on a certain prompt, to ensure the responsibility of the LLM, much attention is drawn to the robustness of LLMs. However, existing evaluation methods mostly rely on traditional question answering datasets with predefined supervised labels, which do not align with the superior generation capabilities of contemporary LLMs. To address this issue, we propose a novel rational evaluation approach that leverages pre-trained reward models as diagnostic tools to evaluate the longer conversation generated from more challenging open questions by LLMs, which we refer to as the Reward Model for Reasonable Robustness Evaluation (TREvaL). Longer conversations manifest the comprehensive grasp of language models in terms of their proficiency in understanding questions, a capability not entirely encompassed by individual words or letters, which may exhibit oversimplification and inherent biases. Our extensive empirical experiments demonstrate that TREvaL provides an innovative method for evaluating the robustness of an LLM. Furthermore, our results demonstrate that LLMs frequently exhibit vulnerability to word-level perturbations that are commonplace in daily language usage. Notably, we are surprised to discover that robustness tends to decrease as fine-tuning (SFT and RLHF) is conducted. The code of TREval is available in https://github.com/Harry-mic/TREvaL.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel rational evaluation approach that leverages pre-trained reward models as diagnostic tools to evaluate the longer conversation generated from more challenging open questions by LLMs, which it refers to as the Reward Model for Reasonable Robustness Evaluation (TREvaL)."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving language model calibration under adversarial settings, and the approach is to use targeted perturbations to probe model confidence and fine-tune the model to be less confident on perturbed inputs.\n\nThe research problem in the paper is evaluating the robustness of large language models to word-level perturbations, and the approach is to use pre-trained reward models to evaluate the quality of generated conversations from perturbed open questions.\n\nWhile both works involve adversarial perturbations, the proposal focuses on improving model calibration, while the paper focuses on evaluating model robustness. The methods are also different: the proposal uses targeted perturbations and fine-tuning, while the paper uses reward models to evaluate generated conversations.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "73c2846b69e88a7e2e831fa9f6bd0a57a7c716fc",
            "paperId": "73c2846b69e88a7e2e831fa9f6bd0a57a7c716fc",
            "title": "Exploring LLMs as a Source of Targeted Synthetic Textual Data to Minimize High Confidence Misclassifications",
            "abstract": "Natural Language Processing (NLP) models optimized for predictive performance often make high confidence errors and suffer from vulnerability to adversarial and out-of-distribution data. Existing work has mainly focused on mitigation of such errors using either humans or an automated approach. In this study, we explore the usage of large language models (LLMs) for data augmentation as a potential solution to the issue of NLP models making wrong predictions with high confidence during classification tasks. We compare the effectiveness of synthetic data generated by LLMs with that of human data obtained via the same procedure. For mitigation, humans or LLMs provide natural language characterizations of high confidence misclassifications to generate synthetic data, which are then used to extend the training set. We conduct an extensive evaluation of our approach on three classification tasks and demonstrate its effectiveness in reducing the number of high confidence misclassifications present in the model, all while maintaining the same level of accuracy. Moreover, we find that the cost gap between humans and LLMs surpasses an order of magnitude, as LLMs attain human-like performance while being more scalable.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The usage of large language models (LLMs) for data augmentation as a potential solution to the issue of NLP models making wrong predictions with high confidence during classification tasks is explored and the cost gap between humans and LLMs surpasses an order of magnitude."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve language model calibration using adversarial examples to probe the model's confidence and fine-tune it to be less overconfident. The paper explores using large language models for data augmentation to reduce high confidence misclassifications in NLP models.\n\nProject proposal: Improving language model calibration using adversarial examples.\nPaper: Using LLMs for data augmentation to reduce high confidence misclassifications.\n\nThe two works have different research problems and approaches. The project focuses on calibration and uses adversarial probing, while the paper focuses on misclassifications and uses LLM-generated data augmentation.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "0a0b121dcc127be734c5199d121946bbe8b1ae5d",
            "paperId": "0a0b121dcc127be734c5199d121946bbe8b1ae5d",
            "title": "Quantifying confidence shifts in a BERT-based question answering system evaluated on perturbed instances",
            "abstract": "Recent work on transformer-based neural networks has led to impressive advances on multiple-choice natural language processing (NLP) problems, such as Question Answering (QA) and abductive reasoning. Despite these advances, there is limited work still on systematically evaluating such models in ambiguous situations where (for example) no correct answer exists for a given prompt among the provided set of choices. Such ambiguous situations are not infrequent in real world applications. We design and conduct an experimental study of this phenomenon using three probes that aim to \u2018confuse\u2019 the model by perturbing QA instances in a consistent and well-defined manner. Using a detailed set of results based on an established transformer-based multiple-choice QA system on two established benchmark datasets, we show that the model\u2019s confidence in its results is very different from that of an expected model that is \u2018agnostic\u2019 to all choices that are incorrect. Our results suggest that high performance on idealized QA instances should not be used to infer or extrapolate similarly high performance on more ambiguous instances. Auxiliary results suggest that the model may not be able to distinguish between these two situations with sufficient certainty. Stronger testing protocols and benchmarking may hence be necessary before such models are deployed in front-facing systems or ambiguous decision making with significant human impact.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is suggested that high performance on idealized QA instances should not be used to infer or extrapolate similarly high performance on more ambiguous instances, and that stronger testing protocols and benchmarking may be necessary before transformer-based multiple-choice NLP models are deployed in front-facing systems or ambiguous decision making with significant human impact."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving language model calibration under adversarial settings, and the approach is to use targeted perturbations to probe model confidence and fine-tune the model to reduce overconfidence.\n\nThe research problem in the paper is evaluating the confidence of a BERT-based question answering system on perturbed instances, and the approach is to use probes that 'confuse' the model by perturbing QA instances and analyze the model's confidence shifts.\n\nWhile both works involve evaluating language models on perturbed instances, the proposal focuses on improving calibration and reducing overconfidence, while the paper focuses on analyzing confidence shifts without proposing a method to improve the model. The tasks (general language modeling vs. question answering) and model architectures (GPT vs. BERT) are also different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "bbd6d6874a8ca1c155bcfb540e8d55199944cdc5",
            "paperId": "bbd6d6874a8ca1c155bcfb540e8d55199944cdc5",
            "title": "RoAST: Robustifying Language Models via Adversarial Perturbation with Selective Training",
            "abstract": "Fine-tuning pre-trained language models (LMs) has become the de facto standard in many NLP tasks. Nevertheless, fine-tuned LMs are still prone to robustness issues, such as adversarial robustness and model calibration. Several perspectives of robustness for LMs have been studied independently, but lacking a unified consideration in multiple perspectives. In this paper, we propose Robustifying LMs via Adversarial perturbation with Selective Training (RoAST), a simple yet effective fine-tuning technique to enhance the multi-perspective robustness of LMs in a unified way. RoAST effectively incorporates two important sources for the model robustness, robustness on the perturbed inputs and generalizable knowledge in pre-trained LMs. To be specific, RoAST introduces adversarial perturbation during fine-tuning while the model parameters are selectively updated upon their relative importance to minimize unnecessary deviation. Under a unified evaluation of fine-tuned LMs by incorporating four representative perspectives of model robustness, we demonstrate the effectiveness of RoAST compared to state-of-the-art fine-tuning methods on six different types of LMs, which indicates its usefulness in practice.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Under a unified evaluation of fine-tuned LMs by incorporating four representative perspectives of model robustness, the effectiveness of RoAST is demonstrated compared to state-of-the-art fine- tuning methods on six different types of LMs, which indicates its usefulness in practice."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve language model calibration under adversarial settings by generating targeted perturbations and fine-tuning the model to be less confident on these perturbed examples.\n\nThe paper abstract proposes a fine-tuning technique called RoAST to enhance the multi-perspective robustness of language models by introducing adversarial perturbations during fine-tuning while selectively updating model parameters.\n\nWhile both the proposal and the paper focus on improving language model robustness using adversarial perturbations, the proposal specifically targets calibration under adversarial settings, whereas the paper considers multiple perspectives of robustness in a unified way.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "b5a624da64475d735f0e298dc6f2f6669b5bb697",
            "paperId": "b5a624da64475d735f0e298dc6f2f6669b5bb697",
            "title": "Robust Safety Classifier for Large Language Models: Adversarial Prompt Shield",
            "abstract": "Large Language Models' safety remains a critical concern due to their vulnerability to adversarial attacks, which can prompt these systems to produce harmful responses. In the heart of these systems lies a safety classifier, a computational model trained to discern and mitigate potentially harmful, offensive, or unethical outputs. However, contemporary safety classifiers, despite their potential, often fail when exposed to inputs infused with adversarial noise. In response, our study introduces the Adversarial Prompt Shield (APS), a lightweight model that excels in detection accuracy and demonstrates resilience against adversarial prompts. Additionally, we propose novel strategies for autonomously generating adversarial training datasets, named Bot Adversarial Noisy Dialogue (BAND) datasets. These datasets are designed to fortify the safety classifier's robustness, and we investigate the consequences of incorporating adversarial examples into the training process. Through evaluations involving Large Language Models, we demonstrate that our classifier has the potential to decrease the attack success rate resulting from adversarial attacks by up to 60%. This advancement paves the way for the next generation of more reliable and resilient conversational agents.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study introduces the Adversarial Prompt Shield (APS), a lightweight model that excels in detection accuracy and demonstrates resilience against adversarial prompts, and proposes novel strategies for autonomously generating adversarial training datasets, designed to fortify the safety classifier's robustness."
            },
            "score": 6
        },
        {
            "id": "3e30a7ac4886b28eb50151f58e14a1d698cccd0e",
            "paperId": "3e30a7ac4886b28eb50151f58e14a1d698cccd0e",
            "title": "Baseline Defenses for Adversarial Attacks Against Aligned Language Models",
            "abstract": "As Large Language Models quickly become ubiquitous, it becomes critical to understand their security vulnerabilities. Recent work shows that text optimizers can produce jailbreaking prompts that bypass moderation and alignment. Drawing from the rich body of work on adversarial machine learning, we approach these attacks with three questions: What threat models are practically useful in this domain? How do baseline defense techniques perform in this new domain? How does LLM security differ from computer vision? We evaluate several baseline defense strategies against leading adversarial attacks on LLMs, discussing the various settings in which each is feasible and effective. Particularly, we look at three types of defenses: detection (perplexity based), input preprocessing (paraphrase and retokenization), and adversarial training. We discuss white-box and gray-box settings and discuss the robustness-performance trade-off for each of the defenses considered. We find that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for LLMs. Future research will be needed to uncover whether more powerful optimizers can be developed, or whether the strength of filtering and preprocessing defenses is greater in the LLMs domain than it has been in computer vision.",
            "year": 2023,
            "citationCount": 97,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that the weakness of existing discrete optimizers for text, combined with the relatively high costs of optimization, makes standard adaptive attacks more challenging for LLMs."
            },
            "score": 6
        },
        {
            "id": "142e934dd5d6c53f877c30243d436255e3a0dde7",
            "paperId": "142e934dd5d6c53f877c30243d436255e3a0dde7",
            "title": "Visual Adversarial Examples Jailbreak Aligned Large Language Models",
            "abstract": "Warning: this paper contains data, prompts, and model outputs that are offensive in nature.\n\nRecently, there has been a surge of interest in integrating vision into Large Language Models (LLMs), exemplified by Visual Language Models (VLMs) such as Flamingo and GPT-4. This paper sheds light on the security and safety implications of this trend. First, we underscore that the continuous and high-dimensional nature of the visual input makes it a weak link against adversarial attacks, representing an expanded attack surface of vision-integrated LLMs. Second, we highlight that the versatility of LLMs also presents visual attackers with a wider array of achievable adversarial objectives, extending the implications of security failures beyond mere misclassification. As an illustration, we present a case study in which we exploit visual adversarial examples to circumvent the safety guardrail of aligned LLMs with integrated vision. Intriguingly, we discover that a single visual adversarial example can universally jailbreak an aligned LLM, compelling it to heed a wide range of harmful instructions (that it otherwise would not) and generate harmful content that transcends the narrow scope of a `few-shot' derogatory corpus initially employed to optimize the adversarial example. Our study underscores the escalating adversarial risks associated with the pursuit of multimodality. Our findings also connect the long-studied adversarial vulnerabilities of neural networks to the nascent field of AI alignment. The presented attack suggests a fundamental adversarial challenge for AI alignment, especially in light of the emerging trend toward multimodality in frontier foundation models.",
            "year": 2023,
            "citationCount": 44,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that a single visual adversarial example can universally jailbreak an aligned LLM, compelling it to heed a wide range of harmful instructions and generate harmful content that transcends the narrow scope of a `few-shot' derogatory corpus initially employed to optimize the adversarial example."
            },
            "score": 6
        },
        {
            "id": "1abfc211793c683972ded8d3268475e3ee7a88b0",
            "paperId": "1abfc211793c683972ded8d3268475e3ee7a88b0",
            "title": "Adversarial Demonstration Attacks on Large Language Models",
            "abstract": "With the emergence of more powerful large language models (LLMs), such as ChatGPT and GPT-4, in-context learning (ICL) has gained significant prominence in leveraging these models for specific tasks by utilizing data-label pairs as precondition prompts. While incorporating demonstrations can greatly enhance the performance of LLMs across various tasks, it may introduce a new security concern: attackers can manipulate only the demonstrations without changing the input to perform an attack. In this paper, we investigate the security concern of ICL from an adversarial perspective, focusing on the impact of demonstrations. We propose a novel attack method named advICL, which aims to manipulate only the demonstration without changing the input to mislead the models. Our results demonstrate that as the number of demonstrations increases, the robustness of in-context learning would decrease. Additionally, we also identify the intrinsic property of the demonstrations is that they can be used (prepended) with different inputs. As a result, it introduces a more practical threat model in which an attacker can attack the test input example even without knowing and manipulating it. To achieve it, we propose the transferable version of advICL, named Transferable-advICL. Our experiment shows that the adversarial demonstration generated by Transferable-advICL can successfully attack the unseen test input examples. We hope that our study reveals the critical security risks associated with ICL and underscores the need for extensive research on the robustness of ICL, particularly given its increasing significance in the advancement of LLMs.",
            "year": 2023,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper investigates the security concern of ICL from an adversarial perspective, focusing on the impact of demonstrations, and proposes a novel attack method named advICL, which aims to manipulate only the demonstration without changing the input to mislead the models."
            },
            "score": 6
        },
        {
            "id": "738852940591ecf864abf402878ecf66e2945267",
            "paperId": "738852940591ecf864abf402878ecf66e2945267",
            "title": "Visual Adversarial Examples Jailbreak Large Language Models",
            "abstract": "for",
            "year": 2023,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 6
        },
        {
            "id": "eae87af6e956532d6842718803179c9ab2386ea9",
            "paperId": "eae87af6e956532d6842718803179c9ab2386ea9",
            "title": "Large Language Models Should Ask Clarifying Questions to Increase Confidence in Generated Code",
            "abstract": "Large language models (LLMs) have significantly improved the ability to perform tasks in the field of code generation. However, there is still a gap between LLMs being capable coders and being top-tier software engineers. Based on the observation that toplevel software engineers often ask clarifying questions to reduce ambiguity in both requirements and coding solutions, I argue that the same should be applied to LLMs for code generation tasks. By asking probing questions in various topics before generating the final code, the challenges of programming with LLMs, such as unclear intent specification, lack of computational thinking, and undesired code quality, may be alleviated. This, in turn, increases confidence in the generated code. In this work, I explore how to leverage better communication skills to achieve greater confidence in generated code. I propose a communication-centered process that uses an LLM-generated communicator to identify issues with high ambiguity or low confidence in problem descriptions and generated code. I then ask clarifying questions to obtain responses from users for refining the code.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a communication-centered process that uses an LLM-generated communicator to identify issues with high ambiguity or low confidence in problem descriptions and generated code, and asks clarifying questions to obtain responses from users for refining the code."
            },
            "score": 6
        },
        {
            "id": "25cee84e3a1541697a7c97443d7526574127c344",
            "paperId": "25cee84e3a1541697a7c97443d7526574127c344",
            "title": "Don't Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration",
            "abstract": "Despite efforts to expand the knowledge of large language models (LLMs), knowledge gaps -- missing or outdated information in LLMs -- might always persist given the evolving nature of knowledge. In this work, we study approaches to identify LLM knowledge gaps and abstain from answering questions when knowledge gaps are present. We first adapt existing approaches to model calibration or adaptation through fine-tuning/prompting and analyze their ability to abstain from generating low-confidence outputs. Motivated by their failures in self-reflection and over-reliance on held-out sets, we propose two novel approaches that are based on model collaboration, i.e., LLMs probing other LLMs for knowledge gaps, either cooperatively or competitively. Extensive experiments with three LLMs on four QA tasks featuring diverse knowledge domains demonstrate that both cooperative and competitive approaches to unveiling LLM knowledge gaps achieve up to 19.3% improvements on abstain accuracy against the strongest baseline. Further analysis reveals that our proposed mechanisms could help identify failure cases in retrieval augmentation and pinpoint knowledge gaps in multi-hop reasoning.",
            "year": 2024,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work first adapt existing approaches to model calibration or adaptation through fine-tuning/prompting and analyze their ability to abstain from generating low-confidence outputs, and proposes two novel approaches that are based on model collaboration, i.e., LLMs probing other LLMs for knowledge gaps, either cooperatively or competitively."
            },
            "score": 6
        },
        {
            "id": "dc761368369156599f6534d8504283ea98cc9318",
            "paperId": "dc761368369156599f6534d8504283ea98cc9318",
            "title": "Calibrated Contrast-Consistent Search",
            "abstract": "Large language models (LLMs) are increasingly being deployed in novel areas ranging from healthcare to education. As these models become more consequential, it is vital to ensure they are safe and aligned with human values. Notably, one key desirable feature for LLMs is truthfulness, something that current LLMs do not always exhibit. The field of probing in NLP aims to discern what linguistic knowledge pre-trained language models encode within their hidden representations (Ivanova et al., 2021). One such probing algorithm, Contrast-Consistent Search (CCS), identifies representations of truth in models in an unsupervised manner by finding a direction in activation space that has logically consistent values for true and false statements (Burns et al., 2022). In this project, we aim to improve CCS and make it more interpretable by exploring calibration of predicted probabilities to better reflect true model confidence. We test several supervised and unsupervised approaches to this problem, including loss function modification (unsupervised) and post-hoc Platt scaling or isotonic regression (supervised). Both loss function modification and Platt scaling improve CCS accuracy and calibration on a sentiment analysis dataset using the deBERTa language model. Our results using loss function modification demonstrate the potential for more calibration-aligned loss functions that still yield similar accuracy. These contributions can make CCS more valuable as a tool for understanding what language models really know and how they represent that information.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Both loss function modification and Platt scaling improve CCS accuracy and calibration on a sentiment analysis dataset using the deBERTa language model and demonstrate the potential for more calibration-aligned loss functions that still yield similar accuracy."
            },
            "score": 6
        },
        {
            "id": "b195b707dc54fab51ae22a0cd7511e151b6533f4",
            "paperId": "b195b707dc54fab51ae22a0cd7511e151b6533f4",
            "title": "Selective-LAMA: Selective Prediction for Confidence-Aware Evaluation of Language Models",
            "abstract": "Recent studies have suggested that neural language models learn and store a large amount of facts and commonsense knowledge from training data. The ability of language models to restore such knowledge is often evaluated via zero-shot cloze-style QA tasks. However, such evaluations rely only on prediction accuracy without punishing the systems for their mistakes, e.g., simply guessing or hallucinating likely answers. Selective prediction is a more informative evaluation framework that takes the confidence of predictions into account. Under the selective prediction setting, a model is evaluated not only by the number of correct predictions, but also by the ability to filter out dubious predictions by estimating the confidence of individual predictions. Such confidence-aware evaluation is crucial for determining whether to trust zero-shot predictions of language models. In this paper, we apply the selective prediction setting to an existing benchmark, LAMA probe, and conduct extensive experiments with recent neural language models and different confidence functions. We empirically show that our Selective-LAMA evaluation is more robust to the effect of simple guesses than the conventional accuracy-based evaluation.Our evaluation reveals the importance of the choice of confidence functions by showing that simply relying on token probabilities is not always the best choice.Further analysis shows that various confidence functions exhibit different preferences over predicted tokens for a given context.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper empirically shows that the Selective-LAMA evaluation is more robust to the effect of simple guesses than the conventional accuracy-based evaluation, and reveals the importance of the choice of confidence functions by showing that simply relying on token probabilities is not always the best choice."
            },
            "score": 6
        },
        {
            "id": "1fa4469e5bc5d096572902fe14b0d66078a24c47",
            "paperId": "1fa4469e5bc5d096572902fe14b0d66078a24c47",
            "title": "Navigating the Grey Area: Expressions of Overconfidence and Uncertainty in Language Models",
            "abstract": "Despite increasingly \ufb02uent, relevant, and coherent language generation, major gaps remain between how humans and machines use language. We argue that a key dimension that is missing from our understanding of language models (LMs) is the model\u2019s ability to interpret and generate expressions of uncertainty . Whether it be the weatherperson announcing a chance of rain or a doctor giving a diagnosis, information is often not black-and-white and expressions of uncertainty provide nuance to support human-decision making. The increasing deployment of LMs in the wild motivates us to investigate whether LMs are capable of interpreting expressions of uncertainty and how LMs\u2019 behaviors change when learning to emit their own expressions of uncertainty. When injecting expressions of uncertainty into prompts (e.g., \"I think the answer is...\"), we discover that GPT3\u2019s generations vary upwards of 80% in accuracy based on the expression used. We analyze the linguistic characteristics of these expressions and \ufb01nd a drop in accuracy when naturalistic expressions of certainty are present. We \ufb01nd similar effects when teaching models to emit their own expressions of uncertainty, where model calibration suffers when teaching models to emit certainty rather than un certainty. Together, these results highlight the challenges of building LMs that interpret and generate trustworthy expressions of uncertainty.",
            "year": 2023,
            "citationCount": 54,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that GPT3\u2019s generations vary upwards of 80% in accuracy based on the expression used, and the challenges of building LMs that interpret and generate trustworthy expressions of uncertainty are highlighted."
            },
            "score": 6
        },
        {
            "id": "217e436fd23fe4184828e02a2b143835d6fd3b28",
            "paperId": "217e436fd23fe4184828e02a2b143835d6fd3b28",
            "title": "Navigating the Grey Area: How Expressions of Uncertainty and Overconfidence Affect Language Models",
            "abstract": "The increased deployment of LMs for real-world tasks involving knowledge and facts makes it important to understand model epistemology: what LMs think they know, and how their attitudes toward that knowledge are affected by language use in their inputs. Here, we study an aspect of model epistemology: how epistemic markers of certainty, uncertainty, or evidentiality like\"I'm sure it's\",\"I think it's\", or\"Wikipedia says it's\"affect models, and whether they contribute to model failures. We develop a typology of epistemic markers and inject 50 markers into prompts for question answering. We find that LMs are highly sensitive to epistemic markers in prompts, with accuracies varying more than 80%. Surprisingly, we find that expressions of high certainty result in a 7% decrease in accuracy as compared to low certainty expressions; similarly, factive verbs hurt performance, while evidentials benefit performance. Our analysis of a popular pretraining dataset shows that these markers of uncertainty are associated with answers on question-answering websites, while markers of certainty are associated with questions. These associations may suggest that the behavior of LMs is based on mimicking observed language use, rather than truly reflecting epistemic uncertainty.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that LMs are highly sensitive to epistemic markers in prompts, with accuracies varying more than 80%, and expressions of high certainty result in a 7% decrease in accuracy as compared to low certainty expressions; similarly, factive verbs hurt performance, while evidentials benefit performance."
            },
            "score": 6
        },
        {
            "id": "3b451fa663704f927e1ec602d7c0845a9826922d",
            "paperId": "3b451fa663704f927e1ec602d7c0845a9826922d",
            "title": "Evaluating the Robustness of Neural Language Models to Input Perturbations",
            "abstract": "High-performance neural language models have obtained state-of-the-art results on a wide range of Natural Language Processing (NLP) tasks. However, results for common benchmark datasets often do not reflect model reliability and robustness when applied to noisy, real-world data. In this study, we design and implement various types of character-level and word-level perturbation methods to simulate realistic scenarios in which input texts may be slightly noisy or different from the data distribution on which NLP systems were trained. Conducting comprehensive experiments on different NLP tasks, we investigate the ability of high-performance language models such as BERT, XLNet, RoBERTa, and ELMo in handling different types of input perturbations. The results suggest that language models are sensitive to input perturbations and their performance can decrease even when small changes are introduced. We highlight that models need to be further improved and that current benchmarks are not reflecting model robustness well. We argue that evaluations on perturbed inputs should routinely complement widely-used benchmarks in order to yield a more realistic understanding of NLP systems\u2019 robustness.",
            "year": 2021,
            "citationCount": 65,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study designs and implements various types of character-level and word-level perturbation methods to simulate realistic scenarios in which input texts may be slightly noisy or different from the data distribution on which NLP systems were trained."
            },
            "score": 6
        },
        {
            "id": "268ba07f529df6a7f20998bb2cf26b16b31709c8",
            "paperId": "268ba07f529df6a7f20998bb2cf26b16b31709c8",
            "title": "Self-Supervised Contrastive Learning with Adversarial Perturbations for Robust Pretrained Language Models",
            "abstract": "In this paper, we present an approach to im- 001 prove the robustness of BERT language mod- 002 els against word substitution-based adversar- 003 ial attacks by leveraging adversarial perturba- 004 tions for self-supervised contrastive learning. 005 We create an ef\ufb01cient word-level adversarial 006 attack, and use it to \ufb01netune BERT on ad- 007 versarial examples generated on the \ufb02y during 008 training. In contrast with previous works, our 009 method improves model robustness without us- 010 ing any labeled data. Experimental results 011 show that our method improves robustness of 012 BERT against four different word substitution- 013 based adversarial attacks, and combining our 014 method with adversarial training gives higher 015 robustness than adversarial training alone. As 016 our method improves the robustness of BERT 017 purely with unlabeled data, it opens up the pos- 018 sibility of using large text datasets to train ro- 019 bust language models. 020",
            "year": 2021,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An approach to prove the robustness of BERT language models against word substitution-based adversarial attacks by leveraging adversarial perturbation for self-supervised contrastive learning."
            },
            "score": 6
        },
        {
            "id": "9ad167529a6365e37825ddea5d29ab2f17651959",
            "paperId": "9ad167529a6365e37825ddea5d29ab2f17651959",
            "title": "Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models",
            "abstract": "We propose the Data Contamination Quiz (DCQ), a simple and effective approach to detect data contamination in large language models (LLMs) and estimate the amount of it. Specifically, we frame data contamination detection as a series of multiple-choice questions and devise a quiz format wherein three perturbed versions of each dataset instance are created. These changes only include word-level perturbations. The generated perturbed versions, along with the original instance, form the options in the DCQ, with an extra option accommodating the possibility that none of the provided choices is correct. Given that the only distinguishing signal among the choices is the exact wording relative to the original instance, an LLM, when tasked with identifying the original instance from the choices, gravitates towards the original one if it has been exposed to it in its pre-training phase--a trait intrinsic to LLMs. Tested over several datasets with GPT-4/3.5, our findings--while fully lacking access to LLMs' pre-training data and internal parameters--suggest that DCQ uncovers greater contamination levels compared to existing detection methods and proficiently bypasses more safety filters, especially those set to avoid generating copyrighted contents.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 6
        },
        {
            "id": "5d437f98b7f3532ba693f13744427e87d61ee952",
            "paperId": "5d437f98b7f3532ba693f13744427e87d61ee952",
            "title": "In and Out-of-Domain Text Adversarial Robustness via Label Smoothing",
            "abstract": "Recently it has been shown that state-of-the-art NLP models are vulnerable to adversarial attacks, where the predictions of a model can be drastically altered by slight modifications to the input (such as synonym substitutions). While several defense techniques have been proposed, and adapted, to the discrete nature of text adversarial attacks, the benefits of general-purpose regularization methods such as label smoothing for language models, have not been studied. In this paper, we study the adversarial robustness provided by label smoothing strategies in foundational models for diverse NLP tasks in both in-domain and out-of-domain settings. Our experiments show that label smoothing significantly improves adversarial robustness in pre-trained models like BERT, against various popular attacks. We also analyze the relationship between prediction confidence and robustness, showing that label smoothing reduces over-confident errors on adversarial examples.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experiments show that label smoothing significantly improves adversarial robustness in pre-trained models like BERT, against various popular attacks, and the relationship between prediction confidence and robustness is analyzed, showing thatlabel smoothing reduces over-confident errors on adversarial examples."
            },
            "score": 6
        },
        {
            "id": "2c72ab10e7a5f2fd32e6f85b20c77bf64e6e220d",
            "paperId": "2c72ab10e7a5f2fd32e6f85b20c77bf64e6e220d",
            "title": "A prompt-based approach to adversarial example generation and robustness enhancement",
            "abstract": null,
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel robust training approach based on prompt paradigm which incorporates prompt texts as the alternatives to adversarial examples and enhances robustness under a lightweight minimax-style optimization framework is proposed."
            },
            "score": 6
        },
        {
            "id": "de2fd685f45ee916b9142bcb983d306b7da643a4",
            "paperId": "de2fd685f45ee916b9142bcb983d306b7da643a4",
            "title": "A Prompting-based Approach for Adversarial Example Generation and Robustness Enhancement",
            "abstract": "Recent years have seen the wide application of NLP models in crucial areas such as finance, medical treatment, and news media, raising concerns of the model robustness and vulnerabilities. In this paper, we propose a novel prompt-based adversarial attack to compromise NLP models and robustness enhancement technique. We first construct malicious prompts for each instance and generate adversarial examples via mask-and-filling under the effect of a malicious purpose. Our attack technique targets the inherent vulnerabilities of NLP models, allowing us to generate samples even without interacting with the victim NLP model, as long as it is based on pre-trained language models (PLMs). Furthermore, we design a prompt-based adversarial training method to improve the robustness of PLMs. As our training method does not actually generate adversarial samples, it can be applied to large-scale training sets efficiently. The experimental results show that our attack method can achieve a high attack success rate with more diverse, fluent and natural adversarial examples. In addition, our robustness enhancement method can significantly improve the robustness of models to resist adversarial attacks. Our work indicates that prompting paradigm has great potential in probing some fundamental flaws of PLMs and fine-tuning them for downstream tasks.",
            "year": 2022,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel prompt-based adversarial attack to compromise NLP models and robustness enhancement technique that can significantly improve the robustness of models to resist adversarial attacks and indicates that prompting paradigm has great potential in probing some fundamental flaws of PLMs and fine-tuning them for downstream tasks."
            },
            "score": 6
        },
        {
            "id": "02819ebba2c60fb9ba4e5fccee3be6db38c52409",
            "paperId": "02819ebba2c60fb9ba4e5fccee3be6db38c52409",
            "title": "Towards Stronger Adversarial Baselines Through Human-AI Collaboration",
            "abstract": "Natural language processing (NLP) systems are often used for adversarial tasks such as detecting spam, abuse, hate speech, and fake news. Properly evaluating such systems requires dynamic evaluation that searches for weaknesses in the model, rather than a static test set. Prior work has evaluated such models on both manually and automatically generated examples, but both approaches have limitations: manually constructed examples are time-consuming to create and are limited by the imagination and intuition of the creators, while automatically constructed examples are often ungrammatical or labeled inconsistently. We propose to combine human and AI expertise in generating adversarial examples, benefiting from humans\u2019 expertise in language and automated attacks\u2019 ability to probe the target system more quickly and thoroughly. We present a system that facilitates attack construction, combining human judgment with automated attacks to create better attacks more efficiently. Preliminary results from our own experimentation suggest that human-AI hybrid attacks are more effective than either human-only or AI-only attacks. A complete user study to validate these hypotheses is still pending.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents a system that facilitates attack construction, combining human judgment with automated attacks to create better attacks more efficiently, and suggests that human-AI hybrid attacks are more effective than either human-only or AI-only attacks."
            },
            "score": 6
        },
        {
            "id": "74e9053d6f44f4507bd40bbea999ee65f0cbefb2",
            "paperId": "74e9053d6f44f4507bd40bbea999ee65f0cbefb2",
            "title": "Pathologies of Neural Models Make Interpretations Difficult",
            "abstract": "One way to interpret neural model predictions is to highlight the most important input features\u2014for example, a heatmap visualization over the words in an input sentence. In existing interpretation methods for NLP, a word\u2019s importance is determined by either input perturbation\u2014measuring the decrease in model confidence when that word is removed\u2014or by the gradient with respect to that word. To understand the limitations of these methods, we use input reduction, which iteratively removes the least important word from the input. This exposes pathological behaviors of neural models: the remaining words appear nonsensical to humans and are not the ones determined as important by interpretation methods. As we confirm with human experiments, the reduced examples lack information to support the prediction of any label, but models still make the same predictions with high confidence. To explain these counterintuitive results, we draw connections to adversarial examples and confidence calibration: pathological behaviors reveal difficulties in interpreting neural models trained with maximum likelihood. To mitigate their deficiencies, we fine-tune the models by encouraging high entropy outputs on reduced examples. Fine-tuned models become more interpretable under input reduction, without accuracy loss on regular examples.",
            "year": 2018,
            "citationCount": 290,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work uses input reduction, which iteratively removes the least important word from the input, to expose pathological behaviors of neural models: the remaining words appear nonsensical to humans and are not the ones determined as important by interpretation methods."
            },
            "score": 5
        },
        {
            "id": "47030369e97cc44d4b2e3cf1be85da0fd134904a",
            "paperId": "47030369e97cc44d4b2e3cf1be85da0fd134904a",
            "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
            "abstract": "Because\"out-of-the-box\"large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called\"jailbreaks\"against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods. Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.",
            "year": 2023,
            "citationCount": 386,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information."
            },
            "score": 5
        },
        {
            "id": "8ecdbfe011b7189fa0ee49ffc4e42a93d728a371",
            "paperId": "8ecdbfe011b7189fa0ee49ffc4e42a93d728a371",
            "title": "On Evaluating Adversarial Robustness of Large Vision-Language Models",
            "abstract": "Large vision-language models (VLMs) such as GPT-4 have achieved unprecedented performance in response generation, especially with visual inputs, enabling more creative and adaptable interaction than large language models such as ChatGPT. Nonetheless, multimodal generation exacerbates safety concerns, since adversaries may successfully evade the entire system by subtly manipulating the most vulnerable modality (e.g., vision). To this end, we propose evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses. In particular, we first craft targeted adversarial examples against pretrained models such as CLIP and BLIP, and then transfer these adversarial examples to other VLMs such as MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt. In addition, we observe that black-box queries on these VLMs can further improve the effectiveness of targeted evasion, resulting in a surprisingly high success rate for generating targeted responses. Our findings provide a quantitative understanding regarding the adversarial vulnerability of large VLMs and call for a more thorough examination of their potential security flaws before deployment in practice. Code is at https://github.com/yunqing-me/AttackVLM.",
            "year": 2023,
            "citationCount": 47,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses is proposed."
            },
            "score": 5
        },
        {
            "id": "92b9d8b8c81c4c53ea62000c0924500b2dd11bce",
            "paperId": "92b9d8b8c81c4c53ea62000c0924500b2dd11bce",
            "title": "Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models",
            "abstract": "We introduce new jailbreak attacks on vision language models (VLMs), which use aligned LLMs and are resilient to text-only jailbreak attacks. Specifically, we develop cross-modality attacks on alignment where we pair adversarial images going through the vision encoder with textual prompts to break the alignment of the language model. Our attacks employ a novel compositional strategy that combines an image, adversarially targeted towards toxic embeddings, with generic prompts to accomplish the jailbreak. Thus, the LLM draws the context to answer the generic prompt from the adversarial image. The generation of benign-appearing adversarial images leverages a novel embedding-space-based methodology, operating with no access to the LLM model. Instead, the attacks require access only to the vision encoder and utilize one of our four embedding space targeting strategies. By not requiring access to the LLM, the attacks lower the entry barrier for attackers, particularly when vision encoders such as CLIP are embedded in closed-source LLMs. The attacks achieve a high success rate across different VLMs, highlighting the risk of cross-modality alignment vulnerabilities, and the need for new alignment approaches for multi-modal models.",
            "year": 2023,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Cross-modality attacks on alignment where adversarial images going through the vision encoder with textual prompts to break the alignment of the language model are developed."
            },
            "score": 5
        },
        {
            "id": "03018253a3a3c237519ed3715098ac22464ab858",
            "paperId": "03018253a3a3c237519ed3715098ac22464ab858",
            "title": "Test-time Augmentation for Factual Probing",
            "abstract": "Factual probing is a method that uses prompts to test if a language model\"knows\"certain world knowledge facts. A problem in factual probing is that small changes to the prompt can lead to large changes in model output. Previous work aimed to alleviate this problem by optimizing prompts via text mining or fine-tuning. However, such approaches are relation-specific and do not generalize to unseen relation types. Here, we propose to use test-time augmentation (TTA) as a relation-agnostic method for reducing sensitivity to prompt variations by automatically augmenting and ensembling prompts at test time. Experiments show improved model calibration, i.e., with TTA, model confidence better reflects prediction accuracy. Improvements in prediction accuracy are observed for some models, but for other models, TTA leads to degradation. Error analysis identifies the difficulty of producing high-quality prompt variations as the main challenge for TTA.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes to use test-time augmentation (TTA) as a relation-agnostic method for reducing sensitivity to prompt variations by automatically augmenting and ensembling prompts at test time."
            },
            "score": 5
        },
        {
            "id": "b93ac10de176c4a7aaa2cc652b90bb25636532cd",
            "paperId": "b93ac10de176c4a7aaa2cc652b90bb25636532cd",
            "title": "Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation",
            "abstract": "This paper presents a benchmark self-evolving framework to dynamically evaluate rapidly advancing Large Language Models (LLMs), aiming for a more accurate assessment of their capabilities and limitations. We utilize a multi-agent system to manipulate the context or question of original instances, reframing new evolving instances with high confidence that dynamically extend existing benchmarks. Towards a more scalable, robust and fine-grained evaluation, we implement six reframing operations to construct evolving instances testing LLMs against diverse queries, data noise and probing their problem-solving sub-abilities. With this framework, we extend benchmark datasets of four tasks. Experimental results show a general performance decline in most LLMs against their original results. This decline under our scalable and robust evaluations, alongside our fine-grained evaluation, more accurately reflect models' capabilities. Besides, our framework widens performance discrepancies both between different models and within the same model across various tasks, facilitating more informed model selection for specific tasks (Code and data are available at https://github.com/NanshineLoong/Self-Evolving-Benchmark).",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A benchmark self-evolving framework to dynamically evaluate rapidly advancing Large Language Models, utilizing a multi-agent system to manipulate the context or question of original instances, reframing new evolving instances with high confidence that dynamically extend existing benchmarks."
            },
            "score": 5
        },
        {
            "id": "c3fb6056ca1ec3f7cfe57103712531fdbfe69e03",
            "paperId": "c3fb6056ca1ec3f7cfe57103712531fdbfe69e03",
            "title": "Syntactic Perturbations Reveal Representational Correlates of Hierarchical Phrase Structure in Pretrained Language Models",
            "abstract": "While vector-based language representations from pretrained language models have set a new standard for many NLP tasks, there is not yet a complete accounting of their inner workings. In particular, it is not entirely clear what aspects of sentence-level syntax are captured by these representations, nor how (if at all) they are built along the stacked layers of the network. In this paper, we aim to address such questions with a general class of interventional, input perturbation-based analyses of representations from pretrained language models. Importing from computational and cognitive neuroscience the notion of representational invariance, we perform a series of probes designed to test the sensitivity of these representations to several kinds of structure in sentences. Each probe involves swapping words in a sentence and comparing the representations from perturbed sentences against the original. We experiment with three different perturbations: (1) random permutations of n-grams of varying width, to test the scale at which a representation is sensitive to word position; (2) swapping of two spans which do or do not form a syntactic phrase, to test sensitivity to global phrase structure; and (3) swapping of two adjacent words which do or do not break apart a syntactic phrase, to test sensitivity to local phrase structure. Results from these probes collectively suggest that Transformers build sensitivity to larger parts of the sentence along their layers, and that hierarchical phrase structure plays a role in this process. More broadly, our results also indicate that structured input perturbations widens the scope of analyses that can be performed on often-opaque deep learning systems, and can serve as a complement to existing tools (such as supervised linear probes) for interpreting complex black-box models.",
            "year": 2021,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Results from a series of probes designed to test the sensitivity of vector-based language representations from pretrained language models suggest that Transformers build sensitivity to larger parts of the sentence along their layers, and that hierarchical phrase structure plays a role in this process."
            },
            "score": 5
        },
        {
            "id": "e5397b37de8d6d53f0328436913a4a6827d501a6",
            "paperId": "e5397b37de8d6d53f0328436913a4a6827d501a6",
            "title": "FairPair: A Robust Evaluation of Biases in Language Models through Paired Perturbations",
            "abstract": "The accurate evaluation of differential treatment in language models to specific groups is critical to ensuring a positive and safe user experience. An ideal evaluation should have the properties of being robust, extendable to new groups or attributes, and being able to capture biases that appear in typical usage (rather than just extreme, rare cases). Relatedly, bias evaluation should surface not only egregious biases but also ones that are subtle and commonplace, such as a likelihood for talking about appearances with regard to women. We present FairPair, an evaluation framework for assessing differential treatment that occurs during ordinary usage. FairPair operates through counterfactual pairs, but crucially, the paired continuations are grounded in the same demographic group, which ensures equivalent comparison. Additionally, unlike prior work, our method factors in the inherent variability that comes from the generation process itself by measuring the sampling variability. We present an evaluation of several commonly used generative models and a qualitative analysis that indicates a preference for discussing family and hobbies with regard to women.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents FairPair, an evaluation framework for assessing differential treatment that occurs during ordinary usage, and presents an evaluation of several commonly used generative models and a qualitative analysis that indicates a preference for discussing family and hobbies with regard to women."
            },
            "score": 5
        },
        {
            "id": "2de50c0f27cdcc6cb000f3b67825d95f7d1ed2c5",
            "paperId": "2de50c0f27cdcc6cb000f3b67825d95f7d1ed2c5",
            "title": "The Sensitivity of Language Models and Humans to Winograd Schema Perturbations",
            "abstract": "Large-scale pretrained language models are the major driving force behind recent improvements in perfromance on the Winograd Schema Challenge, a widely employed test of commonsense reasoning ability. We show, however, with a new diagnostic dataset, that these models are sensitive to linguistic perturbations of the Winograd examples that minimally affect human understanding. Our results highlight interesting differences between humans and language models: language models are more sensitive to number or gender alternations and synonym replacements than humans, and humans are more stable and consistent in their predictions, maintain a much higher absolute performance, and perform better on non-associative instances than associative ones.",
            "year": 2020,
            "citationCount": 32,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Results highlight interesting differences between humans and language models: language models are more sensitive to number or gender alternations and synonym replacements than humans, and humans are more stable and consistent in their predictions, maintain a much higher absolute performance, and perform better on non-associative instances than associative ones."
            },
            "score": 5
        },
        {
            "id": "a1c02351d6bc689150546bcb6829e0c219a305c9",
            "paperId": "a1c02351d6bc689150546bcb6829e0c219a305c9",
            "title": "Whispers of Doubt Amidst Echoes of Triumph in NLP Robustness",
            "abstract": "Do larger and more performant models resolve NLP's longstanding robustness issues? We investigate this question using over 20 models of different sizes spanning different architectural choices and pretraining objectives. We conduct evaluations using (a) out-of-domain and challenge test sets, (b) behavioral testing with CheckLists, (c) contrast sets, and (d) adversarial inputs. Our analysis reveals that not all out-of-domain tests provide insight into robustness. Evaluating with CheckLists and contrast sets shows significant gaps in model performance; merely scaling models does not make them adequately robust. Finally, we point out that current approaches for adversarial evaluations of models are themselves problematic: they can be easily thwarted, and in their current forms, do not represent a sufficiently deep probe of model robustness. We conclude that not only is the question of robustness in NLP as yet unresolved, but even some of the approaches to measure robustness need to be reassessed.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is concluded that not only is the question of robustness in NLP as yet unresolved, but even some of the approaches to measure robustness need to be reassessed."
            },
            "score": 5
        },
        {
            "id": "eb5629efdd4fb2fc195e91a53d5bafbbea696f37",
            "paperId": "eb5629efdd4fb2fc195e91a53d5bafbbea696f37",
            "title": "GENERATING ADVERSARIAL EXAMPLES IN TEXT CLASSIFICATION",
            "abstract": "In this paper, we implement several effective strategies to generate adversarial examples for the text classification task. Specifically, these strategies include occluding, replacing and deleting characters or words based on the final confidence (black box attack) ?, and based on the gradients of inputs to choose our modified target (white box attack) ?. These methods are applied to different kinds of text classification models, include character level CNN (char-CNN) ?, word-level CNN (word-CNN) ? and LSTM with Attention (LSTM) ?. The results of the experiments reveal that our strategies can effectively attack Deep Neural Network models with different architectures through imperceptible disturbances of human beings, which means within a small editing distance. On the other hand, through the analysis of statistical data, we also found that the sensitivity of different models to specific attack methods may vary. These conclusions allow us to design more adversarial examples based on the model structures and improve the performance of NLP tasks more efficiently.",
            "year": 2019,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results of the experiments reveal that the strategies implemented can effectively attack Deep Neural Network models with different architectures through imperceptible disturbances of human beings, which means within a small editing distance."
            },
            "score": 5
        },
        {
            "id": "4b5fefaccd9153da9895f69ee3ec7ce6c0b747d0",
            "paperId": "4b5fefaccd9153da9895f69ee3ec7ce6c0b747d0",
            "title": "Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications",
            "abstract": "Large-scale pre-trained language models have shown outstanding performance in a variety of NLP tasks. However, they are also known to be significantly brittle against specifically crafted adversarial examples, leading to increasing interest in probing the adversarial robustness of NLP systems. We introduce RSMI, a novel two-stage framework that combines randomized smoothing (RS) with masked inference (MI) to improve the adversarial robustness of NLP systems. RS transforms a classifier into a smoothed classifier to obtain robust representations, whereas MI forces a model to exploit the surrounding context of a masked token in an input sequence. RSMI improves adversarial robustness by 2 to 3 times over existing state-of-the-art methods on benchmark datasets. We also perform in-depth qualitative analysis to validate the effectiveness of the different stages of RSMI and probe the impact of its components through extensive ablations. By empirically proving the stability of RSMI, we put it forward as a practical method to robustly train large-scale NLP models. Our code and datasets are available at https://github.com/Han8931/rsmi_nlp",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work introduces RSMI, a novel two-stage framework that combines randomized smoothing (RS) with masked inference (MI) to improve the adversarial robustness of NLP systems."
            },
            "score": 5
        },
        {
            "id": "784f04fdac3bd065ea901a95d7f99907e0507128",
            "paperId": "784f04fdac3bd065ea901a95d7f99907e0507128",
            "title": "PROSAC: Provably Safe Certification for Machine Learning Models under Adversarial Attacks",
            "abstract": "It is widely known that state-of-the-art machine learning models, including vision and language models, can be seriously compromised by adversarial perturbations. It is therefore increasingly relevant to develop capabilities to certify their performance in the presence of the most effective adversarial attacks. Our paper offers a new approach to certify the performance of machine learning models in the presence of adversarial attacks with population level risk guarantees. In particular, we introduce the notion of $(\\alpha,\\zeta)$ machine learning model safety. We propose a hypothesis testing procedure, based on the availability of a calibration set, to derive statistical guarantees providing that the probability of declaring that the adversarial (population) risk of a machine learning model is less than $\\alpha$ (i.e. the model is safe), while the model is in fact unsafe (i.e. the model adversarial population risk is higher than $\\alpha$), is less than $\\zeta$. We also propose Bayesian optimization algorithms to determine efficiently whether a machine learning model is $(\\alpha,\\zeta)$-safe in the presence of an adversarial attack, along with statistical guarantees. We apply our framework to a range of machine learning models including various sizes of vision Transformer (ViT) and ResNet models impaired by a variety of adversarial attacks, such as AutoAttack, SquareAttack and natural evolution strategy attack, to illustrate the operation of our approach. Importantly, we show that ViT's are generally more robust to adversarial attacks than ResNets, and ViT-large is more robust than smaller models. Our approach goes beyond existing empirical adversarial risk-based certification guarantees. It formulates rigorous (and provable) performance guarantees that can be used to satisfy regulatory requirements mandating the use of state-of-the-art technical tools.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper offers a new approach to certify the performance of machine learning models in the presence of adversarial attacks with population level risk guarantees, and introduces the notion of $(\\alpha,\\zeta)$ machine learning model safety."
            },
            "score": 4
        },
        {
            "id": "c7091540c1fa77f1c6b27482f349330f8e559d6f",
            "paperId": "c7091540c1fa77f1c6b27482f349330f8e559d6f",
            "title": "Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks",
            "abstract": "We consider the questions of whether or not large language models (LLMs) have beliefs, and, if they do, how we might measure them. First, we evaluate two existing approaches, one due to Azaria and Mitchell (2023) and the other to Burns et al. (2022). We provide empirical results that show that these methods fail to generalize in very basic ways. We then argue that, even if LLMs have beliefs, these methods are unlikely to be successful for conceptual reasons. Thus, there is still no lie-detector for LLMs. After describing our empirical results we take a step back and consider whether or not we should expect LLMs to have something like beliefs in the first place. We consider some recent arguments aiming to show that LLMs cannot have beliefs. We show that these arguments are misguided. We provide a more productive framing of questions surrounding the status of beliefs in LLMs, and highlight the empirical nature of the problem. We conclude by suggesting some concrete paths for future work.",
            "year": 2023,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Empirical results show that, even if LLMs have beliefs, these methods are unlikely to be successful for conceptual reasons, and it is argued that there is still no lie-detector for LLMs."
            },
            "score": 4
        },
        {
            "id": "ccaff61e0c1e629d91d78f82a64b3cbc8f3f7023",
            "paperId": "ccaff61e0c1e629d91d78f82a64b3cbc8f3f7023",
            "title": "Robust Distortion-free Watermarks for Language Models",
            "abstract": "We propose a methodology for planting watermarks in text from an autoregressive language model that are robust to perturbations without changing the distribution over text up to a certain maximum generation budget. We generate watermarked text by mapping a sequence of random numbers -- which we compute using a randomized watermark key -- to a sample from the language model. To detect watermarked text, any party who knows the key can align the text to the random number sequence. We instantiate our watermark methodology with two sampling schemes: inverse transform sampling and exponential minimum sampling. We apply these watermarks to three language models -- OPT-1.3B, LLaMA-7B and Alpaca-7B -- to experimentally validate their statistical power and robustness to various paraphrasing attacks. Notably, for both the OPT-1.3B and LLaMA-7B models, we find we can reliably detect watermarked text ($p \\leq 0.01$) from $35$ tokens even after corrupting between $40$-$50\\%$ of the tokens via random edits (i.e., substitutions, insertions or deletions). For the Alpaca-7B model, we conduct a case study on the feasibility of watermarking responses to typical user instructions. Due to the lower entropy of the responses, detection is more difficult: around $25\\%$ of the responses -- whose median length is around $100$ tokens -- are detectable with $p \\leq 0.01$, and the watermark is also less robust to certain automated paraphrasing attacks we implement.",
            "year": 2023,
            "citationCount": 52,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 4
        },
        {
            "id": "bc372a09d1528f198d2cbb194cca71a3fc07fede",
            "paperId": "bc372a09d1528f198d2cbb194cca71a3fc07fede",
            "title": "Neighboring Perturbations of Knowledge Editing on Large Language Models",
            "abstract": "Despite their exceptional capabilities, large language models (LLMs) are prone to generating unintended text due to false or outdated knowledge. Given the resource-intensive nature of retraining LLMs, there has been a notable increase in the development of knowledge editing. However, current approaches and evaluations rarely explore the perturbation of editing on neighboring knowledge. This paper studies whether updating new knowledge to LLMs perturbs the neighboring knowledge encapsulated within them. Specifically, we seek to figure out whether appending a new answer into an answer list to a factual question leads to catastrophic forgetting of original correct answers in this list, as well as unintentional inclusion of incorrect answers. A metric of additivity is introduced and a benchmark dubbed as Perturbation Evaluation of Appending Knowledge (PEAK) is constructed to evaluate the degree of perturbation to neighboring knowledge when appending new knowledge. Besides, a plug-and-play framework termed Appending via Preservation and Prevention (APP) is proposed to mitigate the neighboring perturbation by maintaining the integrity of the answer list. Experiments demonstrate the effectiveness of APP coupling with four editing methods on three LLMs.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A metric of additivity is introduced and a benchmark dubbed as Perturbation Evaluation of Appending Knowledge (PEAK) is constructed to evaluate the degree of perturbation to neighboring knowledge when appending new knowledge."
            },
            "score": 4
        },
        {
            "id": "1882849855895456fe842203f245ffaf66b72eff",
            "paperId": "1882849855895456fe842203f245ffaf66b72eff",
            "title": "Bayesian low-rank adaptation for large language models",
            "abstract": "Low-rank adaptation (LoRA) has emerged as a new paradigm for cost-efficient fine-tuning of large language models (LLMs). However, fine-tuned LLMs often become overconfident especially when fine-tuned on small datasets. Bayesian methods, with their inherent ability to estimate uncertainty, serve as potent tools to mitigate overconfidence and enhance calibration. In this work, we introduce Laplace-LoRA, which applies a Bayesian approach to the LoRA parameters. Specifically, Laplace-LoRA applies a Laplace approximation to the posterior over the LoRA parameters, considerably improving the calibration of fine-tuned LLMs.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Laplace-LoRA applies a Laplace approximation to the posterior over the LoRA parameters, considerably improving the calibration of fine-tuned LLMs."
            },
            "score": 4
        },
        {
            "id": "71d68782c3da41b77866c2fd0cb65726f60b3af1",
            "paperId": "71d68782c3da41b77866c2fd0cb65726f60b3af1",
            "title": "Analyzing Chain-of-Thought Prompting in Large Language Models via Gradient-based Feature Attributions",
            "abstract": "Chain-of-thought (CoT) prompting has been shown to empirically improve the accuracy of large language models (LLMs) on various question answering tasks. While understanding why CoT prompting is effective is crucial to ensuring that this phenomenon is a consequence of desired model behavior, little work has addressed this; nonetheless, such an understanding is a critical prerequisite for responsible model deployment. We address this question by leveraging gradient-based feature attribution methods which produce saliency scores that capture the influence of input tokens on model output. Specifically, we probe several open-source LLMs to investigate whether CoT prompting affects the relative importances they assign to particular input tokens. Our results indicate that while CoT prompting does not increase the magnitude of saliency scores attributed to semantically relevant tokens in the prompt compared to standard few-shot prompting, it increases the robustness of saliency scores to question perturbations and variations in model output.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work probes several open-source LLMs to investigate whether CoT prompting affects the relative importances they assign to particular input tokens, and results indicate that while coT prompting does not increase the magnitude of saliency scores attributed to semantically relevant tokens in the prompt compared to standard few-shot prompting, it increases the robustness ofsaliency scores to question perturbations and variations in model output."
            },
            "score": 4
        },
        {
            "id": "f386006ec40ac5c2d92acc715ced86aa3b289644",
            "paperId": "f386006ec40ac5c2d92acc715ced86aa3b289644",
            "title": "Contextual Distortion Reveals Constituency: Masked Language Models are Implicit Parsers",
            "abstract": "Recent advancements in pre-trained language models (PLMs) have demonstrated that these models possess some degree of syntactic awareness. To leverage this knowledge, we propose a novel chart-based method for extracting parse trees from masked language models (LMs) without the need to train separate parsers. Our method computes a score for each span based on the distortion of contextual representations resulting from linguistic perturbations. We design a set of perturbations motivated by the linguistic concept of constituency tests, and use these to score each span by aggregating the distortion scores. To produce a parse tree, we use chart parsing to find the tree with the minimum score. Our method consistently outperforms previous state-of-the-art methods on English with masked LMs, and also demonstrates superior performance in a multilingual setting, outperforming the state-of-the-art in 6 out of 8 languages. Notably, although our method does not involve parameter updates or extensive hyperparameter search, its performance can even surpass some unsupervised parsing methods that require fine-tuning. Our analysis highlights that the distortion of contextual representation resulting from syntactic perturbation can serve as an effective indicator of constituency across languages.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel chart-based method for extracting parse trees from masked language models (LMs) without the need to train separate parsers, and shows that the distortion of contextual representation resulting from syntactic perturbation can serve as an effective indicator of constituency across languages."
            },
            "score": 4
        },
        {
            "id": "1a62a62a4d6bdbe2e1d5ae2c66165c691975b8e8",
            "paperId": "1a62a62a4d6bdbe2e1d5ae2c66165c691975b8e8",
            "title": "Adversary for Social Good: Leveraging Adversarial Attacks to Protect Personal Attribute Privacy",
            "abstract": "Social media has drastically reshaped the world that allows billions of people to engage in such interactive environments to conveniently create and share content with the public. Among them, text data (e.g., tweets, blogs) maintains the basic yet important social activities and generates a rich source of user-oriented information. While those explicit sensitive user data like credentials have been significantly protected by all means, personal private attribute (e.g., age, gender, location) disclosure due to inference attacks is somehow challenging to avoid, especially when powerful natural language processing (NLP) techniques have been effectively deployed to automate attribute inferences from implicit text data. This puts users\u2019 attribute privacy at risk. To address this challenge, in this article, we leverage the inherent vulnerability of machine learning to adversarial attacks, and design a novel text-space Adversarial attack for Social Good, called Adv4SG. In other words, we cast the problem of protecting personal attribute privacy as an adversarial attack formulation problem over the social media text data to defend against NLP-based attribute inference attacks. More specifically, Adv4SG proceeds with a sequence of word perturbations under given constraints such that the probed attribute cannot be identified correctly. Different from the prior works, we advance Adv4SG by considering social media property, and introducing cost-effective mechanisms to expedite attribute obfuscation over text data under the black-box setting. Extensive experiments on real-world social media datasets have demonstrated that our method can effectively degrade the inference accuracy with less computational cost over different attribute settings, which substantially helps mitigate the impacts of inference attacks and thus achieve high performance in user attribute privacy protection.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This article designs a novel text-space Adversarial attack for Social Good over the social media text data to defend against NLP-based attribute inference attacks, called Adv4SG, which substantially helps mitigate the impacts of inference attacks and thus achieve high performance in user attribute privacy protection."
            },
            "score": 4
        },
        {
            "id": "9871606b134fcf84db3cb30c906ce24c73298a7f",
            "paperId": "9871606b134fcf84db3cb30c906ce24c73298a7f",
            "title": "Multi-granularity Textual Adversarial Attack with Behavior Cloning",
            "abstract": "Recently, the textual adversarial attack models become increasingly popular due to their successful in estimating the robustness of NLP models. However, existing works have obvious deficiencies. (1)They usually consider only a single granularity of modification strategies (e.g. word-level or sentence-level), which is insufficient to explore the holistic textual space for generation; (2) They need to query victim models hundreds of times to make a successful attack, which is highly inefficient in practice. To address such problems, in this paper we propose MAYA, a Multi-grAnularitY Attack model to effectively generate high-quality adversarial samples with fewer queries to victim models. Furthermore, we propose a reinforcement-learning based method to train a multi-granularity attack agent through behavior cloning with the expert knowledge from our MAYA algorithm to further reduce the query times. Additionally, we also adapt the agent to attack black-box models that only output labels without confidence scores. We conduct comprehensive experiments to evaluate our attack models by attacking BiLSTM, BERT and RoBERTa in two different black-box attack settings and three benchmark datasets. Experimental results show that our models achieve overall better attacking performance and produce more fluent and grammatical adversarial samples compared to baseline models. Besides, our adversarial attack agent significantly reduces the query times in both attack settings. Our codes are released at https://github.com/Yangyi-Chen/MAYA.",
            "year": 2021,
            "citationCount": 25,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes MAYA, a Multi-grAnularitY Attack model to effectively generate high-quality adversarial samples with fewer queries to victim models and proposes a reinforcement-learning based method to train a multi-granularity attack agent through behavior cloning with the expert knowledge from the MAYA algorithm to further reduce the query times."
            },
            "score": 4
        },
        {
            "id": "80e34f2b7f816113130c536dddd8aa980c95dfd2",
            "paperId": "80e34f2b7f816113130c536dddd8aa980c95dfd2",
            "title": "Interpreting Predictions of NLP Models",
            "abstract": "Although neural NLP models are highly expressive and empirically successful, they also systematically fail in counterintuitive ways and are opaque in their decision-making process. This tutorial will provide a background on interpretation techniques, i.e., methods for explaining the predictions of NLP models. We will first situate example-specific interpretations in the context of other ways to understand models (e.g., probing, dataset analyses). Next, we will present a thorough study of example-specific interpretations, including saliency maps, input perturbations (e.g., LIME, input reduction), adversarial attacks, and influence functions. Alongside these descriptions, we will walk through source code that creates and visualizes interpretations for a diverse set of NLP tasks. Finally, we will discuss open problems in the field, e.g., evaluating, extending, and improving interpretation methods.",
            "year": 2020,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This tutorial will provide a background on interpretation techniques, i.e., methods for explaining the predictions of NLP models, and present a thorough study of example-specific interpretations, including saliency maps, input perturbations, and influence functions."
            },
            "score": 4
        },
        {
            "id": "46dfd46fb41d56bc77055e436dc969de8df0e76b",
            "paperId": "46dfd46fb41d56bc77055e436dc969de8df0e76b",
            "title": "Data augmentation and the role of hardness for feature learning in NLP",
            "abstract": ". Abstract Neural models often exploit features that generalize badly in order to achieve good performance. Overcoming this tendency is a central challenge in areas such as representation learning and ML fairness. We approach this problem, in part, from the perspective of feature hardness and data augmentation. First, we construct a dataset for linguistic acceptability in which multiple, competing features might be used for prediction. We \ufb01nd that in this setting, the downstream model \u2018prefers\u2019 \u2013 to some extent \u2013 the feature that is more easily extracted from the pre-trained model. OUr preliminary results suggest that the learning of downstream tasks in natural language processing (NLP) is governed, in part, by the \u2018clarity\u2019 with which features are represented by pre-trained models. Second, we introduce a toy setting to probe the e\ufb00ectiveness of data augmentation, a widely-used strategy to prevent models from learning undesirable heuristics. Adversarial or counterfactual data augmentation involves generating training examples where these heuristics fail, in order to encourage the model to use more general features. We show that, often, the added training examples help prevent the model from adopting the targeted heuristic, but do not help it learn more general features. We also \ufb01nd in many cases that the number of adversarial examples needed to reach a given error rate is independent of the amount of training data, and that adversarial data augmentation becomes less e\ufb00ective as the number of available heuristics increases and/or as the underlying learning problem becomes more challenging. Finally, we explore several de\ufb01nitions of feature hardness in the context of the same toy setting, including: (1) the area under the classi\ufb01cation learning curve, (2) the sum of weights of a classi\ufb01cation model, (3) the minimum description length given by the probing methods of Voita and Titov [2020], and (4) the number of adversarial counterexamples that are needed to induce a model to learn the feature. We show an correspondence between these de\ufb01nitions for the features in our toy setting.",
            "year": 2020,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Preliminary results suggest that the learning of downstream tasks in natural language processing (NLP) is governed, in part, by the \u2018clarity\u2019 with which features are represented by pre-trained models."
            },
            "score": 4
        },
        {
            "id": "ce177672b00ddf46e4906157a7e997ca9338b8b9",
            "paperId": "ce177672b00ddf46e4906157a7e997ca9338b8b9",
            "title": "Attention is not not Explanation",
            "abstract": "Attention mechanisms play a central role in NLP systems, especially within recurrent neural network (RNN) models. Recently, there has been increasing interest in whether or not the intermediate representations offered by these modules may be used to explain the reasoning for a model\u2019s prediction, and consequently reach insights regarding the model\u2019s decision-making process. A recent paper claims that \u2018Attention is not Explanation\u2019 (Jain and Wallace, 2019). We challenge many of the assumptions underlying this work, arguing that such a claim depends on one\u2019s definition of explanation, and that testing it needs to take into account all elements of the model. We propose four alternative tests to determine when/whether attention can be used as explanation: a simple uniform-weights baseline; a variance calibration based on multiple random seed runs; a diagnostic framework using frozen weights from pretrained models; and an end-to-end adversarial attention training protocol. Each allows for meaningful interpretation of attention mechanisms in RNN models. We show that even when reliable adversarial distributions can be found, they don\u2019t perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability.",
            "year": 2019,
            "citationCount": 727,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that even when reliable adversarial distributions can be found, they don\u2019t perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability."
            },
            "score": 3
        },
        {
            "id": "63e2740dc581b4186b4e277a9955e8048c414521",
            "paperId": "63e2740dc581b4186b4e277a9955e8048c414521",
            "title": "Large Language Models for Code: Security Hardening and Adversarial Testing",
            "abstract": "Large language models (large LMs) are increasingly trained on massive codebases and used to generate code. However, LMs lack awareness of security and are found to frequently produce unsafe code. This work studies the security of LMs along two important axes: (i) security hardening, which aims to enhance LMs' reliability in generating secure code, and (ii) adversarial testing, which seeks to evaluate LMs' security at an adversarial standpoint. We address both of these by formulating a new security task called controlled code generation. The task is parametric and takes as input a binary property to guide the LM to generate secure or unsafe code, while preserving the LM's capability of generating functionally correct code. We propose a novel learning-based approach called SVEN to solve this task. SVEN leverages property-specific continuous vectors to guide program generation towards the given property, without modifying the LM's weights. Our training procedure optimizes these continuous vectors by enforcing specialized loss terms on different regions of code, using a high-quality dataset carefully curated by us. Our extensive evaluation shows that SVEN is highly effective in achieving strong security control. For instance, a state-of-the-art CodeGen LM with 2.7B parameters generates secure code for 59.1% of the time. When we employ SVEN to perform security hardening (or adversarial testing) on this LM, the ratio is significantly boosted to 92.3% (or degraded to 36.8%). Importantly, SVEN closely matches the original LMs in functional correctness.",
            "year": 2023,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel learning-based approach called SVEN, which leverages property-specific continuous vectors to guide program generation towards the given property, without modifying the LM's weights, and closely matches the original LMs in functional correctness."
            },
            "score": 3
        },
        {
            "id": "782a50a48ba5d32839631254285d989bfadfd193",
            "paperId": "782a50a48ba5d32839631254285d989bfadfd193",
            "title": "Interpretable Entity Representations through Large-Scale Typing",
            "abstract": "In standard methodology for natural language processing, entities in text are typically embedded in dense vector spaces with pre-trained models. The embeddings produced this way are effective when fed into downstream models, but they require end-task fine-tuning and are fundamentally difficult to interpret. In this paper, we present an approach to creating entity representations that are human readable and achieve high performance on entity-related tasks out of the box. Our representations are vectors whose values correspond to posterior probabilities over fine-grained entity types, indicating the confidence of a typing model\u2019s decision that the entity belongs to the corresponding type. We obtain these representations using a fine-grained entity typing model, trained either on supervised ultra-fine entity typing data (Choi et al. 2018) or distantly-supervised examples from Wikipedia. On entity probing tasks involving recognizing entity identity, our embeddings used in parameter-free downstream models achieve competitive performance with ELMo- and BERT-based embeddings in trained models. We also show that it is possible to reduce the size of our type set in a learning-based way for particular domains. Finally, we show that these embeddings can be post-hoc modified through a small number of rules to incorporate domain knowledge and improve performance.",
            "year": 2020,
            "citationCount": 19,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents an approach to creating entity representations that are human readable and achieve high performance on entity-related tasks out of the box, and shows that these embeddings can be post-hoc modified through a small number of rules to incorporate domain knowledge and improve performance."
            },
            "score": 3
        },
        {
            "id": "b2c68b708a9f98996b18c8d21b53a815a2c46a8b",
            "paperId": "b2c68b708a9f98996b18c8d21b53a815a2c46a8b",
            "title": "ProPILE: Probing Privacy Leakage in Large Language Models",
            "abstract": "The rapid advancement and widespread use of large language models (LLMs) have raised significant concerns regarding the potential leakage of personally identifiable information (PII). These models are often trained on vast quantities of web-collected data, which may inadvertently include sensitive personal data. This paper presents ProPILE, a novel probing tool designed to empower data subjects, or the owners of the PII, with awareness of potential PII leakage in LLM-based services. ProPILE lets data subjects formulate prompts based on their own PII to evaluate the level of privacy intrusion in LLMs. We demonstrate its application on the OPT-1.3B model trained on the publicly available Pile dataset. We show how hypothetical data subjects may assess the likelihood of their PII being included in the Pile dataset being revealed. ProPILE can also be leveraged by LLM service providers to effectively evaluate their own levels of PII leakage with more powerful prompts specifically tuned for their in-house models. This tool represents a pioneering step towards empowering the data subjects for their awareness and control over their own data on the web.",
            "year": 2023,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ProPILE lets data subjects formulate prompts based on their own PII to evaluate the level of privacy intrusion in LLMs, and can be leveraged by LLM service providers to effectively evaluate their own levels of PII leakage with more powerful prompts specifically tuned for their in-house models."
            },
            "score": 3
        },
        {
            "id": "cb78447bf7d9ef8f10381fa22823e6424f148ba5",
            "paperId": "cb78447bf7d9ef8f10381fa22823e6424f148ba5",
            "title": "Robustness Analysis of Video-Language Models Against Visual and Language Perturbations",
            "abstract": "Joint visual and language modeling on large-scale datasets has recently shown good progress in multi-modal tasks when compared to single modal learning. However, robustness of these approaches against real-world perturbations has not been studied. In this work, we perform the first extensive robustness study of video-language models against various real-world perturbations. We focus on text-to-video retrieval and propose two large-scale benchmark datasets, MSRVTT-P and YouCook2-P, which utilize 90 different visual and 35 different text perturbations. The study reveals some interesting initial findings from the studied models: 1) models are generally more susceptible when only video is perturbed as opposed to when only text is perturbed, 2) models that are pre-trained are more robust than those trained from scratch, 3) models attend more to scene and objects rather than motion and action. We hope this study will serve as a benchmark and guide future research in robust video-language learning. The benchmark introduced in this study along with the code and datasets is available at https://bit.ly/3CNOly4.",
            "year": 2022,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This is the first extensive robustness study of video-language models against various real-world perturbations and focuses on text-to-video retrieval and proposes two large-scale benchmark datasets, MSRVTT-P and YouCook2-P, which utilize 90 different visual and 35 different text perturbation."
            },
            "score": 3
        },
        {
            "id": "e301af8bf91b056a65b7659b6b72f271a8cb0630",
            "paperId": "e301af8bf91b056a65b7659b6b72f271a8cb0630",
            "title": "Adversarial Perturbations Augmented Language Models for Euphemism Identification",
            "abstract": "Euphemisms are mild words or expressions used instead of harsh or direct words while talking to someone to avoid discussing something unpleasant, embarrassing, or offensive. However, they are often ambiguous, thus making it a challenging task. The Third Workshop on Figurative Language Processing, colocated with EMNLP 2022 organized a shared task on Euphemism Detection to better understand euphemisms. We have used the adversarial augmentation technique to construct new data. This augmented data was then trained using two language models: BERT and longformer. To further enhance the overall performance, various combinations of the results obtained using longformer and BERT were passed through a voting ensembler. We achieved an F1 score of 71.5 using the combination of two adversarial longformers, two adversarial BERT, and one non-adversarial BERT.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work has used the adversarial augmentation technique to construct new data and augmented data was then trained using two language models: BERT and longformer to better understand euphemisms."
            },
            "score": 3
        },
        {
            "id": "47019e041ee30165a1dcc36ce9847561ade4df59",
            "paperId": "47019e041ee30165a1dcc36ce9847561ade4df59",
            "title": "Don\u2019t Let Discourse Confine Your Model: Sequence Perturbations for Improved Event Language Models",
            "abstract": "Event language models represent plausible sequences of events. Most existing approaches train autoregressive models on text, which successfully capture event co-occurrence but unfortunately constrain the model to follow the discourse order in which events are presented. Other domains may employ different discourse orders, and for many applications, we may care about different notions of ordering (e.g., temporal) or not care about ordering at all (e.g., when predicting related events in a schema). We propose a simple yet surprisingly effective strategy for improving event language models by perturbing event sequences so we can relax model dependence on text order. Despite generating completely synthetic event orderings, we show that this technique improves the performance of the event language models on both applications and out-of-domain events data.",
            "year": 2021,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a simple yet surprisingly effective strategy for improving event language models by perturbing event sequences so the model can relax model dependence on text order, and shows that this technique improves the performance of theevent language models on both applications and out-of-domain events data."
            },
            "score": 3
        },
        {
            "id": "a706d28b7f032a120d34d334dade674906630b63",
            "paperId": "a706d28b7f032a120d34d334dade674906630b63",
            "title": "Multilingual and Multilabel Emotion Recognition using Virtual Adversarial Training",
            "abstract": "Virtual Adversarial Training (VAT) has been effective in learning robust models under supervised and semi-supervised settings for both computer vision and NLP tasks. However, the efficacy of VAT for multilingual and multilabel emotion recognition has not been explored before. In this work, we explore VAT for multilabel emotion recognition with a focus on leveraging unlabelled data from different languages to improve the model performance. We perform extensive semi-supervised experiments on SemEval2018 multilabel and multilingual emotion recognition dataset and show performance gains of 6.2% (Arabic), 3.8% (Spanish) and 1.8% (English) over supervised learning with same amount of labelled data (10% of training data). We also improve the existing state-of-the-art by 7%, 4.5% and 1% (Jaccard Index) for Spanish, Arabic and English respectively and perform probing experiments for understanding the impact of different layers of the contextual models.",
            "year": 2021,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores VAT for multilabel emotion recognition with a focus on leveraging unlabelled data from different languages to improve the model performance, and performs extensive semi-supervised experiments on SemEval2018Multilabel and multilingual emotion recognition dataset."
            },
            "score": 3
        },
        {
            "id": "2ed12b067f603cc22fba22f3df8adb47fa8cf901",
            "paperId": "2ed12b067f603cc22fba22f3df8adb47fa8cf901",
            "title": "On Adversarial Vulnerability of PHM algorithms: An Initial Study",
            "abstract": "In almost all PHM applications, driving highest possible performance (prediction accuracy and robustness) of PHM models (fault detection, fault diagnosis and prognostics) has been the top development priority, since PHM models\u2019 performance directly impacts how much business value the PHM models can bring. However, recent research work in other domains, e.g., computer vision (CV), has shown that machine learning (ML) models, especially deep learning models, are vulnerable to adversarial attacks; that is, small deliberately-designed perturbations to the original samples can cause the model to make false predictions with high confidence. In fact, adversarial machine learning (AML) targeting security of ML algorithms against adversaries, has become an emerging ML topic and has attracted tremendous research attention in CV and NLP. \n\u00a0 \nYet, in the PHM community, not much attention has been paid to adversarial vulnerability or security of PHM models. We contend that the economic impact of adversarial attacks to a PHM model might be even bigger than that to hard perceptual problems and thus securing PHM models from adversarial attacks is as important as the PHM models themselves. Also, PHM models, since the data used by the models are primarily time-series sensor measurements, have their own unique characteristics and deserve special attention in securing them. \u00a0 \n\u00a0 \nIn this paper we attempt to explore the adversarial vulnerability of PHM models by conducting an initial case study. More specifically, we consider several unique characteristics associated with streaming time-series sensor measurements data in developing attack strategies for attacking PHM models. We hope our initial study here can shed some light on and stimulate more research interests in the area of PHM models\u2019 security.",
            "year": 2021,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper attempts to explore the adversarial vulnerability of PHM models by conducting an initial case study and considers several unique characteristics associated with streaming time-series sensor measurements data in developing attack strategies for attackingPHM models."
            },
            "score": 3
        },
        {
            "id": "c02a6062efa95ac85077886da3219abaaa233c48",
            "paperId": "c02a6062efa95ac85077886da3219abaaa233c48",
            "title": "Can Edge Probing Tests Reveal Linguistic Knowledge in QA Models?",
            "abstract": "There have been many efforts to try to understand what grammatical knowledge (e.g., ability to understand the part of speech of a token) is encoded in large pre-trained language models (LM). This is done through \u2018Edge Probing\u2019 (EP) tests: supervised classification tasks to predict the grammatical properties of a span (whether it has a particular part of speech) using only the token representations coming from the LM encoder. However, most NLP applications fine-tune these LM encoders for specific tasks. Here, we ask: if an LM is fine-tuned, does the encoding of linguistic information in it change, as measured by EP tests? Specifically, we focus on the task of Question Answering (QA) and conduct experiments on multiple datasets. We find that EP test results do not change significantly when the fine-tuned model performs well or in adversarial situations where the model is forced to learn wrong correlations. From a similar finding, some recent papers conclude that fine-tuning does not change linguistic knowledge in encoders but they do not provide an explanation. We find that EP models are susceptible to exploiting spurious correlations in the EP datasets. When this dataset bias is corrected, we do see an improvement in the EP test results as expected.",
            "year": 2021,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work focuses on the task of Question Answering (QA) and finds that EP models are susceptible to exploiting spurious correlations in the EP datasets, and investigates how this dataset bias is corrected."
            },
            "score": 3
        },
        {
            "id": "478da72cc55ce079681c5cfaf3165bd3bd085b6b",
            "paperId": "478da72cc55ce079681c5cfaf3165bd3bd085b6b",
            "title": "Can Edge Probing Tasks Reveal Linguistic Knowledge in QA Models?",
            "abstract": "There have been many efforts to try to understand what grammatical knowledge (e.g., ability to understand the part of speech of a token) is encoded in large pre-trained language models (LM). This is done through \u2018Edge Probing\u2019 (EP) tests: simple ML models that predict the grammatical properties of a span (whether it has a particular part of speech) using only the LM\u2019s token representations. However, most NLP applications use \ufb01ne-tuned LMs. Here, we ask: if a LM is \ufb01ne-tuned, does the encoding of linguistic information in it change, as measured by EP tests? Conducting experiments on multiple question-answering (QA) datasets, we answer that question negatively: the EP test results do not change signi\ufb01cantly when the \ufb01ne-tuned QA model performs well or in adversarial situations where the model is forced to learn wrong correlations. However, a critical analysis of the EP task datasets reveals that EP models may rely on spurious correlations to make predictions. This indicates even if \ufb01ne-tuning changes the encoding of such knowledge, the EP tests might fail to measure it.",
            "year": 2021,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A critical analysis of the EP task datasets reveals that EP models may rely on spurious correlations to make predictions, which indicates even if \ufb01ne-tuning changes the encoding of such knowledge, the EP tests might fail to measure it."
            },
            "score": 3
        },
        {
            "id": "61f329722cd94291898c2c8131606a55f7a07219",
            "paperId": "61f329722cd94291898c2c8131606a55f7a07219",
            "title": "Broken Neural Scaling Laws",
            "abstract": "We present a smoothly broken power law functional form (that we refer to as a Broken Neural Scaling Law (BNSL)) that accurately models&extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as amount of compute used for training (or inference), number of model parameters, training dataset size, model input size, number of training steps, or upstream performance varies) for various architectures&for each of various tasks within a large&diverse set of upstream&downstream tasks, in zero-shot, prompted,&finetuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, AI capabilities, robotics, out-of-distribution (OOD) generalization, continual learning, transfer learning, uncertainty estimation / calibration, OOD detection, adversarial robustness, distillation, sparsity, retrieval, quantization, pruning, fairness, molecules, computer programming/coding, math word problems,\"emergent phase transitions\", arithmetic, supervised learning, unsupervised/self-supervised learning,&reinforcement learning (single agent&multi-agent). When compared to other functional forms for neural scaling, this functional form yields extrapolations of scaling behavior that are considerably more accurate on this set. Moreover, this functional form accurately models&extrapolates scaling behavior that other functional forms are incapable of expressing such as the nonmonotonic transitions present in the scaling behavior of phenomena such as double descent&the delayed, sharp inflection points present in the scaling behavior of tasks such as arithmetic. Lastly, we use this functional form to glean insights about the limit of the predictability of scaling behavior. Code is available at https://github.com/ethancaballero/broken_neural_scaling_laws",
            "year": 2022,
            "citationCount": 42,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A smoothly broken power law functional form that accurately models and extrapolates scaling behavior that other functional forms are incapable of expressing such as the nonmonotonic transitions present in the scaling behavior of phenomena such as double descent and the delayed, sharp inflection points present inThe limit of the predictability of scaling behavior is gleaned."
            },
            "score": 2
        },
        {
            "id": "c435ecd0321dcec1f25e458bf930311f9e1d04b6",
            "paperId": "c435ecd0321dcec1f25e458bf930311f9e1d04b6",
            "title": "Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality",
            "abstract": "We present a novel task and dataset for evaluating the ability of vision and language models to conduct visio-linguistic compositional reasoning, which we call Winoground. Given two images and two captions, the goal is to match them correctly-but crucially, both captions contain a completely identical set of words, only in a different order. The dataset was carefully hand-curated by expert annotators and is labeled with a rich set offine-grained tags to assist in analyzing model performance. We probe a diverse range of state-of-the-art vision and language models and find that, surprisingly, none of them do much better than chance. Evidently, these models are not as skilled at visio-linguistic compositional reasoning as we might have hoped. We perform an extensive analysis to obtain insights into how future work might try to mitigate these models' shortcomings. We aim for Winoground to serve as a useful evaluation set for advancing the state of the art and driving further progress in the field. The dataset is available at https://huggingface.co/datasets/facebook/winoground.",
            "year": 2022,
            "citationCount": 219,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel task and dataset for evaluating the ability of vision and language models to conduct visio-linguistic compositional reasoning, which is called Winoground, and it is found that, surprisingly, none of them do much better than chance."
            },
            "score": 2
        },
        {
            "id": "df2113c867c4836a12dad9c697d11654539ae35e",
            "paperId": "df2113c867c4836a12dad9c697d11654539ae35e",
            "title": "Metaphors in Pre-Trained Language Models: Probing and Generalization Across Datasets and Languages",
            "abstract": "Human languages are full of metaphorical expressions. Metaphors help people understand the world by connecting new concepts and domains to more familiar ones. Large pre-trained language models (PLMs) are therefore assumed to encode metaphorical knowledge useful for NLP systems. In this paper, we investigate this hypothesis for PLMs, by probing metaphoricity information in their encodings, and by measuring the cross-lingual and cross-dataset generalization of this information. We present studies in multiple metaphor detection datasets and in four languages (i.e., English, Spanish, Russian, and Farsi). Our extensive experiments suggest that contextual representations in PLMs do encode metaphorical knowledge, and mostly in their middle layers. The knowledge is transferable between languages and datasets, especially when the annotation is consistent across training and testing sets. Our findings give helpful insights for both cognitive and NLP scientists.",
            "year": 2022,
            "citationCount": 32,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The authors' extensive experiments suggest that contextual representations in PLMs do encode metaphorical knowledge, and mostly in their middle layers, and the knowledge is transferable between languages and datasets, especially when the annotation is consistent across training and testing sets."
            },
            "score": 2
        },
        {
            "id": "9fe29c834afbe1848d9df713ae6e0ca3bd053605",
            "paperId": "9fe29c834afbe1848d9df713ae6e0ca3bd053605",
            "title": "Probing the Role of Positional Information in Vision-Language Models",
            "abstract": "In most Vision-Language models (VL), the understanding of the image structure is enabled by injecting the position information (PI) about objects in the image. In our case study of LXMERT, a state-of-the-art VL model, we probe the use of the PI in the representation and study its effect on Visual Question Answering. We show that the model is not capable of leveraging the PI for the image-text matching task on a challenge set where only position differs. Yet, our experiments with probing confirm that the PI is indeed present in the representation. We introduce two strategies to tackle this: (i) Positional Information Pre-training and (ii) Contrastive Learning on PI using Cross-Modality Matching. Doing so, the model can correctly classify if images with detailed PI statements match. Additionally to the 2D information from bounding boxes, we introduce the object's depth as new feature for a better object localization in the space. Even though we were able to improve the model properties as defined by our probes, it only has a negligible effect on the downstream performance. Our results thus highlight an important issue of multimodal modeling: the mere presence of information detectable by a probing classifier is not a guarantee that the information is available in a cross-modal setup.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This case study of LXMERT, a state-of-the-art VL model, probes the use of the PI in the representation and studies its effect on Visual Question Answering, highlighting an important issue of multimodal modeling: the mere presence of information detectable by a probing classifier is not a guarantee that the information is available in a cross-modal setup."
            },
            "score": 2
        },
        {
            "id": "498d1406fc4cddb05cd46477793f2e726a6fe238",
            "paperId": "498d1406fc4cddb05cd46477793f2e726a6fe238",
            "title": "The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code",
            "abstract": "Causal reasoning, the ability to identify cause-and-effect relationship, is crucial in human thinking. Although large language models (LLMs) succeed in many NLP tasks, it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning. Given the fact that programming code may express causal relations more often and explicitly with conditional statements like ``if``, we want to explore whether Code-LLMs acquire better causal reasoning abilities. Our experiments show that compared to text-only LLMs, Code-LLMs with code prompts are significantly better in causal reasoning. We further intervene on the prompts from different aspects, and discover that the programming structure is crucial in code prompt design, while Code-LLMs are robust towards format perturbations.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experiments show that compared to text-only LLMs, Code-LLMs with code prompts are significantly better in causal reasoning, and that the programming structure is crucial in code prompt design, while Code- LLMs are robust towards format perturbations."
            },
            "score": 2
        },
        {
            "id": "94d931a253fb44b9d2f93d9287aec1bf8bdf0a4b",
            "paperId": "94d931a253fb44b9d2f93d9287aec1bf8bdf0a4b",
            "title": "On the Transformation of Latent Space in Fine-Tuned NLP Models",
            "abstract": "We study the evolution of latent space in fine-tuned NLP models. Different from the commonly used probing-framework, we opt for an unsupervised method to analyze representations. More specifically, we discover latent concepts in the representational space using hierarchical clustering. We then use an alignment function to gauge the similarity between the latent space of a pre-trained model and its fine-tuned version. We use traditional linguistic concepts to facilitate our understanding and also study how the model space transforms towards task-specific information. We perform a thorough analysis, comparing pre-trained and fine-tuned models across three models and three downstream tasks. The notable findings of our work are: i) the latent space of the higher layers evolve towards task-specific concepts, ii) whereas the lower layers retain generic concepts acquired in the pre-trained model, iii) we discovered that some concepts in the higher layers acquire polarity towards the output class, and iv) that these concepts can be used for generating adversarial triggers.",
            "year": 2022,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The latent space of the higher layers of fine-tuned NLP models evolves towards task-specific concepts, and it is discovered that some concepts in theHigher layers acquire polarity towards the output class, and that these concepts can be used for generating adversarial triggers."
            },
            "score": 2
        },
        {
            "id": "a5d26eb03dd52a3d588d5a8057091928b56538f9",
            "paperId": "a5d26eb03dd52a3d588d5a8057091928b56538f9",
            "title": "StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion and Adversarial Training with Large Speech Language Models",
            "abstract": "In this paper, we present StyleTTS 2, a text-to-speech (TTS) model that leverages style diffusion and adversarial training with large speech language models (SLMs) to achieve human-level TTS synthesis. StyleTTS 2 differs from its predecessor by modeling styles as a latent random variable through diffusion models to generate the most suitable style for the text without requiring reference speech, achieving efficient latent diffusion while benefiting from the diverse speech synthesis offered by diffusion models. Furthermore, we employ large pre-trained SLMs, such as WavLM, as discriminators with our novel differentiable duration modeling for end-to-end training, resulting in improved speech naturalness. StyleTTS 2 surpasses human recordings on the single-speaker LJSpeech dataset and matches it on the multispeaker VCTK dataset as judged by native English speakers. Moreover, when trained on the LibriTTS dataset, our model outperforms previous publicly available models for zero-shot speaker adaptation. This work achieves the first human-level TTS on both single and multispeaker datasets, showcasing the potential of style diffusion and adversarial training with large SLMs. The audio demos and source code are available at https://styletts2.github.io/.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 1
        }
    ],
    "novelty": "yes"
}