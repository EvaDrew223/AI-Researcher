{
    "topic_description": "novel prompting methods that can better quantify uncertainty or calibrate the confidence of large language models",
    "idea_name": "Bayesian Prompt Ensembling",
    "raw_idea": {
        "Problem": "Large language models often struggle to accurately estimate their uncertainty, especially when dealing with out-of-distribution or ambiguous inputs. This leads to overconfident predictions that may not reflect the model's true uncertainty.",
        "Existing Methods": "Current approaches to uncertainty estimation in LLMs include using model ensembles, dropout-based methods, and Bayesian neural networks. However, these methods often require training multiple models or rely on approximations that may not fully capture the model's uncertainty.",
        "Motivation": "By using a Bayesian approach to combine the predictions from multiple prompts, we can obtain a more robust and calibrated estimate of the model's uncertainty. This approach is inspired by the success of Bayesian model ensembling in traditional machine learning, where the predictions from multiple models are combined to obtain a more reliable estimate of the predictive uncertainty.",
        "Proposed Method": "We propose Bayesian Prompt Ensembling (BPE), a method that generates multiple prompts for a given input and combines their predictions using a Bayesian framework. The prompts are designed to capture different aspects or interpretations of the input, such as rephrasing the question, providing additional context, or asking for clarification. The LLM is then prompted to generate a response for each prompt, along with an associated confidence score. The final prediction is obtained by combining the individual predictions using Bayesian model averaging, where the weights are determined by the confidence scores and the prior probability of each prompt.",
        "Experiment Plan": "Evaluate BPE on a range of tasks, including question answering, fact verification, and natural language inference. Compare the calibration and uncertainty estimation of BPE with baseline methods such as model ensembles and dropout-based approaches. Use metrics such as Brier score, expected calibration error, and area under the confidence-accuracy curve to assess the effectiveness of the proposed method. Analyze the generated prompts and their associated weights to gain insights into the different aspects captured by each prompt."
    },
    "full_experiment_plan": {
        "Title": "Bayesian Prompt Ensembling: Quantifying Uncertainty in Large Language Models",
        "Problem Statement": "Large language models often struggle to accurately estimate their uncertainty, especially when dealing with out-of-distribution or ambiguous inputs. This leads to overconfident predictions that may not reflect the model's true uncertainty.",
        "Motivation": "Current approaches to uncertainty estimation in LLMs, such as using model ensembles, dropout-based methods, and Bayesian neural networks, often require training multiple models or rely on approximations that may not fully capture the model's uncertainty. Inspired by the success of Bayesian model ensembling in traditional machine learning, where the predictions from multiple models are combined to obtain a more reliable estimate of the predictive uncertainty, we propose Bayesian Prompt Ensembling (BPE). By using a Bayesian approach to combine the predictions from multiple prompts, we aim to obtain a more robust and calibrated estimate of the model's uncertainty.",
        "Proposed Method": "Bayesian Prompt Ensembling (BPE) generates multiple prompts for a given input and combines their predictions using a Bayesian framework. The prompts are designed to capture different aspects or interpretations of the input, such as rephrasing the question, providing additional context, or asking for clarification. The LLM is then prompted to generate a response for each prompt, along with an associated confidence score. The final prediction is obtained by combining the individual predictions using Bayesian model averaging, where the weights are determined by the confidence scores and the prior probability of each prompt.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "Evaluate BPE on a range of tasks, including question answering (SQuAD, TriviaQA), fact verification (FEVER), and natural language inference (SNLI, MNLI). These datasets cover a diverse set of domains and difficulty levels, allowing for a comprehensive evaluation of the proposed method.",
            "Step 2: Construct Prompts": "For each task, design a set of prompts that capture different aspects or interpretations of the input. Examples include:\n- Question Answering: (1) Direct question, (2) Question with additional context, (3) Rephrased question, (4) Question asking for clarification\n- Fact Verification: (1) Direct statement, (2) Statement with additional evidence, (3) Rephrased statement, (4) Statement asking for verification\n- Natural Language Inference: (1) Direct premise and hypothesis, (2) Premise and hypothesis with additional context, (3) Rephrased premise and hypothesis, (4) Premise and hypothesis asking for clarification\nEnsure that the prompts are diverse and cover different aspects of the input while maintaining the core meaning.",
            "Step 3: Select Models": "Use state-of-the-art pre-trained LLMs such as GPT-3.5 (text-davinci-002), GPT-4, and PaLM. These models have shown strong performance across various natural language understanding tasks and are suitable for evaluating the proposed method.",
            "Step 4: Implement Bayesian Prompt Ensembling": "For each input, generate predictions using the LLM for each prompt in the set. Obtain the confidence scores associated with each prediction, either by using the model's built-in confidence estimation or by calibrating the scores using a held-out validation set. Combine the predictions using Bayesian model averaging, where the weights are determined by the confidence scores and the prior probability of each prompt. The prior probabilities can be set based on domain knowledge or learned from data.",
            "Step 5: Evaluate and Compare": "Evaluate the performance of BPE against baseline methods, including (1) single prompt prediction, (2) majority voting ensemble, and (3) unweighted averaging ensemble. Use metrics such as accuracy, F1 score, and calibration error (e.g., Brier score, expected calibration error) to assess the effectiveness of the proposed method. Perform statistical significance tests to determine if the improvements are significant.",
            "Step 6: Analyze and Interpret": "Analyze the results to gain insights into the behavior of BPE. Examine the weights assigned to each prompt and their contribution to the final prediction. Investigate the cases where BPE outperforms the baselines and those where it falls short. Interpret the findings in the context of the specific tasks and datasets, and discuss the implications for uncertainty estimation in LLMs."
        },
        "Test Case Examples": {
            "Example 1": {
                "Input": "What is the capital of France?",
                "Prompts": [
                    "What is the capital of France?",
                    "France is a country in Europe. What is its capital city?",
                    "Can you tell me the name of the capital city of France?",
                    "I'm not sure about the capital of France. Could you clarify?"
                ],
                "Baseline Predictions": [
                    "Paris",
                    "Paris",
                    "Paris",
                    "Paris"
                ],
                "Baseline Confidence Scores": [
                    0.9,
                    0.8,
                    0.7,
                    0.6
                ],
                "BPE Prediction": "Paris",
                "BPE Confidence Score": 0.95,
                "Explanation": "The baseline methods predict the correct answer 'Paris' for all prompts but with varying confidence scores. BPE combines these predictions using Bayesian model averaging, resulting in a higher confidence score for the correct answer. This demonstrates the effectiveness of BPE in quantifying uncertainty and providing a more calibrated prediction."
            },
            "Example 2": {
                "Input": "What is the largest planet in our solar system?",
                "Prompts": [
                    "What is the largest planet in our solar system?",
                    "Our solar system consists of eight planets. Which one is the largest?",
                    "Can you tell me which planet in our solar system is the biggest?",
                    "I'm not sure about the largest planet in our solar system. Could you provide more information?"
                ],
                "Baseline Predictions": [
                    "Jupiter",
                    "Saturn",
                    "Jupiter",
                    "Jupiter"
                ],
                "Baseline Confidence Scores": [
                    0.8,
                    0.6,
                    0.7,
                    0.5
                ],
                "BPE Prediction": "Jupiter",
                "BPE Confidence Score": 0.9,
                "Explanation": "The baseline methods predict different answers for the prompts, with 'Jupiter' being the most common prediction. However, the confidence scores for 'Jupiter' are relatively low. BPE combines these predictions and assigns a higher weight to the correct answer 'Jupiter' based on the confidence scores and prior probabilities. This results in a higher confidence score for 'Jupiter' compared to the baseline methods, demonstrating the ability of BPE to handle ambiguous inputs and provide a more accurate estimate of uncertainty."
            }
        },
        "Fallback Plan": "If the proposed BPE method does not significantly outperform the baseline methods, consider the following alternative approaches:\n1. Analyze the generated prompts and their associated weights to identify potential issues, such as lack of diversity or inadequate coverage of different aspects of the input. Refine the prompt generation process to ensure a more comprehensive set of prompts.\n2. Investigate alternative methods for combining the predictions, such as weighted majority voting or Bayesian model combination with different prior distributions. Compare the performance of these methods with BPE and the baselines.\n3. Explore the impact of using different confidence calibration techniques, such as temperature scaling or isotonic regression, on the performance of BPE. Evaluate if improved calibration leads to better uncertainty estimates.\n4. Conduct a thorough error analysis to identify the types of inputs and tasks where BPE struggles. Use this information to guide further improvements in the prompt generation and combination strategies.\n5. Consider incorporating additional information, such as model-specific features or task-specific knowledge, into the Bayesian framework to improve the uncertainty estimates.\nIf the proposed method and the alternative approaches do not yield satisfactory results, the project can be turned into an analysis paper that provides insights into the challenges and limitations of uncertainty estimation in LLMs. The analysis can focus on understanding the factors that contribute to overconfident predictions, the impact of prompt diversity on uncertainty estimates, and the trade-offs between different combination strategies. The findings can inform future research directions and highlight the need for more advanced techniques to accurately quantify uncertainty in LLMs."
    },
    "novelty_queries": [
        "KeywordQuery(\"bayesian prompt ensembling language models\")",
        "KeywordQuery(\"uncertainty estimation language models\")",
        "KeywordQuery(\"bayesian model averaging language models\")",
        "KeywordQuery(\"prompt diversity uncertainty language models\")",
        "KeywordQuery(\"Bayesian Prompt Ensembling NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "67fa2f2072cca1071ed2c820d6a7f50de6ea2ff3",
            "paperId": "67fa2f2072cca1071ed2c820d6a7f50de6ea2ff3",
            "title": "Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling",
            "abstract": "Uncertainty decomposition refers to the task of decomposing the total uncertainty of a model into data (aleatoric) uncertainty, resulting from the inherent complexity or ambiguity of the data, and model (epistemic) uncertainty, resulting from the lack of knowledge in the model. Performing uncertainty decomposition for large language models (LLMs) is an important step toward improving the reliability, trustworthiness, and interpretability of LLMs, but this research task is very challenging and remains unresolved. The existing canonical method, Bayesian Neural Network (BNN), cannot be applied to LLMs, because BNN requires training and ensembling multiple variants of models, which is infeasible or prohibitively expensive for LLMs. In this paper, we introduce an uncertainty decomposition framework for LLMs, called input clarifications ensemble, which bypasses the need to train new models. Rather than ensembling models with different parameters, our approach generates a set of clarifications for the input, feeds them into the fixed LLMs, and ensembles the corresponding predictions. We show that our framework shares a symmetric decomposition structure with BNN. Empirical evaluations demonstrate that the proposed framework provides accurate and reliable uncertainty quantification on various tasks. Code will be made publicly available at https://github.com/UCSB-NLP-Chang/llm_uncertainty .",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces an uncertainty decomposition framework for LLMs, called input clarifications ensemble, which bypasses the need to train new models, and shares a symmetric decomposition structure with BNN."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is improving uncertainty estimation in large language models, and the proposed approach is Bayesian Prompt Ensembling, which generates multiple prompts for a given input and combines their predictions using a Bayesian framework.\n\nThe research problem in the paper is decomposing uncertainty in large language models into data uncertainty and model uncertainty, and the proposed approach is input clarification ensembling, which generates a set of clarifications for the input, feeds them into the fixed LLMs, and ensembles the corresponding predictions.\n\nWhile both the proposal and the paper aim to improve uncertainty estimation in large language models, the specific research problems and approaches are different. The proposal focuses on improving overall uncertainty estimation, while the paper focuses on decomposing uncertainty into different types. The proposal uses Bayesian Prompt Ensembling, while the paper uses input clarification ensembling.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "8744f24547e2a9053e7a0e71fd6c6fe3fa141927",
            "paperId": "8744f24547e2a9053e7a0e71fd6c6fe3fa141927",
            "title": "A Bayesian approach for prompt optimization in pre-trained language models",
            "abstract": "A prompt is a sequence of symbol or tokens, selected from a vocabulary according to some rule, which is prepended/concatenated to a textual query. A key problem is how to select the sequence of tokens: in this paper we formulate it as a combinatorial optimization problem. The high dimensionality of the token space com-pounded by the length of the prompt sequence requires a very efficient solution. In this paper we propose a Bayesian optimization method, executed in a continuous em-bedding of the combinatorial space. In this paper we focus on hard prompt tuning (HPT) which directly searches for discrete tokens to be added to the text input with-out requiring access to the large language model (LLM) and can be used also when LLM is available only as a black-box. This is critically important if LLMs are made available in the Model as a Service (MaaS) manner as in GPT-4. The current manu-script is focused on the optimization of discrete prompts for classification tasks. The discrete prompts give rise to difficult combinatorial optimization problem which easily become intractable given the dimension of the token space in realistic applications. The optimization method considered in this paper is Bayesian optimization (BO) which has become the dominant approach in black-box optimization for its sample efficiency along with its modular structure and versatility. In this paper we use BoTorch, a library for Bayesian optimization research built on top of pyTorch. Albeit preliminary and obtained using a 'vanilla' version of BO, the experiments on RoB-ERTa on six benchmarks, show a good performance across a variety of tasks and enable an analysis of the tradeoff between size of the search space, accuracy and wall clock time.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper focuses on hard prompt tuning (HPT) which directly searches for discrete tokens to be added to the text input with-out requiring access to the large language model (LLM) and can be used also when LLM is available only as a black-box."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve uncertainty estimation in large language models by using Bayesian prompt ensembling, which generates multiple prompts for a given input and combines their predictions using a Bayesian framework.\n\nThe paper focuses on optimizing discrete prompts for classification tasks using Bayesian optimization in a continuous embedding of the combinatorial space, particularly for hard prompt tuning when the language model is available as a black box.\n\nWhile both the project proposal and the paper use Bayesian methods, the research problems and approaches are different. The proposal addresses uncertainty estimation, while the paper tackles prompt optimization for classification tasks.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "cc0adc6d571c90fbbe0bb5a2a3a6c37db4abb7b2",
            "paperId": "cc0adc6d571c90fbbe0bb5a2a3a6c37db4abb7b2",
            "title": "Prompt Optimization in Large Language Models",
            "abstract": "Prompt optimization is a crucial task for improving the performance of large language models for downstream tasks. In this paper, a prompt is a sequence of n-grams selected from a vocabulary. Consequently, the aim is to select the optimal prompt concerning a certain performance metric. Prompt optimization can be considered as a combinatorial optimization problem, with the number of possible prompts (i.e., the combinatorial search space) given by the size of the vocabulary (i.e., all the possible n-grams) raised to the power of the length of the prompt. Exhaustive search is impractical; thus, an efficient search strategy is needed. We propose a Bayesian Optimization method performed over a continuous relaxation of the combinatorial search space. Bayesian Optimization is the dominant approach in black-box optimization for its sample efficiency, along with its modular structure and versatility. We use BoTorch, a library for Bayesian Optimization research built on top of PyTorch. Specifically, we focus on Hard Prompt Tuning, which directly searches for an optimal prompt to be added to the text input without requiring access to the Large Language Model, using it as a black-box (such as for GPT-4 which is available as a Model as a Service). Albeit preliminary and based on \u201cvanilla\u201d Bayesian Optimization algorithms, our experiments with RoBERTa as a large language model, on six benchmark datasets, show good performances when compared against other state-of-the-art black-box prompt optimization methods and enable an analysis of the trade-off between the size of the search space, accuracy, and wall-clock time.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper focuses on Hard Prompt Tuning, which directly searches for an optimal prompt to be added to the text input without requiring access to the Large Language Model, using it as a black-box (such as for GPT-4 which is available as a Model as a Service)."
            },
            "score": 7,
            "novelty_score": "The research problem in the project proposal is quantifying uncertainty in large language models, and the proposed approach is Bayesian Prompt Ensembling, which generates multiple prompts for a given input and combines their predictions using a Bayesian framework.\n\nThe research problem in the paper is prompt optimization in large language models, and the proposed approach is a Bayesian Optimization method performed over a continuous relaxation of the combinatorial search space for selecting optimal prompts.\n\nWhile both the project proposal and the paper involve large language models and use Bayesian methods, the research problems and approaches are different. The project proposal focuses on quantifying uncertainty, while the paper focuses on optimizing prompts. The project proposal uses Bayesian Prompt Ensembling to combine predictions from multiple prompts, while the paper uses Bayesian Optimization to search for optimal prompts.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "a23d45f22e10173c58a5ee25e5b6c815829671d5",
            "paperId": "a23d45f22e10173c58a5ee25e5b6c815829671d5",
            "title": "Exploring Lottery Prompts for Pre-trained Language Models",
            "abstract": "Consistently scaling pre-trained language models (PLMs) imposes substantial burdens on model adaptation, necessitating more efficient alternatives to conventional fine-tuning.Given the advantage of prompting in the zero-shot setting and the observed performance fluctuation among different prompts, we explore the instance-level prompt and their generalizability.By searching through the prompt space, we first validate the assumption that for every instance, there is almost always a lottery prompt that induces the correct prediction from the PLM, and such prompt can be obtained at a low cost thanks to the inherent ability of PLMs.Meanwhile, it is shown that some strong lottery prompts have high performance over the whole training set, and they are equipped with distinguishable linguistic features.Lastly, we attempt to generalize the searched strong lottery prompts to unseen data with prompt ensembling method.Experiments are conducted on various types of NLP classification tasks and demonstrate that the proposed method can achieve comparable results with other gradient-free and optimization-free baselines.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The instance-level prompt and their generalizability are explored, and it is shown that some strong lottery prompts have high performance over the whole training set, and they are equipped with distinguishable linguistic features."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is quantifying uncertainty in large language models, and the proposed approach is Bayesian Prompt Ensembling, which generates multiple prompts for a given input and combines their predictions using a Bayesian framework.\n\nThe research problem in the paper is exploring instance-level prompts and their generalizability for more efficient model adaptation, and the proposed approach is searching through the prompt space to find lottery prompts that induce correct predictions from pre-trained language models.\n\nThe two works have different research problems and approaches. The proposal focuses on quantifying uncertainty, while the paper focuses on efficient model adaptation. The proposal uses Bayesian methods to combine multiple prompts, while the paper searches for individual lottery prompts.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "67dab0fedfbdc403a2eb882b8d0efa7e16bc25da",
            "paperId": "67dab0fedfbdc403a2eb882b8d0efa7e16bc25da",
            "title": "Strength in Numbers: Estimating Confidence of Large Language Models by Prompt Agreement",
            "abstract": "Large language models have achieved impressive few-shot performance on a wide variety of tasks. However, in many settings, users require confidence estimates for model predictions. While traditional classifiers produce scores for each label, language models instead produce scores for the generation which may not be well calibrated. We compare generations across diverse prompts and show that these can be used to create confidence scores. By utilizing more prompts we can get more precise confidence estimates and use response diversity as a proxy for confidence. We evaluate this approach across ten multiple-choice question-answering datasets using three models: T0, FLAN-T5, and GPT-3. In addition to analyzing multiple human written prompts, we automatically generate more prompts using a language model in order to produce finer-grained confidence estimates. Our method produces more calibrated confidence estimates compared to the log probability of the answer to a single prompt. These improvements could benefit users who rely on prediction confidence for integration into a larger system or in decision-making processes.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work automatically generates more prompts using a language model in order to produce finer-grained confidence estimates and produces more calibrated confidence estimates compared to the log probability of the answer to a single prompt."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving uncertainty estimation in large language models, and the proposed approach is Bayesian Prompt Ensembling, which generates multiple prompts for a given input and combines their predictions using a Bayesian framework.\n\nThe research problem in the paper is also improving confidence estimation in large language models, and the proposed approach is comparing generations across diverse prompts to create confidence scores.\n\nBoth the proposal and the paper aim to improve uncertainty/confidence estimation in large language models by utilizing multiple prompts. However, the proposal focuses on a Bayesian framework for combining predictions, while the paper focuses on using response diversity as a proxy for confidence.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "08bd3e59fdcd8ce3765d081a1255b02ef5146de5",
            "paperId": "08bd3e59fdcd8ce3765d081a1255b02ef5146de5",
            "title": "BatchPrompt: Accomplish more with less",
            "abstract": "As the ever-increasing token limits of large language models (LLMs) have enabled long context as input, prompting with single data samples might no longer an efficient way. A straightforward strategy improving efficiency is to batch data within the token limit (e.g., 8k for gpt-3.5-turbo; 32k for GPT-4), which we call BatchPrompt. We have two initial observations for prompting with batched data. First, we find that prompting with batched data in longer contexts will inevitably lead to worse performance, compared to single-data prompting. Second, the performance of the language model is significantly correlated with the positions and order of the batched data, due to the corresponding change in decoder context. To retain efficiency and overcome performance loss, we propose Batch Permutation and Ensembling (BPE), and a novel Self-reflection-guided EArly Stopping (SEAS) technique. Our comprehensive experimental evaluation demonstrates that BPE can boost the performance of BatchPrompt with a striking margin on a range of popular NLP tasks, including question answering (Boolq), textual entailment (RTE), and duplicate questions identification (QQP). These performances are even competitive with/higher than single-data prompting(SinglePrompt), while BatchPrompt requires much fewer LLM calls and input tokens (For SinglePrompt v.s. BatchPrompt with batch size 32, using just 9%-16% the number of LLM calls, Boolq accuracy 90.6% to 90.9% with 27.4% tokens, QQP accuracy 87.2% to 88.4% with 18.6% tokens, RTE accuracy 91.5% to 91.1% with 30.8% tokens). To the best of our knowledge, this is the first work to technically improve prompting efficiency of large language models. We hope our simple yet effective approach will shed light on the future research of large language models. The code will be released.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This is the first work to technically improve prompting efficiency of large language models by proposing Batch Permutation and Ensembling (BPE), and a novel Self-reflection-guided EArly Stopping (SEAS) technique."
            },
            "score": 7,
            "novelty_score": "The research problem in the project proposal is improving uncertainty estimation in large language models, and the proposed approach is Bayesian Prompt Ensembling (BPE), which generates multiple prompts for a given input and combines their predictions using a Bayesian framework.\n\nThe research problem in the paper is improving the efficiency of prompting large language models with batched data, and the proposed approach is Batch Permutation and Ensembling (BPE) and Self-reflection-guided EArly Stopping (SEAS).\n\nAlthough both the project proposal and the paper use the abbreviation \"BPE\", they refer to different methods. The project proposal focuses on uncertainty estimation, while the paper focuses on efficiency improvement. Therefore, the research problems and approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "f7c21f11dca84d443304e8909c9b87eebda0017c",
            "paperId": "f7c21f11dca84d443304e8909c9b87eebda0017c",
            "title": "Patch-Token Aligned Bayesian Prompt Learning for Vision-Language Models",
            "abstract": "For downstream applications of vision-language pre-trained models, there has been significant interest in constructing effective prompts. Existing works on prompt engineering, which either require laborious manual designs or optimize the prompt tuning as a point estimation problem, may fail to describe diverse characteristics of categories and limit their applications. We introduce a Bayesian probabilistic resolution to prompt learning, where the label-specific stochastic prompts are generated hierarchically by first sampling a latent vector from an underlying distribution and then employing a lightweight generative model. Importantly, we semantically regularize prompt learning with the visual knowledge and view images and the corresponding prompts as patch and token sets under optimal transport, which pushes the prompt tokens to faithfully capture the label-specific visual concepts, instead of overfitting the training categories. Moreover, the proposed model can also be straightforwardly extended to the conditional case where the instance-conditional prompts are generated to improve the generalizability. Extensive experiments on 15 datasets show promising transferability and generalization performance of our proposed model.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Bayesian probabilistic resolution to prompt learning, where the label-specific stochastic prompts are generated hierarchically by first sampling a latent vector from an underlying distribution and then employing a lightweight generative model."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is quantifying uncertainty in large language models, and the proposed approach is Bayesian Prompt Ensembling, which generates multiple prompts for a given input and combines their predictions using a Bayesian framework.\n\nThe research problem in the paper is constructing effective prompts for vision-language pre-trained models, and the proposed approach is a Bayesian probabilistic resolution to prompt learning, where label-specific stochastic prompts are generated hierarchically and semantically regularized with visual knowledge.\n\nWhile both the proposal and the paper use Bayesian methods for prompt learning, the proposal focuses on language models and quantifying uncertainty, while the paper focuses on vision-language models and improving transferability and generalization performance. The research problems and application domains are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "dca6c3927ade6481a1ae080f5c24decbfeced1be",
            "paperId": "dca6c3927ade6481a1ae080f5c24decbfeced1be",
            "title": "Boosted Prompt Ensembles for Large Language Models",
            "abstract": "Methods such as chain-of-thought prompting and self-consistency have pushed the frontier of language model reasoning performance with no additional training. To further improve performance, we propose a prompt ensembling method for large language models, which uses a small dataset to construct a set of few shot prompts that together comprise a ``boosted prompt ensemble''. The few shot examples for each prompt are chosen in a stepwise fashion to be ``hard'' examples on which the previous step's ensemble is uncertain. We show that this outperforms single-prompt output-space ensembles and bagged prompt-space ensembles on the GSM8k and AQuA datasets, among others. We propose both train-time and test-time versions of boosted prompting that use different levels of available annotation and conduct a detailed empirical study of our algorithm.",
            "year": 2023,
            "citationCount": 21,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A prompt ensembling method for large language models, which uses a small dataset to construct a set of few shot prompts that together comprise a ``boosted prompt ensemble'' that outperforms single-prompt output-space ensembles and bagged prompt-spaceEnsemble on the GSM8k and AQuA datasets, among others."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is quantifying uncertainty in large language models, and the proposed approach is Bayesian Prompt Ensembling, which generates multiple prompts and combines their predictions using a Bayesian framework.\n\nThe research problem in the paper is improving language model reasoning performance, and the proposed approach is a prompt ensembling method that constructs a set of few-shot prompts chosen in a stepwise fashion based on the uncertainty of the previous step's ensemble.\n\nWhile both the proposal and the paper focus on ensembling prompts, the research problems and the specific approaches differ. The proposal aims to quantify uncertainty, while the paper aims to improve reasoning performance. The proposal uses a Bayesian framework to combine predictions, while the paper uses a boosting-like approach to select prompts.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "32ef1b5d81af1ca4335f5e2c33cd98ca8c64f658",
            "paperId": "32ef1b5d81af1ca4335f5e2c33cd98ca8c64f658",
            "title": "Bayesian Prompt Learning for Image-Language Model Generalization",
            "abstract": "Foundational image-language models have generated considerable interest due to their efficient adaptation to downstream tasks by prompt learning. Prompt learning treats part of the language model input as trainable while freezing the rest, and optimizes an Empirical Risk Mini-mization objective. However, Empirical Risk Minimization is known to suffer from distributional shifts which hurt gen-eralizability to prompts unseen during training. By leveraging the regularization ability of Bayesian methods, we frame prompt learning from the Bayesian perspective and formulate it as a variational inference problem. Our approach regularizes the prompt space, reduces overfitting to the seen prompts and improves the prompt generalization on unseen prompts. Our framework is implemented by modeling the input prompt space in a probabilistic manner, as an a priori distribution which makes our proposal compatible with prompt learning approaches that are unconditional or conditional on the image. We demonstrate empirically on 15 benchmarks that Bayesian prompt learning provides an appropriate coverage of the prompt space, prevents learning spurious features, and exploits transferable invariant features. This results in better generalization of unseen prompts, even across different datasets and domains.Code available at: https://github.com/saic-fi/Bayesian-Prompt-Learning",
            "year": 2022,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work frames prompt learning from the Bayesian perspective and formulate it as a variational inference problem, and demonstrates empirically on 15 benchmarks that Bayesian prompt learning provides an appropriate coverage of the prompt space, prevents learning spurious features, and exploits transferable invariant features."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving uncertainty estimation in large language models, while the paper focuses on improving the generalization of image-language models to unseen prompts using Bayesian methods.\n\nThe proposed approach in the paper is Bayesian prompt learning, which regularizes the prompt space and reduces overfitting to seen prompts. In contrast, the proposal suggests Bayesian Prompt Ensembling, which combines predictions from multiple prompts using a Bayesian framework to obtain a more robust estimate of the model's uncertainty.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "444f3b7293b85b7d37600372941a289f9163abd1",
            "paperId": "444f3b7293b85b7d37600372941a289f9163abd1",
            "title": "LM-Polygraph: Uncertainty Estimation for Language Models",
            "abstract": "Recent advancements in the capabilities of large language models (LLMs) have paved the way for a myriad of groundbreaking applications in various fields. However, a significant challenge arises as these models often\"hallucinate\", i.e., fabricate facts without providing users an apparent means to discern the veracity of their statements. Uncertainty estimation (UE) methods are one path to safer, more responsible, and more effective use of LLMs. However, to date, research on UE methods for LLMs has been focused primarily on theoretical rather than engineering contributions. In this work, we tackle this issue by introducing LM-Polygraph, a framework with implementations of a battery of state-of-the-art UE methods for LLMs in text generation tasks, with unified program interfaces in Python. Additionally, it introduces an extendable benchmark for consistent evaluation of UE techniques by researchers, and a demo web application that enriches the standard chat dialog with confidence scores, empowering end-users to discern unreliable responses. LM-Polygraph is compatible with the most recent LLMs, including BLOOMz, LLaMA-2, ChatGPT, and GPT-4, and is designed to support future releases of similarly-styled LMs.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LM-Polygraph is introduced, a framework with implementations of a battery of state-of-the-art UE methods for LLMs in text generation tasks, with unified program interfaces in Python, and introduces an extendable benchmark for consistent evaluation of UE techniques by researchers."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve uncertainty estimation in large language models using Bayesian Prompt Ensembling, which generates multiple prompts for a given input and combines their predictions using a Bayesian framework. The paper, on the other hand, introduces LM-Polygraph, a framework that implements state-of-the-art uncertainty estimation methods for LLMs in text generation tasks, along with a benchmark for evaluation and a demo web application.\n\nWhile both the project proposal and the paper address the issue of uncertainty estimation in large language models, their approaches differ. The project proposal focuses on a specific method (Bayesian Prompt Ensembling), while the paper presents a framework that includes various uncertainty estimation techniques.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "ea0d41514a41f8273f13b3b277e7fcbbc65a8549",
            "paperId": "ea0d41514a41f8273f13b3b277e7fcbbc65a8549",
            "title": "Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models",
            "abstract": "The recent performance leap of Large Language Models (LLMs) opens up new opportunities across numerous industrial applications and domains. However, erroneous generations, such as false predictions, misinformation, and hallucination made by LLMs, have also raised severe concerns for the trustworthiness of LLMs', especially in safety-, security- and reliability-sensitive scenarios, potentially hindering real-world adoptions. While uncertainty estimation has shown its potential for interpreting the prediction risks made by general machine learning (ML) models, little is known about whether and to what extent it can help explore an LLM's capabilities and counteract its undesired behavior. To bridge the gap, in this paper, we initiate an exploratory study on the risk assessment of LLMs from the lens of uncertainty. In particular, we experiment with twelve uncertainty estimation methods and four LLMs on four prominent natural language processing (NLP) tasks to investigate to what extent uncertainty estimation techniques could help characterize the prediction risks of LLMs. Our findings validate the effectiveness of uncertainty estimation for revealing LLMs' uncertain/non-factual predictions. In addition to general NLP tasks, we extensively conduct experiments with four LLMs for code generation on two datasets. We find that uncertainty estimation can potentially uncover buggy programs generated by LLMs. Insights from our study shed light on future design and development for reliable LLMs, facilitating further research toward enhancing the trustworthiness of LLMs.",
            "year": 2023,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An exploratory study on the risk assessment of LLMs from the lens of uncertainty is initiated, finding that uncertainty estimation can potentially uncover buggy programs generated by LLMs."
            },
            "score": 6
        },
        {
            "id": "7adb88771376c2a31688e3b0395b0550a35b824d",
            "paperId": "7adb88771376c2a31688e3b0395b0550a35b824d",
            "title": "Uncertainty Decomposition and Quantification for In-Context Learning of Large Language Models",
            "abstract": "In-context learning has emerged as a ground-breaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM\u2019s response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM\u2019s response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model\u2019s configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-context learning in a plug-and-play fashion. Extensive experiments are conducted to demonstrate the effectiveness of the decomposition. The code and data are available at: https://github. com/lingchen0331/UQ_ICL .",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work dives into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations and ambiguities tied to the model\u2019s configurations (epistemic uncertainty)."
            },
            "score": 6
        },
        {
            "id": "be8c90bca14d59f180f40a41126b7cd8c29c5d4e",
            "paperId": "be8c90bca14d59f180f40a41126b7cd8c29c5d4e",
            "title": "Uncertainty Quantification for In-Context Learning of Large Language Models",
            "abstract": "In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM's response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM's response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model's configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-context learning in a plug-and-play fashion. Extensive experiments are conducted to demonstrate the effectiveness of the decomposition. The code and data are available at: https://github.com/lingchen0331/UQ_ICL.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations and ambiguities tied to the model's configurations (epistemic uncertainty)."
            },
            "score": 6
        },
        {
            "id": "4487bdcf1eb42bdec83709ba0df5b32dcf388976",
            "paperId": "4487bdcf1eb42bdec83709ba0df5b32dcf388976",
            "title": "What and How does In-Context Learning Learn? Bayesian Model Averaging, Parameterization, and Generalization",
            "abstract": "In this paper, we conduct a comprehensive study of In-Context Learning (ICL) by addressing several open questions: (a) What type of ICL estimator is learned by large language models? (b) What is a proper performance metric for ICL and what is the error rate? (c) How does the transformer architecture enable ICL? To answer these questions, we adopt a Bayesian view and formulate ICL as a problem of predicting the response corresponding to the current covariate, given a number of examples drawn from a latent variable model. To answer (a), we show that, without updating the neural network parameters, ICL implicitly implements the Bayesian model averaging algorithm, which is proven to be approximately parameterized by the attention mechanism. For (b), we analyze the ICL performance from an online learning perspective and establish a $\\mathcal{O}(1/T)$ regret bound for perfectly pretrained ICL, where $T$ is the number of examples in the prompt. To answer (c), we show that, in addition to encoding Bayesian model averaging via attention, the transformer architecture also enables a fine-grained statistical analysis of pretraining under realistic assumptions. In particular, we prove that the error of pretrained model is bounded by a sum of an approximation error and a generalization error, where the former decays to zero exponentially as the depth grows, and the latter decays to zero sublinearly with the number of tokens in the pretraining dataset. Our results provide a unified understanding of the transformer and its ICL ability with bounds on ICL regret, approximation, and generalization, which deepens our knowledge of these essential aspects of modern language models.",
            "year": 2023,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A unified understanding of the transformer and its ICL ability with bounds on ICL regret, approximation, and generalization is provided, which deepens the knowledge of these essential aspects of modern language models."
            },
            "score": 6
        },
        {
            "id": "29bd550d0ab53296790ceba31dfe0a06754bcdde",
            "paperId": "29bd550d0ab53296790ceba31dfe0a06754bcdde",
            "title": "Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning",
            "abstract": "In recent years, pre-trained large language models (LLMs) have demonstrated remarkable efficiency in achieving an inference-time few-shot learning capability known as in-context learning. However, existing literature has highlighted the sensitivity of this capability to the selection of few-shot demonstrations. Current understandings of the underlying mechanisms by which this capability arises from regular language model pretraining objectives remain disconnected from the real-world LLMs. This study aims to examine the in-context learning phenomenon through a Bayesian lens, viewing real-world LLMs as latent variable models. On this premise, we propose an algorithm to select optimal demonstrations from a set of annotated data with a small LM, and then directly generalize the selected demonstrations to larger LMs. We demonstrate significant improvement over baselines, averaged over eight GPT models on eight real-world text classification datasets. We also demonstrate the real-world usefulness of our algorithm on GSM8K, a math word problem dataset. Our empirical findings support our hypothesis that LLMs implicitly infer a latent variable containing task information.",
            "year": 2023,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The empirical findings support the hypothesis that LLMs implicitly infer a latent variable containing task information, and propose an algorithm to select optimal demonstrations from a set of annotated data with a small LM, and then directly generalize the selected demonstrations to larger LMs."
            },
            "score": 6
        },
        {
            "id": "196ee3bf7724cd6b01972a7eeb4b4416a781b0ee",
            "paperId": "196ee3bf7724cd6b01972a7eeb4b4416a781b0ee",
            "title": "Cold-Start Data Selection for Few-shot Language Model Fine-tuning: A Prompt-Based Uncertainty Propagation Approach",
            "abstract": "Large Language Models have demonstrated remarkable few-shot performance, but the performance can be sensitive to the selection of few-shot instances. We propose PATRON, a new method that uses prompt-based uncertainty estimation for data selection for pre-trained language model fine-tuning under cold-start scenarios, i.e., no initial labeled data are available. In PATRON, we design (1) a prompt-based uncertainty propagation approach to estimate the importance of data points and (2) a partition-then-rewrite (PTR) strategy to promote sample diversity when querying for annotations. Experiments on six text classification datasets show that PATRON outperforms the strongest cold-start data selection baselines by up to 6.9%. Besides, with 128 labels only, PATRON achieves 91.0% and 92.1% of the fully supervised performance based on vanilla fine-tuning and prompt-based learning respectively. Our implementation of PATRON is available at \\url{https://github.com/yueyu1030/Patron}.",
            "year": 2022,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "PATRON is a new method that uses prompt-based uncertainty estimation for data selection for pre-trained language model fine-tuning under cold-start scenarios, i.e., no initial labeled data are available."
            },
            "score": 6
        },
        {
            "id": "9c893f54d86a362b8e62e5883bb38c14240441f5",
            "paperId": "9c893f54d86a362b8e62e5883bb38c14240441f5",
            "title": "Improving Diversity of Demographic Representation in Large Language Models via Collective-Critiques and Self-Voting",
            "abstract": "A crucial challenge for generative large language models (LLMs) is diversity: when a user's prompt is under-specified, models may follow implicit assumptions while generating a response, which may result in homogenization of the responses, as well as certain demographic groups being under-represented or even erased from the generated responses. In this paper, we formalize diversity of representation in generative LLMs. We present evaluation datasets and propose metrics to measure diversity in generated responses along people and culture axes. We find that LLMs understand the notion of diversity, and that they can reason and critique their own responses for that goal. This finding motivated a new prompting technique called collective-critique and self-voting (CCSV) to self-improve people diversity of LLMs by tapping into its diversity reasoning capabilities, without relying on handcrafted examples or prompt tuning. Extensive empirical experiments with both human and automated evaluations show that our proposed approach is effective at improving people and culture diversity, and outperforms all baseline methods by a large margin.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that LLMs understand the notion of diversity, and that they can reason and critique their own responses for that goal, and this paper formalizes diversity of representation in generative LLMs."
            },
            "score": 6
        },
        {
            "id": "72fb75f7c38a83424308c8205bb36cd88995494b",
            "paperId": "72fb75f7c38a83424308c8205bb36cd88995494b",
            "title": "Leveraging Large Language Models for Exploiting ASR Uncertainty",
            "abstract": "While large language models excel in a variety of natural language processing (NLP) tasks, to perform well on spoken language understanding (SLU) tasks, they must either rely on off-the-shelf automatic speech recognition (ASR) systems for transcription, or be equipped with an in-built speech modality. This work focuses on the former scenario, where LLM's accuracy on SLU tasks is constrained by the accuracy of a fixed ASR system on the spoken input. Specifically, we tackle speech-intent classification task, where a high word-error-rate can limit the LLM's ability to understand the spoken intent. Instead of chasing a high accuracy by designing complex or specialized architectures regardless of deployment costs, we seek to answer how far we can go without substantially changing the underlying ASR and LLM, which can potentially be shared by multiple unrelated tasks. To this end, we propose prompting the LLM with an n-best list of ASR hypotheses instead of only the error-prone 1-best hypothesis. We explore prompt-engineering to explain the concept of n-best lists to the LLM; followed by the finetuning of Low-Rank Adapters on the downstream tasks. Our approach using n-best lists proves to be effective on a device-directed speech detection task as well as on a keyword spotting task, where systems using n-best list prompts outperform those using 1-best ASR hypothesis; thus paving the way for an efficient method to exploit ASR uncertainty via LLMs for speech-based applications.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work tackles speech-intent classification task, where a high word-error-rate can limit the LLM's ability to understand the spoken intent, and proposes prompting theLLM with an n-best list of ASR hypotheses instead of only the error-prone 1-best hypothesis."
            },
            "score": 6
        },
        {
            "id": "43b15205c98b5e0693f128ebdd4c57c4ba854049",
            "paperId": "43b15205c98b5e0693f128ebdd4c57c4ba854049",
            "title": "Cold-Start Data Selection for Better Few-shot Language Model Fine-tuning: A Prompt-based Uncertainty Propagation Approach",
            "abstract": "We present PATRON, a prompt-based data selection method for pre-trained language model fine-tuning under cold-start scenarios, i.e., no initial labeled data are available. In PATRON, we design (1) a prompt-based uncertainty propagation approach to estimate the importance of data points and (2) a partition-then-rewrite (PTR) strategy to promote sample diversity when querying for annotations. Experiments on six text classification datasets show that PATRON outperforms the strongest cold-start data selection baselines by up to 6.9%. Besides, with 128 labels only, PATRON achieves 91.0% and 92.1% of the fully supervised performance based on vanilla fine-tuning and prompt-based learning respectively. Our implementation of PATRON will be published upon acceptance.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A prompt-based data selection method for pre-trained language model fine-tuning under cold-start scenarios, i.e., no initial labeled data are available, and a partition-then-rewrite strategy to promote sample diversity when querying for annotations is designed."
            },
            "score": 6
        },
        {
            "id": "1fa4469e5bc5d096572902fe14b0d66078a24c47",
            "paperId": "1fa4469e5bc5d096572902fe14b0d66078a24c47",
            "title": "Navigating the Grey Area: Expressions of Overconfidence and Uncertainty in Language Models",
            "abstract": "Despite increasingly \ufb02uent, relevant, and coherent language generation, major gaps remain between how humans and machines use language. We argue that a key dimension that is missing from our understanding of language models (LMs) is the model\u2019s ability to interpret and generate expressions of uncertainty . Whether it be the weatherperson announcing a chance of rain or a doctor giving a diagnosis, information is often not black-and-white and expressions of uncertainty provide nuance to support human-decision making. The increasing deployment of LMs in the wild motivates us to investigate whether LMs are capable of interpreting expressions of uncertainty and how LMs\u2019 behaviors change when learning to emit their own expressions of uncertainty. When injecting expressions of uncertainty into prompts (e.g., \"I think the answer is...\"), we discover that GPT3\u2019s generations vary upwards of 80% in accuracy based on the expression used. We analyze the linguistic characteristics of these expressions and \ufb01nd a drop in accuracy when naturalistic expressions of certainty are present. We \ufb01nd similar effects when teaching models to emit their own expressions of uncertainty, where model calibration suffers when teaching models to emit certainty rather than un certainty. Together, these results highlight the challenges of building LMs that interpret and generate trustworthy expressions of uncertainty.",
            "year": 2023,
            "citationCount": 54,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that GPT3\u2019s generations vary upwards of 80% in accuracy based on the expression used, and the challenges of building LMs that interpret and generate trustworthy expressions of uncertainty are highlighted."
            },
            "score": 6
        },
        {
            "id": "5851bf82b0a9db2de86828f62c3006a6e1b40798",
            "paperId": "5851bf82b0a9db2de86828f62c3006a6e1b40798",
            "title": "Team UTSA-NLP at SemEval 2024 Task 5: Prompt Ensembling for Argument Reasoning in Civil Procedures with GPT4",
            "abstract": "In this paper, we present our system for the SemEval Task 5, The Legal Argument Reasoning Task in Civil Procedure Challenge. Legal argument reasoning is an essential skill that all law students must master. Moreover, it is important to develop natural language processing solutions that can reason about a question given terse domain-specific contextual information. Our system explores a prompt-based solution using GPT4 to reason over legal arguments. We also evaluate an ensemble of prompting strategies, including chain-of-thought reasoning and in-context learning. Overall, our system results in a Macro F1 of .8095 on the validation dataset and .7315 (5th out of 21 teams) on the final test set. Code for this project is available at https://github.com/danschumac1/CivilPromptReasoningGPT4.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This system explores a prompt-based solution using GPT4 to reason over legal arguments, including chain-of-thought reasoning and in-context learning, and evaluates an ensemble of prompting strategies, including chain-of-thought reasoning and in-context learning."
            },
            "score": 6
        },
        {
            "id": "3d7d385d9ee75a286e8da27f7d3cf9f12651c899",
            "paperId": "3d7d385d9ee75a286e8da27f7d3cf9f12651c899",
            "title": "Model ensemble instead of prompt fusion: a sample-specific knowledge transfer method for few-shot prompt tuning",
            "abstract": "Prompt tuning approaches, which learn task-specific soft prompts for a downstream task conditioning on frozen pre-trained models, have attracted growing interest due to its parameter efficiency. With large language models and sufficient training data, prompt tuning performs comparably to full-model tuning. However, with limited training samples in few-shot settings, prompt tuning fails to match the performance of full-model fine-tuning. In this work, we focus on improving the few-shot performance of prompt tuning by transferring knowledge from soft prompts of source tasks. Recognizing the good generalization capabilities of ensemble methods in low-data regime, we first experiment and show that a simple ensemble of model predictions based on different source prompts, outperforms existing multi-prompt knowledge transfer approaches such as source prompt fusion in the few-shot setting. Motivated by this observation, we further investigate model ensembles and propose Sample-specific Ensemble of Source Models (SESoM). SESoM learns to adjust the contribution of each source model for each target sample separately when ensembling source model outputs. Through this way, SESoM inherits the superior generalization of model ensemble approaches and simultaneously captures the sample-specific competence of each source prompt. We conduct experiments across a diverse set of eight NLP tasks using models of different scales (T5-{base, large, XL}) and find that SESoM consistently outperforms the existing models of the same as well as larger parametric scale by a large margin.",
            "year": 2022,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates model ensembles and proposes Sample-specific Ensemble of Source Models (SESoM), a simple ensemble of model predictions based on different source prompts that consistently outperforms the existing models of the same as well as larger parametric scale by a large margin."
            },
            "score": 6
        },
        {
            "id": "d5a6fc6aa139066e3b66ba63002e7d84c109aebc",
            "paperId": "d5a6fc6aa139066e3b66ba63002e7d84c109aebc",
            "title": "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing",
            "abstract": "Large language models (LLMs) have shown remarkable capabilities in Natural Language Processing (NLP), especially in domains where labeled data is scarce or expensive, such as clinical domain. However, to unlock the clinical knowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform specific clinical NLP tasks without any task-specific training data. This is known as in-context learning, which is an art and science that requires understanding the strengths and weaknesses of different LLMs and prompt engineering approaches. In this paper, we present a comprehensive and systematic experimental study on prompt engineering for five clinical NLP tasks: Clinical Sense Disambiguation, Biomedical Evidence Extraction, Coreference Resolution, Medication Status Extraction, and Medication Attribute Extraction. We assessed the prompts proposed in recent literature, including simple prefix, simple cloze, chain of thought, and anticipatory prompts, and introduced two new types of prompts, namely heuristic prompting and ensemble prompting. We evaluated the performance of these prompts on three state-of-the-art LLMs: GPT-3.5, BARD, and LLAMA2. We also contrasted zero-shot prompting with few-shot prompting, and provide novel insights and guidelines for prompt engineering for LLMs in clinical NLP. To the best of our knowledge, this is one of the first works on the empirical evaluation of different prompt engineering approaches for clinical NLP in this era of generative AI, and we hope that it will inspire and inform future research in this area.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents a comprehensive and systematic experimental study on prompt engineering for five clinical NLP tasks: Clinical Sense Disambiguation, Biomedical Evidence Extraction, Coreference Resolution, Medication Status Ext extraction, and Medication Attribute Extraction."
            },
            "score": 6
        },
        {
            "id": "d33a14592d68da068953cdf37f8bf562740c0085",
            "paperId": "d33a14592d68da068953cdf37f8bf562740c0085",
            "title": "An Empirical Evaluation of Prompting Strategies for Large Language Models in Zero-Shot Clinical Natural Language Processing: Algorithm Development and Validation Study",
            "abstract": "Background Large language models (LLMs) have shown remarkable capabilities in natural language processing (NLP), especially in domains where labeled data are scarce or expensive, such as the clinical domain. However, to unlock the clinical knowledge hidden in these LLMs, we need to design effective prompts that can guide them to perform specific clinical NLP tasks without any task-specific training data. This is known as in-context learning, which is an art and science that requires understanding the strengths and weaknesses of different LLMs and prompt engineering approaches. Objective The objective of this study is to assess the effectiveness of various prompt engineering techniques, including 2 newly introduced types\u2014heuristic and ensemble prompts, for zero-shot and few-shot clinical information extraction using pretrained language models. Methods This comprehensive experimental study evaluated different prompt types (simple prefix, simple cloze, chain of thought, anticipatory, heuristic, and ensemble) across 5 clinical NLP tasks: clinical sense disambiguation, biomedical evidence extraction, coreference resolution, medication status extraction, and medication attribute extraction. The performance of these prompts was assessed using 3 state-of-the-art language models: GPT-3.5 (OpenAI), Gemini (Google), and LLaMA-2 (Meta). The study contrasted zero-shot with few-shot prompting and explored the effectiveness of ensemble approaches. Results The study revealed that task-specific prompt tailoring is vital for the high performance of LLMs for zero-shot clinical NLP. In clinical sense disambiguation, GPT-3.5 achieved an accuracy of 0.96 with heuristic prompts and 0.94 in biomedical evidence extraction. Heuristic prompts, alongside chain of thought prompts, were highly effective across tasks. Few-shot prompting improved performance in complex scenarios, and ensemble approaches capitalized on multiple prompt strengths. GPT-3.5 consistently outperformed Gemini and LLaMA-2 across tasks and prompt types. Conclusions This study provides a rigorous evaluation of prompt engineering methodologies and introduces innovative techniques for clinical information extraction, demonstrating the potential of in-context learning in the clinical domain. These findings offer clear guidelines for future prompt-based clinical NLP research, facilitating engagement by non-NLP experts in clinical NLP advancements. To the best of our knowledge, this is one of the first works on the empirical evaluation of different prompt engineering approaches for clinical NLP in this era of generative artificial intelligence, and we hope that it will inspire and inform future research in this area.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study revealed that task-specific prompt tailoring is vital for the high performance of LLMs for zero-shot clinical NLP, and introduces innovative techniques for clinical information extraction, demonstrating the potential of in-context learning in the clinical domain."
            },
            "score": 6
        },
        {
            "id": "d6fb1c21a46fb8b0f3f4383fd467b21e5b58c55f",
            "paperId": "d6fb1c21a46fb8b0f3f4383fd467b21e5b58c55f",
            "title": "InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models",
            "abstract": "Large language models~(LLMs) are instruction followers, but it can be challenging to find the best instruction for different situations, especially for black-box LLMs on which backpropagation is forbidden. Instead of directly optimizing the discrete instruction, we optimize a low-dimensional soft prompt applied to an open-source LLM to generate the instruction for the black-box LLM. On each iteration of the proposed method, which we call InstructZero, a soft prompt is converted into an instruction using the open-source LLM, which is then submitted to the black-box LLM for zero-shot evaluation, and the performance is sent to Bayesian optimization to produce new soft prompts improving the zero-shot performance. We evaluate InstructZero on different combinations of open-source LLMs and APIs including Vicuna and ChatGPT. Our results show that InstructZero outperforms SOTA auto-instruction methods across a variety of downstream tasks. Our code and data are publicly available at https://github.com/Lichang-Chen/InstructZero.",
            "year": 2023,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work optimized a low-dimensional soft prompt applied to an open-source LLM to generate the instruction for the black-box LLM, showing that InstructZero outperforms SOTA auto-instruction methods across a variety of downstream tasks."
            },
            "score": 5
        },
        {
            "id": "326d8eb214385bcec485b4d9004e3b0ac1a06ba2",
            "paperId": "326d8eb214385bcec485b4d9004e3b0ac1a06ba2",
            "title": "Make Prompts Adaptable: Bayesian Modeling for Vision-Language Prompt Learning with Data-Dependent Prior",
            "abstract": "Recent vision-language pre-trained (VLP) models have become the backbone for many downstream tasks, but they are utilized as frozen model without learning. Prompt learning is a method to improve the pre-trained VLP model by adding a learnable context vector to the inputs of the text encoder. In a few-shot learning scenario of the downstream task, MLE training can lead the context vector to over-fit dominant image features in the training data. This overfitting can potentially harm the generalization ability, especially in the presence of a distribution shift between the training and test dataset. This paper presents a Bayesian-based framework of prompt tuning, which could alleviate the over-fitting issues on few-shot learning application and increase the adaptability of prompts on unobserved instances. Specifically, modeling data-dependent prior enhances the adaptability of text features for both seen and unseen image features without the trade-off of performance between them. Based on the Bayesian framework, we utilize the Wasserstein gradient flow in the estimation of our target posterior distribution, which enables our prompt to be flexible in capturing the complex modes of image features. We demonstrate the effectiveness of our method on benchmark datasets for several experiments by showing statistically significant improvements on performance compared to existing methods.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Bayesian-based framework of prompt tuning is presented, which could alleviate the over-fitting issues on few-shot learning application and increase the adaptability of prompts on unobserved instances and modeling data-dependent prior enhances the adaptability of text features for both seen and unseen image features without the trade-off of performance between them."
            },
            "score": 5
        },
        {
            "id": "5424e311319c58847b4c690d5c91090e3b6a4ac3",
            "paperId": "5424e311319c58847b4c690d5c91090e3b6a4ac3",
            "title": "Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models",
            "abstract": "While Large Language Models (LLMs) have demonstrated remarkable potential in natural language generation and instruction following, a persistent challenge lies in their susceptibility to\"hallucinations\", which erodes trust in their outputs. Although Uncertainty Quantification (UQ) presents a promising solution, its accurate implementation within the context of LLMs remains a significant hurdle. To address this critical roadblock, our research originates from a fundamental heuristic insight: tokens within auto-regressive LLM-generated text do not equally reflect the underlying meaning. Some tokens carry greater relevance and representativeness than others, owing to the phenomenon of\"linguistic redundancy\", wherein a select few keywords suffice to convey the essence of lengthy sentences. Regrettably, existing methodologies treat all tokens with equal importance when estimating uncertainty, disregarding these inherent generative inequalities. Our analysis reveals a significant issue with state-of-the-art: numerous tokens (and sentences) of limited semantic significance receive equal or even excessive weighting during uncertainty estimation. To rectify this bias, we propose to jointly Shifting Attention to more Relevant (SAR) components, at both the token- and the sentence-levels for accurate uncertainty estimation. We conduct extensive experiments involving a range of popular\"off-the-shelf\"LLMs, including instruction-tuned LLMs such as Vicuna, WizardLM, and LLaMA-2-chat, as well as pretrained LLMs like OPT and LLaMA, with model sizes extending up to 33B parameters. We carry out evaluation across various free-form question-answering tasks, encompassing domains such as reading comprehension, science Q&A, and medical Q&A. Our experimental results demonstrate the superior performance of SAR in addressing the challenges of uncertainty estimation within the realm of LLMs.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results demonstrate the superior performance of SAR in addressing the challenges of uncertainty estimation within the realm of LLMs, and propose to jointly Shifting Attention to more Relevant (SAR) components, at both the token- and the sentence-levels for accurate uncertainty estimation."
            },
            "score": 5
        },
        {
            "id": "6d3ae6d6b312b659b3a14ae3f3e86a36db63200d",
            "paperId": "6d3ae6d6b312b659b3a14ae3f3e86a36db63200d",
            "title": "Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning",
            "abstract": "Step-by-step decision planning with large language models (LLMs) is gaining attention in AI agent development. This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models. Existing approaches are either white-box or computationally demanding, limiting use of black-box proprietary LLMs within budgets. The paper's first contribution is a non-parametric uncertainty quantification method for LLMs, efficiently estimating point-wise dependencies between input-decision on the fly with a single inference, without access to token logits. This estimator informs the statistical interpretation of decision trustworthiness. The second contribution outlines a systematic design for a decision-making agent, generating actions like ``turn on the bathroom light'' based on user prompts such as ``take a bath''. Users will be asked to provide preferences when more than one action has high estimated point-wise dependencies. In conclusion, our uncertainty estimation and decision-making agent design offer a cost-efficient approach for AI agent development.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models, and outlines a systematic design for a decision-making agent, offering a cost-efficient approach for AI agent development."
            },
            "score": 5
        },
        {
            "id": "4118c8bca76b4bfafc379d80bf91455df88614b6",
            "paperId": "4118c8bca76b4bfafc379d80bf91455df88614b6",
            "title": "Aligning Language Models with Human Preferences via a Bayesian Approach",
            "abstract": "In the quest to advance human-centric natural language generation (NLG) systems, ensuring alignment between NLG models and human preferences is crucial. For this alignment, current popular methods leverage a reinforcement learning (RL) approach with a reward model trained on feedback from humans. However, inherent disagreements due to the subjective nature of human preferences pose a significant challenge for training the reward model, resulting in a deterioration of the NLG performance. To tackle this issue, previous approaches typically rely on majority voting or averaging to consolidate multiple inconsistent preferences into a merged one. Although straightforward to understand and execute, such methods suffer from an inability to capture the nuanced degrees of disaggregation among humans and may only represent a specialized subset of individuals, thereby lacking the ability to quantitatively disclose the universality of human preferences. To address this challenge, this paper proposes a novel approach, which employs a Bayesian framework to account for the distribution of disagreements among human preferences as training a preference model, and names it as d-PM. Besides, considering the RL strategy's inefficient and complex training process over the training efficiency, we further propose utilizing the contrastive learning strategy to train the NLG model with the preference scores derived from the d-PM model. Extensive experiments on two human-centric NLG tasks, i.e., emotional support conversation and integrity\"Rule-of-Thumb\"generation, show that our method consistently exceeds previous SOTA models in both automatic and human evaluations.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel approach is proposed, which employs a Bayesian framework to account for the distribution of disagreements among human preferences as training a preference model, and names it as d-PM, which consistently exceeds previous SOTA models in both automatic and human evaluations."
            },
            "score": 5
        },
        {
            "id": "ca77c3c815a9b7367a2d912195f3c529d1d78f44",
            "paperId": "ca77c3c815a9b7367a2d912195f3c529d1d78f44",
            "title": "Prompt2Gaussia: Uncertain Prompt-learning for Script Event Prediction",
            "abstract": "Script Event Prediction (SEP) aims to predict the subsequent event for a given event chain from a candidate list. Prior research has achieved great success by integrating external knowledge to enhance the semantics, but it is laborious to acquisite the appropriate knowledge resources and retrieve the script-related knowledge. In this paper, we regard public pre-trained language models as knowledge bases and automatically mine the script-related knowledge via prompt-learning. Still, the scenario-diversity and label-ambiguity in scripts make it uncertain to construct the most functional prompt and label token in prompt learning, i.e., prompt-uncertainty and verbalizer-uncertainty. Considering the innate ability of Gaussian distribution to express uncertainty, we deploy the prompt tokens and label tokens as random variables following Gaussian distributions, where a prompt estimator and a verbalizer estimator are proposed to estimate their probabilistic representations instead of deterministic representations. We take the lead to explore prompt-learning in SEP and provide a fresh perspective to enrich the script semantics. Our method is evaluated on the most widely used benchmark and a newly proposed large-scale one. Experiments show that our method, which benefits from knowledge evoked from pre-trained language models, outperforms prior baselines by 1.46\\% and 1.05\\% on two benchmarks, respectively.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper regards public pre-trained language models as knowledge bases and automatically mine the script-related knowledge via prompt-learning and deploys prompt tokens and label tokens as random variables following Gaussian distributions."
            },
            "score": 5
        },
        {
            "id": "90350aa626bed47b02d0c162462e5b0ca82be6b2",
            "paperId": "90350aa626bed47b02d0c162462e5b0ca82be6b2",
            "title": "Automatic Chain of Thought Prompting in Large Language Models",
            "abstract": "Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like\"Let's think step by step\"to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the\"Let's think step by step\"prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot",
            "year": 2022,
            "citationCount": 295,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An automatic CoT prompting method that samples questions with diversity and generates reasoning chains to construct demonstrations and consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations."
            },
            "score": 5
        },
        {
            "id": "d1500f1dbd62e26ef0753f31e845078f58479968",
            "paperId": "d1500f1dbd62e26ef0753f31e845078f58479968",
            "title": "Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners",
            "abstract": "Large language models (LLMs) exhibit a wide range of promising capabilities -- from step-by-step planning to commonsense reasoning -- that may provide utility for robots, but remain prone to confidently hallucinated predictions. In this work, we present KnowNo, which is a framework for measuring and aligning the uncertainty of LLM-based planners such that they know when they don't know and ask for help when needed. KnowNo builds on the theory of conformal prediction to provide statistical guarantees on task completion while minimizing human help in complex multi-step planning settings. Experiments across a variety of simulated and real robot setups that involve tasks with different modes of ambiguity (e.g., from spatial to numeric uncertainties, from human preferences to Winograd schemas) show that KnowNo performs favorably over modern baselines (which may involve ensembles or extensive prompt tuning) in terms of improving efficiency and autonomy, while providing formal assurances. KnowNo can be used with LLMs out of the box without model-finetuning, and suggests a promising lightweight approach to modeling uncertainty that can complement and scale with the growing capabilities of foundation models. Website: https://robot-help.github.io",
            "year": 2023,
            "citationCount": 91,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents KnowNo, which is a framework for measuring and aligning the uncertainty of LLM-based planners such that they know when they don't know and ask for help when needed, and suggests a promising lightweight approach to modeling uncertainty that can complement and scale with the growing capabilities of foundation models."
            },
            "score": 5
        },
        {
            "id": "55b88be46c22232a5923ce20c745d99bb9562325",
            "paperId": "55b88be46c22232a5923ce20c745d99bb9562325",
            "title": "Bayesian Multi-Task Transfer Learning for Soft Prompt Tuning",
            "abstract": "Prompt tuning, in which prompts are optimized to adapt large-scale pre-trained language models to downstream tasks instead of fine-tuning the full model parameters, has been shown to be particularly effective when the prompts are trained in a multi-task transfer learning setting. These methods generally involve individually training prompts for each source task and then aggregating them to provide the initialization of the prompt for the target task. However, this approach critically ignores the fact that some of the source tasks could be negatively or positively interfering with each other. We argue that when we extract knowledge from source tasks via training source prompts, we need to consider this correlation among source tasks for better transfer to target tasks. To this end, we propose a Bayesian approach where we work with the posterior distribution of prompts across source tasks. We obtain representative source prompts corresponding to the samples from the posterior utilizing Stein Variational Gradient Descent, which are then aggregated to constitute the initial target prompt. We show extensive experimental results on the standard benchmark NLP tasks, where our Bayesian multi-task transfer learning approach outperforms the state-of-the-art methods in many settings. Furthermore, our approach requires no auxiliary models other than the prompt itself, achieving a high degree of parameter efficiency.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a Bayesian approach where the posterior distribution of prompts across source tasks is worked with, and obtains representative source prompts corresponding to the samples from the posterior utilizing Stein Variational Gradient Descent, which are then aggregated to constitute the initial target prompt."
            },
            "score": 5
        },
        {
            "id": "8fa907b2ec895c823d80e165adaa87c1b5f55020",
            "paperId": "8fa907b2ec895c823d80e165adaa87c1b5f55020",
            "title": "Bayesian Ensembles of Crowds and Deep Learners for Sequence Tagging",
            "abstract": "Current methods for sequence tagging, a core task in NLP, are data hungry. Crowdsourcing is a relatively cheap way to obtain labeled data, but the annotators are unreliable. To address this, we develop a modular Bayesian method for aggregating sequence labels from multiple annotators and evaluate different models of annotator errors and labeling biases. Our approach integrates black-box sequence taggers as components in the model to improve the quality of predictions. We evaluate our model on crowdsourced data for named entity recognition and information extraction tasks, showing that our sequential annotator model outperforms previous methods.",
            "year": 2018,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work develops a modular Bayesian method for aggregating sequence labels from multiple annotators and evaluates different models of annotator errors and labeling biases, showing that the sequential annotator model outperforms previous methods."
            },
            "score": 5
        },
        {
            "id": "8f184f85ce05158a0e00055822206a82dc314852",
            "paperId": "8f184f85ce05158a0e00055822206a82dc314852",
            "title": "ECO: Ensembling Context Optimization for Vision-Language Models",
            "abstract": "Image recognition has recently witnessed a paradigm shift, where vision-language models are now used to perform few-shot classification based on textual prompts. Among these, the CLIP model has shown remarkable capabilities for zero-shot transfer by matching an image and a custom textual prompt in its latent space. This has paved the way for several works that focus on engineering or learning textual contexts for maximizing CLIP\u2019s classification capabilities. In this paper, we follow this trend by learning an ensemble of prompts for image classification. We show that learning diverse and possibly shorter contexts improves considerably and consistently the results rather than relying on a single trainable prompt. In particular, we report better few-shot capabilities with no additional cost at inference time. We demonstrate the capabilities of our approach on 11 different benchmarks.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that learning diverse and possibly shorter contexts improves considerably and consistently the results rather than relying on a single trainable prompt for image classification, and reports better few-shot capabilities with no additional cost at inference time."
            },
            "score": 4
        },
        {
            "id": "2f2a430ba6c93bcfaf4818316ff8a27b1e034b1a",
            "paperId": "2f2a430ba6c93bcfaf4818316ff8a27b1e034b1a",
            "title": "Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models",
            "abstract": "Large language models (LLMs) are excellent in-context learners. However, the sensitivity of data contained in prompts raises privacy concerns. Our work first shows that these concerns are valid: we instantiate a simple but highly effective membership inference attack against the data used to prompt LLMs. To address this vulnerability, one could forego prompting and resort to fine-tuning LLMs with known algorithms for private gradient descent. However, this comes at the expense of the practicality and efficiency offered by prompting. Therefore, we propose to privately learn to prompt. We first show that soft prompts can be obtained privately through gradient descent on downstream data. However, this is not the case for discrete prompts. Thus, we orchestrate a noisy vote among an ensemble of LLMs presented with different prompts, i.e., a flock of stochastic parrots. The vote privately transfers the flock's knowledge into a single public prompt. We show that LLMs prompted with our private algorithms closely match the non-private baselines. For example, using GPT3 as the base model, we achieve a downstream accuracy of 92.7% on the sst2 dataset with ($\\epsilon=0.147, \\delta=10^{-6}$)-differential privacy vs. 95.2% for the non-private baseline. Through our experiments, we also show that our prompt-based approach is easily deployed with existing commercial APIs.",
            "year": 2023,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work first shows that privacy concerns are valid: it instantiates a simple but highly effective membership inference attack against the data used to prompt LLMs, and proposes to privately learn to prompt."
            },
            "score": 4
        },
        {
            "id": "4b98dd7341f6907107951dfa8be168c15b36f520",
            "paperId": "4b98dd7341f6907107951dfa8be168c15b36f520",
            "title": "An Empirical Study of Translation Hypothesis Ensembling with Large Language Models",
            "abstract": "Large language models (LLMs) are becoming a one-fits-many solution, but they sometimes hallucinate or produce unreliable output. In this paper, we investigate how hypothesis ensembling can improve the quality of the generated text for the specific problem of LLM-based machine translation. We experiment with several techniques for ensembling hypotheses produced by LLMs such as ChatGPT, LLaMA, and Alpaca. We provide a comprehensive study along multiple dimensions, including the method to generate hypotheses (multiple prompts, temperature-based sampling, and beam search) and the strategy to produce the final translation (instruction-based, quality-based reranking, and minimum Bayes risk (MBR) decoding). Our results show that MBR decoding is a very effective method, that translation quality can be improved using a small number of samples, and that instruction tuning has a strong impact on the relation between the diversity of the hypotheses and the sampling temperature.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper investigates how hypothesis ensembling can improve the quality of the generated text for the specific problem of LLM-based machine translation, and shows that MBR decoding is a very effective method, that translation quality can be improved using a small number of samples, and that instruction tuning has a strong impact on the relation between the diversity of the hypotheses and the sampling temperature."
            },
            "score": 4
        },
        {
            "id": "b3275106a2cf6d4e34bbf2ef6cde4596e5f899a4",
            "paperId": "b3275106a2cf6d4e34bbf2ef6cde4596e5f899a4",
            "title": "Enhancing Knowledge Base Construction from Pre-trained Language Models using Prompt Ensembles",
            "abstract": "Large language models such as ChatGPT and Bard manifest a significant step in the are of artificial intelligence. Yet, extracting useful knowledge from such models is still a challenging task. Due to the nature of language models, responses can be inaccurate, biased or even speculative. Predicting accurate object-entities by utilizing language model probing is the goal of the LM-KBC challenge. Our approach focuses on the concept of prompt ensembles. We employ initial baseline prompts to ChatGPT and then refine those prompts to exclude suboptimal ones. After a few shot learning step, we use prompt elicitation to improve the output. We use the Llama2 model with 70 billion parameters for inference. Our evaluation shows that this technique significantly enhances previous methods for knowledge base construction from language models. Our implementation is available on https://github.com/asdfthefourth/lmkbc.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work employs initial baseline prompts to ChatGPT and then refine those prompts to exclude suboptimal ones to improve the output and significantly enhances previous methods for knowledge base construction from language models."
            },
            "score": 4
        },
        {
            "id": "507465f8d46489a68a527cb5304d76bdb6c31ed9",
            "paperId": "507465f8d46489a68a527cb5304d76bdb6c31ed9",
            "title": "Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation",
            "abstract": "We introduce a method to measure uncertainty in large language models. For tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models. We show that measuring uncertainty in natural language is challenging because of\"semantic equivalence\"-- different sentences can mean the same thing. To overcome these challenges we introduce semantic entropy -- an entropy which incorporates linguistic invariances created by shared meanings. Our method is unsupervised, uses only a single model, and requires no modifications to off-the-shelf language models. In comprehensive ablation studies we show that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines.",
            "year": 2023,
            "citationCount": 85,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "In comprehensive ablation studies, it is shown that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines."
            },
            "score": 4
        },
        {
            "id": "5e7274bcda47b704b6797bb14be8b7a61c047a61",
            "paperId": "5e7274bcda47b704b6797bb14be8b7a61c047a61",
            "title": "Uncertainty-Aware Evaluation for Vision-Language Models",
            "abstract": "Vision-Language Models like GPT-4, LLaVA, and CogVLM have surged in popularity recently due to their impressive performance in several vision-language tasks. Current evaluation methods, however, overlook an essential component: uncertainty, which is crucial for a comprehensive assessment of VLMs. Addressing this oversight, we present a benchmark incorporating uncertainty quantification into evaluating VLMs. Our analysis spans 20+ VLMs, focusing on the multiple-choice Visual Question Answering (VQA) task. We examine models on 5 datasets that evaluate various vision-language capabilities. Using conformal prediction as an uncertainty estimation approach, we demonstrate that the models' uncertainty is not aligned with their accuracy. Specifically, we show that models with the highest accuracy may also have the highest uncertainty, which confirms the importance of measuring it for VLMs. Our empirical findings also reveal a correlation between model uncertainty and its language model part.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that models with the highest accuracy may also have the highest uncertainty, which confirms the importance of measuring it for VLMs, and a correlation between model uncertainty and its language model part is revealed."
            },
            "score": 4
        },
        {
            "id": "4e15901eaaaa9a9c2c30f64e05054ce6f5cdaa97",
            "paperId": "4e15901eaaaa9a9c2c30f64e05054ce6f5cdaa97",
            "title": "On the Importance of Uncertainty in Decision-Making with Large Language Models",
            "abstract": "We investigate the role of uncertainty in decision-making problems with natural language as input. For such tasks, using Large Language Models as agents has become the norm. However, none of the recent approaches employ any additional phase for estimating the uncertainty the agent has about the world during the decision-making task. We focus on a fundamental decision-making framework with natural language as input, which is the one of contextual bandits, where the context information consists of text. As a representative of the approaches with no uncertainty estimation, we consider an LLM bandit with a greedy policy, which picks the action corresponding to the largest predicted reward. We compare this baseline to LLM bandits that make active use of uncertainty estimation by integrating the uncertainty in a Thompson Sampling policy. We employ different techniques for uncertainty estimation, such as Laplace Approximation, Dropout, and Epinets. We empirically show on real-world data that the greedy policy performs worse than the Thompson Sampling policies. These findings suggest that, while overlooked in the LLM literature, uncertainty plays a fundamental role in bandit tasks with LLMs.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work considers an LLM bandit with a greedy policy, which picks the action corresponding to the largest predicted reward, compared to LLM bandits that make active use of uncertainty estimation by integrating the uncertainty in a Thompson Sampling policy."
            },
            "score": 4
        },
        {
            "id": "c76541024ed59403f99a5a73ba69849112959a6e",
            "paperId": "c76541024ed59403f99a5a73ba69849112959a6e",
            "title": "A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models",
            "abstract": "The tendency of Large Language Models to generate hallucinations and exhibit overconfidence in predictions raises concerns regarding their reliability. Confidence or uncertainty estimations indicating the extent of trustworthiness of a model's response are essential to developing reliable AI systems. Current research primarily focuses on LLM confidence estimations in English, remaining a void for other widely used languages and impeding the global development of reliable AI applications. This paper introduces a comprehensive investigation of Multi-lingual confidence estimation (MlingConf) on LLMs. First, we introduce an elaborated and expert-checked multilingual QA dataset. Second, we delve into the performance of confidence estimations and examine how these confidence scores can enhance LLM performance through self-refinement across diverse languages. Finally, we propose a cross-lingual confidence estimation method to achieve more precise confidence scores. The experimental results showcase the performance of various confidence estimation methods across different languages as well as present that our proposed cross-lingual confidence estimation technique significantly enhances confidence estimation and outperforms several baseline methods.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A comprehensive investigation of Multi-lingual confidence estimation (MlingConf) on LLMs is introduced, an elaborated and expert-checked multilingual QA dataset is introduced, and a cross-lingual confidence estimation method is proposed to achieve more precise confidence scores."
            },
            "score": 4
        },
        {
            "id": "8ae920111435a7db8da360c654c771c53f57c69a",
            "paperId": "8ae920111435a7db8da360c654c771c53f57c69a",
            "title": "Uncertainty Estimation of Transformer Predictions for Misclassification Detection",
            "abstract": "Uncertainty estimation (UE) of model predictions is a crucial step for a variety of tasks such as active learning, misclassification detection, adversarial attack detection, out-of-distribution detection, etc. Most of the works on modeling the uncertainty of deep neural networks evaluate these methods on image classification tasks. Little attention has been paid to UE in natural language processing. To fill this gap, we perform a vast empirical investigation of state-of-the-art UE methods for Transformer models on misclassification detection in named entity recognition and text classification tasks and propose two computationally efficient modifications, one of which approaches or even outperforms computationally intensive methods.",
            "year": 2022,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A vast empirical investigation of state-of-the-art UE methods for Transformer models on misclassification detection in named entity recognition and text classification tasks and two computationally efficient modifications are proposed, one of which approaches or even outperforms computationally intensive methods."
            },
            "score": 4
        },
        {
            "id": "f35f9a967fea696f2522d395ceae0988a53ddeae",
            "paperId": "f35f9a967fea696f2522d395ceae0988a53ddeae",
            "title": "Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling",
            "abstract": "Recurrent neural networks (RNNs) have shown promising performance for language modeling. However, traditional training of RNNs using back-propagation through time often suffers from overfitting. One reason for this is that stochastic optimization (used for large training sets) does not provide good estimates of model uncertainty. This paper leverages recent advances in stochastic gradient Markov Chain Monte Carlo (also appropriate for large training sets) to learn weight uncertainty in RNNs. It yields a principled Bayesian learning algorithm, adding gradient noise during training (enhancing exploration of the model-parameter space) and model averaging when testing. Extensive experiments on various RNN models and across a broad range of applications demonstrate the superiority of the proposed approach relative to stochastic optimization.",
            "year": 2016,
            "citationCount": 39,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper leverages recent advances in stochastic gradient Markov Chain Monte Carlo to learn weight uncertainty in RNNs and yields a principled Bayesian learning algorithm, adding gradient noise during training and model averaging when testing."
            },
            "score": 4
        },
        {
            "id": "91ba00ee7775aeef84f644f3b2dd1baabf2cf67b",
            "paperId": "91ba00ee7775aeef84f644f3b2dd1baabf2cf67b",
            "title": "Bayesian Model Averaging: A Tutorial",
            "abstract": "Standard statistical practice ignores model uncertainty. Data analysts typically select a model from some class of models and then proceed as if the selected model had generated the data. This approach ignores the uncertainty in model selection, leading to over-confident in- ferences and decisions that are more risky than one thinks they are. Bayesian model averaging (BMA) provides a coherent mechanism for ac- counting for this model uncertainty. Several methods for implementing BMA have recently emerged. We discuss these methods and present a number of examples. In these examples, BMA provides improved out-of- sample predictive performance. We also provide a catalogue of currently available BMA software.",
            "year": 2016,
            "citationCount": 3502,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Bayesian model averaging (BMA) provides a coherent mechanism for ac- counting for this model uncertainty and provides improved out-of- sample predictive performance."
            },
            "score": 4
        },
        {
            "id": "8255b71fae79c92d1b3d72caa1e563c80dc36a0b",
            "paperId": "8255b71fae79c92d1b3d72caa1e563c80dc36a0b",
            "title": "Improving Generalization of Pre-trained Language Models via Stochastic Weight Averaging",
            "abstract": "Knowledge Distillation (KD) is a commonly used technique for improving the generalization of compact Pre-trained Language Models (PLMs) on downstream tasks. However, such methods impose the additional burden of training a separate teacher model for every new dataset. Alternatively, one may directly work on the improvement of the optimization procedure of the compact model toward better generalization. Recent works observe that the flatness of the local minimum correlates well with better generalization. In this work, we adapt Stochastic Weight Averaging (SWA), a method encouraging convergence to a flatter minimum, to fine-tuning PLMs. We conduct extensive experiments on various NLP tasks (text classification, question answering, and generation) and different model architectures and demonstrate that our adaptation improves the generalization without extra computation cost. Moreover, we observe that this simple optimization technique is able to outperform the state-of-the-art KD methods for compact models.",
            "year": 2022,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work adapts Stochastic Weight Averaging (SWA), a method encouraging convergence to a flatter minimum, to fine-tuning PLMs and observes that this simple optimization technique is able to outperform the state-of-the-art KD methods for compact models."
            },
            "score": 4
        },
        {
            "id": "327e0290fd71609bfc1a30478a95f690668fe622",
            "paperId": "327e0290fd71609bfc1a30478a95f690668fe622",
            "title": "Enhancing Few-shot Text-to-SQL Capabilities of Large Language Models: A Study on Prompt Design Strategies",
            "abstract": "In-context learning (ICL) has emerged as a new approach to various natural language processing tasks, utilizing large language models (LLMs) to make predictions based on context that has been supplemented with a few examples or task-specific instructions. In this paper, we aim to extend this method to question answering tasks that utilize structured knowledge sources, and improve Text-to-SQL systems by exploring various prompt design strategies for employing LLMs. We conduct a systematic investigation into different demonstration selection methods and optimal instruction formats for prompting LLMs in the Text-to-SQL task. Our approach involves leveraging the syntactic structure of an example's SQL query to retrieve demonstrations, and we demonstrate that pursuing both diversity and similarity in demonstration selection leads to enhanced performance. Furthermore, we show that LLMs benefit from database-related knowledge augmentations. Our most effective strategy outperforms the state-of-the-art system by 2.5 points (Execution Accuracy) and the best fine-tuned system by 5.1 points on the Spider dataset. These results highlight the effectiveness of our approach in adapting LLMs to the Text-to-SQL task, and we present an analysis of the factors contributing to the success of our strategy.",
            "year": 2023,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper conducts a systematic investigation into different demonstration selection methods and optimal instruction formats for prompting LLMs in the Text-to-SQL task, and presents an analysis of the factors contributing to the success of this strategy."
            },
            "score": 4
        },
        {
            "id": "ce27e1dd25c3fc398d41f7ef2a89082cb48a2aa8",
            "paperId": "ce27e1dd25c3fc398d41f7ef2a89082cb48a2aa8",
            "title": "Helping Language Models Learn More: Multi-Dimensional Task Prompt for Few-shot Tuning",
            "abstract": "Large language models (LLMs) can be used as accessible and intelligent chatbots by constructing natural language queries and directly inputting the prompt into the large language model. However, different prompt' constructions often lead to uncertainty in the answers and thus make it hard to utilize the specific knowledge of LLMs (like ChatGPT). To alleviate this, we use an interpretable structure to explain the prompt learning principle in LLMs, which certificates that the effectiveness of language models is determined by position changes of the task's related tokens. Therefore, we propose MTPrompt, a multi-dimensional task prompt learning method consisting based on task-related object, summary, and task description information. By automatically building and searching for appropriate prompts, our proposed MTPrompt achieves the best results on few-shot samples setting and five different datasets. In addition, we demonstrate the effectiveness and stability of our method in different experimental settings and ablation experiments. In interaction with large language models, embedding more task-related information into prompts will make it easier to stimulate knowledge embedded in large language models.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes MTPrompt, a multi-dimensional task prompt learning method consisting based on task-related object, summary, and task description information that achieves the best results on few-shot samples setting and five different datasets."
            },
            "score": 4
        },
        {
            "id": "d9d9ceb4c360c95f3305c0381a31389e0a43e45f",
            "paperId": "d9d9ceb4c360c95f3305c0381a31389e0a43e45f",
            "title": "Instance-Aware Hierarchical Structured Policy for Prompt Learning in Vision-Language Models",
            "abstract": "In recent years, learnable prompts have emerged as a major prompt learning paradigm, enhancing the performance of large-scale vision-language pre-trained models in few-shot image classification. However, enhancing methods are often time-consuming and inflexible because 1) class-specific prompts are inefficient in certain situations; 2) instance-specific prompts are put in a fixed position. To address these issues, inspired by the coarse-to-fine decision-making paradigm of human, we propose an Instance-Aware Hierarchical-Structured Policy (IAHSP) that integrates instance-specific prompt selection and appropriate position selection using a reinforcement learning fashion. Specifically, IAHSP consists of two sub-policies: 1) the root policy selects the most suitable prompt from the prompts pool, and 2) the leaf policy identifies the optimal position for inserting the selected prompt. We train these two policies iteratively with rewards constraining the prompts while maintaining their diversity. Extensive experiments on 11 public benchmarks demonstrate that our IAHSP significantly boosts the few-shot image classification performance of vision-language pre-trained models, while also exhibiting superior generalization performance.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An Instance-Aware Hierarchical-Structured Policy (IAHSP) is proposed that integrates instance-specific prompt selection and appropriate position selection using a reinforcement learning fashion and significantly boosts the few-shot image classification performance of vision-language pre-trained models, while also exhibiting superior generalization performance."
            },
            "score": 4
        },
        {
            "id": "5d49c7401c5f2337c4cc88d243ae39ed659afe64",
            "paperId": "5d49c7401c5f2337c4cc88d243ae39ed659afe64",
            "title": "Red Teaming Language Models with Language Models",
            "abstract": "Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of test cases. In this work, we automatically find cases where a target LM behaves in a harmful way, by generating test cases (\u201cred teaming\u201d) using another LM. We evaluate the target LM\u2019s replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot. We explore several methods, from zero-shot generation to reinforcement learning, for generating test cases with varying levels of diversity and difficulty. Furthermore, we use prompt engineering to control LM-generated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot\u2019s own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation. Overall, LM-based red teaming is one promising tool (among many needed) for finding and fixing diverse, undesirable LM behaviors before impacting users.",
            "year": 2022,
            "citationCount": 326,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work automatically finds cases where a target LM behaves in a harmful way, by generating test cases (\u201cred teaming\u201d) using another LM, and evaluates the target LM\u2019s replies to generated test questions using a classifier trained to detect offensive content."
            },
            "score": 4
        },
        {
            "id": "91c37a88c2b320725057260677ae79f3cdaa492b",
            "paperId": "91c37a88c2b320725057260677ae79f3cdaa492b",
            "title": "Active Learning Principles for In-Context Learning with Large Language Models",
            "abstract": "The remarkable advancements in large language models (LLMs) have significantly enhanced the performance in few-shot learning settings. By using only a small number of labeled examples, referred to as demonstrations, LLMs can effectively grasp the task at hand through in-context learning. However, the process of selecting appropriate demonstrations has received limited attention in prior work. This paper addresses the issue of identifying the most informative demonstrations for few-shot learning by approaching it as a pool-based Active Learning (AL) problem over a single iteration. Our objective is to investigate how AL algorithms can serve as effective demonstration selection methods for in-context learning. We compare various standard AL algorithms based on uncertainty, diversity, and similarity, and consistently observe that the latter outperforms all other methods, including random sampling. Notably, uncertainty sampling, despite its success in conventional supervised learning scenarios, performs poorly in this context. Our extensive experimentation involving a diverse range of GPT and OPT models across $24$ classification and multi-choice tasks, coupled with thorough analysis, unambiguously demonstrates that in-context example selection through AL prioritizes high-quality examples that exhibit low uncertainty and bear similarity to the test examples.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper addresses the issue of identifying the most informative demonstrations for few-shot learning by approaching it as a pool-based Active Learning (AL) problem over a single iteration, and unambiguously demonstrates that in-context example selection through AL prioritizes high-quality examples that exhibit low uncertainty and bear similarity to the test examples."
            },
            "score": 4
        },
        {
            "id": "f11d7ff00ac0dacc4248559dc3e14479123d9cbb",
            "paperId": "f11d7ff00ac0dacc4248559dc3e14479123d9cbb",
            "title": "Enhancing Early Detection of Cognitive Decline in the Elderly through Ensemble of NLP Techniques: A Comparative Study Utilizing Large Language Models in Clinical Notes",
            "abstract": "Background: Early detection of cognitive decline in elderly individuals facilitates clinical trial enrollment and timely medical interventions. This study aims to apply, evaluate, and compare advanced natural language processing techniques for identifying signs of cognitive decline in clinical notes. Methods: This study, conducted at Mass General Brigham (MGB), Boston, MA, included clinical notes from the 4 years prior to initial mild cognitive impairment (MCI) diagnosis in 2019 for patients [\u2265] 50 years. Note sections regarding cognitive decline were labeled manually. A random sample of 4,949 note sections filtered with cognitive functions-related keywords were used for traditional AI model development, and 200 random subset were used for LLM and prompt development; another random sample of 1996 note sections without keyword filtering were used for testing. Prompt templates for large language models (LLM), Llama 2 on Amazon Web Service and GPT-4 on Microsoft Azure, were developed with multiple prompting approaches to select the optimal LLM-based method. Baseline comparisons were made with XGBoost and a hierarchical attention-based deep neural network model. An ensemble of the three models was then constructed using majority vote. Results: GPT-4 demonstrated superior accuracy and efficiency to Llama 2. The ensemble model outperformed individual models, achieving a precision of 90.3%, recall of 94.2%, and F1-score of 92.2%. Notably, the ensemble model demonstrated a marked improvement in precision (from a 70%-79% range to above 90%) compared to the best performing single model. Error analysis revealed 63 samples were wrongly predicted by at least one model; however, only 2 cases (3.2%) were mutual errors across all models, indicating diverse error profiles among them. Conclusion: Our findings indicate that LLMs and traditional models exhibit diverse error profiles. The ensemble of LLMs and locally trained machine learning models on EHR data was found to be complementary, enhancing performance and improving diagnostic accuracy.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The ensemble of LLMs and locally trained machine learning models on EHR data was found to be complementary, enhancing performance and improving diagnostic accuracy, indicating that LLMs and traditional models exhibit diverse error profiles."
            },
            "score": 4
        },
        {
            "id": "8ddb165278176f279ace760a8c14f5e0fcb652b0",
            "paperId": "8ddb165278176f279ace760a8c14f5e0fcb652b0",
            "title": "GPTuner: A Manual-Reading Database Tuning System via GPT-Guided Bayesian Optimization",
            "abstract": "Modern database management systems (DBMS) expose hundreds of configurable knobs to control system behaviours. Determining the appropriate values for these knobs to improve DBMS performance is a long-standing problem in the database community. As there is an increasing number of knobs to tune and each knob could be in continuous or categorical values, manual tuning becomes impractical. Recently, automatic tuning systems using machine learning methods have shown great potentials. However, existing approaches still incur significant tuning costs or only yield sub-optimal performance. This is because they either ignore the extensive domain knowledge available (e.g., DBMS manuals and forum discussions) and only rely on the runtime feedback of benchmark evaluations to guide the optimization, or they utilize the domain knowledge in a limited way. Hence, we propose GPTuner, a manual-reading database tuning system. Firstly, we develop a Large Language Model (LLM)-based pipeline to collect and refine heterogeneous knowledge, and propose a prompt ensemble algorithm to unify a structured view of the refined knowledge. Secondly, using the structured knowledge, we (1) design a workload-aware and training-free knob selection strategy, (2) develop a search space optimization technique considering the value range of each knob, and (3) propose a Coarse-to-Fine Bayesian Optimization Framework to explore the optimized space. Finally, we evaluate GPTuner under different benchmarks (TPC-C and TPC-H), metrics (throughput and latency) as well as DBMS (PostgreSQL and MySQL). Compared to the state-of-the-art approaches, GPTuner identifies better configurations in 16x less time on average. Moreover, GPTuner achieves up to 30% performance improvement (higher throughput or lower latency) over the best-performing alternative.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "GPTuner, a manual-reading database tuning system that designs a workload-aware and training-free knob selection strategy, develops a search space optimization technique considering the value range of each knob, and proposes a Coarse-to-Fine Bayesian Optimization Framework to explore the optimized space."
            },
            "score": 4
        },
        {
            "id": "9cef5a098486aeab6ed3700c5e3d29488488d16f",
            "paperId": "9cef5a098486aeab6ed3700c5e3d29488488d16f",
            "title": "Exploring Effective Factors for Improving Visual In-Context Learning",
            "abstract": "The In-Context Learning (ICL) is to understand a new task via a few demonstrations (aka. prompt) and predict new inputs without tuning the models. While it has been widely studied in NLP, it is still a relatively new area of research in computer vision. To reveal the factors influencing the performance of visual in-context learning, this paper shows that prompt selection and prompt fusion are two major factors that have a direct impact on the inference performance of visual context learning. Prompt selection is the process of identifying the most appropriate prompt or example to help the model understand new tasks. This is important because providing the model with relevant prompts can help it learn more effectively and efficiently. Prompt fusion involves combining knowledge from different positions within the large-scale visual model. By doing this, the model can leverage the diverse knowledge stored in different parts of the model to improve its performance on new tasks. Based these findings, we propose a simple framework prompt-SelF for visual in-context learning. Specifically, we first use the pixel-level retrieval method to select a suitable prompt, and then use different prompt fusion methods to activate all the knowledge stored in the large-scale model, and finally ensemble the prediction results obtained from different prompt fusion methods to obtain the final prediction results. And we conduct extensive experiments on single-object segmentation and detection tasks to demonstrate the effectiveness of prompt-SelF. Remarkably, the prompt-SelF has outperformed OSLSM based meta-learning in 1-shot segmentation for the first time. This indicated the great potential of visual in-context learning. The source code and models will be available at \\url{https://github.com/syp2ysy/prompt-SelF}.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper shows that prompt selection and prompt fusion are two major factors that have a direct impact on the inference performance of visual context learning and proposes a simple framework prompt-SelF for visual in-context learning."
            },
            "score": 4
        },
        {
            "id": "97d4145117462177e1244a99d7a25afed4c234f7",
            "paperId": "97d4145117462177e1244a99d7a25afed4c234f7",
            "title": "How Many Validation Labels Do You Need? Exploring the Design Space of Label-Efficient Model Ranking",
            "abstract": "This paper presents LEMR (Label-Efficient Model Ranking) and introduces the MoraBench Benchmark. LEMR is a novel framework that minimizes the need for costly annotations in model selection by strategically annotating instances from an unlabeled validation set. To evaluate LEMR, we leverage the MoraBench Benchmark, a comprehensive collection of model outputs across diverse scenarios. Our extensive evaluation across 23 different NLP tasks in semi-supervised learning, weak supervision, and prompt selection tasks demonstrates LEMR's effectiveness in significantly reducing labeling costs. Key findings highlight the impact of suitable ensemble methods, uncertainty sampling strategies, and model committee selection in enhancing model ranking accuracy. LEMR, supported by the insights from MoraBench, provides a cost-effective and accurate solution for model selection, especially valuable in resource-constrained environments.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents LEMR (Label-Efficient Model Ranking) and introduces the MoraBench Benchmark, a comprehensive collection of model outputs across diverse scenarios that provides a cost-effective and accurate solution for model selection, especially valuable in resource-constrained environments."
            },
            "score": 4
        },
        {
            "id": "cc5e01873caf3ee3a4a29a5c036ab5ee49315a44",
            "paperId": "cc5e01873caf3ee3a4a29a5c036ab5ee49315a44",
            "title": "GPT-DETOX: An In-Context Learning-Based Paraphraser for Text Detoxification",
            "abstract": "Harmful and offensive communication or content is detrimental to social bonding and the mental state of users on social media platforms. Text detoxification is a crucial task in natural language processing (NLP), where the goal is removing profanity and toxicity from text while preserving its content. Supervised and unsupervised learning are common approaches for designing text detoxification solutions. However, these methods necessitate fine-tuning, leading to computational overhead. In this paper, we propose GPT-DETOX as a framework for prompt-based in-context learning for text detoxification using GPT-3.5 Turbo. We utilize zero-shot and few-shot prompting techniques for detoxifying input sentences. To generate few-shot prompts, we propose two methods: word-matching example selection (WMES) and context-matching example selection (CMES). We additionally take into account ensemble in-context learning (EICL) where the ensemble is shaped by base prompts from zero-shot and all few-shot settings. We use ParaDetox and APPDIA as benchmark detoxification datasets. Our experimental results show that the zero-shot solution achieves promising performance, while our best few-shot setting outperforms the state-of-the-art models on ParaDetox and shows comparable results on APPDIA. Our EICL solutions obtain the greatest performance, adding at least 10% improvement.. against both datasets.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes GPT-DETOX as a framework for prompt-based in-context learning for text detoxification using GPT-3.5 Turbo and proposes two methods to generate few-shot prompts: word-matching example selection and context-matching example selection."
            },
            "score": 4
        },
        {
            "id": "2918c61c667d02693e2fc56ccb1441f05953226f",
            "paperId": "2918c61c667d02693e2fc56ccb1441f05953226f",
            "title": "Learning to Prompt with Text Only Supervision for Vision-Language Models",
            "abstract": "Foundational vision-language models such as CLIP are becoming a new paradigm in vision, due to their excellent generalization abilities. However, adapting these models for downstream tasks while maintaining their generalization remains a challenge. In literature, one branch of methods adapts CLIP by learning prompts using visual information. While effective, most of these works require labeled data which is not practical, and often struggle to generalize towards new datasets due to over-fitting on the source data. An alternative approach resorts to training-free methods by generating class descriptions from large language models (LLMs) and perform prompt ensembling. However, these methods often generate class specific prompts that cannot be transferred to other classes, which incur higher costs by generating LLM descriptions for each class separately. In this work, we propose to combine the strengths of these both streams of methods by learning prompts using only text data derived from LLMs. As supervised training of prompts is not trivial due to absence of images, we develop a training approach that allows prompts to extract rich contextual knowledge from LLM data. Moreover, with LLM contextual data mapped within the learned prompts, it enables zero-shot transfer of prompts to new classes and datasets potentially cutting the LLM prompt engineering cost. To the best of our knowledge, this is the first work that learns generalized prompts using text only data. We perform extensive evaluations on 4 benchmarks where our method improves over prior ensembling works while being competitive to those utilizing labeled images. Our code and pre-trained models are available at https://github.com/muzairkhattak/ProText.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work develops a training approach that allows prompts to extract rich contextual knowledge from LLM data and enables zero-shot transfer of prompts to new classes and datasets potentially cutting the LLM prompt engineering cost."
            },
            "score": 3
        },
        {
            "id": "5c7f3e8b4e07e1d1ad9e708b4219b18de5e798e9",
            "paperId": "5c7f3e8b4e07e1d1ad9e708b4219b18de5e798e9",
            "title": "Beyond prompting: Making Pre-trained Language Models Better Zero-shot Learners by Clustering Representations",
            "abstract": "Recent work has demonstrated that pre-trained language models (PLMs) are zero-shot learners. However, most existing zero-shot methods involve heavy human engineering or complicated self-training pipelines, hindering their application to new situations. In this work, we show that zero-shot text classification can be improved simply by clustering texts in the embedding spaces of PLMs. Specifically, we fit the unlabeled texts with a Bayesian Gaussian Mixture Model after initializing cluster positions and shapes using class names. Despite its simplicity, this approach achieves superior or comparable performance on both topic and sentiment classification datasets and outperforms prior works significantly on unbalanced datasets. We further explore the applicability of our clustering approach by evaluating it on 14 datasets with more diverse topics, text lengths, and numbers of classes. Our approach achieves an average of 20% absolute improvement over prompt-based zero-shot learning. Finally, we compare different PLM embedding spaces and find that texts are well-clustered by topics even if the PLM is not explicitly pre-trained to generate meaningful sentence embeddings. This work indicates that PLM embeddings can categorize texts without task-specific fine-tuning, thus providing a new way to analyze and utilize their knowledge and zero-shot learning ability.",
            "year": 2022,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that zero-shot text classification can be improved simply by clustering texts in the embedding spaces of PLMs, indicating that PLM embeddings can categorize texts without task-specific fine-tuning, thus providing a new way to analyze and utilize their knowledge and zero- shot learning ability."
            },
            "score": 3
        },
        {
            "id": "c202f2679dfc8fc4af0967d1a801d72109761763",
            "paperId": "c202f2679dfc8fc4af0967d1a801d72109761763",
            "title": "Pipeline Chain-of-Thought: A Prompt Method for Large Language Model Relation Extraction",
            "abstract": "The development of language models has been influencing approaches to relation extraction (RE) problems. Although large language models (LLMs) have demonstrated breakthrough potential in certain aspects, they are still in the exploratory stage for RE tasks. Currently, the mainstream approach to improving the RE performance of LLMs is through prompt fine-tuning, but most methods require providing entity information in the prompt, which effectively only allows the LLMs to perform relationship classification tasks. We propose the Pipeline Chain-of-Thought (Pipeline-COT), which breaks down the RE task into steps and transforms it into reasoning tasks that have flat scaling curves, thereby enabling the use of Chain-of-Thought (COT) to enhance model inference. In addition, our method utilizes n-shot samples to provide signals for the Bayesian inference of the model by prompting the LLMs to focus on specific concepts to generate answers. We evaluated pipeline-COT on the Chinese dataset DuIE2.0, and compared with baseline methods that require including entity information in the prompt, our method still shows competitive performance.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes the Pipeline Chain-of-Thought (Pipeline-COT), which breaks down the RE task into steps and transforms it into reasoning tasks that have flat scaling curves, thereby enabling the use of Chain- of-Th thought (COT) to enhance model inference."
            },
            "score": 3
        },
        {
            "id": "47d04bcfe0f1bed72d03c68cce76b4cf4be03f11",
            "paperId": "47d04bcfe0f1bed72d03c68cce76b4cf4be03f11",
            "title": "Exploring Small Language Models with Prompt-Learning Paradigm for Efficient Domain-Specific Text Classification",
            "abstract": "Domain-specific text classification faces the challenge of scarce labeled data due to the high cost of manual labeling. Prompt-learning, known for its efficiency in few-shot scenarios, is proposed as an alternative to traditional fine-tuning methods. And besides, although large language models (LLMs) have gained prominence, small language models (SLMs, with under 1B parameters) offer significant customizability, adaptability, and cost-effectiveness for domain-specific tasks, given industry constraints. In this study, we investigate the potential of SLMs combined with prompt-learning paradigm for domain-specific text classification, specifically within customer-agent interactions in retail. Our evaluations show that, in few-shot settings when prompt-based model fine-tuning is possible, T5-base, a typical SLM with 220M parameters, achieve approximately 75% accuracy with limited labeled data (up to 15% of full data), which shows great potentials of SLMs with prompt-learning. Based on this, We further validate the effectiveness of active few-shot sampling and the ensemble strategy in the prompt-learning pipeline that contribute to a remarkable performance gain. Besides, in zero-shot settings with a fixed model, we underscore a pivotal observation that, although the GPT-3.5-turbo equipped with around 154B parameters garners an accuracy of 55.16%, the power of well designed prompts becomes evident when the FLAN-T5-large, a model with a mere 0.5% of GPT-3.5-turbo's parameters, achieves an accuracy exceeding 31% with the optimized prompt, a leap from its sub-18% performance with an unoptimized one. Our findings underscore the promise of prompt-learning in classification tasks with SLMs, emphasizing the benefits of active few-shot sampling, and ensemble strategies in few-shot settings, and the importance of prompt engineering in zero-shot settings.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study investigates the potential of SLMs combined with prompt-learning paradigm for domain-specific text classification, specifically within customer-agent interactions in retail, and underscores the promise of prompt- learning in classification tasks with SLMs."
            },
            "score": 3
        },
        {
            "id": "acbe813244e07f32eb034d6c27547d772a995d1d",
            "paperId": "acbe813244e07f32eb034d6c27547d772a995d1d",
            "title": "Uncertainty Estimation for Language Reward Models",
            "abstract": "Language models can learn a range of capabilities from unsupervised training on text corpora. However, to solve a particular problem (such as text summarization) it is typically necessary to fine-tune them on a task-specific dataset. It is often easier for humans to choose between options than to provide labeled data, and prior work has achieved state-of-the-art performance by training a reward model from such preference comparisons. However, collecting a large preference comparison dataset is still expensive -- and the learned reward models are unreliable out-of-distribution. We seek to address these problems via uncertainty estimation, which can improve sample efficiency and robustness using active learning and risk-averse reinforcement learning (RL). Specifically, we use bootstrap aggregating (bagging) to train an ensemble of reward models differing in the initialization of their final layer. Ensembles have proved successful in prior applications of active learning, but we find that in our setting ensemble active learning does not outperform random sampling. Further experiments show that while the aggregate predictions are well-calibrated, the ensemble's estimated epistemic uncertainty is only weakly correlated with model error. We suspect this is because the ensemble members are fine-tuned from a single model and so are similar to one another. This suggests current pre-training methods will need to be modified to support uncertainty estimation, e.g. by training multiple language models.",
            "year": 2022,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that in this setting ensemble active learning does not outperform random sampling, and current pre-training methods will need to be modified to support uncertainty estimation, e.g. by training multiple language models."
            },
            "score": 3
        },
        {
            "id": "bf4700077294c369f64eda65f677dd4f61b43072",
            "paperId": "bf4700077294c369f64eda65f677dd4f61b43072",
            "title": "Uncertainty Estimation and Reduction of Pre-trained Models for Text Regression",
            "abstract": "Abstract State-of-the-art classification and regression models are often not well calibrated, and cannot reliably provide uncertainty estimates, limiting their utility in safety-critical applications such as clinical decision-making. While recent work has focused on calibration of classifiers, there is almost no work in NLP on calibration in a regression setting. In this paper, we quantify the calibration of pre- trained language models for text regression, both intrinsically and extrinsically. We further apply uncertainty estimates to augment training data in low-resource domains. Our experiments on three regression tasks in both self-training and active-learning settings show that uncertainty estimation can be used to increase overall performance and enhance model generalization.",
            "year": 2022,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper quantifies the calibration of pre- trained language models for text regression, both intrinsically and extrinsically, and applies uncertainty estimates to augment training data in low-resource domains."
            },
            "score": 3
        },
        {
            "id": "645d8c40f2a05f0b06f9338cf7635755532d747c",
            "paperId": "645d8c40f2a05f0b06f9338cf7635755532d747c",
            "title": "Uncertainty Awareness of Large Language Models Under Code Distribution Shifts: A Benchmark Study",
            "abstract": "Large Language Models (LLMs) have been widely employed in programming language analysis to enhance human productivity. Yet, their reliability can be compromised by various code distribution shifts, leading to inconsistent outputs. While probabilistic methods are known to mitigate such impact through uncertainty calibration and estimation, their efficacy in the language domain remains underexplored compared to their application in image-based tasks. In this work, we first introduce a large-scale benchmark dataset, incorporating three realistic patterns of code distribution shifts at varying intensities. Then we thoroughly investigate state-of-the-art probabilistic methods applied to CodeLlama using these shifted code snippets. We observe that these methods generally improve the uncertainty awareness of CodeLlama, with increased calibration quality and higher uncertainty estimation~(UE) precision. However, our study further reveals varied performance dynamics across different criteria (e.g., calibration error vs misclassification detection) and trade-off between efficacy and efficiency, highlighting necessary methodological selection tailored to specific contexts.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work thoroughly investigate state-of-the-art probabilistic methods applied to CodeLlama using three realistic patterns of code distribution shifts at varying intensities, and observes that these methods generally improve the uncertainty awareness of CodeLlama, with increased calibration quality and higher uncertainty estimation~(UE) precision."
            },
            "score": 3
        },
        {
            "id": "a860ba337cead5e2e970460522d6612a49836ff1",
            "paperId": "a860ba337cead5e2e970460522d6612a49836ff1",
            "title": "Uncertainty Estimation of Transformers' Predictions via Topological Analysis of the Attention Matrices",
            "abstract": "Determining the degree of confidence of deep learning model in its prediction is an open problem in the field of natural language processing. Most of the classical methods for uncertainty estimation are quite weak for text classification models. We set the task of obtaining an uncertainty estimate for neural networks based on the Transformer architecture. A key feature of such mo-dels is the attention mechanism, which supports the information flow between the hidden representations of tokens in the neural network. We explore the formed relationships between internal representations using Topological Data Analysis methods and utilize them to predict model's confidence. In this paper, we propose a method for uncertainty estimation based on the topological properties of the attention mechanism and compare it with classical methods. As a result, the proposed algorithm surpasses the existing methods in quality and opens up a new area of application of the attention mechanism, but requires the selection of topological features.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a method for uncertainty estimation based on the topological properties of the attention mechanism and compares it with classical methods, which surpasses the existing methods in quality and opens up a new area of application of the Attention mechanism, but requires the selection of topological features."
            },
            "score": 3
        },
        {
            "id": "7f6d48d7b1641d3d2fd4ee06c434a73af8fce07b",
            "paperId": "7f6d48d7b1641d3d2fd4ee06c434a73af8fce07b",
            "title": "Density-Softmax: Scalable and Calibrated Uncertainty Estimation under Distribution Shifts",
            "abstract": "Prevalent deterministic deep-learning models suffer from significant over-confidence under distribution shifts. Probabilistic approaches can reduce this problem but struggle with computational efficiency. In this paper, we propose Density-Softmax, a fast and lightweight deterministic method to improve calibrated uncertainty estimation via a combination of density function with the softmax layer. By using the latent representation's likelihood value, our approach produces more uncertain predictions when test samples are distant from the training samples. Theoretically, we show that Density-Softmax can produce high-quality uncertainty estimation with neural networks, as it is the solution of minimax uncertainty risk and is distance-aware, thus reducing the over-confidence of the standard softmax. Empirically, our method enjoys similar computational efficiency as a single forward pass deterministic with standard softmax on the shifted toy, vision, and language datasets across modern deep-learning architectures. Notably, Density-Softmax uses 4 times fewer parameters than Deep Ensembles and 6 times lower latency than Rank-1 Bayesian Neural Network, while obtaining competitive predictive performance and lower calibration errors under distribution shifts.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Density-Softmax is proposed, a fast and lightweight deterministic method to improve calibrated uncertainty estimation via a combination of density function with the softmax layer, which enjoys similar computational efficiency as a single forward pass deterministic with standard softmax on the shifted toy, vision, and language datasets across modern deep-learning architectures."
            },
            "score": 3
        },
        {
            "id": "5c69658626150ffd23945132c55427af48edd3e0",
            "paperId": "5c69658626150ffd23945132c55427af48edd3e0",
            "title": "A Bayesian framework for fusing multiple word knowledge models in videotext recognition",
            "abstract": "Videotext recognition is challenging due to low resolution, diverse fonts/styles, and cluttered background. Past methods enhanced recognition by using multiple frame averaging, image interpolation and lexicon correction, but recognition using multi-modality language models has not been explored. In this paper, we present a formal Bayesian framework for videotext recognition by combining multiple knowledge using mixture models, and describe a learning approach based on Expectation-Maximization (EM). In order to handle unseen words, a back-off smoothing approach derived from the Bayesian model is also presented. We exploited a prototype that fuses the model from closed caption and that from the British National Corpus. The model from closed caption is based on a unique time distance distribution model of videotext words and closed caption words. Our method achieves a significant performance gain, with word recognition rate of 76.8% and character recognition rate of 86.7%. The proposed methods also reduce false videotext detection significantly, with a false alarm rate of 8.2% without substantial loss of recall.",
            "year": 2003,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A formal Bayesian framework for videotext recognition by combining multiple knowledge using mixture models, and a learning approach based on Expectation-Maximization (EM) is presented in order to handle unseen words."
            },
            "score": 3
        },
        {
            "id": "adba05ebcd45b6a644f88b5a7da7fd88cc57ba63",
            "paperId": "adba05ebcd45b6a644f88b5a7da7fd88cc57ba63",
            "title": "Detecting irony and sarcasm in microblogs: The role of expressive signals and ensemble classifiers",
            "abstract": "The automatic detection of sarcasm and irony in user generated contents is one of the most challenging task of Natural Language Processing. In this paper we address this problem by introducing Bayesian Model Averaging (BMA), an ensemble approach to take into account several classifiers according to their reliabilities and their marginal probability predictions. The impact of the most used expressive signals (pragmatic particles and POS tags) have been evaluated in baseline models (traditional classifiers and majority voting) as well as in the proposed BMA approach. Experimental results highlight two main findings: (1) not all the features are equally able to characterize sarcasm and irony and (2) BMA not only outperforms traditional state of the art models, but is also able to ensure notable generalization capabilities both on ironic and sarcastic text.",
            "year": 2015,
            "citationCount": 54,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results highlight two main findings: not all the features are equally able to characterize sarcasm and irony and BMA not only outperforms traditional state of the art models, but is also able to ensure notable generalization capabilities both on ironic and sarcastic text."
            },
            "score": 3
        },
        {
            "id": "ea85128b3b237d3ab46578725fe7e38d3a6be0e5",
            "paperId": "ea85128b3b237d3ab46578725fe7e38d3a6be0e5",
            "title": "Probabilistic Deterministic Infinite Automata",
            "abstract": "We propose a novel Bayesian nonparametric approach to learning with probabilistic deterministic finite automata (PDFA). We define and develop a sampler for a PDFA with an infinite number of states which we call the probabilistic deterministic infinite automata (PDIA). Posterior predictive inference in this model, given a finite training sequence, can be interpreted as averaging over multiple PDFAs of varying structure, where each PDFA is biased towards having few states. We suggest that our method for averaging over PDFAs is a novel approach to predictive distribution smoothing. We test PDIA inference both on PDFA structure learning and on both natural language and DNA data prediction tasks. The results suggest that the PDIA presents an attractive compromise between the computational cost of hidden Markov models and the storage requirements of hierarchically smoothed Markov models.",
            "year": 2010,
            "citationCount": 25,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results suggest that the probabilistic deterministic infinite automata (PDIA) presents an attractive compromise between the computational cost of hidden Markov models and the storage requirements of hierarchically smoothedMarkov models."
            },
            "score": 3
        },
        {
            "id": "16d51348772f5debedc1777a7f907e096a3fe9d0",
            "paperId": "16d51348772f5debedc1777a7f907e096a3fe9d0",
            "title": "Bayesian Model Averaging of Chain Event Graphs for Robust Explanatory Modelling",
            "abstract": "Chain Event Graphs (CEGs) are a widely applicable class of probabilistic graphical models that are able to represent context-speci\ufb01c independence statements and asymmetric unfoldings of events in an easily interpretable way. Existing model selection literature on CEGs has focused on obtaining the maximum a posteriori (MAP) CEG. However, MAP selection is well-known to ignore model uncertainty. Here, we explore the use of model averaging over this class. We demonstrate that such methods express model uncertainty and lead to more robust inference. Because the space of possible CEGs is huge, scoring models ex-haustively for model averaging in all but small prob-lems is prohibitive. However we show that a bespoke class of hybrid forward sampling and greedy search algorithms can successfully and intelligently traverse this space of candidate models. By applying a simple version of our search method to two known case studies, we can illustrate the e\ufb03cacy of such methods compared to more standard MAP modelling. We also demonstrate how its outputs systematically inform those component hypotheses that are most robustly supported the data and high-scoring alternative models to the MAP model.",
            "year": 2022,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that a bespoke class of hybrid forward sampling and greedy search algorithms can successfully and intelligently traverse this space of candidate models and express model uncertainty and lead to more robust inference."
            },
            "score": 3
        },
        {
            "id": "b1ad720958d30b684400204e35a4b560de26e84b",
            "paperId": "b1ad720958d30b684400204e35a4b560de26e84b",
            "title": "Bayesian Network Model Averaging Classifiers by Subbagging",
            "abstract": "When applied to classification problems, Bayesian networks are often used to infer a class variable when given feature variables. Earlier reports have described that the classification accuracy of Bayesian network structures achieved by maximizing the marginal likelihood (ML) is lower than that achieved by maximizing the conditional log likelihood (CLL) of a class variable given the feature variables. Nevertheless, because ML has asymptotic consistency, the performance of Bayesian network structures achieved by maximizing ML is not necessarily worse than that achieved by maximizing CLL for large data. However, the error of learning structures by maximizing the ML becomes much larger for small sample sizes. That large error degrades the classification accuracy. As a method to resolve this shortcoming, model averaging has been proposed to marginalize the class variable posterior over all structures. However, the posterior standard error of each structure in the model averaging becomes large as the sample size becomes small; it subsequently degrades the classification accuracy. The main idea of this study is to improve the classification accuracy using subbagging, which is modified bagging using random sampling without replacement, to reduce the posterior standard error of each structure in model averaging. Moreover, to guarantee asymptotic consistency, we use the K-best method with the ML score. The experimentally obtained results demonstrate that our proposed method provides more accurate classification than earlier BNC methods and the other state-of-the-art ensemble methods do.",
            "year": 2022,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The main idea of this study is to improve the classification accuracy using subbagging, which is modified bagging using random sampling without replacement, to reduce the posterior standard error of each structure in model averaging, using the K-best method with the ML score."
            },
            "score": 3
        },
        {
            "id": "ba85bd355fbb85533221181edc8385cef0c83e21",
            "paperId": "ba85bd355fbb85533221181edc8385cef0c83e21",
            "title": "Bayesian Model Averaging Over Directed Acyclic Graphs With Implications for the Predictive Performance of Structural Equation Models",
            "abstract": "This article examines Bayesian model averaging as a means of addressing predictive performance in Bayesian structural equation models. The current approach to addressing the problem of model uncertainty lies in the method of Bayesian model averaging. We expand the work of Madigan and his colleagues by considering a structural equation model as a special case of a directed acyclic graph. We then provide an algorithm that searches the model space for submodels and obtains a weighted average of the submodels using posterior model probabilities as weights. Our simulation study provides a frequentist evaluation of our Bayesian model averaging approach and indicates that when the true model is known, Bayesian model averaging does not yield necessarily better predictive performance compared to nonaveraged models. However, our case study using data from an international large-scale assessment reveals that the model-averaged submodels provide better posterior predictive performance compared to the initially specified model.",
            "year": 2016,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This article examines Bayesian model averaging as a means of addressing predictive performance in Bayesian structural equation models by considering a structural equation model as a special case of a directed acyclic graph and provides an algorithm that searches the model space for submodels and obtains a weighted average of the submodels using posterior model probabilities as weights."
            },
            "score": 3
        },
        {
            "id": "c3822dc3bde80f1b016565efa3e21e6c91e3cb04",
            "paperId": "c3822dc3bde80f1b016565efa3e21e6c91e3cb04",
            "title": "Bayesian Model Averaging for Linear Regression Models",
            "abstract": "We consider the problem of accounting for model uncertainty in linear regression models. Conditioning on a single selected model ignores model uncertainty, and thus leads to the underestimation of uncertainty when making inferences about quantities of interest. A Bayesian solution to this problem involves averaging over all possible models (i.e., combinations of predictors) when making inferences about quantities of Adrian E. Raftery is Professor of Statistics and Sociology, David Madigan is Assistant Professor of Statistics, both at the Department of Statistics,University of Washington, Box 354322, Seattle, WA 98195-4322. Jennifer Hoeting is Assistant Professor of Statistics at the Department of Statistics, Colorado State University, Fort Collins, CO 80523. The research of Raftery and Hoeting was partially supported by ONR Contract N-00014-91-J-1074. Madigan's research was partially supported by NSF grant no. DMS 92111627. The authors are grateful to Danika Lew for research assistance and the Editor, the Associate Editor, two anonymous referees and David Draper for very helpful comments that greatly improved the article.",
            "year": 1997,
            "citationCount": 1631,
            "tldr": null,
            "score": 3
        },
        {
            "id": "e01515c6138bc525f7aec30fc85f2adf028d4156",
            "paperId": "e01515c6138bc525f7aec30fc85f2adf028d4156",
            "title": "Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision",
            "abstract": "Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and guide the LLM through in-context learning from demonstrations (of principles application) to produce helpful, ethical, and reliable responses to user's queries; third, we fine-tune the original LLM with the high-quality self-aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally, we offer a refinement step to address the issues of overly-brief or indirect responses. Applying SELF-ALIGN to the LLaMA-65b base language model, we develop an AI assistant named Dromedary. With fewer than 300 lines of human annotations (including<200 seed prompts, 16 generic principles, and 5 exemplars for in-context learning). Dromedary significantly surpasses the performance of several state-of-the-art AI systems, including Text-Davinci-003 and Alpaca, on benchmark datasets with various settings.",
            "year": 2023,
            "citationCount": 137,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An AI assistant named Dromedary is developed, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision and significantly surpasses the performance of several state-of-the-art AI systems on benchmark datasets with various settings."
            },
            "score": 3
        },
        {
            "id": "628952a4b739cab34e585abe07124299071d874c",
            "paperId": "628952a4b739cab34e585abe07124299071d874c",
            "title": "Prompt-GAN\u2013Customisable Hate Speech and Extremist Datasets via Radicalised Neural Language Models",
            "abstract": "Online hate speech and violent extremism knows no borders, no political boundaries, no remorse. Researchers face an uphill battle to collect hate speech data in volumes and topical diversity suitable for training state-of-the-art content-moderation systems. Neural language models ushered in a new era of synthetic data generation in use across various businesses, all despite calls for research to protect against unintended toxic output. We present a method for radicalising pre-trained neural language models to identify real hate speech and highlight the risks of AI which could undermine our trust in social media. We present Prompt-GAN, a prompt-tuning adversarial approach with three achievements. Namely, we demonstrate prompt-tuning\u2019s ability to generate realistic types of hate and non-hate speech which mimics political extremist discourse. Prompt-GAN\u2019s architecture offers a twofold reduction in memory and runtime requirements compared to fine-tuning. Prompt-GAN improves hate speech classification F1-scores by up to 10.1% and sets a new record in neural language simulation compared to the current state-of-the-art across three benchmark social media datasets.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work demonstrates prompt-tuning\u2019s ability to generate realistic types of hate and non-hate speech which mimics political extremist discourse and presents a method for radicalising pre-trained neural language models to identify real hate speech."
            },
            "score": 3
        },
        {
            "id": "edfb5696b5431bd20eb57964c083e9118e153e97",
            "paperId": "edfb5696b5431bd20eb57964c083e9118e153e97",
            "title": "Extracting accurate materials data from research papers with conversational language models and prompt engineering",
            "abstract": null,
            "year": 2023,
            "citationCount": 26,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes the ChatExtract method, a method that can fully automate very accurate data extraction with minimal initial effort and background, using an advanced conversational LLM, and shows that approaches similar to ChatExtract are likely to become powerful tools for data extraction in the near future."
            },
            "score": 3
        },
        {
            "id": "ed8da147e18cac651564ab9f45523670861d344e",
            "paperId": "ed8da147e18cac651564ab9f45523670861d344e",
            "title": "Multiple detectors-based Bayesian inference and statistics analysis for incipient fault detection",
            "abstract": "To establish a secure and dependable operational setting for practical industrial processes, it is crucial to detect incipient faults promptly and accurately. In this work, a novel data-driven process monitoring approach is presented for incipient fault detection. First, inspired by the ensemble learning algorithm, multiple Mahalanobis distance (MD) detectors are trained, and corresponding detection indices for each sample are calculated. Second, with Bayesian inference, all the obtained indices are converted to probabilities for samples being faulty. Furthermore, the statistics of probabilities are analyzed by implementing sliding time window technique, ultimately achieving incipient fault detection. Finally, the effectiveness of the proposed strategy is demonstrated by a numerical example and the continuous stirred tank reactor benchmark process. The proposed approach inherits the advantages of ensemble learning, so it has the capability to improve the stability of detection performance. Meanwhile, due to the utilization of Bayesian inference, together with the sliding time window technique, it can better differentiate normal and faulty samples and improve the sensitivity to incipient faults.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The proposed approach inherits the advantages of ensemble learning, so it has the capability to improve the stability of detection performance and better differentiate normal and faulty samples and improve the sensitivity to incipient faults."
            },
            "score": 3
        },
        {
            "id": "44f2d8f60bb0a708291c96060ac200b6960bd608",
            "paperId": "44f2d8f60bb0a708291c96060ac200b6960bd608",
            "title": "Bayesian Optimized Ensemble Decision Tree models for MT-VSC-HVDC Transmission Line Protection",
            "abstract": "Over the last few decades, the High Voltage Direct Current (HVDC) technology has experienced significant growth. HVDC grid technologies are increasingly being employed for strengthening transmission systems and improving connectivity. In cases of long-range and bulk power transmission, HVDC systems have proven to be an attractive option compared to HVAC systems. HVDC grids exhibit reduced power loss and almost negligible lines reactive power. Faults must be fixed promptly, regardless of any challenges. This study presents a fault detection and classification method based on Bayesian optimized decision tree classifiers for an MT-VSC-HVDC transmission system. The primary objective of this research is to extract the DC voltage and current signal through the relays installed in the HVDC network. Afterward, fourteen features are formulated using these signals for the experimentation. Based on these features, Bayesian-optimized decision tree classifier is used to identify and differentiate the faults events. The proposed approach enables rapid identification, faster detection, and fixation of both internal and external faults. The proposed approach is rigorously assessed for various probable fault circumstances simulated with varying transmission system operating parameters. This experimental approach considerably reduces the complexity and time required to identify faults at various points on the HVDC transmission grids with high precision.",
            "year": 2023,
            "citationCount": 0,
            "tldr": null,
            "score": 3
        },
        {
            "id": "4f5664d04db25401fcd47bb3099278fbe8ae098f",
            "paperId": "4f5664d04db25401fcd47bb3099278fbe8ae098f",
            "title": "Contamination Source Identification: A Bayesian Framework Integrating Physical and Statistical Models",
            "abstract": "Contamination in the water distribution network poses a serious threat to this critical infrastructure. Detecting contamination sources promptly and accurately, so that remedial action can be taken, is highly desirable. Traditional methods mainly address the source detection problem by brute-force forward simulations, followed by applying statistical or optimization techniques to massive simulation results. Backtracking water parcels from downstream to upstream is more efficient, but it fails in the face of random water demand because tracking requires deterministic hydraulic conditions. To solve this problem, we propose a Bayesian framework that integrates a physical model of forward and backward tracking of contaminant movement into a statistical model to update the probability of contamination events that report the location and time. In the framework, we first impute an ensemble of realizations of water demand. In each demand realization, contaminants are backtracked, and a collection of contamination events is identified. Then, every contamination event is simulated to obtain artificial sensor data. By comparing simulated and incoming field sensor data, the probability of each contamination event is iteratively updated, with high updated probability implying strong suspicion. The efficiency and effectiveness of the framework are demonstrated using a well-studied network and compared with existing methods.",
            "year": 2021,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Bayesian framework is proposed that integrates a physical model of forward and backward tracking of contaminant movement into a statistical model to update the probability of contamination events that report the location and time."
            },
            "score": 3
        },
        {
            "id": "05213fa9aa41f33ed9009a4420ae12d62c25d917",
            "paperId": "05213fa9aa41f33ed9009a4420ae12d62c25d917",
            "title": "Scaling Laws for Hyperparameter Optimization",
            "abstract": "Hyperparameter optimization is an important subfield of machine learning that focuses on tuning the hyperparameters of a chosen algorithm to achieve peak performance. Recently, there has been a stream of methods that tackle the issue of hyperparameter optimization, however, most of the methods do not exploit the dominant power law nature of learning curves for Bayesian optimization. In this work, we propose Deep Power Laws (DPL), an ensemble of neural network models conditioned to yield predictions that follow a power-law scaling pattern. Our method dynamically decides which configurations to pause and train incrementally by making use of gray-box evaluations. We compare our method against 7 state-of-the-art competitors on 3 benchmarks related to tabular, image, and NLP datasets covering 59 diverse tasks. Our method achieves the best results across all benchmarks by obtaining the best any-time results compared to all competitors.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes Deep Power Laws (DPL), an ensemble of neural network models conditioned to yield predictions that follow a power-law scaling pattern that achieves the best results across all benchmarks by obtaining the best any-time results compared to all competitors."
            },
            "score": 3
        },
        {
            "id": "5125d07c02393c3b7754a53738394cbc25c44822",
            "paperId": "5125d07c02393c3b7754a53738394cbc25c44822",
            "title": "Quantitative knowledge retrieval from large language models",
            "abstract": "Large language models (LLMs) have been extensively studied for their abilities to generate convincing natural language sequences, however their utility for quantitative information retrieval is less well understood. In this paper we explore the feasibility of LLMs as a mechanism for quantitative knowledge retrieval to aid data analysis tasks such as elicitation of prior distributions for Bayesian models and imputation of missing data. We present a prompt engineering framework, treating an LLM as an interface to a latent space of scientific literature, comparing responses in different contexts and domains against more established approaches. Implications and challenges of using LLMs as 'experts' are discussed.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The feasibility of LLMs as a mechanism for quantitative knowledge retrieval to aid data analysis tasks such as elicitation of prior distributions for Bayesian models and imputation of missing data is explored."
            },
            "score": 2
        },
        {
            "id": "a54dcfb673e1bbf5c709908160ca2fa70f2b4bb2",
            "paperId": "a54dcfb673e1bbf5c709908160ca2fa70f2b4bb2",
            "title": "Recovering Mental Representations from Large Language Models with Markov Chain Monte Carlo",
            "abstract": "Simulating sampling algorithms with people has proven a useful method for efficiently probing and understanding their mental representations. We propose that the same methods can be used to study the representations of Large Language Models (LLMs). While one can always directly prompt either humans or LLMs to disclose their mental representations introspectively, we show that increased efficiency can be achieved by using LLMs as elements of a sampling algorithm. We explore the extent to which we recover human-like representations when LLMs are interrogated with Direct Sampling and Markov chain Monte Carlo (MCMC). We found a significant increase in efficiency and performance using adaptive sampling algorithms based on MCMC. We also highlight the potential of our method to yield a more general method of conducting Bayesian inference \\textit{with} LLMs.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The extent to which human-like representations are recovered when LLMs are interrogated with Direct Sampling and Markov chain Monte Carlo is explored and a significant increase in efficiency and performance is found using adaptive sampling algorithms based on MCMC."
            },
            "score": 2
        },
        {
            "id": "23c265ba884b92ecbd9d18641078d964697e4590",
            "paperId": "23c265ba884b92ecbd9d18641078d964697e4590",
            "title": "Generating Training Data with Language Models: Towards Zero-Shot Language Understanding",
            "abstract": "Pretrained language models (PLMs) have demonstrated remarkable performance in various natural language processing tasks: Unidirectional PLMs (e.g., GPT) are well known for their superior text generation capabilities; bidirectional PLMs (e.g., BERT) have been the prominent choice for natural language understanding (NLU) tasks. While both types of models have achieved promising few-shot learning performance, their potential for zero-shot learning has been underexplored. In this paper, we present a simple approach that uses both types of PLMs for fully zero-shot learning of NLU tasks without requiring any task-specific data: A unidirectional PLM generates class-conditioned texts guided by prompts, which are used as the training data for fine-tuning a bidirectional PLM. With quality training data selected based on the generation probability and regularization techniques (label smoothing and temporal ensembling) applied to the fine-tuning stage for better generalization and stability, our approach demonstrates strong performance across seven classification tasks of the GLUE benchmark (e.g., 72.3/73.8 on MNLI-m/mm and 92.8 on SST-2), significantly outperforming zero-shot prompting methods and achieving even comparable results to strong few-shot approaches using 32 training samples per class.",
            "year": 2022,
            "citationCount": 129,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper presents a simple approach that uses both types of PLMs for fully zero-shot learning of NLU tasks without requiring any task-specific data: a unidirectional PLM generates class-conditioned texts guided by prompts, which are used as the training data for fine-tuning a bidirectionalPLM."
            },
            "score": 2
        },
        {
            "id": "d479ef0ece2425042c2a80307ea154c85a9b14f9",
            "paperId": "d479ef0ece2425042c2a80307ea154c85a9b14f9",
            "title": "Uncertainty Estimation for Debiased Models: Does Fairness Hurt Reliability?",
            "abstract": "When deploying a machine learning model, one should aim not only to optimize performance metrics such as accuracy but also care about model fairness and reliability. Fairness means that the model is prevented from learning spurious correlations between a target variable and socio-economic attributes, and is generally achieved by applying debiasing techniques. Model reliability stems from the ability to determine whether we can trust model predictions for the given data. This can be achieved using uncertainty estimation (UE) methods. Debi-asing and UE techniques potentially interfere with each other, raising the question of whether we can achieve both reliability and fairness at the same time. This work aims to answer this question empirically based on an extensive series of experiments combining state-of-the-art UE and debiasing methods, and examining the impact on model performance, fairness, and reliability. 1",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work aims to answer the question whether a machine learning model can achieve both reliability and fairness at the same time empirically based on an extensive series of experiments combining state-of-the-art UE and debiasing methods, and examining the impact on model performance, fairness, and reliability."
            },
            "score": 2
        },
        {
            "id": "f2d0f9309a4ca6e9d712f72778a9bcf083ace077",
            "paperId": "f2d0f9309a4ca6e9d712f72778a9bcf083ace077",
            "title": "Uncertainty estimation in deep learning with application to spoken language assessment",
            "abstract": "Since convolutional neural networks (CNNs) achieved top performance on the ImageNet task in 2012, deep learning has become the preferred approach to addressing computer vision, natural language processing, speech recognition and bio-informatics tasks. However, despite impressive performance, neural networks tend to make over-confident predictions. Thus, it is necessary to investigate robust, interpretable and tractable estimates of uncertainty in a model\u2019s predictions in order to construct safer Machine Learning systems. This is crucial to applications where the cost of an error is high, such as in autonomous vehicle control, high-stakes automatic proficiency assessment and in the medical, financial and legal fields. In the first part of this thesis uncertainty estimation via ensemble and single-model approaches is discussed in detail and a new class of models for uncertainty estimation, called Prior Networks, is proposed. Prior Networks are able to emulate an ensemble of models using a single deterministic neural network, which allows sources of uncertainty to be determined within the same probabilistic framework as in ensemble-based approaches, but with the computational simplicity and ease of training of single-model approaches. Thus, Prior Networks combine the advantages of ensemble and single-model approaches to estimating uncertainty. In this thesis Prior Networks are evaluated on a range classification datasets, where they are shown to outperform baseline approaches, such as Monte-Carlo dropout, on the task of detecting out-of-distribution inputs. In the second part of this thesis deep learning and uncertainty estimation approaches are applied to the area of automatic assessment of non-native spoken language proficiency. Specifically deep-learning based graders and spoken response relevance assessment systems are constructed using data from the BULATS and LinguaSkill exams, provided by Cambridge English Language Assessment. Baseline approaches for uncertainty estimation discussed and evaluated in the first half of the thesis are then applied to these models and assessed on the task of rejecting predictions to be graded by human examiners and detecting misclassifications.",
            "year": 2019,
            "citationCount": 63,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Prior Networks combine the advantages of ensemble and single-model approaches to estimating uncertainty and are evaluated on a range classification datasets, where they are shown to outperform baseline approaches on the task of detecting out-of-distribution inputs."
            },
            "score": 2
        },
        {
            "id": "92e8eb55794b208952cf190f56e9d4663ad049cc",
            "paperId": "92e8eb55794b208952cf190f56e9d4663ad049cc",
            "title": "Natural language processing systems for pathology parsing in limited data environments with uncertainty estimation",
            "abstract": "Abstract Objective Cancer is a leading cause of death, but much of the diagnostic information is stored as unstructured data in pathology reports. We aim to improve uncertainty estimates of machine learning-based pathology parsers and evaluate performance in low data settings. Materials and methods Our data comes from the Urologic Outcomes Database at UCSF which includes 3232 annotated prostate cancer pathology reports from 2001 to 2018. We approach 17 separate information extraction tasks, involving a wide range of pathologic features. To handle the diverse range of fields, we required 2 statistical models, a document classification method for pathologic features with a small set of possible values and a token extraction method for pathologic features with a large set of values. For each model, we used isotonic calibration to improve the model\u2019s estimates of its likelihood of being correct. Results Our best document classifier method, a convolutional neural network, achieves a weighted F1 score of 0.97 averaged over 12 fields and our best extraction method achieves an accuracy of 0.93 averaged over 5 fields. The performance saturates as a function of dataset size with as few as 128 data points. Furthermore, while our document classifier methods have reliable uncertainty estimates, our extraction-based methods do not, but after isotonic calibration, expected calibration error drops to below 0.03 for all extraction fields. Conclusions We find that when applying machine learning to pathology parsing, large datasets may not always be needed, and that calibration methods can improve the reliability of uncertainty estimates.",
            "year": 2020,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that when applying machine learning to pathology parsing, large datasets may not always be needed, and that calibration methods can improve the reliability of uncertainty estimates."
            },
            "score": 2
        },
        {
            "id": "2e2c31fd97fc6ce27640bfc56f4b3ceca4f0cb9c",
            "paperId": "2e2c31fd97fc6ce27640bfc56f4b3ceca4f0cb9c",
            "title": "Uncertainty Estimation for Complex Text Detection in Spanish",
            "abstract": "Text simplifcation refers to the transformation of a source text aiming to increase its readiblity and understandability for a specific target population. This task is an important step towards improving inclusivity of such target populations (i.e., low scholarity or visually/hearing impaired groups). The recent advancements in the field brought by Large Language Models improve the performance of machine based text simplification approaches. However, using Language Models to simplify large text segments can be resource demanding. A more simple model to classify whether the text segment is worth to simplify or not can improve resource efficiency, in order to avoid unnecessary text prompts to the Large Language Models. Furthermore, text simplicity categorization can also be used for other purposes, such as text complexity measurement. The discrimination of text segments into simple and complex categories might lead to a number of false positives or negatives for a not well-tuned model. A way to control the acceptance threshold, is the implementation of an uncertainty score for each prediction. In this work we explore two simple uncertainty estimation approaches for complex text identification: a Monte Carlo Dropout and an Deep Ensemble Based approach. We use an in-house dataset in the financial education domain for our tests. We calibrated the two implemented methods to find out which performs better, using a Jensen-Shannon based distance between the correct and incorrect outputs of the discriminator. Our tests showed an important advantage of the Monte Carlo Dropout over the Deep Ensemble Based method.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores two simple uncertainty estimation approaches for complex text identification: a Monte Carlo Dropout and an Deep Ensemble Based approach, and calibrated the two implemented methods to find out which performs better."
            },
            "score": 2
        },
        {
            "id": "3bc4bbe8c21b381067a8687b0c2cbcdf0c475ec9",
            "paperId": "3bc4bbe8c21b381067a8687b0c2cbcdf0c475ec9",
            "title": "Bayesian model averaging: An application to the determinants of airport departure delay in Uganda",
            "abstract": "Bayesian model averaging was employed to study the dynamics of aircraft departure delay based on airport operational data of aviation and meteorological parameters collected on daily basis for the period 2004 through 2008 in matrix X. Models were evaluated using the R programming language mainly to establish the combinations of variables that could formulate the best model through assessing their importance. Findings showed that out of the sixteen covariates, 62.5% were suitable for model inclusion to determine aircraft departure delay of which 40% exhibited negative coefficients. The following parameters were found to negatively affect departure delay; number of aircrafts that departed on time (-0.562), number of persons on board of the arriving aircrafts (-0.002), daily average visibility (-0.001) and year (-1.605). Comparison between Posterior Model Probabilities (PMP Exact) and that based on Markov Chain Monte Carlo (PMP MCMC) revealed a high correlation (0.998; p<0.01).The study recommended the MCMC as providing a more efficient approach to modelling the determinants of aircraft departure delay at an airport.",
            "year": 2014,
            "citationCount": 7,
            "tldr": null,
            "score": 2
        },
        {
            "id": "73890ee155fd35e459ed335744e2f5f1bb5e10c1",
            "paperId": "73890ee155fd35e459ed335744e2f5f1bb5e10c1",
            "title": "Bayesian Model Averaging for Spatial Autoregressive Models Based on Convex Combinations of Different Types of Connectivity Matrices",
            "abstract": "Abstract There is a great deal of literature regarding use of nongeographically based connectivity matrices or combinations of geographic and non-geographic structures in spatial econometric models. We focus on convex combinations of weight matrices that result in a single weight matrix reflecting multiple types of connectivity, where coefficients from the convex combination can be used for inference regarding the relative importance of each type of connectivity in the global cross-sectional dependence scheme. We tackle the question of model uncertainty regarding selection of the best convex combination by Bayesian model averaging. We use Metropolis\u2013Hastings guided Monte Carlo integration during MCMC estimation of the models to produce log-marginal likelihoods and associated posterior model probabilities. We focus on MCMC estimation, computation of posterior model probabilities, model averaged estimates of the parameters, scalar summary measures of the non-linear partial derivative impacts, and their associated empirical measures of dispersion.",
            "year": 2020,
            "citationCount": 26,
            "tldr": null,
            "score": 2
        },
        {
            "id": "9aa4d8461f9d6169e54e10baf3164212c252c80a",
            "paperId": "9aa4d8461f9d6169e54e10baf3164212c252c80a",
            "title": "Enhancing Recommendation Diversity by Re-ranking with Large Language Models",
            "abstract": "It has long been recognized that it is not enough for a Recommender System (RS) to provide recommendations based only on their relevance to users. Among many other criteria, the set of recommendations may need to be diverse in order to handle uncertainty and offer a meaningful choice. The literature reports many ways of measuring diversity and ways of improving the diversity of a set of recommendations, most notably by re-ranking and selecting from a larger set of candidate recommendations. Driven by promising insights from the literature on how to incorporate versatile Large Language Models (LLMs) into the RS pipeline, in this paper, we show how LLMs can be used for diversity re-ranking. We begin with an informal study that verifies that LLMs can be used for re-ranking tasks and do have some understanding of the concept of diversity. Then, we design a more rigorous methodology where LLMs are prompted to generate a diverse ranking from a candidate ranking using various prompt templates with different re-ranking instructions in a zero-shot fashion. We conduct comprehensive experiments testing state-of-the-art conversational LLMs from the GPT and Llama families. We compare their re-ranking capabilities with random re-ranking and various traditional re-ranking methods from the literature (MMR, xQuAD and RxQuAD). We find that LLM-based re-ranking outperforms random re-ranking across all the metrics that we use but does not perform as well as the traditional re-ranking methods. We gain insight into prompt design for this task (e.g.\\ on the whole, it is better to prompt for diversity rather than a balance of diversity and relevance). Given that no special knowledge engineering is needed, we conclude that LLM-based re-ranking is a promising approach, and we highlight directions for future research. We open-source the code of our experiments for reproducibility.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that LLM-based re-ranking outperforms random re-ranking across all the metrics that the authors use but does not perform as well as the traditional re-ranking methods."
            },
            "score": 2
        },
        {
            "id": "0a922b4fdbe923b5161b5c6f5adfe586bf7304c3",
            "paperId": "0a922b4fdbe923b5161b5c6f5adfe586bf7304c3",
            "title": "Large language models will not replace healthcare professionals: curbing popular fears and hype",
            "abstract": "Following the release of ChatGPT, large language models (LLMs) have entered the mainstream. ChatGPT and GPT-4 recently garnered particular attention for attaining expert-level performance in United States Medical Licensing Examinations. However, performance is not perfect, and has not been as impressive in more specialised tests, such as the Membership of the Royal College of General Practitioners Applied Knowledge Test. ChatGPT frequently \u2018hallucinates\u2019, providing false, unverified information in the same manner as which it delivers facts. While performance in clinical tasks is expected to improve dramatically with the release of GPT-4, remaining inaccuracy and lack of an uncertainty indicator preclude autonomous deployment of ChatGPT and LLM chatbots like it in clinical settings. LLM applications may nevertheless revolutionise cognitive work \u2013 tools such as ChatGPT excel in tasks where specialist knowledge is not required, or is provided by the user prompt: examples include correcting language and rephrasing information for different audiences or within other constraints (e.g. word limits), and it has already been proposed as a tool for administrative tasks, clinical work and patient education. While this does represent an impressive advance in natural language processing, and benefits may be manifold across fields including medicine, these limited use-cases do not live up to the hype surrounding LLMs and artificial intelligence (AI) more generally in 2023. This is due to a fundamental misunderstanding about the form of AI represented by LLMs. Do LLMs represent artificial generalised intelligence (AGI)? The answer is currently probably not, despite emergence of interactive conversational interfaces and few-shot or zero-shot properties \u2013 where models execute tasks that they have previously been exposed to only a few times before, or never before, respectively. This is demonstrated by observing how these models are trained, and the composition of their architecture. The backend LLM (GPT-3, from which GPT-3.5 was developed) underpinning older versions of ChatGPT was initially trained on a dataset of billions of words taken from books, Wikipedia and the wider internet. Through a process of machine learning, the GPT-3 accurately encoded the association between individual words in the training dataset. Through \u2018reinforcement learning from human feedback\u2019, GPT-3 was subsequently finetuned to provide appropriate responses to users\u2019 queries \u2013 producing GPT-3.5. Through these processes, ChatGPT has developed an impressive ability to respond appropriately to diverse prompts, albeit equally lucidly with accurate and inaccurate statements. This lucidity, responsiveness and flexibility have led to sensational claims regarding attainment of AGI that could feasibly replace professionals in cognitive roles. The performance of GPT-4 \u2013 which powers newer versions of ChatGPT \u2013 dwarfs that of GPT-3.5 across tasks including logical reasoning and medical aptitude tests. Moreover, GPT-4 can be prompted to adopt different roles on demand, and will accept multimodal input, processing images as well as text. Prominent figures in industry and academia have advocated for a moratorium on development of more advanced AI systems in response to concerns regarding safety, ethics and fears of replacement. Despite these fears and hype, the barriers to implementation of LLMs replacing healthcare professionals in any capacity still look out of reach. Although GPT-4\u2019s architecture and training are confidential, it likely relies on similar schemata to its predecessor as it exhibits similar (albeit fewer) hallucinations and reasoning errors, including in medicine. None of ChatGPT\u2019s published autonomous training involved actual comprehension of language in context; the meaning (as we understand it) of words in the dataset was immaterial throughout. While this brute force linguistic processing may prove sufficient to develop a form of AGI, it appears that these LLMs will continue to be afflicted by mistakes and errors. Journal of the Royal Society of Medicine; 2023, Vol. 116(5) 181\u2013182",
            "year": 2023,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Following the release of ChatGPT, large language models (LLMs) have entered the mainstream and recently garnered particular attention for attaining expert-level performance in United States Medical Licensing Examinations, but performance has not been as impressive in more specialised tests."
            },
            "score": 2
        },
        {
            "id": "b842ec712ff4ac7793016c5d4c03c0b0b37b998b",
            "paperId": "b842ec712ff4ac7793016c5d4c03c0b0b37b998b",
            "title": "LLMatic: Neural Architecture Search via Large Language Models and Quality-Diversity Optimization",
            "abstract": "Large Language Models (LLMs) have emerged as powerful tools capable of accomplishing a broad spectrum of tasks. Their abilities span numerous areas, and one area where they have made a significant impact is in the domain of code generation. Here, we propose using the coding abilities of LLMs to introduce meaningful variations to code defining neural networks. Meanwhile, Quality-Diversity (QD) algorithms are known to discover diverse and robust solutions. By merging the code-generating abilities of LLMs with the diversity and robustness of QD solutions, we introduce \\texttt{LLMatic}, a Neural Architecture Search (NAS) algorithm. While LLMs struggle to conduct NAS directly through prompts, \\texttt{LLMatic} uses a procedural approach, leveraging QD for prompts and network architecture to create diverse and high-performing networks. We test \\texttt{LLMatic} on the CIFAR-10 and NAS-bench-201 benchmarks, demonstrating that it can produce competitive networks while evaluating just $2,000$ candidates, even without prior knowledge of the benchmark domain or exposure to any previous top-performing models for the benchmark. The open-sourced code is available in \\url{https://github.com/umair-nasir14/LLMatic}.",
            "year": 2023,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "While LLMs struggle to conduct NAS directly through prompts, \\texttt{LLMatic} uses a procedural approach, leveraging QD for prompts and network architecture to create diverse and high-performing networks."
            },
            "score": 2
        },
        {
            "id": "6fe5a62053253fd5338e8ca6ff4fe3bd59dd8e69",
            "paperId": "6fe5a62053253fd5338e8ca6ff4fe3bd59dd8e69",
            "title": "Hierarchical Bayesian method for constraining the neutron star equation of state with an ensemble of binary neutron star postmerger remnants",
            "abstract": "Binary neutron star (BNS) post-merger gravitational-wave emission can occur in the aftermath of a BNS merger -- provided the system avoids prompt collapse to a black hole -- as a quasistable hypermassive remnant experiences quadrupolar oscillations and non-axisymmetric deformations. The post-merger gravitational-wave spectrum possesses a characteristic peak frequency that has been shown to be dependent on the binary chirp mass and the neutron star equation of state (EoS), rendering post-merger gravitational waves a powerful tool for constraining neutron star composition. Unfortunately, the BNS post-merger signal is emitted at high ($\\gtrsim 1.5$ kHz) frequencies, where ground-based gravitational wave detectors suffer from reduced sensitivity. It is therefore unlikely that post-merger signals will be detected with sufficient signal-to-noise ratio (SNR) until the advent of next-generation detectors. However, by employing empirical relations derived from numerical relativity simulations, we can combine information across an ensemble of BNS mergers, allowing us to obtain EoS constraints with many low-SNR signals. We present a hierarchical Bayesian method for deriving constraints on $R_{1.6}$, the radius of a 1.6$\\mathrm{M_{\\odot}}$ neutron star, through an ensemble analysis of binary neutron star mergers. We apply this method to simulations of the next two LIGO-Virgo-KAGRA observing runs, O4 and O5, as well as an extended 4-year run at A+ sensitivity, demonstrating the potential of our approach to yield EoS information from the post-merger signal with current-generation detectors. The A+ 4-year scenario is predicted to improve the constraint on $R_{1.6}$ from the currently available multimessenger-based 95\\% credible interval (C.I.) uncertainty of $R_{1.6}=12.07^{+0.98}_{-0.77}$ km to $R_{1.6}=11.91^{+0.80}_{-0.56}$ km, a 22% reduction of the 95% C.I. width.",
            "year": 2022,
            "citationCount": 3,
            "tldr": null,
            "score": 2
        },
        {
            "id": "b63af1ad4623ae90fbaf0d93e425cf3b68ae8112",
            "paperId": "b63af1ad4623ae90fbaf0d93e425cf3b68ae8112",
            "title": "GRETL 2019, Proceedings of the International Conference on the Gnu Regression, Econometrics and Time-series Library",
            "abstract": "Publisher:\u00a0FedOA - Federico II University Press\u00a0 \nSeries:\u00a0School of Human and Social Sciences. Working Papers. \nPages:\u00a0VIII, 168. \nLanguage:\u00a0English. \nNBN:\u00a0http://nbn.depositolegale.it/urn:nbn:it:unina-25411 \nAbstract: This book collects the papers presented at the 6th Gretl Conference, which took place at the Dipartimento di Scienze Politiche, University of Naples, on 13th-14th of June, 2019. \nThe papers (which had been selected through a refereeing process) contain theoretical topics on computational problems as well as empirical applications using the program, and cover a wide range of topics in statistics and applied econometrics, among which Generalized Dynamic Factor Models, Propensity Score Matching, Bayesian Model Averaging, Spatial Models, Cointegration and Boostrap Techniques.",
            "year": 2019,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This book collects the papers presented at the 6th Gretl Conference, which took place at the Dipartimento di Scienze Politiche, University of Naples, on 13th-14th of June, 2019, and cover a wide range of topics in statistics and applied econometrics."
            },
            "score": 1
        }
    ],
    "novelty": "yes"
}