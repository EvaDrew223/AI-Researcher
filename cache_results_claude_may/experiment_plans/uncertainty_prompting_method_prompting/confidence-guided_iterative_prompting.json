{
    "topic_description": "novel prompting methods that can better quantify uncertainty or calibrate the confidence of large language models",
    "idea_name": "Confidence-Guided Iterative Prompting",
    "raw_idea": {
        "Problem": "Large language models often produce overconfident predictions, especially when dealing with out-of-distribution or ambiguous inputs. Existing methods for calibrating confidence scores, such as temperature scaling or ensemble methods, do not fully address this issue.",
        "Existing Methods": "Current approaches to confidence calibration include post-hoc methods like temperature scaling and Platt scaling, as well as ensemble methods that combine predictions from multiple models. However, these methods often rely on access to a labeled validation set and do not directly improve the model's underlying confidence estimates.",
        "Motivation": "Language models have shown the ability to iteratively refine their predictions when prompted to do so. By combining this iterative refinement process with confidence-guided feedback, we can encourage the model to focus on areas of uncertainty and progressively improve its confidence estimates.",
        "Proposed Method": "We propose Confidence-Guided Iterative Prompting (CGIP), a method that alternates between generating predictions and refining confidence estimates over multiple iterations. At each iteration, CGIP prompts the model to generate a prediction and an associated confidence score. If the confidence score falls below a specified threshold, CGIP prompts the model to revise its prediction, focusing on the parts of the input that contribute most to its uncertainty. This process is repeated for a fixed number of iterations or until the confidence score exceeds the threshold. The final prediction and confidence score are then returned. By iteratively refining its predictions and confidence estimates, CGIP allows the model to progressively reduce its uncertainty and improve its calibration.",
        "Experiment Plan": "We will evaluate CGIP on a range of language understanding and generation tasks, such as question answering, natural language inference, and open-ended generation. We will compare the calibration and accuracy of CGIP against baseline methods such as temperature scaling and ensemble methods, using metrics such as expected calibration error (ECE) and negative log likelihood (NLL). We will also conduct ablation studies to investigate the impact of different confidence thresholds and iteration limits on the performance of CGIP. Finally, we will qualitatively analyze the model's iterative refinements to gain insight into how CGIP improves confidence calibration."
    },
    "full_experiment_plan": {
        "Title": "Confidence-Guided Iterative Prompting for Improved Calibration in Large Language Models",
        "Problem Statement": "Large language models often produce overconfident predictions, especially when dealing with out-of-distribution or ambiguous inputs. Existing methods for calibrating confidence scores, such as temperature scaling or ensemble methods, do not fully address this issue.",
        "Motivation": "Recent work has attempted to address the overconfidence issue in large language models through post-hoc calibration methods or ensemble approaches. However, these methods often rely on access to a labeled validation set and do not directly improve the model's underlying confidence estimates. We draw inspiration from the iterative refinement capabilities of language models, where they have shown the ability to progressively improve their predictions when prompted to do so. By combining this iterative refinement process with confidence-guided feedback, we aim to encourage the model to focus on areas of uncertainty and iteratively improve its confidence estimates.",
        "Proposed Method": "We propose Confidence-Guided Iterative Prompting (CGIP), a method that alternates between generating predictions and refining confidence estimates over multiple iterations. At each iteration, CGIP prompts the model to generate a prediction and an associated confidence score. If the confidence score falls below a specified threshold, CGIP prompts the model to revise its prediction, focusing on the parts of the input that contribute most to its uncertainty. This process is repeated for a fixed number of iterations or until the confidence score exceeds the threshold. The final prediction and confidence score are then returned. By iteratively refining its predictions and confidence estimates, CGIP allows the model to progressively reduce its uncertainty and improve its calibration.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "Evaluate CGIP on a range of language understanding and generation tasks, such as question answering (SQuAD, TriviaQA), natural language inference (MNLI, SNLI), and open-ended generation (WritingPrompts). These datasets cover a diverse set of tasks and domains to assess the generalizability of the proposed method.",
            "Step 2: Construct Prompts": "For each task, create a set of prompts that include the input text and task-specific instructions. For the baseline methods (e.g., temperature scaling, ensemble), use the standard prompts without any modifications. For CGIP, construct prompts that guide the model to generate a prediction and confidence score, and to revise its prediction if the confidence is below a threshold. Example CGIP prompt for question answering: 'Question: [input_question]\nAnswer: [generated_answer]\nConfidence: [confidence_score]\nIf confidence is below [threshold], revise the answer focusing on the parts you are least certain about: [revised_answer]'",
            "Step 3: Select Models": "Experiment with state-of-the-art language models such as GPT-3.5 (text-davinci-002), GPT-4, and PaLM. These models have demonstrated strong performance across various language tasks and are suitable for evaluating the effectiveness of CGIP.",
            "Step 4: Implement Baselines": "Implement baseline calibration methods such as temperature scaling and ensemble methods. For temperature scaling, tune the temperature hyperparameter on a validation set to optimize calibration. For ensemble methods, train multiple models with different random seeds and combine their predictions using techniques like majority voting or confidence averaging.",
            "Step 5: Implement CGIP": "Implement the CGIP method as described in the proposed method section. Set the confidence threshold and maximum number of iterations based on preliminary experiments or domain knowledge. Prompt the model to generate an initial prediction and confidence score, and iteratively refine the prediction if the confidence is below the threshold.",
            "Step 6: Evaluate Calibration": "Evaluate the calibration of the baseline methods and CGIP using metrics such as Expected Calibration Error (ECE) and Brier Score. Lower ECE and Brier Score indicate better calibration. Compare the calibration performance of CGIP against the baselines to assess its effectiveness in improving confidence estimates.",
            "Step 7: Evaluate Accuracy": "Measure the accuracy of the predictions generated by the baseline methods and CGIP. While the primary focus is on calibration, it is important to ensure that improvements in calibration do not come at the cost of reduced accuracy. Compare the accuracy of CGIP against the baselines to assess its impact on overall performance.",
            "Step 8: Analyze Iterative Refinements": "Qualitatively analyze the iterative refinements generated by CGIP. Examine how the model revises its predictions and confidence scores over multiple iterations. Look for patterns or insights into how the model identifies and addresses areas of uncertainty. This analysis can provide valuable information on the effectiveness and interpretability of the CGIP method.",
            "Step 9: Conduct Ablation Studies": "Perform ablation studies to investigate the impact of different components and hyperparameters of CGIP. Vary the confidence threshold and maximum number of iterations to understand their effect on calibration and accuracy. Experiment with different prompting strategies or confidence score calculation methods to identify the most effective configuration.",
            "Step 10: Report Results": "Report the calibration and accuracy results for the baseline methods and CGIP across all evaluated tasks and datasets. Present the findings in a clear and concise manner, highlighting the improvements achieved by CGIP. Discuss the implications of the results and potential future directions for research in confidence calibration for large language models."
        },
        "Test Case Examples": {
            "Example 1": {
                "Input": "Question: What is the capital of France?\nAnswer: Paris is the capital of France.\nConfidence: 0.95\nIf confidence is below 0.98, revise the answer focusing on the parts you are least certain about:",
                "Baseline Output": "Paris is the capital of France.",
                "CGIP Output": "Paris is the capital of France. I am highly confident about this answer as it is a well-known fact and I do not have any uncertainty regarding the capital of France being Paris.",
                "Explanation": "In this example, the baseline method simply generates the correct answer without considering the confidence score. CGIP, on the other hand, prompts the model to assess its confidence and revise the answer if necessary. Since the initial confidence score of 0.95 is above the threshold of 0.98, CGIP does not make any revisions and instead reinforces its confidence in the answer."
            },
            "Example 2": {
                "Input": "Question: What is the largest planet in our solar system?\nAnswer: The largest planet in our solar system is Jupiter.\nConfidence: 0.8\nIf confidence is below 0.9, revise the answer focusing on the parts you are least certain about:",
                "Baseline Output": "The largest planet in our solar system is Jupiter.",
                "CGIP Output": "The largest planet in our solar system is Jupiter. I am fairly confident about this answer, but to increase my certainty, I would like to add that Jupiter is a gas giant and has a radius of about 69,911 kilometers, which is more than twice the size of all the other planets combined.",
                "Explanation": "In this example, the baseline method generates the correct answer but with a lower confidence score of 0.8. CGIP prompts the model to revise its answer since the confidence is below the threshold of 0.9. The model focuses on the parts it is least certain about and provides additional information to support its answer, increasing its confidence in the revised response."
            }
        },
        "Fallback Plan": "If the proposed CGIP method does not yield significant improvements in calibration or accuracy compared to the baselines, consider the following alternative approaches:\n1. Analyze the generated confidence scores and iterative refinements to identify potential issues or limitations. Investigate whether the confidence scores accurately reflect the model's uncertainty and if the refinements are meaningful and relevant.\n2. Experiment with different confidence thresholds and iteration limits to find the optimal configuration for each task and dataset. The effectiveness of CGIP may be sensitive to these hyperparameters, and fine-tuning them could lead to better results.\n3. Explore alternative prompting strategies or confidence score calculation methods. Investigate whether using different prompts or incorporating additional information (e.g., context, examples) can improve the model's ability to assess its confidence and generate more accurate refinements.\n4. Consider combining CGIP with other calibration techniques, such as temperature scaling or ensemble methods, to leverage their complementary strengths. Investigate whether a hybrid approach can further improve calibration and accuracy.\n5. If the above steps do not yield satisfactory results, focus on analyzing the limitations and challenges of confidence calibration in large language models. Conduct a thorough error analysis to identify common patterns or factors contributing to overconfidence or poor calibration. Use these insights to propose new research directions or hypotheses for improving confidence calibration in future work."
    },
    "novelty_queries": [
        "KeywordQuery(\"confidence calibration language models\")",
        "KeywordQuery(\"iterative prompting language models\")",
        "KeywordQuery(\"uncertainty estimation language models\")",
        "KeywordQuery(\"overconfidence mitigation language models\")",
        "KeywordQuery(\"Confidence-Guided Iterative Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "ac37accd7aedf1c25c3d54c7982579b297b3ff2b",
            "paperId": "ac37accd7aedf1c25c3d54c7982579b297b3ff2b",
            "title": "Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models",
            "abstract": "Large language models (LLMs) can achieve highly effective performance on various reasoning tasks by incorporating step-by-step chain-of-thought (CoT) prompting as demonstrations. However, the reasoning chains of demonstrations generated by LLMs are prone to errors, which can subsequently lead to incorrect reasoning during inference. Furthermore, inappropriate exemplars (overly simplistic or complex), can affect overall performance among varying levels of difficulty. We introduce Iter-CoT (Iterative bootstrapping in Chain-of-Thoughts Prompting), an iterative bootstrapping approach for selecting exemplars and generating reasoning chains. By utilizing iterative bootstrapping, our approach enables LLMs to autonomously rectify errors, resulting in more precise and comprehensive reasoning chains. Simultaneously, our approach selects challenging yet answerable questions accompanied by reasoning chains as exemplars with a moderate level of difficulty, which enhances the LLMs' generalizability across varying levels of difficulty. Experimental results indicate that Iter-CoT exhibits superiority, achieving competitive performance across three distinct reasoning tasks on ten datasets.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "By utilizing iterative bootstrapping, this approach enables LLMs to autonomously rectify errors, resulting in more precise and comprehensive reasoning chains, which enhances the LLMs' generalizability across varying levels of difficulty."
            },
            "score": 8,
            "novelty_score": "The project proposal aims to improve the calibration of confidence scores in large language models using an iterative prompting approach guided by confidence scores. The paper focuses on enhancing the reasoning capabilities of large language models in chain-of-thought prompting by iteratively bootstrapping exemplars and reasoning chains.\n\nProject proposal: Improving confidence calibration in large language models using confidence-guided iterative prompting.\nPaper: Enhancing reasoning capabilities of large language models in chain-of-thought prompting using iterative bootstrapping of exemplars and reasoning chains.\n\nWhile both the project proposal and the paper involve iterative methods to improve large language models, they address different research problems (confidence calibration vs. reasoning capabilities) and propose different approaches (confidence-guided prompting vs. bootstrapping exemplars and reasoning chains).\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "2aba5bba16dac5cd62683bab9de5d6faaaed0de1",
            "paperId": "2aba5bba16dac5cd62683bab9de5d6faaaed0de1",
            "title": "Shepherd Pre-trained Language Models to Develop a Train of Thought: An Iterative Prompting Approach",
            "abstract": "While Pre-trained Language Models (PLMs) 001 internalize a great amount of world knowledge, 002 they have been shown incapable of recalling 003 these knowledge to solve tasks requiring com-004 plex & multi-step inference procedures. Simi-005 lar to how humans develop a \u201ctrain of thought\u201d 006 for these tasks, how can we equip PLMs with 007 such abilities? In this work, we explore an iter-008 ative prompting framework, a new prompting 009 paradigm which progressively elicits relevant 010 knowledge from PLMs for multi-step inference 011 tasks. We identify key limitations of existing 012 prompting methods, namely they are either re-013 stricted to queries with a single identifiable re-014 lation/predicate, or being agnostic to input con-015 texts, which makes it difficult to capture vari-016 abilities across different inference steps. We 017 propose an iterative context-aware prompter, 018 which addresses these limitations by learning 019 to dynamically synthesize prompts conditioned 020 on the current step\u2019s contexts. Experiments on 021 three datasets involving multi-step inference 022 show the effectiveness of the iterative scheme 023 and the context-aware prompter design. 1 024",
            "year": 2022,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores an iter-008 ative prompting framework, a new prompting 009 paradigm which progressively elicits relevant 010 knowledge from PLMs for multi-step inference 011 tasks, and proposes an iterative context-aware prompter design which addresses key limitations of existing prompting methods."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the calibration of confidence scores in large language models, while the approach is to use iterative prompting guided by confidence scores to refine the model's predictions.\n\nThe research problem in the paper is enabling pre-trained language models to perform multi-step inference, while the approach is to use iterative prompting that progressively elicits relevant knowledge from the model.\n\nAlthough both works involve iterative prompting, the research problems and goals are different. The proposal focuses on improving confidence calibration, while the paper aims to enable multi-step inference in language models.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "3f4d11971f2c64be9125a7fe99c019588bbebf16",
            "paperId": "3f4d11971f2c64be9125a7fe99c019588bbebf16",
            "title": "Iteratively Prompt Pre-trained Language Models for Chain of Thought",
            "abstract": "While Pre-trained Language Models (PLMs) internalize a great amount of world knowledge, they have been shown incapable of recalling these knowledge to solve tasks requiring complex & multi-step reasoning. Similar to how humans develop a \u201cchain of thought\u201d for these tasks, how can we equip PLMs with such abilities? In this work, we explore an iterative prompting framework, a new prompting paradigm which progressively elicits relevant knowledge from PLMs for multi-step inference. We identify key limitations of existing prompting methods, namely they are either restricted to queries with a single identifiable relation/predicate, or being agnostic to input contexts, which makes it difficult to capture variabilities across different inference steps. We propose an iterative context-aware prompter, which addresses these limitations by learning to dynamically synthesize prompts conditioned on the current step\u2019s contexts. Experiments on three datasets involving multi-step reasoning show the effectiveness of the iterative scheme and the context-aware prompter design.",
            "year": 2022,
            "citationCount": 58,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An iterative prompting framework is explored, a new prompting paradigm which progressively elicits relevant knowledge from PLMs for multi-step inference by learning to dynamically synthesize prompts conditioned on the current step\u2019s contexts."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the calibration of confidence scores in large language models, while the approach is to use iterative prompting guided by confidence scores to refine the model's predictions.\n\nThe research problem in the paper is enabling pre-trained language models to perform multi-step reasoning, while the approach is to use an iterative prompting framework with a context-aware prompter to progressively elicit relevant knowledge from the model.\n\nThe proposal focuses on improving confidence calibration, while the paper focuses on multi-step reasoning. The proposed methods also differ, with the proposal using confidence-guided iterative prompting and the paper using a context-aware prompter in an iterative framework.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "1fa4469e5bc5d096572902fe14b0d66078a24c47",
            "paperId": "1fa4469e5bc5d096572902fe14b0d66078a24c47",
            "title": "Navigating the Grey Area: Expressions of Overconfidence and Uncertainty in Language Models",
            "abstract": "Despite increasingly \ufb02uent, relevant, and coherent language generation, major gaps remain between how humans and machines use language. We argue that a key dimension that is missing from our understanding of language models (LMs) is the model\u2019s ability to interpret and generate expressions of uncertainty . Whether it be the weatherperson announcing a chance of rain or a doctor giving a diagnosis, information is often not black-and-white and expressions of uncertainty provide nuance to support human-decision making. The increasing deployment of LMs in the wild motivates us to investigate whether LMs are capable of interpreting expressions of uncertainty and how LMs\u2019 behaviors change when learning to emit their own expressions of uncertainty. When injecting expressions of uncertainty into prompts (e.g., \"I think the answer is...\"), we discover that GPT3\u2019s generations vary upwards of 80% in accuracy based on the expression used. We analyze the linguistic characteristics of these expressions and \ufb01nd a drop in accuracy when naturalistic expressions of certainty are present. We \ufb01nd similar effects when teaching models to emit their own expressions of uncertainty, where model calibration suffers when teaching models to emit certainty rather than un certainty. Together, these results highlight the challenges of building LMs that interpret and generate trustworthy expressions of uncertainty.",
            "year": 2023,
            "citationCount": 54,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that GPT3\u2019s generations vary upwards of 80% in accuracy based on the expression used, and the challenges of building LMs that interpret and generate trustworthy expressions of uncertainty are highlighted."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the calibration of confidence scores in large language models, while the approach is to use iterative prompting guided by confidence scores. The research problem in the paper is understanding how language models interpret and generate expressions of uncertainty, and the approach is to analyze the impact of injecting expressions of uncertainty into prompts and teaching models to emit their own expressions of uncertainty.\n\nThe proposal focuses on improving calibration through iterative refinement, while the paper focuses on understanding the interpretation and generation of uncertainty expressions. Although both deal with the topic of confidence and uncertainty in language models, the specific research problems and approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "217e436fd23fe4184828e02a2b143835d6fd3b28",
            "paperId": "217e436fd23fe4184828e02a2b143835d6fd3b28",
            "title": "Navigating the Grey Area: How Expressions of Uncertainty and Overconfidence Affect Language Models",
            "abstract": "The increased deployment of LMs for real-world tasks involving knowledge and facts makes it important to understand model epistemology: what LMs think they know, and how their attitudes toward that knowledge are affected by language use in their inputs. Here, we study an aspect of model epistemology: how epistemic markers of certainty, uncertainty, or evidentiality like\"I'm sure it's\",\"I think it's\", or\"Wikipedia says it's\"affect models, and whether they contribute to model failures. We develop a typology of epistemic markers and inject 50 markers into prompts for question answering. We find that LMs are highly sensitive to epistemic markers in prompts, with accuracies varying more than 80%. Surprisingly, we find that expressions of high certainty result in a 7% decrease in accuracy as compared to low certainty expressions; similarly, factive verbs hurt performance, while evidentials benefit performance. Our analysis of a popular pretraining dataset shows that these markers of uncertainty are associated with answers on question-answering websites, while markers of certainty are associated with questions. These associations may suggest that the behavior of LMs is based on mimicking observed language use, rather than truly reflecting epistemic uncertainty.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that LMs are highly sensitive to epistemic markers in prompts, with accuracies varying more than 80%, and expressions of high certainty result in a 7% decrease in accuracy as compared to low certainty expressions; similarly, factive verbs hurt performance, while evidentials benefit performance."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the calibration of confidence scores in large language models, and the proposed approach is to use confidence-guided iterative prompting to refine the model's predictions and confidence estimates.\n\nThe research problem in the paper is understanding how epistemic markers of certainty, uncertainty, or evidentiality in the input prompts affect the accuracy and epistemology of language models.\n\nWhile both works deal with the topic of confidence and uncertainty in language models, the research problems and approaches are different. The proposal focuses on improving calibration through iterative prompting, while the paper studies the effect of epistemic markers on model accuracy and behavior.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "27dd800cb087f1575a65fba06c95ec8fd83a0fb4",
            "paperId": "27dd800cb087f1575a65fba06c95ec8fd83a0fb4",
            "title": "Fact-and-Reflection (FaR) Improves Confidence Calibration of Large Language Models",
            "abstract": "For a LLM to be trustworthy, its confidence level should be well-calibrated with its actual performance. While it is now common sense that LLM performances are greatly impacted by prompts, the confidence calibration in prompting LLMs has yet to be thoroughly explored. In this paper, we explore how different prompting strategies influence LLM confidence calibration and how it could be improved. We conduct extensive experiments on six prompting methods in the question-answering context and we observe that, while these methods help improve the expected LLM calibration, they also trigger LLMs to be over-confident when responding to some instances. Inspired by human cognition, we propose Fact-and-Reflection (FaR) prompting, which improves the LLM calibration in two steps. First, FaR elicits the known\"facts\"that are relevant to the input prompt from the LLM. And then it asks the model to\"reflect\"over them to generate the final answer. Experiments show that FaR prompting achieves significantly better calibration; it lowers the Expected Calibration Error by 23.5% on our multi-purpose QA tasks. Notably, FaR prompting even elicits the capability of verbally expressing concerns in less confident scenarios, which helps trigger retrieval augmentation for solving these harder instances.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Fact-and-Reflection prompting is proposed, which improves the LLM calibration in two steps, and even elicits the capability of verbally expressing concerns in less confident scenarios, which helps trigger retrieval augmentation for solving these harder instances."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the calibration of confidence scores in large language models, while the approach is to use iterative prompting guided by confidence scores to refine the model's predictions and confidence estimates.\n\nThe research problem in the paper is also improving the calibration of confidence scores in large language models, but the approach is different. The paper proposes a two-step prompting method called Fact-and-Reflection (FaR), which first elicits relevant facts from the model and then asks the model to reflect on them to generate the final answer.\n\nWhile both the proposal and the paper aim to improve confidence calibration in large language models, their proposed methods are different. The proposal focuses on iterative prompting guided by confidence scores, while the paper introduces a two-step prompting approach that involves fact elicitation and reflection.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "5d3105a5ffa133b873537bda8ff1ec6244c2b841",
            "paperId": "5d3105a5ffa133b873537bda8ff1ec6244c2b841",
            "title": "Think Twice Before Assure: Confidence Estimation for Large Language Models through Reflection on Multiple Answers",
            "abstract": "Confidence estimation aiming to evaluate output trustability is crucial for the application of large language models (LLM), especially the black-box ones. Existing confidence estimation of LLM is typically not calibrated due to the overconfidence of LLM on its generated incorrect answers. Existing approaches addressing the overconfidence issue are hindered by a significant limitation that they merely consider the confidence of one answer generated by LLM. To tackle this limitation, we propose a novel paradigm that thoroughly evaluates the trustability of multiple candidate answers to mitigate the overconfidence on incorrect answers. Building upon this paradigm, we introduce a two-step framework, which firstly instructs LLM to reflect and provide justifications for each answer, and then aggregates the justifications for comprehensive confidence estimation. This framework can be integrated with existing confidence estimation approaches for superior calibration. Experimental results on six datasets of three tasks demonstrate the rationality and effectiveness of the proposed framework.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel paradigm that thoroughly evaluates the trustability of multiple candidate answers to mitigate the overconfidence on incorrect answers and introduces a two-step framework, which firstly instructs LLM to reflect and provide justifications for each answer, and then aggregates the justifications for comprehensive confidence estimation."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is overconfidence in large language models, especially when dealing with out-of-distribution or ambiguous inputs. The proposed approach is Confidence-Guided Iterative Prompting (CGIP), which alternates between generating predictions and refining confidence estimates over multiple iterations.\n\nThe research problem in the paper is also overconfidence in large language models. The proposed approach is a two-step framework that instructs LLM to reflect and provide justifications for each answer, and then aggregates the justifications for comprehensive confidence estimation.\n\nBoth the proposal and the paper aim to address the overconfidence issue in large language models. However, the proposal focuses on iterative prompting and refinement based on confidence scores, while the paper proposes a framework that generates multiple candidate answers and aggregates their justifications for confidence estimation. Although they share the same high-level goal, the specific approaches differ.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "44d74b0d77b4056ddd4c6611a76711c8bab2e0a7",
            "paperId": "44d74b0d77b4056ddd4c6611a76711c8bab2e0a7",
            "title": "Dehallucinating Large Language Models Using Formal Methods Guided Iterative Prompting",
            "abstract": "Large language models (LLMs) such as ChatGPT have been trained to generate human-like responses to natural language prompts. LLMs use a vast corpus of text data for training, and can generate coherent and contextually relevant responses to a wide range of questions and statements. Despite this remarkable progress, LLMs are prone to hallucinations making their application to safety-critical applications such as autonomous systems difficult. The hallucinations in LLMs refer to instances where the model generates responses that are not factually accurate or contextually appropriate. These hallucinations can occur due to a variety of factors, such as the model\u2019s lack of real-world knowledge, the influence of biased or inaccurate training data, or the model\u2019s tendency to generate responses based on statistical patterns rather than a true understanding of the input. While these hallucinations are a nuisance in tasks such as text summarization and question-answering, they can be catastrophic when LLMs are used in autonomy-relevant applications such as planning. In this paper, we focus on the application of LLMs in autonomous systems and sketch a novel self-monitoring and iterative prompting architecture that uses formal methods to detect these errors in the LLM response automatically. We exploit the dialog capability of LLMs to iteratively steer them to responses that are consistent with our correctness specification. We report preliminary experiments that show the promise of the proposed approach on tasks such as automated planning.",
            "year": 2023,
            "citationCount": 18,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper sketches a novel self-monitoring and iterative prompting architecture that uses formal methods to detect errors in the LLM response automatically and exploits the dialog capability of LLMs to iteratively steer them to responses that are consistent with the correctness specification."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the calibration of confidence scores in large language models, while the approach is to use confidence-guided iterative prompting. On the other hand, the research problem in the paper is reducing hallucinations in large language models when used in autonomous systems, and the approach is to use formal methods to detect errors and iteratively steer the model to correct responses.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "161b3e82567b9a9c6911171fa55f05695bf93217",
            "paperId": "161b3e82567b9a9c6911171fa55f05695bf93217",
            "title": "GPT-4 Doesn't Know It's Wrong: An Analysis of Iterative Prompting for Reasoning Problems",
            "abstract": "There has been considerable divergence of opinion on the reasoning abilities of Large Language Models (LLMs). While the initial optimism that reasoning might emerge automatically with scale has been tempered thanks to a slew of counterexamples, a wide spread belief in their iterative self-critique capabilities persists. In this paper, we set out to systematically investigate the effectiveness of iterative prompting of LLMs in the context of Graph Coloring, a canonical NP-complete reasoning problem that is related to propositional satisfiability as well as practical problems like scheduling and allocation. We present a principled empirical study of the performance of GPT4 in solving graph coloring instances or verifying the correctness of candidate colorings. In iterative modes, we experiment with the model critiquing its own answers and an external correct reasoner verifying proposed solutions. In both cases, we analyze whether the content of the criticisms actually affects bottom line performance. The study seems to indicate that (i) LLMs are bad at solving graph coloring instances (ii) they are no better at verifying a solution--and thus are not effective in iterative modes with LLMs critiquing LLM-generated solutions (iii) the correctness and content of the criticisms--whether by LLMs or external solvers--seems largely irrelevant to the performance of iterative prompting. We show that the observed increase in effectiveness is largely due to the correct solution being fortuitously present in the top-k completions of the prompt (and being recognized as such by an external verifier). Our results thus call into question claims about the self-critiquing capabilities of state of the art LLMs.",
            "year": 2023,
            "citationCount": 38,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A principled empirical study of the performance of GPT4 in solving graph coloring instances or verifying the correctness of candidate colorings in the context of Graph Coloring, a canonical NP-complete reasoning problem that is related to propositional satisfiability as well as practical problems like scheduling and allocation."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the calibration of confidence scores in large language models, and the proposed approach is using confidence-guided iterative prompting to refine the model's predictions and confidence estimates.\n\nThe research problem in the paper is analyzing the effectiveness of iterative prompting for solving graph coloring problems, and the approach is experimenting with the model critiquing its own answers and an external correct reasoner verifying proposed solutions.\n\nThe proposal focuses on improving confidence calibration, while the paper focuses on evaluating reasoning abilities. The proposal uses iterative prompting to refine confidence scores, while the paper uses iterative prompting to solve graph coloring problems.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "444f3b7293b85b7d37600372941a289f9163abd1",
            "paperId": "444f3b7293b85b7d37600372941a289f9163abd1",
            "title": "LM-Polygraph: Uncertainty Estimation for Language Models",
            "abstract": "Recent advancements in the capabilities of large language models (LLMs) have paved the way for a myriad of groundbreaking applications in various fields. However, a significant challenge arises as these models often\"hallucinate\", i.e., fabricate facts without providing users an apparent means to discern the veracity of their statements. Uncertainty estimation (UE) methods are one path to safer, more responsible, and more effective use of LLMs. However, to date, research on UE methods for LLMs has been focused primarily on theoretical rather than engineering contributions. In this work, we tackle this issue by introducing LM-Polygraph, a framework with implementations of a battery of state-of-the-art UE methods for LLMs in text generation tasks, with unified program interfaces in Python. Additionally, it introduces an extendable benchmark for consistent evaluation of UE techniques by researchers, and a demo web application that enriches the standard chat dialog with confidence scores, empowering end-users to discern unreliable responses. LM-Polygraph is compatible with the most recent LLMs, including BLOOMz, LLaMA-2, ChatGPT, and GPT-4, and is designed to support future releases of similarly-styled LMs.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LM-Polygraph is introduced, a framework with implementations of a battery of state-of-the-art UE methods for LLMs in text generation tasks, with unified program interfaces in Python, and introduces an extendable benchmark for consistent evaluation of UE techniques by researchers."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the calibration of confidence scores in large language models, while the approach is to use iterative prompting guided by confidence scores. On the other hand, the paper focuses on implementing a framework for uncertainty estimation methods in language models and introducing a benchmark for evaluating these methods.\n\nProposal: Improving calibration of confidence scores in large language models using iterative prompting guided by confidence scores.\nPaper: Implementing a framework for uncertainty estimation methods in language models and introducing a benchmark for evaluating these methods.\n\nWhile both works deal with uncertainty estimation in language models, the specific research problems and approaches are different. The proposal aims to improve calibration through iterative prompting, while the paper focuses on implementing and evaluating various uncertainty estimation methods.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "ea0d41514a41f8273f13b3b277e7fcbbc65a8549",
            "paperId": "ea0d41514a41f8273f13b3b277e7fcbbc65a8549",
            "title": "Look Before You Leap: An Exploratory Study of Uncertainty Measurement for Large Language Models",
            "abstract": "The recent performance leap of Large Language Models (LLMs) opens up new opportunities across numerous industrial applications and domains. However, erroneous generations, such as false predictions, misinformation, and hallucination made by LLMs, have also raised severe concerns for the trustworthiness of LLMs', especially in safety-, security- and reliability-sensitive scenarios, potentially hindering real-world adoptions. While uncertainty estimation has shown its potential for interpreting the prediction risks made by general machine learning (ML) models, little is known about whether and to what extent it can help explore an LLM's capabilities and counteract its undesired behavior. To bridge the gap, in this paper, we initiate an exploratory study on the risk assessment of LLMs from the lens of uncertainty. In particular, we experiment with twelve uncertainty estimation methods and four LLMs on four prominent natural language processing (NLP) tasks to investigate to what extent uncertainty estimation techniques could help characterize the prediction risks of LLMs. Our findings validate the effectiveness of uncertainty estimation for revealing LLMs' uncertain/non-factual predictions. In addition to general NLP tasks, we extensively conduct experiments with four LLMs for code generation on two datasets. We find that uncertainty estimation can potentially uncover buggy programs generated by LLMs. Insights from our study shed light on future design and development for reliable LLMs, facilitating further research toward enhancing the trustworthiness of LLMs.",
            "year": 2023,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An exploratory study on the risk assessment of LLMs from the lens of uncertainty is initiated, finding that uncertainty estimation can potentially uncover buggy programs generated by LLMs."
            },
            "score": 6
        },
        {
            "id": "be8c90bca14d59f180f40a41126b7cd8c29c5d4e",
            "paperId": "be8c90bca14d59f180f40a41126b7cd8c29c5d4e",
            "title": "Uncertainty Quantification for In-Context Learning of Large Language Models",
            "abstract": "In-context learning has emerged as a groundbreaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM's response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM's response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model's configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-context learning in a plug-and-play fashion. Extensive experiments are conducted to demonstrate the effectiveness of the decomposition. The code and data are available at: https://github.com/lingchen0331/UQ_ICL.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations and ambiguities tied to the model's configurations (epistemic uncertainty)."
            },
            "score": 6
        },
        {
            "id": "7adb88771376c2a31688e3b0395b0550a35b824d",
            "paperId": "7adb88771376c2a31688e3b0395b0550a35b824d",
            "title": "Uncertainty Decomposition and Quantification for In-Context Learning of Large Language Models",
            "abstract": "In-context learning has emerged as a ground-breaking ability of Large Language Models (LLMs) and revolutionized various fields by providing a few task-relevant demonstrations in the prompt. However, trustworthy issues with LLM\u2019s response, such as hallucination, have also been actively discussed. Existing works have been devoted to quantifying the uncertainty in LLM\u2019s response, but they often overlook the complex nature of LLMs and the uniqueness of in-context learning. In this work, we delve into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations (aleatoric uncertainty) and ambiguities tied to the model\u2019s configurations (epistemic uncertainty). We propose a novel formulation and corresponding estimation method to quantify both types of uncertainties. The proposed method offers an unsupervised way to understand the prediction of in-context learning in a plug-and-play fashion. Extensive experiments are conducted to demonstrate the effectiveness of the decomposition. The code and data are available at: https://github. com/lingchen0331/UQ_ICL .",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work dives into the predictive uncertainty of LLMs associated with in-context learning, highlighting that such uncertainties may stem from both the provided demonstrations and ambiguities tied to the model\u2019s configurations (epistemic uncertainty)."
            },
            "score": 6
        },
        {
            "id": "8d863cafea3493fb033fcdcf9f272a1a4912628b",
            "paperId": "8d863cafea3493fb033fcdcf9f272a1a4912628b",
            "title": "Upstream Mitigation Is \n Not\n All You Need: Testing the Bias Transfer Hypothesis in Pre-Trained Language Models",
            "abstract": "A few large, homogenous, pre-trained models undergird many machine learning systems \u2014 and often, these models contain harmful stereotypes learned from the internet. We investigate the bias transfer hypothesis: the theory that social biases (such as stereotypes) internalized by large language models during pre-training transfer into harmful task-specific behavior after fine-tuning. For two classification tasks, we find that reducing intrinsic bias with controlled interventions before fine-tuning does little to mitigate the classifier\u2019s discriminatory behavior after fine-tuning. Regression analysis suggests that downstream disparities are better explained by biases in the fine-tuning dataset. Still, pre-training plays a role: simple alterations to co-occurrence rates in the fine-tuning dataset are ineffective when the model has been pre-trained. Our results encourage practitioners to focus more on dataset quality and context-specific harms.",
            "year": 2022,
            "citationCount": 28,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The bias transfer hypothesis is investigated: the theory that social biases internalized by large language models during pre-training transfer into harmful task-specific behavior after fine-tuning."
            },
            "score": 6
        },
        {
            "id": "785bd5915f83f941d36c7996a9742ae695880111",
            "paperId": "785bd5915f83f941d36c7996a9742ae695880111",
            "title": "FairPy: A Toolkit for Evaluation of Social Biases and their Mitigation in Large Language Models",
            "abstract": "Studies have shown that large pretrained language models exhibit biases against social groups based on race, gender etc, which they inherit from the datasets they are trained on. Various researchers have proposed mathematical tools for quantifying and identifying these biases. There have been methods proposed to mitigate such biases. In this paper, we present a comprehensive quantitative evaluation of different kinds of biases such as race, gender, ethnicity, age etc. exhibited by popular pretrained language models such as BERT, GPT-2 etc. and also present a toolkit that provides plug-and-play interfaces to connect mathematical tools to identify biases with large pretrained language models such as BERT, GPT-2 etc. and also present users with the opportunity to test custom models against these metrics. The toolkit also allows users to debias existing and custom models using the debiasing techniques proposed so far. The toolkit is available at https://github.com/HrishikeshVish/Fairpy.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A comprehensive quantitative evaluation of different kinds of biases such as race, gender, ethnicity, age etc exhibited by popular pretrained language models such as BERT, GPT-2 etc are presented."
            },
            "score": 6
        },
        {
            "id": "99bfe503743c5ec8e16e50ab8438159cdb533a89",
            "paperId": "99bfe503743c5ec8e16e50ab8438159cdb533a89",
            "title": "The Troubling Emergence of Hallucination in Large Language Models - An Extensive Definition, Quantification, and Prescriptive Remediations",
            "abstract": "The recent advancements in Large Language Models (LLMs) have garnered widespread acclaim for their remarkable emerging capabilities. However, the issue of hallucination has parallelly emerged as a by-product, posing significant concerns. While some recent endeavors have been made to identify and mitigate different types of hallucination, there has been a limited emphasis on the nuanced categorization of hallucination and associated mitigation methods. To address this gap, we offer a fine-grained discourse on profiling hallucination based on its degree, orientation, and category, along with offering strategies for alleviation. As such, we define two overarching orientations of hallucination: (i) factual mirage (FM) and (ii) silver lining (SL). To provide a more comprehensive understanding, both orientations are further sub-categorized into intrinsic and extrinsic, with three degrees of severity - (i) mild, (ii) moderate, and (iii) alarming. We also meticulously categorize hallucination into six types: (i) acronym ambiguity, (ii) numeric nuisance, (iii) generated golem, (iv) virtual voice, (v) geographic erratum, and (vi) time wrap. Furthermore, we curate HallucInation eLiciTation (HILT), a publicly available dataset comprising of 75,000 samples generated using 15 contemporary LLMs along with human annotations for the aforementioned categories. Finally, to establish a method for quantifying and to offer a comparative spectrum that allows us to evaluate and rank LLMs based on their vulnerability to producing hallucinations, we propose Hallucination Vulnerability Index (HVI). We firmly believe that HVI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making. In conclusion, we propose two solution strategies for mitigating hallucinations.",
            "year": 2023,
            "citationCount": 34,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work defines two overarching orientations of hallucination and proposes two solution strategies for mitigating hallucinations, and firmly believes that HVI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making."
            },
            "score": 6
        },
        {
            "id": "426f728b9023a1815ac273d06ed281e97be74cc6",
            "paperId": "426f728b9023a1815ac273d06ed281e97be74cc6",
            "title": "ArgMed-Agents: Explainable Clinical Decision Reasoning with Large Language Models via Argumentation Schemes",
            "abstract": "There are two main barriers to using large language models (LLMs) in clinical reasoning. Firstly, while LLMs exhibit significant promise in Natural Language Processing (NLP) tasks, their performance in complex reasoning and planning falls short of expectations. Secondly, LLMs use uninterpretable methods to make clinical decisions that are fundamentally different from the clinician's cognitive processes. This leads to user distrust. In this paper, we present a multi-agent framework called ArgMed-Agents, which aims to enable LLM-based agents to make explainable clinical decision reasoning through interaction. ArgMed-Agents performs self-argumentation iterations via Argumentation Scheme for Clinical Decision (a reasoning mechanism for modeling cognitive processes in clinical reasoning), and then constructs the argumentation process as a directed graph representing conflicting relationships. Ultimately, Reasoner(a symbolic solver) identify a series of rational and coherent arguments to support decision. ArgMed-Agents enables LLMs to mimic the process of clinical argumentative reasoning by generating explanations of reasoning in a self-directed manner. The setup experiments show that ArgMed-Agents not only improves accuracy in complex clinical decision reasoning problems compared to other prompt methods, but more importantly, it provides users with decision explanations that increase their confidence.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ArgMed-Agents enables LLMs to mimic the process of clinical argumentative reasoning by generating explanations of reasoning in a self-directed manner and improves accuracy in complex clinical decision reasoning problems compared to other prompt methods."
            },
            "score": 6
        },
        {
            "id": "af44c205c648c21d06064b23613dd60ecbd4adf8",
            "paperId": "af44c205c648c21d06064b23613dd60ecbd4adf8",
            "title": "Improving Zero-shot Visual Question Answering via Large Language Models with Reasoning Question Prompts",
            "abstract": "Zero-shot Visual Question Answering (VQA) is a prominent vision-language task that examines both the visual and textual understanding capability of systems in the absence of training data. Recently, by converting the images into captions, information across multi-modalities is bridged and Large Language Models (LLMs) can apply their strong zero-shot generalization capability to unseen questions. To design ideal prompts for solving VQA via LLMs, several studies have explored different strategies to select or generate question-answer pairs as the exemplar prompts, which guide LLMs to answer the current questions effectively. However, they totally ignore the role of question prompts. The original questions in VQA tasks usually encounter ellipses and ambiguity which require intermediate reasoning. To this end, we present Reasoning Question Prompts for VQA tasks, which can further activate the potential of LLMs in zero-shot scenarios. Specifically, for each question, we first generate self-contained questions as reasoning question prompts via an unsupervised question edition module considering sentence fluency, semantic integrity and syntactic invariance. Each reasoning question prompt clearly indicates the intent of the original question. This results in a set of candidate answers. Then, the candidate answers associated with their confidence scores acting as answer heuristics are fed into LLMs and produce the final answer. We evaluate reasoning question prompts on three VQA challenges, experimental results demonstrate that they can significantly improve the results of LLMs on zero-shot setting and outperform existing state-of-the-art zero-shot methods on three out of four data sets. Our source code is publicly released at https://github.com/ECNU-DASE-NLP/RQP.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Reasoning Question Prompts for VQA tasks are presented, which can further activate the potential of Large Language Models in zero-shot scenarios and can significantly improve the results of LLMs on zero- shot setting and outperform existing state-of-the-art zero-Shot methods on three out of four data sets."
            },
            "score": 6
        },
        {
            "id": "ab4ce5dda7ad4d9032995c9c049a89d65723c6aa",
            "paperId": "ab4ce5dda7ad4d9032995c9c049a89d65723c6aa",
            "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
            "abstract": "A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated. However, the most widely-used LMs are fine-tuned with reinforcement learning from human feedback (RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated. In light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50%.",
            "year": 2023,
            "citationCount": 96,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "For RLHF-LMs such as ChatGPT, GPT-4, and Claude, it is found that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50%."
            },
            "score": 5
        },
        {
            "id": "92746dfa09dcad92ecf1e6272ebb300c1112b7eb",
            "paperId": "92746dfa09dcad92ecf1e6272ebb300c1112b7eb",
            "title": "Automatic Calibration and Error Correction for Large Language Models via Pareto Optimal Self-Supervision",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities out of box for a wide range of applications, yet accuracy still remains a major growth area, especially in mission-critical domains such as biomedicine. An effective method to calibrate the con\ufb01dence level on LLM responses is essential to automatically detect errors and facilitate human-in-the-loop veri\ufb01cation. An important source of calibration signals stems from expert-stipulated programmatic super-vision, which is often available at low cost but has its own limitations such as noise and coverage. In this paper, we introduce a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every response, without any additional manual efforts. This is accomplished by learning a harmonizer model to align LLM output with other available supervision sources, which would assign higher risk scores to more uncertain LLM responses and facilitate error correction. Experiments on standard relation extraction tasks in biomedical and general domains demonstrate the promise of this approach, with our proposed risk scores highly correlated with the real error rate of LLMs. For the most uncertain test instances, dynamic prompting based on our proposed risk scores results in signi\ufb01cant accuracy improvement for off-the-shelf LLMs, boosting GPT-3 results past state-of-the-art (SOTA) weak supervision and GPT-4 results past SOTA supervised results on challenging evaluation datasets.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every response, without any additional manual efforts."
            },
            "score": 5
        },
        {
            "id": "9a61d51212eb4ff677fe777a7ba9ddc4f675b387",
            "paperId": "9a61d51212eb4ff677fe777a7ba9ddc4f675b387",
            "title": "Automatic Calibration and Error Correction for Generative Large Language Models via Pareto Optimal Self-Supervision",
            "abstract": "Generative Large language models (LLMs) have demonstrated remarkable capabilities for a wide range of applications, but reducing ungrounded or erroneous responses remains a major growth area. Unlike task-specific models, there lack an effective method to calibrate the confidence level of LLM responses to indicate potential errors and facilitate human-in-the-loop verification. An important source of calibration stems from expert-stipulated programmatic supervision, which is often available at low cost but has its own limitations such as noise and coverage. In this paper, we introduce a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every LLM response, without any additional manual efforts. This is accomplished by learning a harmonizer model to align with LLM output as well as other weak supervision sources. The model assigns higher risk scores to more uncertain LLM responses and facilitate error correction. Experiments on standard relation extraction and classification tasks in biomedical and general domains demonstrate that the proposed risk score is highly correlated with the actual LLM error rate. By using a dynamic prompting strategy based on the risk score, we observed significant accuracy improvement for off-the-shelf LLMs, boosting GPT-3.5 results past state-of-the-art (SOTA) weak supervision model and GPT-4 results past SOTA supervised results on challenging evaluation datasets.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every LLM response, without any additional manual efforts."
            },
            "score": 5
        },
        {
            "id": "036e96ed196a7f4bb812380f3b76ac75d4a648e4",
            "paperId": "036e96ed196a7f4bb812380f3b76ac75d4a648e4",
            "title": "Calibrating the Confidence of Large Language Models by Eliciting Fidelity",
            "abstract": "Large language models optimized with techniques like RLHF have achieved good alignment in being helpful and harmless. However, post-alignment, these language models often exhibit overconfidence, where the expressed confidence does not accurately calibrate with their correctness rate. In this paper, we decompose the language model confidence into the \\textit{Uncertainty} about the question and the \\textit{Fidelity} to the answer generated by language models. Then, we propose a plug-and-play method to estimate the confidence of language models. Our method has shown good calibration performance by conducting experiments with 6 RLHF-LMs on four MCQA datasets. Moreover, we propose two novel metrics, IPR and CE, to evaluate the calibration of the model, and we have conducted a detailed discussion on \\textit{Truly Well-Calibrated Confidence}. Our method could serve as a strong baseline, and we hope that this work will provide some insights into the model confidence calibration.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper decomposes the language model confidence into the uncertainty about the question and the fidelity to the answer generated by language models, and proposes a plug-and-play method to estimate the confidence of language models."
            },
            "score": 5
        },
        {
            "id": "56e7bda25b83228f91962d3465fd587cfe8908e1",
            "paperId": "56e7bda25b83228f91962d3465fd587cfe8908e1",
            "title": "How Far Can We Extract Diverse Perspectives from Large Language Models? Criteria-Based Diversity Prompting!",
            "abstract": "Collecting diverse human opinions is costly and challenging. This leads to a recent trend in collaborative efforts between humans and Large Language Models (LLMs) for generating diverse data, offering potential scalable and efficient solutions. However, the extent of LLMs' capability to generate diverse perspectives on subjective topics remains an unexplored question. In this study, we investigate LLMs' capacity for generating diverse perspectives and rationales on subjective topics, such as social norms and argumentative texts. We formulate a new problem of maximum diversity extraction from LLMs. Motivated by how humans develop their opinions through their values, we propose a criteria-based prompting technique to ground diverse opinions. To see how far we can extract diverse perspectives from LLMs, or called diversity coverage, we employ a step-by-step recall prompting for generating more outputs from the model in an iterative manner. As we apply our methods to various tasks, indeed we find that LLMs can generate diverse opinions according to the degree of task subjectivity",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study investigates LLMs' capacity for generating diverse perspectives and rationales on subjective topics, such as social norms and argumentative texts, and proposes a criteria-based prompting technique to ground diverse opinions."
            },
            "score": 5
        },
        {
            "id": "06e5828341aa3926e1d839039363b0673b9461cc",
            "paperId": "06e5828341aa3926e1d839039363b0673b9461cc",
            "title": "Errors are Useful Prompts: Instruction Guided Task Programming with Verifier-Assisted Iterative Prompting",
            "abstract": "Generating low-level robot task plans from high-level natural language instructions remains a challenging problem. Although large language models have shown promising results in generating plans, the accuracy of the output remains unverified. Furthermore, the lack of domain-specific language data poses a limitation on the applicability of these models. In this paper, we propose CLAIRIFY, a novel approach that combines automatic iterative prompting with program verification to ensure programs written in data-scarce domain-specific language are syntactically valid and incorporate environment constraints. Our approach provides effective guidance to the language model on generating structured-like task plans by incorporating any errors as feedback, while the verifier ensures the syntactic accuracy of the generated plans. We demonstrate the effectiveness of CLAIRIFY in planning chemistry experiments by achieving state-of-the-art results. We also show that the generated plans can be executed on a real robot by integrating them with a task and motion planner.",
            "year": 2023,
            "citationCount": 30,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes CLAIRIFY, a novel approach that combines automatic iterative prompting with program verification to ensure programs written in data-scarce domain-specific language are syntactically valid and incorporate environment constraints."
            },
            "score": 5
        },
        {
            "id": "c7ae496ae37b5c1fca5c0f7e277bc9356172efd1",
            "paperId": "c7ae496ae37b5c1fca5c0f7e277bc9356172efd1",
            "title": "PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs",
            "abstract": "Large language models (LLMs) have shown great abilities of solving various natural language tasks in different domains. Due to the training objective of LLMs and their pre-training data, LLMs are not very well equipped for tasks involving structured data generation. We propose a framework, Prompting with Iterative Verification (PiVe), to improve graph-based generative capability of LLMs. We show how a small language model could be trained to act as a verifier module for the output of an LLM(i.e., ChatGPT, GPT-4), and to iteratively improve its performance via fine-grained corrective instructions. We also show how the verifier module could apply iterative corrections offline for a more cost-effective solution to the text-to-graph generation task. Experiments on three graph-based datasets show consistent improvement gained via PiVe. Additionally, we create GenWiki-HIQ and highlight that the verifier module can be used as a data augmentation tool to help improve the quality of automatically generated parallel text-graph datasets.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A framework, Prompting with Iterative Verification (PiVe), is proposed, to improve graph-based generative capability of LLMs and shows how a small language model could be trained to act as a verifier module for the output of an LLM, and to iteratively improve its performance via fine-grained corrective instructions."
            },
            "score": 5
        },
        {
            "id": "5424e311319c58847b4c690d5c91090e3b6a4ac3",
            "paperId": "5424e311319c58847b4c690d5c91090e3b6a4ac3",
            "title": "Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models",
            "abstract": "While Large Language Models (LLMs) have demonstrated remarkable potential in natural language generation and instruction following, a persistent challenge lies in their susceptibility to\"hallucinations\", which erodes trust in their outputs. Although Uncertainty Quantification (UQ) presents a promising solution, its accurate implementation within the context of LLMs remains a significant hurdle. To address this critical roadblock, our research originates from a fundamental heuristic insight: tokens within auto-regressive LLM-generated text do not equally reflect the underlying meaning. Some tokens carry greater relevance and representativeness than others, owing to the phenomenon of\"linguistic redundancy\", wherein a select few keywords suffice to convey the essence of lengthy sentences. Regrettably, existing methodologies treat all tokens with equal importance when estimating uncertainty, disregarding these inherent generative inequalities. Our analysis reveals a significant issue with state-of-the-art: numerous tokens (and sentences) of limited semantic significance receive equal or even excessive weighting during uncertainty estimation. To rectify this bias, we propose to jointly Shifting Attention to more Relevant (SAR) components, at both the token- and the sentence-levels for accurate uncertainty estimation. We conduct extensive experiments involving a range of popular\"off-the-shelf\"LLMs, including instruction-tuned LLMs such as Vicuna, WizardLM, and LLaMA-2-chat, as well as pretrained LLMs like OPT and LLaMA, with model sizes extending up to 33B parameters. We carry out evaluation across various free-form question-answering tasks, encompassing domains such as reading comprehension, science Q&A, and medical Q&A. Our experimental results demonstrate the superior performance of SAR in addressing the challenges of uncertainty estimation within the realm of LLMs.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experimental results demonstrate the superior performance of SAR in addressing the challenges of uncertainty estimation within the realm of LLMs, and propose to jointly Shifting Attention to more Relevant (SAR) components, at both the token- and the sentence-levels for accurate uncertainty estimation."
            },
            "score": 5
        },
        {
            "id": "6d3ae6d6b312b659b3a14ae3f3e86a36db63200d",
            "paperId": "6d3ae6d6b312b659b3a14ae3f3e86a36db63200d",
            "title": "Efficient Non-Parametric Uncertainty Quantification for Black-Box Large Language Models and Decision Planning",
            "abstract": "Step-by-step decision planning with large language models (LLMs) is gaining attention in AI agent development. This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models. Existing approaches are either white-box or computationally demanding, limiting use of black-box proprietary LLMs within budgets. The paper's first contribution is a non-parametric uncertainty quantification method for LLMs, efficiently estimating point-wise dependencies between input-decision on the fly with a single inference, without access to token logits. This estimator informs the statistical interpretation of decision trustworthiness. The second contribution outlines a systematic design for a decision-making agent, generating actions like ``turn on the bathroom light'' based on user prompts such as ``take a bath''. Users will be asked to provide preferences when more than one action has high estimated point-wise dependencies. In conclusion, our uncertainty estimation and decision-making agent design offer a cost-efficient approach for AI agent development.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models, and outlines a systematic design for a decision-making agent, offering a cost-efficient approach for AI agent development."
            },
            "score": 5
        },
        {
            "id": "839cc546b58968e2a8cb968337fb2e3a279e2b00",
            "paperId": "839cc546b58968e2a8cb968337fb2e3a279e2b00",
            "title": "CHBias: Bias Evaluation and Mitigation of Chinese Conversational Language Models",
            "abstract": "redWarning: This paper contains content that may be offensive or upsetting.Pretrained conversational agents have been exposed to safety issues, exhibiting a range of stereotypical human biases such as gender bias. However, there are still limited bias categories in current research, and most of them only focus on English. In this paper, we introduce a new Chinese dataset, CHBias, for bias evaluation and mitigation of Chinese conversational language models.Apart from those previous well-explored bias categories, CHBias includes under-explored bias categories, such as ageism and appearance biases, which received less attention. We evaluate two popular pretrained Chinese conversational models, CDial-GPT and EVA2.0, using CHBias. Furthermore, to mitigate different biases, we apply several debiasing methods to the Chinese pretrained models. Experimental results show that these Chinese pretrained models are potentially risky for generating texts that contain social biases, and debiasing methods using the proposed dataset can make response generation less biased while preserving the models\u2019 conversational capabilities.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results show that these Chinese pretrained models are potentially risky for generating texts that contain social biases, and debiasing methods using the proposed dataset can make response generation less biased while preserving the models\u2019 conversational capabilities."
            },
            "score": 5
        },
        {
            "id": "c58db176890fa0695bcc6dcf28ce8ea207189fc5",
            "paperId": "c58db176890fa0695bcc6dcf28ce8ea207189fc5",
            "title": "Intersectional Bias Mitigation in Pre-trained Language Models: A Quantum-Inspired Approach",
            "abstract": "The growing criticality of contextualized language models has raised concerns about the perpetuation of biases. Current fairness research often concentrates on single aspects of social groups. However, our study brings a multifaceted approach to the table, reflecting the intricate realities of intersectionality. We propose a bias mitigation algorithm inspired by quantum theory to fine-tune pre-trained language models. Our method applies Jensen-Shannon divergence alongside an entanglement entropy measure to quantify the complexity of entwined identities. Moreover, we utilize an innovative entanglement embedding neural network to address emergent features from different intersectional groups. An Actor-Critic setup facilitates effective fine-tuning. Our approach broadens the scope of intersectional fairness beyond just statistical parity, providing a strategic tool to tackle complex, interrelated biases. We anticipate that this fresh approach to bias mitigation will substantially enhance fairness in a wide range of language model applications.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study proposes a bias mitigation algorithm inspired by quantum theory to fine-tune pre-trained language models, and utilizes an innovative entanglement embedding neural network to address emergent features from different intersectional groups."
            },
            "score": 5
        },
        {
            "id": "b4e637bc9fd4678b747b07af21ecc435dc1c5441",
            "paperId": "b4e637bc9fd4678b747b07af21ecc435dc1c5441",
            "title": "HuaSLIM: Human Attention Motivated Shortcut Learning Identification and Mitigation for Large Language models",
            "abstract": "Large language models have made remarkable progress on a variety of NLP tasks. However, it has been found that they tend to rely on shortcut features that spuriously correlate with labels for prediction, which weakens their generalization on out-of-distribution samples. In this paper, we propose a human attention guided approach to identifying and mitigating short-cut learning, which encourages the LLM-based target model to learn relevant features. We define an attention-based measurement to capture both model and data bias and identify short-cut tokens by exploring both human and neural attention. In a self-distillation framework, we mitigate shortcut learning by dynamically adjusting the distillation temperature according to the detected shortcut tokens and estimated shortcut degree. Additionally, we utilize human attention as a supervisory signal to constrain large language models to pay more attention to relevant tokens. Experimental results on multiple NLP tasks show that our proposed method can effectively identify shortcut tokens, and significantly improve the robustness of large language models on OOD samples, while not undermining the performance on IID data.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a human attention guided approach to identifying and mitigating short-cut learning, which encourages the LLM-based target model to learn relevant features and utilizes human attention as a supervisory signal to constrain large language models to pay more attention to relevant tokens."
            },
            "score": 5
        },
        {
            "id": "7baaaa623d2c3011a52e2bb515e030825fa6e36c",
            "paperId": "7baaaa623d2c3011a52e2bb515e030825fa6e36c",
            "title": "An Enhanced Prompt-Based LLM Reasoning Scheme via Knowledge Graph-Integrated Collaboration",
            "abstract": "While Large Language Models (LLMs) demonstrate exceptional performance in a multitude of Natural Language Processing (NLP) tasks, they encounter challenges in practical applications, including issues with hallucinations, inadequate knowledge updating, and limited transparency in the reasoning process. To overcome these limitations, this study innovatively proposes a collaborative training-free reasoning scheme involving tight cooperation between Knowledge Graph (KG) and LLMs. This scheme first involves using LLMs to iteratively explore KG, selectively retrieving a task-relevant knowledge subgraph to support reasoning. The LLMs are then guided to further combine inherent implicit knowledge to reason on the subgraph while explicitly elucidating the reasoning process. Through such a cooperative approach, our scheme achieves more reliable knowledge-based reasoning and facilitates the tracing of the reasoning results. Experimental results show that our scheme significantly progressed across multiple datasets, notably achieving over a 10% improvement on the QALD10 dataset compared to the best baseline and the fine-tuned state-of-the-art (SOTA) work. Building on this success, this study hopes to offer a valuable reference for future research in the fusion of KG and LLMs, thereby enhancing LLMs' proficiency in solving complex issues.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study innovatively proposes a collaborative training-free reasoning scheme involving tight cooperation between Knowledge Graph (KG) and LLMs that achieves more reliable knowledge-based reasoning and facilitates the tracing of the reasoning results."
            },
            "score": 5
        },
        {
            "id": "6920de816acd201aadc0de51cf0fa62fa92bb0cc",
            "paperId": "6920de816acd201aadc0de51cf0fa62fa92bb0cc",
            "title": "On the Calibration of Large Language Models and Alignment",
            "abstract": "As large language models attract increasing attention and find widespread application, concurrent challenges of reliability also arise at the same time. Confidence calibration, an effective analysis method for gauging the reliability of deep models, serves as a crucial tool for assessing and improving their reliability. However, such investigation has been comparatively underexplored. In this work, we conduct a systematic examination of the calibration of aligned language models throughout the entire construction process, including pretraining and alignment training. At each stage, we investigate how different training settings, such as parameter scales and training data, affect model calibration. To thoroughly assess model calibration, we evaluate models on three most concerned aspects: generation, factuality and understanding. Our work sheds light on whether popular LLMs are well-calibrated and how the training process influences model calibration.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work sheds light on whether popular LLMs are well-calibrated and how the training process influences model calibration, as well as how different training settings affect model calibration."
            },
            "score": 4
        },
        {
            "id": "47eb0468ba7b6457d32b6aa0ee15ad269c04864d",
            "paperId": "47eb0468ba7b6457d32b6aa0ee15ad269c04864d",
            "title": "Confidently Wrong: Exploring the Calibration and Expression of (Un)Certainty of Large Language Models in a Multilingual Setting",
            "abstract": "While the fluency and coherence of Large Language Models (LLMs) in text generation have seen significant improvements, their competency in generating appropriate expressions of uncertainty remains limited.Using a multilingual closed-book QA task and GPT-3.5, we explore how well LLMs are calibrated and express certainty across a diverse set of languages, including low-resource settings. Our results reveal strong performance in high-resource languages but a marked decline in performance in lower-resource languages. Across all, we observe an exaggerated expression of confidence in the model, which does not align with the correctness or likelihood of its responses. Our findings highlight the need for further research into accurate calibration of LLMs especially in a multilingual setting.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Using a multilingual closed-book QA task and GPT-3.5, how well LLMs are calibrated and express certainty across a diverse set of languages, including low-resource settings is explored."
            },
            "score": 4
        },
        {
            "id": "30669080bc6652f0466fba618b7c59317a346fb2",
            "paperId": "30669080bc6652f0466fba618b7c59317a346fb2",
            "title": "A Formalism and Approach for Improving Robustness of Large Language Models Using Risk-Adjusted Confidence Scores",
            "abstract": "Large Language Models (LLMs), such as ChatGPT, have achieved impressive milestones in natural language processing (NLP). Despite their impressive performance, the models are known to pose important risks. As these models are deployed in real-world applications, a systematic understanding of different risks posed by these models on tasks such as natural language inference (NLI), is much needed. In this paper, we define and formalize two distinct types of risk: decision risk and composite risk. We also propose a risk-centric evaluation framework, and four novel metrics, for assessing LLMs on these risks in both in-domain and out-of-domain settings. Finally, we propose a risk-adjusted calibration method called DwD for helping LLMs minimize these risks in an overall NLI architecture. Detailed experiments, using four NLI benchmarks, three baselines and two LLMs, including ChatGPT, show both the practical utility of the evaluation framework, and the efficacy of DwD in reducing decision and composite risk. For instance, when using DwD, an underlying LLM is able to address an extra 20.1% of low-risk inference tasks (but which the LLM erroneously deems high-risk without risk adjustment) and skip a further 19.8% of high-risk tasks, which would have been answered incorrectly.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper defines and formalizes two distinct types of risk: decision risk and composite risk, and proposes a risk-centric evaluation framework, and four novel metrics, for assessing LLMs on these risks in both in-domain and out-of-domain settings."
            },
            "score": 4
        },
        {
            "id": "48fb667125298cf724f7b652d521686180412351",
            "paperId": "48fb667125298cf724f7b652d521686180412351",
            "title": "A Close Look into the Calibration of Pre-trained Language Models",
            "abstract": "Pre-trained language models (PLMs) may fail in giving reliable estimates of their predictive uncertainty. We take a close look into this problem, aiming to answer two questions: (1) Do PLMs learn to become calibrated in the training process? (2) How effective are existing calibration methods? For the first question, we conduct fine-grained control experiments to study the dynamic change in PLMs\u2019 calibration performance in training. We consider six factors as control variables, including dataset difficulty, available training samples, training steps, the number of tunable parameters, model scale, and pretraining. We observe a consistent change in calibration performance across six factors. We find that PLMs don\u2019t learn to become calibrated in training, evidenced by the continual increase in confidence, no matter whether the predictions are correct or not. We highlight that our finding somewhat contradicts two established conclusions: (a) Larger PLMs are more calibrated; (b) Pretraining improves model calibration. Next, we study the effectiveness of existing calibration methods in mitigating the overconfidence issue. Besides unlearnable calibration methods (e.g., label smoothing), we adapt and extend two recently proposed learnable methods that directly collect data to train models to have reasonable confidence estimations. Experimental results show that learnable methods significantly reduce PLMs\u2019 confidence in wrong predictions.",
            "year": 2022,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that pre-trained language models don\u2019t learn to become calibrated in training, evidenced by the continual increase in confidence, no matter whether the predictions are correct or not."
            },
            "score": 4
        },
        {
            "id": "33422275fbb9958f55419620697faf531482699b",
            "paperId": "33422275fbb9958f55419620697faf531482699b",
            "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering",
            "abstract": "Abstract Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question, \u201cHow can we know when language models know, with confidence, the answer to a particular query?\u201d We examine this question from the point of view of calibration, the property of a probabilistic model\u2019s predicted probabilities actually being well correlated with the probabilities of correctness. We examine three strong generative models\u2014T5, BART, and GPT-2\u2014and study whether their probabilities on QA tasks are well calibrated, finding the answer is a relatively emphatic no. We then examine methods to calibrate such models to make their confidence scores correlate better with the likelihood of correctness through fine-tuning, post-hoc probability modification, or adjustment of the predicted outputs or inputs. Experiments on a diverse range of datasets demonstrate the effectiveness of our methods. We also perform analysis to study the strengths and limitations of these methods, shedding light on further improvements that may be made in methods for calibrating LMs. We have released the code at https://github.com/jzbjyb/lm-calibration.",
            "year": 2020,
            "citationCount": 233,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper examines three strong generative models -- T5, BART, and GPT-2 -- and examines methods to calibrate such models to make their confidence scores correlate better with the likelihood of correctness through fine-tuning, post-hoc probability modification, or adjustment of the predicted outputs or inputs."
            },
            "score": 4
        },
        {
            "id": "ba63e1ab5b6e9d849982ae293ac0483053badaff",
            "paperId": "ba63e1ab5b6e9d849982ae293ac0483053badaff",
            "title": "Uncertainty in Language Models: Assessment through Rank-Calibration",
            "abstract": "Language Models (LMs) have shown promising performance in natural language generation. However, as LMs often generate incorrect or hallucinated responses, it is crucial to correctly quantify their uncertainty in responding to given inputs. In addition to verbalized confidence elicited via prompting, many uncertainty measures ($e.g.$, semantic entropy and affinity-graph-based measures) have been proposed. However, these measures can differ greatly, and it is unclear how to compare them, partly because they take values over different ranges ($e.g.$, $[0,\\infty)$ or $[0,1]$). In this work, we address this issue by developing a novel and practical framework, termed $Rank$-$Calibration$, to assess uncertainty and confidence measures for LMs. Our key tenet is that higher uncertainty (or lower confidence) should imply lower generation quality, on average. Rank-calibration quantifies deviations from this ideal relationship in a principled manner, without requiring ad hoc binary thresholding of the correctness score ($e.g.$, ROUGE or METEOR). The broad applicability and the granular interpretability of our methods are demonstrated empirically.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel and practical framework, termed $Rank$-$Calibration$ is developed, to assess uncertainty and confidence measures for LMs, with the key tenet that higher uncertainty should imply lower generation quality, on average."
            },
            "score": 4
        },
        {
            "id": "b58b319d2b3f933ae201f747dabb4b9ea070e50e",
            "paperId": "b58b319d2b3f933ae201f747dabb4b9ea070e50e",
            "title": "Linguistic Calibration of Language Models",
            "abstract": "Language models (LMs) may lead their users to make suboptimal downstream decisions when they confidently hallucinate. This issue can be mitigated by having the LM verbally convey the probability that its claims are correct, but existing models cannot produce text with calibrated confidence statements. Through the lens of decision-making, we formalize linguistic calibration for long-form generations: an LM is linguistically calibrated if its generations enable its users to make calibrated probabilistic predictions. This definition enables a training framework where a supervised finetuning step bootstraps an LM to emit long-form generations with confidence statements such as\"I estimate a 30% chance of...\"or\"I am certain that...\", followed by a reinforcement learning step which rewards generations that enable a user to provide calibrated answers to related questions. We linguistically calibrate Llama 2 7B and find in automated and human evaluations of long-form generations that it is significantly more calibrated than strong finetuned factuality baselines with comparable accuracy. These findings generalize under distribution shift on question-answering and under a significant task shift to person biography generation. Our results demonstrate that long-form generations may be calibrated end-to-end by constructing an objective in the space of the predictions that users make in downstream decision-making.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results demonstrate that long-form generations may be calibrated end-to-end by constructing an objective in the space of the predictions that users make in downstream decision-making."
            },
            "score": 4
        },
        {
            "id": "f197bf0fc2f228483f6af3285000d54d8d97f9eb",
            "paperId": "f197bf0fc2f228483f6af3285000d54d8d97f9eb",
            "title": "Voyager: An Open-Ended Embodied Agent with Large Language Models",
            "abstract": "We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.",
            "year": 2023,
            "citationCount": 336,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 4
        },
        {
            "id": "afee8cdc51e95b50d7574ed1700a797874bf792c",
            "paperId": "afee8cdc51e95b50d7574ed1700a797874bf792c",
            "title": "Adversarial Fine-Tuning of Language Models: An Iterative Optimisation Approach for the Generation and Detection of Problematic Content",
            "abstract": "In this paper, we tackle the emerging challenge of unintended harmful content generation in Large Language Models (LLMs) with a novel dual-stage optimisation technique using adversarial fine-tuning. Our two-pronged approach employs an adversarial model, fine-tuned to generate potentially harmful prompts, and a judge model, iteratively optimised to discern these prompts. In this adversarial cycle, the two models seek to outperform each other in the prompting phase, generating a dataset of rich examples which are then used for fine-tuning. This iterative application of prompting and fine-tuning allows continuous refinement and improved performance. The performance of our approach is evaluated through classification accuracy on a dataset consisting of problematic prompts not detected by GPT-4, as well as a selection of contentious but unproblematic prompts. We show considerable increase in classification accuracy of the judge model on this challenging dataset as it undergoes the optimisation process. Furthermore, we show that a rudimentary model \\texttt{ada} can achieve 13\\% higher accuracy on the hold-out test set than GPT-4 after only a few rounds of this process, and that this fine-tuning improves performance in parallel tasks such as toxic comment identification.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper shows that a rudimentary model can achieve 13\\% higher accuracy on the hold-out test set than GPT-4 after only a few rounds of this process, and that this fine-tuning improves performance in parallel tasks such as toxic comment identification."
            },
            "score": 4
        },
        {
            "id": "0885471c0215b3c0d31c82518066913f7f738128",
            "paperId": "0885471c0215b3c0d31c82518066913f7f738128",
            "title": "Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement",
            "abstract": "The ability to derive underlying principles from a handful of observations and then generalize to novel situations -- known as inductive reasoning -- is central to human intelligence. Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks. In this work, we conduct a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting. Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules. By examining the intermediate rules, we observe that LMs are phenomenal hypothesis proposers (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the proposed set of rules, this hybrid approach achieves strong results across inductive reasoning benchmarks that require inducing causal relations, language-like instructions, and symbolic concepts. However, they also behave as puzzling inductive reasoners, showing notable performance gaps between rule induction (i.e., identifying plausible rules) and rule application (i.e., applying proposed rules to instances), suggesting that LMs are proposing hypotheses without being able to actually apply the rules. Through empirical and human analyses, we further reveal several discrepancies between the inductive reasoning processes of LMs and humans, shedding light on both the potentials and limitations of using LMs in inductive reasoning tasks.",
            "year": 2023,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work conducts a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting."
            },
            "score": 4
        },
        {
            "id": "ea6b0b5904d6e8eccbccb609ac35911ae967cd2c",
            "paperId": "ea6b0b5904d6e8eccbccb609ac35911ae967cd2c",
            "title": "Creating Suspenseful Stories: Iterative Planning with Large Language Models",
            "abstract": "Automated story generation has been one of the long-standing challenges in NLP. Among all dimensions of stories, *suspense* is very common in human-written stories but relatively under-explored in AI-generated stories. While recent advances in large language models (LLMs) have greatly promoted language generation in general, state-of-the-art LLMs are still unreliable when it comes to suspenseful story generation. We propose a novel iterative-prompting-based planning method that is grounded in two theoretical foundations of story suspense from cognitive psychology and narratology. This theory-grounded method works in a fully zero-shot manner and does not rely on any supervised story corpora. To the best of our knowledge, this paper is the first attempt at suspenseful story generation with LLMs. Extensive human evaluations of the generated suspenseful stories demonstrate the effectiveness of our method.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel iterative-prompting-based planning method that is grounded in two theoretical foundations of story suspense from cognitive psychology and narratology and works in a fully zero-shot manner and does not rely on any supervised story corpora."
            },
            "score": 4
        },
        {
            "id": "50bdea5132ef4b8cf25b0d9f3ac2ee0d09bf18cb",
            "paperId": "50bdea5132ef4b8cf25b0d9f3ac2ee0d09bf18cb",
            "title": "Iterative Zero-Shot LLM Prompting for Knowledge Graph Construction",
            "abstract": "In the current digitalization era, capturing and effectively representing knowledge is crucial in most real-world scenarios. In this context, knowledge graphs represent a potent tool for retrieving and organizing a vast amount of information in a properly interconnected and interpretable structure. However, their generation is still challenging and often requires considerable human effort and domain expertise, hampering the scalability and flexibility across different application fields. This paper proposes an innovative knowledge graph generation approach that leverages the potential of the latest generative large language models, such as GPT-3.5, that can address all the main critical issues in knowledge graph building. The approach is conveyed in a pipeline that comprises novel iterative zero-shot and external knowledge-agnostic strategies in the main stages of the generation process. Our unique manifold approach may encompass significant benefits to the scientific community. In particular, the main contribution can be summarized by: (i) an innovative strategy for iteratively prompting large language models to extract relevant components of the final graph; (ii) a zero-shot strategy for each prompt, meaning that there is no need for providing examples for\"guiding\"the prompt result; (iii) a scalable solution, as the adoption of LLMs avoids the need for any external resources or human expertise. To assess the effectiveness of our proposed model, we performed experiments on a dataset that covered a specific domain. We claim that our proposal is a suitable solution for scalable and versatile knowledge graph construction and may be applied to different and novel contexts.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An innovative knowledge graph generation approach that leverages the potential of the latest generative large language models, such as GPT-3.5, that can address all the main critical issues in knowledge graph building."
            },
            "score": 4
        },
        {
            "id": "a5960c6674f26118e1e81b95d5c2482dce159bfb",
            "paperId": "a5960c6674f26118e1e81b95d5c2482dce159bfb",
            "title": "PIEClass: Weakly-Supervised Text Classification with Prompting and Noise-Robust Iterative Ensemble Training",
            "abstract": "Weakly-supervised text classification trains a classifier using the label name of each target class as the only supervision, which largely reduces human annotation efforts. Most existing methods first use the label names as static keyword-based features to generate pseudo labels, which are then used for final classifier training. While reasonable, such a commonly adopted framework suffers from two limitations: (1) keywords can have different meanings in different contexts and some text may not have any keyword, so keyword matching can induce noisy and inadequate pseudo labels; (2) the errors made in the pseudo label generation stage will directly propagate to the classifier training stage without a chance of being corrected. In this paper, we propose a new method, PIEClass, consisting of two modules: (1) a pseudo label acquisition module that uses zero-shot prompting of pre-trained language models (PLM) to get pseudo labels based on contextualized text understanding beyond static keyword matching, and (2) a noise-robust iterative ensemble training module that iteratively trains classifiers and updates pseudo labels by utilizing two PLM fine-tuning methods that regularize each other. Extensive experiments show that PIEClass achieves overall better performance than existing strong baselines on seven benchmark datasets and even achieves similar performance to fully-supervised classifiers on sentiment classification tasks.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new method, PIEClass, consisting of a pseudo label acquisition module that uses zero-shot prompting of pre-trained language models (PLM) to get pseudo labels based on contextualized text understanding beyond static keyword matching, and a noise-robust iterative ensemble training module that iteratively trains classifiers and updates pseudo labels by utilizing two PLM fine-tuning methods that regularize each other."
            },
            "score": 4
        },
        {
            "id": "507465f8d46489a68a527cb5304d76bdb6c31ed9",
            "paperId": "507465f8d46489a68a527cb5304d76bdb6c31ed9",
            "title": "Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation",
            "abstract": "We introduce a method to measure uncertainty in large language models. For tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models. We show that measuring uncertainty in natural language is challenging because of\"semantic equivalence\"-- different sentences can mean the same thing. To overcome these challenges we introduce semantic entropy -- an entropy which incorporates linguistic invariances created by shared meanings. Our method is unsupervised, uses only a single model, and requires no modifications to off-the-shelf language models. In comprehensive ablation studies we show that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines.",
            "year": 2023,
            "citationCount": 85,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "In comprehensive ablation studies, it is shown that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines."
            },
            "score": 4
        },
        {
            "id": "4e15901eaaaa9a9c2c30f64e05054ce6f5cdaa97",
            "paperId": "4e15901eaaaa9a9c2c30f64e05054ce6f5cdaa97",
            "title": "On the Importance of Uncertainty in Decision-Making with Large Language Models",
            "abstract": "We investigate the role of uncertainty in decision-making problems with natural language as input. For such tasks, using Large Language Models as agents has become the norm. However, none of the recent approaches employ any additional phase for estimating the uncertainty the agent has about the world during the decision-making task. We focus on a fundamental decision-making framework with natural language as input, which is the one of contextual bandits, where the context information consists of text. As a representative of the approaches with no uncertainty estimation, we consider an LLM bandit with a greedy policy, which picks the action corresponding to the largest predicted reward. We compare this baseline to LLM bandits that make active use of uncertainty estimation by integrating the uncertainty in a Thompson Sampling policy. We employ different techniques for uncertainty estimation, such as Laplace Approximation, Dropout, and Epinets. We empirically show on real-world data that the greedy policy performs worse than the Thompson Sampling policies. These findings suggest that, while overlooked in the LLM literature, uncertainty plays a fundamental role in bandit tasks with LLMs.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work considers an LLM bandit with a greedy policy, which picks the action corresponding to the largest predicted reward, compared to LLM bandits that make active use of uncertainty estimation by integrating the uncertainty in a Thompson Sampling policy."
            },
            "score": 4
        },
        {
            "id": "c76541024ed59403f99a5a73ba69849112959a6e",
            "paperId": "c76541024ed59403f99a5a73ba69849112959a6e",
            "title": "A Comprehensive Study of Multilingual Confidence Estimation on Large Language Models",
            "abstract": "The tendency of Large Language Models to generate hallucinations and exhibit overconfidence in predictions raises concerns regarding their reliability. Confidence or uncertainty estimations indicating the extent of trustworthiness of a model's response are essential to developing reliable AI systems. Current research primarily focuses on LLM confidence estimations in English, remaining a void for other widely used languages and impeding the global development of reliable AI applications. This paper introduces a comprehensive investigation of Multi-lingual confidence estimation (MlingConf) on LLMs. First, we introduce an elaborated and expert-checked multilingual QA dataset. Second, we delve into the performance of confidence estimations and examine how these confidence scores can enhance LLM performance through self-refinement across diverse languages. Finally, we propose a cross-lingual confidence estimation method to achieve more precise confidence scores. The experimental results showcase the performance of various confidence estimation methods across different languages as well as present that our proposed cross-lingual confidence estimation technique significantly enhances confidence estimation and outperforms several baseline methods.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A comprehensive investigation of Multi-lingual confidence estimation (MlingConf) on LLMs is introduced, an elaborated and expert-checked multilingual QA dataset is introduced, and a cross-lingual confidence estimation method is proposed to achieve more precise confidence scores."
            },
            "score": 4
        },
        {
            "id": "8ae920111435a7db8da360c654c771c53f57c69a",
            "paperId": "8ae920111435a7db8da360c654c771c53f57c69a",
            "title": "Uncertainty Estimation of Transformer Predictions for Misclassification Detection",
            "abstract": "Uncertainty estimation (UE) of model predictions is a crucial step for a variety of tasks such as active learning, misclassification detection, adversarial attack detection, out-of-distribution detection, etc. Most of the works on modeling the uncertainty of deep neural networks evaluate these methods on image classification tasks. Little attention has been paid to UE in natural language processing. To fill this gap, we perform a vast empirical investigation of state-of-the-art UE methods for Transformer models on misclassification detection in named entity recognition and text classification tasks and propose two computationally efficient modifications, one of which approaches or even outperforms computationally intensive methods.",
            "year": 2022,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A vast empirical investigation of state-of-the-art UE methods for Transformer models on misclassification detection in named entity recognition and text classification tasks and two computationally efficient modifications are proposed, one of which approaches or even outperforms computationally intensive methods."
            },
            "score": 4
        },
        {
            "id": "3dbbe6909d7b53dd49e059c7f61a3613045a8db0",
            "paperId": "3dbbe6909d7b53dd49e059c7f61a3613045a8db0",
            "title": "Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation",
            "abstract": "Large language models (large LMs) are susceptible to producing text that contains hallucinated content. An important instance of this problem is self-contradiction, where the LM generates two contradictory sentences within the same context. In this work, we present a comprehensive investigation into self-contradiction for various instruction-tuned LMs, covering evaluation, detection, and mitigation. Our primary evaluation task is open-domain text generation, but we also demonstrate the applicability of our approach to shorter question answering. Our analysis reveals the prevalence of self-contradictions, e.g., in 17.7% of all sentences produced by ChatGPT. We then propose a novel prompting-based framework designed to effectively detect and mitigate self-contradictions. Our detector achieves high accuracy, e.g., around 80% F1 score when prompting ChatGPT. The mitigation algorithm iteratively refines the generated text to remove contradictory information while preserving text fluency and informativeness. Importantly, our entire framework is applicable to black-box LMs and does not require retrieval of external knowledge. Rather, our method complements retrieval-based methods, as a large portion of self-contradictions (e.g., 35.2% for ChatGPT) cannot be verified using online text. Our approach is practically effective and has been released as a push-button tool to benefit the public at https://chatprotect.ai/.",
            "year": 2023,
            "citationCount": 44,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents a comprehensive investigation into self-contradiction for various instruction-tuned LMs, covering evaluation, detection, and mitigation, and proposes a novel prompting-based framework designed to effectively detect and mitigate self-contradictions."
            },
            "score": 4
        },
        {
            "id": "a6a2df6b37121a673a44ad3b06a55002d5acf192",
            "paperId": "a6a2df6b37121a673a44ad3b06a55002d5acf192",
            "title": "Evaluation and Mitigation of Agnosia in Multimodal Large Language Models",
            "abstract": "While Multimodal Large Language Models (MLLMs) are widely used for a variety of vision-language tasks, one observation is that they sometimes misinterpret visual inputs or fail to follow textual instructions even in straight-forward cases, leading to irrelevant responses, mistakes, and ungrounded claims. This observation is analogous to a phenomenon in neuropsychology known as Agnosia , an inability to correctly process sensory modalities and recognize things (e.g., objects, colors, relations). In our study, we adapt this similar concept to define \u201cagnosia in MLLMs\u201d, and our goal is to comprehensively evaluate and mitigate such agnosia in MLLMs. Inspired by the diagnosis and treatment process in neuropsychology, we propose a novel framework EMMA ( E valuation and M itigation of M ultimodal A gnosia). In EMMA , we develop an evaluation module that automatically creates fine-grained and diverse visual question answering examples to assess the extent of agnosia in MLLMs comprehensively. We also develop a mitigation module to reduce agnosia in MLLMs through multimodal instruction tuning on fine-grained conversations. To verify the effectiveness of our framework, we evaluate and analyze agnosia in seven state-of-the-art MLLMs using 9K test samples. The results reveal that most of them exhibit agnosia across various aspects and degrees. We further develop a fine-grained instruction set and tune MLLMs to mitigate agnosia, which led to notable improvement in accuracy.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel framework EMMA is proposed that develops an evaluation module that automatically creates fine-grained and diverse visual question answering examples to assess the extent of agnosia in MLLMs comprehensively and develops a mitigation module to reduce agnosia in MLLMs through multimodal instruction tuning on fine-grained conversations."
            },
            "score": 4
        },
        {
            "id": "8cf9b49698fdb1b754df2556576412a7b44929f6",
            "paperId": "8cf9b49698fdb1b754df2556576412a7b44929f6",
            "title": "SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks",
            "abstract": "Despite efforts to align large language models (LLMs) with human values, widely-used LLMs such as GPT, Llama, Claude, and PaLM are susceptible to jailbreaking attacks, wherein an adversary fools a targeted LLM into generating objectionable content. To address this vulnerability, we propose SmoothLLM, the first algorithm designed to mitigate jailbreaking attacks on LLMs. Based on our finding that adversarially-generated prompts are brittle to character-level changes, our defense first randomly perturbs multiple copies of a given input prompt, and then aggregates the corresponding predictions to detect adversarial inputs. SmoothLLM reduces the attack success rate on numerous popular LLMs to below one percentage point, avoids unnecessary conservatism, and admits provable guarantees on attack mitigation. Moreover, our defense uses exponentially fewer queries than existing attacks and is compatible with any LLM. Our code is publicly available at the following link: https://github.com/arobey1/smooth-llm.",
            "year": 2023,
            "citationCount": 59,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes SmoothLLM, the first algorithm designed to mitigate jailbreaking attacks on LLMs, which first randomly perturbs multiple copies of a given input prompt, and then aggregates the corresponding predictions to detect adversarial inputs."
            },
            "score": 4
        },
        {
            "id": "dd7a2613ee15cd10938e7ad40f3aa052e98fda3c",
            "paperId": "dd7a2613ee15cd10938e7ad40f3aa052e98fda3c",
            "title": "GROVE: A Retrieval-augmented Complex Story Generation Framework with A Forest of Evidence",
            "abstract": "Conditional story generation is significant in human-machine interaction, particularly in producing stories with complex plots. While Large language models (LLMs) perform well on multiple NLP tasks, including story generation, it is challenging to generate stories with both complex and creative plots. Existing methods often rely on detailed prompts to guide LLMs to meet target conditions, which inadvertently restrict the creative potential of the generated stories. We argue that leveraging information from exemplary human-written stories facilitates generating more diverse plotlines. Delving deeper into story details helps build complex and credible plots. In this paper, we propose a retrieval-au\\textbf{G}mented sto\\textbf{R}y generation framework with a f\\textbf{O}rest of e\\textbf{V}id\\textbf{E}nce (GROVE) to enhance stories' complexity. We build a retrieval repository for target conditions to produce few-shot examples to prompt LLMs. Additionally, we design an ``asking-why'' prompting scheme that extracts a forest of evidence, providing compensation for the ambiguities that may occur in the generated story. This iterative process uncovers underlying story backgrounds. Finally, we select the most fitting chains of evidence from the evidence forest and integrate them into the generated story, thereby enhancing the narrative's complexity and credibility. Experimental results and numerous examples verify the effectiveness of our method.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is argued that leveraging information from exemplary human-written stories facilitates generating more diverse plotlines, and a retrieval-au-Gmented sto-R-y generation framework with arest of GROVE to enhance stories' complexity and credibility is proposed."
            },
            "score": 4
        },
        {
            "id": "4feb412574eb5d0b187276069fe6024c22629c0e",
            "paperId": "4feb412574eb5d0b187276069fe6024c22629c0e",
            "title": "The Calibration Gap between Model and Human Confidence in Large Language Models",
            "abstract": "For large language models (LLMs) to be trusted by humans they need to be well-calibrated in the sense that they can accurately assess and communicate how likely it is that their predictions are correct. Recent work has focused on the quality of internal LLM confidence assessments, but the question remains of how well LLMs can communicate this internal model confidence to human users. This paper explores the disparity between external human confidence in an LLM's responses and the internal confidence of the model. Through experiments involving multiple-choice questions, we systematically examine human users' ability to discern the reliability of LLM outputs. Our study focuses on two key areas: (1) assessing users' perception of true LLM confidence and (2) investigating the impact of tailored explanations on this perception. The research highlights that default explanations from LLMs often lead to user overestimation of both the model's confidence and its' accuracy. By modifying the explanations to more accurately reflect the LLM's internal confidence, we observe a significant shift in user perception, aligning it more closely with the model's actual confidence levels. This adjustment in explanatory approach demonstrates potential for enhancing user trust and accuracy in assessing LLM outputs. The findings underscore the importance of transparent communication of confidence levels in LLMs, particularly in high-stakes applications where understanding the reliability of AI-generated information is essential.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "By modifying the explanations of large language models to more accurately reflect the LLM's internal confidence, a significant shift in user perception is observed, aligning it more closely with the model's actual confidence levels."
            },
            "score": 3
        },
        {
            "id": "05f6628948f79d0cce8664cc8146fd459d53e9d5",
            "paperId": "05f6628948f79d0cce8664cc8146fd459d53e9d5",
            "title": "On the Calibration of Pre-trained Language Models using Mixup Guided by Area Under the Margin and Saliency",
            "abstract": "A well-calibrated neural model produces confidence (probability outputs) closely approximated by the expected accuracy. While prior studies have shown that mixup training as a data augmentation technique can improve model calibration on image classification tasks, little is known about using mixup for model calibration on natural language understanding (NLU) tasks. In this paper, we explore mixup for model calibration on several NLU tasks and propose a novel mixup strategy for pre-trained language models that improves model calibration further. Our proposed mixup is guided by both the Area Under the Margin (AUM) statistic (Pleiss et al., 2020) and the saliency map of each sample (Simonyan et al., 2013). Moreover, we combine our mixup strategy with model miscalibration correction techniques (i.e., label smoothing and temperature scaling) and provide detailed analyses of their impact on our proposed mixup. We focus on systematically designing experiments on three NLU tasks: natural language inference, paraphrase detection, and commonsense reasoning. Our method achieves the lowest expected calibration error compared to strong baselines on both in-domain and out-of-domain test samples while maintaining competitive accuracy.",
            "year": 2022,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper systematically designs experiments on three NLU tasks and proposes a novel mixup strategy for pre-trained language models that improves model calibration further and achieves the lowest expected calibration error compared to strong baselines on both in-domain and out-of-domain test samples while maintaining competitive accuracy."
            },
            "score": 3
        },
        {
            "id": "a2b89d2196b4cc88797d4907ce7458bb7584f6b6",
            "paperId": "a2b89d2196b4cc88797d4907ce7458bb7584f6b6",
            "title": "On the Calibration of Massively Multilingual Language Models",
            "abstract": "Massively Multilingual Language Models (MMLMs) have recently gained popularity due to their surprising effectiveness in cross-lingual transfer. While there has been much work in evaluating these models for their performance on a variety of tasks and languages, little attention has been paid on how well calibrated these models are with respect to the confidence in their predictions. We first investigate the calibration of MMLMs in the zero-shot setting and observe a clear case of miscalibration in low-resource languages or those which are typologically diverse from English. Next, we empirically show that calibration methods like temperature scaling and label smoothing do reasonably well in improving calibration in the zero-shot scenario. We also find that few-shot examples in the language can further help reduce calibration errors, often substantially. Overall, our work contributes towards building more reliable multilingual models by highlighting the issue of their miscalibration, understanding what language and model-specific factors influence it, and pointing out the strategies to improve the same.",
            "year": 2022,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates the calibration of MMLMs in the zero-shot setting and observes a clear case of miscalibration in low-resource languages or those which are typologically diverse from English, and empirically shows that calibration methods like temperature scaling and label smoothing do reasonably well in improving calibration in thezero-shot scenario."
            },
            "score": 3
        },
        {
            "id": "208d9e72a80c9333c36f8ede204128e3c808af84",
            "paperId": "208d9e72a80c9333c36f8ede204128e3c808af84",
            "title": "C3: Confidence Calibration Model Cascade for Inference-Efficient Cross-Lingual Natural Language Understanding",
            "abstract": "Cross-lingual natural language understanding (NLU) is a critical task in natural language processing (NLP). Recent advancements have seen multilingual pre-trained language models (mPLMs) significantly enhance the performance of these tasks. However, mPLMs necessitate substantial resources and incur high computational costs during inference, posing challenges for deployment in real-world and real-time systems. Existing model cascade methods seek to enhance inference efficiency by greedily selecting the lightest model capable of processing the current input from a variety of models, based on model confidence scores. Nonetheless, deep models tend to exhibit overconfidence, and confidence distributions vary across languages. This leads to the emission of confident but incorrect predictions by smaller models, hindering their ability to generalize effectively across test languages. In this study, we introduce a confidence calibration model cascade ($C^3$) method. This approach, simple yet effective, involves calibration prior to cascade inference, thereby enhancing cascade accuracy through more reliable predictions. Extensive experiments conducted on three cross-lingual benchmarks demonstrate that $C^3$ significantly outperforms all state-of-the-art baselines.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This approach involves calibration prior to cascade inference, thereby enhancing cascade accuracy through more reliable predictions, and significantly outperforms all state-of-the-art baselines."
            },
            "score": 3
        },
        {
            "id": "995b2f650f55de6077b87db6dadb01cecd86dbd7",
            "paperId": "995b2f650f55de6077b87db6dadb01cecd86dbd7",
            "title": "Advanced prompting as a catalyst: Empowering large language models in the management of gastrointestinal cancers",
            "abstract": "Large Language Models' (LLMs) performance in healthcare can be significantly impacted by prompt engineering. However, the area of study remains relatively uncharted in gastrointestinal oncology until now. Our research delves into this unexplored territory, investigating the efficacy of varied prompting strategies, including simple prompts, templated prompts, in-context learning (ICL), and multi-round iterative questioning, for optimizing the performance of LLMs within a medical setting. We develop a comprehensive evaluation system to assess the performance of LLMs across multiple dimensions. This robust evaluation system ensures a thorough assessment of the LLMs' capabilities in the field of medicine. Our findings suggest a positive relationship between the comprehensiveness of the prompts and the LLMs' performance. Notably, the multi-round strategy, which is characterized by iterative question-and-answer rounds, consistently yields the best results. ICL, a strategy that capitalizes on interrelated contextual learning, also displays significant promise, surpassing the outcomes achieved with simpler prompts. The research underscores the potential of advanced prompt engineering and iterative learning approaches for boosting the applicability of LLMs in healthcare. We recommend that additional research be conducted to refine these strategies and investigate their potential integration, to truly harness the full potential of LLMs in medical applications.\n",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The research investigates the efficacy of varied prompting strategies, including simple prompts, templated prompts, in-context learning (ICL), and multi-round iterative questioning, for optimizing the performance of LLMs within a medical setting, and develops a comprehensive evaluation system."
            },
            "score": 3
        },
        {
            "id": "fd80f7f3673fc6ca02f192d5d73426f11a4be659",
            "paperId": "fd80f7f3673fc6ca02f192d5d73426f11a4be659",
            "title": "The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation",
            "abstract": "Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iterative development of MT systems. While considerable progress has been made on estimating a single scalar quality score, current metrics lack the informativeness of more detailed schemes that annotate individual errors, such as Multidimensional Quality Metrics (MQM). In this paper, we help fill this gap by proposing AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AutoMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particularly large gains for larger models) while providing interpretability through error spans that align with human annotations.",
            "year": 2023,
            "citationCount": 24,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations, and finds that it improves performance compared to just prompting for scores."
            },
            "score": 3
        },
        {
            "id": "ee414ff78922ac70dfb31abfff37bd40c661ac92",
            "paperId": "ee414ff78922ac70dfb31abfff37bd40c661ac92",
            "title": "Decomposed Prompting: Unveiling Multilingual Linguistic Structure Knowledge in English-Centric Large Language Models",
            "abstract": "Despite the predominance of English in their training data, English-centric Large Language Models (LLMs) like GPT-3 and LLaMA display a remarkable ability to perform multilingual tasks, raising questions about the depth and nature of their cross-lingual capabilities. This paper introduces the decomposed prompting approach to probe the linguistic structure understanding of these LLMs in sequence labeling tasks. Diverging from the single text-to-text prompt, our method generates for each token of the input sentence an individual prompt which asks for its linguistic label. We assess our method on the Universal Dependencies part-of-speech tagging dataset for 38 languages, utilizing both English-centric and multilingual LLMs. Our findings show that decomposed prompting surpasses the iterative prompting baseline in efficacy and efficiency under zero- and few-shot settings. Further analysis reveals the influence of evaluation methods and the use of instructions in prompts. Our multilingual investigation shows that English-centric language models perform better on average than multilingual models. Our study offers insights into the multilingual transferability of English-centric LLMs, contributing to the understanding of their multilingual linguistic knowledge.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The decomposed prompting approach is introduced to probe the linguistic structure understanding of English-centric Large Language Models in sequence labeling tasks, showing that English-centric language models perform better on average than multilingual models."
            },
            "score": 3
        },
        {
            "id": "437cbaee4eaee0bf84abbe11750b86b091b9b756",
            "paperId": "437cbaee4eaee0bf84abbe11750b86b091b9b756",
            "title": "MacGyver: Are Large Language Models Creative Problem Solvers?",
            "abstract": "We explore the creative problem-solving capabilities of modern LLMs in a novel constrained setting. To this end, we create MACGYVER, an automatically generated dataset consisting of over 1,600 real-world problems deliberately designed to trigger innovative usage of objects and necessitate out-of-the-box thinking. We then present our collection to both LLMs and humans to compare and contrast their problem-solving abilities. MACGYVER is challenging for both groups, but in unique and complementary ways. For instance, humans excel in tasks they are familiar with but struggle with domain-specific knowledge, leading to a higher variance. In contrast, LLMs, exposed to a variety of specialized knowledge, attempt broader problems but fail by proposing physically-infeasible actions. Finally, we provide a detailed error analysis of LLMs, and demonstrate the potential of enhancing their problem-solving ability with novel prompting techniques such as iterative step-wise reflection and divergent-convergent thinking. This work (1) introduces a fresh arena for intelligent agents focusing on intricate aspects of physical reasoning, planning, and unconventional thinking, which supplements the existing spectrum of machine intelligence; and (2) provides insight into the constrained problem-solving capabilities of both humans and AI.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "MACGYVER is an automatically generated dataset consisting of over 1,600 real-world problems deliberately designed to trigger innovative usage of objects and necessitate out-of-the-box thinking, and provides insight into the constrained problem-solving capabilities of both humans and AI."
            },
            "score": 3
        },
        {
            "id": "acbe813244e07f32eb034d6c27547d772a995d1d",
            "paperId": "acbe813244e07f32eb034d6c27547d772a995d1d",
            "title": "Uncertainty Estimation for Language Reward Models",
            "abstract": "Language models can learn a range of capabilities from unsupervised training on text corpora. However, to solve a particular problem (such as text summarization) it is typically necessary to fine-tune them on a task-specific dataset. It is often easier for humans to choose between options than to provide labeled data, and prior work has achieved state-of-the-art performance by training a reward model from such preference comparisons. However, collecting a large preference comparison dataset is still expensive -- and the learned reward models are unreliable out-of-distribution. We seek to address these problems via uncertainty estimation, which can improve sample efficiency and robustness using active learning and risk-averse reinforcement learning (RL). Specifically, we use bootstrap aggregating (bagging) to train an ensemble of reward models differing in the initialization of their final layer. Ensembles have proved successful in prior applications of active learning, but we find that in our setting ensemble active learning does not outperform random sampling. Further experiments show that while the aggregate predictions are well-calibrated, the ensemble's estimated epistemic uncertainty is only weakly correlated with model error. We suspect this is because the ensemble members are fine-tuned from a single model and so are similar to one another. This suggests current pre-training methods will need to be modified to support uncertainty estimation, e.g. by training multiple language models.",
            "year": 2022,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that in this setting ensemble active learning does not outperform random sampling, and current pre-training methods will need to be modified to support uncertainty estimation, e.g. by training multiple language models."
            },
            "score": 3
        },
        {
            "id": "bf4700077294c369f64eda65f677dd4f61b43072",
            "paperId": "bf4700077294c369f64eda65f677dd4f61b43072",
            "title": "Uncertainty Estimation and Reduction of Pre-trained Models for Text Regression",
            "abstract": "Abstract State-of-the-art classification and regression models are often not well calibrated, and cannot reliably provide uncertainty estimates, limiting their utility in safety-critical applications such as clinical decision-making. While recent work has focused on calibration of classifiers, there is almost no work in NLP on calibration in a regression setting. In this paper, we quantify the calibration of pre- trained language models for text regression, both intrinsically and extrinsically. We further apply uncertainty estimates to augment training data in low-resource domains. Our experiments on three regression tasks in both self-training and active-learning settings show that uncertainty estimation can be used to increase overall performance and enhance model generalization.",
            "year": 2022,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper quantifies the calibration of pre- trained language models for text regression, both intrinsically and extrinsically, and applies uncertainty estimates to augment training data in low-resource domains."
            },
            "score": 3
        },
        {
            "id": "645d8c40f2a05f0b06f9338cf7635755532d747c",
            "paperId": "645d8c40f2a05f0b06f9338cf7635755532d747c",
            "title": "Uncertainty Awareness of Large Language Models Under Code Distribution Shifts: A Benchmark Study",
            "abstract": "Large Language Models (LLMs) have been widely employed in programming language analysis to enhance human productivity. Yet, their reliability can be compromised by various code distribution shifts, leading to inconsistent outputs. While probabilistic methods are known to mitigate such impact through uncertainty calibration and estimation, their efficacy in the language domain remains underexplored compared to their application in image-based tasks. In this work, we first introduce a large-scale benchmark dataset, incorporating three realistic patterns of code distribution shifts at varying intensities. Then we thoroughly investigate state-of-the-art probabilistic methods applied to CodeLlama using these shifted code snippets. We observe that these methods generally improve the uncertainty awareness of CodeLlama, with increased calibration quality and higher uncertainty estimation~(UE) precision. However, our study further reveals varied performance dynamics across different criteria (e.g., calibration error vs misclassification detection) and trade-off between efficacy and efficiency, highlighting necessary methodological selection tailored to specific contexts.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work thoroughly investigate state-of-the-art probabilistic methods applied to CodeLlama using three realistic patterns of code distribution shifts at varying intensities, and observes that these methods generally improve the uncertainty awareness of CodeLlama, with increased calibration quality and higher uncertainty estimation~(UE) precision."
            },
            "score": 3
        },
        {
            "id": "a860ba337cead5e2e970460522d6612a49836ff1",
            "paperId": "a860ba337cead5e2e970460522d6612a49836ff1",
            "title": "Uncertainty Estimation of Transformers' Predictions via Topological Analysis of the Attention Matrices",
            "abstract": "Determining the degree of confidence of deep learning model in its prediction is an open problem in the field of natural language processing. Most of the classical methods for uncertainty estimation are quite weak for text classification models. We set the task of obtaining an uncertainty estimate for neural networks based on the Transformer architecture. A key feature of such mo-dels is the attention mechanism, which supports the information flow between the hidden representations of tokens in the neural network. We explore the formed relationships between internal representations using Topological Data Analysis methods and utilize them to predict model's confidence. In this paper, we propose a method for uncertainty estimation based on the topological properties of the attention mechanism and compare it with classical methods. As a result, the proposed algorithm surpasses the existing methods in quality and opens up a new area of application of the attention mechanism, but requires the selection of topological features.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a method for uncertainty estimation based on the topological properties of the attention mechanism and compares it with classical methods, which surpasses the existing methods in quality and opens up a new area of application of the Attention mechanism, but requires the selection of topological features."
            },
            "score": 3
        },
        {
            "id": "7f6d48d7b1641d3d2fd4ee06c434a73af8fce07b",
            "paperId": "7f6d48d7b1641d3d2fd4ee06c434a73af8fce07b",
            "title": "Density-Softmax: Scalable and Calibrated Uncertainty Estimation under Distribution Shifts",
            "abstract": "Prevalent deterministic deep-learning models suffer from significant over-confidence under distribution shifts. Probabilistic approaches can reduce this problem but struggle with computational efficiency. In this paper, we propose Density-Softmax, a fast and lightweight deterministic method to improve calibrated uncertainty estimation via a combination of density function with the softmax layer. By using the latent representation's likelihood value, our approach produces more uncertain predictions when test samples are distant from the training samples. Theoretically, we show that Density-Softmax can produce high-quality uncertainty estimation with neural networks, as it is the solution of minimax uncertainty risk and is distance-aware, thus reducing the over-confidence of the standard softmax. Empirically, our method enjoys similar computational efficiency as a single forward pass deterministic with standard softmax on the shifted toy, vision, and language datasets across modern deep-learning architectures. Notably, Density-Softmax uses 4 times fewer parameters than Deep Ensembles and 6 times lower latency than Rank-1 Bayesian Neural Network, while obtaining competitive predictive performance and lower calibration errors under distribution shifts.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Density-Softmax is proposed, a fast and lightweight deterministic method to improve calibrated uncertainty estimation via a combination of density function with the softmax layer, which enjoys similar computational efficiency as a single forward pass deterministic with standard softmax on the shifted toy, vision, and language datasets across modern deep-learning architectures."
            },
            "score": 3
        },
        {
            "id": "fba0b0817dbc8200b41a1de22654b54b778a11e9",
            "paperId": "fba0b0817dbc8200b41a1de22654b54b778a11e9",
            "title": "Recommending Root-Cause and Mitigation Steps for Cloud Incidents using Large Language Models",
            "abstract": "Incident management for cloud services is a complex process involving several steps and has a huge impact on both service health and developer productivity. On-call engineers require significant amount of domain knowledge and manual effort for root causing and mitigation of production incidents. Recent advances in artificial intelligence has resulted in state-of-the-art large language models like GPT-3.x (both GPT-3.0 and GPT-3.5), which have been used to solve a variety of problems ranging from question answering to text summarization. In this work, we do the first large-scale study to evaluate the effectiveness of these models for helping engineers root cause and mitigate production incidents. We do a rigorous study at Microsoft, on more than 40,000 incidents and compare several large language models in zero-shot, fine-tuned and multi-task setting using semantic and lexical metrics. Lastly, our human evaluation with actual incident owners show the efficacy and future potential of using artificial intelligence for resolving cloud incidents.",
            "year": 2023,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work does the first large-scale study to evaluate the effectiveness of GPT-3.x models for helping engineers root cause and mitigate production incidents and compares several large language models in zero-shot, fine-tuned and multi-task setting using semantic and lexical metrics."
            },
            "score": 3
        },
        {
            "id": "faffb6320ff1f25cd472ba0afe7cdba5dde79a5f",
            "paperId": "faffb6320ff1f25cd472ba0afe7cdba5dde79a5f",
            "title": "Test-time Backdoor Mitigation for Black-Box Large Language Models with Defensive Demonstrations",
            "abstract": "Existing studies in backdoor defense have predominantly focused on the training phase, overlooking the critical aspect of testing time defense. This gap becomes particularly pronounced in the context of Large Language Models (LLMs) deployed as Web Services, which typically offer only black-box access, rendering training-time defenses impractical. To bridge this gap, our work introduces defensive demonstrations, an innovative backdoor defense strategy for blackbox large language models. Our method involves identifying the task and retrieving task-relevant demonstrations from an uncontaminated pool. These demonstrations are then combined with user queries and presented to the model during testing, without requiring any modifications/tuning to the black-box model or insights into its internal mechanisms. Defensive demonstrations are designed to counteract the adverse effects of triggers, aiming to recalibrate and correct the behavior of poisoned models during test-time evaluations. Extensive experiments show that defensive demonstrations are effective in defending both instance-level and instruction-level backdoor attacks, not only rectifying the behavior of poisoned models but also surpassing existing baselines in most scenarios.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Defensive demonstrations are designed to counteract the adverse effects of triggers, aiming to recalibrate and correct the behavior of poisoned models during test-time evaluations, and are effective in defending both instance-level and instruction-level backdoor attacks."
            },
            "score": 3
        },
        {
            "id": "c9ad9d69d7568110dd5527598a92c7f8b335eef4",
            "paperId": "c9ad9d69d7568110dd5527598a92c7f8b335eef4",
            "title": "Generative Language Models and Automated Influence Operations: Emerging Threats and Potential Mitigations",
            "abstract": "Generative language models have improved drastically, and can now produce realistic text outputs that are difficult to distinguish from human-written content. For malicious actors, these language models bring the promise of automating the creation of convincing and misleading text for use in influence operations. This report assesses how language models might change influence operations in the future, and what steps can be taken to mitigate this threat. We lay out possible changes to the actors, behaviors, and content of online influence operations, and provide a framework for stages of the language model-to-influence operations pipeline that mitigations could target (model construction, model access, content dissemination, and belief formation). While no reasonable mitigation can be expected to fully prevent the threat of AI-enabled influence operations, a combination of multiple mitigations may make an important difference.",
            "year": 2023,
            "citationCount": 128,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This report lays out possible changes to the actors, behaviors, and content of online influence operations, and provides a framework for stages of the language model-to-influence operations pipeline that mitigations could target."
            },
            "score": 3
        },
        {
            "id": "f74ab4f3ed355fbfbb47368ef6f58548f0afa3aa",
            "paperId": "f74ab4f3ed355fbfbb47368ef6f58548f0afa3aa",
            "title": "Virtual Teaching and Painting Platform for the Colour Blind",
            "abstract": "Education involves perception of colour and unfortunately some of us are not blessed with this gift of proper vision. Lacking the ability to distinguish certain colours is commonly known as Colour Blindness. In schools and colleges, colour blind students are not supported much in the classroom, some lose confidence and struggle to cope. Often colour blind teachers are unable to distinguish important colours of pens or markers and for an efficient and interactive learning environment it is seen that the results are tilted in favour of classrooms where multiple colours are used on the board. The numbers corresponding to people having this disease(or its variations) are compelling enough to make it highly imperative for us to seek a solution for it. Hence, we propose a software which can be used as a virtual teaching and virtual painting platform by these differently abled people. Built on the Graphical User Interface Development Environment(GUIDE) in the numerical computing environment MATLAB, this system helps the user to identify the colour of the object (marker) withwhich he can teach virtually by moving it in air.These movementsget traced on the screen which serves as a virtual white board. I. Proposed System The software we propose can be used as a dual purpose tool. 1. It can be used as a virtual painting tool for the color blind children 2. It can be used as a virtual teaching tool for the color blind teachers. Both purposes are served by a set of functionalities which use the basic web camera of a computer system only as a resource. \uf0b7 The system using the web camera records the video of the user who holds an object (usually a marker) of a particular color( one of Red, Green, Blue, Cyan, Magenta and Yellow) as a tool used for drawing or writing on the Virtual White Board present on the interface. \uf0b7 This video is displayed separately on the left side of the interface for the user\u2019s convenience in case he/she wants to track his/her hand movements. The user holds this object in front of the camera for some time for the system to detect the color of the object on a real time basis. \uf0b7 As soon as the color is detected, the user is prompted about its information on the virtual white board mentioned above. After this, the paint tool gets activated which traces the movement of the object in the user\u2019s hands and paints the area (pixels) on the white board according to it. \uf0b7 The tracing of movement is visible to the user and he/she is able to see a rectangular bounding box, surrounding the colored portion of the object, which moves along with the object simultaneously depending on the user\u2019s hand movement. \uf0b7 The painting occurs with the color detected by the system in the initial phase. In case the user wants to add a space or a pause in the process of writing or drawing then he/she can click on aPause button present at the bottom of the interface which halts the paint tool accordingly. \uf0b7 The user can also clear the virtual white board using a Clear button. \uf0b7 While writing or drawing if the user feels the need to change the color which he wants to use then he/she can do that using a Change Color button present on the interface. Once this button is clicked, the system re runs the detection tool to detect the color of the next object to be used by the user after which the user can continue his work. \uf0b7 The user can also save his work using a Save button present on the interface which saves the image of the virtual white board in .jpeg format. \uf0b7 The system also allows the user to paint or write on an image or a set of images which can be used as the same virtual board instead of the plain white one given before. This can be done by clicking on a button named Open present on the same interface. It also allows us to open a set of multiple images . Virtual Teaching and Painting Platform for the Colour Blind www.iosrjournals.org 2 | Page \uf0b7 Open functionality provides an experience equivalent to that of a power-point presentation on which the user can mark or write anything virtually. A set of buttons Next and Previous allow the user to switch between the set of images selected by the user before. \uf0b7 After the user has finished working with the system he/she can close/terminate it using a button named Close provided for the same on the interface. The juxtaposition of all the aforementioned functionalities in our proposed would serve as a handy and uniqueteaching or drawing tool which can prove to be highly effective in addressing the needs of the color blind children as well as the teachers. II. Why Matlab? MATLAB (matrix laboratory) is a multi-paradigm numerical computing environment and fourthgeneration programming language. Developed by MathWorks, MATLAB allows matrix manipulations, plotting of functions and data, implementation of algorithms, creation of user interfaces, and interfacing with programs written in other languages, including C, C++, Java, and Fortran. Although MATLAB is intended primarily for numerical computing, an optional toolbox uses the MuPAD symbolic engine, allowing access to symbolic computing capabilities. An additional package, Simulink, adds graphical multi-domain simulation and Model-Based Design for dynamic and embedded systems. MATLAB \u00ae is a high-level language and interactive environment for numerical computation, visualization, and programming. Using MATLAB, you can analyze data, develop algorithms, and create models and applications. The language, tools, and built-in math functions enable you to explore multiple approaches and reach a solution faster than with spreadsheets or traditional programming languages, such as C/C++ or Java TM . Key Features: \uf0b7 High-level language for numerical computation, visualization, and application development \uf0b7 Interactive environment for iterative exploration, design, and problem solving \uf0b7 Mathematical functions for linear algebra, statistics, Fourier analysis, filtering, optimization, numerical integration, and solving ordinary differential equations \uf0b7 Built-in graphics for visualizing data and tools for creating custom plots \uf0b7 Development tools for improving code quality and maintainability and maximizing performance \uf0b7 Tools for building applications with custom graphical interfaces \uf0b7 Functions for integrating MATLAB based algorithms with external applications and languages such as C, Java, .NET, and Microsoft \u00ae Excel \u00ae III. Advantages Of Matlab Over Open Cv \uf0b7 Ease of use: Matlab is a relatively easy language to get to grips with. Matlab is a pretty high-level scripting language, meaning that you don\u2019t have to worry about libraries, declaring variables, memory management or other lower-level programming issues. As such, it can be very easy to throw together some code to prototype your image processing idea. Say for example I want to read in an image from file and display it. In Matlab, you could write this \uf0b7 Easy, right? Now, if you wanted to do the same using OpenCV, it would look like: Virtual Teaching and Painting Platform for the Colour Blind www.iosrjournals.org 3 | Page \uf0b7 Memory Management: OpenCV is based on C. As such, every time you allocate a chunk of memory you will have to release it again. If you have a loop in your code where you allocate a chunk of memory in that loop and forget release it afterwards, you will get what is called a \u201cleak\u201d. This is where the program will use a growing amount of memory until it crashes from no remaining memory. Due to the high-level nature of Matlab, it is \u201csmart\u201d enough to automatically allocate and release memory in the background. \uf0b7 Development Environment: Matlab comes with its own development environment. For OpenCV, there is no particular IDE that you have to use. Instead, you have a choice of any C programming IDE depending on whether you are using Windows, Linux, or OS X. For Windows, Microsoft Visual Studio or NetBeans is the typical IDE used for OpenCV. In Linux, its Eclipse or NetBeans, and in OSX, we use Apple\u2019s Xcode. IV. Algorithms Change Color 1. Click on the Change Color button. 2. Flip the video. 3. Convert the video into CMY format and finally store the grayscale forms of both formats in Gray and Cgray. 4. Find the individual color forms R,G,B,C,M,Y of the image by subtracting the grayscale forms of the image(s) from the individual color components of the same. 5. Assume a rectangular bounding box of a particular area. 6. Calculate the R,G,B,C,M,Y count for each pixel within the bounding box. 7. Calculate the sum of R,G and B counts and C,M and Y counts and store them separately. 8. Calculate the probability of each color by dividing individual pixel count of each color with the sum of RGB count(for red, green and blue only) or CMY count(for cyan, magenta and yellow only). 9. Repeat the steps 2 down to 8 for a designated no. of frames. 10. Find the color with the maximum probability and pass its id to the paint function. Virtual Teaching and Painting Platform for the Colour Blind www.iosrjournals.org 4 | Page Colour Detection Algorithm Virtual Teaching and Painting Platform for the Colour Blind www.iosrjournals.org 5 | Page Algorithm For Colour Extraction Paint 1. The id of the color with maximum probability is received. 2. The current color is set to the color with the value id. 3. Flip the video. 4. Display the video in the designated axes. 5. Find the area, centroid and bounding box properties of the black and white form of the extracted color component of the image. 6. Find the region of the image with maximum area and assume a rectangular bounding box around that region. 7. Find the centroid of the bounding box. 8. Set the value of the color components(R,G and B) of the adjoining pixels of the centroid as 255 or 0 according to the color with the value id. 9. Display the current image in the designated axes. 10. Repeat steps 7 and 8 until the status of pause button is true. 11. Repeat steps 5 down to 10 till the video is ON. 12. Flush the video and stop. Virtual Teaching and Painting Platfo",
            "year": 2014,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A software which can be used as a virtual teaching and virtual painting platform by these differently abled people and served by a set of functionalities which use the basic web camera of a computer system only as a resource."
            },
            "score": 3
        },
        {
            "id": "77b4e11cf494be085f506cdc4ab77946b07b6b52",
            "paperId": "77b4e11cf494be085f506cdc4ab77946b07b6b52",
            "title": "Open-Vocabulary Calibration for Vision-Language Models",
            "abstract": "Vision-language models (VLMs) have emerged as formidable tools, showing their strong capability in handling various open-vocabulary tasks in image recognition, text-driven visual content generation, and visual chatbots, to name a few. In recent years, considerable efforts and resources have been devoted to adaptation methods for improving downstream performance of VLMs, particularly on parameter-efficient fine-tuning methods like prompt learning. However, a crucial aspect that has been largely overlooked is the confidence calibration problem in fine-tuned VLMs, which could greatly reduce reliability when deploying such models in the real world. This paper bridges the gap by systematically investigating the confidence calibration problem in the context of prompt learning and reveals that existing calibration methods are insufficient to address the problem, especially in the open-vocabulary setting. To solve the problem, we present a simple and effective approach called Distance-Aware Calibration (DAC), which is based on scaling the temperature using as guidance the distance between predicted text labels and base classes. The experiments with 7 distinct prompt learning methods applied across 11 diverse downstream datasets demonstrate the effectiveness of DAC, which achieves high efficacy without sacrificing the inference speed.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A simple and effective approach called Distance-Aware Calibration (DAC) is presented, based on scaling the temperature using as guidance the distance between predicted text labels and base classes, which achieves high efficacy without sacrificing the inference speed."
            },
            "score": 2
        },
        {
            "id": "05301eb9dc89a4f75ba601c1fddf3d5fb868ab35",
            "paperId": "05301eb9dc89a4f75ba601c1fddf3d5fb868ab35",
            "title": "When Quantization Affects Confidence of Large Language Models?",
            "abstract": "Recent studies introduced effective compression techniques for Large Language Models (LLMs) via post-training quantization or low-bit weight representation. Although quantized weights offer storage efficiency and allow for faster inference, existing works have indicated that quantization might compromise performance and exacerbate biases in LLMs. This study investigates the confidence and calibration of quantized models, considering factors such as language model type and scale as contributors to quantization loss. Firstly, we reveal that quantization with GPTQ to 4-bit results in a decrease in confidence regarding true labels, with varying impacts observed among different language models. Secondly, we observe fluctuations in the impact on confidence across different scales. Finally, we propose an explanation for quantization loss based on confidence levels, indicating that quantization disproportionately affects samples where the full model exhibited low confidence levels in the first place.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is revealed that quantization with GPTQ to 4-bit results in a decrease in confidence regarding true labels, with varying impacts observed among different language models, and an explanation for quantization loss based on confidence levels is proposed."
            },
            "score": 2
        },
        {
            "id": "2392b6d3a5cad9e5cf349169eaeee848266adf6a",
            "paperId": "2392b6d3a5cad9e5cf349169eaeee848266adf6a",
            "title": "LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models",
            "abstract": "Large language models (LLMs) have been applied in various applications due to their astonishing capabilities. With advancements in technologies such as chain-of-thought (CoT) prompting and in-context learning (ICL), the prompts fed to LLMs are becoming increasingly lengthy, even exceeding tens of thousands of tokens. To accelerate model inference and reduce cost, this paper presents LLMLingua, a coarse-to-fine prompt compression method that involves a budget controller to maintain semantic integrity under high compression ratios, a token-level iterative compression algorithm to better model the interdependence between compressed contents, and an instruction tuning based method for distribution alignment between language models. We conduct experiments and analysis over four datasets from different scenarios, i.e., GSM8K, BBH, ShareGPT, and Arxiv-March23; showing that the proposed approach yields state-of-the-art performance and allows for up to 20x compression with little performance loss. Our code is available at https://aka.ms/LLMLingua.",
            "year": 2023,
            "citationCount": 17,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A coarse-to-fine prompt compression method that involves a budget controller to maintain semantic integrity under high compression ratios, a token-level iterative compression algorithm to better model the interdependence between compressed contents, and an instruction tuning based method for distribution alignment between language models."
            },
            "score": 2
        },
        {
            "id": "d479ef0ece2425042c2a80307ea154c85a9b14f9",
            "paperId": "d479ef0ece2425042c2a80307ea154c85a9b14f9",
            "title": "Uncertainty Estimation for Debiased Models: Does Fairness Hurt Reliability?",
            "abstract": "When deploying a machine learning model, one should aim not only to optimize performance metrics such as accuracy but also care about model fairness and reliability. Fairness means that the model is prevented from learning spurious correlations between a target variable and socio-economic attributes, and is generally achieved by applying debiasing techniques. Model reliability stems from the ability to determine whether we can trust model predictions for the given data. This can be achieved using uncertainty estimation (UE) methods. Debi-asing and UE techniques potentially interfere with each other, raising the question of whether we can achieve both reliability and fairness at the same time. This work aims to answer this question empirically based on an extensive series of experiments combining state-of-the-art UE and debiasing methods, and examining the impact on model performance, fairness, and reliability. 1",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work aims to answer the question whether a machine learning model can achieve both reliability and fairness at the same time empirically based on an extensive series of experiments combining state-of-the-art UE and debiasing methods, and examining the impact on model performance, fairness, and reliability."
            },
            "score": 2
        },
        {
            "id": "5e7274bcda47b704b6797bb14be8b7a61c047a61",
            "paperId": "5e7274bcda47b704b6797bb14be8b7a61c047a61",
            "title": "Uncertainty-Aware Evaluation for Vision-Language Models",
            "abstract": "Vision-Language Models like GPT-4, LLaVA, and CogVLM have surged in popularity recently due to their impressive performance in several vision-language tasks. Current evaluation methods, however, overlook an essential component: uncertainty, which is crucial for a comprehensive assessment of VLMs. Addressing this oversight, we present a benchmark incorporating uncertainty quantification into evaluating VLMs. Our analysis spans 20+ VLMs, focusing on the multiple-choice Visual Question Answering (VQA) task. We examine models on 5 datasets that evaluate various vision-language capabilities. Using conformal prediction as an uncertainty estimation approach, we demonstrate that the models' uncertainty is not aligned with their accuracy. Specifically, we show that models with the highest accuracy may also have the highest uncertainty, which confirms the importance of measuring it for VLMs. Our empirical findings also reveal a correlation between model uncertainty and its language model part.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that models with the highest accuracy may also have the highest uncertainty, which confirms the importance of measuring it for VLMs, and a correlation between model uncertainty and its language model part is revealed."
            },
            "score": 2
        },
        {
            "id": "f2d0f9309a4ca6e9d712f72778a9bcf083ace077",
            "paperId": "f2d0f9309a4ca6e9d712f72778a9bcf083ace077",
            "title": "Uncertainty estimation in deep learning with application to spoken language assessment",
            "abstract": "Since convolutional neural networks (CNNs) achieved top performance on the ImageNet task in 2012, deep learning has become the preferred approach to addressing computer vision, natural language processing, speech recognition and bio-informatics tasks. However, despite impressive performance, neural networks tend to make over-confident predictions. Thus, it is necessary to investigate robust, interpretable and tractable estimates of uncertainty in a model\u2019s predictions in order to construct safer Machine Learning systems. This is crucial to applications where the cost of an error is high, such as in autonomous vehicle control, high-stakes automatic proficiency assessment and in the medical, financial and legal fields. In the first part of this thesis uncertainty estimation via ensemble and single-model approaches is discussed in detail and a new class of models for uncertainty estimation, called Prior Networks, is proposed. Prior Networks are able to emulate an ensemble of models using a single deterministic neural network, which allows sources of uncertainty to be determined within the same probabilistic framework as in ensemble-based approaches, but with the computational simplicity and ease of training of single-model approaches. Thus, Prior Networks combine the advantages of ensemble and single-model approaches to estimating uncertainty. In this thesis Prior Networks are evaluated on a range classification datasets, where they are shown to outperform baseline approaches, such as Monte-Carlo dropout, on the task of detecting out-of-distribution inputs. In the second part of this thesis deep learning and uncertainty estimation approaches are applied to the area of automatic assessment of non-native spoken language proficiency. Specifically deep-learning based graders and spoken response relevance assessment systems are constructed using data from the BULATS and LinguaSkill exams, provided by Cambridge English Language Assessment. Baseline approaches for uncertainty estimation discussed and evaluated in the first half of the thesis are then applied to these models and assessed on the task of rejecting predictions to be graded by human examiners and detecting misclassifications.",
            "year": 2019,
            "citationCount": 63,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Prior Networks combine the advantages of ensemble and single-model approaches to estimating uncertainty and are evaluated on a range classification datasets, where they are shown to outperform baseline approaches on the task of detecting out-of-distribution inputs."
            },
            "score": 2
        },
        {
            "id": "92e8eb55794b208952cf190f56e9d4663ad049cc",
            "paperId": "92e8eb55794b208952cf190f56e9d4663ad049cc",
            "title": "Natural language processing systems for pathology parsing in limited data environments with uncertainty estimation",
            "abstract": "Abstract Objective Cancer is a leading cause of death, but much of the diagnostic information is stored as unstructured data in pathology reports. We aim to improve uncertainty estimates of machine learning-based pathology parsers and evaluate performance in low data settings. Materials and methods Our data comes from the Urologic Outcomes Database at UCSF which includes 3232 annotated prostate cancer pathology reports from 2001 to 2018. We approach 17 separate information extraction tasks, involving a wide range of pathologic features. To handle the diverse range of fields, we required 2 statistical models, a document classification method for pathologic features with a small set of possible values and a token extraction method for pathologic features with a large set of values. For each model, we used isotonic calibration to improve the model\u2019s estimates of its likelihood of being correct. Results Our best document classifier method, a convolutional neural network, achieves a weighted F1 score of 0.97 averaged over 12 fields and our best extraction method achieves an accuracy of 0.93 averaged over 5 fields. The performance saturates as a function of dataset size with as few as 128 data points. Furthermore, while our document classifier methods have reliable uncertainty estimates, our extraction-based methods do not, but after isotonic calibration, expected calibration error drops to below 0.03 for all extraction fields. Conclusions We find that when applying machine learning to pathology parsing, large datasets may not always be needed, and that calibration methods can improve the reliability of uncertainty estimates.",
            "year": 2020,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that when applying machine learning to pathology parsing, large datasets may not always be needed, and that calibration methods can improve the reliability of uncertainty estimates."
            },
            "score": 2
        },
        {
            "id": "2e2c31fd97fc6ce27640bfc56f4b3ceca4f0cb9c",
            "paperId": "2e2c31fd97fc6ce27640bfc56f4b3ceca4f0cb9c",
            "title": "Uncertainty Estimation for Complex Text Detection in Spanish",
            "abstract": "Text simplifcation refers to the transformation of a source text aiming to increase its readiblity and understandability for a specific target population. This task is an important step towards improving inclusivity of such target populations (i.e., low scholarity or visually/hearing impaired groups). The recent advancements in the field brought by Large Language Models improve the performance of machine based text simplification approaches. However, using Language Models to simplify large text segments can be resource demanding. A more simple model to classify whether the text segment is worth to simplify or not can improve resource efficiency, in order to avoid unnecessary text prompts to the Large Language Models. Furthermore, text simplicity categorization can also be used for other purposes, such as text complexity measurement. The discrimination of text segments into simple and complex categories might lead to a number of false positives or negatives for a not well-tuned model. A way to control the acceptance threshold, is the implementation of an uncertainty score for each prediction. In this work we explore two simple uncertainty estimation approaches for complex text identification: a Monte Carlo Dropout and an Deep Ensemble Based approach. We use an in-house dataset in the financial education domain for our tests. We calibrated the two implemented methods to find out which performs better, using a Jensen-Shannon based distance between the correct and incorrect outputs of the discriminator. Our tests showed an important advantage of the Monte Carlo Dropout over the Deep Ensemble Based method.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work explores two simple uncertainty estimation approaches for complex text identification: a Monte Carlo Dropout and an Deep Ensemble Based approach, and calibrated the two implemented methods to find out which performs better."
            },
            "score": 2
        },
        {
            "id": "bff7b8aa4f92fe16d9bd24158b9baef317ef7cc5",
            "paperId": "bff7b8aa4f92fe16d9bd24158b9baef317ef7cc5",
            "title": "Enhancing Large Language Models for Secure Code Generation: A Dataset-driven Study on Vulnerability Mitigation",
            "abstract": "Large language models (LLMs) have brought significant advancements to code generation, benefiting both novice and experienced developers. However, their training using unsanitized data from open-source repositories, like GitHub, introduces the risk of inadvertently propagating security vulnerabilities. To effectively mitigate this concern, this paper presents a comprehensive study focused on evaluating and enhancing code LLMs from a software security perspective. We introduce SecuCoGen\\footnote{SecuCoGen has been uploaded as supplemental material and will be made publicly available after publication.}, a meticulously curated dataset targeting 21 critical vulnerability types. SecuCoGen comprises 180 samples and serves as the foundation for conducting experiments on three crucial code-related tasks: code generation, code repair and vulnerability classification, with a strong emphasis on security. Our experimental results reveal that existing models often overlook security concerns during code generation, leading to the generation of vulnerable code. To address this, we propose effective approaches to mitigate the security vulnerabilities and enhance the overall robustness of code generated by LLMs. Moreover, our study identifies weaknesses in existing models' ability to repair vulnerable code, even when provided with vulnerability information. Additionally, certain vulnerability types pose challenges for the models, hindering their performance in vulnerability classification. Based on these findings, we believe our study will have a positive impact on the software engineering community, inspiring the development of improved methods for training and utilizing LLMs, thereby leading to safer and more trustworthy model deployment.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study identifies weaknesses in existing models' ability to repair vulnerable code, even when provided with vulnerability information, and proposes effective approaches to mitigate the security vulnerabilities and enhance the overall robustness of code generated by LLMs."
            },
            "score": 2
        },
        {
            "id": "09426036b6a320a1886fea6b864bf9b1a91a94eb",
            "paperId": "09426036b6a320a1886fea6b864bf9b1a91a94eb",
            "title": "Effects and Mitigation of Out-of-vocabulary in Universal Language Models",
            "abstract": ": One of the most important recent natural language processing (NLP) trends is transfer learning \u2013 using representations from language models implemented through a neural network to perform other tasks. While transfer learning is a promising and robust method, downstream task performance in transfer learning depends on the robustness of the backbone model\u2019s vocabulary, which in turn represents both the positive and negative characteristics of the corpus used to train it. With subword tokenization, out-of-vocabulary (OOV) is generally assumed to be a solved problem. Still, in languages with a large alphabet such as Chinese, Japanese, and Korean (CJK), this assumption does not hold. In our work, we demonstrate the adverse e \ufb00 ects of OOV in the context of transfer learning in CJK languages, then propose a novel approach to maximize the utility of a pre-trained model su \ufb00 ering from OOV. Additionally, we further investigate the correlation of OOV to task performance and explore if and how mitigation can salvage a model with high OOV.",
            "year": 2021,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The adverse effects of OOV are demonstrated in the context of transfer learning in CJK languages, then a novel approach to maximize the utility of a pre-trained model is proposed to maximize the utility of a pre-trained model from OOV."
            },
            "score": 2
        },
        {
            "id": "c653e3726792e45416fe014fe03531bc0ada2226",
            "paperId": "c653e3726792e45416fe014fe03531bc0ada2226",
            "title": "Enhancing Collaborative Learning: Impact of \"Question Prompts\" Design for Online Discussion",
            "abstract": "Abstract Objective: The purpose of study was to investigate the impact of question prompts designed to guide students' focus on context-related issues as they solve problems in a web-based environment. Background: Online discussions integrated with collaborative learning were used to examine student interactions and behaviors in an online discussion. Method: Twenty graduate students were randomly assigned to either the treatment (ten participants) or the control group (ten participants); one group received question prompts while working in the web-based learning environment while the other did not. At the end, they were asked to fill out an online questionnaire rating their confidence and competence levels in the problem solving process. Analyses were conducted to determine the impact of question prompts. Results: Statistical results showed that students who received question prompts from the instructor received significantly higher evaluations than students who did not receive question prompts when analyzed overall and when each rubric was analyzed individually. In addition, the study also suggested that online discussion had significant positive impact in developing students' deeper teaming. Conclusion: Question prompts designed by the instructor helped to enhance students' self-efficacy about their ability to solve complex problems. Application: The outcome indicated the importance of teachers' guidance and assistance to lead students into deeper learning. Although online learning provides flexibility and potential for deeper learning, the online discussion needs to be structured and moderated throughout the process. Deeper learning engages the learners who actively explore, reflect, and produce knowledge (Wickersham & McGee, 2008), and it has been gaining popularity over the past decade. Although current discourse and literature tends to encourage deeper learning as a desired outcome (Carmean & Haefner, 2002) what actually happens when online discussion tools are used may not reflect these principles. This disparity may be deeper than what researchers observe in actual practice. Embedding deeper learning theory into online discussion design by actively engaging the learners, providing them ownership of knowledge they have produced can only ensure that learning transcends the classroom. In order for deeper learning principles to be applied to online collaborative discussions, teachers must consciously consider the entire learning environment and how they articulate the processes within that environment. For example, Laurillard and McAndrew (2003) propose the design of generic learning activities that shifts teaching from a transmission model to a construction model. Their conversational framework is an iterative process that requires learners to engage, act, and reflect upon what they know and how they come to learn through the effectiveness of discussion in a web-based learning environment. There has been prolonged discussion about deficiencies in students' abilities to establish a higher order thinking base resulting from oversimplifying complex new knowledge and generating vague relationships between prior and new knowledge (Feltovich, Spiro, Coulson, & Feltovich, 1996). Students often process information superficially and mindlessly (Bransford, Brown, & Cocking, 2000). Lack of knowledge integration hinders knowledge acquisition and further impairs abilities to solve real-world problems. Deeper understanding is crucial because it stimulates awareness and capacity for monitoring, active reflecting, evaluating, and modifying one's own knowledge (Linn & Hsi, 2000). Collaborative discussions have the potential to enhance individualized learning experiences for learners in which their learning styles, prior knowledge, and specific learning needs are taken into account. Group discussions have been demonstrated to have a positive impact on learning by facilitating collaborative thinking and better understanding (Levin, 1995), and by raising self-efficacy through observing peer models (Brown & Inouye, 1978; Schunk, 1987). \u2026",
            "year": 2011,
            "citationCount": 20,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Investigating the impact of question prompts designed to guide students' focus on context-related issues as they solve problems in a web-based environment indicated the importance of teachers' guidance and assistance to lead students into deeper learning."
            },
            "score": 2
        },
        {
            "id": "c44d928c25624daed35c96235469a759c3811eef",
            "paperId": "c44d928c25624daed35c96235469a759c3811eef",
            "title": "Development of an intervention to facilitate dissemination of community-based training to respond to out-of-hospital cardiac arrest: FirstCPR",
            "abstract": "Background and aim Out-of-hospital cardiac arrest (OHCA) is a significant public health issue with low survival rates. Prompt bystander action can more than double survival odds. OHCA response training is primarily pursued due to work-related mandates, with few programs targeting communities with lower training levels. The aim of this research was to describe the development process of a targeted multicomponent intervention package designed to enhance confidence and training among laypeople in responding to an OHCA. Methods An iterative, three-phase program development process was employed using a mixed methods approach. The initial phase involved establishment of a multidisciplinary panel that informed decisions on key messages, program content, format, and delivery modes. These decisions were based on scientific evidence and guided by behavioural theories. The second phase comprised the development of the intervention package, identifying existing information and developing new material to fill identified gaps. The third phase involved refining and finalising the material via feedback from panel members, stakeholders, and community members. Results Through this approach, we collaboratively developed a comprehensive evidence-based education and training package consisting of a digital intervention supplemented with free access to in-person education and training. The package was designed to teach community members the specific steps in recognising and responding to a cardiac arrest, while addressing commonly known barriers and fears related to bystander response. The tailored program and delivery format addressed the needs of individuals of diverse ages, cultural backgrounds, and varied training needs and preferences. Conclusion The study highlights the importance of community engagement in intervention development and demonstrates the need of evidence-based and collaborative approaches in creating a comprehensive, localised, relatively low-cost intervention package to improve bystander response to OHCA.",
            "year": 2022,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The study highlights the importance of community engagement in intervention development and demonstrates the need of evidence-based and collaborative approaches in creating a comprehensive, localised, relatively low-cost intervention package to improve bystander response to OHCA."
            },
            "score": 1
        },
        {
            "id": "d83741fd9ddde06eac055171bb6944a2ea2a2887",
            "paperId": "d83741fd9ddde06eac055171bb6944a2ea2a2887",
            "title": "Is there a role for endoscopic ultrasound-guided fine-needle biopsy in pancreatic cancer?",
            "abstract": "Endoscopic ultrasound-guided fine-needle aspiration (EUS-FNA) has transformed the practice of diagnosing pancreatic adenocarcinoma by displacing surgical biopsy, and thus has resulted in lower cost, technical ease, and decreased morbidity [1]. However, there is now an increased demand for more precise \u201ctumor-specific\u201d diagnoses, as the treatment paradigm is shifting to a more personalized approach. This, in turn, has led to the need for better tissue fragments for cancer biomarker testing and grading. The two major limiting factors encountered in EUS-FNA are the quality and quantity of the tissue obtained. These limitations have prompted investigation into the use of larger-caliber needles, which have led to the analysis of fine-needle biopsy (FNB) as a workable alternative. Intuitively, the assumption has been made that larger or more specialized needles would yield better tissue for subsequent molecular biological analysis and histologic grading. This issue of Endoscopy contains the report from a study by Larghi et al., which evaluated a standard 19-G or 19-G ProCore needle for performing preoperative EUS-FNB in 42 patients with pancreatic ductal adenocarcinoma (PDAC) [2]. Four experienced pathologists who were blinded to the results of surgical resection performed the histologic grading. The accuracy of preoperative grading of PDAC by EUS-FNB was 56%, with significant heterogeneity and suboptimal agreement among the four pathologists (\u03ba=0.27; 95% confidence interval 0.14\u20130.38). The results of this study suggest that EUS-FNB does not allow for reliable preoperative grading of PDAC. Moreover, these results align with the results of two other studies that were reported recently in Endoscopy. Lee et al. compared the ProCore needle with the standard FNA needle (n=116) and found no significant difference in overall diagnostic accuracy (FNA 94.8% vs. FNB 98.3%; P=0.67) or histological accuracy (FNA 77.6 % vs. FNB 82.8%; P=0.64) [3]. In a randomized, crossover study by Vanbiervliet et al. (n=80), which used the standard 22-G FNA needle and 22-G ProCore needle, the quality of the tissue was statistically better with the standard needle as evaluated by two pathologists (expert 1, P=0.009; expert 2, P=0.002) [4]. The issue is not that FNB does not obtain histologic tissue, but rather it does not consistently obtain optimal (diagnostic and sufficient) tissue. This is supported by a recent study by Iwashita et al., which evaluated the macroscopic appearance of 111 biopsy specimens procured using the 19-G needle. Although a macroscopically visible core tissue was obtained in 91%, histologic core tissue was seen in only 79%, with a particularly high false-negative rate in pancreatic lesions [5]. Unlike surgical resection specimens, which embody the entire tumor, EUS-guided biopsies sample only a focal area of the suspected lesion. Histologic examination of such biopsies increases the probability of sampling and interpretation errors secondary to tumor heterogeneity. This is corroborated by two studies evaluating pancreatic neuroendocrine grading on EUS-FNA using the Ki-67 index. Both studies found high reproducibility and interobserver agreement when sufficient tissue was present, and significant discordance between the histologic and cytologic findings when limited EUS-guided material was evaluated [6,7]. Advances in EUS tissue acquisition have been an iterative process, where continual re-evaluation of the best tools and techniques has indisputably led to its success. Therefore, in an effort to rectify our shortcomings, we must start by asking, \u201cIs there truly a role for EUS-FNB in pancreatic malignancy?\u201d and \u201cWhat can we do to make EUS-FNB successful?\u201d Pancreatic ductal adenocarcinoma is extremely aggressive and carries a poor prognosis. There are limited therapeutic options, such as gemcitabine and paclitaxel, or leucovorin and fluoroura-",
            "year": 2015,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results of this study suggest that EUS-FNB does not allow for reliable preoperative grading of PDAC, and these results align with the results of two other studies that were reported recently in Endoscopy."
            },
            "score": 1
        },
        {
            "id": "fc27d1f1235130d93894391413b493f58ea10db9",
            "paperId": "fc27d1f1235130d93894391413b493f58ea10db9",
            "title": "Co-development of an evidence-based personalised smoking cessation intervention for use in a lung cancer screening context",
            "abstract": null,
            "year": 2022,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A booklet containing pictures of a person\u2019s own lungs and heart taken during a lung cancer screening scan, developed for delivery by a trained smoking cessation practitioner, highlights the benefit of co-development during intervention development and the need for further evaluation of effectiveness."
            },
            "score": 1
        },
        {
            "id": "42667d2286162d001f5154a36e8d0a1ee549dab8",
            "paperId": "42667d2286162d001f5154a36e8d0a1ee549dab8",
            "title": "Measure Development and Validation of a Self-Efficacy Scale for Advance Care Planning",
            "abstract": "A disparity exists between the medical intervention people say they want to receive at end-of-life and the care that is typically delivered. Advance care planning (ACP) involves discussing end-of-life care wishes, including relevant values and cultural beliefs, and documenting these preferences for medical providers and loved ones to minimize unwanted suffering and maximize quality of life. Numerous healthcare institutions have emphasized the importance of doing ACP prior to an imminent medical need, prompting researchers to implement awareness campaigns and interventions in earlier stages of healthcare interactions (e.g., primary care). However, motivation to follow through with ACP varies depending on numerous factors including overall readiness, understanding of the risks and benefits, and how one manages the internal experience of facing one\u2019s own mortality. One intrinsic experience that has been shown to be important for health behavior change is situation specific-confidence, or selfefficacy. This work builds on previous research that approaches ACP intervention from the theoretical framework of the Transtheoretical model (TTM) of behavior change, in which self-efficacy is a core component. Study 1 of this dissertation sought to explore the construct of self-efficacy specific to doing ACP with qualitative work including expert interviews and focus groups with older adults in the community about their experiences. The work presented describes the efforts to understand self-efficacy as a barrier to engagement in end of life care planning. Self-efficacy was associated with interpersonal support, access to structured tools to guide discussions, and tolerance of the unpleasantness of negative emotions. Assessment of themes from focus groups and expert interviews was conducted to write items of a scale of self-efficacy to do ACP. Study 2 of this dissertation describes the development and validation of a scale of ACP self-efficacy using a sequential approach to measure development. Qualitative and quantitative methods were utilized for item development/refinement and scale validation. Split-half validation procedures were conducted, with exploratory and confirmatory factor analyses on randomly selected subsamples. The results of several iterations of exploratory factor analyses supported a final set of 12 items loading on one factor, with high internal consistency. The final 12-item ACP self-efficacy scale was found to have good overall model fit in confirmatory analyses, assessed with \u03c7 tests of significance and fit indices. Further, the developed scale was validated using previously developed TTM measures of ACP behavior change (Stage of Change, Decisional Balance) and related constructs (General Self-Efficacy, Attitudes Values & Cultural Beliefs). As expected, ACP self-efficacy varied by stage of change, with those in more progressed stages endorsing higher levels of confidence that they could complete ACP behaviors, upholding the relationships hypothesized by the TTM. Together, these two studies address the importance of self-efficacy for engagement in complex behaviors and provide a tool for future use to gain a deeper understanding and increase behavior change in this area.",
            "year": 2019,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Two studies address the importance of self-efficacy for engagement in complex behaviors and provide a tool for future use to gain a deeper understanding and increase behavior change in this area."
            },
            "score": 1
        },
        {
            "id": "cb8df902cddf9b7dc5cb62bd26e31589421e5090",
            "paperId": "cb8df902cddf9b7dc5cb62bd26e31589421e5090",
            "title": "Health visitors\u2019 perception of their role in the universal childhood immunisation programme and their communication strategies with parents",
            "abstract": "Aim This study explored health visitors\u2019 perception of their role in the universal childhood immunisation programme with particular emphasis on influencing factors and communication strategies. Background The majority of parents\u2019 consent to immunisation, but some find decision-making in this area difficult and have unmet information needs. In the United Kingdom, health visitors routinely provide immunisation information for parents, whereas general practitioners (GPs) and practice nurses tend to administer vaccines and respond to parents/carers\u2019 questions. Research has investigated health professionals\u2019 views and knowledge about immunisation, but less is understood about health visitors\u2019 role and how they communicate with parents. Method Following the Local Research Ethics and Research Governance permissions, all health visitors (n = 120) working in one county in the United Kingdom were invited to participate in the study. Semistructured interviews (n = 22) were undertaken using a prompt guide. The interviews were transcribed verbatim. Thematic analysis using an iterative approach was used to explore the data facilitated by NVIVO\u2122 software. Findings Five themes emerged from the interviews. These were health visitors\u2019 professional role; identity and perceived barriers and communication strategies, parents\u2019 right to choose, confidence in measles, mumps, and rubella (MMR) vaccination and communicating with migrant families about immunisation. There were differences between the health visitors in their perceptions of their roles, skills and knowledge and communication strategies. Health visitors perceived that GPs and practice nurses took a paternalistic approach to the provision of immunisation information, while they used a parental decision-making model. Health visitors reported a loss of professional confidence following the MMR crisis. Conclusion Given the evidence that some parents find it difficult to gain the information they need about immunisation and health visitors\u2019 acknowledgement that their usual communication models were not effective during the MMR crisis, we feel specific communication skills training is needed to enable health professionals to provide parents with appropriate decision support.",
            "year": 2009,
            "citationCount": 21,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Specific communication skills training is needed to enable health professionals to provide parents with appropriate decision support during the MMR crisis, given the evidence that some parents find it difficult to gain the information they need about immunisation."
            },
            "score": 1
        },
        {
            "id": "d9afe4e200a9725f9829e55218b43c49889d77d9",
            "paperId": "d9afe4e200a9725f9829e55218b43c49889d77d9",
            "title": "Image-based EUVL aberration metrology",
            "abstract": "Kate Gleason College of Engineering Rochester Institute of Technology Degree: Doctor of Philosophy Program: Microsystems Engineering Authors Name: Germain L. Fenger Advisors Name: Dr. Bruce W. Smith Dissertation Title: Image-Based EUVL Aberration Metrology A significant factor in the degradation of nanolithographic image fidelity is optical wavefront aberration. As resolution of nanolithography systems increases, effects of wavefront aberrations on aerial image become more influential. The tolerance of such aberrations is governed by the requirements of features that are being imaged, often requiring lenses that can be corrected with a high degree of accuracy and precision. Resolution of lithographic systems is driven by scaling wavelength down and numerical aperture (NA) up. However, aberrations are also affected from the changes in wavelength and NA. Reduction in wavelength or increase in NA result in greater impact of aberrations, where the latter shows a quadratic dependence. Current demands in semiconductor manufacturing are constantly pushing lithographic systems to operate at the diffraction limit; hence, prompting a need to reduce all degrading effects on image properties to achieve maximum performance. Therefore, the need for highly accurate insitu aberration measurement and correction is paramount. In this work, an approach has been developed in which several targets including phase wheel, phase disk, phase edges, and binary structures are used to generate optical images to detect and monitor aberrations in extreme ultraviolet (EUV) lithographic systems. The benefit of using printed patterns as opposed to other techniques is that the lithography system is tested under standard operating conditions. Mathematical models in conjunction with iterative lithographic simulations are used to determine pupil phase wavefront errors and describe them as combinations of Zernike polynomials. ACKNOWLEDGMENTS I would like first thank my advisor, Dr. Bruce W. Smith, specifically for guiding me when needed, but giving me the confidence and room to think independently and critically. I would also like to thank my committee members Dr. Karl D. Hirschman, Dr. Zoran Ninkov, and Dr. Obert R. Wood II for guiding me through the PhD process. I would like to especially acknowledge and thank Obert, without his support much of this work would not have been possible. I would like to acknowledge and thanks KLA Tencor for the use of PROLITH TM , GLOBALFOUNDRIES for the fabrication of a phase-shifting reticle and processing support, the IBM alliance and partners for use of the cleanroom facilities including scanner and SEM tool time, Dr. Lena Zavyalova for her many discussions, Luke Orsini, Dr. Lei Sun, and Dr. Sudhar Raghunathan for help with exposures. I would also like to thank and acknowledge my fellow graduate students for their encouragement and support, Burak Baylav, Dr. Monica Sears, Andrew Estroff, Chris Maloney, Dr. Neal Lafferty and Dr. Peng Xie. I would also like to the U.S. Department of Education Graduate Assistance in Areas of National Need (GAAN) program and the Semiconductor Research Corporation (SRC/GRC) for their support. Table of",
            "year": 2013,
            "citationCount": 3,
            "tldr": null,
            "score": 1
        }
    ],
    "novelty": "yes"
}