{
    "topic_description": "novel prompting methods that can better quantify uncertainty or calibrate the confidence of large language models",
    "idea_name": "Uncertainty-Aware Counterfactual Prompting",
    "raw_idea": {
        "Problem": "Large language models often struggle to generate consistent and coherent counterfactual responses, especially when dealing with uncertain or ambiguous contexts. This limits their applicability in scenarios such as creative writing, policy analysis, and decision support.",
        "Existing Methods": "Existing methods for generating counterfactual responses with LLMs include using structured prompts, fine-tuning on counterfactual datasets, and incorporating causal reasoning frameworks. However, these methods often rely on deterministic outcomes and do not explicitly model the uncertainty in the counterfactual scenarios.",
        "Motivation": "We propose an uncertainty-aware counterfactual prompting approach, where the LLM is prompted to generate multiple possible counterfactual scenarios and their associated likelihoods based on the uncertainty in the context. By explicitly modeling the uncertainty and generating a distribution over possible outcomes, we can improve the coherence and robustness of the counterfactual responses.",
        "Proposed Method": "Our method, Uncertainty-Aware Counterfactual Prompting (UACP), consists of the following steps: 1) Given an input context and a counterfactual query, prompt the LLM to identify the uncertain or ambiguous aspects of the context that are relevant to the query. 2) For each uncertain aspect, prompt the LLM to generate multiple possible counterfactual scenarios and their associated likelihoods based on the degree of uncertainty. 3) Prompt the LLM to generate a response for each counterfactual scenario, taking into account the likelihood of that scenario. 4) Aggregate the generated responses based on their likelihoods to produce a final counterfactual response that captures the uncertainty in the context. 5) Prompt the LLM to generate an explanation of the uncertainty and its impact on the counterfactual response.",
        "Experiment Plan": "We will evaluate UACP on a range of counterfactual reasoning tasks, such as story generation, policy analysis, and decision support. We will compare the coherence, diversity, and robustness of the generated counterfactual responses against baselines such as deterministic counterfactual prompting and fine-tuning on counterfactual datasets. Metrics will include measures of coherence, diversity, and consistency with the input context. We will also conduct human evaluations to assess the quality and usefulness of the uncertainty-aware counterfactual responses in different application scenarios."
    },
    "full_experiment_plan": {
        "Title": "Uncertainty-Aware Counterfactual Prompting for Coherent and Robust Reasoning in Large Language Models",
        "Problem Statement": "Large language models often struggle to generate consistent and coherent counterfactual responses, especially when dealing with uncertain or ambiguous contexts. This limits their applicability in scenarios such as creative writing, policy analysis, and decision support.",
        "Motivation": "Existing methods for generating counterfactual responses with LLMs, such as using structured prompts, fine-tuning on counterfactual datasets, and incorporating causal reasoning frameworks, often rely on deterministic outcomes and do not explicitly model the uncertainty in the counterfactual scenarios. Inspired by human reasoning, where uncertainty is explicitly considered and multiple possible outcomes are weighed based on their likelihoods, we propose an uncertainty-aware counterfactual prompting approach. By prompting the LLM to generate multiple possible counterfactual scenarios and their associated likelihoods based on the uncertainty in the context, we aim to improve the coherence and robustness of the counterfactual responses.",
        "Proposed Method": "Our method, Uncertainty-Aware Counterfactual Prompting (UACP), consists of the following steps:\n1. Given an input context and a counterfactual query, prompt the LLM to identify the uncertain or ambiguous aspects of the context that are relevant to the query.\n2. For each uncertain aspect, prompt the LLM to generate multiple possible counterfactual scenarios and their associated likelihoods based on the degree of uncertainty.\n3. Prompt the LLM to generate a response for each counterfactual scenario, taking into account the likelihood of that scenario.\n4. Aggregate the generated responses based on their likelihoods to produce a final counterfactual response that captures the uncertainty in the context.\n5. Prompt the LLM to generate an explanation of the uncertainty and its impact on the counterfactual response.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "We will evaluate UACP on a range of counterfactual reasoning tasks, such as story generation (WritingPrompts dataset), policy analysis (synthetic dataset with policy scenarios), and decision support (synthetic dataset with decision-making scenarios). For each dataset, we will create a subset of examples that involve uncertain or ambiguous contexts.",
            "Step 2: Construct Prompts": "1. Baseline prompts: a) Direct prompting: Provide the input context and counterfactual query directly to the LLM. b) Structured prompting: Use templates to guide the LLM in generating counterfactual responses, e.g., \"Given [context], what would happen if [counterfactual query]?\"\n2. UACP prompts: a) Uncertainty identification: \"Given [context] and [counterfactual query], what are the uncertain or ambiguous aspects of the context that are relevant to the query?\" b) Counterfactual scenario generation: \"For [uncertain aspect], generate [N] possible counterfactual scenarios and their associated likelihoods based on the degree of uncertainty.\" c) Response generation: \"For [counterfactual scenario], generate a response taking into account its likelihood of [likelihood].\" d) Response aggregation: \"Aggregate the following responses based on their likelihoods: [generated responses].\" e) Explanation generation: \"Explain how the uncertainty in the context impacts the counterfactual response.\"",
            "Step 3: Select Models": "We will use GPT-3.5 (text-davinci-003) and GPT-4 from the OpenAI API for our experiments.",
            "Step 4: Get Results": "For each dataset and model combination, we will generate counterfactual responses using the baseline prompts and UACP prompts. We will evaluate the generated responses using both automatic metrics (e.g., coherence score, diversity score) and human evaluation (e.g., coherence, consistency, robustness to uncertainty).",
            "Step 5: Analyze Results": "We will compare the performance of UACP against the baselines in terms of the coherence, diversity, and robustness of the generated counterfactual responses. We will also analyze the impact of the number of counterfactual scenarios generated (N) and the quality of the uncertainty identification and explanation steps on the overall performance of UACP."
        },
        "Test Case Examples": {
            "Test Case 1": {
                "Input Context": "John is considering investing in a new startup. The startup has a novel idea but lacks a clear business plan.",
                "Counterfactual Query": "What would happen if the startup manages to secure a major client?",
                "Baseline Prompt (Direct)": "John is considering investing in a new startup. The startup has a novel idea but lacks a clear business plan. What would happen if the startup manages to secure a major client?",
                "Baseline Output (Direct)": "If the startup manages to secure a major client, it would likely see a significant increase in revenue and growth. This could help the startup attract more investors and expand its operations. However, without a clear business plan, the startup may struggle to sustain this growth in the long term.",
                "UACP Prompt (Uncertainty Identification)": "John is considering investing in a new startup. The startup has a novel idea but lacks a clear business plan. What would happen if the startup manages to secure a major client? What are the uncertain or ambiguous aspects of the context that are relevant to the query?",
                "UACP Output (Uncertainty Identification)": "The uncertain aspects relevant to the query are: 1) The viability of the startup's novel idea, 2) The startup's ability to execute its idea and deliver value to the major client, 3) The terms of the contract with the major client, 4) The startup's capacity to handle the increased demand and scale its operations.",
                "UACP Prompt (Counterfactual Scenario Generation)": "For the uncertain aspect of the startup's ability to execute its idea and deliver value to the major client, generate 3 possible counterfactual scenarios and their associated likelihoods.",
                "UACP Output (Counterfactual Scenario Generation)": "Scenario 1 (60% likelihood): The startup successfully executes its idea and delivers significant value to the major client, leading to a long-term partnership and stable revenue stream.\nScenario 2 (30% likelihood): The startup faces challenges in executing its idea and delivers only partial value to the major client, leading to a short-term contract and limited revenue growth.\nScenario 3 (10% likelihood): The startup fails to execute its idea and delivers no value to the major client, leading to contract termination and reputational damage.",
                "UACP Prompt (Response Generation)": "For Scenario 1 (60% likelihood), generate a response taking into account its likelihood.",
                "UACP Output (Response Generation)": "If the startup successfully executes its idea and delivers significant value to the major client (60% likelihood), it would likely secure a long-term partnership and stable revenue stream. This would greatly increase the startup's chances of success and make it an attractive investment opportunity for John. The startup could use this success to refine its business plan, attract more clients, and scale its operations sustainably.",
                "UACP Prompt (Response Aggregation)": "Aggregate the following responses based on their likelihoods: [Response 1], [Response 2], [Response 3].",
                "UACP Output (Response Aggregation)": "Based on the counterfactual scenarios and their likelihoods, if the startup manages to secure a major client, there is a 60% chance that it will successfully execute its idea, leading to a long-term partnership, stable revenue, and increased attractiveness for investors. However, there is a 30% chance that the startup will face challenges in delivering value, resulting in limited revenue growth, and a 10% chance of failing to deliver any value, leading to contract termination and reputational damage. The startup's success depends heavily on its ability to execute its idea and deliver value to the client. John should carefully consider these uncertainties before making his investment decision.",
                "UACP Prompt (Explanation Generation)": "Explain how the uncertainty in the context impacts the counterfactual response.",
                "UACP Output (Explanation Generation)": "The uncertainty in the context, particularly regarding the startup's ability to execute its idea and deliver value to the major client, significantly impacts the counterfactual response. The likelihood of the startup's success varies based on how well it can handle this uncertainty. If the startup can successfully navigate this challenge, it has a higher chance of securing a long-term partnership and stable revenue. However, if it fails to do so, it may face limited growth or even contract termination. This uncertainty makes the investment decision more complex and risky for John, as the startup's success is not guaranteed even if it manages to secure a major client."
            },
            "Test Case 2": {
                "Input Context": "A city is considering implementing a new public transportation system to reduce traffic congestion and improve air quality. However, the city has limited funds and is unsure about the public's willingness to adopt the new system.",
                "Counterfactual Query": "What would happen if the city invests in an extensive marketing campaign to promote the new public transportation system?",
                "Baseline Prompt (Structured)": "Given that a city is considering implementing a new public transportation system to reduce traffic congestion and improve air quality, but has limited funds and is unsure about the public's willingness to adopt the new system, what would happen if the city invests in an extensive marketing campaign to promote the new public transportation system?",
                "Baseline Output (Structured)": "If the city invests in an extensive marketing campaign to promote the new public transportation system, it could potentially increase public awareness and interest in the system. This could lead to higher adoption rates and a more successful implementation of the system. However, the effectiveness of the marketing campaign would depend on various factors, such as the quality of the messaging, the target audience, and the public's perception of the new system. Additionally, investing in a marketing campaign would further strain the city's limited funds, which could impact the overall quality and sustainability of the public transportation system.",
                "UACP Prompt (Uncertainty Identification)": "A city is considering implementing a new public transportation system to reduce traffic congestion and improve air quality. However, the city has limited funds and is unsure about the public's willingness to adopt the new system. What would happen if the city invests in an extensive marketing campaign to promote the new public transportation system? What are the uncertain or ambiguous aspects of the context that are relevant to the query?",
                "UACP Output (Uncertainty Identification)": "The uncertain aspects relevant to the query are: 1) The effectiveness of the marketing campaign in increasing public awareness and interest, 2) The public's perception of the new public transportation system, 3) The impact of the marketing campaign on the city's limited funds, 4) The quality and sustainability of the public transportation system given the city's limited resources.",
                "UACP Prompt (Counterfactual Scenario Generation)": "For the uncertain aspect of the public's perception of the new public transportation system, generate 3 possible counterfactual scenarios and their associated likelihoods.",
                "UACP Output (Counterfactual Scenario Generation)": "Scenario 1 (50% likelihood): The public perceives the new public transportation system as a convenient, reliable, and eco-friendly alternative to private vehicles, leading to high adoption rates.\nScenario 2 (30% likelihood): The public has mixed opinions about the new system, with some embracing it while others remain skeptical, leading to moderate adoption rates.\nScenario 3 (20% likelihood): The public perceives the new system as inconvenient, unreliable, or unsafe, leading to low adoption rates.",
                "UACP Prompt (Response Generation)": "For Scenario 2 (30% likelihood), generate a response taking into account its likelihood.",
                "UACP Output (Response Generation)": "If the public has mixed opinions about the new public transportation system (30% likelihood), the city's investment in an extensive marketing campaign may have a limited impact on adoption rates. Some residents would embrace the new system, while others would remain skeptical. The marketing campaign would need to address the concerns of the skeptical residents and highlight the benefits of the new system to encourage adoption. The city may need to allocate additional resources to improve the quality and reliability of the system to win over more residents.",
                "UACP Prompt (Response Aggregation)": "Aggregate the following responses based on their likelihoods: [Response 1], [Response 2], [Response 3].",
                "UACP Output (Response Aggregation)": "Based on the counterfactual scenarios and their likelihoods, if the city invests in an extensive marketing campaign to promote the new public transportation system, there is a 50% chance that the public will perceive the system positively, leading to high adoption rates. However, there is a 30% chance that the public will have mixed opinions, resulting in moderate adoption rates, and a 20% chance that the public will perceive the system negatively, leading to low adoption rates. The success of the marketing campaign and the public transportation system depends on the public's perception and the city's ability to address any concerns or skepticism. The city should carefully consider the potential outcomes and allocate its limited resources accordingly.",
                "UACP Prompt (Explanation Generation)": "Explain how the uncertainty in the context impacts the counterfactual response.",
                "UACP Output (Explanation Generation)": "The uncertainty in the context, particularly regarding the public's perception of the new public transportation system, significantly impacts the counterfactual response. The effectiveness of the marketing campaign in increasing adoption rates depends on how the public perceives the system. If the public has a positive perception, the marketing campaign is more likely to succeed in encouraging high adoption rates. However, if the public has mixed opinions or a negative perception, the marketing campaign may have a limited impact on adoption rates. This uncertainty makes it challenging for the city to predict the outcome of its investment in the marketing campaign and the overall success of the public transportation system. The city must carefully consider the potential scenarios and allocate its limited resources in a way that maximizes the chances of success while minimizing the risks associated with the uncertainty in public perception."
            }
        },
        "Fallback Plan": "If the proposed UACP method does not outperform the baselines, we can conduct additional analyses to identify the reasons behind its underperformance. Some potential areas to investigate include:\n1. Quality of uncertainty identification: Analyze the generated uncertainty aspects to determine if they accurately capture the relevant uncertainties in the context. If the uncertainty identification step is not performing well, it could lead to less coherent and robust counterfactual responses.\n2. Quality of counterfactual scenario generation: Analyze the generated counterfactual scenarios and their associated likelihoods to determine if they are plausible and cover a diverse range of possible outcomes. If the scenarios are not diverse or plausible enough, it could limit the effectiveness of the UACP method.\n3. Impact of the number of counterfactual scenarios: Experiment with different values of N (the number of counterfactual scenarios generated) to determine if increasing or decreasing N affects the performance of UACP. There may be an optimal range of N that balances the diversity and coherence of the generated responses.\n4. Quality of response aggregation: Analyze the aggregated responses to determine if they effectively combine the information from the individual counterfactual scenarios. If the aggregation step is not performing well, it could lead to less coherent and robust final responses.\n5. Impact of the selected language models: Experiment with different language models (e.g., GPT-3.5 vs. GPT-4) to determine if the choice of model affects the performance of UACP. Some models may be better suited for handling uncertainty and generating coherent counterfactual responses than others.\nBased on the findings from these additional analyses, we can propose modifications to the UACP method or develop alternative approaches that address the identified limitations. For example, if the uncertainty identification step is found to be a key bottleneck, we could explore alternative prompting strategies or incorporate external knowledge sources to improve the accuracy and relevance of the identified uncertainties.\nIf the UACP method and its variations still do not yield satisfactory results, we can pivot the project to focus on analyzing the challenges and limitations of generating coherent and robust counterfactual responses using large language models. This could involve conducting a more in-depth error analysis of the generated responses, comparing the performance of different language models and prompting strategies, and exploring the trade-offs between diversity and coherence in counterfactual reasoning. The insights gained from this analysis could inform future research directions and the development of more effective methods for counterfactual reasoning with large language models."
    },
    "novelty_queries": [
        "KeywordQuery(\"uncertainty-aware counterfactual prompting language models\")",
        "KeywordQuery(\"coherent robust counterfactual reasoning language models\")",
        "KeywordQuery(\"counterfactual scenario generation language models\")",
        "KeywordQuery(\"Uncertainty-Aware Counterfactual Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "0a51afdcd7cf4f33987d766082a7d3f174936c8a",
            "paperId": "0a51afdcd7cf4f33987d766082a7d3f174936c8a",
            "title": "Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models",
            "abstract": "In the face of uncertainty, the ability to seek information is of fundamental importance. In many practical applications, such as medical diagnosis and troubleshooting, the information needed to solve the task is not initially given, and has to be actively sought by asking follow-up questions (for example, a doctor asking a patient for more details about their symptoms). In this work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment large language models with the ability to actively seek information by asking effective questions. UoT combines 1) an uncertainty-aware simulation approach which enables the model to simulate possible future scenarios and how likely they are to occur, 2) uncertainty-based rewards motivated by information gain which incentivizes the model to seek information, and 3) a reward propagation scheme to select the optimal question to ask in a way that maximizes the expected reward. In experiments on medical diagnosis, troubleshooting and the '20 Questions' game, UoT achieves an average performance improvement of 57.8% in the rate of successful task completion across multiple LLMs compared with direct prompting, and also improves efficiency (i.e., the number of questions needed to complete the task).",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Uncertainty of Thoughts is introduced, an algorithm to augment large language models with the ability to actively seek information by asking effective questions and achieves an average performance improvement of 57.8% in the rate of successful task completion across multiple LLMs compared with direct prompting."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is improving the coherence and robustness of counterfactual reasoning in large language models by explicitly modeling uncertainty. The proposed approach is to use uncertainty-aware prompting to generate multiple possible counterfactual scenarios and their likelihoods, and then aggregate the responses based on the likelihoods.\n\nThe research problem in the paper is enhancing the ability of large language models to actively seek information by asking effective questions in the face of uncertainty. The proposed approach is to use an uncertainty-aware simulation to generate possible future scenarios, uncertainty-based rewards to incentivize information seeking, and reward propagation to select the optimal question.\n\nWhile both works deal with uncertainty in large language models, the research problems and approaches are different. The proposal focuses on counterfactual reasoning and uses uncertainty-aware prompting, while the paper focuses on information seeking and uses uncertainty-aware planning and reward propagation.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "c3287da21ddf6f6c87b7b328996c1cf3fec92942",
            "paperId": "c3287da21ddf6f6c87b7b328996c1cf3fec92942",
            "title": "Prompting Large Language Models for Counterfactual Generation: An Empirical Study",
            "abstract": "Large language models (LLMs) have made remarkable progress in a wide range of natural language understanding and generation tasks. However, their ability to generate counterfactuals has not been examined systematically. To bridge this gap, we present a comprehensive evaluation framework on various types of NLU tasks, which covers all key factors in determining LLMs' capability of generating counterfactuals. Based on this framework, we 1) investigate the strengths and weaknesses of LLMs as the counterfactual generator, and 2) disclose the factors that affect LLMs when generating counterfactuals, including both the intrinsic properties of LLMs and prompt designing. The results show that, though LLMs are promising in most cases, they face challenges in complex tasks like RE since they are bounded by task-specific performance, entity constraints, and inherent selection bias. We also find that alignment techniques, e.g., instruction-tuning and reinforcement learning from human feedback, may potentially enhance the counterfactual generation ability of LLMs. On the contrary, simply increasing the parameter size does not yield the desired improvements. Besides, from the perspective of prompt designing, task guidelines unsurprisingly play an important role. However, the chain-of-thought approach does not always help due to inconsistency issues.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A comprehensive evaluation framework on various types of NLU tasks is presented, which covers all key factors in determining LLMs' capability of generating counterfactuals and finds that alignment techniques, e.g., instruction-tuning and reinforcement learning from human feedback, may potentially enhance the counterfactual generation ability of LLMs."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving the coherence and robustness of counterfactual responses generated by large language models, especially when dealing with uncertain or ambiguous contexts. The proposed approach is an uncertainty-aware counterfactual prompting method that explicitly models uncertainty by generating multiple possible counterfactual scenarios and their likelihoods.\n\nThe research problem in the paper is evaluating the ability of large language models to generate counterfactuals in various natural language understanding tasks. The approach is a comprehensive evaluation framework that investigates the factors affecting the counterfactual generation performance of LLMs, including their intrinsic properties and prompt design.\n\nWhile both the proposal and the paper focus on counterfactual generation using large language models, the proposal aims to improve the coherence and robustness of the generated responses by explicitly modeling uncertainty, whereas the paper focuses on evaluating the factors affecting the counterfactual generation performance of LLMs using a comprehensive evaluation framework. The proposal and the paper have different research problems and approaches.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "10632e0a667cbc3c52cc8f11a46d8e8e9c7739e3",
            "paperId": "10632e0a667cbc3c52cc8f11a46d8e8e9c7739e3",
            "title": "Causal Reasoning and Large Language Models: Opening a New Frontier for Causality",
            "abstract": "The causal capabilities of large language models (LLMs) is a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We further our understanding of LLMs and their causal implications, considering the distinctions between different types of causal reasoning tasks, as well as the entangled threats of construct and measurement validity. LLM-based methods establish new state-of-the-art accuracies on multiple causal benchmarks. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain), and actual causality (86% accuracy in determining necessary and sufficient causes in vignettes). At the same time, LLMs exhibit unpredictable failure modes and we provide some techniques to interpret their robustness. Crucially, LLMs perform these causal tasks while relying on sources of knowledge and methods distinct from and complementary to non-LLM based approaches. Specifically, LLMs bring capabilities so far understood to be restricted to humans, such as using collected knowledge to generate causal graphs or identifying background causal context from natural language. We envision LLMs to be used alongside existing causal methods, as a proxy for human domain knowledge and to reduce human effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods. We also see existing causal methods as promising tools for LLMs to formalize, validate, and communicate their reasoning especially in high-stakes scenarios. In capturing common sense and domain knowledge about causal mechanisms and supporting translation between natural language and formal methods, LLMs open new frontiers for advancing the research, practice, and adoption of causality.",
            "year": 2023,
            "citationCount": 96,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LLMs are envisioned to be used alongside existing causal methods, as a proxy for human domain knowledge and to reduce human effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve the coherence and robustness of counterfactual responses generated by large language models (LLMs) by introducing an uncertainty-aware counterfactual prompting approach. The paper explores the causal reasoning capabilities of LLMs and proposes using them alongside existing causal methods to reduce human effort in setting up causal analyses.\n\nThe project focuses on generating coherent and robust counterfactual responses by explicitly modeling uncertainty, while the paper investigates the causal reasoning abilities of LLMs and their potential to complement existing causal methods. Although both involve counterfactual reasoning and LLMs, the project emphasizes the generation of counterfactual responses, while the paper explores the broader causal reasoning capabilities of LLMs.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "0aa5940fda7c994675d08c41eca2a6909eb6d205",
            "paperId": "0aa5940fda7c994675d08c41eca2a6909eb6d205",
            "title": "Improving the Reliability of Large Language Models by Leveraging Uncertainty-Aware In-Context Learning",
            "abstract": "In recent years, large-scale language models (LLMs) have gained attention for their impressive text generation capabilities. However, these models often face the challenge of\"hallucination,\"which undermines their reliability. In this study, we introduce an uncertainty-aware in-context learning framework to empower the model to enhance or reject its output in response to uncertainty. Human-defined methods for estimating uncertainty typically assume that\"uncertainty is lower when the model's response is correct compared to when it is incorrect.\"However, setting a precise threshold to distinguish correctness is challenging. Therefore, we introduce uncertainty information as an intermediary variable that implicitly influences the model's behavior. Our innovative uncertainty-aware in-context learning framework involves fine-tuning the LLM using a calibration dataset. Our aim is to improve the model's responses by filtering out answers with high uncertainty while considering the model's knowledge limitations. We evaluate the model's knowledge by examining multiple responses to the same question for the presence of a correct answer. When the model lacks relevant knowledge, the response should indicate that the question cannot be answered. Conversely, when the model has relevant knowledge, the response should provide the correct answer. Extensive experiments confirm the effectiveness of our framework, leading to two key findings. First, the logit output values of the LLM partly reflect inherent uncertainty. Second, our model autonomously recognizes uncertainty, resulting in improved responses.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study introduces an uncertainty-aware in-context learning framework to empower the model to enhance or reject its output in response to uncertainty, and introduces uncertainty information as an intermediary variable that implicitly influences the model's behavior."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the coherence and robustness of counterfactual responses generated by large language models, especially when dealing with uncertain or ambiguous contexts. The proposed approach is an uncertainty-aware counterfactual prompting method that generates multiple possible counterfactual scenarios and their associated likelihoods based on the uncertainty in the context.\n\nThe research problem in the paper is improving the reliability of large language models by addressing the issue of \"hallucination.\" The proposed approach is an uncertainty-aware in-context learning framework that fine-tunes the model using a calibration dataset to enhance or reject its output in response to uncertainty.\n\nWhile both the proposal and the paper aim to improve the performance of large language models by considering uncertainty, their specific research problems and approaches differ. The proposal focuses on counterfactual reasoning and generating coherent responses, while the paper addresses the issue of hallucination and aims to improve the model's reliability by leveraging uncertainty information during in-context learning.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "12c826f4195da172b212a529f8fcf10cc79e35da",
            "paperId": "12c826f4195da172b212a529f8fcf10cc79e35da",
            "title": "Context-faithful Prompting for Large Language Models",
            "abstract": "Large language models (LLMs) encode parametric knowledge about world facts and have shown remarkable performance in knowledge-driven NLP tasks. However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks). In this paper, we seek to assess and enhance LLMs' contextual faithfulness in two aspects: knowledge conflict and prediction with abstention. We demonstrate that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies. In particular, we identify opinion-based prompts and counterfactual demonstrations as the most effective methods. Opinion-based prompts reframe the context as a narrator's statement and inquire about the narrator's opinions, while counterfactual demonstrations use instances containing false facts to improve faithfulness in knowledge conflict situations. Neither technique requires additional training. We conduct experiments on three datasets of two standard NLP tasks, machine reading comprehension and relation extraction, and the results demonstrate significant improvement in faithfulness to contexts. Code and data are released at https://github.com/wzhouad/context-faithful-llm.",
            "year": 2023,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies, and opinion-based prompts and counterfactual demonstrations are identified as the most effective methods."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the coherence and robustness of counterfactual responses generated by large language models, especially when dealing with uncertain or ambiguous contexts. The proposed approach is an uncertainty-aware counterfactual prompting method that explicitly models uncertainty and generates multiple possible counterfactual scenarios.\n\nThe research problem in the paper is assessing and enhancing large language models' contextual faithfulness in knowledge conflict and prediction with abstention situations. The proposed approach is using carefully designed prompting strategies, such as opinion-based prompts and counterfactual demonstrations, to improve faithfulness to contexts without additional training.\n\nWhile both the proposal and the paper involve counterfactual reasoning and prompting strategies for large language models, their research problems and approaches are different. The proposal focuses on generating coherent and robust counterfactual responses under uncertainty, while the paper aims to improve contextual faithfulness in knowledge conflict and abstention situations.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "d32764d479f338e0a1897cc3c35630f4ed0a39bf",
            "paperId": "d32764d479f338e0a1897cc3c35630f4ed0a39bf",
            "title": "SelectIT: Selective Instruction Tuning for Large Language Models via Uncertainty-Aware Self-Reflection",
            "abstract": "Instruction tuning (IT) is crucial to tailoring large language models (LLMs) towards human-centric interactions. Recent advancements have shown that the careful selection of a small, high-quality subset of IT data can significantly enhance the performance of LLMs. Despite this, common approaches often rely on additional models or data sets, which increases costs and limits widespread adoption. In this work, we propose a novel approach, termed SelectIT, that capitalizes on the foundational capabilities of the LLM itself. Specifically, we exploit the intrinsic uncertainty present in LLMs to more effectively select high-quality IT data, without the need for extra resources. Furthermore, we introduce a novel IT dataset, the Selective Alpaca, created by applying SelectIT to the Alpaca-GPT4 dataset. Empirical results demonstrate that IT using Selective Alpaca leads to substantial model ability enhancement. The robustness of SelectIT has also been corroborated in various foundation models and domain-specific tasks. Our findings suggest that longer and more computationally intensive IT data may serve as superior sources of IT, offering valuable insights for future research in this area. Data, code, and scripts are freely available at https://github.com/Blue-Raincoat/SelectIT.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work exploits the intrinsic uncertainty present in LLMs to more effectively select high-quality IT data, without the need for extra resources, and introduces a novel IT dataset, the Selective Alpaca, created by applying SelectIT to the Alpaca-GPT4 dataset."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the coherence and robustness of counterfactual responses generated by large language models, especially when dealing with uncertain or ambiguous contexts. The proposed approach is an uncertainty-aware counterfactual prompting method that generates multiple possible counterfactual scenarios and their associated likelihoods based on the uncertainty in the context.\n\nThe research problem in the paper is enhancing the performance of large language models through selective instruction tuning. The proposed approach is SelectIT, which exploits the intrinsic uncertainty present in LLMs to more effectively select high-quality instruction tuning data without the need for extra resources.\n\nThe proposal focuses on counterfactual reasoning and generating coherent responses under uncertainty, while the paper focuses on selective instruction tuning to improve model performance. The approaches are different, as the proposal uses uncertainty-aware prompting and the paper uses uncertainty-based data selection.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "9028fd54ecbbd58da6f3d86629b48bb95405fff2",
            "paperId": "9028fd54ecbbd58da6f3d86629b48bb95405fff2",
            "title": "Counterfactual reasoning: Testing language models\u2019 understanding of hypothetical scenarios",
            "abstract": "Current pre-trained language models have enabled remarkable improvements in downstream tasks, but it remains difficult to distinguish effects of statistical correlation from more systematic logical reasoning grounded on the understanding of real world. We tease these factors apart by leveraging counterfactual conditionals, which force language models to predict unusual consequences based on hypothetical propositions. We introduce a set of tests from psycholinguistic experiments, as well as larger-scale controlled datasets, to probe counterfactual predictions from five pre-trained language models. We find that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge\u2014however, we also find that for most models this effect appears largely to be driven by simple lexical cues. When we mitigate effects of both world knowledge and lexical cues to test knowledge of linguistic nuances of counterfactuals, we find that only GPT-3 shows sensitivity to these nuances, though this sensitivity is also non-trivially impacted by lexical associative factors.",
            "year": 2023,
            "citationCount": 12,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge\u2014however, it is also found that for most models this effect appears largely to be driven by simple lexical cues."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the coherence and robustness of counterfactual reasoning in large language models by explicitly modeling uncertainty. The approach is to use uncertainty-aware prompting to generate multiple counterfactual scenarios and aggregate them based on their likelihoods.\n\nThe research problem in the paper is testing language models' understanding of counterfactual scenarios and their ability to override real-world knowledge. The approach is to use psycholinguistic experiments and controlled datasets to probe counterfactual predictions from pre-trained language models.\n\nWhile both works involve counterfactual reasoning in language models, the proposal focuses on improving the generation of counterfactual responses by modeling uncertainty, while the paper focuses on testing the understanding of counterfactual scenarios and the ability to override real-world knowledge. The approaches are also different, with the proposal using uncertainty-aware prompting and the paper using psycholinguistic experiments and controlled datasets.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "f8e99be4f9a01761fab74bade2c3c18de9fc686b",
            "paperId": "f8e99be4f9a01761fab74bade2c3c18de9fc686b",
            "title": "Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks",
            "abstract": "The impressive performance of recent language models across a wide range of tasks suggests that they possess a degree of abstract reasoning skills. Are these skills general and transferable, or specialized to specific tasks seen during pretraining? To disentangle these effects, we propose an evaluation framework based on\"counterfactual\"task variants that deviate from the default assumptions underlying standard tasks. Across a suite of 11 tasks, we observe nontrivial performance on the counterfactual variants, but nevertheless find that performance substantially and consistently degrades compared to the default conditions. This suggests that while current LMs may possess abstract task-solving skills to an extent, they often also rely on narrow, non-transferable procedures for task-solving. These results motivate a more careful interpretation of language model performance that teases apart these aspects of behavior.",
            "year": 2023,
            "citationCount": 69,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An evaluation framework based on task variants that deviate from the default assumptions underlying standard tasks that suggests that while current LMs may possess abstract task-solving skills to an extent, they often also rely on narrow, non-transferable procedures for task-solving."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the coherence and robustness of counterfactual reasoning in large language models by explicitly modeling uncertainty. The approach is to use uncertainty-aware prompting to generate multiple counterfactual scenarios and aggregate them based on their likelihoods.\n\nThe research problem in the paper is evaluating the abstract reasoning skills of language models and whether they are general or task-specific. The approach is to use counterfactual task variants that deviate from default assumptions to disentangle the effects of general reasoning skills and narrow, task-specific procedures.\n\nWhile both works involve counterfactual reasoning and language models, the research problems and approaches are different. The proposal focuses on improving counterfactual reasoning by modeling uncertainty, while the paper focuses on evaluating the generality of reasoning skills using counterfactual tasks.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "0dc6e2476005d1aae30bede8ebbae92bdc831f6a",
            "paperId": "0dc6e2476005d1aae30bede8ebbae92bdc831f6a",
            "title": "Counterfactual reasoning: Do Language Models need world knowledge for causal inference?",
            "abstract": "Current pre-trained language models have enabled remarkable improvements in downstream tasks, but it remains difficult to distinguish effects of statistical correlation from more systematic logical reasoning grounded on understanding of the real world. In this paper we tease these factors apart by leveraging counterfactual conditionals , which force language models to predict unusual consequences based on hypothetical propositions. We introduce a set of tests drawn from psycholinguistic experiments, as well as larger-scale controlled datasets, to probe counterfactual predictions from a variety of popular pre-trained language models. We find that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge\u2014however, we also find that for most models this effect appears largely to be driven by simple lexical cues. When we mitigate effects of both world knowledge and lexical cues to test knowledge of linguistic nuances of counterfactuals, we find that only GPT-3 shows sensitivity to these nuances, though this sensitivity is also non-trivially impacted by lexical associative factors.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge\u2014however, it is also found that for most models this effect appears largely to be driven by simple lexical cues."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the coherence and robustness of counterfactual reasoning in large language models by explicitly modeling uncertainty. The approach is to use uncertainty-aware prompting to generate multiple counterfactual scenarios and aggregate them based on their likelihoods.\n\nThe research problem in the paper is evaluating the counterfactual reasoning abilities of pre-trained language models and understanding the factors that influence their performance. The approach is to use psycholinguistic tests and controlled datasets to probe the models' counterfactual predictions.\n\nWhile both works focus on counterfactual reasoning in language models, the proposal aims to improve the models' performance by introducing a new method, while the paper aims to evaluate and understand the models' existing capabilities. The proposal emphasizes coherence and robustness, while the paper focuses on teasing apart the effects of statistical correlation and logical reasoning.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "91a82593721c03ecffdef1c72ea55c6d87c42473",
            "paperId": "91a82593721c03ecffdef1c72ea55c6d87c42473",
            "title": "Counterfactual reasoning: Do language models need world knowledge for causal understanding?",
            "abstract": "Current pre-trained language models have enabled remarkable improvements in downstream tasks, but it remains difficult to distinguish effects of statistical correlation from more systematic logical reasoning grounded on understanding of the real world. In this paper we tease these factors apart by leveraging counterfactual conditionals, which force language models to predict unusual consequences based on hypothetical propositions. We introduce a set of tests drawn from psycholinguistic experiments, as well as larger-scale controlled datasets, to probe counterfactual predictions from a variety of popular pre-trained language models. We find that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge -- however, we also find that for most models this effect appears largely to be driven by simple lexical cues. When we mitigate effects of both world knowledge and lexical cues to test knowledge of linguistic nuances of counterfactuals, we find that only GPT-3 shows sensitivity to these nuances, though this sensitivity is also non-trivially impacted by lexical associative factors.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that models are consistently able to override real-world knowledge in counterfactual scenarios, and that this effect is more robust in case of stronger baseline world knowledge -- however, it is also found that for most models this effect appears largely to be driven by simple lexical cues."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving the coherence and robustness of counterfactual reasoning in large language models by explicitly modeling uncertainty. The approach is to prompt the model to generate multiple counterfactual scenarios with associated likelihoods and aggregate the responses based on the likelihoods.\n\nThe research problem in the paper is probing the counterfactual reasoning abilities of pre-trained language models and understanding the factors that influence their predictions. The approach is to use psycholinguistic tests and controlled datasets to tease apart the effects of statistical correlation, world knowledge, and linguistic nuances on counterfactual predictions.\n\nWhile both the proposal and the paper focus on counterfactual reasoning in language models, the research problems and approaches are different. The proposal aims to improve counterfactual reasoning by modeling uncertainty, while the paper aims to probe and understand the factors influencing counterfactual predictions in existing models.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "eebdf7303256f081ab1f6a36ff0ea6126e4da484",
            "paperId": "eebdf7303256f081ab1f6a36ff0ea6126e4da484",
            "title": "CRASS: A Novel Data Set and Benchmark to Test Counterfactual Reasoning of Large Language Models",
            "abstract": "We introduce the CRASS (counterfactual reasoning assessment) data set and benchmark utilizing questionized counterfactual conditionals as a novel and powerful tool to evaluate large language models. We present the data set design and benchmark. We test six state-of-the-art models against our benchmark. Our results show that it poses a valid challenge for these models and opens up considerable room for their improvement.",
            "year": 2021,
            "citationCount": 14,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The CRASS data set and benchmark utilizing questionized counterfactual conditionals is introduced as a novel and powerful tool to evaluate large language models and poses a valid challenge for these models and opens up considerable room for their improvement."
            },
            "score": 6
        },
        {
            "id": "b69ae70abf1e8519a565c40bed07ab499576a8f6",
            "paperId": "b69ae70abf1e8519a565c40bed07ab499576a8f6",
            "title": "Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models",
            "abstract": "Large language models (LLMs) have performed well on several reasoning benchmarks, including ones that test analogical reasoning abilities. However, it has been debated whether they are actually performing humanlike abstract reasoning or instead employing less general processes that rely on similarity to what has been seen in their training data. Here we investigate the generality of analogy-making abilities previously claimed for LLMs (Webb, Holyoak,&Lu, 2023). We take one set of analogy problems used to evaluate LLMs and create a set of\"counterfactual\"variants-versions that test the same abstract reasoning abilities but that are likely dissimilar from any pre-training data. We test humans and three GPT models on both the original and counterfactual problems, and show that, while the performance of humans remains high for all the problems, the GPT models' performance declines sharply on the counterfactual set. This work provides evidence that, despite previously reported successes of LLMs on analogical reasoning, these models lack the robustness and generality of human analogy-making.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work takes one set of analogy problems used to evaluate LLMs and creates a set of \"counterfactual\"variants-versions that test the same abstract reasoning abilities but that are likely dissimilar from any pre-training data, providing evidence that these models lack the robustness and generality of human analogy-making."
            },
            "score": 6
        },
        {
            "id": "e67dd9230cd4a8199ca6c9c9ebaa31ab29be23d2",
            "paperId": "e67dd9230cd4a8199ca6c9c9ebaa31ab29be23d2",
            "title": "Evidence from counterfactual tasks supports emergent analogical reasoning in large language models",
            "abstract": "We recently reported evidence that large language models are capable of solving a wide range of text-based analogy problems in a zero-shot manner, indicating the presence of an emergent capacity for analogical reasoning. Two recent commentaries have challenged these results, citing evidence from so-called `counterfactual' tasks in which the standard sequence of the alphabet is arbitrarily permuted so as to decrease similarity with materials that may have been present in the language model's training data. Here, we reply to these critiques, clarifying some misunderstandings about the test materials used in our original work, and presenting evidence that language models are also capable of generalizing to these new counterfactual task variants.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 6
        },
        {
            "id": "c6e4518dfd687a2a5bed4e78d5d9f999292a1746",
            "paperId": "c6e4518dfd687a2a5bed4e78d5d9f999292a1746",
            "title": "Counterfactual Recipe Generation: Exploring Compositional Generalization in a Realistic Scenario",
            "abstract": "People can acquire knowledge in an unsupervised manner by reading, and compose the knowledge to make novel combinations. In this paper, we investigate whether pretrained language models can perform compositional generalization in a realistic setting: recipe generation. We design the counterfactual recipe generation task, which asks models to modify a base recipe according to the change of an ingredient. This task requires compositional generalization at two levels: the surface level of incorporating the new ingredient into the base recipe, and the deeper level of adjusting actions related to the changing ingredient. We collect a large-scale recipe dataset in Chinese for models to learn culinary knowledge, and a subset of action-level fine-grained annotations for evaluation.We finetune pretrained language models on the recipe corpus, and use unsupervised counterfactual generation methods to generate modified recipes.Results show that existing models have difficulties in modifying the ingredients while preserving the original text style, and often miss actions that need to be adjusted. Although pretrained language models can generate fluent recipe texts, they fail to truly learn and use the culinary knowledge in a compositional way. Code and data are available at https://github.com/xxxiaol/counterfactual-recipe-generation.",
            "year": 2022,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper designs the counterfactual recipe generation task, which asks models to modify a base recipe according to the change of an ingredient, and finetune pretrained language models on the recipe corpus, and uses unsupervised counterfactUAL generation methods to generate modified recipes."
            },
            "score": 6
        },
        {
            "id": "ce913026f693101e54d3ab9152e107034d81fce1",
            "paperId": "ce913026f693101e54d3ab9152e107034d81fce1",
            "title": "Holistic Evaluation of Language Models",
            "abstract": "Language models (LMs) like GPT\u20103, PaLM, and ChatGPT are the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of LMs. LMs can serve many purposes and their behavior should satisfy many desiderata. To navigate the vast space of potential scenarios and metrics, we taxonomize the space and select representative subsets. We evaluate models on 16 core scenarios and 7 metrics, exposing important trade\u2010offs. We supplement our core evaluation with seven targeted evaluations to deeply analyze specific aspects (including world knowledge, reasoning, regurgitation of copyrighted content, and generation of disinformation). We benchmark 30 LMs, from OpenAI, Microsoft, Google, Meta, Cohere, AI21 Labs, and others. Prior to HELM, models were evaluated on just 17.9% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0%: all 30 models are now benchmarked under the same standardized conditions. Our evaluation surfaces 25 top\u2010level findings. For full transparency, we release all raw model prompts and completions publicly. HELM is a living benchmark for the community, continuously updated with new scenarios, metrics, and models https://crfm.stanford.edu/helm/latest/.",
            "year": 2023,
            "citationCount": 485,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "HELM is a living benchmark for the community, continuously updated with new scenarios, metrics, and models https://crfm.stanford.edu/helm/latest/."
            },
            "score": 6
        },
        {
            "id": "dab4f70d75a04e62553e583f2450d9bb1f0ead46",
            "paperId": "dab4f70d75a04e62553e583f2450d9bb1f0ead46",
            "title": "CLOMO: Counterfactual Logical Modification with Large Language Models",
            "abstract": "In this study, we delve into the realm of counterfactual reasoning capabilities of large language models (LLMs). Our primary objective is to cultivate the counterfactual thought processes within LLMs and rigorously assess these processes for their validity. Specifically, we introduce a novel task, Counterfactual Logical Modification (CLOMO), and a high-quality human-annotated benchmark. In this task, LLMs must adeptly alter a given argumentative text to uphold a predetermined logical relationship. To effectively evaluate a generation model's counterfactual capabilities, we propose an innovative evaluation metric, the LogicAware Counterfactual Score to directly evaluate the natural language output of LLMs instead of modeling the task as a multiple-choice problem. Analysis shows that the proposed automatic metric aligns well with human preference. Our experimental results show that while LLMs demonstrate a notable capacity for logical counterfactual thinking, there remains a discernible gap between their current abilities and human performance.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study introduces a novel task, Counterfactual Logical Modification (CLOMO), and proposes an innovative evaluation metric, the LogicAware CounterfactUAL Score, to directly evaluate the natural language output of LLMs instead of modeling the task as a multiple-choice problem."
            },
            "score": 6
        },
        {
            "id": "d7386e8859b22e05ce9c4a972613d4b1e1e44198",
            "paperId": "d7386e8859b22e05ce9c4a972613d4b1e1e44198",
            "title": "Prompting Large Language Models With the Socratic Method",
            "abstract": "This paper presents a systematic approach to using the Socratic method in developing prompt templates that effectively interact with large language models, including GPT-3. Various methods are examined, and those that yield precise answers and justifications while fostering creativity and imagination to enhance creative writing are identified. Techniques such as definition, elenchus, dialectic, maieutics, generalization, and counterfactual reasoning are discussed for their application in engineering prompt templates and their connections to inductive, deductive, and abductive reasoning. Through examples, the effectiveness of these dialogue and reasoning methods is demonstrated. An interesting observation is made that when the task's goal and user intent are conveyed to GPT-3 via ChatGPT before the start of a dialogue, the large language model seems to connect to the external context expressed in the intent and perform more effectively.",
            "year": 2023,
            "citationCount": 20,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "An interesting observation is made that when the task's goal and user intent are conveyed to GPT-3 via ChatGPT before the start of a dialogue, the large language model seems to connect to the external context expressed in the intent and perform more effectively."
            },
            "score": 5
        },
        {
            "id": "fa8b5cb63b67a348599632f5c007f6c4d520a12d",
            "paperId": "fa8b5cb63b67a348599632f5c007f6c4d520a12d",
            "title": "A C T UNE : Uncertainty-Aware Active Self-Training for Active Fine-Tuning of Pretrained Language Models",
            "abstract": "Although \ufb01ne-tuning pre-trained language 001 models (PLMs) renders strong performance in 002 many NLP tasks, it relies on excessive labeled 003 data. Recently, researchers have resorted to 004 active \ufb01ne-tuning for enhancing the label ef\ufb01-005 ciency of PLM \ufb01ne-tuning, but existing meth-006 ods of this type usually ignore the potential of 007 unlabeled data. We develop A C T UNE , a new 008 framework that improves the label ef\ufb01ciency 009 of active PLM \ufb01ne-tuning by unleashing the 010 power of unlabeled data via self training. A C - 011 T UNE switches between data annotation and 012 model self-training based on uncertainty: the 013 unlabeled samples of high-uncertainty are se-014 lected for annotation, while the ones from low-015 uncertainty regions are used for model self-016 training. Additionally, we design (1) a region-017 aware sampling strategy to avoid redundant 018 samples when querying annotations and (2) 019 a momentum-based memory bank to dynam-020 ically aggregate the model\u2019s pseudo labels to 021 suppress label noise in self-training. Exper-022 iments on 6 text classi\ufb01cation datasets show 023 that A C T UNE outperforms the strongest active 024 learning and self-training baselines and im-025 proves the label ef\ufb01ciency of PLM \ufb01ne-tuning 026 by 56.2% on average. 027",
            "year": 2022,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A C T UNE is developed, a new 008 framework that improves the label ef\ufb01ciency 009 of active PLM \ufb01ne-tuning by unleashing the 010 power of unlabeled data via self training by unleashing the 010 power of unlabeled data via self training."
            },
            "score": 5
        },
        {
            "id": "449257147fe40e0016f3ef89a62f20db8ff29039",
            "paperId": "449257147fe40e0016f3ef89a62f20db8ff29039",
            "title": "Pattern-Aware Chain-of-Thought Prompting in Large Language Models",
            "abstract": "Chain-of-thought (CoT) prompting can guide language models to engage in complex multi-step reasoning. The quality of provided demonstrations significantly impacts the success of downstream inference tasks. While existing automated methods prioritize accuracy and semantics in these demonstrations, we show that the underlying reasoning patterns play a more crucial role in such tasks. In this paper, we propose Pattern-Aware CoT, a prompting method that considers the diversity of demonstration patterns. By incorporating patterns such as step length and reasoning process within intermediate steps, PA-CoT effectively mitigates the issue of bias induced by demonstrations and enables better generalization to diverse scenarios. We conduct experiments on nine reasoning benchmark tasks using two open-source LLMs. The results show that our method substantially enhances reasoning performance and exhibits robustness to errors. The code will be made publicly available.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "By incorporating patterns such as step length and reasoning process within intermediate steps, PA-CoT effectively mitigates the issue of bias induced by demonstrations and enables better generalization to diverse scenarios."
            },
            "score": 5
        },
        {
            "id": "d4695ae2a7eef298ba1dce5cc79eb3dbff1ba0e1",
            "paperId": "d4695ae2a7eef298ba1dce5cc79eb3dbff1ba0e1",
            "title": "AcTune: Uncertainty-aware Active Self-Training for Semi-Supervised Active Learning with Pretrained Language Models",
            "abstract": "While pre-trained language model (PLM) fine-tuning has achieved strong performance in many NLP tasks, the fine-tuning stage can be still demanding in labeled data. Recent works have resorted to active fine-tuning to improve the label efficiency of PLM fine-tuning, but none of them investigate the potential of unlabeled data. We propose {\\ours}, a new framework that leverages unlabeled data to improve the label efficiency of active PLM fine-tuning. AcTune switches between data annotation and model self-training based on uncertainty: it selects high-uncertainty unlabeled samples for active annotation and low-uncertainty ones for model self-training. Under this framework, we design (1) a region-aware sampling strategy that reduces redundancy when actively querying for annotations and (2) a momentum-based memory bank that dynamically aggregates the model's pseudo labels to suppress label noise in self-training. Experiments on 6 text classification datasets show that AcTune outperforms the strongest active learning and self-training baselines and improves the label efficiency of PLM fine-tuning by 56.2\\% on average. Our implementation will be available at \\url{https://github.com/yueyu1030/actune}.",
            "year": 2021,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experiments show that AcTune outperforms the strongest active learning and self-training baselines and improves the label efficiency of PLM fine-tuning by 56.2\\% on average."
            },
            "score": 5
        },
        {
            "id": "20ae101289965d36dbd93e9b8c47ec9deab03ed0",
            "paperId": "20ae101289965d36dbd93e9b8c47ec9deab03ed0",
            "title": "What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models",
            "abstract": "Counterfactual reasoning ability is one of the core abilities of human intelligence. This reasoning process involves the processing of alternatives to observed states or past events, and this process can improve our ability for planning and decision-making. In this work, we focus on benchmarking the counterfactual reasoning ability of multimodal large language models. We take the question and answer pairs from the VQAv2 dataset and add one counterfactual presupposition to the questions, with the answer being modified accordingly. After generating counterfactual questions and answers using ChatGPT, we manually examine all generated questions and answers to ensure correctness. This results in over 2k counterfactual question and answer pairs. We evaluate recent vision language models on our newly collected test dataset and found that all models exhibit a large performance drop compared to the results tested on questions without counterfactual presupposition. This result indicates that there still exists space for developing vision language models. We hope our proposed benchmark can help the development of future systems.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work benchmarking the counterfactual reasoning ability of multimodal large language models found that all models exhibit a large performance drop compared to the results tested on questions without counterfactual presupposition, indicating that there still exists space for developing vision language models."
            },
            "score": 5
        },
        {
            "id": "498d1406fc4cddb05cd46477793f2e726a6fe238",
            "paperId": "498d1406fc4cddb05cd46477793f2e726a6fe238",
            "title": "The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code",
            "abstract": "Causal reasoning, the ability to identify cause-and-effect relationship, is crucial in human thinking. Although large language models (LLMs) succeed in many NLP tasks, it is still challenging for them to conduct complex causal reasoning like abductive reasoning and counterfactual reasoning. Given the fact that programming code may express causal relations more often and explicitly with conditional statements like ``if``, we want to explore whether Code-LLMs acquire better causal reasoning abilities. Our experiments show that compared to text-only LLMs, Code-LLMs with code prompts are significantly better in causal reasoning. We further intervene on the prompts from different aspects, and discover that the programming structure is crucial in code prompt design, while Code-LLMs are robust towards format perturbations.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The experiments show that compared to text-only LLMs, Code-LLMs with code prompts are significantly better in causal reasoning, and that the programming structure is crucial in code prompt design, while Code- LLMs are robust towards format perturbations."
            },
            "score": 5
        },
        {
            "id": "415c594667bb94f086ea980f897c4f62f9a22d18",
            "paperId": "415c594667bb94f086ea980f897c4f62f9a22d18",
            "title": "Eyes Can Deceive: Benchmarking Counterfactual Reasoning Abilities of Multi-modal Large Language Models",
            "abstract": "Counterfactual reasoning, as a crucial manifestation of human intelligence, refers to making presuppositions based on established facts and extrapolating potential outcomes. Existing multimodal large language models (MLLMs) have exhibited impressive cognitive and reasoning capabilities, which have been examined across a wide range of Visual Question Answering (VQA) benchmarks. Nevertheless, how will existing MLLMs perform when faced with counterfactual questions? To answer this question, we first curate a novel \\textbf{C}ounter\\textbf{F}actual \\textbf{M}ulti\\textbf{M}odal reasoning benchmark, abbreviated as \\textbf{CFMM}, to systematically assess the counterfactual reasoning capabilities of MLLMs. Our CFMM comprises six challenging tasks, each including hundreds of carefully human-labeled counterfactual questions, to evaluate MLLM's counterfactual reasoning capabilities across diverse aspects. Through experiments, interestingly, we find that existing MLLMs prefer to believe what they see, but ignore the counterfactual presuppositions presented in the question, thereby leading to inaccurate responses. Furthermore, we evaluate a wide range of prevalent MLLMs on our proposed CFMM. The significant gap between their performance on our CFMM and that on several VQA benchmarks indicates that there is still considerable room for improvement in existing MLLMs toward approaching human-level intelligence. On the other hand, through boosting MLLMs performances on our CFMM in the future, potential avenues toward developing MLLMs with advanced intelligence can be explored.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that existing MLLMs prefer to believe what they see, but ignore the counterfactual presuppositions presented in the question, thereby leading to inaccurate responses on the proposed CFMM."
            },
            "score": 5
        },
        {
            "id": "6ac627f57b26354ab537734d820da4a6a7dde2c6",
            "paperId": "6ac627f57b26354ab537734d820da4a6a7dde2c6",
            "title": "CLadder: Assessing Causal Reasoning in Language Models",
            "abstract": "The ability to perform causal reasoning is widely considered a core feature of intelligence. In this work, we investigate whether large language models (LLMs) can coherently reason about causality. Much of the existing work in natural language processing (NLP) focuses on evaluating commonsense causal reasoning in LLMs, thus failing to assess whether a model can perform causal inference in accordance with a set of well-defined formal rules. To address this, we propose a new NLP task, causal inference in natural language, inspired by the\"causal inference engine\"postulated by Judea Pearl et al. We compose a large dataset, CLadder, with 10K samples: based on a collection of causal graphs and queries (associational, interventional, and counterfactual), we obtain symbolic questions and ground-truth answers, through an oracle causal inference engine. These are then translated into natural language. We evaluate multiple LLMs on our dataset, and we introduce and evaluate a bespoke chain-of-thought prompting strategy, CausalCoT. We show that our task is highly challenging for LLMs, and we conduct an in-depth analysis to gain deeper insights into the causal reasoning abilities of LLMs. Our data is open-sourced at https://huggingface.co/datasets/causalNLP/cladder, and our code can be found at https://github.com/causalNLP/cladder.",
            "year": 2023,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates whether large language models (LLMs) can coherently reason about causality, and proposes a new NLP task, causal inference in natural language, inspired by the \"causal inference engine\" proposed by Judea Pearl et al."
            },
            "score": 5
        },
        {
            "id": "28e2ecb4183ebc0eec504b12dddc677f8aef8745",
            "paperId": "28e2ecb4183ebc0eec504b12dddc677f8aef8745",
            "title": "Benchmarking Large Language Models in Retrieval-Augmented Generation",
            "abstract": "Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.",
            "year": 2023,
            "citationCount": 51,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information, indicating that there is still a considerable journey ahead to effectively apply RAG to LLMs."
            },
            "score": 5
        },
        {
            "id": "7c4be464e68a11c8f254d9608f31280e9bcda85c",
            "paperId": "7c4be464e68a11c8f254d9608f31280e9bcda85c",
            "title": "CPL: Counterfactual Prompt Learning for Vision and Language Models",
            "abstract": "Prompt tuning is a new few-shot transfer learning technique that only tunes the learnable prompt for pre-trained vision and language models such as CLIP. However, existing prompt tuning methods tend to learn spurious or entangled representations, which leads to poor generalization to unseen concepts.Towards non-spurious and efficient prompt learning from limited examples, this paper presents a novel Counterfactual Prompt Learning (CPL) method for vision and language models, which simultaneously employs counterfactual generation and contrastive learning in a joint optimization framework.Particularly, CPL constructs counterfactual by identifying minimal non-spurious feature change between semantically-similar positive and negative samples that causes concept change, and learns more generalizable prompt representation from both factual and counterfactual examples via contrastive learning. Extensive experiments demonstrate that CPL can obtain superior few-shot performance on different vision and language tasks than previous prompt tuning methods on CLIP. On image classification, we achieve 3.55% average relative improvement on unseen classes across seven datasets; on image-text retrieval and visual question answering, we gain up to 4.09% and 25.08% relative improvements across three few-shot scenarios on unseen test sets respectively.",
            "year": 2022,
            "citationCount": 10,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel Counterfactual Prompt Learning method for vision and language models, which simultaneously employs counterfactual generation and contrastive learning in a joint optimization framework, and can obtain superior few-shot performance on different vision andlanguage tasks than previous prompt tuning methods on CLIP."
            },
            "score": 5
        },
        {
            "id": "eaeed0144d6a8000638c884f38d2d1e046aaa400",
            "paperId": "eaeed0144d6a8000638c884f38d2d1e046aaa400",
            "title": "Investigating the Robustness of Natural Language Generation from Logical Forms via Counterfactual Samples",
            "abstract": "The aim of Logic2Text is to generate controllable and faithful texts conditioned on tables and logical forms, which not only requires a deep understanding of the tables and logical forms, but also warrants symbolic reasoning over the tables according to the logical forms. State-of-the-art methods based on pre-trained models have achieved remarkable performance on the standard test dataset. However, we question whether these methods really learn how to perform logical reasoning, rather than just relying on the spurious correlations between the headers of the tables and operators of the logical form. To verify this hypothesis, we manually construct a set of counterfactual samples, which modify the original logical forms to generate counterfactual logical forms with rare co-occurred headers and operators and corresponding counterfactual references. SOTA methods give much worse results on these counterfactual samples compared with the results on the original test dataset, which verifies our hypothesis. To deal with this problem, we firstly analyze this bias from a causal perspective, based on which we propose two approaches to reduce the model\u2019s reliance on the shortcut. The first one incorporates the hierarchical structure of the logical forms into the model. The second one exploits automatically generated counterfactual data for training. Automatic and manual experimental results on the original test dataset and counterfactual dataset show that our method is effective to alleviate the spurious correlation. Our work points out the weakness of current methods and takes a further step toward developing Logic2Text models with real logical reasoning ability.",
            "year": 2022,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work points out the weakness of current methods and takes a further step toward developing Logic2Text models with real logical reasoning ability, by proposing two approaches to reduce the model\u2019s reliance on the shortcut."
            },
            "score": 5
        },
        {
            "id": "6dc5e5726131e4b4cb214503d5d996b40859beac",
            "paperId": "6dc5e5726131e4b4cb214503d5d996b40859beac",
            "title": "Aligning Large Language Models with Counterfactual DPO",
            "abstract": "Advancements in large language models (LLMs) have demonstrated remarkable capabilities across a diverse range of applications. These models excel in generating text completions that are contextually coherent and cover an extensive array of subjects. However, the vast datasets required for their training make aligning response styles during the pretraining and instruction tuning phases challenging. Consequently, an additional alignment phase is typically employed, wherein the model is further trained with human preference data to better align its outputs with human expectations. While this process doesn't introduce new capabilities per se, it does accentuate generation styles innate to the model. This paper explores the utilization of counterfactual prompting within the framework of Direct Preference Optimization (DPO) to align the model's style without relying on human intervention. We demonstrate that this method effectively instils desirable behaviour, mitigates undesirable ones, and encourages the model to disregard inappropriate instructions. Our findings suggest that counterfactual prompting with DPO presents a low-resource way to fine-tune LLMs to meet the demands for responsible and ethically aligned AI systems.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper explores the utilization of counterfactual prompting within the framework of Direct Preference Optimization (DPO) to align the model's style without relying on human intervention, and demonstrates that this method effectively instils desirable behaviour, mitigates undesirable ones, and encourages the model to disregard inappropriate instructions."
            },
            "score": 5
        },
        {
            "id": "761ed225b59e34d2f5b0848beed5b1842af8cf8b",
            "paperId": "761ed225b59e34d2f5b0848beed5b1842af8cf8b",
            "title": "RT-LM: Uncertainty-Aware Resource Management for Real-Time Inference of Language Models",
            "abstract": "Recent advancements in language models (LMs) have gained substantial attentions on their capability to generate human-like responses. Though exhibiting a promising future for various applications such as conversation AI, these LMs face deployment challenges on various devices due to their extreme computational cost and unpredictable inference latency. Such varied inference latency, identified as a consequence of uncertainty intrinsic to the nature of language, can lead to computational inefficiency and degrade the overall performance of LMs, especially under high-traffic workloads. Unfortunately, the bandwidth of these uncertainty sources is extensive, complicating the prediction of latency and the effects emanating from such uncertainties. To understand and mitigate the impact of uncertainty on real-time response-demanding systems, we take the first step to comprehend, quantify and optimize these uncertainty-induced latency performance variations in LMs. Specifically, we present RT-LM, an uncertainty-aware resource management ecosystem for real-time inference of LMs. RT-LM innovatively quantifies how specific input uncertainties, recognized within the NLP community, adversely affect latency, often leading to an increased output length. Exploiting these insights, we devise a lightweight yet effective method to dynamically correlate input text uncertainties with output length at runtime. Utilizing this quantification as a latency heuristic, we integrate the uncertainty information into a system-level scheduler which explores several uncertainty-induced optimization opportunities, including uncertainty-aware prioritization, dynamic consolidation, and strategic CPU offloading. Quantitative experiments across five state-of-the-art LMs on two hardware platforms demonstrates that RT-LM can significantly reduce the average response time and improve throughput while incurring a rather small runtime overhead.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "RT-LM innovatively quantifies how specific input uncertainties, recognized within the NLP community, adversely affect latency, often leading to an increased output length at runtime, and integrates the uncertainty information into a system-level scheduler which explores several uncertainty-induced optimization opportunities."
            },
            "score": 4
        },
        {
            "id": "5e7274bcda47b704b6797bb14be8b7a61c047a61",
            "paperId": "5e7274bcda47b704b6797bb14be8b7a61c047a61",
            "title": "Uncertainty-Aware Evaluation for Vision-Language Models",
            "abstract": "Vision-Language Models like GPT-4, LLaVA, and CogVLM have surged in popularity recently due to their impressive performance in several vision-language tasks. Current evaluation methods, however, overlook an essential component: uncertainty, which is crucial for a comprehensive assessment of VLMs. Addressing this oversight, we present a benchmark incorporating uncertainty quantification into evaluating VLMs. Our analysis spans 20+ VLMs, focusing on the multiple-choice Visual Question Answering (VQA) task. We examine models on 5 datasets that evaluate various vision-language capabilities. Using conformal prediction as an uncertainty estimation approach, we demonstrate that the models' uncertainty is not aligned with their accuracy. Specifically, we show that models with the highest accuracy may also have the highest uncertainty, which confirms the importance of measuring it for VLMs. Our empirical findings also reveal a correlation between model uncertainty and its language model part.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that models with the highest accuracy may also have the highest uncertainty, which confirms the importance of measuring it for VLMs, and a correlation between model uncertainty and its language model part is revealed."
            },
            "score": 4
        },
        {
            "id": "96328033cd5fba1973c81fefc69a4f9f956985d2",
            "paperId": "96328033cd5fba1973c81fefc69a4f9f956985d2",
            "title": "LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision & Language Models",
            "abstract": "Soft prompt learning has recently emerged as one of the methods of choice for adapting V&L models to a downstream task using a few training examples. However, current methods significantly overfit the training data, suffering from large accuracy degradation when tested on unseen classes from the same domain. To this end, in this paper, we make the following 4 contributions: (1) To alleviate base class overfitting, we propose a novel Language- Aware Soft Prompting (LASP) learning method by means of a text-to-text cross-entropy loss that maximizes the probability of the learned prompts to be correctly classified with respect to pre-defined hand-crafted textual prompts. (2) To increase the representation capacity of the prompts, we propose grouped LASP where each group of prompts is optimized with respect to a separate subset of textual prompts. (3) We identify a visual-language misalignment introduced by prompt learning and LASP, and more importantly, propose a re-calibration mechanism to address it. (4) We show that LASP is inherently amenable to including, during training, virtual classes, i.e. class names for which no visual samples are available, further increasing the robustness of the learned prompts. Through evaluations on 11 datasets, we show that our approach (a) significantly outperforms all prior works on soft prompting, and (b) matches and surpasses, for the first time, the accuracy on novel classes obtained by hand-crafted prompts and CLIP for 8 out of 11 test datasets. Code will be made available here.",
            "year": 2022,
            "citationCount": 15,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes a novel Language- Aware Soft Prompting (LASP) learning method by means of a text-to-text cross-entropy loss that maximizes the probability of the learned prompts to be correctly classified with respect to pre-defined hand-crafted textual prompts and shows that LASP is inherently amenable to including, during training, virtual classes."
            },
            "score": 4
        },
        {
            "id": "7dc93d32613e8277eca1fdd8f414703f8969c132",
            "paperId": "7dc93d32613e8277eca1fdd8f414703f8969c132",
            "title": "Model Uncertainty-Aware Knowledge Amalgamation for Pre-Trained Language Models",
            "abstract": "As many fine-tuned pre-trained language models~(PLMs) with promising performance are generously released, investigating better ways to reuse these models is vital as it can greatly reduce the retraining computational cost and the potential environmental side-effects. In this paper, we explore a novel model reuse paradigm, Knowledge Amalgamation~(KA) for PLMs. Without human annotations available, KA aims to merge the knowledge from different teacher-PLMs, each of which specializes in a different classification problem, into a versatile student model. The achieve this, we design a Model Uncertainty--aware Knowledge Amalgamation~(MUKA) framework, which identifies the potential adequate teacher using Monte-Carlo Dropout for approximating the golden supervision to guide the student. Experimental results demonstrate that MUKA achieves substantial improvements over baselines on benchmark datasets. Further analysis shows that MUKA can generalize well under several complicate settings with multiple teacher models, heterogeneous teachers, and even cross-dataset teachers.",
            "year": 2021,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel model reuse paradigm, Knowledge Amalgamation~(KA) for PLMs, which aims to merge the knowledge from different teacher-PLMs, each of which specializes in a different classification problem, into a versatile student model."
            },
            "score": 4
        },
        {
            "id": "614d51530e8d75e5a916778fe0b513aa53721daf",
            "paperId": "614d51530e8d75e5a916778fe0b513aa53721daf",
            "title": "Enhancing Large Language Models Against Inductive Instructions with Dual-critique Prompting",
            "abstract": "Numerous works are proposed to align large language models (LLMs) with human intents to better fulfill instructions, ensuring they are trustful and helpful. Nevertheless, some human instructions are often malicious or misleading and following them will lead to untruthful and unsafe responses. Previous work rarely focused on understanding how LLMs manage instructions based on counterfactual premises, referred to here as \\textit{inductive instructions}, which may stem from users' false beliefs or malicious intents. In this paper, we aim to reveal the behaviors of LLMs towards \\textit{inductive instructions} and enhance their truthfulness and helpfulness accordingly. Specifically, we first introduce a benchmark of \\underline{\\textbf{Indu}}ctive {In\\underline{\\textbf{st}}ruct}ions (\\textsc{\\textbf{INDust}}), where the false knowledge is incorporated into instructions in multiple different styles. After extensive human and automatic evaluations, we uncovered a universal vulnerability among LLMs in processing inductive instructions. Additionally, we identified that different inductive styles affect the models' ability to identify the same underlying errors, and the complexity of the underlying assumptions also influences the model's performance. Motivated by these results, we propose \\textsc{Dual-critique} prompting to improve LLM robustness against inductive instructions. Our experiments demonstrate that \\textsc{Dual-critique} prompting significantly bolsters the robustness of a diverse array of LLMs, even when confronted with varying degrees of inductive instruction complexity and differing inductive styles.",
            "year": 2023,
            "citationCount": 9,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that Dual-critique prompting significantly bolsters the robustness of a diverse array of LLMs, even when confronted with varying degrees of inductive instruction complexity and differing inductive styles."
            },
            "score": 4
        },
        {
            "id": "896ca0a68e4d33d76a7366bcab85eb7d2605a8c4",
            "paperId": "896ca0a68e4d33d76a7366bcab85eb7d2605a8c4",
            "title": "Metacognitive Prompting Improves Understanding in Large Language Models",
            "abstract": "In Large Language Models (LLMs), there have been consistent advancements in task-specific performance, largely influenced by effective prompt design. Recent advancements in prompting have enhanced reasoning in logic-intensive tasks for LLMs, yet the nuanced understanding abilities of these models, crucial for processing and interpreting complex information, remain underexplored. In this study, we introduce Metacognitive Prompting (MP), a strategy inspired by human introspective reasoning processes. Using MP, LLMs undergo a systematic series of structured, self-aware evaluations, drawing on both their vast inherent knowledge and new insights. We conduct extensive experiments on four prevalent LLMs: Llama2, PaLM2, GPT-3.5, and GPT-4, across ten natural language understanding (NLU) datasets from GLUE, SuperGLUE, BLUE, and LexGLUE benchmarks. Additionally, we compare our method with chain-of-thought prompting and its advanced versions. The results show that GPT-4 consistently excels across all tasks, while other models have shown significant progress in some tasks when used in conjunction with MP. Furthermore, MP consistently outperforms existing prompting methods in both general and domain-specific NLU tasks. This study underscores the potential to amplify the understanding abilities of LLMs and highlights the benefits of mirroring human introspective reasoning in NLU tasks.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study introduces Metacognitive Prompting (MP), a strategy inspired by human introspective reasoning processes that consistently outperforms existing prompting methods in both general and domain-specific NLU tasks."
            },
            "score": 4
        },
        {
            "id": "162e2e9ac70702c146c0aa8432e4a6806bb8c42e",
            "paperId": "162e2e9ac70702c146c0aa8432e4a6806bb8c42e",
            "title": "Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text",
            "abstract": "While large language models (LLMs), such as GPT-3, appear to be robust and general, their reasoning ability is not at a level to compete with the best models trained for specific natural language reasoning problems. In this study, we observe that a large language model can serve as a highly effective few-shot semantic parser. It can convert natural language sentences into a logical form that serves as input for answer set programs, a logic-based declarative knowledge representation formalism. The combination results in a robust and general system that can handle multiple question-answering tasks without requiring retraining for each new task. It only needs a few examples to guide the LLM's adaptation to a specific task, along with reusable ASP knowledge modules that can be applied to multiple tasks. We demonstrate that this method achieves state-of-the-art performance on several NLP benchmarks, including bAbI, StepGame, CLUTRR, and gSCAN. Additionally, it successfully tackles robot planning tasks that an LLM alone fails to solve.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study observes that a large language model can serve as a highly effective few-shot semantic parser that can convert natural language sentences into a logical form that serves as input for answer set programs, a logic-based declarative knowledge representation formalism."
            },
            "score": 4
        },
        {
            "id": "7ef9aafc68511afab5b287e62b754576ea37b4ce",
            "paperId": "7ef9aafc68511afab5b287e62b754576ea37b4ce",
            "title": "Structured, flexible, and robust: benchmarking and improving large language models towards more human-like behavior in out-of-distribution reasoning tasks",
            "abstract": "Human language offers a powerful window into our thoughts -- we tell stories, give explanations, and express our beliefs and goals through words. Abundant evidence also suggests that language plays a developmental role in structuring our learning. Here, we ask: how much of human-like thinking can be captured by learning statistical patterns in language alone? We first contribute a new challenge benchmark for comparing humans and distributional large language models (LLMs). Our benchmark contains two problem-solving domains (planning and explanation generation) and is designed to require generalization to new, out-of-distribution problems expressed in language. We find that humans are far more robust than LLMs on this benchmark. Next, we propose a hybrid Parse-and-Solve model, which augments distributional LLMs with a structured symbolic reasoning module. We find that this model shows more robust adaptation to out-of-distribution planning problems, demonstrating the promise of hybrid AI models for more human-like reasoning.",
            "year": 2022,
            "citationCount": 35,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A hybrid Parse-and-Solve model is proposed, which augments distributional LLMs with a structured symbolic reasoning module, which shows more robust adaptation to out-of-distribution planning problems, demonstrating the promise of hybrid AI models for more human-like reasoning."
            },
            "score": 4
        },
        {
            "id": "ddcd2bcc809bd0c2755a4a9487473d61ac327c50",
            "paperId": "ddcd2bcc809bd0c2755a4a9487473d61ac327c50",
            "title": "Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models",
            "abstract": "The escalating debate on AI\u2019s capabilities warrants developing reliable metrics to assess machine \u201cintelligence.\u201d Recently, many anecdotal examples were used to suggest that newer Large Language Models (LLMs) like ChatGPT and GPT-4 exhibit Neural Theory-of-Mind (N-ToM); however, prior work reached conflicting conclusions regarding those abilities. We investigate the extent of LLMs\u2019 N-ToM through an extensive evaluation of 6 tasks and find that while LLMs exhibit certain N-ToM abilities, this behavior is far from being robust. We further examine the factors impacting performance on N-ToM tasks and discover that LLMs struggle with adversarial examples, indicating reliance on shallow heuristics rather than robust ToM abilities. We caution against drawing conclusions from anecdotal examples, limited benchmark testing, and using human-designed psychological tests to evaluate models.",
            "year": 2023,
            "citationCount": 50,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates the extent of LLMs\u2019 N-ToM through an extensive evaluation of 6 tasks and finds that while LLMs exhibit certain N-ToM abilities, this behavior is far from being robust."
            },
            "score": 4
        },
        {
            "id": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
            "paperId": "2f3822eb380b5e753a6d579f31dfc3ec4c4a0820",
            "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
            "abstract": "Language models are increasingly being deployed for general problem solving across a wide range of tasks, but are still confined to token-level, left-to-right decision-making processes during inference. This means they can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To surmount these challenges, we introduce a new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving. ToT allows LMs to perform deliberate decision making by considering multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices. Our experiments show that ToT significantly enhances language models' problem-solving abilities on three novel tasks requiring non-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in Game of 24, while GPT-4 with chain-of-thought prompting only solved 4% of tasks, our method achieved a success rate of 74%. Code repo with all prompts: https://github.com/princeton-nlp/tree-of-thought-llm.",
            "year": 2023,
            "citationCount": 587,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new framework for language model inference, Tree of Thoughts (ToT), which generalizes over the popular Chain of Thought approach to prompting language models, and enables exploration over coherent units of text (thoughts) that serve as intermediate steps toward problem solving."
            },
            "score": 4
        },
        {
            "id": "bc451fbacc864277d71847103535fbadc8d8cb5f",
            "paperId": "bc451fbacc864277d71847103535fbadc8d8cb5f",
            "title": "Toward Value Scenario Generation Through Large Language Models",
            "abstract": "We propose a method of generating value scenarios for design research by leveraging ChatGPT, an AI-powered chatbot based on large language models. Identifying the needs of a vulnerable population, such as North Korean defectors, is challenging for researchers. To address this, we introduce ChatGPT-generated value scenarios, an extension of scenario-based design that supports critical, systemic, long-term thinking in current design practice, technology development, and deployment. Using our proposed method, we created a prompt to generate value scenarios on ChatGPT. Based on our analysis of the generated scenarios, we identified that ChatGPT could generate plausible information about Value Implications. However, it lacks details on Pervasiveness and Systemic Effects. After discussing the limitations and opportunities of ChatGPT in generating value scenarios, we conclude with suggestions for how ChatGPT might be better used to generate value scenarios.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ChatGPT-generated value scenarios are introduced, an extension of scenario-based design that supports critical, systemic, long-term thinking in current design practice, technology development, and deployment and identifies that ChatGPT could generate plausible information about Value Implications, but lacks details on Pervasiveness and Systemic Effects."
            },
            "score": 4
        },
        {
            "id": "3c759e2f16bfde8d31189631e4893d3ac8ff05f2",
            "paperId": "3c759e2f16bfde8d31189631e4893d3ac8ff05f2",
            "title": "Mitigating Gender Bias in Distilled Language Models via Counterfactual Role Reversal",
            "abstract": "Language models excel at generating coherent text, and model compression techniques such as knowledge distillation have enabled their use in resource-constrained settings. However, these models can be biased in multiple ways, including the unfounded association of male and female genders with gender-neutral professions. Therefore, knowledge distillation without any fairness constraints may preserve or exaggerate the teacher model\u2019s biases onto the distilled model. To this end, we present a novel approach to mitigate gender disparity in text generation by learning a fair model during knowledge distillation. We propose two modifications to the base knowledge distillation based on counterfactual role reversal\u2014modifying teacher probabilities and augmenting the training set. We evaluate gender polarity across professions in open-ended text generated from the resulting distilled and finetuned GPT\u20132 models and demonstrate a substantial reduction in gender disparity with only a minor compromise in utility. Finally, we observe that language models that reduce gender polarity in language generation do not improve embedding fairness or downstream classification fairness.",
            "year": 2022,
            "citationCount": 34,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel approach to mitigate gender disparity in text generation by learning a fair model during knowledge distillation is presented and two modifications based on counterfactual role reversal are proposed\u2014modifying teacher probabilities and augmenting the training set."
            },
            "score": 4
        },
        {
            "id": "85e7d63f75c0916bd350a229e040c5fbb1472e7a",
            "paperId": "85e7d63f75c0916bd350a229e040c5fbb1472e7a",
            "title": "Making Pre-trained Language Models Better Few-shot Learners",
            "abstract": "The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF\u2014better few-shot fine-tuning of language models\u2014a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30% absolute improvement, and 11% on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning.",
            "year": 2021,
            "citationCount": 1424,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The LM-BFF approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning."
            },
            "score": 4
        },
        {
            "id": "8e8f2081007a0380856774444d5ea18cb5096351",
            "paperId": "8e8f2081007a0380856774444d5ea18cb5096351",
            "title": "Uncertainty-Aware Explainable Recommendation with Large Language Models",
            "abstract": "Providing explanations within the recommendation system would boost user satisfaction and foster trust, especially by elaborating on the reasons for selecting recommended items tailored to the user. The predominant approach in this domain revolves around generating text-based explanations, with a notable emphasis on applying large language models (LLMs). However, refining LLMs for explainable recommendations proves impractical due to time constraints and computing resource limitations. As an alternative, the current approach involves training the prompt rather than the LLM. In this study, we developed a model that utilizes the ID vectors of user and item inputs as prompts for GPT-2. We employed a joint training mechanism within a multi-task learning framework to optimize both the recommendation task and explanation task. This strategy enables a more effective exploration of users' interests, improving recommendation effectiveness and user satisfaction. Through the experiments, our method achieving 1.59 DIV, 0.57 USR and 0.41 FCR on the Yelp, TripAdvisor and Amazon dataset respectively, demonstrates superior performance over four SOTA methods in terms of explainability evaluation metric. In addition, we identified that the proposed model is able to ensure stable textual quality on the three public datasets.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A model is developed that utilizes the ID vectors of user and item inputs as prompts for GPT-2 and is able to ensure stable textual quality on the three public datasets, demonstrating superior performance over four SOTA methods in terms of explainability evaluation metric."
            },
            "score": 3
        },
        {
            "id": "d18b5cc5d1b37a670ea42012a04373e157f2ec06",
            "paperId": "d18b5cc5d1b37a670ea42012a04373e157f2ec06",
            "title": "Language-Aware Soft Prompting: Text-to-Text Optimization for Few- and Zero-Shot Adaptation of V &L Models",
            "abstract": null,
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel Language-Aware Soft Prompting (LASP) learning method by means of a text-to-text cross-entropy loss that maximizes the probability of the learned prompts to be correctly classified with respect to pre-defined hand-crafted textual prompts and presents a novel zero-shot variant of LASP."
            },
            "score": 3
        },
        {
            "id": "b626560f19f815808a289ef5c24a17c57320da70",
            "paperId": "b626560f19f815808a289ef5c24a17c57320da70",
            "title": "MathPrompter: Mathematical Reasoning using Large Language Models",
            "abstract": "Large Language Models (LLMs) have limited performance when solving arithmetic reasoning tasks and often provide incorrect answers. Unlike natural language understanding, math problems typically have a single correct answer, making the task of generating accurate solutions more challenging for LLMs. To the best of our knowledge, we are not aware of any LLMs that indicate their level of confidence in their responses which fuels a trust deficit in these models impeding their adoption. To address this deficiency, we propose \u2018MathPrompter\u2019, a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions. MathPrompter uses the Zero-shot chain-of-thought prompting technique to generate multiple algebraic expressions or python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results. This is in contrast to other prompt based CoT methods, where there is no check on the validity of the intermediate steps followed. Our technique improves over state-of-the-art on the \u2018MultiArith\u2019 dataset (78.7% - 92.5%) evaluated using 175B parameter GPT-based LLM.",
            "year": 2023,
            "citationCount": 89,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes \u2018MathPrompter\u2019, a technique that improves performance of LLMs on arithmetic problems along with increased reliance in the predictions, using the Zero-shot chain-of-thought prompting technique to generate multiple algebraic expressions or python functions to solve the same math problem in different ways and thereby raise the confidence level in the output results."
            },
            "score": 3
        },
        {
            "id": "066dc4d3550dce456856344acb1434a5ef46ac5d",
            "paperId": "066dc4d3550dce456856344acb1434a5ef46ac5d",
            "title": "Towards Robust Temporal Reasoning of Large Language Models via a Multi-Hop QA Dataset and Pseudo-Instruction Tuning",
            "abstract": "Knowledge in the real world is being updated constantly. However, it is costly to frequently update large language models (LLMs). Therefore, it is crucial for LLMs to understand the concept of temporal knowledge. However, prior works on temporal question answering did not emphasize multi-answer and multi-hop types of temporal reasoning. In this paper, we propose a complex temporal question-answering (QA) dataset Complex-TR that focuses on multi-answer and multi-hop temporal reasoning. Besides, we also propose a novel data augmentation strategy to improve the complex temporal reasoning capability and robustness of LLMs. We conducted experiments on multiple temporal QA datasets. Experimental results show that our method is able to improve LLMs' performance on temporal QA benchmarks by significant margins.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A complex temporal question-answering (QA) dataset Complex-TR that focuses on multi-answer and multi-hop temporal reasoning and a novel data augmentation strategy to improve the complex temporal reasoning capability and robustness of LLMs is proposed."
            },
            "score": 3
        },
        {
            "id": "3aee33831e0bdea1a1eaae21c7586e4f7c0396d6",
            "paperId": "3aee33831e0bdea1a1eaae21c7586e4f7c0396d6",
            "title": "Making Retrieval-Augmented Language Models Robust to Irrelevant Context",
            "abstract": "Retrieval-augmented language models (RALMs) hold promise to produce language understanding systems that are are factual, efficient, and up-to-date. An important desideratum of RALMs, is that retrieved information helps model performance when it is relevant, and does not harm performance when it is not. This is particularly important in multi-hop reasoning scenarios, where misuse of irrelevant evidence can lead to cascading errors. However, recent work has shown that retrieval augmentation can sometimes have a negative effect on performance. In this work, we present a thorough analysis on five open-domain question answering benchmarks, characterizing cases when retrieval reduces accuracy. We then propose two methods to mitigate this issue. First, a simple baseline that filters out retrieved passages that do not entail question-answer pairs according to a natural language inference (NLI) model. This is effective in preventing performance reduction, but at a cost of also discarding relevant passages. Thus, we propose a method for automatically generating data to fine-tune the language model to properly leverage retrieved passages, using a mix of relevant and irrelevant contexts at training time. We empirically show that even 1,000 examples suffice to train the model to be robust to irrelevant contexts while maintaining high performance on examples with relevant ones.",
            "year": 2023,
            "citationCount": 40,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work empirically shows that even 1,000 examples suffice to train the model to be robust to irrelevant contexts while maintaining high performance on examples with relevant ones, and proposes a method for automatically generating data to fine-tune the language model to properly leverage retrieved passages."
            },
            "score": 3
        },
        {
            "id": "239522a364f9872f9c3d0d61bf3aca32d8186d0b",
            "paperId": "239522a364f9872f9c3d0d61bf3aca32d8186d0b",
            "title": "A Generative AI-driven Application: Use of Large Language Models for Traffic Scenario Generation",
            "abstract": "Artificial Intelligence (AI) has revolutionized the way we interact with technology. One of its forefronts is Natural Language Processing (NLP), an essential sub-field that focuses on human language understanding and generation. Large Language Models (LLMs) such as PaLM, BERT, GPT have emerged as the powerhouses of NLPs with remarkable capabilities. Today, we also utilize AI in other industries like automotive. While using AI-backed powerful instruments in autonomous vehicle technologies, we are still suffering to create virtual traffic scenarios for special purposes such as safety testing and validation, autonomous driver training and regulatory compliance. In this study, we introduce a solution which harnesses the power of LLMs to overcome scenario creation difficulties. Thus, our approach provides creation of the traffic scenarios in linguistic way rather than using user interfaces of the editors or writing special formatted structured scenario files.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study introduces a solution which harnesses the power of LLMs to overcome scenario creation difficulties and provides creation of the traffic scenarios in linguistic way rather than using user interfaces of the editors or writing special formatted structured scenario files."
            },
            "score": 3
        },
        {
            "id": "d011b9cc8bad2d29595f1ef5cff344abd2c5c5ab",
            "paperId": "d011b9cc8bad2d29595f1ef5cff344abd2c5c5ab",
            "title": "CFL: Causally Fair Language Models Through Token-level Attribute Controlled Generation",
            "abstract": "We propose a method to control the attributes of Language Models (LMs) for the text generation task using Causal Average Treatment Effect (ATE) scores and counterfactual augmentation. We explore this method, in the context of LM detoxification, and propose the Causally Fair Language (CFL) architecture for detoxifying pre-trained LMs in a plug-and-play manner. Our architecture is based on a Structural Causal Model (SCM) that is mathematically transparent and computationally efficient as compared with many existing detoxification techniques. We also propose several new metrics that aim to better understand the behaviour of LMs in the context of toxic text generation. Further, we achieve state of the art performance for toxic degeneration, which are computed using \\RTP (RTP) benchmark. Our experiments show that CFL achieves such a detoxification without much impact on the model perplexity. We also show that CFL mitigates the unintended bias problem through experiments on the BOLD dataset.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Causally Fair Language (CFL) architecture for detoxifying pre-trained LMs in a plug-and-play manner is proposed, based on a Structural Causal Model (SCM) that is mathematically transparent and computationally efficient as compared with many existing detoxification techniques."
            },
            "score": 3
        },
        {
            "id": "a8dbe570adee6e5f6031f4ce3d49007406377438",
            "paperId": "a8dbe570adee6e5f6031f4ce3d49007406377438",
            "title": "Building a hospitable and reliable dialogue system for android robots: a scenario-based approach with large language models",
            "abstract": "Dialogue systems implemented with android robots are expected to provide not only advanced conversational ability but also reliability and hospitality due to their human-like appearance. In this study, we aim to develop a hospitable dialogue system by encouraging open-ended utterances and responding adaptively to give users the feeling of being heard. To achieve this, utilizing large language models (LLMs) is a promising option, but task-oriented dialogue systems implemented with only LLMs often generate irrelevant, inconsistent, or contradictory utterances. Therefore, we propose a scenario-based dialogue system that subdivides the task into smaller sub-tasks, such as summarization, information extraction, and response generation, and uses LLMs in a fine-grained manner to overcome such shortcomings of the LLM-based dialogue system. Our system was evaluated in the tourist-spot recommendation task of the Dialogue Robot Competition 2022 and achieved second place in the preliminary round and first place in the final round, outperforming other rule-based dialogue systems. However, we also identified several challenges when using LLMs for android dialogue systems, including response delays due to computational complexity, hallucinations, and coordination issues between generated utterances and robot control. GRAPHICAL ABSTRACT",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study aims to develop a hospitable dialogue system by encouraging open-ended utterances and responding adaptively to give users the feeling of being heard by proposing a scenario-based dialogue system that subdivides the task into smaller sub-tasks, such as summarization, information extraction, and response generation."
            },
            "score": 3
        },
        {
            "id": "7e32aac43e9f1df49e116add03327ee6f365dbf3",
            "paperId": "7e32aac43e9f1df49e116add03327ee6f365dbf3",
            "title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality",
            "abstract": "Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tune a low-rank adaption (LoRA) module on LLM and the abstractor module by freezing the visual knowledge module. We carefully build a visually-related instruction evaluation set OwlEval. Experimental results show that our model outperforms existing multi-modal models, demonstrating mPLUG-Owl's impressive instruction and visual understanding ability, multi-turn conversation ability, and knowledge reasoning ability. Besides, we observe some unexpected and exciting abilities such as multi-image correlation and scene text understanding, which makes it possible to leverage it for harder real scenarios, such as vision-only document comprehension. Our code, pre-trained model, instruction-tuned models, and evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The online demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl.",
            "year": 2023,
            "citationCount": 419,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 3
        },
        {
            "id": "0ff029e91b8185739646a4ac6ab0713909d31d16",
            "paperId": "0ff029e91b8185739646a4ac6ab0713909d31d16",
            "title": "Large Language Models of Code Fail at Completing Code with Potential Bugs",
            "abstract": "Large language models of code (Code-LLMs) have recently brought tremendous advances to code completion, a fundamental feature of programming assistance and code intelligence. However, most existing works ignore the possible presence of bugs in the code context for generation, which are inevitable in software development. Therefore, we introduce and study the buggy-code completion problem, inspired by the realistic scenario of real-time code suggestion where the code context contains potential bugs -- anti-patterns that can become bugs in the completed program. To systematically study the task, we introduce two datasets: one with synthetic bugs derived from semantics-altering operator changes (buggy-HumanEval) and one with realistic bugs derived from user submissions to coding problems (buggy-FixEval). We find that the presence of potential bugs significantly degrades the generation performance of the high-performing Code-LLMs. For instance, the passing rates of CODEGEN-2B-MONO on test cases of buggy-HumanEval drop more than 50% given a single potential bug in the context. Finally, we investigate several post-hoc methods for mitigating the adverse effect of potential bugs and find that there remains a significant gap in post-mitigation performance.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The buggy-code completion problem is introduced, inspired by the realistic scenario of real-time code suggestion where the code context contains potential bugs -- anti-patterns that can become bugs in the completed program."
            },
            "score": 3
        },
        {
            "id": "5daf2620c2244bff9b740939ba55887d11f9d7e4",
            "paperId": "5daf2620c2244bff9b740939ba55887d11f9d7e4",
            "title": "Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models",
            "abstract": "Image-based precision medicine aims to personalize treatment decisions based on an individual's unique imaging features so as to improve their clinical outcome. Machine learning frameworks that integrate uncertainty estimation as part of their treatment recommendations would be safer and more reliable. However, little work has been done in adapting uncertainty estimation techniques and validation metrics for precision medicine. In this paper, we use Bayesian deep learning for estimating the posterior distribution over factual and counterfactual outcomes on several treatments. This allows for estimating the uncertainty for each treatment option and for the individual treatment effects (ITE) between any two treatments. We train and evaluate this model to predict future new and enlarging T2 lesion counts on a large, multi-center dataset of MR brain images of patients with multiple sclerosis, exposed to several treatments during randomized controlled trials. We evaluate the correlation of the uncertainty estimate with the factual error, and, given the lack of ground truth counterfactual outcomes, demonstrate how uncertainty for the ITE prediction relates to bounds on the ITE error. Lastly, we demonstrate how knowledge of uncertainty could modify clinical decision-making to improve individual patient and clinical trial outcomes.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Bayesian deep learning is used for estimating the posterior distribution over factual and counterfactual outcomes on several treatments on several treatment options and for the individual treatment effects (ITE) between any two treatments."
            },
            "score": 2
        },
        {
            "id": "f221eccdd96122a42c5e65532373e6974b30c20c",
            "paperId": "f221eccdd96122a42c5e65532373e6974b30c20c",
            "title": "Large Language Models on the Chessboard: A Study on ChatGPT's Formal Language Comprehension and Complex Reasoning Skills",
            "abstract": "While large language models have made strides in natural language processing, their proficiency in complex reasoning tasks requiring formal language comprehension, such as chess, remains less investigated. This paper probes the performance of ChatGPT, a sophisticated language model by OpenAI in tackling such complex reasoning tasks, using chess as a case study. Through robust metrics examining both the legality and quality of moves, we assess ChatGPT's understanding of the chessboard, adherence to chess rules, and strategic decision-making abilities. Our evaluation identifies limitations within ChatGPT's attention mechanism that affect its formal language comprehension and uncovers the model's underdeveloped self-regulation abilities. Our study also reveals ChatGPT's propensity for a coherent strategy in its gameplay and a noticeable uptick in decision-making assertiveness when the model is presented with a greater volume of natural language or possesses a more lucid understanding of the state of the chessboard. These findings contribute to the growing exploration of language models' abilities beyond natural language processing, providing valuable information for future research towards models demonstrating human-like cognitive abilities.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study probes the performance of ChatGPT, a sophisticated language model by OpenAI in tackling complex reasoning tasks, using chess as a case study, and identifies limitations withinChatGPT's attention mechanism that affect its formal language comprehension and uncovers the model's underdeveloped self-regulation abilities."
            },
            "score": 2
        },
        {
            "id": "af8123ecdff838f63e4eba0b36b8babe4c5cee65",
            "paperId": "af8123ecdff838f63e4eba0b36b8babe4c5cee65",
            "title": "LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models",
            "abstract": "Quantization is an indispensable technique for serving Large Language Models (LLMs) and has recently found its way into LoRA fine-tuning. In this work we focus on the scenario where quantization and LoRA fine-tuning are applied together on a pre-trained model. In such cases it is common to observe a consistent gap in the performance on downstream tasks between full fine-tuning and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ (LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that simultaneously quantizes an LLM and finds a proper low-rank initialization for LoRA fine-tuning. Such an initialization alleviates the discrepancy between the quantized and full-precision model and significantly improves generalization in downstream tasks. We evaluate our method on natural language understanding, question answering, summarization, and natural language generation tasks. Experiments show that our method is highly effective and outperforms existing quantization methods, especially in the challenging 2-bit and 2/4-bit mixed precision regimes. The code is available on https://github.com/yxli2123/LoftQ.",
            "year": 2023,
            "citationCount": 23,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "LoftQ (LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that simultaneously quantizes an LLM and finds a proper low-rank initialization for LoRA fine-tuning, which alleviates the discrepancy between the quantized and full-precision model and significantly improves generalization in downstream tasks."
            },
            "score": 2
        },
        {
            "id": "a525cbb8600ab6d2a1351f6883937c1774642d76",
            "paperId": "a525cbb8600ab6d2a1351f6883937c1774642d76",
            "title": "Generating High-Precision Feedback for Programming Syntax Errors using Large Language Models",
            "abstract": "Large language models (LLMs), such as Codex, hold great promise in enhancing programming education by automatically generating feedback for students. We investigate using LLMs to generate feedback for fixing syntax errors in Python programs, a key scenario in introductory programming. More concretely, given a student's buggy program, our goal is to generate feedback comprising a fixed program along with a natural language explanation describing the errors/fixes, inspired by how a human tutor would give feedback. While using LLMs is promising, the critical challenge is to ensure high precision in the generated feedback, which is imperative before deploying such technology in classrooms. The main research question we study is: Can we develop LLMs-based feedback generation techniques with a tunable precision parameter, giving educators quality control over the feedback that students receive? To this end, we introduce PyFiXV, our technique to generate high-precision feedback powered by Codex. The key idea behind PyFiXV is to use a novel run-time validation mechanism to decide whether the generated feedback is suitable for sharing with the student; notably, this validation mechanism also provides a precision knob to educators. We perform an extensive evaluation using two real-world datasets of Python programs with syntax errors and show the efficacy of PyFiXV in generating high-precision feedback.",
            "year": 2023,
            "citationCount": 20,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The key idea behind PyFiXV is to use a novel run-time validation mechanism to decide whether the generated feedback is suitable for sharing with the student, and this validation mechanism also provides a precision knob to educators."
            },
            "score": 2
        },
        {
            "id": "0bfc804e31eecfd77f45e4ee7f4d629fffdcd628",
            "paperId": "0bfc804e31eecfd77f45e4ee7f4d629fffdcd628",
            "title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs",
            "abstract": "Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.",
            "year": 2023,
            "citationCount": 206,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ToolLLM is introduced, a general tool-use framework encompassing data construction, model training, and evaluation, and a novel depth-first search-based decision tree algorithm that enables LLMs to evaluate multiple reasoning traces and expand the search space."
            },
            "score": 2
        },
        {
            "id": "87c1e8eff9f2a2f2374b5eac4791d34b30477619",
            "paperId": "87c1e8eff9f2a2f2374b5eac4791d34b30477619",
            "title": "Clinical Natural Language Processing in Languages Other Than English",
            "abstract": "Natural Language Processing (NLP) of clinical free-text has received a lot of attention from the scientific community. Clinical documents are routinely created across health care providing institutions and are generally written in the official language(s) of the country these institutions are located in. As a result, free-text clinical information is written in a large variety of languages. While most of the efforts for clinical NLP have focused on English, there is a strong need to extend this work to other languages, for instance in order to gain medical information about patient cohorts in geographical areas where English is not an official language. Furthermore, adapting current NLP methods developed for English to other languages may provide useful insight on the generalizability of algorithms and lead to increased robustness. This panel aims to provide an overview of clinical NLP for languages other than English, as for example French, Swedish and Japanese and discuss future methodological advances of clinical NLP in a context that encompasses English as well as other languages. General Description of the Panel The goal of this panel is to engage the medical informatics and clinical Natural Language Processing community in a discussion about ways to advance research through languages other than English. We will provide an overview the current state of clinical NLP in a variety of European and non-European languages as well as focused reports on French, Swedish and Bulgarian. We will motivate the need for developing clinical NLP in languages other than English by the potential for methodological and medical advances. Finally, we will propose strategies to contribute to advance work on languages other than English and integrate it in a state-of-the art platform. Clinical NLP in languages other than English Natural Language Processing (NLP) of clinical free-text has received a lot of attention from the scientific community, demonstrating its potential to provide the means to analyze large quantities of documents rapidly and accurately (Demner-Fushman et al. 2010). Prime clinical applications for NLP include assisting healthcare professionals with restrospective studies and clinical decision making. The ability to analyze clinical text in languages other than English opens access to important medical data concerning cohorts of patients who are treated in countries where English is not the official language. Recently, Kohane et al. (2012) also showed the impact of methods allowing an aggregated exploitation of clinical data. In this context, data extracted from clinical texts in languages other than English adds another dimension to data aggregation. As the importance of clinical NLP gains recognition, clinical corpora become available to researchers in languages other than English, prompting work that sometimes builds on methods validated for English. Adapting systems that work well for English to another language is a difficult task that may be carried out with varying level of success depending on the task and language (Grouin et al., 2009; Velupillai et al. 2014; T\u00e4ckstr\u00f6m et al., 2012). For nonEuropean languages, approaches that account for entirely different word and sentence structures sometimes need to be developped (Shinohara et al. 2013), and cultural differences between clinical narrative styles accounted for (Wu et al. 2013). Access to terminologies and corpora in languages other than English can also be challenging (Schulz et al. 2013; Xu et al. 2013). These experiments prompt a reflexion on how to carry out clinical NLP in a more global context: should methods be developed for one language and then ported to other languages? Can the source language method benefit from the porting? Can algorithms be more robust if they are designed with a multilanguage perspective from the start? French is widely spoken around the world and benefits from one of the largest coverage in the UMLS. Automatic de-identification is becoming quite advanced for French (Grouin & N\u00e9v\u00e9ol, 2013), leading to good results for targeted clinical information extraction tasks (Del\u00e9ger et al. 2010; Grouin et al. 2011). Recent efforts from the French biomedical Informatics community have addressed rules and regulations to improve the access of NLP researchers to clinical corpus. Furthermore, the success of initiatives such as that reported by Grouin et al. (2011) increased the awareness of the potential implication of clinical NLP in clinical practice and contributed to making the timing ripe for making clinical corpus available for annotation and NLP tool development. On-going efforts currently address the annotation of clinical corpora for entity, modality and relations. Tools are being designed for information extraction as well as semantic indexing, information retrieval and clinical data visualization. Much of the research in Swedish clinical NLP has used the Stockholm EPR Corpus, (Dalianis 2012), that contains more than one million patient records encompassing the years 2006-2010, from over 550 clinical units origin from Karolinska University Hospital. Part of this corpus has been manually annotated for Protected Health Information, negations, uncertainty levels, symptoms, diseases, drugs, body parts and abbreviations. The annotated corpora have been used both for training of machine learning systems and evaluation. Some applications are explorative as comorbidity networks, warning and reporting systems detecting hospital acquired infections or adverse drug events, but also work on text simplification of patient record content for the layman patient, (Dalianis 2012). Tools that have been developed for this is an adaptation of NegEx for Swedish (Skeppstedt 2012), a system for classifying terms into six levels of assertion levels pyConTextSwe, (Velupillai et al. 2014), abbreviation detection, (Isenius et al. 2012) and machine learning system based on CRF++ that recognizes named clinical entities as symptoms, diseases, drugs and body, (Skeppstedt et al. 2014). Integrating languages other than English in Apache cTAKES Apache cTAKES (ctakes.apache.org) has been quite successful in assembling and sustaining a global community of developers and users of state-of-the-art English language clinical NLP. Because these techniques involve computational machine learning methods, datasets from the targeted language are needed to train and evaluate the algorithms on. We will discuss what types and size of data were used to build the various cTAKES components \u2013 sentence boundary detector, tokenizer, part of speech tagger, syntactic parser, event and temporal expression detector, temporal relation modules, general relation module. We will also discuss what types of gold standard labels (and how much of each type) are needed to port cTAKES components to other language within the light of some use cases such as porting the temporal expression discovery and normalization module originally developed for English (Bethard, 2013) to Swedish. We will outline available resources in other languages such as Swedish, Finnish, Bulgarian. This is a step towards globalization of information extraction from the clinical narrative.",
            "year": 2014,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This panel aims to provide an overview of clinical NLP for languages other than English, as for example French, Swedish and Japanese and discuss future methodological advances of clinical NLP in a context that encompasses English as well as other languages."
            },
            "score": 2
        }
    ],
    "novelty": "yes"
}