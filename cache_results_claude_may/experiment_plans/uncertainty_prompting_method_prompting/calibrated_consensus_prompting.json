{
    "topic_description": "novel prompting methods that can better quantify uncertainty or calibrate the confidence of large language models",
    "idea_name": "Calibrated Consensus Prompting",
    "raw_idea": {
        "Type": "prompting",
        "Problem": "Large language models often exhibit overconfidence in their predictions, leading to unreliable outputs in real-world applications.",
        "Existing Methods": "Current methods for confidence calibration in LLMs include temperature scaling, ensembling, and post-hoc calibration using additional models.",
        "Motivation": "Inspired by the wisdom of crowds, we hypothesize that prompting an LLM to generate multiple diverse responses and assessing their consensus can provide a more reliable estimate of the model's confidence.",
        "Proposed Method": "We propose Calibrated Consensus Prompting (CCP), a novel prompting approach that involves three main steps: 1) Diverse Response Generation: Prompt the LLM to generate multiple responses to the same input, encouraging diversity through techniques like nucleus sampling or diverse beam search. 2) Consensus Assessment: Analyze the generated responses to quantify their agreement, using metrics such as semantic similarity or exact match. 3) Confidence Calibration: Calibrate the model's confidence based on the degree of consensus among the generated responses, assigning higher confidence to inputs with strong agreement and lower confidence to those with divergent responses.",
        "Experiment Plan": "Evaluate CCP on a range of tasks, including question answering, natural language inference, and text classification. Compare the calibration performance of CCP against baseline methods using metrics like expected calibration error (ECE) and maximum calibration error (MCE)."
    },
    "full_experiment_plan": {
        "Title": "Calibrated Consensus Prompting: Improving Confidence Calibration in Large Language Models",
        "Problem Statement": "Large language models often exhibit overconfidence in their predictions, leading to unreliable outputs in real-world applications. This can be problematic in high-stakes scenarios where accurate uncertainty estimation is crucial.",
        "Motivation": "Existing methods for confidence calibration in LLMs, such as temperature scaling, ensembling, and post-hoc calibration using additional models, have limitations. Temperature scaling and ensembling can be computationally expensive, while post-hoc calibration requires training additional models. Inspired by the wisdom of crowds, we hypothesize that prompting an LLM to generate multiple diverse responses and assessing their consensus can provide a more reliable estimate of the model's confidence without the need for additional training or computation.",
        "Proposed Method": "We propose Calibrated Consensus Prompting (CCP), a novel prompting approach that involves three main steps:\n1. Diverse Response Generation: Prompt the LLM to generate multiple responses to the same input, encouraging diversity through techniques like nucleus sampling or diverse beam search.\n2. Consensus Assessment: Analyze the generated responses to quantify their agreement, using metrics such as semantic similarity or exact match.\n3. Confidence Calibration: Calibrate the model's confidence based on the degree of consensus among the generated responses, assigning higher confidence to inputs with strong agreement and lower confidence to those with divergent responses.",
        "Step-by-Step Experiment Plan": {
            "Step 1: Gather Datasets": "Evaluate CCP on a range of tasks, including question answering (SQuAD, TriviaQA), natural language inference (MNLI, SNLI), and text classification (AG News, DBpedia). These datasets cover different domains and task types to assess the generalizability of the proposed method.",
            "Step 2: Construct Prompts": "For each task, create a set of prompts that encourage diverse response generation. Example prompts for question answering:\n- 'Provide a concise answer to the following question:'\n- 'Generate a detailed response to the given question:'\n- 'Answer the question below, considering multiple perspectives:'\nFor natural language inference and text classification, use prompts like:\n- 'Determine the relationship between the premise and hypothesis:'\n- 'Classify the given text into one of the following categories:'\nAppend the input (question, premise-hypothesis pair, or text) to these prompts.",
            "Step 3: Generate Diverse Responses": "For each input, use the constructed prompts to generate multiple responses from the LLM. Experiment with different diversity-promoting techniques:\n- Nucleus sampling: Set a probability threshold (e.g., 0.9) and sample from the top tokens whose cumulative probability exceeds the threshold.\n- Diverse beam search: Maintain a beam of diverse candidates during decoding, penalizing similar responses.\nGenerate 3-5 responses per input to balance diversity and computational efficiency.",
            "Step 4: Assess Consensus": "Quantify the agreement among the generated responses using suitable metrics for each task:\n- Question answering: Exact match and token-level F1 score between the responses.\n- Natural language inference: Percentage of responses with the same predicted label (entailment, contradiction, or neutral).\n- Text classification: Percentage of responses with the same predicted class.\nCalculate the average consensus score across the generated responses for each input.",
            "Step 5: Calibrate Confidence": "Map the consensus scores to calibrated confidence estimates. Experiment with different mapping functions:\n- Linear mapping: Normalize the consensus scores to the range [0, 1] and use them directly as confidence estimates.\n- Threshold-based mapping: Set a threshold (e.g., 0.7) and assign high confidence to inputs with consensus scores above the threshold, and low confidence otherwise.\n- Learned mapping: Train a small calibration model (e.g., logistic regression) to predict the probability of correctness based on the consensus scores, using a held-out validation set.",
            "Step 6: Evaluate Calibration": "Compare the calibration performance of CCP against baseline methods (temperature scaling, ensembling, post-hoc calibration) using metrics like Expected Calibration Error (ECE) and Maximum Calibration Error (MCE). Lower ECE and MCE values indicate better calibration.\nAdditionally, assess the accuracy-confidence correlation by plotting accuracy vs. confidence curves. Well-calibrated models should exhibit a strong positive correlation between accuracy and confidence.",
            "Step 7: Analyze Efficiency": "Measure the computational overhead of CCP compared to the baselines. Report the average inference time per input and the total computational cost (e.g., FLOPs) for each method. Discuss the trade-offs between calibration performance and computational efficiency."
        },
        "Test Case Examples": {
            "Question Answering": {
                "Test Case Input": "When was the Declaration of Independence signed?",
                "Baseline Output": "The Declaration of Independence was signed on July 4, 1776. (Confidence: 0.95)",
                "CCP Output": "Response 1: The Declaration of Independence was signed on July 4, 1776.\nResponse 2: The Declaration of Independence was formally adopted on July 4, 1776, but the actual signing took place on August 2, 1776.\nResponse 3: The Declaration of Independence was signed by most of the delegates on August 2, 1776, although the formal adoption took place on July 4, 1776.\nConsensus Score: 0.67\nCalibrated Confidence: 0.75",
                "Explanation": "The baseline method assigns high confidence to the output, even though it is partially incorrect. CCP generates multiple responses, revealing some disagreement about the exact signing date. The calibrated confidence is lower, reflecting the uncertainty in the responses."
            },
            "Natural Language Inference": {
                "Test Case Input": "Premise: A group of people are sitting in a circle on the grass.\nHypothesis: The people are having a picnic.",
                "Baseline Output": "Entailment (Confidence: 0.85)",
                "CCP Output": "Response 1: Neutral\nResponse 2: Neutral\nResponse 3: Entailment\nResponse 4: Neutral\nResponse 5: Neutral\nConsensus Score: 0.8 (Neutral)\nCalibrated Confidence: 0.7",
                "Explanation": "The baseline method incorrectly predicts entailment with high confidence. CCP generates multiple responses, with the majority agreeing on the 'neutral' label. The calibrated confidence is lower, aligning with the uncertainty in the hypothesis given the premise."
            }
        },
        "Fallback Plan": "If the proposed CCP method does not significantly improve calibration performance compared to the baselines, consider the following alternative approaches:\n1. Analyze the quality and diversity of the generated responses. If the responses lack sufficient diversity or fail to capture different perspectives, explore alternative prompting techniques or diversity-promoting objectives during generation.\n2. Investigate the relationship between the number of generated responses and calibration performance. Increase the number of responses per input to see if it leads to better consensus assessment and calibration.\n3. Experiment with different consensus assessment metrics or combine multiple metrics to better capture the agreement among the responses. Consider using more advanced natural language processing techniques, such as semantic similarity measures or clustering algorithms.\n4. Explore alternative calibration methods, such as Bayesian calibration or isotonic regression, to map the consensus scores to well-calibrated confidence estimates.\n5. Conduct a thorough error analysis to identify the types of inputs or tasks where CCP struggles the most. Use these insights to refine the prompting strategies or develop task-specific calibration approaches.\nIf the proposed method still fails to yield satisfactory results after exploring these alternatives, consider pivoting the project to focus on analyzing the limitations of consensus-based calibration in LLMs. Investigate the factors that contribute to overconfidence or poor calibration, such as dataset biases, model architectures, or training objectives. Use these findings to propose new research directions or provide guidance for future work on confidence calibration in LLMs."
    },
    "novelty_queries": [
        "KeywordQuery(\"calibrated consensus prompting language models\")",
        "KeywordQuery(\"confidence calibration language models\")",
        "KeywordQuery(\"diverse response generation confidence language models\")",
        "KeywordQuery(\"Calibrated Consensus Prompting NLP\")"
    ],
    "novelty_papers": [
        {
            "id": "67dab0fedfbdc403a2eb882b8d0efa7e16bc25da",
            "paperId": "67dab0fedfbdc403a2eb882b8d0efa7e16bc25da",
            "title": "Strength in Numbers: Estimating Confidence of Large Language Models by Prompt Agreement",
            "abstract": "Large language models have achieved impressive few-shot performance on a wide variety of tasks. However, in many settings, users require confidence estimates for model predictions. While traditional classifiers produce scores for each label, language models instead produce scores for the generation which may not be well calibrated. We compare generations across diverse prompts and show that these can be used to create confidence scores. By utilizing more prompts we can get more precise confidence estimates and use response diversity as a proxy for confidence. We evaluate this approach across ten multiple-choice question-answering datasets using three models: T0, FLAN-T5, and GPT-3. In addition to analyzing multiple human written prompts, we automatically generate more prompts using a language model in order to produce finer-grained confidence estimates. Our method produces more calibrated confidence estimates compared to the log probability of the answer to a single prompt. These improvements could benefit users who rely on prediction confidence for integration into a larger system or in decision-making processes.",
            "year": 2023,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work automatically generates more prompts using a language model in order to produce finer-grained confidence estimates and produces more calibrated confidence estimates compared to the log probability of the answer to a single prompt."
            },
            "score": 8,
            "novelty_score": "The research problem in the proposal is improving confidence calibration in large language models, and the proposed approach is to use consensus among multiple diverse prompts to estimate confidence.\n\nThe research problem in the paper is also improving confidence calibration in large language models, and the proposed approach is also to use agreement among multiple diverse prompts to estimate confidence.\n\nThey are both about improving confidence calibration in large language models by using consensus among multiple prompts.\nYes",
            "novelty_judgment": "yes"
        },
        {
            "id": "5d3105a5ffa133b873537bda8ff1ec6244c2b841",
            "paperId": "5d3105a5ffa133b873537bda8ff1ec6244c2b841",
            "title": "Think Twice Before Assure: Confidence Estimation for Large Language Models through Reflection on Multiple Answers",
            "abstract": "Confidence estimation aiming to evaluate output trustability is crucial for the application of large language models (LLM), especially the black-box ones. Existing confidence estimation of LLM is typically not calibrated due to the overconfidence of LLM on its generated incorrect answers. Existing approaches addressing the overconfidence issue are hindered by a significant limitation that they merely consider the confidence of one answer generated by LLM. To tackle this limitation, we propose a novel paradigm that thoroughly evaluates the trustability of multiple candidate answers to mitigate the overconfidence on incorrect answers. Building upon this paradigm, we introduce a two-step framework, which firstly instructs LLM to reflect and provide justifications for each answer, and then aggregates the justifications for comprehensive confidence estimation. This framework can be integrated with existing confidence estimation approaches for superior calibration. Experimental results on six datasets of three tasks demonstrate the rationality and effectiveness of the proposed framework.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes a novel paradigm that thoroughly evaluates the trustability of multiple candidate answers to mitigate the overconfidence on incorrect answers and introduces a two-step framework, which firstly instructs LLM to reflect and provide justifications for each answer, and then aggregates the justifications for comprehensive confidence estimation."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving confidence calibration in large language models to address their overconfidence issue. The proposed approach is Calibrated Consensus Prompting (CCP), which generates multiple diverse responses, assesses their consensus, and calibrates confidence based on the degree of consensus.\n\nThe research problem in the paper is also confidence estimation for large language models to evaluate output trustability and address the overconfidence issue. The proposed approach is a two-step framework that instructs the LLM to reflect and provide justifications for each answer, and then aggregates the justifications for comprehensive confidence estimation.\n\nBoth the proposal and the paper aim to address the overconfidence issue in large language models for better confidence estimation. However, their approaches differ. The proposal focuses on generating multiple responses and assessing their consensus, while the paper focuses on generating justifications for each answer and aggregating them for confidence estimation.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "ad934a9344f68fcc0b9aa704102aa48c39c5b591",
            "paperId": "ad934a9344f68fcc0b9aa704102aa48c39c5b591",
            "title": "Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models",
            "abstract": "Large language models (LLMs) specializing in natural language generation (NLG) have recently started exhibiting promising capabilities across a variety of domains. However, gauging the trustworthiness of responses generated by LLMs remains an open challenge, with limited research on uncertainty quantification (UQ) for NLG. Furthermore, existing literature typically assumes white-box access to language models, which is becoming unrealistic either due to the closed-source nature of the latest LLMs or computational constraints. In this work, we investigate UQ in NLG for black-box LLMs. We first differentiate uncertainty vs confidence: the former refers to the\"dispersion\"of the potential predictions for a fixed input, and the latter refers to the confidence on a particular prediction/generation. We then propose and compare several confidence/uncertainty metrics, applying them to selective NLG where unreliable results could either be ignored or yielded for further assessment. Experiments were carried out with several popular LLMs on question-answering datasets (for evaluation purposes). Results reveal that a simple metric for the semantic dispersion can be a reliable predictor of the quality of LLM responses, providing valuable insights for practitioners on uncertainty management when adopting LLMs. The code to replicate our experiments is available at https://github.com/zlin7/UQ-NLG.",
            "year": 2023,
            "citationCount": 37,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Results reveal that a simple metric for the semantic dispersion can be a reliable predictor of the quality of LLM responses, providing valuable insights for practitioners on uncertainty management when adopting LLMs."
            },
            "score": 7,
            "novelty_score": "The research problem in the proposal is improving confidence calibration in large language models, while the paper focuses on uncertainty quantification for black-box large language models in natural language generation. The proposal suggests using consensus among diverse generated responses to calibrate confidence, whereas the paper proposes and compares several confidence/uncertainty metrics for selective natural language generation.\n\nThe proposal and the paper address related but distinct problems and propose different approaches.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "9ffefdf1fcd780cb71450b0a7a29247c66aa87be",
            "paperId": "9ffefdf1fcd780cb71450b0a7a29247c66aa87be",
            "title": "The Unreliability of Explanations in Few-shot Prompting for Textual Reasoning",
            "abstract": "Does prompting a large language model (LLM) like GPT-3 with explanations improve in-context learning? We study this question on two NLP tasks that involve reasoning over text, namely question answering and natural language inference. We test the performance of four LLMs on three textual reasoning datasets using prompts that include explanations in multiple different styles. For these tasks, we find that including explanations in the prompts for OPT, GPT-3 (davinci), and InstructGPT (text-davinci-001) only yields small to moderate accuracy improvements over standard few-show learning. However, text-davinci-002 is able to benefit more substantially. We further show that explanations generated by the LLMs may not entail the models' predictions nor be factually grounded in the input, even on simple tasks with extractive explanations. However, these flawed explanations can still be useful as a way to verify LLMs' predictions post-hoc. Through analysis in our three settings, we show that explanations judged by humans to be good--logically consistent with the input and the prediction--more likely cooccur with accurate predictions. Following these observations, we train calibrators using automatically extracted scores that assess the reliability of explanations, allowing us to improve performance post-hoc across all of our datasets.",
            "year": 2022,
            "citationCount": 95,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work studies two NLP tasks that involve reasoning over text, namely question answering and natural language inference, and shows that explanations judged by humans to be good--logically consistent with the input and the prediction--more likely cooccur with accurate predictions."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve confidence calibration in large language models using a novel prompting approach called Calibrated Consensus Prompting (CCP), which generates multiple diverse responses and assesses their consensus to estimate the model's confidence.\n\nThe paper studies whether including explanations in prompts improves the performance of large language models on textual reasoning tasks like question answering and natural language inference. It also explores the reliability of explanations generated by LLMs and proposes using explanation reliability scores to improve performance post-hoc.\n\nThe project proposal and the paper address different research problems and propose different approaches. The project focuses on confidence calibration, while the paper investigates the impact of explanations on few-shot prompting performance and the reliability of LLM-generated explanations.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "e811e771f5950d86eafe50655c0d1e5b571e19b6",
            "paperId": "e811e771f5950d86eafe50655c0d1e5b571e19b6",
            "title": "The Unreliability of Explanations in Few-Shot In-Context Learning",
            "abstract": "How can prompting a large language model like GPT-3 with explanations improve in-context learning? We focus speci\ufb01cally on two NLP tasks that involve reasoning over text, namely question answering and natural language inference. Including explanations in the prompt and having the model generate them does not consistently improve performance in the settings we study, contrary to recent results on symbolic reasoning tasks (Nye et al., 2021; Wei et al., 2022). Despite careful prompting, explanations generated by GPT-3 may not even be factually grounded in the input, even on simple tasks with straightforward extractive explanations. However, these \ufb02awed explanations can still be useful as a way to verify GPT-3\u2019s predictions post-hoc. Through analysis in three settings, we show that explanations judged as good by humans\u2014those that are logically consistent with the input and the prediction\u2014usually indicate more accurate predictions. Following these observations, we present a framework for calibrating model predictions based on the reliability of the explanations. Our framework trains calibrators using automatically extracted scores that approximately assess the reliability of explanations, which helps improve performance across three different datasets",
            "year": 2022,
            "citationCount": 29,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A framework for calibrating model predictions based on the reliability of explanations is presented, showing that explanations judged as good by humans\u2014those that are logically consistent with the input and the prediction\u2014usually indicate more accurate predictions."
            },
            "score": 7,
            "novelty_score": "The project proposal aims to improve confidence calibration in large language models using a novel prompting approach called Calibrated Consensus Prompting (CCP). The paper, on the other hand, investigates the reliability of explanations in few-shot in-context learning and proposes a framework for calibrating model predictions based on the reliability of the explanations.\n\nProject proposal: Improving confidence calibration in LLMs using Calibrated Consensus Prompting (CCP).\nPaper: Investigating the reliability of explanations in few-shot in-context learning and calibrating predictions based on explanation reliability.\n\nWhile both works involve calibration, the project focuses on confidence calibration using a prompting approach, while the paper studies the reliability of explanations and uses them for calibrating predictions. The research problems and approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "33935c64228d249e20fb41ac9da7de85463c1ec4",
            "paperId": "33935c64228d249e20fb41ac9da7de85463c1ec4",
            "title": "PACE-LM: Prompting and Augmentation for Calibrated Confidence Estimation with GPT-4 in Cloud Incident Root Cause Analysis",
            "abstract": "Major cloud providers have employed advanced AI-based solutions like large language models to aid humans in identifying the root causes of cloud incidents. Despite the growing prevalence of AI-driven assistants in the root cause analysis process, their effectiveness in assisting on-call engineers is constrained by low accuracy due to the intrinsic difficulty of the task, a propensity for LLM-based approaches to hallucinate, and difficulties in distinguishing these well-disguised hallucinations. To address this challenge, we propose to perform confidence estimation for the predictions to help on-call engineers make decisions on whether to adopt the model prediction. Considering the black-box nature of many LLM-based root cause predictors, fine-tuning or temperature-scaling-based approaches are inapplicable. We therefore design an innovative confidence estimation framework based on prompting retrieval-augmented large language models (LLMs) that demand a minimal amount of information from the root cause predictor. This approach consists of two scoring phases: the LLM-based confidence estimator first evaluates its confidence in making judgments in the face of the current incident that reflects its ``grounded-ness\"level in reference data, then rates the root cause prediction based on historical references. An optimization step combines these two scores for a final confidence assignment. We show that our method is able to produce calibrated confidence estimates for predicted root causes, validate the usefulness of retrieved historical data and the prompting strategy as well as the generalizability across different root cause prediction models. Our study takes an important move towards reliably and effectively embedding LLMs into cloud incident management systems.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study takes an important move towards reliably and effectively embedding LLMs into cloud incident management systems by designing an innovative confidence estimation framework based on prompting retrieval-augmented large language models that demand a minimal amount of information from the root cause predictor."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve confidence calibration in large language models using a prompting approach called Calibrated Consensus Prompting (CCP), which generates multiple diverse responses and assesses their consensus to estimate confidence.\n\nThe paper focuses on confidence estimation for root cause predictions in cloud incident analysis using a prompting and retrieval-augmented approach that evaluates the model's confidence based on its groundedness in reference data and historical references.\n\nWhile both works address confidence estimation in language models, the project proposal targets the general problem of overconfidence in LLMs, while the paper specifically tackles confidence estimation for root cause predictions in the context of cloud incident analysis. Additionally, the proposed methods differ: the project uses consensus prompting, while the paper employs a retrieval-augmented approach.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "27dd800cb087f1575a65fba06c95ec8fd83a0fb4",
            "paperId": "27dd800cb087f1575a65fba06c95ec8fd83a0fb4",
            "title": "Fact-and-Reflection (FaR) Improves Confidence Calibration of Large Language Models",
            "abstract": "For a LLM to be trustworthy, its confidence level should be well-calibrated with its actual performance. While it is now common sense that LLM performances are greatly impacted by prompts, the confidence calibration in prompting LLMs has yet to be thoroughly explored. In this paper, we explore how different prompting strategies influence LLM confidence calibration and how it could be improved. We conduct extensive experiments on six prompting methods in the question-answering context and we observe that, while these methods help improve the expected LLM calibration, they also trigger LLMs to be over-confident when responding to some instances. Inspired by human cognition, we propose Fact-and-Reflection (FaR) prompting, which improves the LLM calibration in two steps. First, FaR elicits the known\"facts\"that are relevant to the input prompt from the LLM. And then it asks the model to\"reflect\"over them to generate the final answer. Experiments show that FaR prompting achieves significantly better calibration; it lowers the Expected Calibration Error by 23.5% on our multi-purpose QA tasks. Notably, FaR prompting even elicits the capability of verbally expressing concerns in less confident scenarios, which helps trigger retrieval augmentation for solving these harder instances.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Fact-and-Reflection prompting is proposed, which improves the LLM calibration in two steps, and even elicits the capability of verbally expressing concerns in less confident scenarios, which helps trigger retrieval augmentation for solving these harder instances."
            },
            "score": 6,
            "novelty_score": "The research problem in the proposal is improving confidence calibration in large language models, while the approach is using consensus among multiple generated responses to estimate confidence. The paper also aims to improve confidence calibration, but the approach is a two-step prompting method called Fact-and-Reflection (FaR) that first elicits relevant facts and then reflects on them to generate the answer.\n\nThe proposal focuses on generating multiple diverse responses and using their consensus to calibrate confidence, while the paper uses a specific prompting strategy to improve calibration. Although both address the same problem, their approaches are different.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "6bf34b4a1937ca5ae692594eda880ff671b8ee57",
            "paperId": "6bf34b4a1937ca5ae692594eda880ff671b8ee57",
            "title": "Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration",
            "abstract": "Membership Inference Attacks (MIA) aim to infer whether a target data record has been utilized for model training or not. Prior attempts have quantified the privacy risks of language models (LMs) via MIAs, but there is still no consensus on whether existing MIA algorithms can cause remarkable privacy leakage on practical Large Language Models (LLMs). Existing MIAs designed for LMs can be classified into two categories: reference-free and reference-based attacks. They are both based on the hypothesis that training records consistently strike a higher probability of being sampled. Nevertheless, this hypothesis heavily relies on the overfitting of target models, which will be mitigated by multiple regularization methods and the generalization of LLMs. The reference-based attack seems to achieve promising effectiveness in LLMs, which measures a more reliable membership signal by comparing the probability discrepancy between the target model and the reference model. However, the performance of reference-based attack is highly dependent on a reference dataset that closely resembles the training dataset, which is usually inaccessible in the practical scenario. Overall, existing MIAs are unable to effectively unveil privacy leakage over practical fine-tuned LLMs that are overfitting-free and private. We propose a Membership Inference Attack based on Self-calibrated Probabilistic Variation (SPV-MIA). Specifically, since memorization in LLMs is inevitable during the training process and occurs before overfitting, we introduce a more reliable membership signal, probabilistic variation, which is based on memorization rather than overfitting. Furthermore, we introduce a self-prompt approach, which constructs the dataset to fine-tune the reference model by prompting the target LLM itself. In this manner, the adversary can collect a dataset with a similar distribution from public APIs.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A Membership Inference Attack based on Self-calibrated Probabilistic Variation (SPV-MIA), which introduces a more reliable membership signal, probabilistic variation, which is based on memorization rather than overfitting in LLMs."
            },
            "score": 6,
            "novelty_score": "The project proposal aims to improve confidence calibration in large language models using a novel prompting approach called Calibrated Consensus Prompting (CCP). The paper, on the other hand, proposes a Membership Inference Attack based on Self-calibrated Probabilistic Variation (SPV-MIA) to unveil privacy leakage in fine-tuned large language models.\n\nThe project focuses on improving the reliability of LLMs' outputs by better calibrating their confidence, while the paper addresses the privacy risks associated with LLMs and proposes an attack to infer whether a target data record has been used for model training.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "6948b1147305a4e27540efd76858a0424b91b857",
            "paperId": "6948b1147305a4e27540efd76858a0424b91b857",
            "title": "Revisiting k-NN for Fine-tuning Pre-trained Language Models",
            "abstract": "\u201cPre-trained Language Models (PLMs), as parametric-based eager learners, have become thede-facto choice for current paradigms of Natural Language Processing (NLP). In contrast, k-Nearest-Neighbor (k-NN) classifiers, as the lazy learning paradigm, tend to mitigate over-fittingand isolated noise. In this paper, we revisit k-NN classifiers for augmenting the PLMs-based clas-sifiers. From the methodological level, we propose to adopt k-NN with textual representationsof PLMs in two steps: (1) Utilize k-NN as prior knowledge to calibrate the training process.(2) Linearly interpolate the probability distribution predicted by k-NN with that of the PLMs\u2019classifier. At the heart of our approach is the implementation of k-NN-calibrated training, whichtreats predicted results as indicators for easy versus hard examples during the training process.From the perspective of the diversity of application scenarios, we conduct extensive experimentson fine-tuning, prompt-tuning paradigms and zero-shot, few-shot and fully-supervised settings,respectively, across eight diverse end-tasks. We hope our exploration will encourage the commu-nity to revisit the power of classical methods for efficient NLP1.\u201d",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "K-NN classifiers for augmenting the PLMs-based clas-sifiers are revisited and the implementation of k-NN-calibrated training, which treats predicted results as indicators for easy versus hard examples during the training process, is implemented."
            },
            "score": 6,
            "novelty_score": "The research problem in the project proposal is improving confidence calibration in large language models, and the proposed approach is Calibrated Consensus Prompting (CCP), which generates multiple diverse responses and assesses their consensus to estimate the model's confidence.\n\nThe research problem in the paper is augmenting PLM-based classifiers with k-NN classifiers, and the proposed approach is to use k-NN with textual representations of PLMs to calibrate the training process and interpolate the probability distribution predicted by k-NN with that of the PLMs' classifier.\n\nWhile both works aim to improve certain aspects of language models, the project proposal focuses specifically on confidence calibration, while the paper focuses on augmenting PLM-based classifiers with k-NN. The approaches are also different, with the project proposal using consensus prompting and the paper using k-NN for calibration and interpolation.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "ab4ce5dda7ad4d9032995c9c049a89d65723c6aa",
            "paperId": "ab4ce5dda7ad4d9032995c9c049a89d65723c6aa",
            "title": "Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback",
            "abstract": "A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated. However, the most widely-used LMs are fine-tuned with reinforcement learning from human feedback (RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated. In light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50%.",
            "year": 2023,
            "citationCount": 96,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "For RLHF-LMs such as ChatGPT, GPT-4, and Claude, it is found that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA benchmarks, often reducing the expected calibration error by a relative 50%."
            },
            "score": 5,
            "novelty_score": "The project proposal aims to improve confidence calibration in large language models (LLMs) by using a novel prompting approach called Calibrated Consensus Prompting (CCP), which generates multiple diverse responses and assesses their consensus to estimate the model's confidence.\n\nThe paper focuses on evaluating methods for extracting well-calibrated confidence scores from LLMs fine-tuned with reinforcement learning from human feedback (RLHF-LMs), finding that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities.\n\nWhile both the project proposal and the paper address the issue of confidence calibration in LLMs, they propose different approaches. The project proposal introduces a novel prompting technique (CCP), while the paper evaluates existing methods for extracting confidence scores from RLHF-LMs.\n\nNo",
            "novelty_judgment": "no"
        },
        {
            "id": "036e96ed196a7f4bb812380f3b76ac75d4a648e4",
            "paperId": "036e96ed196a7f4bb812380f3b76ac75d4a648e4",
            "title": "Calibrating the Confidence of Large Language Models by Eliciting Fidelity",
            "abstract": "Large language models optimized with techniques like RLHF have achieved good alignment in being helpful and harmless. However, post-alignment, these language models often exhibit overconfidence, where the expressed confidence does not accurately calibrate with their correctness rate. In this paper, we decompose the language model confidence into the \\textit{Uncertainty} about the question and the \\textit{Fidelity} to the answer generated by language models. Then, we propose a plug-and-play method to estimate the confidence of language models. Our method has shown good calibration performance by conducting experiments with 6 RLHF-LMs on four MCQA datasets. Moreover, we propose two novel metrics, IPR and CE, to evaluate the calibration of the model, and we have conducted a detailed discussion on \\textit{Truly Well-Calibrated Confidence}. Our method could serve as a strong baseline, and we hope that this work will provide some insights into the model confidence calibration.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper decomposes the language model confidence into the uncertainty about the question and the fidelity to the answer generated by language models, and proposes a plug-and-play method to estimate the confidence of language models."
            },
            "score": 5
        },
        {
            "id": "b21670e8061a06ab97e7d6052c9345a326e84ff8",
            "paperId": "b21670e8061a06ab97e7d6052c9345a326e84ff8",
            "title": "UL2: Unifying Language Learning Paradigms",
            "abstract": "Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized&unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5&GPT-like models across multiple diverse setups. By scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised finetuning based NLP tasks. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. On 0-shot MMLU, UL2 20B outperforms T0 and T5 models. UL2 20B also works well with chain-of-thought prompting and reasoning, making it an appealing choice for research into reasoning at a small to medium scale of 20B parameters. Finally, we apply FLAN instruction tuning to the UL2 20B model, achieving MMLU and Big-Bench scores competitive to FLAN-PaLM 62B. We release Flax-based T5X checkpoints for the UL2 20B&Flan-UL2 20B.",
            "year": 2022,
            "citationCount": 181,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A unified framework for pre-training models that are universally effective across datasets and setups is presented and Mixture-of-Denoisers (MoD) is proposed, a pre- Training objective that combines diverse pre- training paradigms together."
            },
            "score": 5
        },
        {
            "id": "c4eed5d2d72f1ad63bd2bc96cc323ed3dd1fa38b",
            "paperId": "c4eed5d2d72f1ad63bd2bc96cc323ed3dd1fa38b",
            "title": "SELF-EXPLAIN: Teaching Large Language Models to Reason Complex Questions by Themselves",
            "abstract": "Large language models (LLMs) can generate intermediate reasoning steps. To elicit the reliable reasoning, the common practice is to employ few-shot chain-of-thought prompting, where several in-context demonstrations for reasoning are prepended to the question. However, such chain-of-thought examples are expensive to craft, especially for professional domains, and can have high variance depending on human annotators. Therefore, this work investigates whether LLMs can teach themselves to reason without human-crafted demonstrations. We propose SELF-EXPLAIN to generate CoT examples by LLMs inspired by\"encoding specificity\"in human memory retrieval. We find using self-explanations makes LLMs more confident, more calibrated and less biased when answering complex questions. Moreover, we find prompting with self-explanations can even significantly outperform using human-crafted CoTs on several complex question answering dataset.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes SELF-EXPLAIN to generate CoT examples by LLMs inspired by\"encoding specificity\" in human memory retrieval, and finds using self-explanations makes LLMs more confident, more calibrated and less biased when answering complex questions."
            },
            "score": 4
        },
        {
            "id": "8d4b9420bf3c8d81d2edbc5c8c4425e30033685f",
            "paperId": "8d4b9420bf3c8d81d2edbc5c8c4425e30033685f",
            "title": "C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion",
            "abstract": "In deep learning, test-time adaptation has gained attention as a method for model fine-tuning without the need for labeled data. A prime exemplification is the recently proposed test-time prompt tuning for large-scale vision-language models such as CLIP. Unfortunately, these prompts have been mainly developed to improve accuracy, overlooking the importance of calibration, which is a crucial aspect for quantifying prediction uncertainty. However, traditional calibration methods rely on substantial amounts of labeled data, making them impractical for test-time scenarios. To this end, this paper explores calibration during test-time prompt tuning by leveraging the inherent properties of CLIP. Through a series of observations, we find that the prompt choice significantly affects the calibration in CLIP, where the prompts leading to higher text feature dispersion result in better-calibrated predictions. Introducing the Average Text Feature Dispersion (ATFD), we establish its relationship with calibration error and present a novel method, Calibrated Test-time Prompt Tuning (C-TPT), for optimizing prompts during test-time with enhanced calibration. Through extensive experiments on different CLIP architectures and datasets, we show that C-TPT can effectively improve the calibration of test-time prompt tuning without needing labeled data. The code is publicly accessible at https://github.com/hee-suk-yoon/C-TPT.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that C-TPT can effectively improve the calibration of test-time prompt tuning without needing labeled data, and introduced the Average Text Feature Dispersion (ATFD), a novel method for optimizing prompts during test-time with enhanced calibration."
            },
            "score": 4
        },
        {
            "id": "ea8067ee6c3923b259bda0a07d3001e18c2d97bf",
            "paperId": "ea8067ee6c3923b259bda0a07d3001e18c2d97bf",
            "title": "Unsupervised Calibration through Prior Adaptation for Text Classification using Large Language Models",
            "abstract": "A wide variety of natural language tasks are currently being addressed with large-scale language models (LLMs). These models are usually trained with a very large amount of unsupervised text data and adapted to perform a downstream natural language task using methods like fine-tuning, calibration or in-context learning. In this work, we propose an approach to adapt the prior class distribution to perform text classification tasks without the need for labelled samples and only a few in-domain sample queries. The proposed approach treats the LLM as a black box, adding a stage where the model posteriors are calibrated to the task. Results show that these methods outperform the un-adapted model for different number of training shots in the prompt and a previous approach where calibration is performed without using any adaptation data.",
            "year": 2023,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work proposes an approach to adapt the prior class distribution to perform text classification tasks without the need for labelled samples and only a few in-domain sample queries, and shows that these methods outperform the un-adapted model for different number of training shots in the prompt."
            },
            "score": 4
        },
        {
            "id": "fd81018bc72b030545a2d3f3010f3758ec4d48c3",
            "paperId": "fd81018bc72b030545a2d3f3010f3758ec4d48c3",
            "title": "Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions",
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in various NLP tasks. However, previous works have shown these models are sensitive towards prompt wording, and few-shot demonstrations and their order, posing challenges to fair assessment of these models. As these models become more powerful, it becomes imperative to understand and address these limitations. In this paper, we focus on LLMs robustness on the task of multiple-choice questions -- commonly adopted task to study reasoning and fact-retrieving capability of LLMs. Investigating the sensitivity of LLMs towards the order of options in multiple-choice questions, we demonstrate a considerable performance gap of approximately 13% to 75% in LLMs on different benchmarks, when answer options are reordered, even when using demonstrations in a few-shot setting. Through a detailed analysis, we conjecture that this sensitivity arises when LLMs are uncertain about the prediction between the top-2/3 choices, and specific options placements may favor certain prediction between those top choices depending on the question caused by positional bias. We also identify patterns in top-2 choices that amplify or mitigate the model's bias toward option placement. We found that for amplifying bias, the optimal strategy involves positioning the top two choices as the first and last options. Conversely, to mitigate bias, we recommend placing these choices among the adjacent options. To validate our conjecture, we conduct various experiments and adopt two approaches to calibrate LLMs' predictions, leading to up to 8 percentage points improvement across different models and benchmarks.",
            "year": 2023,
            "citationCount": 30,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper investigates the sensitivity of LLMs towards the order of options in multiple-choice questions, and conjecture that this sensitivity arises when LLMs are uncertain about the prediction between the top-2/3 choices, and specific options placements may favor certain prediction between those top choices depending on the question caused by positional bias."
            },
            "score": 4
        },
        {
            "id": "6920de816acd201aadc0de51cf0fa62fa92bb0cc",
            "paperId": "6920de816acd201aadc0de51cf0fa62fa92bb0cc",
            "title": "On the Calibration of Large Language Models and Alignment",
            "abstract": "As large language models attract increasing attention and find widespread application, concurrent challenges of reliability also arise at the same time. Confidence calibration, an effective analysis method for gauging the reliability of deep models, serves as a crucial tool for assessing and improving their reliability. However, such investigation has been comparatively underexplored. In this work, we conduct a systematic examination of the calibration of aligned language models throughout the entire construction process, including pretraining and alignment training. At each stage, we investigate how different training settings, such as parameter scales and training data, affect model calibration. To thoroughly assess model calibration, we evaluate models on three most concerned aspects: generation, factuality and understanding. Our work sheds light on whether popular LLMs are well-calibrated and how the training process influences model calibration.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work sheds light on whether popular LLMs are well-calibrated and how the training process influences model calibration, as well as how different training settings affect model calibration."
            },
            "score": 4
        },
        {
            "id": "4feb412574eb5d0b187276069fe6024c22629c0e",
            "paperId": "4feb412574eb5d0b187276069fe6024c22629c0e",
            "title": "The Calibration Gap between Model and Human Confidence in Large Language Models",
            "abstract": "For large language models (LLMs) to be trusted by humans they need to be well-calibrated in the sense that they can accurately assess and communicate how likely it is that their predictions are correct. Recent work has focused on the quality of internal LLM confidence assessments, but the question remains of how well LLMs can communicate this internal model confidence to human users. This paper explores the disparity between external human confidence in an LLM's responses and the internal confidence of the model. Through experiments involving multiple-choice questions, we systematically examine human users' ability to discern the reliability of LLM outputs. Our study focuses on two key areas: (1) assessing users' perception of true LLM confidence and (2) investigating the impact of tailored explanations on this perception. The research highlights that default explanations from LLMs often lead to user overestimation of both the model's confidence and its' accuracy. By modifying the explanations to more accurately reflect the LLM's internal confidence, we observe a significant shift in user perception, aligning it more closely with the model's actual confidence levels. This adjustment in explanatory approach demonstrates potential for enhancing user trust and accuracy in assessing LLM outputs. The findings underscore the importance of transparent communication of confidence levels in LLMs, particularly in high-stakes applications where understanding the reliability of AI-generated information is essential.",
            "year": 2024,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "By modifying the explanations of large language models to more accurately reflect the LLM's internal confidence, a significant shift in user perception is observed, aligning it more closely with the model's actual confidence levels."
            },
            "score": 4
        },
        {
            "id": "47eb0468ba7b6457d32b6aa0ee15ad269c04864d",
            "paperId": "47eb0468ba7b6457d32b6aa0ee15ad269c04864d",
            "title": "Confidently Wrong: Exploring the Calibration and Expression of (Un)Certainty of Large Language Models in a Multilingual Setting",
            "abstract": "While the fluency and coherence of Large Language Models (LLMs) in text generation have seen significant improvements, their competency in generating appropriate expressions of uncertainty remains limited.Using a multilingual closed-book QA task and GPT-3.5, we explore how well LLMs are calibrated and express certainty across a diverse set of languages, including low-resource settings. Our results reveal strong performance in high-resource languages but a marked decline in performance in lower-resource languages. Across all, we observe an exaggerated expression of confidence in the model, which does not align with the correctness or likelihood of its responses. Our findings highlight the need for further research into accurate calibration of LLMs especially in a multilingual setting.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Using a multilingual closed-book QA task and GPT-3.5, how well LLMs are calibrated and express certainty across a diverse set of languages, including low-resource settings is explored."
            },
            "score": 4
        },
        {
            "id": "48fb667125298cf724f7b652d521686180412351",
            "paperId": "48fb667125298cf724f7b652d521686180412351",
            "title": "A Close Look into the Calibration of Pre-trained Language Models",
            "abstract": "Pre-trained language models (PLMs) may fail in giving reliable estimates of their predictive uncertainty. We take a close look into this problem, aiming to answer two questions: (1) Do PLMs learn to become calibrated in the training process? (2) How effective are existing calibration methods? For the first question, we conduct fine-grained control experiments to study the dynamic change in PLMs\u2019 calibration performance in training. We consider six factors as control variables, including dataset difficulty, available training samples, training steps, the number of tunable parameters, model scale, and pretraining. We observe a consistent change in calibration performance across six factors. We find that PLMs don\u2019t learn to become calibrated in training, evidenced by the continual increase in confidence, no matter whether the predictions are correct or not. We highlight that our finding somewhat contradicts two established conclusions: (a) Larger PLMs are more calibrated; (b) Pretraining improves model calibration. Next, we study the effectiveness of existing calibration methods in mitigating the overconfidence issue. Besides unlearnable calibration methods (e.g., label smoothing), we adapt and extend two recently proposed learnable methods that directly collect data to train models to have reasonable confidence estimations. Experimental results show that learnable methods significantly reduce PLMs\u2019 confidence in wrong predictions.",
            "year": 2022,
            "citationCount": 22,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is found that pre-trained language models don\u2019t learn to become calibrated in training, evidenced by the continual increase in confidence, no matter whether the predictions are correct or not."
            },
            "score": 4
        },
        {
            "id": "33422275fbb9958f55419620697faf531482699b",
            "paperId": "33422275fbb9958f55419620697faf531482699b",
            "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering",
            "abstract": "Abstract Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question, \u201cHow can we know when language models know, with confidence, the answer to a particular query?\u201d We examine this question from the point of view of calibration, the property of a probabilistic model\u2019s predicted probabilities actually being well correlated with the probabilities of correctness. We examine three strong generative models\u2014T5, BART, and GPT-2\u2014and study whether their probabilities on QA tasks are well calibrated, finding the answer is a relatively emphatic no. We then examine methods to calibrate such models to make their confidence scores correlate better with the likelihood of correctness through fine-tuning, post-hoc probability modification, or adjustment of the predicted outputs or inputs. Experiments on a diverse range of datasets demonstrate the effectiveness of our methods. We also perform analysis to study the strengths and limitations of these methods, shedding light on further improvements that may be made in methods for calibrating LMs. We have released the code at https://github.com/jzbjyb/lm-calibration.",
            "year": 2020,
            "citationCount": 233,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper examines three strong generative models -- T5, BART, and GPT-2 -- and examines methods to calibrate such models to make their confidence scores correlate better with the likelihood of correctness through fine-tuning, post-hoc probability modification, or adjustment of the predicted outputs or inputs."
            },
            "score": 4
        },
        {
            "id": "608a985e0cfb73aeca5e5e80bde5473bea1070dd",
            "paperId": "608a985e0cfb73aeca5e5e80bde5473bea1070dd",
            "title": "Evaluating Large Language Models for Document-grounded Response Generation in Information-Seeking Dialogues",
            "abstract": "In this paper, we investigate the use of large language models (LLMs) like ChatGPT for document-grounded response generation in the context of information-seeking dialogues. For evaluation, we use the MultiDoc2Dial corpus of task-oriented dialogues in four social service domains previously used in the DialDoc 2022 Shared Task. Information-seeking dialogue turns are grounded in multiple documents providing relevant information. We generate dialogue completion responses by prompting a ChatGPT model, using two methods: Chat-Completion and LlamaIndex. ChatCompletion uses knowledge from ChatGPT model pre-training while LlamaIndex also extracts relevant information from documents. Observing that document-grounded response generation via LLMs cannot be adequately assessed by automatic evaluation metrics as they are significantly more verbose, we perform a human evaluation where annotators rate the output of the shared task winning system, the two ChatGPT variants outputs, and human responses. While both ChatGPT variants are more likely to include information not present in the relevant segments, possibly including a presence of hallucinations, they are rated higher than both the shared task winning system and human responses.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper uses the MultiDoc2Dial corpus of task-oriented dialogues in four social service domains previously used in the DialDoc 2022 Shared Task to investigate the use of large language models (LLMs) like ChatGPT for document-grounded response generation in the context of information-seeking dialogues."
            },
            "score": 4
        },
        {
            "id": "421a3f9355c83e30f9b6fd0df2f295430fc1766d",
            "paperId": "421a3f9355c83e30f9b6fd0df2f295430fc1766d",
            "title": "Promptify: Text-to-Image Generation through Interactive Prompt Exploration with Large Language Models",
            "abstract": "Text-to-image generative models have demonstrated remarkable capabilities in generating high-quality images based on textual prompts. However, crafting prompts that accurately capture the user\u2019s creative intent remains challenging. It often involves laborious trial-and-error procedures to ensure that the model interprets the prompts in alignment with the user\u2019s intention. To address these challenges, we present Promptify, an interactive system that supports prompt exploration and refinement for text-to-image generative models. Promptify utilizes a suggestion engine powered by large language models to help users quickly explore and craft diverse prompts. Our interface allows users to organize the generated images flexibly, and based on their preferences, Promptify suggests potential changes to the original prompt. This feedback loop enables users to iteratively refine their prompts and enhance desired features while avoiding unwanted ones. Our user study shows that Promptify effectively facilitates the text-to-image workflow, allowing users to create visually appealing images on their first attempt while requiring significantly less cognitive load than a widely-used baseline tool.",
            "year": 2023,
            "citationCount": 13,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work presents Promptify, an interactive system that supports prompt exploration and refinement for text-to-image generative models, and utilizes a suggestion engine powered by large language models to help users quickly explore and craft diverse prompts."
            },
            "score": 4
        },
        {
            "id": "84c20d2ee6e87d2c8fdb3e11cf3ecf5c48881333",
            "paperId": "84c20d2ee6e87d2c8fdb3e11cf3ecf5c48881333",
            "title": "Prompt-Calibrated Tuning: Improving Black-Box Optimization for Few-Shot Scenarios",
            "abstract": "When there is limited downstream data available, using prompts can still yield good results. With prompts, large language models(LLMs) have already achieved success in numerous NLP tasks. Therefore, it is necessary to find the optimal prompts for each task. Due to the limitation of only having access to LLMs' API without the ability to perform backpropagation, we optimize the prompts using derivative-free optimization. In this paper, we propose Prompt-Calibrated Tuning(PCT), which modifies the outputs of large language models before prompt tuning and keeps their parameters frozen. Besides, we adopt the Roberta-large model as the backbone model, which might split a word into several subwords during tokenization, resulting in insufficient masking. Therefore, we utilize whole-word mask(wwm) in this paper. In addition, we have also expanded the label words. The experimental results show that Prompt-Calibrated Tuning is more effective than existing black-box optimization.",
            "year": 2023,
            "citationCount": 2,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 4
        },
        {
            "id": "d13dc7f94991e4cf7fba70c340b1d9d36346f238",
            "paperId": "d13dc7f94991e4cf7fba70c340b1d9d36346f238",
            "title": "Enhancing Emotional Generation Capability of Large Language Models via Emotional Chain-of-Thought",
            "abstract": "Large Language Models (LLMs) have shown remarkable performance in various emotion recognition tasks, thereby piquing the research community's curiosity for exploring their potential in emotional intelligence. However, several issues in the field of emotional generation tasks remain unresolved, including human preference alignment and emotional generation assessment. In this paper, we propose the Emotional Chain-of-Thought (ECoT), a plug-and-play prompting method that enhances the performance of LLMs on various emotional generation tasks by aligning with human emotional intelligence guidelines. To assess the reliability of ECoT, we propose an automated model-based evaluation method called Emotional Generation Score (EGS). EGS incorporates Goleman's Emotional Intelligence Theory as a consensus of human experts, providing a new perspective on the evaluation of emotional generation tasks. Extensive experimental results demonstrate the effectiveness of ECoT and EGS. Further, we discuss the promise of LLMs in the field of emotional intelligence and present key insights into the LLMs with the ECoT in emotional generation tasks.",
            "year": 2024,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Emotional Chain-of-Thought (ECoT) is proposed, a plug-and-play prompting method that enhances the performance of LLMs on various emotional generation tasks by aligning with human emotional intelligence guidelines."
            },
            "score": 3
        },
        {
            "id": "956d488dca9f032376d2711d98d07a1485cbdca4",
            "paperId": "956d488dca9f032376d2711d98d07a1485cbdca4",
            "title": "VLM-CPL: Consensus Pseudo Labels from Vision-Language Models for Human Annotation-Free Pathological Image Classification",
            "abstract": "Despite that deep learning methods have achieved remarkable performance in pathology image classification, they heavily rely on labeled data, demanding extensive human annotation efforts. In this study, we present a novel human annotation-free method for pathology image classification by leveraging pre-trained Vision-Language Models (VLMs). Without human annotation, pseudo labels of the training set are obtained by utilizing the zero-shot inference capabilities of VLM, which may contain a lot of noise due to the domain shift between the pre-training data and the target dataset. To address this issue, we introduce VLM-CPL, a novel approach based on consensus pseudo labels that integrates two noisy label filtering techniques with a semi-supervised learning strategy. Specifically, we first obtain prompt-based pseudo labels with uncertainty estimation by zero-shot inference with the VLM using multiple augmented views of an input. Then, by leveraging the feature representation ability of VLM, we obtain feature-based pseudo labels via sample clustering in the feature space. Prompt-feature consensus is introduced to select reliable samples based on the consensus between the two types of pseudo labels. By rejecting low-quality pseudo labels, we further propose High-confidence Cross Supervision (HCS) to learn from samples with reliable pseudo labels and the remaining unlabeled samples. Experimental results showed that our method obtained an accuracy of 87.1% and 95.1% on the HPH and LC25K datasets, respectively, and it largely outperformed existing zero-shot classification and noisy label learning methods. The code is available at https://github.com/lanfz2000/VLM-CPL.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study presents VLM-CPL, a novel approach based on consensus pseudo labels that integrates two noisy label filtering techniques with a semi-supervised learning strategy and proposes High-confidence Cross Supervision (HCS) to learn from samples with reliable pseudo labels and the remaining unlabeled samples."
            },
            "score": 3
        },
        {
            "id": "de1c7ae2818aa26fc86a0ea8ed70014cffc8b20a",
            "paperId": "de1c7ae2818aa26fc86a0ea8ed70014cffc8b20a",
            "title": "Fine-tuning language models to find agreement among humans with diverse preferences",
            "abstract": "Recent work in large language modeling (LLMs) has used fine-tuning to align outputs with the preferences of a prototypical user. This work assumes that human preferences are static and homogeneous across individuals, so that aligning to a a single\"generic\"user will confer more general alignment. Here, we embrace the heterogeneity of human preferences to consider a different challenge: how might a machine help people with diverse views find agreement? We fine-tune a 70 billion parameter LLM to generate statements that maximize the expected approval for a group of people with potentially diverse opinions. Human participants provide written opinions on thousands of questions touching on moral and political issues (e.g.,\"should we raise taxes on the rich?\"), and rate the LLM's generated candidate consensus statements for agreement and quality. A reward model is then trained to predict individual preferences, enabling it to quantify and rank consensus statements in terms of their appeal to the overall group, defined according to different aggregation (social welfare) functions. The model produces consensus statements that are preferred by human users over those from prompted LLMs (>70%) and significantly outperforms a tight fine-tuned baseline that lacks the final ranking step. Further, our best model's consensus statements are preferred over the best human-generated opinions (>65%). We find that when we silently constructed consensus statements from only a subset of group members, those who were excluded were more likely to dissent, revealing the sensitivity of the consensus to individual contributions. These results highlight the potential to use LLMs to help groups of humans align their values with one another.",
            "year": 2022,
            "citationCount": 107,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work fine-tunes a 70 billion parameter LLM to generate statements that maximize the expected approval for a group of people with potentially diverse opinions and produces consensus statements that are preferred by human users over those from prompted LLMs and significantly outperforms a tight fine-tuned baseline that lacks the final ranking step."
            },
            "score": 3
        },
        {
            "id": "35f0cf9f70c408dbaf106e6a675244e2867c164e",
            "paperId": "35f0cf9f70c408dbaf106e6a675244e2867c164e",
            "title": "PEARL: Personalizing Large Language Model Writing Assistants with Generation-Calibrated Retrievers",
            "abstract": "Powerful large language models have facilitated the development of writing assistants that promise to significantly improve the quality and efficiency of composition and communication. However, a barrier to effective assistance is the lack of personalization in LLM outputs to the author's communication style and specialized knowledge. In this paper, we address this challenge by proposing PEARL, a retrieval-augmented LLM writing assistant personalized with a generation-calibrated retriever. Our retriever is trained to select historic user-authored documents for prompt augmentation, such that they are likely to best personalize LLM generations for a user request. We propose two key novelties for training our retriever: 1) A training data selection method that identifies user requests likely to benefit from personalization and documents that provide that benefit; and 2) A scale-calibrating KL-divergence objective that ensures that our retriever closely tracks the benefit of a document for personalized generation. We demonstrate the effectiveness of PEARL in generating personalized workplace social media posts and Reddit comments. Finally, we showcase the potential of a generation-calibrated retriever to double as a performance predictor and further improve low-quality generations via LLM chaining.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper proposes PEARL, a retrieval-augmented LLM writing assistant personalized with a generation-calibrated retriever, and proposes two key novelties for training the retriever: a training data selection method that identifies user requests likely to benefit from personalization and documents that provide that benefit."
            },
            "score": 3
        },
        {
            "id": "92746dfa09dcad92ecf1e6272ebb300c1112b7eb",
            "paperId": "92746dfa09dcad92ecf1e6272ebb300c1112b7eb",
            "title": "Automatic Calibration and Error Correction for Large Language Models via Pareto Optimal Self-Supervision",
            "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities out of box for a wide range of applications, yet accuracy still remains a major growth area, especially in mission-critical domains such as biomedicine. An effective method to calibrate the con\ufb01dence level on LLM responses is essential to automatically detect errors and facilitate human-in-the-loop veri\ufb01cation. An important source of calibration signals stems from expert-stipulated programmatic super-vision, which is often available at low cost but has its own limitations such as noise and coverage. In this paper, we introduce a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every response, without any additional manual efforts. This is accomplished by learning a harmonizer model to align LLM output with other available supervision sources, which would assign higher risk scores to more uncertain LLM responses and facilitate error correction. Experiments on standard relation extraction tasks in biomedical and general domains demonstrate the promise of this approach, with our proposed risk scores highly correlated with the real error rate of LLMs. For the most uncertain test instances, dynamic prompting based on our proposed risk scores results in signi\ufb01cant accuracy improvement for off-the-shelf LLMs, boosting GPT-3 results past state-of-the-art (SOTA) weak supervision and GPT-4 results past SOTA supervised results on challenging evaluation datasets.",
            "year": 2023,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every response, without any additional manual efforts."
            },
            "score": 3
        },
        {
            "id": "9a61d51212eb4ff677fe777a7ba9ddc4f675b387",
            "paperId": "9a61d51212eb4ff677fe777a7ba9ddc4f675b387",
            "title": "Automatic Calibration and Error Correction for Generative Large Language Models via Pareto Optimal Self-Supervision",
            "abstract": "Generative Large language models (LLMs) have demonstrated remarkable capabilities for a wide range of applications, but reducing ungrounded or erroneous responses remains a major growth area. Unlike task-specific models, there lack an effective method to calibrate the confidence level of LLM responses to indicate potential errors and facilitate human-in-the-loop verification. An important source of calibration stems from expert-stipulated programmatic supervision, which is often available at low cost but has its own limitations such as noise and coverage. In this paper, we introduce a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every LLM response, without any additional manual efforts. This is accomplished by learning a harmonizer model to align with LLM output as well as other weak supervision sources. The model assigns higher risk scores to more uncertain LLM responses and facilitate error correction. Experiments on standard relation extraction and classification tasks in biomedical and general domains demonstrate that the proposed risk score is highly correlated with the actual LLM error rate. By using a dynamic prompting strategy based on the risk score, we observed significant accuracy improvement for off-the-shelf LLMs, boosting GPT-3.5 results past state-of-the-art (SOTA) weak supervision model and GPT-4 results past SOTA supervised results on challenging evaluation datasets.",
            "year": 2023,
            "citationCount": 3,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper introduces a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every LLM response, without any additional manual efforts."
            },
            "score": 3
        },
        {
            "id": "30669080bc6652f0466fba618b7c59317a346fb2",
            "paperId": "30669080bc6652f0466fba618b7c59317a346fb2",
            "title": "A Formalism and Approach for Improving Robustness of Large Language Models Using Risk-Adjusted Confidence Scores",
            "abstract": "Large Language Models (LLMs), such as ChatGPT, have achieved impressive milestones in natural language processing (NLP). Despite their impressive performance, the models are known to pose important risks. As these models are deployed in real-world applications, a systematic understanding of different risks posed by these models on tasks such as natural language inference (NLI), is much needed. In this paper, we define and formalize two distinct types of risk: decision risk and composite risk. We also propose a risk-centric evaluation framework, and four novel metrics, for assessing LLMs on these risks in both in-domain and out-of-domain settings. Finally, we propose a risk-adjusted calibration method called DwD for helping LLMs minimize these risks in an overall NLI architecture. Detailed experiments, using four NLI benchmarks, three baselines and two LLMs, including ChatGPT, show both the practical utility of the evaluation framework, and the efficacy of DwD in reducing decision and composite risk. For instance, when using DwD, an underlying LLM is able to address an extra 20.1% of low-risk inference tasks (but which the LLM erroneously deems high-risk without risk adjustment) and skip a further 19.8% of high-risk tasks, which would have been answered incorrectly.",
            "year": 2023,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper defines and formalizes two distinct types of risk: decision risk and composite risk, and proposes a risk-centric evaluation framework, and four novel metrics, for assessing LLMs on these risks in both in-domain and out-of-domain settings."
            },
            "score": 3
        },
        {
            "id": "a2b89d2196b4cc88797d4907ce7458bb7584f6b6",
            "paperId": "a2b89d2196b4cc88797d4907ce7458bb7584f6b6",
            "title": "On the Calibration of Massively Multilingual Language Models",
            "abstract": "Massively Multilingual Language Models (MMLMs) have recently gained popularity due to their surprising effectiveness in cross-lingual transfer. While there has been much work in evaluating these models for their performance on a variety of tasks and languages, little attention has been paid on how well calibrated these models are with respect to the confidence in their predictions. We first investigate the calibration of MMLMs in the zero-shot setting and observe a clear case of miscalibration in low-resource languages or those which are typologically diverse from English. Next, we empirically show that calibration methods like temperature scaling and label smoothing do reasonably well in improving calibration in the zero-shot scenario. We also find that few-shot examples in the language can further help reduce calibration errors, often substantially. Overall, our work contributes towards building more reliable multilingual models by highlighting the issue of their miscalibration, understanding what language and model-specific factors influence it, and pointing out the strategies to improve the same.",
            "year": 2022,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work investigates the calibration of MMLMs in the zero-shot setting and observes a clear case of miscalibration in low-resource languages or those which are typologically diverse from English, and empirically shows that calibration methods like temperature scaling and label smoothing do reasonably well in improving calibration in thezero-shot scenario."
            },
            "score": 3
        },
        {
            "id": "ba63e1ab5b6e9d849982ae293ac0483053badaff",
            "paperId": "ba63e1ab5b6e9d849982ae293ac0483053badaff",
            "title": "Uncertainty in Language Models: Assessment through Rank-Calibration",
            "abstract": "Language Models (LMs) have shown promising performance in natural language generation. However, as LMs often generate incorrect or hallucinated responses, it is crucial to correctly quantify their uncertainty in responding to given inputs. In addition to verbalized confidence elicited via prompting, many uncertainty measures ($e.g.$, semantic entropy and affinity-graph-based measures) have been proposed. However, these measures can differ greatly, and it is unclear how to compare them, partly because they take values over different ranges ($e.g.$, $[0,\\infty)$ or $[0,1]$). In this work, we address this issue by developing a novel and practical framework, termed $Rank$-$Calibration$, to assess uncertainty and confidence measures for LMs. Our key tenet is that higher uncertainty (or lower confidence) should imply lower generation quality, on average. Rank-calibration quantifies deviations from this ideal relationship in a principled manner, without requiring ad hoc binary thresholding of the correctness score ($e.g.$, ROUGE or METEOR). The broad applicability and the granular interpretability of our methods are demonstrated empirically.",
            "year": 2024,
            "citationCount": 1,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A novel and practical framework, termed $Rank$-$Calibration$ is developed, to assess uncertainty and confidence measures for LMs, with the key tenet that higher uncertainty should imply lower generation quality, on average."
            },
            "score": 3
        },
        {
            "id": "b58b319d2b3f933ae201f747dabb4b9ea070e50e",
            "paperId": "b58b319d2b3f933ae201f747dabb4b9ea070e50e",
            "title": "Linguistic Calibration of Language Models",
            "abstract": "Language models (LMs) may lead their users to make suboptimal downstream decisions when they confidently hallucinate. This issue can be mitigated by having the LM verbally convey the probability that its claims are correct, but existing models cannot produce text with calibrated confidence statements. Through the lens of decision-making, we formalize linguistic calibration for long-form generations: an LM is linguistically calibrated if its generations enable its users to make calibrated probabilistic predictions. This definition enables a training framework where a supervised finetuning step bootstraps an LM to emit long-form generations with confidence statements such as\"I estimate a 30% chance of...\"or\"I am certain that...\", followed by a reinforcement learning step which rewards generations that enable a user to provide calibrated answers to related questions. We linguistically calibrate Llama 2 7B and find in automated and human evaluations of long-form generations that it is significantly more calibrated than strong finetuned factuality baselines with comparable accuracy. These findings generalize under distribution shift on question-answering and under a significant task shift to person biography generation. Our results demonstrate that long-form generations may be calibrated end-to-end by constructing an objective in the space of the predictions that users make in downstream decision-making.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The results demonstrate that long-form generations may be calibrated end-to-end by constructing an objective in the space of the predictions that users make in downstream decision-making."
            },
            "score": 3
        },
        {
            "id": "1fb2bde5c2f3a3c4d7b810b29ec3f21f60e75d35",
            "paperId": "1fb2bde5c2f3a3c4d7b810b29ec3f21f60e75d35",
            "title": "Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Reliable Response Generation in Chinese",
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable success in diverse natural language processing (NLP) tasks in general domains. However, LLMs sometimes generate responses with the hallucination about medical facts due to limited domain knowledge. Such shortcomings pose potential risks in the utilization of LLMs within medical contexts. To address this challenge, we propose knowledge-tuning, which leverages structured medical knowledge bases for the LLMs to grasp domain knowledge efficiently and facilitate reliable response generation. We also release cMedKnowQA, a Chinese medical knowledge question-answering dataset constructed from medical knowledge bases to assess the medical knowledge proficiency of LLMs. Experimental results show that the LLMs which are knowledge-tuned with cMedKnowQA, can exhibit higher levels of accuracy in response generation compared with vanilla instruction-tuning and offer a new reliable way for the domain adaptation of LLMs.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Experimental results show that the LLMs which are knowledge-tuned with cMedKnowQA, can exhibit higher levels of accuracy in response generation compared with vanilla instruction-tuning and offer a new reliable way for the domain adaptation of LLMs."
            },
            "score": 3
        },
        {
            "id": "e47d08f5bb01acb56ab998e987d44d9d85dee1ba",
            "paperId": "e47d08f5bb01acb56ab998e987d44d9d85dee1ba",
            "title": "Harnessing the Power of Large Language Models for Empathetic Response Generation: Empirical Investigations and Improvements",
            "abstract": "Empathetic dialogue is an indispensable part of building harmonious social relationships and contributes to the development of a helpful AI. Previous approaches are mainly based on fine small-scale language models. With the advent of ChatGPT, the application effect of large language models (LLMs) in this field has attracted great attention. This work empirically investigates the performance of LLMs in generating empathetic responses and proposes three improvement methods of semantically similar in-context learning, two-stage interactive generation, and combination with the knowledge base. Extensive experiments show that LLMs can significantly benefit from our proposed methods and is able to achieve state-of-the-art performance in both automatic and human evaluations. Additionally, we explore the possibility of GPT-4 simulating human evaluators.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Empathetic dialogue is an indispensable part of building harmonious social relationships and contributes to the development of a helpful AI and three improvement methods of semantically similar in-context learning, two-stage interactive generation, and combination with the knowledge base are proposed."
            },
            "score": 3
        },
        {
            "id": "375a571174ea59b1f4aa62ad2619e9593fc03436",
            "paperId": "375a571174ea59b1f4aa62ad2619e9593fc03436",
            "title": "Large Language Models are Few-Shot Summarizers: Multi-Intent Comment Generation via In-Context Learning",
            "abstract": "Code comment generation aims at generating natural language descriptions for a code snippet to facilitate developers' program comprehension activities. Despite being studied for a long time, a bottleneck for existing approaches is that given a code snippet, they can only generate one comment while developers usually need to know information from diverse perspectives such as what is the functionality of this code snippet and how to use it. To tackle this limitation, this study empirically investigates the feasibility of utilizing large language models (LLMs) to generate comments that can fulfill developers' diverse intents. Our intuition is based on the facts that (1) the code and its pairwise comment are used during the pre-training process of LLMs to build the semantic connection between the natural language and programming language, and (2) comments in the real-world projects, which are collected for the pre-training, usually contain different developers' intents. We thus postulate that the LLMs can already understand the code from different perspectives after the pre-training. Indeed, experiments on two large-scale datasets demonstrate the rationale of our insights: by adopting the in-context learning paradigm and giving adequate prompts to the LLM (e.g., providing it with ten or more examples), the LLM can significantly outperform a state-of-the-art supervised learning approach on generating comments with multiple intents. Results also show that customized strategies for constructing the prompts and post-processing strategies for reranking the results can both boost the LLM's performances, which shed light on future research directions for using LLMs to achieve comment generation.",
            "year": 2023,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This study empirically investigates the feasibility of utilizing large language models (LLMs) to generate comments that can fulfill developers' diverse intents and shows that the LLM can significantly outperform a state-of-the-art supervised learning approach on generating comments with multiple intents."
            },
            "score": 3
        },
        {
            "id": "37383c30930391c1f67fb5817563c98f1889d3ef",
            "paperId": "37383c30930391c1f67fb5817563c98f1889d3ef",
            "title": "Variational Transformers for Diverse Response Generation",
            "abstract": "Despite the great promise of Transformers in many sequence modeling tasks (e.g., machine translation), their deterministic nature hinders them from generalizing to high entropy tasks such as dialogue response generation. Previous work proposes to capture the variability of dialogue responses with a recurrent neural network (RNN)-based conditional variational autoencoder (CVAE). However, the autoregressive computation of the RNN limits the training efficiency. Therefore, we propose the Variational Transformer (VT), a variational self-attentive feed-forward sequence model. The VT combines the parallelizability and global receptive field of the Transformer with the variational nature of the CVAE by incorporating stochastic latent variables into Transformers. We explore two types of the VT: 1) modeling the discourse-level diversity with a global latent variable; and 2) augmenting the Transformer decoder with a sequence of fine-grained latent variables. Then, the proposed models are evaluated on three conversational datasets with both automatic metric and human evaluation. The experimental results show that our models improve standard Transformers and other baselines in terms of diversity, semantic relevance, and human judgment.",
            "year": 2020,
            "citationCount": 47,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The Variational Transformer is proposed, a variational self-attentive feed-forward sequence model that combines the parallelizability and global receptive field of the Transformer with the variational nature of the CVAE by incorporating stochastic latent variables into Transformers."
            },
            "score": 3
        },
        {
            "id": "97010556749971d3e54039edb26fd47c713a735c",
            "paperId": "97010556749971d3e54039edb26fd47c713a735c",
            "title": "ETHICIST: Targeted Training Data Extraction Through Loss Smoothed Soft Prompting and Calibrated Confidence Estimation",
            "abstract": "Large pre-trained language models achieve impressive results across many tasks. However, recent works point out that pre-trained language models may memorize a considerable fraction of their training data, leading to the privacy risk of information leakage. In this paper, we propose a method named Ethicist for targeted training data extraction through loss smoothed soft prompting and calibrated confidence estimation, investigating how to recover the suffix in the training data when given a prefix. To elicit memorization in the attacked model, we tune soft prompt embeddings while keeping the model fixed. We further propose a smoothing loss that smooths the loss distribution of the suffix tokens to make it easier to sample the correct suffix. In order to select the most probable suffix from a collection of sampled suffixes and estimate the prediction confidence, we propose a calibrated confidence estimation method, which normalizes the confidence of the generated suffixes with a local estimation. We show that Ethicist significantly improves the extraction performance on a recently proposed public benchmark. We also investigate several factors influencing the data extraction performance, including decoding strategy, model scale, prefix length, and suffix length. Our code is availabel at https://github.com/thu-coai/Targeted-Data-Extraction.",
            "year": 2023,
            "citationCount": 8,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A method named Ethicist is proposed for targeted training data extraction through loss smoothed soft prompting and calibrated confidence estimation, investigating how to recover the suffix in the training data when given a prefix."
            },
            "score": 2
        },
        {
            "id": "850b71c61d9fa4a153eee6e2c912c16ecc459b7a",
            "paperId": "850b71c61d9fa4a153eee6e2c912c16ecc459b7a",
            "title": "Debiasing Vision-Language Models via Biased Prompts",
            "abstract": "Machine learning models have been shown to inherit biases from their training datasets. This can be particularly problematic for vision-language foundation models trained on uncurated datasets scraped from the internet. The biases can be amplified and propagated to downstream applications like zero-shot classifiers and text-to-image generative models. In this study, we propose a general approach for debiasing vision-language foundation models by projecting out biased directions in the text embedding. In particular, we show that debiasing only the text embedding with a calibrated projection matrix suffices to yield robust classifiers and fair generative models. The proposed closed-form solution enables easy integration into large-scale pipelines, and empirical results demonstrate that our approach effectively reduces social bias and spurious correlation in both discriminative and generative vision-language models without the need for additional data or training.",
            "year": 2023,
            "citationCount": 43,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is shown that debiasing only the text embedding with a calibrated projection matrix suffices to yield robust classifiers and fair generative models and effectively reduces social bias and spurious correlation in both discriminative and generative vision-language models without the need for additional data or training."
            },
            "score": 2
        },
        {
            "id": "c6be8510ea66521cf9d48befce4b012ac0cb0aea",
            "paperId": "c6be8510ea66521cf9d48befce4b012ac0cb0aea",
            "title": "pFedPrompt: Learning Personalized Prompt for Vision-Language Models in Federated Learning",
            "abstract": "Pre-trained vision-language models like CLIP show great potential in learning representations that capture latent characteristics of users. A recently proposed method called Contextual Optimization (CoOp) introduces the concept of training prompt for adapting pre-trained vision-language models. Given the lightweight nature of this method, researchers have migrated the paradigm from centralized to decentralized system to innovate the collaborative training framework of Federated Learning (FL). However, current prompt training in FL mainly focuses on modeling user consensus and lacks the adaptation to user characteristics, leaving the personalization of prompt largely under-explored. Researches over the past few years have applied personalized FL (pFL) approaches to customizing models for heterogeneous users. Unfortunately, we find that with the variation of modality and training behavior, directly applying the pFL methods to prompt training leads to insufficient personalization and performance. To bridge the gap, we present pFedPrompt, which leverages the unique advantage of multimodality in vision-language models by learning user consensus from linguistic space and adapting to user characteristics in visual space in a non-parametric manner. Through this dual collaboration, the learned prompt will be fully personalized and aligned to the user\u2019s local characteristics. We conduct extensive experiments across various datasets under the FL setting with statistical heterogeneity. The results demonstrate the superiority of our pFedPrompt against the alternative approaches with robust performance.",
            "year": 2023,
            "citationCount": 16,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 2
        },
        {
            "id": "05f6628948f79d0cce8664cc8146fd459d53e9d5",
            "paperId": "05f6628948f79d0cce8664cc8146fd459d53e9d5",
            "title": "On the Calibration of Pre-trained Language Models using Mixup Guided by Area Under the Margin and Saliency",
            "abstract": "A well-calibrated neural model produces confidence (probability outputs) closely approximated by the expected accuracy. While prior studies have shown that mixup training as a data augmentation technique can improve model calibration on image classification tasks, little is known about using mixup for model calibration on natural language understanding (NLU) tasks. In this paper, we explore mixup for model calibration on several NLU tasks and propose a novel mixup strategy for pre-trained language models that improves model calibration further. Our proposed mixup is guided by both the Area Under the Margin (AUM) statistic (Pleiss et al., 2020) and the saliency map of each sample (Simonyan et al., 2013). Moreover, we combine our mixup strategy with model miscalibration correction techniques (i.e., label smoothing and temperature scaling) and provide detailed analyses of their impact on our proposed mixup. We focus on systematically designing experiments on three NLU tasks: natural language inference, paraphrase detection, and commonsense reasoning. Our method achieves the lowest expected calibration error compared to strong baselines on both in-domain and out-of-domain test samples while maintaining competitive accuracy.",
            "year": 2022,
            "citationCount": 27,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper systematically designs experiments on three NLU tasks and proposes a novel mixup strategy for pre-trained language models that improves model calibration further and achieves the lowest expected calibration error compared to strong baselines on both in-domain and out-of-domain test samples while maintaining competitive accuracy."
            },
            "score": 2
        },
        {
            "id": "208d9e72a80c9333c36f8ede204128e3c808af84",
            "paperId": "208d9e72a80c9333c36f8ede204128e3c808af84",
            "title": "C3: Confidence Calibration Model Cascade for Inference-Efficient Cross-Lingual Natural Language Understanding",
            "abstract": "Cross-lingual natural language understanding (NLU) is a critical task in natural language processing (NLP). Recent advancements have seen multilingual pre-trained language models (mPLMs) significantly enhance the performance of these tasks. However, mPLMs necessitate substantial resources and incur high computational costs during inference, posing challenges for deployment in real-world and real-time systems. Existing model cascade methods seek to enhance inference efficiency by greedily selecting the lightest model capable of processing the current input from a variety of models, based on model confidence scores. Nonetheless, deep models tend to exhibit overconfidence, and confidence distributions vary across languages. This leads to the emission of confident but incorrect predictions by smaller models, hindering their ability to generalize effectively across test languages. In this study, we introduce a confidence calibration model cascade ($C^3$) method. This approach, simple yet effective, involves calibration prior to cascade inference, thereby enhancing cascade accuracy through more reliable predictions. Extensive experiments conducted on three cross-lingual benchmarks demonstrate that $C^3$ significantly outperforms all state-of-the-art baselines.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This approach involves calibration prior to cascade inference, thereby enhancing cascade accuracy through more reliable predictions, and significantly outperforms all state-of-the-art baselines."
            },
            "score": 2
        },
        {
            "id": "77b4e11cf494be085f506cdc4ab77946b07b6b52",
            "paperId": "77b4e11cf494be085f506cdc4ab77946b07b6b52",
            "title": "Open-Vocabulary Calibration for Vision-Language Models",
            "abstract": "Vision-language models (VLMs) have emerged as formidable tools, showing their strong capability in handling various open-vocabulary tasks in image recognition, text-driven visual content generation, and visual chatbots, to name a few. In recent years, considerable efforts and resources have been devoted to adaptation methods for improving downstream performance of VLMs, particularly on parameter-efficient fine-tuning methods like prompt learning. However, a crucial aspect that has been largely overlooked is the confidence calibration problem in fine-tuned VLMs, which could greatly reduce reliability when deploying such models in the real world. This paper bridges the gap by systematically investigating the confidence calibration problem in the context of prompt learning and reveals that existing calibration methods are insufficient to address the problem, especially in the open-vocabulary setting. To solve the problem, we present a simple and effective approach called Distance-Aware Calibration (DAC), which is based on scaling the temperature using as guidance the distance between predicted text labels and base classes. The experiments with 7 distinct prompt learning methods applied across 11 diverse downstream datasets demonstrate the effectiveness of DAC, which achieves high efficacy without sacrificing the inference speed.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A simple and effective approach called Distance-Aware Calibration (DAC) is presented, based on scaling the temperature using as guidance the distance between predicted text labels and base classes, which achieves high efficacy without sacrificing the inference speed."
            },
            "score": 2
        },
        {
            "id": "05301eb9dc89a4f75ba601c1fddf3d5fb868ab35",
            "paperId": "05301eb9dc89a4f75ba601c1fddf3d5fb868ab35",
            "title": "When Quantization Affects Confidence of Large Language Models?",
            "abstract": "Recent studies introduced effective compression techniques for Large Language Models (LLMs) via post-training quantization or low-bit weight representation. Although quantized weights offer storage efficiency and allow for faster inference, existing works have indicated that quantization might compromise performance and exacerbate biases in LLMs. This study investigates the confidence and calibration of quantized models, considering factors such as language model type and scale as contributors to quantization loss. Firstly, we reveal that quantization with GPTQ to 4-bit results in a decrease in confidence regarding true labels, with varying impacts observed among different language models. Secondly, we observe fluctuations in the impact on confidence across different scales. Finally, we propose an explanation for quantization loss based on confidence levels, indicating that quantization disproportionately affects samples where the full model exhibited low confidence levels in the first place.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is revealed that quantization with GPTQ to 4-bit results in a decrease in confidence regarding true labels, with varying impacts observed among different language models, and an explanation for quantization loss based on confidence levels is proposed."
            },
            "score": 2
        },
        {
            "id": "99832586d55f540f603637e458a292406a0ed75d",
            "paperId": "99832586d55f540f603637e458a292406a0ed75d",
            "title": "LANGUAGE MODELS",
            "abstract": "While large language models (LLMs) have demonstrated impressive performance across tasks in language understanding and interactive decision making, their abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation) have primarily been studied as separate topics. In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner, allowing for greater synergy between the two: reasoning traces help the model induce, track, and update action plans as well as handle exceptions, while actions allow it to interface with and gather additional information from external sources such as knowledge bases or environments. We apply our approach, named ReAct, to a diverse set of language and decision making tasks and demonstrate its effectiveness over state-of-the-art baselines in addition to improved human interpretability and trustworthiness. Concretely, on question answering (HotpotQA) and fact verification (Fever), ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces. Furthermore, on two interactive decision making benchmarks (ALFWorld and WebShop), ReAct outperforms imitation and reinforcement learning methods by an absolute success rate of 34% and 10% respectively, while being prompted with only one or two in-context examples.",
            "year": 2023,
            "citationCount": 600,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ReAct overcomes prevalent issues of hallucination and error propagation in chain-of-thought reasoning by interacting with a simple Wikipedia API, and generating human-like task-solving trajectories that are more interpretable than baselines without reasoning traces."
            },
            "score": 2
        },
        {
            "id": "cbd81507197e2d32b6f6c8ac99039f3a607ee8f1",
            "paperId": "cbd81507197e2d32b6f6c8ac99039f3a607ee8f1",
            "title": "VerilogEval: Evaluating Large Language Models for Verilog Code Generation",
            "abstract": "The increasing popularity of large language models (LLMs) has paved the way for their application in diverse domains. This paper proposes a benchmarking framework tailored specifically for evaluating LLM performance in the context of Verilog code generation for hardware design and verification. We present a comprehensive evaluation dataset consisting of 156 problems from the Verilog instructional website HDLBits. The evaluation set consists of a diverse set of Verilog code generation tasks, ranging from simple combinational circuits to complex finite state machines. The Verilog code completions can be automatically tested for functional correctness by comparing the transient simulation outputs of the generated design with a golden solution. We also demonstrate that the Verilog code generation capability of pretrained language models could be improved with supervised fine-tuning by bootstrapping with LLM generated synthetic problem-code pairs.",
            "year": 2023,
            "citationCount": 20,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A benchmarking framework tailored specifically for evaluating LLM performance in the context of Verilog code generation for hardware design and verification and it is demonstrated that the Verilogs code generation capability of pretrained language models could be improved with supervised fine-tuning by bootstrapping with LLM generated synthetic problem-code pairs."
            },
            "score": 2
        },
        {
            "id": "7e32aac43e9f1df49e116add03327ee6f365dbf3",
            "paperId": "7e32aac43e9f1df49e116add03327ee6f365dbf3",
            "title": "mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality",
            "abstract": "Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tune a low-rank adaption (LoRA) module on LLM and the abstractor module by freezing the visual knowledge module. We carefully build a visually-related instruction evaluation set OwlEval. Experimental results show that our model outperforms existing multi-modal models, demonstrating mPLUG-Owl's impressive instruction and visual understanding ability, multi-turn conversation ability, and knowledge reasoning ability. Besides, we observe some unexpected and exciting abilities such as multi-image correlation and scene text understanding, which makes it possible to leverage it for harder real scenarios, such as vision-only document comprehension. Our code, pre-trained model, instruction-tuned models, and evaluation set are available at https://github.com/X-PLUG/mPLUG-Owl. The online demo is available at https://www.modelscope.cn/studios/damo/mPLUG-Owl.",
            "year": 2023,
            "citationCount": 419,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": null
            },
            "score": 2
        },
        {
            "id": "0bfc804e31eecfd77f45e4ee7f4d629fffdcd628",
            "paperId": "0bfc804e31eecfd77f45e4ee7f4d629fffdcd628",
            "title": "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs",
            "abstract": "Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.",
            "year": 2023,
            "citationCount": 206,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ToolLLM is introduced, a general tool-use framework encompassing data construction, model training, and evaluation, and a novel depth-first search-based decision tree algorithm that enables LLMs to evaluate multiple reasoning traces and expand the search space."
            },
            "score": 2
        },
        {
            "id": "47030369e97cc44d4b2e3cf1be85da0fd134904a",
            "paperId": "47030369e97cc44d4b2e3cf1be85da0fd134904a",
            "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models",
            "abstract": "Because\"out-of-the-box\"large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called\"jailbreaks\"against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods. Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.",
            "year": 2023,
            "citationCount": 386,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information."
            },
            "score": 2
        },
        {
            "id": "dfbfa21a93c3164ae8a033398c8de42b03b1b84d",
            "paperId": "dfbfa21a93c3164ae8a033398c8de42b03b1b84d",
            "title": "ChatGPT Beyond English: Towards a Comprehensive Evaluation of Large Language Models in Multilingual Learning",
            "abstract": "Over the last few years, large language models (LLMs) have emerged as the most important breakthroughs in natural language processing (NLP) that fundamentally transform research and developments in the field. ChatGPT represents one of the most exciting LLM systems developed recently to showcase impressive skills for language generation and highly attract public attention. Among various exciting applications discovered for ChatGPT in English, the model can process and generate texts for multiple languages due to its multilingual training data. Given the broad adoption of ChatGPT for English in different problems and areas, a natural question is whether ChatGPT can also be applied effectively for other languages or it is necessary to develop more language-specific technologies. The answer to this question requires a thorough evaluation of ChatGPT over multiple tasks with diverse languages and large datasets (i.e., beyond reported anecdotes), which is still missing or limited in current research. Our work aims to fill this gap for the evaluation of ChatGPT and similar LLMs to provide more comprehensive information for multilingual NLP applications. While this work will be an ongoing effort to include additional experiments in the future, our current paper evaluates ChatGPT on 7 different tasks, covering 37 diverse languages with high, medium, low, and extremely low resources. We also focus on the zero-shot learning setting for ChatGPT to improve reproducibility and better simulate the interactions of general users. Compared to the performance of previous models, our extensive experimental results demonstrate a worse performance of ChatGPT for different NLP tasks and languages, calling for further research to develop better models and understanding for multilingual learning.",
            "year": 2023,
            "citationCount": 137,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Compared to the performance of previous models, extensive experimental results demonstrate a worse performance of ChatGPT for different NLP tasks and languages, calling for further research to develop better models and understanding for multilingual learning."
            },
            "score": 2
        },
        {
            "id": "17170575aa8b4fa4e3eef5d366ada706a94dd836",
            "paperId": "17170575aa8b4fa4e3eef5d366ada706a94dd836",
            "title": "LaMP: When Large Language Models Meet Personalization",
            "abstract": "This paper highlights the importance of personalization in large language models and introduces the LaMP benchmark -- a novel benchmark for training and evaluating language models for producing personalized outputs. LaMP offers a comprehensive evaluation framework with diverse language tasks and multiple entries for each user profile. It consists of seven personalized tasks, spanning three text classification and four text generation tasks. We additionally propose two retrieval augmentation approaches that retrieve personal items from each user profile for personalizing language model outputs. To this aim, we study various retrieval models, including term matching, semantic matching, and time-aware methods. Extensive experiments on LaMP for zero-shot and fine-tuned language models demonstrate the efficacy of the proposed retrieval augmentation approach and highlight the impact of personalization in various natural language tasks.",
            "year": 2023,
            "citationCount": 71,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The LaMP benchmark is introduced -- a novel benchmark for training and evaluating language models for producing personalized outputs and two retrieval augmentation approaches that retrieve personal items from each user profile for personalizing language model outputs are proposed."
            },
            "score": 2
        },
        {
            "id": "d53f4cdddd7494d5f6e64d81f627691f6d7dff95",
            "paperId": "d53f4cdddd7494d5f6e64d81f627691f6d7dff95",
            "title": "Think Outside the Code: Brainstorming Boosts Large Language Models in Code Generation",
            "abstract": "Code generation aims to automatically generate source code from high-level task specifications, which can significantly increase productivity of software engineering. Recently, approaches based on large language models (LLMs) have shown remarkable code generation abilities on simple tasks. However, generate code for more complex tasks, such as competition-level problems, remains challenging. In this paper, we introduce Brainstorm framework for code generation. It leverages a brainstorming step that generates and selects diverse thoughts on the problem to facilitate algorithmic reasoning, where the thoughts are possible blueprint of solving the problem. We demonstrate that Brainstorm significantly enhances the ability of LLMs to solve competition-level programming problems, resulting in a more than 50% increase in the pass@$k$ metrics for ChatGPT on the CodeContests benchmark, achieving state-of-the-art performance. Furthermore, our experiments conducted on LeetCode contests show that our framework boosts the ability of ChatGPT to a level comparable to that of human programmers.",
            "year": 2023,
            "citationCount": 11,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is demonstrated that Brainstorm significantly enhances the ability of LLMs to solve competition-level programming problems, resulting in a more than 50% increase in the pass@$k$ metrics for ChatGPT on the CodeContests benchmark, achieving state-of-the-art performance."
            },
            "score": 2
        },
        {
            "id": "c91f6eb320c70e2f64b6fb935494978a8699f06a",
            "paperId": "c91f6eb320c70e2f64b6fb935494978a8699f06a",
            "title": "An Empirical Study on Using Large Language Models to Analyze Software Supply Chain Security Failures",
            "abstract": "As we increasingly depend on software systems, the consequences of breaches in the software supply chain become more severe. High-profile cyber attacks like SolarWinds and ShadowHammer have resulted in significant financial and data losses, underlining the need for stronger cybersecurity. One way to prevent future breaches is by studying past failures. However, traditional methods of analyzing past failures require manually reading and summarizing reports about them. Automated support could reduce costs and allow analysis of more failures. Natural Language Processing (NLP) techniques such as Large Language Models (LLMs) could be leveraged to assist the analysis of failures. In this study, we assessed the ability of Large Language Models (LLMs) to analyze historical software supply chain breaches. We used LLMs to replicate the manual analysis of 69 software supply chain security failures performed by members of the Cloud Native Computing Foundation (CNCF). We developed prompts for LLMs to categorize these by four dimensions: type of compromise, intent, nature, and impact. GPT 3.5's categorizations had an average accuracy of 68% and Bard's had an accuracy of 58% over these dimensions. We report that LLMs effectively characterize software supply chain failures when the source articles are detailed enough for consensus among manual analysts, but cannot yet replace human analysts. Future work can improve LLM performance in this context, and study a broader range of articles and failures.",
            "year": 2023,
            "citationCount": 4,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "It is reported that LLMs effectively characterize software supply chain failures when the source articles are detailed enough for consensus among manual analysts, but cannot yet replace human analysts."
            },
            "score": 2
        },
        {
            "id": "b67eb8213a63be8a4b0274728ffdc50bfa109e10",
            "paperId": "b67eb8213a63be8a4b0274728ffdc50bfa109e10",
            "title": "XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models",
            "abstract": "Without proper safeguards, large language models will readily follow malicious instructions and generate toxic content. This risk motivates safety efforts such as red-teaming and large-scale feedback learning, which aim to make models both helpful and harmless. However, there is a tension between these two objectives, since harmlessness requires models to refuse to comply with unsafe prompts, and thus not be helpful. Recent anecdotal evidence suggests that some models may have struck a poor balance, so that even clearly safe prompts are refused if they use similar language to unsafe prompts or mention sensitive topics. In this paper, we introduce a new test suite called XSTest to identify such eXaggerated Safety behaviours in a systematic way. XSTest comprises 250 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with, and 200 unsafe prompts as contrasts that models, for most applications, should refuse. We describe XSTest's creation and composition, and then use the test suite to highlight systematic failure modes in state-of-the-art language models as well as more general challenges in building safer language models.",
            "year": 2023,
            "citationCount": 26,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "A new test suite called XSTest is introduced to identify eXaggerated Safety behaviours in state-of-the-art language models as well as more general challenges in building safer language models."
            },
            "score": 1
        },
        {
            "id": "02f6ac9e84aa87247dfe1480f42072f9ff45aff5",
            "paperId": "02f6ac9e84aa87247dfe1480f42072f9ff45aff5",
            "title": "Using Large Language Models for OntoClean-based Ontology Refinement",
            "abstract": "This paper explores the integration of Large Language Models (LLMs) such as GPT-3.5 and GPT-4 into the ontology refinement process, specifically focusing on the OntoClean methodology. OntoClean, critical for assessing the metaphysical quality of ontologies, involves a two-step process of assigning meta-properties to classes and verifying a set of constraints. Manually conducting the first step proves difficult in practice, due to the need for philosophical expertise and lack of consensus among ontologists. By employing LLMs with two prompting strategies, the study demonstrates that high accuracy in the labelling process can be achieved. The findings suggest the potential for LLMs to enhance ontology refinement, proposing the development of plugin software for ontology tools to facilitate this integration.",
            "year": 2024,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "This paper explores the integration of Large Language Models such as GPT-3.5 and GPT-4 into the ontology refinement process, specifically focusing on the OntoClean methodology, and suggests the potential for LLMs to enhance ontology refinement."
            },
            "score": 1
        },
        {
            "id": "1554d7e72a8b5bcad108ff1d0c9014ddfaaebd0f",
            "paperId": "1554d7e72a8b5bcad108ff1d0c9014ddfaaebd0f",
            "title": "Safety of Large Language Models in Addressing Depression",
            "abstract": "Background Generative artificial intelligence (AI) models, exemplified by systems such as ChatGPT, Bard, and Anthropic, are currently under intense investigation for their potential to address existing gaps in mental health support. One implementation of these large language models involves the development of mental health-focused conversational agents, which utilize pre-structured prompts to facilitate user interaction without requiring specialized knowledge in prompt engineering. However, uncertainties persist regarding the safety and efficacy of these agents in recognizing severe depression and suicidal tendencies. Given the well-established correlation between the severity of depression and the risk of suicide, improperly calibrated conversational agents may inadequately identify and respond to crises. Consequently, it is crucial to investigate whether publicly accessible repositories of mental health-focused conversational agents can consistently and safely address crisis scenarios before considering their adoption in clinical settings. This study assesses the safety of publicly available ChatGPT-3.5 conversational agents by evaluating their responses to a patient simulation indicating worsening depression and suicidality. Methodology This study evaluated ChatGPT-3.5 conversational agents on a publicly available repository specifically designed for mental health counseling. Each conversational agent was evaluated twice by a highly structured patient simulation. First, the simulation indicated escalating suicide risk based on the Patient Health Questionnaire (PHQ-9). For the second patient simulation, the escalating risk was presented in a more generalized manner not associated with an existing risk scale to assess the more generalized ability of the conversational agent to recognize suicidality. Each simulation recorded the exact point at which the conversational agent recommended human support. Then, the simulation continued until the conversational agent stopped entirely and shut down completely, insisting on human intervention. Results All 25 agents available on the public repository FlowGPT.com were evaluated. The point at which the conversational agents referred to a human occurred around the mid-point of the simulation, and definitive shutdown predominantly only happened at the highest risk levels. For the PHQ-9 simulation, the average initial referral and shutdown aligned with PHQ-9 scores of 12 (moderate depression) and 25 (severe depression). Few agents included crisis resources - only two referenced suicide hotlines. Despite the conversational agents insisting on human intervention, 22 out of 25 agents would eventually resume the dialogue if the simulation reverted to a lower risk level. Conclusions Current generative AI-based conversational agents are slow to escalate mental health risk scenarios, postponing referral to a human to potentially dangerous levels. More rigorous testing and oversight of conversational agents are needed before deployment in mental healthcare settings. Additionally, further investigation should explore if sustained engagement worsens outcomes and whether enhanced accessibility outweighs the risks of improper escalation. Advancing AI safety in mental health remains imperative as these technologies continue rapidly advancing.",
            "year": 2023,
            "citationCount": 6,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Current generative AI-based conversational agents are slow to escalate mental health risk scenarios, postponing referral to a human to potentially dangerous levels."
            },
            "score": 1
        },
        {
            "id": "835c305e52769a8433f8383e91d33ba6c66ad55b",
            "paperId": "835c305e52769a8433f8383e91d33ba6c66ad55b",
            "title": "Large language models generate functional protein sequences across diverse families",
            "abstract": null,
            "year": 2023,
            "citationCount": 287,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "ProGen is described, a language model that can generate protein sequences with a predictable function across large protein families, akin to generating grammatically and semantically correct natural language sentences on diverse topics."
            },
            "score": 1
        },
        {
            "id": "bbe197158adb4b6e85a6eeab4619ea0fc6857941",
            "paperId": "bbe197158adb4b6e85a6eeab4619ea0fc6857941",
            "title": "MarioGPT: Open-Ended Text2Level Generation through Large Language Models",
            "abstract": "Procedural Content Generation (PCG) algorithms provide a technique to generate complex and diverse environments in an automated way. However, while generating content with PCG methods is often straightforward, generating meaningful content that reflects specific intentions and constraints remains challenging. Furthermore, many PCG algorithms lack the ability to generate content in an open-ended manner. Recently, Large Language Models (LLMs) have shown to be incredibly effective in many diverse domains. These trained LLMs can be fine-tuned, re-using information and accelerating training for new tasks. In this work, we introduce MarioGPT, a fine-tuned GPT2 model trained to generate tile-based game levels, in our case Super Mario Bros levels. We show that MarioGPT can not only generate diverse levels, but can be text-prompted for controllable level generation, addressing one of the key challenges of current PCG techniques. As far as we know, MarioGPT is the first text-to-level model. We also combine MarioGPT with novelty search, enabling it to generate diverse levels with varying play-style dynamics (i.e. player paths). This combination allows for the open-ended generation of an increasingly diverse range of content.",
            "year": 2023,
            "citationCount": 21,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "MarioGPT is the first text-to-level model trained to generate tile-based game levels, and it is shown that MarioGPT can not only generate diverse levels, but can be text-prompted for controllable level generation, addressing one of the key challenges of current PCG techniques."
            },
            "score": 1
        },
        {
            "id": "6ca16c1c2c60ceda87242c8f8e522d12cc4a13bc",
            "paperId": "6ca16c1c2c60ceda87242c8f8e522d12cc4a13bc",
            "title": "Eureka: Human-Level Reward Design via Coding Large Language Models",
            "abstract": "Large Language Models (LLMs) have excelled as high-level semantic planners for sequential decision-making tasks. However, harnessing them to learn complex low-level manipulation tasks, such as dexterous pen spinning, remains an open problem. We bridge this fundamental gap and present Eureka, a human-level reward design algorithm powered by LLMs. Eureka exploits the remarkable zero-shot generation, code-writing, and in-context improvement capabilities of state-of-the-art LLMs, such as GPT-4, to perform evolutionary optimization over reward code. The resulting rewards can then be used to acquire complex skills via reinforcement learning. Without any task-specific prompting or pre-defined reward templates, Eureka generates reward functions that outperform expert human-engineered rewards. In a diverse suite of 29 open-source RL environments that include 10 distinct robot morphologies, Eureka outperforms human experts on 83% of the tasks, leading to an average normalized improvement of 52%. The generality of Eureka also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF), readily incorporating human inputs to improve the quality and the safety of the generated rewards without model updating. Finally, using Eureka rewards in a curriculum learning setting, we demonstrate for the first time, a simulated Shadow Hand capable of performing pen spinning tricks, adeptly manipulating a pen in circles at rapid speed.",
            "year": 2023,
            "citationCount": 69,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "Eureka is presented, a human-level reward design algorithm powered by LLMs that exploits the remarkable zero-shot generation, code-writing, and in-context improvement capabilities of state-of-the-art LLMs to perform evolutionary optimization over reward code."
            },
            "score": 1
        },
        {
            "id": "e5656503ae333076103e24f9e003e36b8e56fb9f",
            "paperId": "e5656503ae333076103e24f9e003e36b8e56fb9f",
            "title": "Development and Validation of Payment Performance Assessment Tool for Construction Industry",
            "abstract": "The late payment has been the major issues facing the construction industry across the globe. Many studies have looked at this issues from management and legal perspectives. Despite its endemic effect on the construction industry, there is no any assessment tools for measuring the payment performance (late and prompt payment). This paper developed and validated a payment performance assessment tool in the context of construction industry client organization, particularly in the Nigerian construction industry. The tool was developed based on the organizational culture and payment performance attributes for the client organization. Analytical Hierarchy Process (AHP) was used to elicit pair-wise comparison and weight of each variable. This led to the development of the tool. The tool was then validated in the assessment of client\u2019s payment performance of a case organization. The assessment tool was calibrated based on consensus benchmarking. However, the calibration ranges from 0.00-0.69 as late payment performance, 0.70-0.98 prompt payment performance and 0.99-1.00. Absolute prompt payment performance. The result shows that the payment assessment tool succeeded in measuring payment performance of a case organization with a score of 0.50 points indicating that the organization do not pay its contractors on time. Therefore, the organization is experiencing late payment. However, there is need to replication the study to revalidate the tool in another organization or country.",
            "year": 2018,
            "citationCount": 0,
            "tldr": null,
            "score": 1
        },
        {
            "id": "d898624f4609bfff1d52168a6d3f0dcd575577d1",
            "paperId": "d898624f4609bfff1d52168a6d3f0dcd575577d1",
            "title": "Validation of the Genetically-Defined DLBCL Subtypes and Generation of a Parsimonious Probabilistic Classifier",
            "abstract": "Diffuse large B-cell lymphoma (DLBCL) is a clinically and molecularly heterogeneous disease with recognized transcriptional subtypes associated with normal cells of origin, activated B-cell (ABC) and germinal center B-cell (GCB) tumors. Emerging data suggested that additional heterogeneity existed, prompting us to comprehensively characterize genomic signatures of 304 newly diagnosed DLBCLs from patients treated with state-of-the-art therapy. We integrated recurrent mutations, somatic copy number alterations (SCNAs) and structural variants (SVs) and identified 5 genetically distinct DLBCL clusters (C1- C5 DLBCLs; Chapuy, Stewart, Dunford, et al. Nat Med 2018). Specifically, we identified two genetically distinct ABC subtypes, including favorable-risk C1 DLBCLs with features of extrafollicular origin and alterations also seen in transformed marginal zone lymphomas (NOTCH2 and NF-\u03baB pathway member mutations and BCL6 SVs). Unfavorable-risk C5 ABC DLBCLs harbored frequent 18q/BCL2 copy gain and co-occurring CD79B and MYD88L265P mutations. We also identified two genetically distinct GCB subtypes, including unfavorable-risk C3 DLBCLs with frequent BCL2 SVs, mutations in chromatin-modifying enzymes (CREBBP, MLL2, EZH2) and BCR/PI3K signaling pathway members (including inactivating PTEN mutations and copy loss). Favorable-risk C4 GCB DLBCLs had frequent mutations in core and linker histones and signaling intermediates (SGK1, BRAF and STAT3). Additionally, we identified an ABC/GCB-independent subtype, C2 DLBCLs, characterized by frequent bi-allelic TP53 inactivation, 9p21.23/CDKN2A copy loss and associated genomic instability reflected in recurrent SCNAs, increased genome doublings and a distinct outcome following induction therapy.\n A next step in utilizing the characterized genetic substructure was to confirm it in an independent series and develop a molecular classifier that allows prospective identification of C1-C5 DLBCLs. To this end, we accessed whole exome sequencing, copy number and SV data from a recent cohort of newly diagnosed DLBCLs (39 tumor-normal pairs, 462 tumor-only samples; Schmitz et al. NEJM 2018). All samples were re-analyzed using our mutational and SCNA pipelines and our newly generated tumor-only algorithm (Chapuy, Stewart, Dunford, et al. Nat Med 2018) to avoid batch effects and harmonize the datasets. SVs were used as reported. Purity and ploidy were inferred using ABSOLUTE and samples with missing data or low purity were removed. For the combined cohort (579 samples), we assessed our previously characterized 158 genetic drivers (Chapuy, Stewart, Dunford, et al. Nat Med 2018) and confirmed equal distribution of their marginal frequencies (R=0.88, p=1.5e-51), excluding batch effects. Next, we applied non-negative matrix factorization (NNF) consensus clustering to the combined dataset (158 genetic drivers vs. 579 tumors) and confirmed the C1-C5 DLBCL genetic clusters. Notably, tumors from both series contributed at comparable frequencies to the respective C1-C5 DLBCLs. We also noted an enrichment of the alternative genetic labels from Schmitz et al. in 3 of our C1-C5 DLBCL subtypes (B2N in C1 DLBCLs, p<0.0001; EZB in C3 DLBCLs, p<0.0001; MCD in C5 DLBCLs, p<0.0001). These data confirmed the identity of the C1-C5 DLBCL clusters in an independent cohort.\n Next, we developed a molecular classifier that prospectively identified C1-C5 DLBCLs using a minimum number of easy-to-measure features. The NMF-defined classes of the combined cohort were used as gold-standard training and validation datasets. We tested different models for classification and selected an artificial neural network approach which provides accurate classification of individual samples and well-calibrated confidence metrics. To minimize potential overtraining, we developed a reduced input feature set of the 22 most discriminating features, constructed confidence metrics for each sample and trained an ensemble of Feed-Forward Neural Networks via 10-fold cross validation. With this approach, our classifier had 84% accuracy for the total set and 94% accuracy for the high-confidence samples (70% of all samples).\n The newly developed parsimonious classifier will allow prospective identification of the independently confirmed C1-C5 DLBCL subtypes in newly diagnosed patients, a necessity for clinical application.\n \n \n Getz: Pharmacyclics: Research Funding; IBM: Research Funding; MuTect, ABSOLTUE, MutSig and POLYSOLVER: Patents & Royalties: MuTect, ABSOLTUE, MutSig and POLYSOLVER. Shipp:BMS: Consultancy, Honoraria, Membership on an entity's Board of Directors or advisory committees, Research Funding; Merck & Co.: Honoraria, Membership on an entity's Board of Directors or advisory committees, Research Funding; AstraZeneca: Honoraria, Membership on an entity's Board of Directors or advisory committees; Bayer: Research Funding; Gilead Sciences: Honoraria, Membership on an entity's Board of Directors or advisory committees; Takeda Pharmaceuticals: Honoraria, Membership on an entity's Board of Directors or advisory committees.\n",
            "year": 2019,
            "citationCount": 5,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The comprehensively characterize genomic signatures of 304 newly diagnosed DLBCLs from patients treated with state-of-the-art therapy were accessed and a molecular classifier that allows prospective identification of C1-C5 DLBCls was developed."
            },
            "score": 1
        },
        {
            "id": "bd553eb9d821114ef2fe179d27444bce5d1de770",
            "paperId": "bd553eb9d821114ef2fe179d27444bce5d1de770",
            "title": "[Bleedings under NOAC (non Vitamin-K dependent oral anticoagulants). Evidence and practical management].",
            "abstract": "UNLABELLED\nThe doses of these drugs are barely tested and the potential clinical thromboembolic risk must be taken into account. Despite the widespread use of NOAC (non vitamin-K dependent oral anticoagulants) and recommendations of regulatory agencies and first consensus meeting on handling the bleeding situation under NOAC, especially in hospitals without a large hemostatic focus, uncertainty still exists. In case of mild bleeding from a clinical perspective, the medical care of these patients and the delay of the next dose or discontinuation is advised. A special laboratory analysis is indicated i.e. in case of known higher grade liver and kidney failure, which can cause a prolonged elimination of NOAC. The administration of factor concentrates is not indicated in this situation. In case of moderate to severe bleeding, the primary measures focus on the stabilization of the heart and circulatory function and parallel on the treatment depending on the localization of the bleeding source. According to experience, mostly gastrointestinal bleeding occurs under the NOAC, which should be supplied endoscopically. In life-threatening bleeding in addition to the measures of hemodynamic stabilization usually a special haemostasis management is required, which should be mainly clinically oriented. After the assessment of bleeding predictor, the time of the last dose and the dose of NOAC should be learned, but other causes of bleeding, including Fibrinolysis, should be excluded or treated. Subsequently, routinely promptly rivaroxaban and/or apixaban sensitive thromboplastin time (Quick's value) and a thrombin time (thrombin-poor calibrator) for qualitative assessment can be carried out because only very few hospitals have specific tests (anti-Xa measurements, bovine thrombin), which could be promptly done. If there is a significant deviation from the normal range or to present preliminary value of particular patient, an effect of NOAC most likely exists. In life-threatening bleeding the use of factor concentrates (procoagulants) is indicated. The first-line therapy should be PPSB. Only in exceptional cases, especially when dabigatran is taken, the use of aPPSB (FEIBA\u00ae) for prompt haemostasis can be considered. The haemostasis should be always clinically estimated and not according to coagulation tests. The use of rFVIIa (Novo Seven\u00ae) shows different results in the bleeding therapy (reversal) under Dabigatran. The doses of these drugs are barely tested and the potential clinical thromboembolic risk must be taken into account.\n\n\nCONCLUSION\nThe current concepts of the newly developed antidotes are not clinically validated. First prospective, clinical registries have been started.",
            "year": 2015,
            "citationCount": 7,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The current concepts of the newly developed antidotes are not clinically validated and only in exceptional cases, especially when dabigatran is taken, the use of aPPSB for prompt haemostasis can be considered."
            },
            "score": 1
        },
        {
            "id": "cba5991a880e200077de4960f21db8a1b15f781c",
            "paperId": "cba5991a880e200077de4960f21db8a1b15f781c",
            "title": "Development of payment performance assessment tools for the Nigerian construction industry",
            "abstract": "The widespread of late and non-payment in the construction industry is increasing at an alarming rate. Many researchers have investigated these problems in many countries. However, most of the studies focused on solutions based on managerial/administrative approaches emphasizing the causes, effects, reactions, and impacts. Besides, many of them tend to see the solutions to the payment problems from a legal perspective. This has led to the introduction of the legislative Payment Act in various countries of the world. Despite the rampant occurrence of late and non-payment, none of the research has looked at the criteria and attributes for evaluating client\u2019s payment performance in the industry. This study developed and validated the payment performance assessment tools on four selected organizations. Two methodologies, the Delphi method and Analytical Hierarchy Process (AHP) were used. After three rounds of inquiries with a panel of experts, a consensus was reached on organizational culture and payment performance attributes for public and private organizations. Moreover, Spearman Correlation was run for public and private client organizational culture, and the results revealed the existence of positive relationships between client\u2019s organizational culture and payment performance attributes when the level of significance was one percent (r = -0.561, Sig. = 0.004). The payment performance assessment tools were developed by integrating organizational culture and payment performance attributes. The tools were calibrated based on consensus benchmarking, ranging from 0.00-0.69 as late payment performance, 0.70-0.98 as prompt payment performance, and 0.99-1.00 as absolute/ perfect prompt payment performance. Finally, the payment performance levels of the 4 organizations were evaluated, and the results showed that organization (A) obtained 0.88 point, organization (B) recorded 0.509 point, organization (C) had 0.945 and organization (D) obtained 0.114 point. None of the organizations fell into the absolute prompt payment performance category. However, two of these organizations experienced late payment performance and the other two had prompt payment performance. The findings showed that a strong organizational culture resulted in prompt payment performance and vice-versa. This research provides payment performance assessment tools for the construction community that will be useful for clients and contractors. Besides, the present findings provide insights into a client\u2019s payment performance level. Based on the study, the clients should focus on supportive and innovative culture to achieve prompt payment performance and it is recommended that the study be extended across regions.",
            "year": 2019,
            "citationCount": 0,
            "tldr": null,
            "score": 1
        },
        {
            "id": "6eab40aca492c056fb84d2a89918778e5e478cc1",
            "paperId": "6eab40aca492c056fb84d2a89918778e5e478cc1",
            "title": "Blutungen unter NOAK",
            "abstract": "Summary The doses of these drugs are barely tested and the potential clinical thromboembolic risk must be taken into account. Despite the widespread use of NOAC (non vitamin-K dependent oral anticoagulants) and recommendations of regulatory agencies and first consensus meeting on handling the bleeding situation under NOAC, especially in hospitals without a large hemostatic focus, uncertainty still exists. In case of mild bleeding from a clinical perspective, the medical care of these patients and the delay of the next dose or discontinuation is advised. A special laboratory analysis is indicated i.e. in case of known higher grade liver and kidney failure, which can cause a prolonged elimination of NOAC. The administration of factor concentrates is not indicated in this situation. In case of moderate to severe bleeding, the primary measures focus on the stabilization of the heart and circulatory function and parallel on the treatment depending on the localization of the bleeding source. According to experience, mostly gastrointestinal bleeding occurs under the NOAC, which should be supplied endoscopically. In life-threatening bleeding in addition to the measures of hemodynamic stabilization usually a special haemostasis management is required, which should be mainly clinically oriented. After the assessment of bleeding predictor, the time of the last dose and the dose of NOAC should be learned, but other causes of bleeding, including Fibrinolysis, should be excluded or treated. Subsequently, routinely promptly rivaroxaban and/ or apixaban sensitive thromboplastin time (Quick\u2019s value) and a thrombin time (thrombin- poor calibrator) for qualitative assessment can be carried out because only very few hospitals have specific tests (anti-Xa measurements, bovine thrombin), which could be promptly done. If there is a significant deviation from the normal range or to present preliminary value of particular patient, an effect of NOAC most likely exists. In life-threatening bleeding the use of factor concentrates (procoagulants) is indicated. The first-line therapy should be PPSB. Only in exceptional cases, especially when dabigatran is taken, the use of aPPSB (FEIBA\u00ae) for prompt haemostasis can be considered. The haemostasis should be always clinically estimated and not according to coagulation tests. The use of rFVIIa (Novo Seven\u00ae) shows different results in the bleeding therapy (reversal) under Dabigatran. The doses of these drugs are barely tested and the potential clinical thromboembolic risk must be taken into account. Conclusion The current concepts of the newly developed antidotes are not clinically validated. First prospective, clinical registries have been started.",
            "year": 2015,
            "citationCount": 0,
            "tldr": {
                "model": "tldr@v2.0.0",
                "text": "The current concepts of the newly developed antidotes are not clinically validated and the doses of these drugs are barely tested and the potential clinical thromboembolic risk must be taken into account."
            },
            "score": 1
        },
        {
            "id": "7cd725937cc8b0ee80827def8b469aba489988f2",
            "paperId": "7cd725937cc8b0ee80827def8b469aba489988f2",
            "title": "Survival and Health at the Oldest Ages",
            "abstract": "Survival and Health at the Oldest Ages Tony Warnes John R. Wilmoth. 1995. Are mortality rates falling at extremely high ages? An investigation based on a model proposed by Coale and Kisker. Population Studies, 49, 281-295. The relationship between increasing years of age after the ninth decade of life and increments of mortality risk has recently attracted the intense attention of some demographers. The special fascination is prompted partly by the healthy survivor hypothesis (that people who attain exceptionally high ages are selectively vigorous), partly by the utilitarian interest in projecting trends in healthy versus unhealthy life expectancy, and partly by the search for evidence that the maximum human life span is either greater than the scientific consensus asserts (i.e. around 121 years), or that it is increasing. Like many other disputed fields, competing claims survive partly because of the scarcity or unreliability of data. The author, from the Department of Demography at the University of California, Berkeley, begins with the question, ' If there are limits to the continued reduction in human mortality, how do they operate?' and then considers the difficulties of establishing the extent and pace of recent reductions in late-age mortality: namely random variation among small numbers, and inaccurate age reporting through the heaping of zeros and fives and age-exaggeration. To adjust for ageexaggeration in the United States, Coale and Kisker (1990) proposed a model which assumes (a) that the rate of increase in age-specific mortality above 85 years declines as a linear function of age, and (b) that the level of mortality at age 110 years is constant across time and in various populations. Wilmoth critically examines these assumptions with data from Sweden and Japan for 1971\u201490. Only the pattern for Japanese men deviates substantially from the linear function. Coale and Kisker regarded United States mortality rates for ages over 85 years as unreliable, hence their second assumption. Wilmoth has more confidence in his data and uses alternative procedures for fitting functions to (a) the Swedish empirical death rates from 85 to 100 years for 1896-1900 to 1986-90, and (b) the Japanese death rates for the same ages from 1951-55 to 1986-90. Additionally, he calibrates",
            "year": 1996,
            "citationCount": 1,
            "tldr": null,
            "score": 1
        }
    ],
    "novelty": "no"
}